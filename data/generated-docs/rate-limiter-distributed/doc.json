{"html":"<h1 id=\"distributed-rate-limiter-design-document\">Distributed Rate Limiter: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A distributed rate limiting system that enforces request quotas across multiple application instances using Redis as a shared state store. The key challenge is maintaining accurate rate limits in a distributed environment while handling Redis failures, clock skew, and achieving high performance through sharding.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this foundational understanding applies to rate limiting algorithms, multi-tier systems, Redis integration, sharding, and API design.</p>\n</blockquote>\n<h3 id=\"mental-model-the-nightclub-bouncer-system\">Mental Model: The Nightclub Bouncer System</h3>\n<p>Think of distributed rate limiting like managing capacity across a chain of popular nightclubs during New Year&#39;s Eve. Each nightclub has a fire safety capacity limit, but customers don&#39;t just visit one location - they might hop between venues throughout the night. The challenge is ensuring that the combined crowd across all locations doesn&#39;t exceed what the fire department allows, while also managing individual venue limits and VIP customer quotas.</p>\n<p>In a traditional single-venue scenario, you&#39;d have one bouncer with a mechanical clicker counter at the door. Every person entering gets counted up, every person leaving gets counted down. The bouncer always knows exactly how many people are inside and can make instant decisions. This represents <strong>local rate limiting</strong> - one application instance tracking its own request counts in memory.</p>\n<p>But when you expand to multiple venues (distributed application instances), the problem becomes exponentially more complex. Now you need to coordinate between multiple bouncers across the city. If your fire department limit is 1000 people total across all venues, and Bouncer Alice at Club Downtown has seen 400 people enter while Bouncer Bob at Club Uptown has seen 450 people, what happens when someone approaches Bouncer Charlie at Club Westside? Charlie needs to know the current total (850) before deciding whether to allow the 851st person in.</p>\n<p>The naive approach would be to have each bouncer call every other bouncer before making a decision: &quot;Hey Alice, how many people do you have? Hey Bob, what&#39;s your count?&quot; This creates several problems. First, it&#39;s slow - every admission decision requires multiple phone calls. Second, it&#39;s fragile - if Alice doesn&#39;t answer her phone, does that mean Charlie should assume zero people at Downtown? Third, it creates race conditions - while Charlie is making his phone calls, Alice might admit 50 more people, making Charlie&#39;s decision based on stale data.</p>\n<p>The <strong>distributed rate limiting solution</strong> introduces a <strong>central coordination system</strong> (Redis) that acts like a real-time dispatch center. Instead of bouncers calling each other, they all report to and check with dispatch. When someone wants to enter Charlie&#39;s venue, Charlie radios dispatch: &quot;Request to admit one person.&quot; Dispatch has the real-time total across all venues and responds: &quot;Approved, new total is 851&quot; or &quot;Denied, at capacity.&quot; This provides several critical capabilities:</p>\n<p><strong>Atomic Decision Making</strong>: Dispatch can check the current count and increment it in a single atomic operation, preventing the race condition where two bouncers simultaneously think they&#39;re admitting the 1000th person when they&#39;re actually admitting the 999th and 1000th.</p>\n<p><strong>Global State Visibility</strong>: Every bouncer gets decisions based on the true global state, not stale local information or incomplete peer-to-peer communication.</p>\n<p><strong>Hierarchical Limits</strong>: Dispatch can enforce multiple types of limits simultaneously. Maybe the fire department allows 1000 total people, but Club Downtown has a structural limit of 300, and VIP customers get guaranteed access to 50 spots regardless of the general admission count.</p>\n<p><strong>Resilience Through Fallback</strong>: If the radio system goes down, bouncers can fall back to local-only decisions. They might occasionally exceed the global limit during the outage, but the venues remain operational rather than shutting down entirely.</p>\n<p>However, this coordination comes with new challenges that don&#39;t exist in the single-venue scenario:</p>\n<p><strong>Network Latency</strong>: Radio calls to dispatch take time. In high-traffic periods, the delay between &quot;request to admit&quot; and &quot;approved/denied&quot; could cause customer frustration.</p>\n<p><strong>Central Point of Failure</strong>: If dispatch goes offline, all venues are affected. The system needs fallback strategies and redundancy.</p>\n<p><strong>Clock Synchronization</strong>: If Club Downtown&#39;s clocks are 5 minutes fast, their &quot;hourly&quot; limits might reset at different times than Club Uptown&#39;s, creating windows where the global limit can be exceeded.</p>\n<p><strong>Hot Spot Management</strong>: If a celebrity shows up at Club Downtown and generates massive traffic, that venue might overwhelm dispatch with admission requests, slowing down decisions for all other venues.</p>\n<p>This nightclub analogy maps directly to distributed rate limiting concepts:</p>\n<ul>\n<li><strong>Venues</strong> = Application instances</li>\n<li><strong>Bouncers</strong> = Rate limiter components within each instance</li>\n<li><strong>Customers</strong> = Incoming HTTP requests</li>\n<li><strong>Fire department capacity</strong> = Global rate limits</li>\n<li><strong>Venue structural limits</strong> = Per-instance or per-API limits</li>\n<li><strong>VIP quotas</strong> = Per-user rate limits</li>\n<li><strong>Dispatch center</strong> = Redis cluster</li>\n<li><strong>Radio communication</strong> = Network calls to Redis</li>\n<li><strong>Mechanical clickers</strong> = In-memory counters for local fallback</li>\n<li><strong>Clock synchronization</strong> = Time-based window alignment across nodes</li>\n</ul>\n<h3 id=\"existing-approaches-comparison\">Existing Approaches Comparison</h3>\n<p>Understanding the landscape of rate limiting approaches helps illuminate why distributed rate limiting requires careful design decisions. Each approach represents different trade-offs between accuracy, performance, complexity, and resilience.</p>\n<h4 id=\"local-vs-distributed-rate-limiting\">Local vs Distributed Rate Limiting</h4>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Local Rate Limiting</th>\n<th>Distributed Rate Limiting</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>State Storage</strong></td>\n<td>In-memory within each application instance</td>\n<td>Shared external store (Redis, database)</td>\n</tr>\n<tr>\n<td><strong>Decision Latency</strong></td>\n<td>Microseconds (memory access)</td>\n<td>Milliseconds (network + storage access)</td>\n</tr>\n<tr>\n<td><strong>Accuracy</strong></td>\n<td>Perfect for single-instance traffic</td>\n<td>Perfect for cluster-wide traffic</td>\n</tr>\n<tr>\n<td><strong>Failure Behavior</strong></td>\n<td>Independent failures per instance</td>\n<td>Coordinated failures across cluster</td>\n</tr>\n<tr>\n<td><strong>Implementation Complexity</strong></td>\n<td>Simple hash maps and timers</td>\n<td>Atomic operations, consensus, fallback logic</td>\n</tr>\n<tr>\n<td><strong>Resource Usage</strong></td>\n<td>Minimal CPU and memory overhead</td>\n<td>Network bandwidth, Redis memory, connection pools</td>\n</tr>\n<tr>\n<td><strong>Scaling Characteristics</strong></td>\n<td>Limits scale with instance count</td>\n<td>Limits remain constant regardless of instance count</td>\n</tr>\n</tbody></table>\n<p><strong>Local rate limiting</strong> excels in scenarios where each application instance handles completely independent traffic or where approximate limiting is acceptable. For example, if you have 10 application instances and want to limit each user to 100 requests per hour, local limiting would give each user 1000 requests per hour cluster-wide (100 per instance). This might be acceptable for rough abuse prevention but fails for precise quota enforcement or protecting downstream services with hard capacity limits.</p>\n<p><strong>Distributed rate limiting</strong> becomes essential when you need precise control over cluster-wide request rates. Consider a payment processing API where the downstream banking service can only handle 1000 transactions per minute total. With local limiting, you&#39;d need to divide this quota across instances (100 per instance if you have 10 instances), but this creates problems: if traffic is unevenly distributed, some instances might exhaust their quota while others remain idle, leading to artificial throttling despite available global capacity.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The choice between local and distributed rate limiting fundamentally depends on whether your limiting goal is per-instance abuse prevention (local) or cluster-wide resource protection (distributed).</p>\n</blockquote>\n<h4 id=\"in-memory-vs-persistent-storage\">In-Memory vs Persistent Storage</h4>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>In-Memory Storage</th>\n<th>Persistent Storage (Redis)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Performance</strong></td>\n<td>Fastest (no I/O)</td>\n<td>Fast (network + memory access)</td>\n</tr>\n<tr>\n<td><strong>Durability</strong></td>\n<td>Lost on process restart</td>\n<td>Survives restarts and failures</td>\n</tr>\n<tr>\n<td><strong>Sharing</strong></td>\n<td>Cannot share between instances</td>\n<td>Natural sharing across cluster</td>\n</tr>\n<tr>\n<td><strong>Memory Usage</strong></td>\n<td>Grows with active rate limit keys</td>\n<td>Centralized memory usage</td>\n</tr>\n<tr>\n<td><strong>Consistency</strong></td>\n<td>Eventually consistent across instances</td>\n<td>Strongly consistent with atomic operations</td>\n</tr>\n<tr>\n<td><strong>Operational Complexity</strong></td>\n<td>No external dependencies</td>\n<td>Requires Redis cluster management</td>\n</tr>\n</tbody></table>\n<p><strong>In-memory storage</strong> using hash maps or similar structures provides the lowest latency for rate limiting decisions. Popular libraries like Google&#39;s <code>golang.org/x/time/rate</code> or Java&#39;s Guava RateLimiter implement sophisticated algorithms entirely in memory. However, this approach has fundamental limitations in distributed systems:</p>\n<ul>\n<li><strong>State Isolation</strong>: Each instance maintains separate counters, making precise global limits impossible</li>\n<li><strong>Cold Start Problems</strong>: New instances start with empty rate limit state, potentially allowing bursts that exceed intended limits</li>\n<li><strong>Restart Penalties</strong>: Process restarts reset all counters, effectively giving users fresh quota allocations</li>\n</ul>\n<p><strong>Persistent storage</strong> through Redis or similar systems enables true distributed coordination but introduces new complexities:</p>\n<ul>\n<li><strong>Network Partitions</strong>: What happens when an application instance can reach Redis but other instances cannot?</li>\n<li><strong>Redis Failures</strong>: How do you maintain availability when the coordination layer fails?</li>\n<li><strong>Data Consistency</strong>: Ensuring that concurrent updates from multiple instances don&#39;t corrupt rate limit counters</li>\n</ul>\n<blockquote>\n<p><strong>Decision: Redis as Coordination Layer</strong></p>\n<ul>\n<li><strong>Context</strong>: Need for atomic operations, high availability, and performance in distributed rate limiting</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Database-backed counters with transactions</li>\n<li>Redis with Lua scripts for atomicity</li>\n<li>Distributed consensus systems (etcd, Consul)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Redis with Lua scripts</li>\n<li><strong>Rationale</strong>: Redis provides microsecond-latency atomic operations through Lua scripts, built-in expiration for time-window management, and mature high-availability clustering. Database transactions add unnecessary overhead for simple counter operations, while consensus systems optimize for different use cases (configuration management) rather than high-throughput counting.</li>\n<li><strong>Consequences</strong>: Enables precise distributed rate limiting with excellent performance, but requires Redis operational expertise and introduces dependency on Redis availability.</li>\n</ul>\n</blockquote>\n<h4 id=\"algorithm-complexity-vs-accuracy-trade-offs\">Algorithm Complexity vs Accuracy Trade-offs</h4>\n<p>Different rate limiting algorithms make different trade-offs between implementation complexity, memory usage, and limiting accuracy:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Memory Usage</th>\n<th>Accuracy</th>\n<th>Burst Handling</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Fixed Window Counter</strong></td>\n<td>O(1) per key</td>\n<td>Poor (2x limit possible)</td>\n<td>Allows full limit per window</td>\n<td>Simple</td>\n</tr>\n<tr>\n<td><strong>Sliding Window Log</strong></td>\n<td>O(n) per key (n=requests)</td>\n<td>Perfect</td>\n<td>Precise burst control</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td><strong>Sliding Window Counter</strong></td>\n<td>O(k) per key (k=sub-windows)</td>\n<td>Good (configurable accuracy)</td>\n<td>Smooth burst handling</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td><strong>Token Bucket</strong></td>\n<td>O(1) per key</td>\n<td>Good (allows configured burst)</td>\n<td>Explicit burst capacity</td>\n<td>Simple</td>\n</tr>\n<tr>\n<td><strong>Leaky Bucket</strong></td>\n<td>O(n) per key (n=queued requests)</td>\n<td>Perfect (no bursts)</td>\n<td>No burst allowance</td>\n<td>Complex</td>\n</tr>\n</tbody></table>\n<p><strong>Fixed Window Counter</strong> represents the simplest approach: reset a counter every time period (e.g., every minute). This creates the &quot;boundary problem&quot; where a user could make 1000 requests in the last second of one minute and another 1000 requests in the first second of the next minute, effectively achieving 2000 requests per minute despite a 1000/minute limit.</p>\n<p><strong>Sliding Window Log</strong> maintains a timestamp for every request within the current window. This provides perfect accuracy but consumes memory proportional to the request rate, making it expensive for high-traffic scenarios.</p>\n<p><strong>Token Bucket</strong> offers a middle ground by modeling rate limits as a bucket that starts full of tokens, loses tokens with each request, and refills at a steady rate. This naturally handles bursts (empty the bucket quickly) while enforcing long-term rate limits (bucket refill rate).</p>\n<blockquote>\n<p><strong>Architecture Decision</strong>: The distributed rate limiter implements multiple algorithms to handle different use cases. Token bucket for APIs that benefit from burst allowance, sliding window counter for smooth rate enforcement, and sliding window log for scenarios requiring perfect accuracy despite higher memory costs.</p>\n</blockquote>\n<h4 id=\"fallback-strategy-comparison\">Fallback Strategy Comparison</h4>\n<p>When the coordination layer (Redis) becomes unavailable, different fallback strategies offer different trade-offs:</p>\n<table>\n<thead>\n<tr>\n<th>Fallback Strategy</th>\n<th>Availability</th>\n<th>Accuracy During Outage</th>\n<th>Recovery Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Fail Open</strong></td>\n<td>100% (no limiting)</td>\n<td>0% (unlimited requests)</td>\n<td>Immediate return to normal</td>\n</tr>\n<tr>\n<td><strong>Fail Closed</strong></td>\n<td>0% (reject all)</td>\n<td>100% (no limit violations)</td>\n<td>Immediate return to normal</td>\n</tr>\n<tr>\n<td><strong>Local Fallback</strong></td>\n<td>High (degraded limiting)</td>\n<td>Poor (per-instance limits)</td>\n<td>Gradual convergence</td>\n</tr>\n<tr>\n<td><strong>Circuit Breaker</strong></td>\n<td>Variable (configurable)</td>\n<td>Variable (configurable)</td>\n<td>Staged recovery testing</td>\n</tr>\n</tbody></table>\n<p><strong>Fail Open</strong> prioritizes availability by allowing all requests through during Redis outages. This suits scenarios where brief periods of unlimited access are preferable to service disruption, such as content delivery or social media APIs.</p>\n<p><strong>Fail Closed</strong> prioritizes protection by rejecting requests during outages. This suits scenarios where exceeding limits could cause cascading failures, such as payment processing or database write APIs.</p>\n<p><strong>Local Fallback</strong> attempts to maintain degraded rate limiting by switching to per-instance limits during outages. Each instance divides the global limit by the number of instances and enforces this reduced limit locally. This provides some protection while maintaining availability, but can lead to under-utilization if traffic is unevenly distributed.</p>\n<p><strong>Circuit Breaker Pattern</strong> implements intelligent failure detection and recovery testing. Rather than immediately resuming full Redis usage when connectivity returns, the circuit breaker gradually increases traffic to Redis while monitoring for continued failures.</p>\n<blockquote>\n<p><strong>Decision: Graceful Degradation with Local Fallback</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance availability with protection during Redis outages</li>\n<li><strong>Options Considered</strong>: Fail open, fail closed, local fallback with circuit breaker</li>\n<li><strong>Decision</strong>: Local fallback with gradual recovery</li>\n<li><strong>Rationale</strong>: Maintains both availability and some level of protection during outages. Circuit breaker prevents thundering herd problems when Redis recovers. Local fallback provides predictable behavior that operations teams can reason about.</li>\n<li><strong>Consequences</strong>: Enables continued operation during Redis outages with degraded but predictable rate limiting. Requires careful configuration of per-instance quotas and circuit breaker thresholds.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls-in-distributed-rate-limiting\">Common Pitfalls in Distributed Rate Limiting</h3>\n<p>⚠️ <strong>Pitfall: Assuming Network Calls Are Instantaneous</strong></p>\n<p>Many developers initially design distributed rate limiters as if Redis calls have zero latency. They write code that makes synchronous Redis calls in the request path without considering timeout handling, connection pooling, or retry logic. Under load, this creates cascading delays where rate limiting decisions become the bottleneck rather than the protection mechanism.</p>\n<p>The fix requires treating every Redis interaction as a potentially slow network operation with explicit timeouts, connection reuse, and circuit breaker patterns. Rate limiting checks should typically complete within 1-2 milliseconds; anything slower suggests architectural problems.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Clock Skew Between Instances</strong></p>\n<p>Time-based rate limiting algorithms assume synchronized clocks across all application instances. In practice, server clocks can drift by seconds or minutes, causing time window boundaries to misalign. This creates windows where users can exceed limits by making requests to instances with fast clocks just before window boundaries and slow clocks just after boundaries.</p>\n<p>The solution involves using Redis server time for all time-based calculations rather than application instance time, or implementing clock synchronization monitoring with alerts when drift exceeds acceptable thresholds.</p>\n<p>⚠️ <strong>Pitfall: Not Handling Redis Memory Pressure</strong></p>\n<p>Redis operates as an in-memory database, and rate limiting can generate enormous numbers of keys (user IDs, IP addresses, API endpoints combined with time windows). Without proper key expiration and memory management, Redis can run out of memory, causing either data eviction or service failures.</p>\n<p>The fix requires careful key naming with TTL management, monitoring Redis memory usage, and implementing key cleanup strategies for inactive rate limit entries. Every rate limit key should have an explicit expiration time, typically 2x the rate limit window duration.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete technology recommendations and starter code for building the distributed rate limiter foundation.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Redis Client</strong></td>\n<td><code>go-redis/redis/v9</code> (Redis client)</td>\n<td><code>go-redis/redis/v9</code> with cluster support</td>\n</tr>\n<tr>\n<td><strong>HTTP Framework</strong></td>\n<td><code>net/http</code> with middleware</td>\n<td><code>gin-gonic/gin</code> or <code>gorilla/mux</code></td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>Environment variables + <code>os.Getenv</code></td>\n<td><code>viper</code> configuration management</td>\n</tr>\n<tr>\n<td><strong>Metrics</strong></td>\n<td><code>expvar</code> for basic metrics</td>\n<td><code>prometheus/client_golang</code></td>\n</tr>\n<tr>\n<td><strong>Logging</strong></td>\n<td><code>log/slog</code> (Go 1.21+)</td>\n<td><code>uber-go/zap</code> for structured logging</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td><code>testing</code> + <code>testcontainers</code> for Redis</td>\n<td><code>stretchr/testify</code> + test containers</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>distributed-rate-limiter/\n├── cmd/\n│   ├── server/main.go              ← HTTP server with rate limiting middleware\n│   └── dashboard/main.go           ← Management dashboard server\n├── internal/\n│   ├── ratelimit/                  ← Core rate limiting logic\n│   │   ├── limiter.go             ← Main rate limiter interface\n│   │   ├── algorithms/            ← Rate limiting algorithm implementations\n│   │   │   ├── token_bucket.go    ← Token bucket algorithm\n│   │   │   ├── sliding_window.go  ← Sliding window algorithms\n│   │   │   └── algorithm.go       ← Common algorithm interface\n│   │   ├── storage/               ← Storage backends\n│   │   │   ├── redis.go          ← Redis backend implementation\n│   │   │   ├── local.go          ← Local fallback storage\n│   │   │   └── storage.go        ← Storage interface\n│   │   └── config/               ← Configuration and rule management\n│   │       ├── rules.go          ← Rate limit rule definitions\n│   │       └── manager.go        ← Dynamic configuration management\n│   ├── api/                      ← REST API for management\n│   │   ├── handlers.go           ← HTTP handlers for CRUD operations\n│   │   └── middleware.go         ← Rate limiting middleware\n│   ├── dashboard/                ← Real-time dashboard components\n│   │   ├── websocket.go          ← WebSocket handlers for real-time updates\n│   │   └── metrics.go            ← Metrics collection and aggregation\n│   └── sharding/                 ← Consistent hashing and sharding\n│       ├── consistent_hash.go    ← Consistent hash ring implementation\n│       └── node_manager.go       ← Redis node health and management\n├── pkg/                          ← Public interfaces (if needed)\n├── configs/                      ← Configuration files\n│   └── rate_limits.yaml         ← Default rate limit rules\n├── scripts/                      ← Deployment and utility scripts\n│   ├── redis_setup.sh           ← Redis cluster setup script\n│   └── load_test.sh             ← Load testing script\n├── docs/                         ← Additional documentation\n└── docker-compose.yml            ← Local development environment</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Redis Connection Manager</strong> (<code>internal/ratelimit/storage/redis.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/redis/go-redis/v9</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisConfig holds Redis connection configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Addresses    []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Password     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"password\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DB           </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"db\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PoolSize     </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"pool_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DialTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"dial_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisStorage implements the Storage interface using Redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisStorage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#B392F0\">RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRedisStorage creates a new Redis storage backend with connection pooling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create Redis universal client (handles both single instance and cluster)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Configure connection pool with proper timeouts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test connectivity with ping command</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return configured storage instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckAndUpdate atomically checks current count and updates if limit allows</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckAndUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#FFAB70\">allowed</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">remaining</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">resetTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: This will be implemented with Lua scripts in Redis Backend Integration section</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For now, return placeholder values for basic connectivity testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">, limit</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(window), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Rate Limit Rule Configuration</strong> (<code>internal/ratelimit/config/rules.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitRule defines a single rate limiting rule</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"id\" yaml:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"name\" yaml:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    KeyPattern  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"key_pattern\" yaml:\"key_pattern\"`</span><span style=\"color:#6A737D\">   // e.g., \"user:{user_id}\", \"ip:{ip_address}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"algorithm\" yaml:\"algorithm\"`</span><span style=\"color:#6A737D\">        // \"token_bucket\", \"sliding_window_counter\", etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Limit       </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"limit\" yaml:\"limit\"`</span><span style=\"color:#6A737D\">               // Number of requests allowed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"window\" yaml:\"window\"`</span><span style=\"color:#6A737D\">             // Time window for the limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BurstLimit  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"burst_limit,omitempty\" yaml:\"burst_limit,omitempty\"`</span><span style=\"color:#6A737D\"> // For token bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"enabled\" yaml:\"enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Priority    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"priority\" yaml:\"priority\"`</span><span style=\"color:#6A737D\">         // Higher priority rules checked first</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"created_at\" yaml:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"updated_at\" yaml:\"updated_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RuleManager handles loading and updating rate limit rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RuleManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rules </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add mutex for concurrent access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add file watcher for dynamic updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add Redis pub/sub for distributed rule updates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadRules loads rate limit rules from configuration file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LoadRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Read YAML configuration file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Parse rules and validate configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Store rules in memory with indexing by key pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set up file watcher for automatic reloading</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetMatchingRules returns all rules that match the given request context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetMatchingRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">userID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ipAddress</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">apiEndpoint</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Match request attributes against key patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return rules sorted by priority (highest first)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Handle wildcard patterns and parameter substitution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Basic Rate Limiter Interface</strong> (<code>internal/ratelimit/limiter.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ratelimit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitResult contains the result of a rate limit check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Allowed     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"allowed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Remaining   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"remaining\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"retry_after,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResetTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"reset_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RuleID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"rule_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitRequest contains parameters for a rate limit check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"user_id,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IPAddress   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"ip_address,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    APIEndpoint </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"api_endpoint,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserAgent   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"user_agent,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tokens      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">  `json:\"tokens,omitempty\"`</span><span style=\"color:#6A737D\"> // Number of tokens to consume (default: 1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Limiter is the main interface for rate limiting operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Limiter</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check performs a rate limit check and updates counters if allowed</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Preview checks rate limit status without updating counters</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Preview</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Reset clears rate limit counters for the given request pattern</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Reset</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DistributedLimiter implements the Limiter interface with Redis backend</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DistributedLimiter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage     </span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localFallback </span><span style=\"color:#B392F0\">Limiter</span><span style=\"color:#6A737D\"> // Used when Redis is unavailable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewDistributedLimiter creates a new distributed rate limiter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDistributedLimiter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> Storage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ruleManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:     storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ruleManager: ruleManager,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Initialize local fallback limiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check performs distributed rate limiting with multi-tier evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Get matching rules from rule manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Evaluate rules in priority order with short-circuit logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: For each rule, generate Redis key and check limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return first rule that denies the request, or allow if all pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Handle Redis failures with local fallback</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-algorithm-skeleton\">Core Algorithm Skeleton</h4>\n<p><strong>Token Bucket Algorithm</strong> (<code>internal/ratelimit/algorithms/token_bucket.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> algorithms</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TokenBucketConfig defines parameters for token bucket algorithm</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucketConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Capacity   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"capacity\"`</span><span style=\"color:#6A737D\">    // Maximum tokens in bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RefillRate </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"refill_rate\"`</span><span style=\"color:#6A737D\"> // Tokens added per second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"window\"`</span><span style=\"color:#6A737D\">      // Window for rate calculation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TokenBucketState represents current state stored in Redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucketState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tokens        </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"tokens\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastRefillTime </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"last_refill_time\"`</span><span style=\"color:#6A737D\"> // Unix nanoseconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TokenBucket implements token bucket rate limiting algorithm</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucket</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#B392F0\">TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckAndUpdate performs atomic check-and-update for token bucket algorithm</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckAndUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tokensRequested</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#FFAB70\">allowed</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">remaining</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">resetTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate current time in nanoseconds for precision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute Lua script in Redis for atomic check-and-update:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Get current bucket state (tokens, last_refill_time)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Calculate tokens to add based on time elapsed and refill rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add tokens to bucket, capping at capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If sufficient tokens available, subtract requested tokens and allow</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If insufficient tokens, deny and calculate retry-after time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Update bucket state with new token count and refill time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Return result with remaining tokens and reset time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle Redis errors with appropriate fallback strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse Lua script response and construct result object</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Placeholder implementation for initial testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">, tb.config.Capacity </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> tokensRequested, time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(tb.config.Window), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Redis Lua Script Execution in Go</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Use redis.NewScript() to precompile Lua scripts for better performance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">script </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#B392F0\">NewScript</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    -- Your Lua script here</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    return {allowed, remaining, reset_time}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">`</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Execute with automatic retry and connection management</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> script.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(ctx, redisClient, []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{key}, arg1, arg2).</span><span style=\"color:#B392F0\">Result</span><span style=\"color:#E1E4E8\">()</span></span></code></pre></div>\n\n<p><strong>Time Handling for Rate Limiting</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Always use UTC to avoid timezone issues across distributed instances</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">now </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UTC</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Use UnixNano() for high-precision timestamps in Redis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">timestamp </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">UnixNano</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Calculate time windows with proper boundary alignment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">windowStart </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">Truncate</span><span style=\"color:#E1E4E8\">(windowDuration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">windowEnd </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> windowStart.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(windowDuration)</span></span></code></pre></div>\n\n<p><strong>Context and Timeout Management</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Set reasonable timeouts for Redis operations (1-2ms typical)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Millisecond)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Always check for context cancellation in long-running operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ctx.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Continue with Redis operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-basic-rate-limiter\">Milestone Checkpoint: Basic Rate Limiter</h4>\n<p>After implementing the foundational components, verify the system with these checkpoints:</p>\n<p><strong>Test 1: Redis Connectivity</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Server starts without errors, connects to Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check: Redis logs show successful connections from Go client</span></span></code></pre></div>\n\n<p><strong>Test 2: Basic Rate Limiting</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Send requests to test endpoint</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..10}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  curl</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-User-ID: test-user\"</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/test</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: First N requests succeed, subsequent requests return 429 Too Many Requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check: Response includes X-RateLimit-* headers with correct values</span></span></code></pre></div>\n\n<p><strong>Test 3: Configuration Loading</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Modify configs/rate_limits.yaml and restart server</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: New rules take effect without code changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check: Different endpoints have different rate limits as configured</span></span></code></pre></div>\n\n<p><strong>Common Issues and Debugging</strong>:</p>\n<ul>\n<li><strong>&quot;Connection refused&quot;</strong>: Ensure Redis is running on expected port (6379)</li>\n<li><strong>&quot;WRONGTYPE Operation&quot;</strong>: Redis key collision with existing data, use <code>FLUSHDB</code> to clear</li>\n<li><strong>&quot;Context deadline exceeded&quot;</strong>: Increase Redis operation timeouts or check network latency</li>\n<li><strong>&quot;Rate limit headers missing&quot;</strong>: Ensure middleware is properly installed in HTTP handler chain</li>\n</ul>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this foundational understanding applies to rate limiting algorithms, multi-tier systems, Redis integration, sharding, and API design throughout the project.</p>\n</blockquote>\n<h3 id=\"mental-model-the-traffic-control-center\">Mental Model: The Traffic Control Center</h3>\n<p>Think of our distributed rate limiter as a <strong>city-wide traffic control center</strong> coordinating traffic lights across an entire metropolitan area. Each traffic light (application instance) needs to make real-time decisions about allowing vehicles (requests) to pass through intersections (API endpoints). However, unlike independent traffic lights that only consider local conditions, our traffic control center must coordinate globally to prevent citywide congestion.</p>\n<p>The traffic control center maintains a <strong>shared understanding</strong> of traffic flow across all intersections. When a major event creates a surge of vehicles heading downtown, every traffic light needs to know about the overall traffic load, not just what&#39;s happening at their local intersection. Some intersections might need stricter controls (per-user limits), while major highways require global coordination (system-wide limits). The control center must continue functioning even when communication to some traffic lights is temporarily disrupted (graceful degradation).</p>\n<p>This analogy captures the essential challenge: <strong>local decisions with global coordination</strong>. Each application instance must make millisecond decisions about request acceptance while maintaining awareness of system-wide resource consumption patterns.</p>\n<h3 id=\"functional-goals\">Functional Goals</h3>\n<p>The distributed rate limiter must provide comprehensive request quota enforcement across multiple application instances with precise control over different limiting strategies. These capabilities form the foundation for protecting system resources while maintaining fair access for legitimate users.</p>\n<h4 id=\"core-rate-limiting-capabilities\">Core Rate Limiting Capabilities</h4>\n<p>Our system must support multiple <strong>rate limiting algorithms</strong> with different behavioral characteristics. The token bucket algorithm provides burst handling capabilities, allowing short periods of activity above the sustained rate while preventing long-term abuse. Users can temporarily exceed their baseline quota during legitimate usage spikes, but cannot sustain high request rates indefinitely. The sliding window counter algorithm offers memory-efficient approximate limiting with configurable accuracy trade-offs. This approach reduces memory overhead compared to exact tracking while maintaining reasonable accuracy for most use cases. The sliding window log algorithm provides precise request tracking with exact compliance checking, storing individual request timestamps to enable perfect accuracy at the cost of increased memory usage.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Accuracy</th>\n<th>Memory Usage</th>\n<th>Burst Handling</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket</td>\n<td>Good</td>\n<td>Low</td>\n<td>Excellent</td>\n<td>API endpoints with bursty traffic</td>\n</tr>\n<tr>\n<td>Sliding Window Counter</td>\n<td>Good</td>\n<td>Low</td>\n<td>Limited</td>\n<td>High-traffic endpoints requiring efficiency</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n<td>Perfect</td>\n<td>High</td>\n<td>None</td>\n<td>Critical endpoints requiring exact limits</td>\n</tr>\n</tbody></table>\n<p>The system must implement <strong>multi-tier rate limiting</strong> with hierarchical enforcement across different dimensions. Per-user limits prevent individual users from consuming excessive resources, with different quotas based on subscription tiers or usage patterns. Per-IP limits protect against unauthenticated abuse and distributed attacks, providing a safety net when user identification is unavailable or compromised. Per-API endpoint limits ensure that no single API can overwhelm system resources, with different thresholds based on endpoint complexity and resource requirements. Global system limits provide the ultimate protection against total system overload, aggregating usage across all users, IPs, and endpoints.</p>\n<p>The tier evaluation must follow a <strong>short-circuit strategy</strong> where exceeding any tier immediately blocks the request without evaluating remaining tiers. This approach minimizes computational overhead while ensuring the most restrictive applicable limit takes precedence. The system must support configurable priority ordering, allowing administrators to specify whether user limits should be checked before IP limits or vice versa based on their specific threat model.</p>\n<h4 id=\"dynamic-configuration-management\">Dynamic Configuration Management</h4>\n<p>Rate limit rules must be <strong>dynamically configurable</strong> without requiring application restarts or deployments. The <code>RuleManager</code> must support real-time rule updates, additions, and deletions through the management API. Rule changes must propagate to all application instances within a configurable time window, typically under 30 seconds for non-emergency changes and under 5 seconds for emergency rate limit adjustments.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Operation</th>\n<th>Target Latency</th>\n<th>Consistency Model</th>\n<th>Rollback Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Creation</td>\n<td>&lt; 30 seconds</td>\n<td>Eventually consistent</td>\n<td>Full rollback</td>\n</tr>\n<tr>\n<td>Rule Modification</td>\n<td>&lt; 30 seconds</td>\n<td>Eventually consistent</td>\n<td>Previous version restore</td>\n</tr>\n<tr>\n<td>Emergency Rate Limit</td>\n<td>&lt; 5 seconds</td>\n<td>Strong consistency</td>\n<td>Manual override</td>\n</tr>\n<tr>\n<td>Rule Deletion</td>\n<td>&lt; 60 seconds</td>\n<td>Eventually consistent</td>\n<td>Soft delete with restore</td>\n</tr>\n</tbody></table>\n<p>The system must support <strong>rule pattern matching</strong> using flexible key patterns that can incorporate user IDs, IP addresses, API endpoints, and custom attributes. Pattern matching should support wildcards, regular expressions, and hierarchical matching to enable sophisticated routing of requests to appropriate rate limit rules.</p>\n<h4 id=\"atomic-operations-and-consistency\">Atomic Operations and Consistency</h4>\n<p>All rate limit checks must be <strong>atomic operations</strong> that prevent race conditions between checking current usage and updating counters. The <code>CheckAndUpdate</code> method must implement check-and-increment as a single atomic operation, ensuring that concurrent requests cannot bypass limits by checking usage simultaneously before any updates occur.</p>\n<p>The system must maintain <strong>eventual consistency</strong> across the distributed cluster while providing strong consistency for individual rate limit decisions. When a request is approved and counted against a limit, that decision must be immediately reflected in subsequent rate limit checks for the same key, even under high concurrency.</p>\n<h4 id=\"state-persistence-and-recovery\">State Persistence and Recovery</h4>\n<p>Rate limit state must survive individual application instance failures and restarts. The Redis backend must maintain all rate limiting counters, token bucket states, and sliding window data with appropriate expiration policies to prevent indefinite memory growth.</p>\n<p>The system must support <strong>state recovery</strong> mechanisms for reconstructing local fallback state from Redis data when application instances restart. This capability ensures that transitioning between Redis-backed and local fallback modes maintains reasonable accuracy rather than resetting all limits to their initial values.</p>\n<h3 id=\"non-functional-goals\">Non-Functional Goals</h3>\n<p>The distributed rate limiter must meet stringent performance, reliability, and scalability requirements while maintaining operational simplicity and cost-effectiveness.</p>\n<h4 id=\"performance-requirements\">Performance Requirements</h4>\n<p>Rate limit checks must complete with <strong>sub-5 millisecond latency</strong> for the 95th percentile under normal load conditions. This requirement ensures that rate limiting does not become a bottleneck in request processing pipelines. The system must support at least <strong>100,000 rate limit checks per second per application instance</strong> while maintaining this latency target.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Metric</th>\n<th>Target</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>P50 Latency</td>\n<td>&lt; 1ms</td>\n<td>Request-response time for CheckAndUpdate</td>\n</tr>\n<tr>\n<td>P95 Latency</td>\n<td>&lt; 5ms</td>\n<td>95th percentile across all rate limit algorithms</td>\n</tr>\n<tr>\n<td>P99 Latency</td>\n<td>&lt; 10ms</td>\n<td>Including Redis network round trips</td>\n</tr>\n<tr>\n<td>Throughput</td>\n<td>100K ops/sec</td>\n<td>Per application instance sustained load</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>&lt; 100MB</td>\n<td>Per application instance excluding Redis</td>\n</tr>\n</tbody></table>\n<p>The Redis integration must implement <strong>connection pooling</strong> with configurable pool sizes to minimize connection establishment overhead. Connection reuse must be balanced with connection health monitoring to prevent using stale or failed connections that would increase error rates.</p>\n<h4 id=\"reliability-and-availability\">Reliability and Availability</h4>\n<p>The system must achieve <strong>99.9% availability</strong> for rate limiting decisions, measured as successful rate limit checks divided by total check attempts. This requirement must be met even during Redis node failures, network partitions, and planned maintenance activities.</p>\n<p><strong>Graceful degradation</strong> must ensure that rate limiting continues functioning when Redis is unavailable, falling back to local per-instance limiting with reduced accuracy. The fallback mode should maintain at least 80% of the intended rate limiting effectiveness while preserving system stability.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Scenario</th>\n<th>Recovery Time</th>\n<th>Degraded Capability</th>\n<th>Maintained Capability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single Redis Node</td>\n<td>&lt; 1 second</td>\n<td>Cross-instance coordination</td>\n<td>Per-instance limiting</td>\n</tr>\n<tr>\n<td>Redis Cluster</td>\n<td>&lt; 30 seconds</td>\n<td>Global rate limits</td>\n<td>Local fallback limits</td>\n</tr>\n<tr>\n<td>Network Partition</td>\n<td>&lt; 5 seconds</td>\n<td>Distributed coordination</td>\n<td>Independent operation</td>\n</tr>\n<tr>\n<td>Application Restart</td>\n<td>&lt; 10 seconds</td>\n<td>Warm cache state</td>\n<td>Cold start with Redis sync</td>\n</tr>\n</tbody></table>\n<p>The system must implement <strong>circuit breaker patterns</strong> for Redis operations, automatically switching to local fallback mode when Redis error rates exceed configurable thresholds. Circuit breaker state must be shared across application instances to prevent cascading failures.</p>\n<h4 id=\"scalability-requirements\">Scalability Requirements</h4>\n<p>The system must <strong>horizontally scale</strong> to support at least 1000 application instances sharing rate limit state through the Redis backend. This scaling must be achieved through consistent hashing and sharding strategies that distribute load evenly across Redis nodes.</p>\n<p><strong>Hot key detection</strong> must identify when specific rate limit keys experience disproportionate access patterns and automatically implement mitigation strategies such as key replication or local caching. The system should handle scenarios where a small number of users or API endpoints generate the majority of rate limit checks.</p>\n<p>Redis cluster scaling must support <strong>dynamic node addition and removal</strong> with minimal disruption to ongoing rate limiting operations. Consistent hashing with virtual nodes must minimize key redistribution when cluster topology changes occur.</p>\n<h4 id=\"monitoring-and-observability\">Monitoring and Observability</h4>\n<p>The system must provide comprehensive metrics for <strong>rate limiting effectiveness, performance, and resource usage</strong>. Metrics must be exported in Prometheus format with appropriate labels for filtering and aggregation across different dimensions.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Key Metrics</th>\n<th>Labels</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate Limiting</td>\n<td>Requests allowed/denied, limit utilization</td>\n<td>algorithm, tier, rule_id, endpoint</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>Check latency, throughput, error rate</td>\n<td>instance, redis_node, operation</td>\n</tr>\n<tr>\n<td>Resource Usage</td>\n<td>Redis memory, connection pool size, CPU</td>\n<td>node, algorithm, key_pattern</td>\n</tr>\n<tr>\n<td>Health</td>\n<td>Circuit breaker state, fallback mode</td>\n<td>instance, failure_type, recovery_time</td>\n</tr>\n</tbody></table>\n<p>Real-time dashboards must display current rate limit utilization across all tiers and provide alerting when usage approaches configured thresholds. Historical data must be retained for capacity planning and pattern analysis.</p>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>To maintain project scope and complexity at an intermediate level, several advanced features are explicitly excluded from this implementation. These non-goals help focus development efforts on core distributed rate limiting concepts while avoiding enterprise-grade complexity.</p>\n<h4 id=\"advanced-algorithm-features\">Advanced Algorithm Features</h4>\n<p><strong>Adaptive rate limiting</strong> that automatically adjusts limits based on system load, response times, or external signals is not included. While adaptive limiting can provide superior resource utilization, it introduces significant complexity around feedback loops, oscillation prevention, and parameter tuning that would distract from learning core distributed systems concepts.</p>\n<p><strong>Machine learning-based anomaly detection</strong> for identifying suspicious traffic patterns or predicting optimal rate limits is excluded. Such features require expertise in ML model training, feature engineering, and online learning systems that exceed the scope of an intermediate distributed systems project.</p>\n<p><strong>Geographic distribution</strong> with multi-region rate limiting coordination is not implemented. While global rate limiting across continents presents interesting challenges around latency, consistency, and network partitions, it requires infrastructure complexity beyond the Redis cluster approach used here.</p>\n<h4 id=\"enterprise-integration-features\">Enterprise Integration Features</h4>\n<p><strong>Authentication and authorization</strong> for the rate limiting system itself is simplified. Production systems require sophisticated access controls, API key management, and integration with enterprise identity providers, but these concerns are orthogonal to rate limiting algorithms and distributed coordination.</p>\n<table>\n<thead>\n<tr>\n<th>Excluded Feature</th>\n<th>Rationale</th>\n<th>Alternative Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>OAuth2/JWT Integration</td>\n<td>Authentication complexity exceeds scope</td>\n<td>Simple API key validation</td>\n</tr>\n<tr>\n<td>RBAC for Rate Limit Rules</td>\n<td>Authorization logic unrelated to core learning</td>\n<td>Basic admin/read-only roles</td>\n</tr>\n<tr>\n<td>Audit Logging</td>\n<td>Compliance features beyond technical focus</td>\n<td>Basic operation logging</td>\n</tr>\n<tr>\n<td>Encryption at Rest</td>\n<td>Security implementation complexity</td>\n<td>Redis AUTH password only</td>\n</tr>\n</tbody></table>\n<p><strong>Service mesh integration</strong> with Istio, Envoy, or similar platforms is not provided. While transparent rate limiting through sidecar proxies offers operational advantages, it requires understanding service mesh architectures, Envoy filter development, and Kubernetes operators that distract from rate limiting fundamentals.</p>\n<p><strong>Database persistence</strong> for rate limit rules and historical data is excluded in favor of file-based configuration and Redis storage. Database integration introduces schema design, migration management, and ORM complexity without teaching additional rate limiting concepts.</p>\n<h4 id=\"operational-complexity-features\">Operational Complexity Features</h4>\n<p><strong>Multi-tenancy</strong> with strict isolation between different customer environments is not implemented. True multi-tenancy requires namespace isolation, resource quotas, and security boundaries that significantly complicate the architecture without adding educational value for rate limiting concepts.</p>\n<p><strong>Automatic scaling and provisioning</strong> of the Redis cluster based on load patterns is excluded. While auto-scaling is valuable for production systems, it requires infrastructure automation, monitoring thresholds, and capacity planning logic that exceeds the project scope.</p>\n<p><strong>Disaster recovery and backup/restore</strong> capabilities are simplified. Production systems require point-in-time recovery, cross-region replication, and automated failover procedures that involve significant operational complexity beyond rate limiting algorithms.</p>\n<blockquote>\n<p><strong>Design Principle: Learning-Focused Scope</strong>\nThese non-goals ensure that learners can focus on mastering distributed rate limiting concepts without being overwhelmed by peripheral enterprise features. Each excluded feature represents a potential future enhancement once core concepts are solidified.</p>\n</blockquote>\n<h4 id=\"performance-and-scale-limitations\">Performance and Scale Limitations</h4>\n<p>The system is designed for <strong>medium-scale deployments</strong> rather than hyperscale environments. Supporting millions of unique rate limit keys, petabytes of historical data, or tens of thousands of application instances would require specialized data structures, storage engines, and coordination protocols that exceed intermediate complexity.</p>\n<p><strong>Real-time analytics and complex aggregations</strong> over rate limiting data are limited to basic usage metrics. Advanced analytics like percentile calculations, time-series forecasting, or correlation analysis with business metrics require specialized analytics databases and query engines.</p>\n<table>\n<thead>\n<tr>\n<th>Scale Limitation</th>\n<th>Design Target</th>\n<th>Beyond Scope</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Application Instances</td>\n<td>1,000 instances</td>\n<td>10,000+ instances</td>\n</tr>\n<tr>\n<td>Rate Limit Keys</td>\n<td>1M active keys</td>\n<td>100M+ keys</td>\n</tr>\n<tr>\n<td>Redis Cluster Size</td>\n<td>10-20 nodes</td>\n<td>100+ nodes</td>\n</tr>\n<tr>\n<td>Historical Retention</td>\n<td>30 days metrics</td>\n<td>Long-term data warehouse</td>\n</tr>\n</tbody></table>\n<p>These limitations ensure that the implementation remains comprehensible and deployable on modest infrastructure while teaching all essential distributed rate limiting concepts. Organizations requiring hyperscale capabilities can use this implementation as a foundation for understanding the principles before adopting specialized commercial solutions.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Client</td>\n<td><code>go-redis/redis</code> with basic connection pooling</td>\n<td><code>go-redis/redis</code> with cluster support and sentinel</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>YAML files with <code>gopkg.in/yaml.v3</code></td>\n<td>etcd with watch-based dynamic updates</td>\n</tr>\n<tr>\n<td>Metrics</td>\n<td>Prometheus <code>prometheus/client_golang</code></td>\n<td>Custom metrics with multiple exporters</td>\n</tr>\n<tr>\n<td>HTTP Framework</td>\n<td>Standard <code>net/http</code> with <code>gorilla/mux</code></td>\n<td>Gin or Echo with middleware support</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td><code>logrus</code> or <code>zap</code> structured logging</td>\n<td>OpenTelemetry with distributed tracing</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Standard <code>testing</code> package with testify</td>\n<td>Property-based testing with <code>gopter</code></td>\n</tr>\n</tbody></table>\n<p>For this intermediate-level project, we recommend the simple options to maintain focus on rate limiting concepts rather than framework complexity. The advanced options can be adopted later as operational requirements grow.</p>\n<h4 id=\"project-structure\">Project Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>distributed-rate-limiter/\n├── cmd/\n│   ├── server/main.go              ← HTTP API server entry point\n│   ├── cli/main.go                 ← Command-line management tool\n│   └── dashboard/main.go           ← Real-time dashboard server\n├── internal/\n│   ├── limiter/                    ← Core rate limiting logic\n│   │   ├── distributed.go          ← DistributedLimiter implementation\n│   │   ├── algorithms/             ← Rate limiting algorithms\n│   │   │   ├── token_bucket.go     ← TokenBucket implementation\n│   │   │   ├── sliding_window.go   ← Sliding window algorithms\n│   │   │   └── interface.go        ← Common algorithm interface\n│   │   └── fallback.go             ← Local fallback limiter\n│   ├── storage/                    ← Redis backend integration\n│   │   ├── redis.go                ← RedisStorage implementation\n│   │   ├── scripts.go              ← Lua script management\n│   │   └── sharding.go             ← Consistent hashing logic\n│   ├── config/                     ← Configuration management\n│   │   ├── rules.go                ← RuleManager implementation\n│   │   ├── loader.go               ← Configuration file loading\n│   │   └── validation.go           ← Rule validation logic\n│   ├── api/                        ← HTTP API handlers\n│   │   ├── handlers.go             ← Rate limit check endpoints\n│   │   ├── management.go           ← Rule management endpoints\n│   │   └── middleware.go           ← Rate limit headers middleware\n│   └── metrics/                    ← Monitoring and metrics\n│       ├── collector.go            ← Prometheus metrics\n│       └── dashboard.go            ← Real-time dashboard data\n├── configs/\n│   ├── rules.yaml                  ← Rate limit rule definitions\n│   └── redis.yaml                  ← Redis cluster configuration\n├── scripts/\n│   └── lua/                        ← Redis Lua scripts\n│       ├── token_bucket.lua        ← Token bucket algorithm\n│       ├── sliding_window.lua      ← Sliding window algorithm\n│       └── utils.lua               ← Common script utilities\n└── tests/\n    ├── integration/                ← Multi-instance integration tests\n    ├── chaos/                      ← Chaos engineering tests\n    └── benchmarks/                 ← Performance benchmarks</code></pre></div>\n\n<p>This structure separates concerns clearly while maintaining the <code>internal/</code> convention for non-exported packages. Each package has a single responsibility and minimal dependencies on other internal packages.</p>\n<h4 id=\"core-configuration-structures\">Core Configuration Structures</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// RedisConfig defines connection parameters for Redis cluster integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Addresses     []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `yaml:\"addresses\"`</span><span style=\"color:#6A737D\">     // Redis cluster node addresses</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Password      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"password\"`</span><span style=\"color:#6A737D\">      // AUTH password for Redis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DB            </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"db\"`</span><span style=\"color:#6A737D\">            // Database number (0-15)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PoolSize      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"pool_size\"`</span><span style=\"color:#6A737D\">     // Connection pool size per node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"read_timeout\"`</span><span style=\"color:#6A737D\">  // Socket read timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"write_timeout\"`</span><span style=\"color:#6A737D\"> // Socket write timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DialTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"dial_timeout\"`</span><span style=\"color:#6A737D\">  // Connection establishment timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRedisConfig creates a RedisConfig with sensible defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisConfig</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">RedisConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addresses:    []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"localhost:6379\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Password:     </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DB:           </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        PoolSize:     DEFAULT_POOL_SIZE,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  DEFAULT_TIMEOUT,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: DEFAULT_TIMEOUT,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DialTimeout:  DEFAULT_TIMEOUT,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"rate-limit-rule-definition\">Rate Limit Rule Definition</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// RateLimitRule defines a rate limiting policy that can be applied to requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"id\" json:\"id\"`</span><span style=\"color:#6A737D\">                    // Unique rule identifier</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"name\" json:\"name\"`</span><span style=\"color:#6A737D\">                // Human-readable rule name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    KeyPattern  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"key_pattern\" json:\"key_pattern\"`</span><span style=\"color:#6A737D\">  // Pattern for matching requests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"algorithm\" json:\"algorithm\"`</span><span style=\"color:#6A737D\">      // Algorithm: \"token_bucket\", \"sliding_window_counter\", \"sliding_window_log\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Limit       </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `yaml:\"limit\" json:\"limit\"`</span><span style=\"color:#6A737D\">              // Maximum requests per window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"window\" json:\"window\"`</span><span style=\"color:#6A737D\">            // Time window duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BurstLimit  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `yaml:\"burst_limit\" json:\"burst_limit\"`</span><span style=\"color:#6A737D\">  // Maximum burst size (token bucket only)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `yaml:\"enabled\" json:\"enabled\"`</span><span style=\"color:#6A737D\">          // Whether rule is active</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Priority    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"priority\" json:\"priority\"`</span><span style=\"color:#6A737D\">        // Evaluation priority (higher = first)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `yaml:\"created_at\" json:\"created_at\"`</span><span style=\"color:#6A737D\">    // Rule creation timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `yaml:\"updated_at\" json:\"updated_at\"`</span><span style=\"color:#6A737D\">    // Last modification timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"essential-constants\">Essential Constants</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Redis connection pool configuration</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_POOL_SIZE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#6A737D\">               // Connections per Redis node</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_TIMEOUT</span><span style=\"color:#F97583\">   =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Millisecond  </span><span style=\"color:#6A737D\">// Redis operation timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Rule priority levels for common use cases</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PRIORITY_HIGH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#6A737D\">                  // Emergency rate limits</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PRIORITY_LOW</span><span style=\"color:#F97583\">  =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">                   // Default background limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Rate limiting algorithms</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ALGORITHM_TOKEN_BUCKET</span><span style=\"color:#F97583\">       =</span><span style=\"color:#9ECBFF\"> \"token_bucket\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ALGORITHM_SLIDING_WINDOW_LOG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sliding_window_log\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ALGORITHM_SLIDING_COUNTER</span><span style=\"color:#F97583\">    =</span><span style=\"color:#9ECBFF\"> \"sliding_window_counter\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Rate limit header names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEADER_LIMIT_REMAINING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"X-RateLimit-Remaining\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEADER_LIMIT_RESET</span><span style=\"color:#F97583\">     =</span><span style=\"color:#9ECBFF\"> \"X-RateLimit-Reset\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEADER_RETRY_AFTER</span><span style=\"color:#F97583\">     =</span><span style=\"color:#9ECBFF\"> \"Retry-After\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After completing Milestone 1 (Rate Limiting Algorithms):</strong></p>\n<ul>\n<li>Run <code>go test ./internal/limiter/algorithms/...</code> - all algorithm tests should pass</li>\n<li>Create a simple CLI tool that can test each algorithm with configurable parameters</li>\n<li>Verify that token bucket allows bursts above sustained rate but prevents long-term abuse</li>\n<li>Test sliding window algorithms with requests clustered at window boundaries</li>\n<li>Expected behavior: Token bucket should allow 10 requests immediately if capacity=10, then throttle to refill rate</li>\n</ul>\n<p><strong>After completing Milestone 2 (Multi-tier Rate Limiting):</strong></p>\n<ul>\n<li>Run integration test with simultaneous per-user and global limits</li>\n<li>Verify that the most restrictive limit takes precedence using short-circuit evaluation</li>\n<li>Test rule pattern matching with wildcards and regular expressions</li>\n<li>Expected behavior: Request matching both user limit (100/hour) and global limit (1000/hour) should be limited by user quota first</li>\n</ul>\n<p><strong>After completing Milestone 3 (Redis Backend Integration):</strong></p>\n<ul>\n<li>Start Redis locally and run <code>go test ./internal/storage/...</code></li>\n<li>Test graceful degradation by stopping Redis mid-test and verifying local fallback</li>\n<li>Verify that concurrent requests don&#39;t bypass limits using atomic Lua scripts</li>\n<li>Expected behavior: Two application instances should coordinate limits correctly through Redis</li>\n</ul>\n<p><strong>After completing Milestone 4 (Consistent Hashing &amp; Sharding):</strong></p>\n<ul>\n<li>Deploy Redis cluster with 3 nodes and test key distribution</li>\n<li>Add/remove Redis nodes and verify minimal key redistribution</li>\n<li>Test hot key detection with skewed request patterns</li>\n<li>Expected behavior: Keys should be evenly distributed, with &lt;10% redistribution when adding nodes</li>\n</ul>\n<p><strong>After completing Milestone 5 (Rate Limit API &amp; Dashboard):</strong></p>\n<ul>\n<li>Start the HTTP API server and test rule management endpoints with curl</li>\n<li>Verify rate limit headers are included in all API responses</li>\n<li>Test the real-time dashboard updates as rate limits are consumed</li>\n<li>Expected behavior: Dashboard should show live usage percentages updating every second</li>\n</ul>\n<p>Each milestone should build incrementally, with later milestones reusing and extending earlier implementations rather than replacing them.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this architectural foundation enables rate limiting algorithms (Milestone 1), multi-tier evaluation (Milestone 2), Redis backend integration (Milestone 3), consistent hashing and sharding (Milestone 4), and API management (Milestone 5).</p>\n</blockquote>\n<p>The distributed rate limiter requires careful coordination between multiple components to maintain consistent quota enforcement across application instances while handling failures gracefully. Think of this system like a <strong>coordinated nightclub security operation</strong> - instead of just one bouncer at the door counting patrons, you have multiple entry points (application instances) that must communicate with a central dispatch system (Redis) to maintain an accurate headcount and enforce capacity limits consistently across all entrances.</p>\n<p>This mental model captures the core challenge: each application instance acts as an independent bouncer checking IDs and counting entries, but they all must coordinate through a shared communication system to ensure the overall venue doesn&#39;t exceed capacity. When the radio system goes down (Redis failure), each bouncer falls back to local counting with reduced accuracy, but the doors stay open.</p>\n<p>The architecture balances several competing concerns through a layered design. At the foundation, Redis provides atomic operations and cluster-wide state sharing. Above that, the rate limiting algorithms implement different quota enforcement strategies with varying accuracy and performance trade-offs. The multi-tier evaluation layer applies hierarchical limits across user, IP, API, and global dimensions. Finally, the management layer provides dynamic configuration and real-time monitoring capabilities.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<blockquote>\n<p><strong>Key Architectural Principle</strong>: The system maintains functionality even during partial failures through graceful degradation. Each component can operate in a reduced-capability mode when dependencies are unavailable, preventing cascading failures while maintaining core rate limiting capabilities.</p>\n</blockquote>\n<h3 id=\"component-overview\">Component Overview</h3>\n<p>The distributed rate limiter consists of seven primary components, each with distinct responsibilities and failure modes. Understanding these components and their interactions is crucial for implementing a robust system that handles the complexities of distributed quota enforcement.</p>\n<h4 id=\"distributedlimiter-component\">DistributedLimiter Component</h4>\n<p>The <code>DistributedLimiter</code> serves as the orchestration layer, coordinating multi-tier rate limit evaluation and managing fallback strategies during Redis failures. This component implements the primary <code>Check()</code> method that application code calls to verify whether a request should be allowed or rejected.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Failure Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Orchestration</td>\n<td>Coordinates multi-tier rate limit checks with short-circuit evaluation</td>\n<td>Continues with remaining tiers if one fails</td>\n</tr>\n<tr>\n<td>Algorithm Selection</td>\n<td>Routes requests to appropriate algorithm implementation based on rule configuration</td>\n<td>Falls back to token bucket if specified algorithm unavailable</td>\n</tr>\n<tr>\n<td>Fallback Coordination</td>\n<td>Switches to local rate limiting when Redis becomes unavailable</td>\n<td>Maintains reduced accuracy with per-instance limits</td>\n</tr>\n<tr>\n<td>Result Aggregation</td>\n<td>Combines results from multiple tiers into final allow/deny decision</td>\n<td>Uses most restrictive result from successful tier checks</td>\n</tr>\n</tbody></table>\n<p>The <code>DistributedLimiter</code> maintains no persistent state itself - it acts purely as a coordinator that delegates to storage backends and algorithm implementations. This stateless design ensures that multiple application instances can run identical rate limiting logic without coordination overhead.</p>\n<blockquote>\n<p><strong>Decision: Stateless Coordinator Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to run identical rate limiting logic across multiple application instances</li>\n<li><strong>Options Considered</strong>: Stateful coordinator with instance coordination, stateless coordinator with shared storage, leader-election based coordination</li>\n<li><strong>Decision</strong>: Stateless coordinator with all persistent state in Redis</li>\n<li><strong>Rationale</strong>: Eliminates complex coordination between application instances, simplifies deployment and scaling, enables any instance to handle any request</li>\n<li><strong>Consequences</strong>: All state must be externalized to Redis, requires careful atomic operation design, enables horizontal scaling without instance affinity</li>\n</ul>\n</blockquote>\n<h4 id=\"storage-backend-components\">Storage Backend Components</h4>\n<p>The storage layer abstracts persistent state management through a common <code>Storage</code> interface, with <code>RedisStorage</code> as the primary implementation and local storage options for fallback scenarios.</p>\n<p><strong>RedisStorage Component</strong></p>\n<p><code>RedisStorage</code> implements distributed state management using Redis cluster operations with atomic Lua scripts. This component handles the complexity of coordinating rate limit state across multiple application instances while maintaining consistency guarantees.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Implementation</th>\n<th>Consistency Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Operations</td>\n<td>Lua scripts for check-and-update</td>\n<td>Strong consistency within single Redis node</td>\n</tr>\n<tr>\n<td>Connection Pooling</td>\n<td>Redis universal client with configurable pool size</td>\n<td>Automatic failover between cluster nodes</td>\n</tr>\n<tr>\n<td>Cluster Support</td>\n<td>Consistent hashing for key distribution</td>\n<td>Eventually consistent across cluster during splits</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>Periodic ping with circuit breaker logic</td>\n<td>Automatic fallback when health check fails</td>\n</tr>\n</tbody></table>\n<p>The Redis integration uses Lua scripts to ensure atomicity of complex operations like token bucket refill-and-consume cycles. Without atomic operations, race conditions between multiple application instances could lead to incorrect quota enforcement - imagine two bouncers at different doors both thinking they&#39;re letting in the &quot;last&quot; patron simultaneously.</p>\n<p><strong>Local Storage Fallback</strong></p>\n<p>When Redis becomes unavailable, the system maintains functionality through local storage implementations that provide per-instance rate limiting. While accuracy is reduced (each instance enforces limits independently), the system continues protecting against abuse rather than failing open.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Type</th>\n<th>Use Case</th>\n<th>Accuracy Trade-off</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>In-Memory Map</td>\n<td>Development and fallback</td>\n<td>Lost on process restart</td>\n</tr>\n<tr>\n<td>Local Database</td>\n<td>Persistent fallback</td>\n<td>Per-instance limits only</td>\n</tr>\n<tr>\n<td>File-based Storage</td>\n<td>Simple persistence</td>\n<td>High latency, suitable for low-traffic fallback</td>\n</tr>\n</tbody></table>\n<h4 id=\"algorithm-implementation-components\">Algorithm Implementation Components</h4>\n<p>Each rate limiting algorithm is implemented as a separate component that works with the storage abstraction. This design allows mixing different algorithms for different use cases within the same deployment.</p>\n<p><strong>TokenBucket Component</strong></p>\n<p>The <code>TokenBucket</code> implementation manages refill rates and burst capacity using atomic compare-and-swap operations in Redis. The algorithm maintains a bucket state with current token count and last refill timestamp.</p>\n<table>\n<thead>\n<tr>\n<th>State Field</th>\n<th>Purpose</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>tokens</td>\n<td>Current available tokens</td>\n<td>Every request (decremented) and refill interval</td>\n</tr>\n<tr>\n<td>last_refill_time</td>\n<td>Nanosecond timestamp of last refill</td>\n<td>Every refill calculation</td>\n</tr>\n<tr>\n<td>capacity</td>\n<td>Maximum bucket size</td>\n<td>Configuration only</td>\n</tr>\n<tr>\n<td>refill_rate</td>\n<td>Tokens added per time window</td>\n<td>Configuration only</td>\n</tr>\n</tbody></table>\n<p>The token bucket algorithm allows controlled bursts above the sustained rate, making it ideal for APIs that need to handle occasional spikes while maintaining average throughput limits. The refill calculation happens atomically with token consumption to prevent race conditions.</p>\n<p><strong>SlidingWindow Implementations</strong></p>\n<p>Both sliding window counter and sliding window log algorithms track request patterns over time windows, but with different memory and accuracy trade-offs.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Memory Usage</th>\n<th>Accuracy</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sliding Window Counter</td>\n<td>O(1) per key</td>\n<td>Approximate (up to 2x limit at boundaries)</td>\n<td>High-traffic APIs needing memory efficiency</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n<td>O(requests) per key</td>\n<td>Exact</td>\n<td>Critical APIs requiring precise enforcement</td>\n</tr>\n</tbody></table>\n<h4 id=\"rulemanager-component\">RuleManager Component</h4>\n<p>The <code>RuleManager</code> handles dynamic configuration of rate limiting rules, supporting real-time updates without service restarts. Rules are organized by priority and matching patterns to enable complex hierarchical rate limiting scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Rule Matching</th>\n<th>Pattern Type</th>\n<th>Example</th>\n<th>Priority Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>User ID</td>\n<td>Exact match</td>\n<td><code>user:12345</code></td>\n<td>Higher priority overrides lower</td>\n</tr>\n<tr>\n<td>IP Address</td>\n<td>CIDR blocks</td>\n<td><code>192.168.1.0/24</code></td>\n<td>More specific patterns win</td>\n</tr>\n<tr>\n<td>API Endpoint</td>\n<td>Path patterns</td>\n<td><code>/api/v1/upload/*</code></td>\n<td>Endpoint-specific before global</td>\n</tr>\n<tr>\n<td>User Agent</td>\n<td>Regex patterns</td>\n<td><code>bot|crawler</code></td>\n<td>Pattern complexity affects performance</td>\n</tr>\n</tbody></table>\n<p>The rule evaluation engine uses short-circuit evaluation to minimize Redis operations - once a rate limit is exceeded, evaluation stops immediately rather than checking remaining tiers.</p>\n<blockquote>\n<p><strong>Decision: Priority-Based Rule Evaluation</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to handle overlapping rate limit rules for the same request</li>\n<li><strong>Options Considered</strong>: First-match wins, most restrictive wins, priority-based evaluation</li>\n<li><strong>Decision</strong>: Priority-based evaluation with configurable rule precedence</li>\n<li><strong>Rationale</strong>: Provides predictable behavior for complex rule sets, allows override patterns for special cases, enables debugging through explicit rule ordering</li>\n<li><strong>Consequences</strong>: Requires careful priority assignment, evaluation order affects performance, enables sophisticated rate limiting policies</li>\n</ul>\n</blockquote>\n<h4 id=\"consistenthashring-component\">ConsistentHashRing Component</h4>\n<p>For horizontally scaled Redis deployments, the <code>ConsistentHashRing</code> distributes rate limiting keys across multiple Redis nodes while minimizing redistribution during topology changes.</p>\n<table>\n<thead>\n<tr>\n<th>Hash Ring Feature</th>\n<th>Purpose</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Virtual Nodes</td>\n<td>Uniform distribution</td>\n<td>100-500 virtual nodes per physical node</td>\n</tr>\n<tr>\n<td>Key Placement</td>\n<td>Consistent node assignment</td>\n<td>SHA-256 hash of rate limit key</td>\n</tr>\n<tr>\n<td>Node Addition</td>\n<td>Minimal key movement</td>\n<td>Only keys between old and new node positions move</td>\n</tr>\n<tr>\n<td>Hot Key Detection</td>\n<td>Load balancing</td>\n<td>Monitor request frequency per key</td>\n</tr>\n</tbody></table>\n<p>The consistent hashing approach ensures that adding or removing Redis nodes only affects a small fraction of keys, maintaining cache locality and avoiding thundering herd problems during topology changes.</p>\n<h4 id=\"metrics-and-monitoring-components\">Metrics and Monitoring Components</h4>\n<p>Real-time observability is crucial for distributed rate limiting since quota violations may indicate either legitimate traffic spikes or attack patterns.</p>\n<p><strong>MetricsCollector Component</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Examples</th>\n<th>Collection Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate Limit Decisions</td>\n<td>Allowed/denied counts per rule</td>\n<td>In-process counters</td>\n</tr>\n<tr>\n<td>Algorithm Performance</td>\n<td>Token bucket refill latency</td>\n<td>Histogram metrics</td>\n</tr>\n<tr>\n<td>Storage Operations</td>\n<td>Redis operation latency/errors</td>\n<td>Redis client middleware</td>\n</tr>\n<tr>\n<td>Hot Key Detection</td>\n<td>Request frequency distribution</td>\n<td>Sliding window counters</td>\n</tr>\n</tbody></table>\n<p><strong>Dashboard Components</strong></p>\n<p>The real-time dashboard requires efficient data streaming to avoid overwhelming the rate limiting system with monitoring queries.</p>\n<table>\n<thead>\n<tr>\n<th>Dashboard Feature</th>\n<th>Data Source</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Current Usage Gauges</td>\n<td>Redis state queries</td>\n<td>1 second intervals</td>\n</tr>\n<tr>\n<td>Historical Trends</td>\n<td>Time-series metrics</td>\n<td>10 second aggregation</td>\n</tr>\n<tr>\n<td>Rule Configuration</td>\n<td>RuleManager API</td>\n<td>On-demand with WebSocket push</td>\n</tr>\n<tr>\n<td>Health Status</td>\n<td>Component health checks</td>\n<td>5 second intervals</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The monitoring system must not become a bottleneck or single point of failure for the rate limiting system. Dashboard queries are rate-limited and use read replicas where possible to avoid affecting production rate limiting performance.</p>\n</blockquote>\n<h3 id=\"recommended-module-structure\">Recommended Module Structure</h3>\n<p>A well-organized module structure enables parallel development of different components while maintaining clear dependency boundaries. The structure follows domain-driven design principles with infrastructure concerns separated from business logic.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>distributed-rate-limiter/\n├── cmd/\n│   ├── server/                    # HTTP API server\n│   │   └── main.go               # Server entry point with configuration\n│   ├── cli/                      # Management CLI tool\n│   │   └── main.go               # CLI for rule management and testing\n│   └── dashboard/                # Dashboard web server\n│       └── main.go               # Dashboard with WebSocket endpoints\n├── internal/\n│   ├── limiter/                  # Core rate limiting logic\n│   │   ├── limiter.go           # DistributedLimiter implementation\n│   │   ├── limiter_test.go      # Multi-instance integration tests\n│   │   ├── request.go           # RateLimitRequest and RateLimitResult\n│   │   └── interface.go         # Limiter interface definition\n│   ├── algorithms/               # Rate limiting algorithm implementations\n│   │   ├── tokenbucket/         # Token bucket algorithm\n│   │   │   ├── tokenbucket.go   # TokenBucket component\n│   │   │   ├── tokenbucket_test.go # Algorithm correctness tests\n│   │   │   └── state.go         # TokenBucketState and TokenBucketConfig\n│   │   ├── slidingwindow/       # Sliding window algorithms\n│   │   │   ├── counter.go       # Sliding window counter implementation\n│   │   │   ├── log.go           # Sliding window log implementation\n│   │   │   └── window_test.go   # Boundary condition tests\n│   │   └── interface.go         # Common algorithm interface\n│   ├── storage/                  # Storage backend implementations\n│   │   ├── redis/               # Redis backend\n│   │   │   ├── storage.go       # RedisStorage implementation\n│   │   │   ├── scripts.go       # Lua scripts for atomic operations\n│   │   │   ├── pool.go          # Connection pool management\n│   │   │   ├── health.go        # Health checking and circuit breaker\n│   │   │   └── redis_test.go    # Redis integration tests\n│   │   ├── local/               # Local storage fallback\n│   │   │   ├── memory.go        # In-memory map storage\n│   │   │   └── file.go          # File-based persistence\n│   │   └── interface.go         # Storage interface definition\n│   ├── config/                   # Configuration management\n│   │   ├── rules.go             # RuleManager implementation\n│   │   ├── loader.go            # Configuration file loading\n│   │   ├── validation.go        # Rule validation logic\n│   │   └── config_test.go       # Rule matching tests\n│   ├── sharding/                 # Consistent hashing and node management\n│   │   ├── hasher.go            # ConsistentHashRing implementation\n│   │   ├── nodes.go             # Node health and topology management\n│   │   ├── rebalancer.go        # Hot key detection and rebalancing\n│   │   └── sharding_test.go     # Hash distribution tests\n│   ├── metrics/                  # Observability and monitoring\n│   │   ├── collector.go         # MetricsCollector component\n│   │   ├── dashboard.go         # Dashboard data aggregation\n│   │   └── alerts.go            # Alerting logic for quota violations\n│   ├── api/                      # HTTP API handlers\n│   │   ├── handlers/            # Request handlers\n│   │   │   ├── ratelimit.go     # Rate limit check endpoints\n│   │   │   ├── rules.go         # Rule management CRUD endpoints\n│   │   │   └── metrics.go       # Metrics and dashboard endpoints\n│   │   ├── middleware/          # HTTP middleware\n│   │   │   ├── headers.go       # Rate limit header injection\n│   │   │   ├── logging.go       # Request logging and tracing\n│   │   │   └── recovery.go      # Panic recovery middleware\n│   │   └── server.go            # HTTP server setup and routing\n│   └── util/                     # Shared utilities\n│       ├── time.go              # Time utilities for clock skew handling\n│       ├── hash.go              # Consistent hashing utilities\n│       └── testing.go           # Test helpers for integration tests\n├── pkg/                          # Public API packages\n│   ├── client/                  # Go client library\n│   │   ├── client.go            # HTTP client for rate limit checks\n│   │   └── client_test.go       # Client integration tests\n│   └── types/                   # Shared type definitions\n│       ├── rules.go             # RateLimitRule and related types\n│       ├── results.go           # RateLimitResult and error types\n│       └── config.go            # Configuration structures\n├── configs/                      # Configuration file examples\n│   ├── rules.yaml               # Example rate limiting rules\n│   ├── redis.yaml               # Redis cluster configuration\n│   └── server.yaml              # Server configuration\n├── scripts/                      # Deployment and development scripts\n│   ├── setup-redis.sh           # Redis cluster setup script\n│   ├── load-test.sh             # Rate limiter load testing\n│   └── migrate-rules.sh         # Rule migration utilities\n├── deployments/                  # Kubernetes and Docker configurations\n│   ├── k8s/                     # Kubernetes manifests\n│   └── docker/                  # Docker compose configurations\n└── docs/                         # Additional documentation\n    ├── algorithms.md            # Algorithm comparison and selection guide\n    ├── deployment.md            # Production deployment guide\n    └── troubleshooting.md       # Common issues and solutions</code></pre></div>\n\n<h4 id=\"module-dependency-guidelines\">Module Dependency Guidelines</h4>\n<p>The module structure enforces clear dependency boundaries to prevent circular imports and enable independent testing of components.</p>\n<table>\n<thead>\n<tr>\n<th>Layer</th>\n<th>Allowed Dependencies</th>\n<th>Prohibited Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>cmd/</code> packages</td>\n<td>Any internal package</td>\n<td>None - these are entry points</td>\n</tr>\n<tr>\n<td><code>internal/limiter/</code></td>\n<td><code>algorithms/</code>, <code>storage/</code>, <code>config/</code>, <code>metrics/</code></td>\n<td><code>api/</code>, <code>cmd/</code> - business logic independent of transport</td>\n</tr>\n<tr>\n<td><code>internal/algorithms/</code></td>\n<td><code>storage/</code> interface only</td>\n<td>Concrete storage implementations</td>\n</tr>\n<tr>\n<td><code>internal/storage/</code></td>\n<td><code>util/</code> only</td>\n<td>Algorithm or limiter packages</td>\n</tr>\n<tr>\n<td><code>internal/config/</code></td>\n<td><code>util/</code>, <code>pkg/types/</code></td>\n<td>Storage or algorithm implementations</td>\n</tr>\n<tr>\n<td><code>internal/api/</code></td>\n<td><code>limiter/</code>, <code>config/</code>, <code>metrics/</code></td>\n<td>Storage implementations directly</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Hexagonal Architecture with Interface Boundaries</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to enable parallel development and independent testing of components</li>\n<li><strong>Options Considered</strong>: Layered architecture, microservices with RPC, hexagonal architecture with interfaces</li>\n<li><strong>Decision</strong>: Hexagonal architecture with storage and algorithm abstractions</li>\n<li><strong>Rationale</strong>: Enables testing without Redis dependencies, supports multiple storage backends, allows algorithm experimentation without changing core logic</li>\n<li><strong>Consequences</strong>: Requires discipline to maintain interface boundaries, adds abstraction overhead, enables comprehensive unit testing</li>\n</ul>\n</blockquote>\n<h4 id=\"configuration-management-structure\">Configuration Management Structure</h4>\n<p>Rate limiting rules require dynamic updates without service restarts, necessitating a structured approach to configuration management.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Type</th>\n<th>File Location</th>\n<th>Update Method</th>\n<th>Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate Limit Rules</td>\n<td><code>configs/rules.yaml</code></td>\n<td>HTTP API with immediate propagation</td>\n<td>Schema validation + rule conflict detection</td>\n</tr>\n<tr>\n<td>Redis Configuration</td>\n<td><code>configs/redis.yaml</code></td>\n<td>Environment variables + config file</td>\n<td>Connection testing during startup</td>\n</tr>\n<tr>\n<td>Server Settings</td>\n<td><code>configs/server.yaml</code></td>\n<td>Environment variables only</td>\n<td>Port availability and permission checks</td>\n</tr>\n<tr>\n<td>Algorithm Parameters</td>\n<td>Embedded in rules</td>\n<td>Rule update API</td>\n<td>Algorithm-specific parameter validation</td>\n</tr>\n</tbody></table>\n<p>The configuration system supports environment variable overrides for deployment-specific settings while maintaining rule definitions in version-controlled YAML files.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Understanding common mistakes in distributed rate limiter architecture helps avoid subtle bugs that only manifest under high load or failure conditions.</p>\n<p>⚠️ <strong>Pitfall: Circular Dependencies Between Components</strong></p>\n<p>Many implementations create circular import cycles by having storage components depend on algorithm implementations while algorithms depend on storage interfaces. This typically happens when trying to implement algorithm-specific optimizations in the storage layer.</p>\n<p><strong>Why it&#39;s wrong</strong>: Circular dependencies prevent independent testing and make the codebase fragile to changes. They also indicate that separation of concerns is violated.</p>\n<p><strong>How to fix</strong>: Use dependency inversion with interfaces. Storage components should only know about generic operations, while algorithms implement their logic using storage interface methods.</p>\n<p>⚠️ <strong>Pitfall: Blocking Operations in Request Path</strong></p>\n<p>Synchronous Redis operations in the request handling path can cause cascading failures when Redis latency spikes. This is especially problematic when checking multiple rate limit tiers sequentially.</p>\n<p><strong>Why it&#39;s wrong</strong>: A slow Redis query blocks the entire request, leading to thread pool exhaustion and service unavailability even when Redis is only temporarily slow.</p>\n<p><strong>How to fix</strong>: Use timeouts for all Redis operations, implement circuit breaker patterns, and provide local fallback that can execute when Redis operations timeout.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Clock Sources</strong></p>\n<p>Using different time sources (system clock vs Redis TIME command) across components leads to inconsistent rate limit window calculations and can cause requests to be incorrectly allowed or denied.</p>\n<p><strong>Why it&#39;s wrong</strong>: Clock skew between application instances and Redis nodes causes time-based calculations to diverge, leading to unpredictable rate limiting behavior.</p>\n<p><strong>How to fix</strong>: Use a consistent time source strategy - either always use Redis TIME command for distributed consistency, or ensure NTP synchronization across all nodes and use local clocks consistently.</p>\n<p>⚠️ <strong>Pitfall: Missing Graceful Degradation Strategy</strong></p>\n<p>Failing hard when Redis is unavailable means rate limiting becomes a single point of failure that can take down the entire application.</p>\n<p><strong>Why it&#39;s wrong</strong>: Rate limiting is a protection mechanism, not a core business function. Its unavailability should not prevent the application from functioning.</p>\n<p><strong>How to fix</strong>: Implement local fallback storage that provides reduced-accuracy rate limiting when Redis is unavailable. Design the system to &quot;fail open&quot; with monitoring rather than &quot;fail closed&quot; and break the application.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides the foundational code structure and infrastructure components needed to implement the distributed rate limiter architecture. The code focuses on providing complete, working infrastructure that learners can build upon rather than implementing the core rate limiting algorithms themselves.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Client</td>\n<td><code>github.com/go-redis/redis/v8</code> with single instance</td>\n<td><code>github.com/go-redis/redis/v8</code> with cluster support</td>\n</tr>\n<tr>\n<td>HTTP Framework</td>\n<td>Standard <code>net/http</code> with custom routing</td>\n<td><code>github.com/gin-gonic/gin</code> for middleware ecosystem</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>YAML files with <code>gopkg.in/yaml.v3</code></td>\n<td><code>github.com/spf13/viper</code> for multiple formats</td>\n</tr>\n<tr>\n<td>Metrics</td>\n<td>Simple counters with <code>expvar</code></td>\n<td><code>github.com/prometheus/client_golang</code></td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard <code>log</code> package</td>\n<td><code>github.com/sirupsen/logrus</code> with structured logging</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Standard <code>testing</code> package</td>\n<td><code>github.com/stretchr/testify</code> for assertions</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure-setup\">Recommended File Structure Setup</h4>\n<p>Start by creating the basic directory structure and go module:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">mkdir</span><span style=\"color:#9ECBFF\"> distributed-rate-limiter</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> distributed-rate-limiter</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> mod</span><span style=\"color:#9ECBFF\"> init</span><span style=\"color:#9ECBFF\"> github.com/yourusername/distributed-rate-limiter</span></span></code></pre></div>\n\n<p>Create the core directories:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">mkdir</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> {cmd/{server,cli,dashboard},internal/{limiter,algorithms/{tokenbucket,slidingwindow},storage/{redis,local},config,sharding,metrics,api/{handlers,middleware},util},pkg/{client,types},configs,scripts,deployments/{k8s,docker},docs}</span></span></code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Redis Configuration and Connection Management</strong></p>\n<p><code>internal/storage/redis/config.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/tls</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/go-redis/redis/v8</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisConfig holds all Redis connection and operation parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Addresses contains Redis node addresses for cluster or single instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Addresses []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"addresses\" json:\"addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Password for Redis authentication</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Password </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"password\" json:\"password\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // DB database number (ignored in cluster mode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DB </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `yaml:\"db\" json:\"db\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // PoolSize maximum number of socket connections</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PoolSize </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `yaml:\"pool_size\" json:\"pool_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ReadTimeout for socket reads</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"read_timeout\" json:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // WriteTimeout for socket writes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"write_timeout\" json:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // DialTimeout for establishing new connections</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DialTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"dial_timeout\" json:\"dial_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // EnableTLS enables TLS encryption</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EnableTLS </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\"> `yaml:\"enable_tls\" json:\"enable_tls\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TLSConfig for custom TLS settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TLSConfig </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">tls</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#9ECBFF\"> `yaml:\"-\" json:\"-\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DefaultRedisConfig returns configuration with sensible defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> DefaultRedisConfig</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">RedisConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addresses:    []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"localhost:6379\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Password:     </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DB:           </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        PoolSize:     DEFAULT_POOL_SIZE,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  DEFAULT_TIMEOUT,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: DEFAULT_TIMEOUT,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DialTimeout:  DEFAULT_TIMEOUT,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        EnableTLS:    </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks configuration parameters for correctness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#B392F0\">RedisConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(c.Addresses) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"at least one Redis address required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.PoolSize </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"pool_size must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ReadTimeout </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> c.WriteTimeout </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> c.DialTimeout </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"all timeout values must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Complete Redis Storage Implementation with Health Checking</strong></p>\n<p><code>internal/storage/redis/storage.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/go-redis/redis/v8</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisStorage implements distributed rate limiting storage using Redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisStorage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#B392F0\">RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Health checking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    healthy    </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    healthLock </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastCheck  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Circuit breaker state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureCount </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFailure  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRedisStorage creates Redis storage with connection pooling and health monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> config.</span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid Redis configuration: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create universal client (works with both single instance and cluster)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> client </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(config.Addresses) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Single instance mode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#B392F0\">NewClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Options</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Addr:         config.Addresses[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Password:     config.Password,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            DB:           config.DB,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            PoolSize:     config.PoolSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  config.ReadTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: config.WriteTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            DialTimeout:  config.DialTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TLSConfig:    config.TLSConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Cluster mode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#B392F0\">NewClusterClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ClusterOptions</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Addrs:        config.Addresses,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Password:     config.Password,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            PoolSize:     config.PoolSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  config.ReadTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: config.WriteTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            DialTimeout:  config.DialTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TLSConfig:    config.TLSConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client:  client,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:  config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        healthy: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Verify connection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> storage.client.</span><span style=\"color:#B392F0\">Ping</span><span style=\"color:#E1E4E8\">(ctx).</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to connect to Redis: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start health checking goroutine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> storage.</span><span style=\"color:#B392F0\">healthChecker</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> storage, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// healthChecker runs periodic health checks in background</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">healthChecker</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ticker.C {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.client.</span><span style=\"color:#B392F0\">Ping</span><span style=\"color:#E1E4E8\">(ctx).</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        r.healthLock.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        r.lastCheck </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            r.healthy </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            r.failureCount</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            r.lastFailure </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            r.healthy </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            r.failureCount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        r.healthLock.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IsHealthy returns current health status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IsHealthy</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r.healthLock.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> r.healthLock.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> r.healthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close closes Redis connections</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> r.client.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Type Definitions</strong></p>\n<p><code>pkg/types/rules.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitRule defines a rate limiting rule with all configuration parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ID uniquely identifies this rule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"id\" json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Name human-readable rule name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"name\" json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // KeyPattern defines which requests this rule applies to</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Examples: \"user:*\", \"ip:192.168.1.*\", \"api:/upload/*\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    KeyPattern </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"key_pattern\" json:\"key_pattern\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Algorithm specifies which rate limiting algorithm to use</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"algorithm\" json:\"algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Limit maximum number of requests allowed in the time window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Limit </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `yaml:\"limit\" json:\"limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Window time duration for the rate limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"window\" json:\"window\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // BurstLimit allows temporary bursts above the sustained limit (token bucket only)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BurstLimit </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `yaml:\"burst_limit\" json:\"burst_limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Enabled whether this rule is currently active</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\"> `yaml:\"enabled\" json:\"enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Priority determines evaluation order (higher numbers evaluated first)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Priority </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `yaml:\"priority\" json:\"priority\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // CreatedAt timestamp when rule was created</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `yaml:\"created_at\" json:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // UpdatedAt timestamp when rule was last modified</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `yaml:\"updated_at\" json:\"updated_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitRequest contains all context needed to evaluate rate limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // UserID for authenticated user rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserID </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"user_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // IPAddress for IP-based rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IPAddress </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"ip_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // APIEndpoint for per-endpoint rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    APIEndpoint </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"api_endpoint\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // UserAgent for bot detection and user-agent-based limiting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserAgent </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"user_agent\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Tokens number of tokens to consume (default 1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tokens </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `json:\"tokens\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitResult contains the decision and metadata from a rate limit check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Allowed whether the request should be permitted</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Allowed </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\"> `json:\"allowed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Remaining number of requests remaining in current window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Remaining </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `json:\"remaining\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // RetryAfter duration to wait before next allowed request</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"retry_after\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ResetTime when the rate limit window resets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResetTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"reset_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // RuleID which rule triggered this result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RuleID </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"rule_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Algorithm which algorithm was used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Storage Interface Definition</strong></p>\n<p><code>internal/storage/interface.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Storage defines the interface for rate limiting storage backends</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Storage</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // CheckAndUpdate atomically checks current count and updates if under limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Returns: allowed, remaining, resetTime, error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CheckAndUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Preview checks current status without updating counters</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Preview</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Reset clears all counters for a key</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Reset</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // IsHealthy returns whether storage backend is available</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    IsHealthy</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Close releases resources</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-component-skeletons\">Core Component Skeletons</h4>\n<p><strong>Rate Limiter Interface and Core Types</strong></p>\n<p><code>internal/limiter/interface.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> limiter</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/distributed-rate-limiter/pkg/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Limiter defines the interface for rate limiting implementations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Limiter</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check performs rate limit evaluation and updates counters</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Preview checks rate limit status without updating counters  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Preview</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Reset clears rate limit counters for a request context</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Reset</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Main DistributedLimiter Component (Core Logic Skeleton)</strong></p>\n<p><code>internal/limiter/limiter.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> limiter</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/distributed-rate-limiter/internal/config</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/distributed-rate-limiter/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourusername/distributed-rate-limiter/pkg/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DistributedLimiter coordinates rate limiting across multiple tiers with fallback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DistributedLimiter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage       </span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localFallback </span><span style=\"color:#B392F0\">Limiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewDistributedLimiter creates distributed rate limiter instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDistributedLimiter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Storage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ruleManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create local fallback limiter for when Redis is unavailable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize metrics collection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:     storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ruleManager: ruleManager,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // localFallback: localLimiter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check performs multi-tier rate limit evaluation with short-circuit logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if storage backend is healthy, switch to fallback if not</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get matching rules from rule manager based on request context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Sort rules by priority (highest priority first)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Evaluate each rule using short-circuit logic:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - Stop immediately if any rule denies the request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - Track the most restrictive remaining count across all rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: For each rule that applies:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - Generate appropriate Redis key (user:123, ip:1.2.3.4, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - Call storage.CheckAndUpdate with rule's limit and window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - If denied, return immediately with rule details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If all rules pass, return allowed=true with most restrictive remaining count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Handle storage errors by falling back to local limiter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update metrics for allowed/denied decisions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Preview checks rate limit status without updating counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Preview</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Same logic as Check() but call storage.Preview instead of CheckAndUpdate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return most restrictive result across all matching rules</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Reset clears rate limit counters for a request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get matching rules for the request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each rule, generate Redis key and call storage.Reset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle partial failures gracefully</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Configuration Constants</strong></p>\n<p><code>internal/storage/redis/constants.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // DEFAULT_POOL_SIZE maximum Redis connections per instance</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_POOL_SIZE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // DEFAULT_TIMEOUT for Redis operations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_TIMEOUT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Millisecond</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Circuit breaker thresholds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MAX_FAILURE_COUNT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CIRCUIT_OPEN_TIME</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><code>internal/config/constants.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Priority levels for rule evaluation order</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PRIORITY_HIGH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PRIORITY_MEDIUM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PRIORITY_LOW</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Algorithm identifiers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ALGORITHM_TOKEN_BUCKET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"token_bucket\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ALGORITHM_SLIDING_WINDOW_LOG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sliding_window_log\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ALGORITHM_SLIDING_COUNTER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sliding_window_counter\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Redis Lua Script Development</strong></p>\n<p>When implementing atomic operations, Redis Lua scripts ensure consistency across distributed instances:</p>\n<ul>\n<li>Use <code>redis.call(&#39;TIME&#39;)</code> within Lua scripts to get consistent timestamps</li>\n<li>Store nanosecond timestamps as integers to avoid floating point precision issues  </li>\n<li>Return multiple values from Lua scripts to minimize round-trips: <code>return {allowed, remaining, reset_time}</code></li>\n<li>Test Lua scripts thoroughly - Redis script errors are harder to debug than Go code</li>\n</ul>\n<p><strong>Go Context and Timeout Handling</strong></p>\n<ul>\n<li>Always pass context with timeouts to Redis operations: <code>ctx, cancel := context.WithTimeout(parent, 50*time.Millisecond)</code></li>\n<li>Use <code>context.WithCancel</code> for health checking goroutines to enable graceful shutdown</li>\n<li>Check <code>ctx.Done()</code> in long-running operations to respect cancellation</li>\n<li>Wrap Redis errors with <code>fmt.Errorf(&quot;redis operation failed: %w&quot;, err)</code> for better error tracking</li>\n</ul>\n<p><strong>Concurrent Map Access</strong></p>\n<p>The rule manager needs thread-safe access patterns:</p>\n<ul>\n<li>Use <code>sync.RWMutex</code> for rule maps - most operations are reads</li>\n<li>Hold read locks for the minimum time necessary: get rule list, release lock, then process</li>\n<li>Use <code>sync.Map</code> for metrics counters that are updated frequently from multiple goroutines</li>\n<li>Avoid nested locking - always acquire locks in the same order to prevent deadlocks</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After completing the architecture setup:</strong></p>\n<ol>\n<li><strong>Connectivity Test</strong>: Run <code>go test ./internal/storage/redis/...</code> - should successfully connect to Redis and perform basic operations</li>\n<li><strong>Rule Loading Test</strong>: Create a test YAML file with sample rules and verify <code>RuleManager.LoadRules()</code> parses correctly</li>\n<li><strong>Interface Compliance</strong>: Run <code>go build ./...</code> - all interface implementations should compile without errors</li>\n<li><strong>Health Check Test</strong>: Start Redis, verify health status is <code>true</code>, stop Redis, verify it changes to <code>false</code> within 10 seconds</li>\n</ol>\n<p><strong>Expected behavior verification:</strong></p>\n<ul>\n<li><code>RedisStorage.IsHealthy()</code> should return <code>true</code> when Redis is running</li>\n<li>Rule manager should load rules from YAML and match patterns correctly</li>\n<li>DistributedLimiter creation should not panic when given valid storage and rule manager</li>\n</ul>\n<p><strong>Common setup issues:</strong></p>\n<ul>\n<li>Redis connection failures: Check Redis is running on expected port with <code>redis-cli ping</code></li>\n<li>Module import errors: Ensure <code>go mod tidy</code> has been run and all dependencies are available</li>\n<li>Configuration validation errors: Verify YAML syntax and required fields are present</li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this data model foundation enables rate limiting algorithms (Milestone 1), multi-tier evaluation (Milestone 2), Redis backend integration (Milestone 3), sharding (Milestone 4), and API design (Milestone 5).</p>\n</blockquote>\n<p>The data model serves as the foundation for distributed rate limiting by defining how rate limit rules are configured, how state is tracked across multiple application instances, and how metrics are collected for monitoring and alerting. Think of this as the blueprint that architects use before constructing a building - every component needs to understand the same data structures to work together effectively.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fdata-model.svg\" alt=\"Data Model Relationships\"></p>\n<p>The data model addresses three critical challenges in distributed rate limiting. First, it must represent complex rate limit rules that can apply to multiple dimensions (user, IP, API endpoint) with different algorithms and priorities. Second, it must define how rate limiting state is stored in Redis with atomic operations to prevent race conditions across multiple application instances. Third, it must capture metrics and monitoring data that provide visibility into system behavior and enable proactive capacity management.</p>\n<h3 id=\"rate-limit-rule-structure\">Rate Limit Rule Structure</h3>\n<p>Rate limit rules define the policies that govern request admission across different dimensions and tiers. Think of rate limit rules as the security protocols at different checkpoints in an airport - each checkpoint has specific criteria (first class vs economy, domestic vs international) and enforcement mechanisms (document check, metal detector, baggage scan) that determine whether a passenger can proceed.</p>\n<p>The rule structure must support multi-dimensional matching, where a single HTTP request might be subject to multiple overlapping rules based on user identity, source IP address, API endpoint, and global system capacity. This creates a hierarchical evaluation system where rules are applied in priority order, with higher priority rules taking precedence over lower priority ones.</p>\n<blockquote>\n<p><strong>Decision: Unified Rule Structure with Pattern Matching</strong></p>\n<ul>\n<li><strong>Context</strong>: Rate limits need to apply across multiple dimensions (user, IP, API) with different algorithms and priorities</li>\n<li><strong>Options Considered</strong>: Separate rule types per dimension vs unified rule structure vs hardcoded tiers</li>\n<li><strong>Decision</strong>: Single unified <code>RateLimitRule</code> structure with pattern-based key matching</li>\n<li><strong>Rationale</strong>: Unified structure reduces complexity, enables flexible rule composition, and simplifies rule management API</li>\n<li><strong>Consequences</strong>: More flexible but requires careful pattern design to avoid rule conflicts and performance issues</li>\n</ul>\n</blockquote>\n<p>The <code>RateLimitRule</code> structure captures all necessary configuration for rate limit enforcement:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>id</code></td>\n<td>string</td>\n<td>Unique identifier for the rule, used for updates and deletion</td>\n</tr>\n<tr>\n<td><code>name</code></td>\n<td>string</td>\n<td>Human-readable name for the rule, displayed in dashboard and logs</td>\n</tr>\n<tr>\n<td><code>key_pattern</code></td>\n<td>string</td>\n<td>Pattern for matching rate limit keys (supports wildcards and templating)</td>\n</tr>\n<tr>\n<td><code>algorithm</code></td>\n<td>string</td>\n<td>Rate limiting algorithm identifier (<code>token_bucket</code>, <code>sliding_window_counter</code>, <code>sliding_window_log</code>)</td>\n</tr>\n<tr>\n<td><code>limit</code></td>\n<td>int64</td>\n<td>Maximum number of requests allowed within the time window</td>\n</tr>\n<tr>\n<td><code>window</code></td>\n<td>time.Duration</td>\n<td>Time window duration for rate limit evaluation</td>\n</tr>\n<tr>\n<td><code>burst_limit</code></td>\n<td>int64</td>\n<td>Maximum burst capacity for token bucket algorithm (ignored for other algorithms)</td>\n</tr>\n<tr>\n<td><code>enabled</code></td>\n<td>bool</td>\n<td>Whether this rule is actively enforced</td>\n</tr>\n<tr>\n<td><code>priority</code></td>\n<td>int</td>\n<td>Rule evaluation priority (higher numbers evaluated first)</td>\n</tr>\n<tr>\n<td><code>created_at</code></td>\n<td>time.Time</td>\n<td>Timestamp when rule was created</td>\n</tr>\n<tr>\n<td><code>updated_at</code></td>\n<td>time.Time</td>\n<td>Timestamp when rule was last modified</td>\n</tr>\n</tbody></table>\n<p>The <code>key_pattern</code> field uses a templating system that allows rules to match multiple rate limit dimensions. For example, a pattern like <code>user:{user_id}:api:/v1/upload</code> creates rate limit keys specific to both user identity and API endpoint. This enables fine-grained control where a user might have different rate limits for different API operations.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The pattern-based approach allows a single rule to generate different Redis keys based on request context, enabling both specific limits (per-user API limits) and aggregate limits (global API limits) using the same rule structure.</p>\n</blockquote>\n<p>The <code>priority</code> field determines rule evaluation order during multi-tier rate limiting. Rules with higher priority values are evaluated first, allowing system-wide protections (priority 100) to take precedence over user-specific limits (priority 50). This prevents lower-priority rules from consuming system resources when higher-priority limits are already exceeded.</p>\n<p><strong>Common Rule Configuration Patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pattern Type</th>\n<th>Key Pattern</th>\n<th>Example Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-User Global</td>\n<td><code>user:{user_id}</code></td>\n<td>1000 requests per hour per user across all APIs</td>\n</tr>\n<tr>\n<td>Per-User API</td>\n<td><code>user:{user_id}:api:{api_endpoint}</code></td>\n<td>100 requests per minute for upload API per user</td>\n</tr>\n<tr>\n<td>Per-IP</td>\n<td><code>ip:{ip_address}</code></td>\n<td>500 requests per hour per IP address</td>\n</tr>\n<tr>\n<td>Global API</td>\n<td><code>api:{api_endpoint}</code></td>\n<td>10000 requests per minute for upload API across all users</td>\n</tr>\n<tr>\n<td>Global System</td>\n<td><code>global</code></td>\n<td>100000 requests per minute system-wide</td>\n</tr>\n</tbody></table>\n<h3 id=\"redis-data-structures\">Redis Data Structures</h3>\n<p>Redis serves as the distributed state store for rate limiting counters and algorithm-specific state. The data structures must support atomic check-and-update operations to prevent race conditions when multiple application instances evaluate the same rate limit key simultaneously. Think of Redis as a high-speed synchronized ledger that multiple bank tellers can access simultaneously, with built-in mechanisms to ensure no two tellers can modify the same account balance at exactly the same time.</p>\n<blockquote>\n<p><strong>Decision: Algorithm-Specific Redis Key Schemas</strong></p>\n<ul>\n<li><strong>Context</strong>: Different rate limiting algorithms require different state representations in Redis</li>\n<li><strong>Options Considered</strong>: Unified state format vs algorithm-specific schemas vs separate Redis databases</li>\n<li><strong>Decision</strong>: Algorithm-specific key schemas with consistent naming conventions</li>\n<li><strong>Rationale</strong>: Optimizes Redis operations for each algorithm while maintaining operational simplicity</li>\n<li><strong>Consequences</strong>: More complex key management but better performance and clearer debugging</li>\n</ul>\n</blockquote>\n<p>Each rate limiting algorithm requires different data structures in Redis to maintain its state effectively:</p>\n<p><strong>Token Bucket Algorithm Redis Schema:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Key Format</th>\n<th>Data Type</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>tb:{rule_id}:{key}:state</code></td>\n<td>Redis Hash</td>\n<td><code>tokens</code>, <code>last_refill_time</code></td>\n<td>Current token count and last refill timestamp</td>\n</tr>\n<tr>\n<td><code>tb:{rule_id}:{key}:config</code></td>\n<td>Redis Hash</td>\n<td><code>capacity</code>, <code>refill_rate</code>, <code>window</code></td>\n<td>Algorithm configuration parameters</td>\n</tr>\n</tbody></table>\n<p>The token bucket state uses a Redis hash to store both the current token count and the timestamp of the last refill operation. This enables the atomic Lua script to calculate how many tokens should be added based on elapsed time and then update both the token count and timestamp in a single atomic operation.</p>\n<p><strong>Sliding Window Counter Algorithm Redis Schema:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Key Format</th>\n<th>Data Type</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>swc:{rule_id}:{key}:{bucket_id}</code></td>\n<td>Redis String</td>\n<td>counter value</td>\n<td>Request count for specific time bucket</td>\n</tr>\n<tr>\n<td><code>swc:{rule_id}:{key}:meta</code></td>\n<td>Redis Hash</td>\n<td><code>window_size</code>, <code>bucket_size</code>, <code>last_cleanup</code></td>\n<td>Algorithm parameters and maintenance info</td>\n</tr>\n</tbody></table>\n<p>The sliding window counter divides the time window into smaller buckets, with each bucket stored as a separate Redis key. This allows expired buckets to be cleaned up automatically using Redis TTL, while active buckets can be summed to get the current window count. The bucket ID is calculated as <code>floor(current_timestamp / bucket_size)</code>.</p>\n<p><strong>Sliding Window Log Algorithm Redis Schema:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Key Format</th>\n<th>Data Type</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>swl:{rule_id}:{key}:log</code></td>\n<td>Redis Sorted Set</td>\n<td>score=timestamp, member=request_id</td>\n<td>Individual request timestamps</td>\n</tr>\n<tr>\n<td><code>swl:{rule_id}:{key}:count</code></td>\n<td>Redis String</td>\n<td>current count</td>\n<td>Cached count for performance optimization</td>\n</tr>\n</tbody></table>\n<p>The sliding window log uses a Redis sorted set where the score is the request timestamp and the member is a unique request identifier. This allows efficient range queries to count requests within the current time window and automatic cleanup of expired entries using <code>ZREMRANGEBYSCORE</code>.</p>\n<p><strong>Cross-Algorithm Metadata Schema:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Key Format</th>\n<th>Data Type</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>rl:rules:{rule_id}</code></td>\n<td>Redis Hash</td>\n<td>serialized <code>RateLimitRule</code></td>\n<td>Rule configuration for distributed access</td>\n</tr>\n<tr>\n<td><code>rl:metrics:{key}:{timestamp}</code></td>\n<td>Redis Hash</td>\n<td><code>requests</code>, <code>allowed</code>, <code>denied</code>, <code>algorithm</code></td>\n<td>Aggregated metrics for monitoring</td>\n</tr>\n<tr>\n<td><code>rl:health:{node_id}</code></td>\n<td>Redis String</td>\n<td>timestamp</td>\n<td>Node health heartbeat</td>\n</tr>\n</tbody></table>\n<p>The metadata keys enable configuration distribution and health monitoring across the distributed rate limiting system. Rule configurations are cached in Redis so that any application instance can access the current rule set without requiring a central configuration service.</p>\n<h3 id=\"redis-key-expiration-strategy\">Redis Key Expiration Strategy</h3>\n<p>All rate limiting keys use Redis TTL (Time To Live) to prevent memory leaks and automatically clean up obsolete state. The TTL values are set based on the rate limit window duration plus a safety buffer:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>TTL Formula</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket</td>\n<td><code>2 * window_duration</code></td>\n<td>Allows for clock skew and delayed cleanup</td>\n</tr>\n<tr>\n<td>Sliding Window Counter</td>\n<td><code>bucket_duration + window_duration</code></td>\n<td>Ensures all buckets in window remain available</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n<td><code>window_duration + 1 minute</code></td>\n<td>Prevents log growth while handling delayed requests</td>\n</tr>\n</tbody></table>\n<h3 id=\"metrics-and-monitoring-data\">Metrics and Monitoring Data</h3>\n<p>Comprehensive metrics collection enables operators to understand system behavior, detect anomalies, and plan capacity upgrades. The metrics system captures both real-time operational data and historical trends for analysis. Think of this as the instrument panel in an aircraft cockpit - pilots need both immediate readings (altitude, speed) and trend information (fuel consumption over time) to make informed decisions.</p>\n<p>The metrics data model supports multiple aggregation levels to balance storage efficiency with query flexibility. Raw metrics are collected at high granularity for immediate operational needs, while aggregated metrics provide efficient historical analysis.</p>\n<p><strong>Real-Time Metrics Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>timestamp</code></td>\n<td>int64</td>\n<td>Unix timestamp in milliseconds for precise ordering</td>\n</tr>\n<tr>\n<td><code>rule_id</code></td>\n<td>string</td>\n<td>Rate limit rule that generated this metric</td>\n</tr>\n<tr>\n<td><code>key</code></td>\n<td>string</td>\n<td>Rate limit key (may be hashed for privacy)</td>\n</tr>\n<tr>\n<td><code>algorithm</code></td>\n<td>string</td>\n<td>Rate limiting algorithm used</td>\n</tr>\n<tr>\n<td><code>requests</code></td>\n<td>int64</td>\n<td>Number of requests evaluated</td>\n</tr>\n<tr>\n<td><code>allowed</code></td>\n<td>int64</td>\n<td>Number of requests allowed</td>\n</tr>\n<tr>\n<td><code>denied</code></td>\n<td>int64</td>\n<td>Number of requests denied</td>\n</tr>\n<tr>\n<td><code>remaining_quota</code></td>\n<td>int64</td>\n<td>Remaining capacity in current window</td>\n</tr>\n<tr>\n<td><code>window_reset_time</code></td>\n<td>int64</td>\n<td>When current window resets (Unix timestamp)</td>\n</tr>\n<tr>\n<td><code>node_id</code></td>\n<td>string</td>\n<td>Application instance that recorded this metric</td>\n</tr>\n</tbody></table>\n<p><strong>Aggregated Metrics Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>time_bucket</code></td>\n<td>string</td>\n<td>Time bucket identifier (e.g., &quot;2024-01-15T14:30:00Z&quot;)</td>\n</tr>\n<tr>\n<td><code>bucket_size</code></td>\n<td>string</td>\n<td>Aggregation interval (&quot;1m&quot;, &quot;5m&quot;, &quot;1h&quot;, &quot;1d&quot;)</td>\n</tr>\n<tr>\n<td><code>dimensions</code></td>\n<td>map[string]string</td>\n<td>Aggregation dimensions (rule_id, algorithm, etc.)</td>\n</tr>\n<tr>\n<td><code>total_requests</code></td>\n<td>int64</td>\n<td>Sum of all requests in time bucket</td>\n</tr>\n<tr>\n<td><code>total_allowed</code></td>\n<td>int64</td>\n<td>Sum of allowed requests</td>\n</tr>\n<tr>\n<td><code>total_denied</code></td>\n<td>int64</td>\n<td>Sum of denied requests</td>\n</tr>\n<tr>\n<td><code>avg_remaining_quota</code></td>\n<td>float64</td>\n<td>Average remaining quota across measurements</td>\n</tr>\n<tr>\n<td><code>min_remaining_quota</code></td>\n<td>int64</td>\n<td>Minimum remaining quota observed</td>\n</tr>\n<tr>\n<td><code>max_remaining_quota</code></td>\n<td>int64</td>\n<td>Maximum remaining quota observed</td>\n</tr>\n<tr>\n<td><code>unique_keys</code></td>\n<td>int64</td>\n<td>Number of distinct rate limit keys active</td>\n</tr>\n</tbody></table>\n<p><strong>System Health Metrics Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>node_id</code></td>\n<td>string</td>\n<td>Application instance identifier</td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td>int64</td>\n<td>Metric collection timestamp</td>\n</tr>\n<tr>\n<td><code>redis_latency_p50</code></td>\n<td>float64</td>\n<td>50th percentile Redis operation latency (milliseconds)</td>\n</tr>\n<tr>\n<td><code>redis_latency_p99</code></td>\n<td>float64</td>\n<td>99th percentile Redis operation latency (milliseconds)</td>\n</tr>\n<tr>\n<td><code>redis_errors</code></td>\n<td>int64</td>\n<td>Number of Redis operation errors</td>\n</tr>\n<tr>\n<td><code>local_fallback_active</code></td>\n<td>bool</td>\n<td>Whether local fallback is currently active</td>\n</tr>\n<tr>\n<td><code>rules_loaded</code></td>\n<td>int64</td>\n<td>Number of rate limit rules currently loaded</td>\n</tr>\n<tr>\n<td><code>active_keys</code></td>\n<td>int64</td>\n<td>Number of rate limit keys with recent activity</td>\n</tr>\n<tr>\n<td><code>memory_usage_bytes</code></td>\n<td>int64</td>\n<td>Application memory usage</td>\n</tr>\n<tr>\n<td><code>cpu_usage_percent</code></td>\n<td>float64</td>\n<td>Application CPU usage percentage</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Hierarchical Metric Aggregation</strong></p>\n<ul>\n<li><strong>Context</strong>: Need both real-time monitoring and historical trend analysis with storage efficiency</li>\n<li><strong>Options Considered</strong>: Store only raw metrics vs pre-aggregate everything vs hierarchical aggregation</li>\n<li><strong>Decision</strong>: Hierarchical aggregation with multiple time granularities</li>\n<li><strong>Rationale</strong>: Balances query performance, storage efficiency, and operational flexibility</li>\n<li><strong>Consequences</strong>: More complex metric processing but enables both real-time dashboards and historical analysis</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Redis Key Explosion</strong>\nWhen designing Redis key schemas, it&#39;s tempting to create highly granular keys for every possible combination of dimensions. However, this can lead to millions of keys in Redis, consuming excessive memory and degrading performance. Instead, use consistent key patterns with appropriate TTL values and consider using hashed key suffixes for high-cardinality dimensions like user IDs.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Rule Priority Handling</strong>\nRule priorities must be consistently interpreted across all application instances. A common mistake is using different priority comparison logic (ascending vs descending) in different components, leading to inconsistent rate limit enforcement. Always document priority semantics clearly and use constants like <code>PRIORITY_HIGH</code> and <code>PRIORITY_LOW</code> to make ordering explicit.</p>\n<p>⚠️ <strong>Pitfall: Missing Rule Validation</strong>\nRate limit rules should be validated when loaded to prevent runtime errors. Common validation failures include negative limits, zero or negative time windows, invalid algorithm names, and circular rule dependencies. Implement comprehensive validation with clear error messages to prevent misconfigured rules from reaching production.</p>\n<p>⚠️ <strong>Pitfall: Metric Storage Overflow</strong>\nWithout proper aggregation and cleanup, metrics can consume more storage than the actual application data. Implement time-based partitioning for metrics storage and automatically delete old partitions. Use sampling for high-frequency metrics and focus detailed collection on anomalous events.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides the concrete data structures and Redis integration patterns needed to build the distributed rate limiting data model.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Format</td>\n<td>YAML files with validation</td>\n<td>etcd/Consul with dynamic updates</td>\n</tr>\n<tr>\n<td>Redis Client</td>\n<td><code>go-redis/redis</code> with connection pooling</td>\n<td><code>go-redis/redis</code> with cluster support</td>\n</tr>\n<tr>\n<td>Metrics Storage</td>\n<td>In-memory aggregation with periodic export</td>\n<td>ClickHouse/InfluxDB for time series</td>\n</tr>\n<tr>\n<td>Rule Validation</td>\n<td>JSON Schema validation</td>\n<td>Custom validation with dependency analysis</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  config/\n    rule.go              ← RateLimitRule definition and validation\n    loader.go            ← Rule loading from YAML/JSON\n    manager.go           ← RuleManager with pattern matching\n  storage/\n    redis.go             ← RedisStorage implementation\n    interface.go         ← Storage interface definition  \n    lua_scripts.go       ← Embedded Lua scripts for atomic operations\n  metrics/\n    collector.go         ← Metrics collection and aggregation\n    types.go            ← Metric data structures\n    exporter.go         ← Export metrics to external systems\n  models/\n    types.go            ← Core data type definitions</code></pre></div>\n\n<h4 id=\"core-data-structure-definitions\">Core Data Structure Definitions</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/go-redis/redis/v8</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisConfig holds Redis connection and behavior configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Addresses    []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `yaml:\"addresses\" json:\"addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Password     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"password\" json:\"password\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DB           </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">          `yaml:\"db\" json:\"db\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PoolSize     </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">          `yaml:\"pool_size\" json:\"pool_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"read_timeout\" json:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"write_timeout\" json:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DialTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"dial_timeout\" json:\"dial_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisStorage provides Redis-backed rate limiting state storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisStorage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#B392F0\">RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitRule defines a rate limiting policy with matching criteria and limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"id\" json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"name\" json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    KeyPattern  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"key_pattern\" json:\"key_pattern\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"algorithm\" json:\"algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Limit       </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `yaml:\"limit\" json:\"limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"window\" json:\"window\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BurstLimit  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `yaml:\"burst_limit\" json:\"burst_limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `yaml:\"enabled\" json:\"enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Priority    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"priority\" json:\"priority\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `yaml:\"created_at\" json:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `yaml:\"updated_at\" json:\"updated_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RuleManager manages rate limit rules with pattern matching and priority sorting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RuleManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rules </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitRequest contains context information for rate limit evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"user_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IPAddress   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"ip_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    APIEndpoint </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"api_endpoint\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserAgent   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"user_agent\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tokens      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">  `json:\"tokens\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitResult contains the outcome of rate limit evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Allowed    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"allowed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Remaining  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"remaining\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"retry_after\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResetTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"reset_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RuleID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"rule_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"redis-storage-implementation-skeleton\">Redis Storage Implementation Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRedisStorage creates a Redis storage instance with connection pooling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate config parameters (addresses not empty, positive timeouts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create Redis universal client with cluster support</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Test connection with PING command</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Load Lua scripts into Redis for atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return configured RedisStorage instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckAndUpdate atomically checks current usage and updates counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckAndUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Determine algorithm from key prefix</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Select appropriate Lua script for algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Prepare script arguments (key, limit, window, current_timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute Lua script atomically in Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse script response (allowed, remaining, reset_time)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle Redis errors with circuit breaker pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return rate limit decision with metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"rule-management-implementation-skeleton\">Rule Management Implementation Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadRules loads rate limit rules from YAML configuration file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LoadRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Read YAML file from configPath</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse YAML into slice of RateLimitRule structs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate each rule (positive limits, valid algorithms, pattern syntax)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check for rule ID conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Sort rules by priority (highest first)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update rm.rules map atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Log successful rule loading with count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetMatchingRules returns rules that match the request context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetMatchingRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">userID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ipAddress</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">apiEndpoint</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create request context map with userID, ipAddress, apiEndpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through rules in priority order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each rule, expand key_pattern template with request context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check if expanded pattern matches request (wildcards, exact match)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Add matching rules to result slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return rules sorted by priority (highest first)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use text/template package for pattern expansion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"metrics-collection-implementation-skeleton\">Metrics Collection Implementation Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> metrics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CollectMetrics gathers rate limiting metrics for monitoring and alerting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CollectMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract timestamp and dimensions from result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create RealTimeMetric struct with current values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add to in-memory buffer with timestamp-based key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update running counters (total requests, allowed, denied)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check if aggregation interval has passed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If interval passed, compute aggregated metrics and export</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Clean up old metrics from memory buffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"configuration-file-example\">Configuration File Example</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example rate limit rules configuration</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">rules</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"user-global\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Per-User Global Limit\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    key_pattern</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"user:{user_id}\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    algorithm</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"token_bucket\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1000</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    window</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1h\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    burst_limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">50</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    enabled</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    priority</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">50</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"upload-api\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Upload API Limit\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    key_pattern</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"api:/v1/upload\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    algorithm</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"sliding_window_counter\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">10000</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    window</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1m\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    enabled</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    priority</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ip-limit\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Per-IP Address Limit\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    key_pattern</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ip:{ip_address}\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    algorithm</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"sliding_window_log\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">500</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    window</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1h\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    enabled</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    priority</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">75</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing data structures:</strong></p>\n<ul>\n<li>Run <code>go test ./internal/models/...</code> - all type definitions should compile without errors</li>\n<li>Test rule validation with invalid configurations - should return specific error messages</li>\n<li>Verify Redis key generation matches expected patterns for each algorithm</li>\n</ul>\n<p><strong>After implementing Redis storage:</strong></p>\n<ul>\n<li>Run <code>go test ./internal/storage/...</code> - Redis operations should be atomic and consistent</li>\n<li>Test with Redis cluster to verify consistent hashing works correctly</li>\n<li>Verify Lua scripts execute atomically even under high concurrency</li>\n</ul>\n<p><strong>After implementing rule management:</strong></p>\n<ul>\n<li>Load sample configuration file - should parse all rules and sort by priority</li>\n<li>Test pattern matching with various request contexts - should match correct rules</li>\n<li>Verify rule updates propagate to all application instances within configured interval</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rules not loading</td>\n<td>YAML syntax error</td>\n<td>Check application logs for parsing errors</td>\n<td>Validate YAML syntax and required fields</td>\n</tr>\n<tr>\n<td>Redis keys growing infinitely</td>\n<td>Missing TTL on keys</td>\n<td>Use <code>redis-cli</code> to check key expiration</td>\n<td>Set appropriate TTL in Lua scripts</td>\n</tr>\n<tr>\n<td>Inconsistent rate limiting</td>\n<td>Clock skew between nodes</td>\n<td>Compare timestamps in Redis vs application logs</td>\n<td>Use Redis TIME command for consistent timestamps</td>\n</tr>\n<tr>\n<td>High memory usage</td>\n<td>Too many metric data points</td>\n<td>Monitor metrics buffer size</td>\n<td>Implement time-based cleanup and aggregation</td>\n</tr>\n<tr>\n<td>Pattern matching fails</td>\n<td>Template expansion error</td>\n<td>Log expanded patterns vs expected keys</td>\n<td>Debug template syntax and variable names</td>\n</tr>\n</tbody></table>\n<h2 id=\"rate-limiting-algorithms\">Rate Limiting Algorithms</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 - Rate Limiting Algorithms</p>\n</blockquote>\n<p>The heart of any distributed rate limiting system lies in its algorithms for tracking and enforcing request quotas over time. While the distributed coordination and Redis integration add complexity, the fundamental challenge remains: how do we accurately measure request rates and make allow/deny decisions in real-time? This section explores three complementary algorithms that form the foundation of our rate limiting system, each with distinct characteristics that make them suitable for different use cases and performance requirements.</p>\n<h3 id=\"mental-model-water-flow-control-systems\">Mental Model: Water Flow Control Systems</h3>\n<p>Before diving into the technical details of rate limiting algorithms, it&#39;s helpful to think about water flow control systems that we encounter in everyday life. Each rate limiting algorithm maps naturally to a different type of water control mechanism, helping us understand their behaviors and trade-offs intuitively.</p>\n<p><strong>Token Bucket as a Water Storage Tank</strong>: Imagine a water storage tank with a fixed capacity that receives water at a steady rate from a source pipe. When you need water (make a request), you can take it instantly from the tank if water is available. The key insight is that the tank allows you to consume water faster than the refill rate for short periods - you can drain the entire tank quickly if needed, but then you must wait for it to refill at the steady rate. This mirrors how token bucket allows controlled bursts above the sustained rate while preventing long-term overconsumption.</p>\n<p><strong>Sliding Window Counter as a Dam with Spillways</strong>: Picture a dam with multiple spillway gates that open and close on a schedule. Every minute, one gate opens to release exactly 1000 gallons, while simultaneously another gate closes. You&#39;re measuring the total flow by looking at all currently open gates. This approximates the flow rate over the last hour by dividing it into discrete time buckets. It&#39;s memory-efficient (you only track a few bucket counts) but imprecise at bucket boundaries - if all gates opened at the start of their window, you might see 2000 gallons flow in one minute even though the average is 1000 per minute.</p>\n<p><strong>Sliding Window Log as Individual Raindrop Tracking</strong>: Imagine tracking every individual raindrop that falls on your roof, recording the exact timestamp of each drop. To know the rainfall rate in the last hour, you count all drops within the 60-minute window ending right now. This gives you perfect accuracy - you know exactly how many drops fell in any time period. However, during a heavy downpour, you must remember millions of individual timestamps, making this approach memory-intensive but perfectly precise.</p>\n<p>These water analogies capture the essential trade-offs: token bucket provides controlled bursting, sliding window counter offers memory efficiency with some accuracy loss, and sliding window log delivers perfect accuracy at the cost of memory usage. Understanding these mental models helps us choose the right algorithm for different scenarios in our distributed rate limiting system.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Falgorithm-state-machine.svg\" alt=\"Token Bucket State Machine\"></p>\n<h3 id=\"token-bucket-algorithm-design\">Token Bucket Algorithm Design</h3>\n<p>The <strong>token bucket algorithm</strong> serves as our primary rate limiting mechanism because it naturally handles the most common real-world requirement: allowing controlled bursts while maintaining a sustainable long-term rate. This algorithm maintains a bucket that holds a fixed number of tokens, with new tokens added at a steady rate. Each request consumes one or more tokens, and requests are denied when insufficient tokens remain.</p>\n<p>The algorithm operates on two fundamental parameters that define its behavior. The <strong>bucket capacity</strong> determines the maximum burst size - how many requests can be processed instantly when the system has been idle. The <strong>refill rate</strong> controls the sustained throughput - how many requests per second the system allows over extended periods. These parameters work together to provide both responsiveness and protection.</p>\n<p><strong>Token Bucket State Management</strong></p>\n<p>Our token bucket implementation maintains state through a <code>TokenBucketState</code> structure that tracks the current token count and the timestamp of the last refill operation. This design enables efficient state updates by calculating how many tokens should be added based on elapsed time since the last refill, avoiding the need for background processes or timers.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>tokens</code></td>\n<td><code>int64</code></td>\n<td>Current number of tokens available in the bucket</td>\n</tr>\n<tr>\n<td><code>last_refill_time</code></td>\n<td><code>int64</code></td>\n<td>Unix timestamp in nanoseconds of the last refill calculation</td>\n</tr>\n</tbody></table>\n<p>The state transitions follow a predictable pattern that maps to our water storage tank analogy. When the bucket is <strong>full</strong>, incoming tokens are discarded (the tank overflows), and requests are immediately approved until tokens are exhausted. During <strong>partial fill</strong> states, each request decreases the token count, and the refill calculation determines how many tokens to add based on elapsed time. When <strong>empty</strong>, requests must wait for tokens to be replenished through the steady refill process.</p>\n<p><strong>Atomic Token Bucket Operations</strong></p>\n<p>The core challenge in distributed token bucket implementation lies in ensuring atomic check-and-update operations. In a single-threaded environment, checking the current token count and updating it can be separate operations. However, in a distributed system with multiple application instances, we must prevent race conditions where two instances simultaneously check the same token count and both approve requests that should collectively exceed the limit.</p>\n<p>Our solution employs Redis Lua scripts to guarantee atomicity. The script performs the following operations as a single indivisible unit:</p>\n<ol>\n<li><strong>Calculate elapsed time</strong> since the last refill by comparing the current timestamp with <code>last_refill_time</code> stored in Redis</li>\n<li><strong>Compute tokens to add</strong> by multiplying elapsed time by the refill rate, respecting the maximum bucket capacity</li>\n<li><strong>Update the token count</strong> with newly calculated tokens, ensuring it never exceeds the configured capacity</li>\n<li><strong>Check token sufficiency</strong> by comparing the requested token amount with the available tokens</li>\n<li><strong>Deduct tokens if sufficient</strong> and update both the token count and last refill timestamp</li>\n<li><strong>Return the decision</strong> along with remaining tokens and time until next token availability</li>\n</ol>\n<p>This atomic operation prevents the classic race condition where multiple instances read the same token count, each conclude sufficient tokens exist, and all approve requests that collectively exceed the limit.</p>\n<p><strong>Burst Handling and Refill Logic</strong></p>\n<p>The token bucket&#39;s burst handling capability distinguishes it from simpler rate limiting approaches. When a system has been idle, the bucket accumulates tokens up to its full capacity, enabling it to handle sudden traffic spikes without rejecting requests. This behavior closely mimics real-world usage patterns where traffic often arrives in bursts rather than smooth, evenly-distributed streams.</p>\n<p>The refill logic implements a <strong>continuous refill model</strong> rather than periodic batch refills. Instead of adding tokens every second through a background process, we calculate the exact number of tokens that should have been added based on elapsed time whenever a rate limit check occurs. This approach eliminates the need for background workers and ensures consistent behavior regardless of request timing patterns.</p>\n<p>For example, consider a token bucket configured with 100 tokens capacity and 50 tokens per second refill rate. If the bucket starts full and receives 100 requests instantly, all are approved and the bucket becomes empty. Subsequent requests must wait, but after 2 seconds of no activity, the bucket will have accumulated 100 tokens again (2 seconds × 50 tokens/second), ready for another burst.</p>\n<p><strong>Configuration and Tuning Parameters</strong></p>\n<p>Token bucket behavior is controlled through the <code>TokenBucketConfig</code> structure, which encapsulates all necessary parameters for proper operation:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>capacity</code></td>\n<td><code>int64</code></td>\n<td>Maximum number of tokens the bucket can hold (burst size)</td>\n</tr>\n<tr>\n<td><code>refill_rate</code></td>\n<td><code>int64</code></td>\n<td>Number of tokens added per second (sustained rate)</td>\n</tr>\n<tr>\n<td><code>window</code></td>\n<td><code>time.Duration</code></td>\n<td>Time window for rate calculations (typically 1 second)</td>\n</tr>\n</tbody></table>\n<p>The relationship between these parameters determines the algorithm&#39;s characteristics. A high capacity relative to refill rate creates a &quot;bursty&quot; system that can handle large traffic spikes but takes longer to recover. A low capacity relative to refill rate creates a &quot;smooth&quot; system that provides steady throughput with limited burst capability.</p>\n<p><strong>Token Bucket Implementation Architecture</strong></p>\n<p>The <code>TokenBucket</code> type encapsulates the algorithm logic and integrates with our distributed storage layer:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Check</code></td>\n<td><code>ctx context.Context, key string, tokens int64</code></td>\n<td><code>*RateLimitResult, error</code></td>\n<td>Atomically checks and updates token bucket state</td>\n</tr>\n<tr>\n<td><code>Preview</code></td>\n<td><code>ctx context.Context, key string, tokens int64</code></td>\n<td><code>*RateLimitResult, error</code></td>\n<td>Returns current state without consuming tokens</td>\n</tr>\n<tr>\n<td><code>Reset</code></td>\n<td><code>ctx context.Context, key string</code></td>\n<td><code>error</code></td>\n<td>Resets bucket to full capacity</td>\n</tr>\n<tr>\n<td><code>GetState</code></td>\n<td><code>ctx context.Context, key string</code></td>\n<td><code>*TokenBucketState, error</code></td>\n<td>Returns current bucket state for monitoring</td>\n</tr>\n</tbody></table>\n<p>The <code>Check</code> method serves as the primary interface for rate limiting decisions. It accepts a context for timeout control, a key identifying the specific rate limit bucket, and the number of tokens requested. The method returns a <code>RateLimitResult</code> containing the decision, remaining tokens, and timing information needed for proper HTTP response headers.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Token bucket&#39;s strength lies in its intuitive burst behavior that matches real-world traffic patterns. Unlike algorithms that strictly enforce per-second limits, token bucket allows natural traffic spikes while preventing sustained overload. This makes it ideal for user-facing APIs where occasional bursts are acceptable but sustained abuse must be prevented.</p>\n</blockquote>\n<h3 id=\"sliding-window-algorithms-design\">Sliding Window Algorithms Design</h3>\n<p>While token bucket algorithms excel at handling burst traffic patterns, sliding window algorithms provide more predictable rate limiting behavior by measuring request rates over precise time windows. Our system implements two sliding window variants: <strong>sliding window counter</strong> for memory efficiency and <strong>sliding window log</strong> for perfect accuracy. Understanding their trade-offs helps us select the appropriate algorithm based on specific rate limiting requirements.</p>\n<p><strong>Sliding Window Counter Algorithm</strong></p>\n<p>The sliding window counter algorithm divides time into fixed-size buckets and maintains a count of requests in each bucket. To determine the current rate, it examines all buckets within the sliding window and aggregates their counts. This approach provides memory efficiency by storing only bucket counts rather than individual request timestamps, making it suitable for high-traffic scenarios where memory usage must be controlled.</p>\n<p>Our implementation maintains a configurable number of sub-windows within each rate limiting window. For example, a 60-second rate limit window might be divided into 12 sub-windows of 5 seconds each. This granularity affects both memory usage and accuracy - more sub-windows provide better accuracy but consume more memory.</p>\n<p>The algorithm tracks the following data for each rate limit key:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>window_start</code></td>\n<td><code>int64</code></td>\n<td>Unix timestamp of the current window start time</td>\n</tr>\n<tr>\n<td><code>bucket_counts</code></td>\n<td><code>[]int64</code></td>\n<td>Array of request counts for each sub-window bucket</td>\n</tr>\n<tr>\n<td><code>bucket_timestamps</code></td>\n<td><code>[]int64</code></td>\n<td>Array of timestamps for each bucket to handle expiration</td>\n</tr>\n<tr>\n<td><code>total_count</code></td>\n<td><code>int64</code></td>\n<td>Cached total count across all buckets for efficiency</td>\n</tr>\n</tbody></table>\n<p>When processing a rate limit check, the algorithm follows these steps:</p>\n<ol>\n<li><strong>Calculate current bucket index</strong> based on the current timestamp and bucket duration</li>\n<li><strong>Expire old buckets</strong> by zeroing counts for buckets outside the sliding window</li>\n<li><strong>Update current bucket</strong> by incrementing the count for the active time bucket</li>\n<li><strong>Aggregate total count</strong> across all non-expired buckets within the window</li>\n<li><strong>Compare against limit</strong> and return the appropriate decision with remaining quota</li>\n</ol>\n<p>The key advantage of this approach lies in its <strong>bounded memory usage</strong>. Regardless of request volume, each rate limit key consumes memory proportional to the number of sub-windows rather than the number of requests. A system processing millions of requests per second still only stores a few dozen bucket counts per rate limit key.</p>\n<p>However, this efficiency comes with an accuracy trade-off known as the <strong>boundary condition problem</strong>. Consider a rate limit of 1000 requests per minute using 5-second buckets. If 1000 requests arrive in the first second of minute 1 and another 1000 requests arrive in the first second of minute 2, the algorithm might allow both bursts because they fall in different minute-windows, even though 2000 requests occurred within a 61-second period.</p>\n<p><strong>Sliding Window Log Algorithm</strong></p>\n<p>The sliding window log algorithm eliminates the boundary condition problem by maintaining exact timestamps for each request within the sliding window. When evaluating a rate limit, it counts all requests that occurred within the precise time window ending at the current moment, providing perfect accuracy regardless of request timing patterns.</p>\n<p>This algorithm stores request timestamps in a time-ordered data structure:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>request_timestamps</code></td>\n<td><code>[]int64</code></td>\n<td>Sorted array of Unix timestamps for requests in the window</td>\n</tr>\n<tr>\n<td><code>window_duration</code></td>\n<td><code>int64</code></td>\n<td>Duration of the sliding window in nanoseconds</td>\n</tr>\n<tr>\n<td><code>last_cleanup</code></td>\n<td><code>int64</code></td>\n<td>Timestamp of last cleanup operation to remove expired entries</td>\n</tr>\n</tbody></table>\n<p>The request evaluation process provides exact rate calculations:</p>\n<ol>\n<li><strong>Remove expired timestamps</strong> older than the current window by scanning from the beginning of the array</li>\n<li><strong>Count remaining timestamps</strong> to determine current request count within the window</li>\n<li><strong>Compare against rate limit</strong> to make the allow/deny decision</li>\n<li><strong>Add current timestamp</strong> if the request is allowed</li>\n<li><strong>Return result</strong> with exact remaining quota and reset time</li>\n</ol>\n<p>This precision comes at a significant memory cost. During traffic spikes, the algorithm must store individual timestamps for every request within the window duration. A system allowing 10,000 requests per minute must maintain up to 10,000 timestamps in memory per rate limit key. For applications with thousands of rate-limited keys, this memory usage can become prohibitive.</p>\n<p><strong>Algorithm Accuracy Comparison</strong></p>\n<p>To understand the practical implications of each algorithm&#39;s accuracy characteristics, consider how they handle edge cases:</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Token Bucket Behavior</th>\n<th>Sliding Window Counter</th>\n<th>Sliding Window Log</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Burst at window boundary</td>\n<td>Allows up to full capacity instantly</td>\n<td>May allow up to 2× limit at boundaries</td>\n<td>Perfect accuracy, never exceeds limit</td>\n</tr>\n<tr>\n<td>Steady traffic</td>\n<td>Enforces long-term average rate</td>\n<td>Enforces average with small variations</td>\n<td>Enforces exact rate continuously</td>\n</tr>\n<tr>\n<td>Irregular patterns</td>\n<td>Smooths bursts through token accumulation</td>\n<td>Approximates rate with bucket granularity</td>\n<td>Measures exact rate regardless of pattern</td>\n</tr>\n<tr>\n<td>Memory usage</td>\n<td>O(1) per key</td>\n<td>O(buckets) per key</td>\n<td>O(requests in window) per key</td>\n</tr>\n<tr>\n<td>Computational cost</td>\n<td>O(1) per request</td>\n<td>O(1) per request</td>\n<td>O(log n) per request for cleanup</td>\n</tr>\n</tbody></table>\n<p><strong>Sliding Window Implementation Strategy</strong></p>\n<p>Both sliding window algorithms integrate with our Redis backend through specialized Lua scripts that ensure atomic operations. The scripts handle the complex logic of timestamp management, expiration, and count aggregation while maintaining consistency across multiple application instances.</p>\n<p>For sliding window counter, the Lua script manages bucket rotation and count aggregation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Sliding window counter operations happen atomically in Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 1. Calculate current bucket based on timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 2. Expire old buckets outside the window  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 3. Increment current bucket count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 4. Sum all active bucket counts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 5. Return decision and remaining quota</span></span></code></pre></div>\n\n<p>For sliding window log, the script manages timestamp arrays with careful memory cleanup:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Sliding window log operations happen atomically in Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 1. Remove timestamps older than window duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 2. Count remaining timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 3. Add new timestamp if under limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 4. Return exact remaining quota</span></span></code></pre></div>\n\n<blockquote>\n<p><strong>Performance Consideration</strong>: Sliding window log&#39;s memory usage can grow large during traffic spikes, making it unsuitable for high-volume systems without careful memory management. However, its perfect accuracy makes it ideal for strict rate limiting scenarios where boundary conditions cannot be tolerated, such as payment processing or security-sensitive APIs.</p>\n</blockquote>\n<p><strong>Hybrid Sliding Window Approach</strong></p>\n<p>For scenarios requiring both memory efficiency and improved accuracy, our system supports a hybrid approach that combines sliding window counter granularity with boundary condition detection. This algorithm uses fine-grained buckets (1-second buckets for 60-second windows) and applies smoothing logic to reduce boundary effects.</p>\n<p>The hybrid algorithm applies a <strong>weighted calculation</strong> that considers partial bucket contributions when evaluating rate limits near window boundaries. Instead of simply summing bucket counts, it calculates what portion of each bucket falls within the exact sliding window and weights the counts accordingly.</p>\n<p>This approach provides significantly better accuracy than standard sliding window counter while maintaining bounded memory usage. The computational overhead increases slightly due to weighted calculations, but remains much more efficient than maintaining individual request timestamps.</p>\n<h3 id=\"algorithm-selection-adr\">Algorithm Selection ADR</h3>\n<p>Choosing the appropriate rate limiting algorithm significantly impacts system performance, accuracy, and operational characteristics. This decision affects memory usage, computational overhead, user experience, and the types of traffic patterns the system can handle effectively.</p>\n<blockquote>\n<p><strong>Decision: Multi-Algorithm Support with Configurable Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Different rate limiting scenarios have conflicting requirements. API endpoints serving user traffic need burst handling for good UX, while security-sensitive endpoints require strict accuracy. High-volume systems need memory efficiency, while low-volume critical systems can afford precision overhead.</li>\n<li><strong>Options Considered</strong>: Single algorithm for simplicity, token bucket only for burst handling, sliding window log only for accuracy, configurable algorithm selection per rule</li>\n<li><strong>Decision</strong>: Implement all three algorithms with per-rule configuration, defaulting to token bucket for most use cases</li>\n<li><strong>Rationale</strong>: Different endpoints have fundamentally different requirements that cannot be satisfied by a single algorithm. The additional complexity is justified by the flexibility to optimize each rate limit rule for its specific constraints and requirements.</li>\n<li><strong>Consequences</strong>: Increases implementation complexity and testing surface area, but enables optimal algorithm selection for each use case, improving both performance and user experience across different scenarios.</li>\n</ul>\n</blockquote>\n<p><strong>Algorithm Selection Decision Matrix</strong></p>\n<p>The choice of rate limiting algorithm should be based on specific requirements and constraints:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Memory Usage</th>\n<th>Accuracy</th>\n<th>Burst Handling</th>\n<th>Best Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket</td>\n<td>O(1) per key</td>\n<td>Good for sustained rate</td>\n<td>Excellent - natural bursting</td>\n<td>User-facing APIs, general rate limiting</td>\n</tr>\n<tr>\n<td>Sliding Window Counter</td>\n<td>O(buckets) per key</td>\n<td>Good with boundary effects</td>\n<td>Limited - depends on bucket size</td>\n<td>High-volume systems, memory-constrained environments</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n<td>O(requests) per key</td>\n<td>Perfect</td>\n<td>None - strict enforcement</td>\n<td>Security APIs, payment processing, strict SLA enforcement</td>\n</tr>\n</tbody></table>\n<p><strong>Configuration Recommendations by Scenario</strong></p>\n<p>Based on common distributed system patterns, we recommend the following algorithm selections:</p>\n<p><strong>User-Facing API Endpoints</strong> should use token bucket with generous burst capacity. Users often interact with applications in bursts - opening a page might trigger multiple API calls simultaneously. Token bucket naturally accommodates this pattern while preventing sustained abuse.</p>\n<p>Recommended configuration:</p>\n<ul>\n<li>Algorithm: <code>ALGORITHM_TOKEN_BUCKET</code></li>\n<li>Capacity: 3-5× the sustained rate (allows reasonable bursts)</li>\n<li>Refill rate: Target sustained requests per second</li>\n<li>Example: 100 requests/minute with 20-token capacity allows 20 instant requests followed by ~1.67 requests/second</li>\n</ul>\n<p><strong>Security-Sensitive Endpoints</strong> should use sliding window log for perfect accuracy. Authentication attempts, password resets, and payment operations cannot tolerate the boundary condition issues that might allow double the intended rate limit.</p>\n<p>Recommended configuration:</p>\n<ul>\n<li>Algorithm: <code>ALGORITHM_SLIDING_WINDOW_LOG</code></li>\n<li>Window: Short duration to limit memory usage (5-15 minutes)</li>\n<li>Limit: Conservative values with no burst allowance</li>\n<li>Example: 5 login attempts per 15 minutes with exact enforcement</li>\n</ul>\n<p><strong>High-Volume Internal APIs</strong> should use sliding window counter with fine-grained buckets. Service-to-service communication often has predictable patterns and can tolerate slight accuracy variations in exchange for memory efficiency.</p>\n<p>Recommended configuration:</p>\n<ul>\n<li>Algorithm: <code>ALGORITHM_SLIDING_COUNTER</code></li>\n<li>Buckets: 10-20 sub-windows for reasonable accuracy</li>\n<li>Window: Match service SLA requirements</li>\n<li>Example: 10,000 requests/minute divided into 12 five-second buckets</li>\n</ul>\n<p><strong>Global Rate Limits</strong> protecting overall system capacity should use sliding window counter with coarse granularity. These limits serve as circuit breakers preventing total system overload, where approximate enforcement is acceptable for memory efficiency.</p>\n<p><strong>Algorithm Performance Characteristics</strong></p>\n<p>Understanding the computational and memory overhead of each algorithm helps in capacity planning:</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Token Bucket</th>\n<th>Sliding Window Counter</th>\n<th>Sliding Window Log</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis operations per check</td>\n<td>1 Lua script execution</td>\n<td>1 Lua script execution</td>\n<td>1 Lua script execution</td>\n</tr>\n<tr>\n<td>Memory per key (idle)</td>\n<td>~32 bytes</td>\n<td>~64 bytes + (8 × buckets)</td>\n<td>~64 bytes</td>\n</tr>\n<tr>\n<td>Memory per key (active)</td>\n<td>~32 bytes</td>\n<td>~64 bytes + (8 × buckets)</td>\n<td>~64 bytes + (8 × requests in window)</td>\n</tr>\n<tr>\n<td>CPU overhead</td>\n<td>Minimal</td>\n<td>Low</td>\n<td>Moderate (timestamp cleanup)</td>\n</tr>\n<tr>\n<td>Network overhead</td>\n<td>Low</td>\n<td>Low</td>\n<td>Higher (larger data structures)</td>\n</tr>\n</tbody></table>\n<p><strong>Migration Strategy Between Algorithms</strong></p>\n<p>Systems may need to change algorithms as requirements evolve. Our design supports algorithm migration through versioned rate limit rules:</p>\n<ol>\n<li><strong>Gradual Rollout</strong>: Deploy new rules with different algorithms alongside existing rules</li>\n<li><strong>A/B Testing</strong>: Route percentage of traffic to new algorithm for validation</li>\n<li><strong>State Transition</strong>: Both algorithms can coexist during migration periods</li>\n<li><strong>Monitoring</strong>: Compare accuracy, performance, and user experience metrics</li>\n<li><strong>Cutover</strong>: Switch traffic once new algorithm proves stable and effective</li>\n</ol>\n<blockquote>\n<p><strong>Common Pitfall</strong>: Choosing sliding window log for high-volume endpoints without considering memory implications. A popular API endpoint processing 100,000 requests per minute with a 5-minute window could require storing 500,000 timestamps per rate limit key. With multiple rate limit dimensions (per-user, per-IP, global), memory usage can quickly become unsustainable.</p>\n</blockquote>\n<p>The flexibility to select appropriate algorithms per rate limit rule enables optimization for diverse requirements within a single system, providing both the performance needed for high-volume endpoints and the accuracy required for security-sensitive operations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance focuses on building the core rate limiting algorithms that serve as the foundation for our distributed rate limiting system. The algorithms themselves are the primary learning objective, while Redis integration and HTTP handling serve as supporting infrastructure.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time Handling</td>\n<td><code>time.Now()</code> with Unix timestamps</td>\n<td>High-resolution timestamps with <code>time.Now().UnixNano()</code></td>\n</tr>\n<tr>\n<td>Math Operations</td>\n<td>Standard int64 arithmetic</td>\n<td><code>math</code> package for precision calculations</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Hard-coded constants</td>\n<td>YAML configuration with validation</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Basic unit tests</td>\n<td>Property-based testing with random inputs</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/algorithms/\n  token_bucket.go          ← Token bucket algorithm implementation\n  token_bucket_test.go     ← Unit tests for token bucket\n  sliding_window.go        ← Sliding window counter and log algorithms  \n  sliding_window_test.go   ← Unit tests for sliding windows\n  types.go                 ← Common algorithm types and interfaces\n  config.go                ← Algorithm configuration structures\n  testdata/\n    algorithm_configs.yaml ← Test configuration files</code></pre></div>\n\n<p><strong>Infrastructure Starter Code</strong></p>\n<p>The following infrastructure code handles time management, configuration, and basic Redis operations, allowing you to focus on implementing the algorithm logic:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> algorithms</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Storage interface abstracts the underlying storage mechanism</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This allows algorithms to work with Redis, in-memory, or other backends</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Storage</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Lua script execution for atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ExecuteLua</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">script</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">keys</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">args</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) (</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simple get/set operations for non-atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expiration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Delete operations for cleanup</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Delete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">keys</span><span style=\"color:#F97583\"> ...</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeProvider allows mocking time in tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeProvider</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Now</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RealTimeProvider</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#B392F0\">RealTimeProvider</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Configuration structures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucketConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Capacity   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `yaml:\"capacity\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RefillRate </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `yaml:\"refill_rate\"`</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"window\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SlidingWindowConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Limit      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `yaml:\"limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"window\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Buckets    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"buckets\"`</span><span style=\"color:#6A737D\">    // For counter algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Result structures  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Allowed     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"allowed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Remaining   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"remaining\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"retry_after\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResetTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"reset_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RuleID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"rule_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// State structures for Redis storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucketState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tokens         </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `json:\"tokens\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastRefillTime </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `json:\"last_refill_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Algorithm Skeleton Code</strong></p>\n<p><strong>Token Bucket Implementation Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> algorithms</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucket</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#B392F0\">TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage      </span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeProvider </span><span style=\"color:#B392F0\">TimeProvider</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTokenBucket</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> TokenBucketConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> Storage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:       config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:      storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeProvider: </span><span style=\"color:#B392F0\">RealTimeProvider</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check performs atomic token bucket rate limiting check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tokensRequested</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get current timestamp in nanoseconds using tb.timeProvider.Now().UnixNano()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute Redis Lua script with key, timestamp, and configuration parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse Lua script result containing allowed flag, remaining tokens, and next refill time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Build RateLimitResult with appropriate retry_after and reset_time values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle Redis errors by returning error for upstream handling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Lua script for atomic token bucket operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    luaScript </span><span style=\"color:#F97583\">:=</span><span style=\"color:#9ECBFF\"> `</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 6: Parse input parameters (key timestamp, tokens_requested, capacity, refill_rate, window_ns)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 7: Get current state from Redis or initialize if doesn't exist  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 8: Calculate tokens to add based on elapsed time since last refill</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 9: Update token count (new_tokens = min(current + added, capacity))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 10: Check if sufficient tokens available for request</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 11: If allowed, subtract requested tokens and update state in Redis</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 12: Return result array with [allowed, remaining_tokens, next_refill_timestamp]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    `</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use tb.storage.ExecuteLua() with keys=[key] and args=[timestamp, tokensRequested, tb.config.Capacity, tb.config.RefillRate, tb.config.Window.Nanoseconds()]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Redis returns []interface{} that you need to type assert to []interface{} then individual elements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Calculate retry_after as time until next token becomes available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Preview checks current state without consuming tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Preview</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 13: Similar to Check() but with tokensRequested = 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 14: Lua script should not subtract tokens, only return current state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Reuse most logic from Check() but don't modify state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Reset clears the bucket state, effectively filling it to capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 15: Delete the Redis key to reset state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 16: Handle Redis errors appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use tb.storage.Delete(ctx, key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Sliding Window Counter Implementation Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SlidingWindowCounter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#B392F0\">SlidingWindowConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage      </span><span style=\"color:#B392F0\">Storage</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeProvider </span><span style=\"color:#B392F0\">TimeProvider</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSlidingWindowCounter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> SlidingWindowConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> Storage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlidingWindowCounter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">SlidingWindowCounter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:       config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:      storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeProvider: </span><span style=\"color:#B392F0\">RealTimeProvider</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check performs atomic sliding window counter rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">swc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlidingWindowCounter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">increment</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 17: Calculate current timestamp and bucket duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 18: Execute Lua script for atomic bucket operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 19: Parse results and build RateLimitResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    luaScript </span><span style=\"color:#F97583\">:=</span><span style=\"color:#9ECBFF\"> `</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 20: Calculate current bucket index from timestamp</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 21: Load existing bucket counts from Redis hash</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 22: Expire buckets outside the sliding window</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 23: Increment current bucket count  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 24: Sum all active bucket counts</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 25: Compare total against limit and return result</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    `</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use Redis hash to store bucket counts with bucket index as field name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Calculate bucket index as (timestamp / bucket_duration) % num_buckets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Window contains last N buckets, expire older ones</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Sliding Window Log Implementation Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SlidingWindowLog</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#B392F0\">SlidingWindowConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage      </span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeProvider </span><span style=\"color:#B392F0\">TimeProvider</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check performs atomic sliding window log rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">swl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlidingWindowLog</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">increment</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 26: Get current timestamp for window calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 27: Execute Lua script for atomic timestamp operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 28: Handle memory cleanup to prevent unbounded growth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    luaScript </span><span style=\"color:#F97583\">:=</span><span style=\"color:#9ECBFF\"> `</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 29: Load current timestamp list from Redis sorted set</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 30: Remove timestamps older than window duration</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 31: Count remaining timestamps in window</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 32: If under limit, add current timestamp to set</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        -- TODO 33: Return decision with exact remaining count</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    `</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use Redis sorted set (ZSET) with timestamps as scores for efficient range operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use ZREMRANGEBYSCORE to remove old timestamps atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use ZCARD to count current timestamps in set</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Algorithm Factory and Selection Logic:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// AlgorithmFactory creates algorithm instances based on configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlgorithmFactory</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">af </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlgorithmFactory</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateAlgorithm</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">algorithmType</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) (</span><span style=\"color:#B392F0\">RateLimitingAlgorithm</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 34: Switch on algorithmType (ALGORITHM_TOKEN_BUCKET, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 35: Cast config to appropriate type and validate parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 36: Return appropriate algorithm instance or error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use type switches for config casting: config.(*TokenBucketConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Common interface for all algorithms</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitingAlgorithm</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tokens</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Preview</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Reset</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints</strong></p>\n<ul>\n<li>Use <code>time.Now().UnixNano()</code> for high-precision timestamps to avoid race conditions with multiple requests in the same millisecond</li>\n<li>Redis Lua scripts return <code>[]interface{}</code> that require type assertion: <code>result[0].(int64)</code></li>\n<li>Handle Redis connection errors gracefully - rate limiting failures should not crash the application</li>\n<li>Use <code>math.Min()</code> for token calculations to prevent overflow: <code>math.Min(float64(current + added), float64(capacity))</code></li>\n<li>Validate configuration parameters at startup to fail fast on invalid settings</li>\n</ul>\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the rate limiting algorithms, verify your implementation:</p>\n<p><strong>Testing Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/algorithms/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/algorithms/...</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#6A737D\">  # Check for race conditions</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/algorithms/...</span><span style=\"color:#79B8FF\"> -bench=.</span><span style=\"color:#6A737D\"> # Performance benchmarks</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ul>\n<li>Token bucket allows bursts up to capacity, then enforces sustained rate</li>\n<li>Sliding window counter approximates rate with small boundary effects  </li>\n<li>Sliding window log provides exact rate enforcement with higher memory usage</li>\n<li>All algorithms handle concurrent access safely through atomic Lua scripts</li>\n</ul>\n<p><strong>Manual Verification:</strong></p>\n<ul>\n<li>Create token bucket with 10 capacity, 1/second refill rate</li>\n<li>Make 10 instant requests → all should succeed</li>\n<li>Make 11th request immediately → should be denied</li>\n<li>Wait 5 seconds, make 5 requests → all should succeed</li>\n<li>Monitor Redis keys to verify state is properly maintained</li>\n</ul>\n<p><strong>Debugging Signs:</strong></p>\n<ul>\n<li>&quot;Token count negative&quot; → Race condition in Lua script logic</li>\n<li>&quot;Memory usage growing unbounded&quot; → Sliding window log not cleaning up old timestamps  </li>\n<li>&quot;Inconsistent rate limiting&quot; → Time synchronization issues between instances</li>\n<li>&quot;Redis timeout errors&quot; → Connection pool exhaustion or network issues</li>\n</ul>\n<p>The algorithms implemented in this milestone form the foundation for all subsequent distributed rate limiting functionality. Focus on correctness and atomicity - performance optimizations can be added later once the core logic is solid.</p>\n<h2 id=\"multi-tier-rate-limiting\">Multi-Tier Rate Limiting</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 - Multi-tier Rate Limiting</p>\n</blockquote>\n<p>The power of distributed rate limiting extends far beyond simple request counting. Real-world applications require sophisticated hierarchical controls that operate across multiple dimensions simultaneously - protecting individual users from abuse, preventing single IP addresses from overwhelming the system, ensuring fair API resource allocation, and maintaining global system capacity. This multi-tier approach transforms rate limiting from a binary gate into an intelligent traffic management system that can adapt to complex usage patterns while maintaining predictable system performance.</p>\n<h3 id=\"mental-model-cascading-security-checkpoints\">Mental Model: Cascading Security Checkpoints</h3>\n<p>Think of multi-tier rate limiting like airport security checkpoints, where passengers must pass through multiple stages of screening before reaching their destination. Each checkpoint serves a different purpose and operates independently, but they work together to ensure overall security and flow management.</p>\n<p>At the document check station, each individual passenger (user) has personal limits - they can only travel on their specific ticket with their allocated baggage allowance. This represents <strong>per-user rate limits</strong> that prevent individual accounts from consuming excessive resources through either malicious behavior or buggy client code.</p>\n<p>The security scanner represents <strong>per-IP rate limits</strong>, where the physical location (IP address) has throughput constraints regardless of how many different passengers (users) might be coming from that location. A single corporate NAT gateway might have hundreds of employees behind it, but the checkpoint can only process a certain number of people per minute from any single entry point.</p>\n<p>The gate area introduces <strong>per-API rate limits</strong>, where different flight destinations (API endpoints) have different capacity constraints. The international terminal might handle fewer but more complex departures, while domestic gates process higher volumes of simpler operations. Each API endpoint has distinct resource requirements and thus different rate limiting policies.</p>\n<p>Finally, the airport itself has <strong>global rate limits</strong> - a maximum number of passengers it can process per hour across all gates, regardless of individual tickets or origins. Even if every individual checkpoint is under capacity, the airport&#39;s overall throughput ceiling prevents system-wide congestion that would affect everyone.</p>\n<p>The critical insight is that passengers must successfully pass through ALL checkpoints in sequence. Being approved at document check doesn&#39;t guarantee passage through security, and having space at your gate doesn&#39;t matter if the airport has reached its global capacity limit. However, once any checkpoint rejects a passenger, there&#39;s no need to continue checking the remaining stations - this <strong>short-circuit evaluation</strong> saves resources and provides faster feedback.</p>\n<p>Just as airport security adapts to different threat levels and passenger volumes, multi-tier rate limiting systems must handle varying load patterns, user behaviors, and attack scenarios while maintaining fair resource allocation across all dimensions.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fmulti-tier-evaluation.svg\" alt=\"Multi-Tier Rate Limit Evaluation\"></p>\n<h3 id=\"tier-evaluation-strategy\">Tier Evaluation Strategy</h3>\n<p>Multi-tier rate limiting requires a sophisticated evaluation strategy that balances accuracy, performance, and resource consumption. The system must check multiple rate limit dimensions efficiently while avoiding unnecessary computation when limits are already exceeded. The evaluation algorithm determines not just whether a request should be allowed, but which specific limits are constraining the user and how much capacity remains across all tiers.</p>\n<p>The <strong>tier precedence hierarchy</strong> establishes the order in which rate limits are evaluated, typically flowing from most specific to most general: per-user limits, per-IP limits, per-API limits, and finally global limits. This ordering reflects both the logical dependency relationships and the performance characteristics of different limit types. User-specific limits are typically the most restrictive and fastest to evaluate since they operate on a single key, while global limits require aggregation across many keys and are computationally expensive.</p>\n<p>However, tier precedence alone is insufficient - the system must also consider <strong>limit severity</strong> and <strong>evaluation cost</strong> when determining the optimal checking sequence. A per-user limit that allows 1000 requests per hour should be checked before a global limit that allows 10 million requests per hour, even if the global limit is technically &quot;higher&quot; in the hierarchy. Similarly, expensive operations like sliding window log evaluations should be deferred until cheaper approximations (like sliding window counters) have passed.</p>\n<p>The <strong>short-circuit evaluation algorithm</strong> implements this strategy through a prioritized sequence of checks:</p>\n<ol>\n<li><p><strong>Load applicable rules</strong> for the request context by matching the user ID, IP address, and API endpoint against the rule patterns stored in the <code>RuleManager</code>. This initial filtering reduces the evaluation set from potentially thousands of rules to a manageable subset of 3-10 relevant limits.</p>\n</li>\n<li><p><strong>Sort rules by priority and cost</strong> to establish the evaluation order. Rules with higher priority values (indicating more restrictive or important limits) are checked first, with tie-breaking based on computational cost estimates for each algorithm type.</p>\n</li>\n<li><p><strong>Evaluate each rule in sequence</strong> using the appropriate rate limiting algorithm. For each rule, extract the required tokens from the request, construct the Redis key using the rule&#39;s key pattern, and perform the atomic check-and-update operation.</p>\n</li>\n<li><p><strong>Short-circuit on first failure</strong> - as soon as any rule denies the request, immediately return a rejection response without evaluating remaining rules. The response includes the specific rule ID, remaining capacity, and retry-after timing from the failing rule.</p>\n</li>\n<li><p><strong>Aggregate success results</strong> - if all rules pass, combine the results to determine the most restrictive remaining capacity and earliest reset time across all checked tiers. This aggregate information populates the rate limit headers in the successful response.</p>\n</li>\n</ol>\n<p>The evaluation strategy must also handle <strong>rule conflicts</strong> where multiple rules apply to the same request dimension but specify different limits or algorithms. The system resolves conflicts by giving precedence to rules with higher priority values, with the most restrictive rule taking effect when priorities are equal.</p>\n<p><strong>Rule matching</strong> uses pattern-based key composition to determine which rules apply to each request. A rule with key pattern <code>user:{user_id}:api:orders</code> matches requests from any user to the orders API endpoint, while <code>user:premium_user_*:api:*</code> applies broader limits to all premium users across all endpoints. The pattern matching system supports wildcards, prefix matching, and parameterized substitution to create flexible rule targeting.</p>\n<p>The evaluation strategy incorporates <strong>performance optimizations</strong> to minimize latency and Redis load. Rule results are cached locally for short periods (typically 100-500 milliseconds) to avoid redundant checks for burst traffic patterns. The system also maintains separate connections to Redis for different rule priorities, allowing high-priority user limits to bypass congestion from expensive global limit calculations.</p>\n<p><strong>Error handling during evaluation</strong> follows a fail-open strategy by default - if a specific rule cannot be evaluated due to Redis connectivity issues, the system logs the failure and continues checking remaining rules. Only if all applicable rules fail to evaluate does the system fall back to local rate limiting or (in strict security mode) deny the request entirely.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The tier evaluation strategy serves as the nervous system of the distributed rate limiter, coordinating decisions across multiple dimensions while maintaining sub-millisecond response times. The short-circuit approach not only improves performance but also provides clearer user feedback by identifying the specific limit that constrained their request rather than an ambiguous &quot;rate limited&quot; message.</p>\n</blockquote>\n<h4 id=\"tier-evaluation-algorithm-details\">Tier Evaluation Algorithm Details</h4>\n<p>The core tier evaluation algorithm operates through a series of well-defined phases that transform a raw request into a comprehensive rate limiting decision:</p>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Discovery</td>\n<td><code>RateLimitRequest</code> with user/IP/API context</td>\n<td>Pattern matching against <code>RuleManager</code> rule set</td>\n<td>Filtered list of applicable <code>RateLimitRule</code> objects</td>\n<td>O(log n) with indexed patterns</td>\n</tr>\n<tr>\n<td>Rule Prioritization</td>\n<td>List of applicable rules</td>\n<td>Sort by priority field, then by algorithm cost</td>\n<td>Ordered evaluation sequence</td>\n<td>O(k log k) where k is rule count</td>\n</tr>\n<tr>\n<td>Sequential Evaluation</td>\n<td>Ordered rule list + request tokens</td>\n<td>Redis-backed algorithm execution per rule</td>\n<td>First failure or aggregated success</td>\n<td>O(k) Redis operations, short-circuits</td>\n</tr>\n<tr>\n<td>Result Aggregation</td>\n<td>Individual rule results</td>\n<td>Compute most restrictive limits and earliest reset</td>\n<td>Final <code>RateLimitResult</code> with headers</td>\n<td>O(k) in-memory computation</td>\n</tr>\n</tbody></table>\n<p>The <strong>rule discovery phase</strong> leverages efficient pattern matching to avoid evaluating irrelevant rules. The <code>RuleManager</code> maintains separate indices for user patterns, IP patterns, and API patterns, allowing the system to quickly identify candidate rules without scanning the entire rule set. For a request from user &quot;user123&quot; to IP &quot;192.168.1.10&quot; accessing &quot;/api/orders&quot;, the discovery process queries each index independently and takes the intersection of matching rules.</p>\n<p><strong>Rule prioritization</strong> considers both the explicit priority field and implicit algorithm costs when determining evaluation order:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm Type</th>\n<th>Relative Cost</th>\n<th>Reason</th>\n<th>Evaluation Priority</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ALGORITHM_TOKEN_BUCKET</code></td>\n<td>Low</td>\n<td>Single Redis key, simple arithmetic</td>\n<td>Check first</td>\n</tr>\n<tr>\n<td><code>ALGORITHM_SLIDING_COUNTER</code></td>\n<td>Medium</td>\n<td>Multiple bucket keys, time-based logic</td>\n<td>Check second</td>\n</tr>\n<tr>\n<td><code>ALGORITHM_SLIDING_WINDOW_LOG</code></td>\n<td>High</td>\n<td>List operations, timestamp management</td>\n<td>Check last</td>\n</tr>\n</tbody></table>\n<p><strong>Sequential evaluation</strong> implements the short-circuit logic with careful attention to atomicity and consistency. Each rule evaluation calls the <code>CheckAndUpdate</code> method on the appropriate algorithm implementation, passing the constructed Redis key and required token count. The evaluation immediately terminates on the first rule that returns <code>allowed: false</code>, capturing the specific failure details for client feedback.</p>\n<p><strong>Result aggregation</strong> becomes necessary only when all individual rule checks succeed. The aggregation process identifies the most restrictive remaining capacity across all checked rules, calculates the earliest reset time when any limit will refresh, and determines the appropriate retry-after value for rate limit headers.</p>\n<h4 id=\"common-pitfalls-in-tier-evaluation\">Common Pitfalls in Tier Evaluation</h4>\n<p>⚠️ <strong>Pitfall: Evaluating All Tiers Even After Failure</strong></p>\n<p>A common mistake in multi-tier rate limiting implementations is continuing to evaluate all applicable rules even after one has already denied the request. This occurs when developers implement rule checking as a validation pipeline that collects all failures rather than a gate system that stops at the first barrier.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>// WRONG: Evaluating all rules regardless of failures\nresults := make([]*RateLimitResult, 0)\nfor _, rule := range applicableRules {\n    result := evaluateRule(rule, request)  \n    results = append(results, result)\n}\n// Then checking if any failed...</code></pre></div>\n\n<p>This approach wastes Redis operations, increases response latency, and provides confusing user feedback since the client receives information about multiple limit violations when only the first one is actionable. The correct approach uses immediate return on first failure:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>// CORRECT: Short-circuit evaluation\nfor _, rule := range applicableRules {\n    result := evaluateRule(rule, request)\n    if !result.allowed {\n        return result  // Immediate failure, no further evaluation\n    }\n}</code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Inconsistent Rule Priority Handling</strong></p>\n<p>Another frequent error involves inconsistent interpretation of priority values across different parts of the system. Some implementations treat higher numeric values as higher priority while others use the reverse convention, leading to rules being evaluated in the wrong order.</p>\n<p>The problem becomes especially acute when priority ties need resolution - without consistent tie-breaking logic, the same request might be evaluated differently across application instances, leading to unpredictable rate limiting behavior. Always use explicit priority ordering with well-defined tie-breaking rules based on secondary criteria like rule creation time or alphabetical rule ID ordering.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Rule Pattern Overlap</strong></p>\n<p>Overlapping rule patterns can create unexpected interactions where multiple rules apply to the same request dimension but specify conflicting limits. For example, a general rule <code>user:*:api:*</code> allowing 1000 requests per hour might conflict with a specific rule <code>user:premium_*:api:orders</code> allowing 5000 requests per hour for premium users accessing the orders endpoint.</p>\n<p>Without proper conflict resolution, the system might apply the more restrictive limit even when a more specific, permissive rule should take precedence. Always implement explicit precedence rules that favor more specific patterns and higher priority values when resolving overlaps.</p>\n<h3 id=\"rate-limit-key-composition\">Rate Limit Key Composition</h3>\n<p>The foundation of effective multi-tier rate limiting lies in <strong>Redis key composition</strong> - the systematic construction of unique identifiers that organize rate limit state across different dimensions, time windows, and algorithms. Key composition determines not only how rate limiting data is stored and retrieved, but also how efficiently the system can scale, how clearly administrators can debug issues, and how reliably the system maintains consistency under load.</p>\n<p>Effective key composition balances several competing requirements: keys must be <strong>unique</strong> to prevent collisions between different rate limits, <strong>predictable</strong> to enable debugging and monitoring, <strong>efficient</strong> to minimize Redis memory usage, and <strong>hierarchical</strong> to support operations like bulk deletion or pattern-based queries. The key structure also influences Redis clustering behavior, as keys determine which Redis node stores each rate limit counter.</p>\n<p>The <strong>base key structure</strong> follows a hierarchical pattern that encodes the rate limit dimension, resource identifier, time window, and algorithm type:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ratelimit:{dimension}:{resource}:{algorithm}:{window}:{additional_context}</code></pre></div>\n\n<p>This structure provides natural groupings that align with Redis operations and administrative needs. The <code>ratelimit:</code> prefix enables easy identification of rate limiting keys in mixed-use Redis instances. The dimension component (<code>user</code>, <code>ip</code>, <code>api</code>, <code>global</code>) creates logical separation between different rate limit types. The resource identifier specifies the exact entity being limited, while algorithm and window components ensure that different rate limiting approaches for the same resource don&#39;t interfere.</p>\n<p><strong>User-scoped rate limit keys</strong> incorporate the user identifier and any relevant context that affects the user&#39;s rate limit tier:</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Example</th>\n<th>Use Case</th>\n<th>Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ratelimit:user:{user_id}:token_bucket:{window}</code></td>\n<td><code>ratelimit:user:user123:token_bucket:3600</code></td>\n<td>Per-user global limits</td>\n<td>Simple, efficient, scales linearly</td>\n</tr>\n<tr>\n<td><code>ratelimit:user:{user_id}:api:{endpoint}:sliding_counter:{window}</code></td>\n<td><code>ratelimit:user:user123:api:orders:sliding_counter:3600</code></td>\n<td>Per-user API endpoint limits</td>\n<td>More granular, higher memory usage</td>\n</tr>\n<tr>\n<td><code>ratelimit:user:{user_tier}:{user_id}:global:token_bucket:{window}</code></td>\n<td><code>ratelimit:user:premium:user123:global:token_bucket:3600</code></td>\n<td>Tier-aware user limits</td>\n<td>Enables tier-specific limit policies</td>\n</tr>\n</tbody></table>\n<p>The user ID component typically uses the application&#39;s native user identifier (database primary key, UUID, or username) rather than derived values, ensuring consistency across application restarts and reducing the likelihood of key collisions during user management operations.</p>\n<p><strong>IP-scoped rate limit keys</strong> must handle the complexities of network addressing, proxy scenarios, and IPv4/IPv6 compatibility:</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Example</th>\n<th>Use Case</th>\n<th>Special Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ratelimit:ip:{ip_address}:sliding_log:{window}</code></td>\n<td><code>ratelimit:ip:192.168.1.10:sliding_log:3600</code></td>\n<td>Direct IP rate limiting</td>\n<td>IPv6 colon escaping required</td>\n</tr>\n<tr>\n<td><code>ratelimit:ip:{ip_subnet}:token_bucket:{window}</code></td>\n<td><code>ratelimit:ip:192.168.1.0_24:token_bucket:3600</code></td>\n<td>Subnet-based rate limiting</td>\n<td>Subnet calculation for each request</td>\n</tr>\n<tr>\n<td><code>ratelimit:ip:{ip_hash}:api:{endpoint}:sliding_counter:{window}</code></td>\n<td><code>ratelimit:ip:a1b2c3d4:api:upload:sliding_counter:300</code></td>\n<td>Privacy-preserving IP limits</td>\n<td>Hash consistency across instances</td>\n</tr>\n</tbody></table>\n<p>IP address handling requires careful consideration of IPv6 compatibility, as IPv6 addresses contain colons that conflict with Redis key delimiters. The system typically normalizes IPv6 addresses to a canonical form and replaces colons with underscores or uses base64 encoding for the address component.</p>\n<p><strong>API-scoped rate limit keys</strong> organize limits by endpoint, HTTP method, and resource type to provide fine-grained control over different API operations:</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Example</th>\n<th>Use Case</th>\n<th>Granularity Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ratelimit:api:{endpoint}:sliding_counter:{window}</code></td>\n<td><code>ratelimit:api:orders:sliding_counter:3600</code></td>\n<td>Endpoint-level rate limiting</td>\n<td>Coarse-grained, efficient</td>\n</tr>\n<tr>\n<td><code>ratelimit:api:{method}:{endpoint}:token_bucket:{window}</code></td>\n<td><code>ratelimit:api:POST:orders:token_bucket:3600</code></td>\n<td>Method-specific rate limiting</td>\n<td>Distinguishes read/write operations</td>\n</tr>\n<tr>\n<td><code>ratelimit:api:{service}:{version}:{endpoint}:sliding_log:{window}</code></td>\n<td><code>ratelimit:api:orders:v2:create:sliding_log:300</code></td>\n<td>Versioned API rate limiting</td>\n<td>Supports API evolution and migration</td>\n</tr>\n</tbody></table>\n<p>API endpoint normalization presents significant challenges in key composition, as URLs may contain variable path parameters, query strings, and other dynamic elements. The system typically applies endpoint normalization rules that replace path parameters with placeholders (<code>/orders/123</code> becomes <code>/orders/{id}</code>) and ignore query parameters unless they&#39;re explicitly included in the rate limiting policy.</p>\n<p><strong>Global rate limit keys</strong> aggregate usage across all users, IPs, and endpoints to enforce system-wide capacity constraints:</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Example</th>\n<th>Use Case</th>\n<th>Aggregation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ratelimit:global:requests:sliding_counter:{window}</code></td>\n<td><code>ratelimit:global:requests:sliding_counter:60</code></td>\n<td>Total request rate limiting</td>\n<td>Simple counter increment</td>\n</tr>\n<tr>\n<td><code>ratelimit:global:{resource_type}:token_bucket:{window}</code></td>\n<td><code>ratelimit:global:database_writes:token_bucket:3600</code></td>\n<td>Resource-specific global limits</td>\n<td>Categorized request counting</td>\n</tr>\n<tr>\n<td><code>ratelimit:global:{region}:api:{endpoint}:sliding_log:{window}</code></td>\n<td><code>ratelimit:global:us-west:api:search:sliding_log:300</code></td>\n<td>Regional global rate limiting</td>\n<td>Geographic request distribution</td>\n</tr>\n</tbody></table>\n<p>Global rate limits present unique challenges in distributed systems, as they require coordination across all application instances and potentially multiple Redis nodes. The key composition must support efficient aggregation while avoiding hot-spotting on a single Redis key.</p>\n<p><strong>Time window encoding</strong> within rate limit keys determines how algorithm implementations track temporal boundaries and handle window transitions:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm Type</th>\n<th>Time Window Encoding</th>\n<th>Example Component</th>\n<th>Window Alignment</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ALGORITHM_TOKEN_BUCKET</code></td>\n<td>Window duration in seconds</td>\n<td><code>:3600</code></td>\n<td>Floating window based on first request</td>\n</tr>\n<tr>\n<td><code>ALGORITHM_SLIDING_COUNTER</code></td>\n<td>Current time bucket + duration</td>\n<td><code>:1609459200:3600</code></td>\n<td>Fixed time bucket boundaries</td>\n</tr>\n<tr>\n<td><code>ALGORITHM_SLIDING_WINDOW_LOG</code></td>\n<td>Window duration only</td>\n<td><code>:3600</code></td>\n<td>Sliding based on request timestamps</td>\n</tr>\n</tbody></table>\n<p>The time window encoding affects both key uniqueness and Redis memory usage patterns. Sliding window counter algorithms generate multiple keys per time window (one per sub-bucket), while token bucket algorithms typically use a single key with embedded timestamp data.</p>\n<p><strong>Redis cluster considerations</strong> influence key composition decisions to ensure optimal data distribution and avoid hot-spotting. The Redis cluster uses CRC16 hashing of the key to determine node assignment, meaning that keys with similar prefixes may cluster on the same node:</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Effective key composition serves as the addressing system for distributed rate limiting state, determining not just where data lives but how efficiently it can be accessed, updated, and managed. The key structure becomes the foundation for Redis clustering, monitoring queries, and administrative operations.</p>\n</blockquote>\n<h4 id=\"key-composition-implementation-strategy\">Key Composition Implementation Strategy</h4>\n<p>The <code>RateLimitRule</code> structure includes a <code>key_pattern</code> field that serves as a template for generating actual Redis keys based on request context. The pattern uses placeholder syntax to inject dynamic values:</p>\n<table>\n<thead>\n<tr>\n<th>Pattern Component</th>\n<th>Placeholder Syntax</th>\n<th>Example Pattern</th>\n<th>Generated Key</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>User ID</td>\n<td><code>{user_id}</code></td>\n<td><code>ratelimit:user:{user_id}:api:orders</code></td>\n<td><code>ratelimit:user:user123:api:orders</code></td>\n</tr>\n<tr>\n<td>IP Address</td>\n<td><code>{ip_address}</code></td>\n<td><code>ratelimit:ip:{ip_address}:global</code></td>\n<td><code>ratelimit:ip:192.168.1.10:global</code></td>\n</tr>\n<tr>\n<td>API Endpoint</td>\n<td><code>{api_endpoint}</code></td>\n<td><code>ratelimit:api:{api_endpoint}:sliding_counter</code></td>\n<td><code>ratelimit:api:orders:sliding_counter</code></td>\n</tr>\n<tr>\n<td>Algorithm Type</td>\n<td><code>{algorithm}</code></td>\n<td><code>ratelimit:user:{user_id}:{algorithm}</code></td>\n<td><code>ratelimit:user:user123:token_bucket</code></td>\n</tr>\n<tr>\n<td>Time Window</td>\n<td><code>{window}</code></td>\n<td><code>ratelimit:global:requests:{window}</code></td>\n<td><code>ratelimit:global:requests:3600</code></td>\n</tr>\n</tbody></table>\n<p>The key composition system includes <strong>validation logic</strong> to ensure generated keys meet Redis requirements and avoid common pitfalls:</p>\n<ul>\n<li><strong>Length validation</strong>: Redis keys are limited to 512 MB, but practical considerations limit keys to under 250 characters for optimal performance</li>\n<li><strong>Character validation</strong>: Keys avoid problematic characters like spaces, newlines, and Redis command separators</li>\n<li><strong>Collision detection</strong>: The system validates that different rule patterns cannot generate identical keys under normal operation</li>\n<li><strong>Cluster compatibility</strong>: Keys are structured to distribute evenly across Redis cluster nodes</li>\n</ul>\n<p><strong>Key lifecycle management</strong> handles the creation, updates, and cleanup of rate limiting keys as rules change and time windows expire:</p>\n<ol>\n<li><strong>Key creation</strong> occurs dynamically when the first request matching a rule arrives, with initial values set according to the algorithm type</li>\n<li><strong>Key updates</strong> happen atomically through Lua scripts that ensure consistency during concurrent access</li>\n<li><strong>Key expiration</strong> uses Redis TTL mechanisms to automatically clean up expired time windows and reduce memory usage</li>\n<li><strong>Key migration</strong> handles rule changes that affect key patterns, typically through background processes that move data to new key structures</li>\n</ol>\n<p>The key composition strategy also supports <strong>administrative operations</strong> like bulk key deletion, usage reporting, and debugging through predictable key patterns that enable efficient Redis pattern matching and iteration.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fdata-model.svg\" alt=\"Data Model Relationships\"></p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Frate-check-sequence.svg\" alt=\"Rate Limit Check Sequence\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Multi-tier rate limiting requires careful orchestration of rule management, key composition, and evaluation logic to achieve both correctness and performance. The implementation bridges the conceptual design with practical Redis operations while handling the complexities of distributed coordination and failure scenarios.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Storage</td>\n<td>YAML configuration files with file watching</td>\n<td>Redis-based rule storage with pub/sub updates</td>\n</tr>\n<tr>\n<td>Pattern Matching</td>\n<td>Simple string templates with Go text/template</td>\n<td>Regex-based patterns with named capture groups</td>\n</tr>\n<tr>\n<td>Key Composition</td>\n<td>String concatenation with validation</td>\n<td>Template engine with type-safe placeholders</td>\n</tr>\n<tr>\n<td>Tier Coordination</td>\n<td>Sequential rule evaluation with early termination</td>\n<td>Parallel rule evaluation with context cancellation</td>\n</tr>\n<tr>\n<td>Result Caching</td>\n<td>In-memory LRU cache with TTL</td>\n<td>Redis-based shared cache across instances</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  limiter/\n    multi_tier.go           ← Main multi-tier limiter implementation\n    rule_manager.go         ← Rule loading and matching logic\n    key_composer.go         ← Redis key composition utilities\n    tier_evaluator.go       ← Sequential evaluation with short-circuiting\n    result_aggregator.go    ← Success result combination logic\n  config/\n    rules.yaml             ← Rate limit rule definitions\n    rule_loader.go         ← YAML rule parsing and validation\n    rule_watcher.go        ← File change detection for rule updates\n  storage/\n    redis_operations.go    ← Atomic Redis operations for rule evaluation</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Rule Configuration Structure (config/rules.yaml):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Complete working rule configuration</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">rules</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"user_global_limit\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Per-user global rate limit\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    key_pattern</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ratelimit:user:{user_id}:global:token_bucket:{window}\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    algorithm</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"token_bucket\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1000</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    window</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1h\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    burst_limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1200</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    priority</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    enabled</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ip_burst_protection\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Per-IP burst protection\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    key_pattern</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ratelimit:ip:{ip_address}:burst:sliding_counter:{window}\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    algorithm</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"sliding_window_counter\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    window</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1m\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    priority</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">200</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    enabled</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"api_endpoint_limit\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Per-API endpoint rate limit\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    key_pattern</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ratelimit:api:{api_endpoint}:requests:sliding_log:{window}\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    algorithm</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"sliding_window_log\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">10000</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    window</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1h\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    priority</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">50</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    enabled</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span></span></code></pre></div>\n\n<p><strong>Rule Loader Implementation (config/rule_loader.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">gopkg.in/yaml.v2</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Complete rule loading with validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RuleConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Rules []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#9ECBFF\"> `yaml:\"rules\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadRules loads and validates rate limit rules from YAML configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(configPath)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read rule config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">RuleConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> yaml.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse rule config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Validate and normalize rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, rule </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> config.Rules {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> validateRule</span><span style=\"color:#E1E4E8\">(rule); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid rule </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, rule.ID, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Parse window duration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> duration, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">ParseDuration</span><span style=\"color:#E1E4E8\">(rule.WindowStr); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid window duration for rule </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, rule.ID, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            rule.Window </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Set timestamps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rule.CreatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rule.UpdatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config.Rules, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> validateRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> rule.ID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"rule ID is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> rule.KeyPattern </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key pattern is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> rule.Limit </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"limit must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Validate algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validAlgorithms </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ALGORITHM_TOKEN_BUCKET:         </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ALGORITHM_SLIDING_WINDOW_LOG:   </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ALGORITHM_SLIDING_COUNTER:      </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">validAlgorithms[rule.Algorithm] {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unsupported algorithm: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, rule.Algorithm)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Key Composer Implementation (limiter/key_composer.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> limiter</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// KeyComposer handles Redis key generation from rule patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> KeyComposer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    patternCache </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Regexp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewKeyComposer creates a key composer with pattern caching</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewKeyComposer</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">KeyComposer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">KeyComposer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        patternCache: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Regexp</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ComposeKey generates Redis key from rule pattern and request context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">kc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">KeyComposer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ComposeKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rule.KeyPattern</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Replace standard placeholders</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    replacements </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"{user_id}\"</span><span style=\"color:#E1E4E8\">:      </span><span style=\"color:#B392F0\">normalizeUserID</span><span style=\"color:#E1E4E8\">(req.UserID),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"{ip_address}\"</span><span style=\"color:#E1E4E8\">:   </span><span style=\"color:#B392F0\">normalizeIPAddress</span><span style=\"color:#E1E4E8\">(req.IPAddress),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"{api_endpoint}\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#B392F0\">normalizeAPIEndpoint</span><span style=\"color:#E1E4E8\">(req.APIEndpoint),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"{algorithm}\"</span><span style=\"color:#E1E4E8\">:    rule.Algorithm,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"{window}\"</span><span style=\"color:#E1E4E8\">:       fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(rule.Window.</span><span style=\"color:#B392F0\">Seconds</span><span style=\"color:#E1E4E8\">())),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> placeholder, value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> replacements {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">ReplaceAll</span><span style=\"color:#E1E4E8\">(key, placeholder, value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Validate final key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> validateRedisKey</span><span style=\"color:#E1E4E8\">(key); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid Redis key generated: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> key, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Utility functions for key normalization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> normalizeUserID</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">userID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> userID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"anonymous\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Remove problematic characters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalized </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> regexp.</span><span style=\"color:#B392F0\">MustCompile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`[^a-zA-Z0-9_-]`</span><span style=\"color:#E1E4E8\">).</span><span style=\"color:#B392F0\">ReplaceAllString</span><span style=\"color:#E1E4E8\">(userID, </span><span style=\"color:#9ECBFF\">\"_\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(normalized) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalized[:</span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> normalized</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> normalizeIPAddress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ipAddr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ip </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> net.</span><span style=\"color:#B392F0\">ParseIP</span><span style=\"color:#E1E4E8\">(ipAddr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> ip </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"invalid_ip\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Handle IPv6 - replace colons with underscores</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(ipAddr, </span><span style=\"color:#9ECBFF\">\":\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">ReplaceAll</span><span style=\"color:#E1E4E8\">(ip.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#9ECBFF\">\":\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"_\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ip.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> normalizeAPIEndpoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">endpoint</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Remove leading slash</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalized </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">TrimPrefix</span><span style=\"color:#E1E4E8\">(endpoint, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Replace path separators with underscores</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">ReplaceAll</span><span style=\"color:#E1E4E8\">(normalized, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"_\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Replace query parameters and fragments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> idx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Index</span><span style=\"color:#E1E4E8\">(normalized, </span><span style=\"color:#9ECBFF\">\"?\"</span><span style=\"color:#E1E4E8\">); idx </span><span style=\"color:#F97583\">!=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalized[:idx]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> idx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Index</span><span style=\"color:#E1E4E8\">(normalized, </span><span style=\"color:#9ECBFF\">\"#\"</span><span style=\"color:#E1E4E8\">); idx </span><span style=\"color:#F97583\">!=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalized[:idx]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Limit length</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(normalized) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalized[:</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> normalized</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> validateRedisKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(key) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key cannot be empty\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(key) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 250</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key too long: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> characters\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(key))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check for problematic characters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">ContainsAny</span><span style=\"color:#E1E4E8\">(key, </span><span style=\"color:#9ECBFF\">\" </span><span style=\"color:#79B8FF\">\\n\\r\\t</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key contains whitespace characters\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Multi-Tier Limiter Implementation (limiter/multi_tier.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> limiter</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MultiTierLimiter implements hierarchical rate limiting across multiple dimensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MultiTierLimiter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage      </span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keyComposer  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">KeyComposer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    algorithms   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localFallback </span><span style=\"color:#B392F0\">Limiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMultiTierLimiter creates a multi-tier rate limiter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMultiTierLimiter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> Storage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ruleManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultiTierLimiter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MultiTierLimiter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:     storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ruleManager: ruleManager,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        keyComposer: </span><span style=\"color:#B392F0\">NewKeyComposer</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        algorithms:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Algorithm</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check performs multi-tier rate limit evaluation with short-circuit logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mtl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultiTierLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get all applicable rules for this request using ruleManager.GetMatchingRules()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Pass req.UserID, req.IPAddress, req.APIEndpoint to find matching patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Sort rules by priority (higher priority first) and algorithm cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sort.Slice with custom comparison function checking rule.Priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Evaluate each rule in sequence with short-circuit on first failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Call evaluateRule() for each rule, return immediately if result.Allowed == false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If all rules pass, aggregate the most restrictive limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Find minimum remaining capacity and earliest reset time across all results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle Redis failures by falling back to local rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check error types and call mtl.localFallback.Check() on storage errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evaluateRule performs rate limit check for a single rule</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mtl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultiTierLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluateRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate Redis key using keyComposer.ComposeKey(rule, req)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get the appropriate algorithm implementation for rule.Algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Look up in mtl.algorithms map, return error if not found</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call algorithm.Check() with the generated key and required tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use req.Tokens for token count, handle context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Populate result with rule metadata (rule ID, algorithm type)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply burst limit adjustments if rule has burst_limit configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: For token bucket, allow temporary exceeding of base limit up to burst_limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Preview checks rate limit status without updating counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mtl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultiTierLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Preview</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Similar to Check() but call Preview() on individual algorithms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return aggregate status without modifying any counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Rule Manager Implementation (limiter/rule_manager.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> limiter</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RuleManager handles rule storage, matching, and updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RuleManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rules       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    userIndex   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ipIndex     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    apiIndex    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    globalRules []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRuleManager creates a rule manager with indexing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRuleManager</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rules:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        userIndex: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ipIndex:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        apiIndex:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetMatchingRules returns all rules applicable to the given request context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetMatchingRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">userID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ipAddress</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">apiEndpoint</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rm.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rm.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check user-specific rules by looking up userID in userIndex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check IP-specific rules by looking up ipAddress in ipIndex  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check API-specific rules by looking up apiEndpoint in apiIndex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Always include global rules from globalRules slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Deduplicate rules that match multiple patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Filter out disabled rules (rule.Enabled == false)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use map[string]bool to track seen rule IDs and avoid duplicates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadRules loads rules from configuration and rebuilds indices</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LoadRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rm.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rm.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Call config.LoadRules() to parse YAML configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Clear existing rules and indices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Rebuild rule indices by pattern type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate that no two rules generate conflicting Redis keys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// buildIndices creates lookup indices for efficient rule matching</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">buildIndices</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Clear all existing indices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through all rules and categorize by key pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Rules with {user_id} patterns go into userIndex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Rules with {ip_address} patterns go into ipIndex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Rules with {api_endpoint} patterns go into apiIndex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Rules matching all requests go into globalRules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use strings.Contains() to detect pattern placeholders</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing multi-tier rate limiting, verify the following behaviors:</p>\n<p><strong>Test Command</strong>: </p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/limiter/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestMultiTier</span></span></code></pre></div>\n\n<p><strong>Expected Behavior</strong>:</p>\n<ol>\n<li><strong>Rule Loading</strong>: Configuration loads successfully with validation errors for malformed rules</li>\n<li><strong>Pattern Matching</strong>: Rules correctly match requests based on user ID, IP address, and API endpoint patterns</li>\n<li><strong>Short-Circuit Evaluation</strong>: Evaluation stops immediately when the first rule denies a request</li>\n<li><strong>Priority Ordering</strong>: Higher priority rules are evaluated before lower priority rules</li>\n<li><strong>Result Aggregation</strong>: Successful requests return the most restrictive remaining capacity across all checked rules</li>\n</ol>\n<p><strong>Manual Testing</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test per-user rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-User-ID: testuser\"</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/orders</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should show X-RateLimit-Remaining header</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test IP rate limiting  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..10}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span><span style=\"color:#B392F0\"> curl</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-Forwarded-For: 192.168.1.100\"</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/search</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should eventually return 429 Too Many Requests</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test API endpoint limits</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/upload</span><span style=\"color:#6A737D\">  # Different limits than /api/orders</span></span></code></pre></div>\n\n<p><strong>Signs of Issues</strong>:</p>\n<ul>\n<li>Rules not matching expected requests → Check pattern normalization logic</li>\n<li>All rules being evaluated despite failures → Verify short-circuit implementation  </li>\n<li>Inconsistent rate limiting across instances → Check Redis key composition</li>\n<li>Poor performance under load → Profile rule matching and Redis operations</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rules not triggering</td>\n<td>Pattern matching failure</td>\n<td>Check generated Redis keys in logs</td>\n<td>Fix key normalization functions</td>\n</tr>\n<tr>\n<td>Inconsistent rate limiting</td>\n<td>Rule priority conflicts</td>\n<td>Review rule evaluation order</td>\n<td>Implement explicit priority tie-breaking</td>\n</tr>\n<tr>\n<td>High Redis CPU usage</td>\n<td>Inefficient Lua scripts</td>\n<td>Monitor Redis SLOWLOG</td>\n<td>Optimize atomic operations</td>\n</tr>\n<tr>\n<td>Memory leaks</td>\n<td>Key expiration not working</td>\n<td>Check Redis key TTLs</td>\n<td>Implement proper cleanup</td>\n</tr>\n</tbody></table>\n<h2 id=\"redis-backend-integration\">Redis Backend Integration</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 - Redis Backend Integration</p>\n</blockquote>\n<p>The transition from local rate limiting to distributed rate limiting represents one of the most critical architectural decisions in building scalable systems. While local rate limiting provides excellent performance and simplicity, it falls apart the moment you deploy multiple application instances. Each instance operates with its own view of request counts, leading to effective limits that are N times higher than intended, where N is the number of instances. Redis backend integration solves this fundamental problem by providing a shared state store that all application instances can access atomically, ensuring that rate limits are enforced accurately across the entire cluster.</p>\n<h3 id=\"mental-model-bank-transaction-processing\">Mental Model: Bank Transaction Processing</h3>\n<p>Understanding atomic check-and-update operations in distributed rate limiting becomes much clearer when we think about how banks handle account transactions. Consider what happens when you try to withdraw money from an ATM. The bank doesn&#39;t simply check your balance and then subtract the withdrawal amount in two separate operations—that would create a race condition where multiple ATMs could simultaneously check the same balance and approve withdrawals that collectively exceed your available funds.</p>\n<p>Instead, banks use atomic transactions that combine the balance check and deduction into a single, indivisible operation. The ATM sends a request to the bank&#39;s central system saying &quot;if account 12345 has at least $100, subtract $100 and tell me the new balance.&quot; This atomic check-and-update ensures that no matter how many ATMs are processing transactions simultaneously, the account balance remains consistent and never goes below zero (assuming no overdraft protection).</p>\n<p>Rate limiting with Redis follows exactly the same pattern. When an application instance wants to allow a request, it can&#39;t simply check the current count in Redis and then increment it in a separate operation. Between the check and the increment, another instance might have processed requests that push the count over the limit. Instead, we need atomic check-and-update operations that say &quot;if the current request count is below the limit, increment the count and tell me if the request is allowed.&quot;</p>\n<p>This banking analogy extends to other aspects of our Redis integration. Just as banks have redundant systems and backup procedures for when the main transaction system fails, our rate limiter needs graceful degradation strategies when Redis becomes unavailable. And just as banks use connection pooling to efficiently handle thousands of simultaneous ATM transactions, our Redis integration uses connection pooling to manage the network resources efficiently across multiple application instances.</p>\n<p>The key insight here is that distributed rate limiting isn&#39;t just about storing counters in a shared location—it&#39;s about ensuring that the fundamental check-and-update operation remains atomic even when performed by multiple processes across a network. This atomicity requirement drives every aspect of our Redis integration design, from Lua script implementation to connection management strategies.</p>\n<h3 id=\"lua-script-design\">Lua Script Design</h3>\n<p>Redis Lua scripts provide the atomicity guarantees we need for distributed rate limiting by ensuring that complex multi-step operations execute without interruption from other clients. Unlike regular Redis commands that might be interleaved with operations from other connections, Lua scripts run atomically within Redis, providing the equivalent of database transactions for our rate limiting logic.</p>\n<p>The fundamental challenge in implementing rate limiting algorithms as Lua scripts lies in translating the conceptual algorithms we designed earlier into Redis operations that efficiently manipulate the underlying data structures. Each algorithm has different storage requirements and update patterns, requiring carefully crafted scripts that balance correctness, performance, and memory usage.</p>\n<p><strong>Token Bucket Lua Script Design</strong></p>\n<p>The token bucket algorithm requires maintaining two pieces of state: the current token count and the timestamp of the last refill operation. Our Lua script must atomically read these values, calculate how many tokens should be added based on elapsed time, update the bucket state, and determine whether the request can be allowed.</p>\n<table>\n<thead>\n<tr>\n<th>Script Operation</th>\n<th>Redis Commands Used</th>\n<th>Purpose</th>\n<th>Complexity Consideration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Read current state</td>\n<td><code>HMGET key tokens last_refill</code></td>\n<td>Retrieve bucket state</td>\n<td>Single round-trip, O(1)</td>\n</tr>\n<tr>\n<td>Calculate elapsed time</td>\n<td>Lua math operations</td>\n<td>Determine refill amount</td>\n<td>Time precision handling</td>\n</tr>\n<tr>\n<td>Refill tokens</td>\n<td>Lua math, bounded by capacity</td>\n<td>Update token count</td>\n<td>Prevent integer overflow</td>\n</tr>\n<tr>\n<td>Check allowance</td>\n<td>Compare tokens to request</td>\n<td>Rate limit decision</td>\n<td>Handle burst scenarios</td>\n</tr>\n<tr>\n<td>Update state</td>\n<td><code>HMSET key tokens new_tokens last_refill now</code></td>\n<td>Persist new state</td>\n<td>Atomic state update</td>\n</tr>\n<tr>\n<td>Set expiration</td>\n<td><code>EXPIRE key window_seconds</code></td>\n<td>Cleanup old buckets</td>\n<td>Memory management</td>\n</tr>\n</tbody></table>\n<p>The token bucket script must handle several edge cases that make the implementation more complex than a simple counter increment. Time calculations require careful handling of integer precision to avoid drift over long periods. The refill calculation must prevent token counts from exceeding the bucket capacity while handling cases where the elapsed time is so large that multiple full refills should have occurred.</p>\n<p><strong>Sliding Window Counter Lua Script Design</strong></p>\n<p>Sliding window counter scripts operate on a hash structure where each field represents a time bucket and its value contains the request count for that bucket. The script must determine the current bucket, increment the appropriate counter, calculate the total count across all buckets within the window, and clean up expired buckets to prevent unbounded memory growth.</p>\n<table>\n<thead>\n<tr>\n<th>Script Phase</th>\n<th>Operations</th>\n<th>Redis Commands</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bucket identification</td>\n<td>Calculate current bucket ID from timestamp</td>\n<td>Lua math operations</td>\n<td>Handle clock skew</td>\n</tr>\n<tr>\n<td>Current bucket update</td>\n<td>Increment counter for current bucket</td>\n<td><code>HINCRBY key bucket_id tokens</code></td>\n<td>Initialize if missing</td>\n</tr>\n<tr>\n<td>Window calculation</td>\n<td>Sum counts across active buckets</td>\n<td><code>HMGET key bucket1 bucket2 ...</code></td>\n<td>Handle missing buckets</td>\n</tr>\n<tr>\n<td>Expired cleanup</td>\n<td>Remove buckets outside window</td>\n<td><code>HDEL key expired_bucket1 ...</code></td>\n<td>Batch deletions</td>\n</tr>\n<tr>\n<td>Rate limit decision</td>\n<td>Compare total to limit</td>\n<td>Lua comparison</td>\n<td>Return detailed result</td>\n</tr>\n<tr>\n<td>Expiration management</td>\n<td>Set key TTL based on window</td>\n<td><code>EXPIRE key ttl_seconds</code></td>\n<td>Prevent memory leaks</td>\n</tr>\n</tbody></table>\n<p>The sliding window counter script faces the challenge of maintaining accuracy while managing memory efficiently. The number of buckets affects both accuracy and memory usage—more buckets provide smoother rate limiting but require more Redis memory and script execution time. The script must balance these trade-offs while ensuring that bucket cleanup doesn&#39;t interfere with active rate limiting decisions.</p>\n<p><strong>Sliding Window Log Lua Script Design</strong></p>\n<p>The sliding window log approach stores individual request timestamps, providing the highest accuracy at the cost of memory usage proportional to the request rate. The Lua script must add new timestamps to a Redis list or sorted set, remove expired timestamps, count remaining timestamps, and make the rate limiting decision—all atomically.</p>\n<table>\n<thead>\n<tr>\n<th>Implementation Choice</th>\n<th>Redis Structure</th>\n<th>Script Operations</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sorted Set approach</td>\n<td><code>ZADD key timestamp uuid</code></td>\n<td>Add, remove by score, count</td>\n<td>Memory efficient, complex cleanup</td>\n</tr>\n<tr>\n<td>List approach</td>\n<td><code>LPUSH key timestamp</code></td>\n<td>Push, trim, count</td>\n<td>Simple operations, less precise cleanup</td>\n</tr>\n<tr>\n<td>Hybrid approach</td>\n<td>Sorted set with periodic cleanup</td>\n<td>Add new, batch remove old</td>\n<td>Best of both, complex logic</td>\n</tr>\n</tbody></table>\n<p>The sorted set implementation provides the most precise sliding window behavior because it can efficiently remove timestamps that fall outside the window using <code>ZREMRANGEBYSCORE</code>. However, it requires generating unique scores for timestamps that might be identical, typically by appending a random component or sequence number.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The choice between Redis data structures for sliding window log has profound implications for memory usage patterns. Sorted sets provide O(log N) operations but require 16 bytes overhead per entry, while lists provide O(1) append but only O(N) cleanup. For high-traffic keys, this memory difference can determine whether the rate limiter scales economically.</p>\n</blockquote>\n<p><strong>Script Error Handling and Return Values</strong></p>\n<p>All Lua scripts must return consistent, structured results that allow the calling application to make informed decisions about request handling and error recovery. The script return format needs to convey not just whether the request is allowed, but also the current state information needed for rate limit headers and monitoring.</p>\n<table>\n<thead>\n<tr>\n<th>Return Field</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>allowed</code></td>\n<td>Boolean</td>\n<td>Whether request should proceed</td>\n<td><code>1</code> (allowed) or <code>0</code> (denied)</td>\n</tr>\n<tr>\n<td><code>remaining</code></td>\n<td>Integer</td>\n<td>Tokens/requests remaining in window</td>\n<td><code>45</code></td>\n</tr>\n<tr>\n<td><code>reset_time</code></td>\n<td>Integer</td>\n<td>Unix timestamp when limit resets</td>\n<td><code>1699123456</code></td>\n</tr>\n<tr>\n<td><code>retry_after</code></td>\n<td>Integer</td>\n<td>Seconds to wait before retrying</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td><code>current_count</code></td>\n<td>Integer</td>\n<td>Current usage within window</td>\n<td><code>55</code></td>\n</tr>\n<tr>\n<td><code>error</code></td>\n<td>String</td>\n<td>Error message if script failed</td>\n<td><code>nil</code> or error description</td>\n</tr>\n</tbody></table>\n<p>Script error handling must distinguish between Redis operational errors (like out of memory) and rate limiting logic errors (like invalid parameters). Operational errors should typically cause the script to return an error result, triggering fallback behavior in the application. Logic errors should return a deny result to fail safely.</p>\n<blockquote>\n<p><strong>Architecture Decision: Lua Script Deployment Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Lua scripts can be embedded in application code or loaded into Redis once and called by SHA hash</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Embed scripts in application code and use <code>EVAL</code> for each call</li>\n<li>Load scripts at startup using <code>SCRIPT LOAD</code> and call via <code>EVALSHA</code></li>\n<li>Hybrid approach with fallback from <code>EVALSHA</code> to <code>EVAL</code> if script not cached</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use hybrid approach with <code>EVALSHA</code> primary and <code>EVAL</code> fallback</li>\n<li><strong>Rationale</strong>: Provides performance benefits of cached scripts while handling Redis restarts gracefully. Network bandwidth savings significant for complex scripts.</li>\n<li><strong>Consequences</strong>: Requires script loading logic at startup and error handling for cache misses, but eliminates script transmission overhead for normal operations</li>\n</ul>\n</blockquote>\n<h3 id=\"connection-pool-management\">Connection Pool Management</h3>\n<p>Effective connection pool management forms the backbone of reliable Redis integration, determining both the performance characteristics and failure resilience of the distributed rate limiter. Unlike simple client-server applications where connection management can be relatively straightforward, a distributed rate limiter must handle high concurrency, varying load patterns, and network failures while maintaining low latency for every rate limit decision.</p>\n<p>The fundamental challenge lies in balancing connection resource usage against performance and reliability requirements. Too few connections create bottlenecks during traffic spikes, leading to increased latency and potential timeouts. Too many connections waste system resources and can overwhelm Redis servers. The optimal pool size depends on factors that change dynamically: request volume, Redis response times, network latency, and the concurrency patterns of the application using the rate limiter.</p>\n<p><strong>Connection Pool Sizing Strategy</strong></p>\n<p>Connection pool sizing requires understanding both the mathematical relationships between throughput, latency, and concurrency, and the practical constraints of Redis server capacity and network resources. The pool must accommodate not just average load but also traffic bursts that are common in rate limiting scenarios—after all, rate limiters are most critical exactly when traffic spikes occur.</p>\n<table>\n<thead>\n<tr>\n<th>Pool Sizing Factor</th>\n<th>Calculation Method</th>\n<th>Typical Value Range</th>\n<th>Monitoring Metric</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Base pool size</td>\n<td><code>(average_rps * avg_latency_ms) / 1000</code></td>\n<td>5-20 connections</td>\n<td>Connection utilization %</td>\n</tr>\n<tr>\n<td>Burst capacity</td>\n<td><code>base_size * burst_multiplier</code></td>\n<td>2x-5x base size</td>\n<td>Peak concurrent connections</td>\n</tr>\n<tr>\n<td>Redis server limit</td>\n<td>Server max connections / number of app instances</td>\n<td>Varies by Redis config</td>\n<td>Server connection count</td>\n</tr>\n<tr>\n<td>Network overhead</td>\n<td>Account for connection setup/teardown cost</td>\n<td>10-20% buffer</td>\n<td>Connection churn rate</td>\n</tr>\n</tbody></table>\n<p>The connection pool must implement intelligent sizing that adapts to observed load patterns while respecting hard limits imposed by Redis server configuration and network infrastructure. Static pool sizing often leads to either resource waste during low traffic or bottlenecks during high traffic.</p>\n<p><strong>Health Checking and Circuit Breaker Integration</strong></p>\n<p>Connection health checking in a rate limiting context goes beyond simple ping/pong tests because rate limiting requires not just connectivity but also acceptable response times. A Redis connection that takes 5 seconds to respond is effectively unusable for rate limiting, even though it&#39;s technically healthy from a connectivity perspective.</p>\n<table>\n<thead>\n<tr>\n<th>Health Check Type</th>\n<th>Test Method</th>\n<th>Success Criteria</th>\n<th>Failure Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Connectivity check</td>\n<td><code>PING</code> command</td>\n<td>Response within 100ms</td>\n<td>Mark connection unhealthy</td>\n</tr>\n<tr>\n<td>Performance check</td>\n<td>Simple <code>INCR</code> operation</td>\n<td>Response within 50ms</td>\n<td>Reduce connection priority</td>\n</tr>\n<tr>\n<td>Functionality check</td>\n<td>Execute minimal rate limit script</td>\n<td>Correct result within 100ms</td>\n<td>Flag script issues</td>\n</tr>\n<tr>\n<td>Memory pressure check</td>\n<td><code>INFO memory</code> command</td>\n<td>Used memory &lt; 90%</td>\n<td>Trigger degradation mode</td>\n</tr>\n</tbody></table>\n<p>The circuit breaker pattern becomes essential when Redis experiences problems that manifest as slow responses rather than complete failures. A Redis server under memory pressure might accept connections and even respond to commands, but with latencies that make rate limiting ineffective. The circuit breaker must detect these degraded performance conditions and trigger fallback behavior before the entire rate limiting system becomes unresponsive.</p>\n<p><strong>Connection Lifecycle Management</strong></p>\n<p>Managing the lifecycle of Redis connections involves more than simple creation and destruction—it requires handling the various states a connection can be in and the transitions between those states. Connections in a rate limiting system experience different usage patterns than typical application database connections, with potentially bursty traffic and stringent latency requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Connection State</th>\n<th>Characteristics</th>\n<th>Transition Triggers</th>\n<th>Pool Management Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Available</td>\n<td>Idle, ready for use</td>\n<td>Request arrives</td>\n<td>Assign to request</td>\n</tr>\n<tr>\n<td>Active</td>\n<td>Executing Redis command</td>\n<td>Command completion</td>\n<td>Return to available pool</td>\n</tr>\n<tr>\n<td>Degraded</td>\n<td>Slow but functional</td>\n<td>High latency detected</td>\n<td>Mark for replacement</td>\n</tr>\n<tr>\n<td>Failed</td>\n<td>Connection error occurred</td>\n<td>Network/Redis error</td>\n<td>Remove and create new</td>\n</tr>\n<tr>\n<td>Draining</td>\n<td>Being retired gracefully</td>\n<td>Pool size reduction</td>\n<td>Complete current ops, close</td>\n</tr>\n</tbody></table>\n<p>Connection lifecycle management must handle the reality that Redis connections can fail in various ways—network timeouts, Redis server restarts, memory pressure causing slow responses, or even subtle issues like clock skew affecting time-based operations. The pool manager needs strategies for detecting each type of failure and responding appropriately without causing cascading failures in the rate limiting system.</p>\n<p><strong>Retry Logic and Backoff Strategies</strong></p>\n<p>Retry logic for Redis operations in rate limiting systems requires careful design because failed rate limit checks can&#39;t simply be retried indefinitely—the request being rate limited is waiting for a decision, and excessive retry delays defeat the purpose of rate limiting. The retry strategy must balance reliability against latency while avoiding retry storms that could worsen Redis server problems.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Retry Strategy</th>\n<th>Backoff Pattern</th>\n<th>Max Retry Time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network timeout</td>\n<td>2-3 retries</td>\n<td>Linear: 10ms, 20ms, 30ms</td>\n<td>100ms total</td>\n</tr>\n<tr>\n<td>Connection refused</td>\n<td>1 retry with new connection</td>\n<td>No backoff</td>\n<td>50ms total</td>\n</tr>\n<tr>\n<td>Redis overload</td>\n<td>No immediate retry</td>\n<td>Exponential for background</td>\n<td>Trigger fallback</td>\n</tr>\n<tr>\n<td>Script error</td>\n<td>1 retry, then fallback</td>\n<td>No backoff</td>\n<td>50ms total</td>\n</tr>\n</tbody></table>\n<p>The retry logic must integrate with the fallback strategy—if Redis operations are failing frequently enough to trigger retries, the system should consider switching to local fallback mode rather than continuing to hammer the failing Redis infrastructure. This requires monitoring retry rates and making intelligent decisions about when distributed rate limiting is more harmful than helpful.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Connection pool management in distributed rate limiting differs fundamentally from typical database connection pools because rate limiting decisions are latency-critical and failure-sensitive. A database query can be retried or delayed, but a rate limiting decision that takes too long effectively becomes an allow decision, potentially compromising the entire rate limiting scheme.</p>\n</blockquote>\n<h3 id=\"graceful-degradation-strategy\">Graceful Degradation Strategy</h3>\n<p>Graceful degradation represents one of the most critical aspects of distributed rate limiting design, as it determines how the system behaves when the shared state store becomes unavailable. The challenge lies in maintaining some level of rate limiting effectiveness while avoiding complete service disruption, requiring careful balance between protection and availability.</p>\n<p>When Redis becomes unavailable, the distributed rate limiter faces a fundamental choice: fail open (allow all requests) or fail closed (deny all requests). Neither option is ideal—failing open provides no rate limiting protection and could lead to system overload, while failing closed effectively creates a denial-of-service condition. The graceful degradation strategy must provide a third option that preserves some rate limiting capability while maintaining service availability.</p>\n<p><strong>Local Fallback Implementation Strategy</strong></p>\n<p>Local fallback involves each application instance switching to per-instance rate limiting when the shared Redis backend becomes unavailable. This approach provides continued protection against abuse while maintaining service availability, though with reduced accuracy compared to true distributed rate limiting.</p>\n<p>The key insight is that local rate limiting with adjusted limits can approximate distributed rate limiting behavior. If the distributed system normally allows 1000 requests per minute across 10 application instances, each instance can implement local rate limiting at 100 requests per minute during fallback mode. This provides similar protection levels, though with less accurate enforcement and potential for slightly higher actual limits due to uneven traffic distribution.</p>\n<table>\n<thead>\n<tr>\n<th>Fallback Aspect</th>\n<th>Local Implementation</th>\n<th>Accuracy Impact</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate limit scaling</td>\n<td>Divide distributed limit by instance count</td>\n<td>Uneven traffic causes over/under limiting</td>\n<td>Use dynamic instance discovery</td>\n</tr>\n<tr>\n<td>State isolation</td>\n<td>Each instance tracks separately</td>\n<td>No cross-instance coordination</td>\n<td>Monitor aggregate metrics</td>\n</tr>\n<tr>\n<td>Rule synchronization</td>\n<td>Use last known good configuration</td>\n<td>Rules may become stale</td>\n<td>Cache rules with TTL</td>\n</tr>\n<tr>\n<td>Metrics collection</td>\n<td>Local counters only</td>\n<td>Missing distributed view</td>\n<td>Aggregate in monitoring system</td>\n</tr>\n</tbody></table>\n<p>The local fallback implementation must handle the transition periods carefully—when Redis becomes available again, instances shouldn&#39;t immediately switch back to distributed mode, as this could cause thundering herd problems. Instead, a gradual transition with health checking ensures stable operation.</p>\n<p><strong>Failure Detection and Mode Switching</strong></p>\n<p>Accurate failure detection determines how quickly the system can switch to fallback mode and how effectively it can detect recovery. The failure detection strategy must distinguish between temporary network hiccups that should be retried and genuine Redis failures that require fallback activation.</p>\n<table>\n<thead>\n<tr>\n<th>Detection Signal</th>\n<th>Threshold</th>\n<th>Confidence Level</th>\n<th>Action Triggered</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Connection timeout</td>\n<td>3 consecutive failures</td>\n<td>High</td>\n<td>Immediate fallback</td>\n</tr>\n<tr>\n<td>Slow response</td>\n<td>&gt;500ms for 10 requests</td>\n<td>Medium</td>\n<td>Gradual degradation</td>\n</tr>\n<tr>\n<td>Redis memory errors</td>\n<td>Any OOM response</td>\n<td>High</td>\n<td>Immediate fallback</td>\n</tr>\n<tr>\n<td>Script execution errors</td>\n<td>5 in 60 seconds</td>\n<td>Medium</td>\n<td>Disable scripts, use simple commands</td>\n</tr>\n<tr>\n<td>Network partitions</td>\n<td>No response for 30s</td>\n<td>High</td>\n<td>Full fallback mode</td>\n</tr>\n</tbody></table>\n<p>Mode switching must be implemented with hysteresis to prevent oscillation between distributed and local modes. The criteria for entering fallback mode should be more sensitive than the criteria for returning to distributed mode, ensuring that the system doesn&#39;t constantly switch back and forth during marginal conditions.</p>\n<p><strong>Rate Limit Accuracy During Degradation</strong></p>\n<p>During fallback mode, rate limiting accuracy degrades in predictable ways that must be understood and monitored. The degradation patterns help operations teams understand the current protection level and make informed decisions about additional protective measures.</p>\n<table>\n<thead>\n<tr>\n<th>Degradation Scenario</th>\n<th>Accuracy Impact</th>\n<th>Burst Behavior</th>\n<th>Recommended Monitoring</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Even traffic distribution</td>\n<td>90-95% of intended limit</td>\n<td>Normal burst handling</td>\n<td>Per-instance rate metrics</td>\n</tr>\n<tr>\n<td>Uneven traffic (80/20 split)</td>\n<td>60-120% of intended limit</td>\n<td>Some instances allow full bursts</td>\n<td>Traffic distribution monitoring</td>\n</tr>\n<tr>\n<td>Single hot instance</td>\n<td>Up to 200% of intended limit</td>\n<td>Full burst on hot instance</td>\n<td>Instance-level alerting</td>\n</tr>\n<tr>\n<td>Partial Redis availability</td>\n<td>Mixed accuracy across instances</td>\n<td>Inconsistent burst behavior</td>\n<td>Redis connectivity per instance</td>\n</tr>\n</tbody></table>\n<p>Understanding these accuracy patterns allows the system to provide meaningful rate limit headers even during degradation. Clients can receive information about the current rate limiting state and adjust their behavior accordingly.</p>\n<p><strong>Recovery and State Synchronization</strong></p>\n<p>Recovery from fallback mode requires careful orchestration to prevent thundering herd effects and ensure smooth transition back to distributed operation. The challenge lies in synchronizing state between the local fallback counters and the Redis-based distributed state without causing sudden changes in rate limiting behavior.</p>\n<table>\n<thead>\n<tr>\n<th>Recovery Phase</th>\n<th>Actions</th>\n<th>Validation</th>\n<th>Rollback Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis availability confirmation</td>\n<td>Health checks pass for 60s</td>\n<td>Script execution successful</td>\n<td>Any health check failure</td>\n</tr>\n<tr>\n<td>Gradual transition start</td>\n<td>10% of requests use Redis</td>\n<td>Compare local vs distributed decisions</td>\n<td>&gt;50% decision disagreement</td>\n</tr>\n<tr>\n<td>Transition scaling</td>\n<td>Increase to 50%, then 90%</td>\n<td>Monitor error rates</td>\n<td>Redis error rate &gt;1%</td>\n</tr>\n<tr>\n<td>Full distributed mode</td>\n<td>100% requests use Redis</td>\n<td>Full functionality restored</td>\n<td>Sustained high error rate</td>\n</tr>\n</tbody></table>\n<p>The state synchronization strategy must handle the reality that local counters during fallback mode may not accurately represent what the distributed state should be. Rather than trying to perfectly synchronize state, the recovery process should focus on ensuring that the transition doesn&#39;t create sudden changes in rate limiting behavior that could surprise clients or cause traffic spikes.</p>\n<blockquote>\n<p><strong>Architecture Decision: Fallback Trigger Sensitivity</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance between false positives (unnecessary fallbacks) and false negatives (delayed fallback during real failures)</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Conservative: Only fallback on complete Redis failure</li>\n<li>Aggressive: Fallback on any performance degradation</li>\n<li>Adaptive: Adjust sensitivity based on recent failure patterns</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Adaptive approach with configurable base sensitivity</li>\n<li><strong>Rationale</strong>: Different deployments have different tolerance for degraded Redis performance. Some can handle 200ms Redis responses, others need &lt;50ms for effective rate limiting.</li>\n<li><strong>Consequences</strong>: Requires more complex configuration and monitoring, but provides better operational flexibility and fewer false positive fallbacks</li>\n</ul>\n</blockquote>\n<h3 id=\"redis-backend-adr\">Redis Backend ADR</h3>\n<p>The choice of backend storage for distributed rate limiting state represents one of the most fundamental architectural decisions in the system, affecting everything from performance characteristics to operational complexity. This decision impacts not just the immediate implementation but also long-term scalability, operational procedures, and integration patterns with existing infrastructure.</p>\n<blockquote>\n<p><strong>Architecture Decision: Redis as Primary Backend Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Need shared storage for rate limiting state that supports atomic operations, high performance, and horizontal scaling. Must handle thousands of rate limit checks per second across multiple application instances with sub-10ms latency requirements.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Redis with Lua scripts for atomic operations</li>\n<li>etcd with compare-and-swap operations for consistency</li>\n<li>PostgreSQL with advisory locks and ACID transactions</li>\n<li>DynamoDB with conditional writes and TTL</li>\n<li>Cassandra with lightweight transactions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Redis with Lua scripts as primary backend, with etcd as alternative for environments requiring strong consistency</li>\n<li><strong>Rationale</strong>: Redis provides the optimal combination of performance (sub-millisecond operations), atomic operation support (Lua scripts), memory efficiency for time-series data, and operational maturity for high-traffic systems.</li>\n<li><strong>Consequences</strong>: Requires Redis operational expertise, introduces eventual consistency considerations during network partitions, but provides excellent performance and horizontal scaling capabilities.</li>\n</ul>\n</blockquote>\n<p><strong>Detailed Options Analysis</strong></p>\n<p>The storage backend decision required extensive analysis of how each option handles the specific requirements of distributed rate limiting, particularly around atomic operations, performance characteristics, and operational complexity.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Option</th>\n<th>Atomic Operations</th>\n<th>Typical Latency</th>\n<th>Memory Efficiency</th>\n<th>Operational Complexity</th>\n<th>Horizontal Scaling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis + Lua</td>\n<td>Lua scripts</td>\n<td>0.1-1ms</td>\n<td>Excellent</td>\n<td>Medium</td>\n<td>Hash-based sharding</td>\n</tr>\n<tr>\n<td>etcd</td>\n<td>Compare-and-swap</td>\n<td>1-5ms</td>\n<td>Good</td>\n<td>High</td>\n<td>Raft consensus</td>\n</tr>\n<tr>\n<td>PostgreSQL</td>\n<td>ACID transactions</td>\n<td>5-20ms</td>\n<td>Poor for counters</td>\n<td>High</td>\n<td>Read replicas only</td>\n</tr>\n<tr>\n<td>DynamoDB</td>\n<td>Conditional writes</td>\n<td>10-50ms</td>\n<td>Good</td>\n<td>Low</td>\n<td>Automatic</td>\n</tr>\n<tr>\n<td>Cassandra</td>\n<td>Lightweight transactions</td>\n<td>5-15ms</td>\n<td>Good</td>\n<td>Very High</td>\n<td>Excellent</td>\n</tr>\n</tbody></table>\n<p>Redis emerged as the optimal choice primarily due to its combination of performance and atomic operation support. Lua scripts provide true atomicity for complex rate limiting algorithms, while Redis&#39;s in-memory architecture delivers the low latency required for inline rate limiting decisions. The memory efficiency for storing counters and timestamps makes Redis particularly well-suited for the access patterns typical in rate limiting workloads.</p>\n<p><strong>etcd Comparison and Use Cases</strong></p>\n<p>etcd represents the primary alternative to Redis, particularly in environments where strong consistency requirements outweigh performance considerations. Understanding when to choose etcd over Redis helps inform deployment decisions and architectural trade-offs.</p>\n<table>\n<thead>\n<tr>\n<th>Consideration</th>\n<th>Redis Advantages</th>\n<th>etcd Advantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Consistency model</td>\n<td>Eventual consistency, faster</td>\n<td>Strong consistency, slower</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>Sub-millisecond operations</td>\n<td>Millisecond operations</td>\n</tr>\n<tr>\n<td>Operational complexity</td>\n<td>Familiar to most teams</td>\n<td>Requires Raft understanding</td>\n</tr>\n<tr>\n<td>Failure handling</td>\n<td>Manual failover or Redis Sentinel</td>\n<td>Automatic leader election</td>\n</tr>\n<tr>\n<td>Multi-datacenter</td>\n<td>Complex setup</td>\n<td>Native support</td>\n</tr>\n<tr>\n<td>Kubernetes integration</td>\n<td>Requires external setup</td>\n<td>Often pre-installed</td>\n</tr>\n</tbody></table>\n<p>etcd becomes the preferred choice in environments where rate limiting must integrate with existing Kubernetes control plane infrastructure, or where strong consistency requirements justify the performance trade-offs. For example, financial systems might prefer etcd&#39;s consistency guarantees even at the cost of higher latency.</p>\n<p><strong>PostgreSQL and Traditional Database Analysis</strong></p>\n<p>Traditional relational databases like PostgreSQL initially seem attractive for rate limiting because they provide strong consistency guarantees and familiar operational models. However, deeper analysis reveals fundamental mismatches with rate limiting requirements.</p>\n<p>The primary challenge with PostgreSQL for rate limiting lies in the access patterns. Rate limiting requires high-frequency updates to counter values with minimal read complexity—essentially the inverse of typical web application database usage. PostgreSQL&#39;s MVCC (Multi-Version Concurrency Control) system creates overhead for high-frequency counter updates, and the persistence guarantees designed for critical business data become unnecessary overhead for rate limiting state.</p>\n<table>\n<thead>\n<tr>\n<th>Database Limitation</th>\n<th>Impact on Rate Limiting</th>\n<th>Mitigation Cost</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MVCC overhead for updates</td>\n<td>Higher CPU usage for counters</td>\n<td>Requires more database capacity</td>\n</tr>\n<tr>\n<td>Disk I/O for durability</td>\n<td>Slower response times</td>\n<td>SSD required, higher costs</td>\n</tr>\n<tr>\n<td>Connection overhead</td>\n<td>Fewer concurrent rate checks</td>\n<td>Larger connection pools needed</td>\n</tr>\n<tr>\n<td>Limited atomic operations</td>\n<td>Complex application logic</td>\n<td>More application complexity</td>\n</tr>\n</tbody></table>\n<p>PostgreSQL remains viable for rate limiting in environments where extreme consistency requirements justify the performance costs, or where existing PostgreSQL expertise and infrastructure make operational complexity the primary concern.</p>\n<p><strong>Cloud-Native Options Analysis</strong></p>\n<p>Cloud-native storage options like DynamoDB offer compelling operational simplicity but introduce different trade-offs around performance predictability and cost scaling.</p>\n<p>DynamoDB&#39;s conditional write operations provide the atomicity needed for rate limiting, and automatic scaling eliminates capacity planning concerns. However, the performance characteristics vary significantly based on provisioned capacity and hot key patterns common in rate limiting workloads. The pricing model also creates challenges for high-traffic rate limiting where the cost can become substantial.</p>\n<table>\n<thead>\n<tr>\n<th>Cloud Option</th>\n<th>Operational Burden</th>\n<th>Performance Predictability</th>\n<th>Cost Scaling</th>\n<th>Lock-in Risk</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DynamoDB</td>\n<td>Very Low</td>\n<td>Variable</td>\n<td>High at scale</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Cloud Redis</td>\n<td>Low</td>\n<td>Predictable</td>\n<td>Moderate</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Cloud SQL</td>\n<td>Medium</td>\n<td>Predictable</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>The choice between cloud-native and self-managed options often depends more on organizational factors than technical requirements. Teams with strong infrastructure automation capabilities may prefer self-managed Redis for cost control and performance predictability, while teams prioritizing operational simplicity may accept the trade-offs of managed cloud services.</p>\n<p><strong>Future Migration Considerations</strong></p>\n<p>The storage backend decision should consider not just current requirements but also likely evolution paths. Rate limiting systems often start simple and grow more sophisticated over time, requiring migration strategies that minimize service disruption.</p>\n<p>Redis provides good migration paths to other storage options because its simple key-value model can be replicated in most other systems. The atomic operation patterns established using Lua scripts can be translated to stored procedures in databases or conditional operations in other NoSQL systems.</p>\n<p>The modular storage interface design allows for backend migration without changing the core rate limiting logic. This architectural separation enables gradual migration strategies where different rate limiting tiers or different types of keys can use different storage backends during transition periods.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The storage backend choice for distributed rate limiting differs significantly from typical application data storage decisions. Rate limiting requires high write throughput with simple access patterns, making it more similar to metrics collection or event streaming workloads than traditional CRUD operations. This fundamental difference in access patterns explains why Redis often outperforms traditional databases for this specific use case.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance focuses on building production-ready Redis integration for distributed rate limiting, with emphasis on the connection management, atomic operations, and graceful degradation patterns that separate robust systems from fragile prototypes.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Client</td>\n<td><code>go-redis/redis/v8</code> with basic clustering</td>\n<td><code>go-redis/redis/v8</code> with Redis Sentinel for HA</td>\n</tr>\n<tr>\n<td>Connection Pooling</td>\n<td>Built-in go-redis connection pool</td>\n<td>Custom pool with circuit breaker integration</td>\n</tr>\n<tr>\n<td>Lua Script Management</td>\n<td>Embedded scripts with EVALSHA fallback</td>\n<td>External script files with hot reload</td>\n</tr>\n<tr>\n<td>Health Checking</td>\n<td>Simple PING commands</td>\n<td>Comprehensive health with performance metrics</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>Static YAML configuration</td>\n<td>Dynamic config with Redis pub/sub updates</td>\n</tr>\n<tr>\n<td>Monitoring Integration</td>\n<td>Basic metrics collection</td>\n<td>Prometheus metrics with custom collectors</td>\n</tr>\n</tbody></table>\n<p>For production deployments, the advanced options provide the reliability and observability needed to debug issues under load. The simple options work well for development and testing environments.</p>\n<p><strong>B. Recommended Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/redis/\n├── client.go              ← Redis client wrapper with pooling\n├── scripts/\n│   ├── token_bucket.lua    ← Token bucket Lua script\n│   ├── sliding_counter.lua ← Sliding window counter script\n│   └── sliding_log.lua     ← Sliding window log script\n├── storage.go             ← RedisStorage implementation\n├── health.go              ← Health checking and circuit breaker\n├── fallback.go            ← Local fallback implementation\n└── config.go              ← Redis configuration structures\n\ninternal/scripts/\n├── loader.go              ← Lua script loading and caching\n└── registry.go           ← Script SHA management</code></pre></div>\n\n<p>This structure separates Redis-specific concerns from the core rate limiting algorithms while providing clear boundaries for testing and future backend alternatives.</p>\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p><strong>Redis Configuration and Client Setup</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/go-redis/redis/v8</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Addresses      []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `yaml:\"addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Password       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"password\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DB            </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"db\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PoolSize      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"pool_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DialTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"dial_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> client </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(config.Addresses) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#B392F0\">NewClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Options</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Addr:         config.Addresses[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Password:     config.Password,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            DB:           config.DB,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            PoolSize:     config.PoolSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  config.ReadTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: config.WriteTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            DialTimeout:  config.DialTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#B392F0\">NewClusterClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ClusterOptions</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Addrs:        config.Addresses,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Password:     config.Password,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            PoolSize:     config.PoolSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  config.ReadTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: config.WriteTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            DialTimeout:  config.DialTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client: client,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config: config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scripts: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Load Lua scripts on startup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> storage.</span><span style=\"color:#B392F0\">loadScripts</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to load scripts: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> storage, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Lua Script Management</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _ </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#B392F0\">embed</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha1</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">//go:embed scripts/token_bucket.lua</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> tokenBucketScript </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">//go:embed scripts/sliding_counter.lua</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> slidingCounterScript </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">//go:embed scripts/sliding_log.lua</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> slidingLogScript </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisStorage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client  </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config  </span><span style=\"color:#B392F0\">RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scripts </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // script name -> SHA hash</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">loadScripts</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scripts </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"token_bucket\"</span><span style=\"color:#E1E4E8\">:    tokenBucketScript,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"sliding_counter\"</span><span style=\"color:#E1E4E8\">: slidingCounterScript,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"sliding_log\"</span><span style=\"color:#E1E4E8\">:     slidingLogScript,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, script </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> scripts {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sha, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.client.</span><span style=\"color:#B392F0\">ScriptLoad</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), script).</span><span style=\"color:#B392F0\">Result</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to load script </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, name, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        r.scripts[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sha</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteLua</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">script</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">keys</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">args</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) (</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sha, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.scripts[script]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"script </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> not found\"</span><span style=\"color:#E1E4E8\">, script)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Try EVALSHA first for performance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.client.</span><span style=\"color:#B392F0\">EvalSha</span><span style=\"color:#E1E4E8\">(ctx, sha, keys, args</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">).</span><span style=\"color:#B392F0\">Result</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // If script not in cache, fall back to EVAL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"NOSCRIPT No matching script. Please use EVAL.\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> r.client.</span><span style=\"color:#B392F0\">Eval</span><span style=\"color:#E1E4E8\">(ctx, r.</span><span style=\"color:#B392F0\">getScriptContent</span><span style=\"color:#E1E4E8\">(script), keys, args</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">).</span><span style=\"color:#B392F0\">Result</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Health Checking with Circuit Breaker</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthChecker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    healthy         </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastCheck       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureCount    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitOpen     </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nextRetryTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHealthChecker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hc </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage: storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        healthy: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> hc.</span><span style=\"color:#B392F0\">startHealthChecking</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IsHealthy</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hc.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hc.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hc.healthy </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">hc.circuitOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">startHealthChecking</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ticker.C {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hc.</span><span style=\"color:#B392F0\">performHealthCheck</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">performHealthCheck</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hc.storage.client.</span><span style=\"color:#B392F0\">Ping</span><span style=\"color:#E1E4E8\">(ctx).</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    latency </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hc.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hc.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> latency </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Millisecond {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hc.failureCount</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> hc.failureCount </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            hc.healthy </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            hc.circuitOpen </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            hc.nextRetryTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hc.failureCount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hc.healthy </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">After</span><span style=\"color:#E1E4E8\">(hc.nextRetryTime) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            hc.circuitOpen </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hc.lastCheck </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p><strong>Token Bucket Redis Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckAndUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Prepare Redis key for token bucket (prefix + key)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get current timestamp in nanoseconds for precise timing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Calculate refill rate as tokens per nanosecond (limit / window)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute token bucket Lua script with key, current time, limit, refill rate, requested tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse script result - [allowed, remaining_tokens, reset_time]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Convert reset_time from nanoseconds to time.Time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return whether allowed, remaining tokens, and reset time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Now().UnixNano() for high precision timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Script should handle token refill calculation atomically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Sliding Window Counter Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckSlidingWindow</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">buckets</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate current bucket ID based on current time and bucket size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine which buckets fall within the current window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute sliding counter Lua script with key, current bucket, window buckets, limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse script result - [allowed, current_count, window_total, oldest_bucket]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate remaining quota (limit - window_total)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Calculate reset time (when oldest bucket expires)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return structured RateLimitResult with all computed values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Bucket size = window / buckets, bucket ID = current_time / bucket_size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Script should clean up expired buckets to prevent memory growth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Graceful Degradation Manager</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if Redis backend is healthy using health checker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If healthy, attempt Redis-based rate limiting with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If Redis fails or times out, record failure for circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: On Redis failure, switch to local fallback with adjusted limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Scale down limits for local mode (divide by estimated instance count)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Include fallback mode indicator in result for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Implement retry logic with exponential backoff for Redis recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context.WithTimeout for Redis operations to prevent hanging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Local limits should be distributed_limit / instance_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints for Go</strong></p>\n<ul>\n<li>Use <code>go-redis/redis/v8</code> for Redis connectivity with built-in connection pooling</li>\n<li>Embed Lua scripts using <code>//go:embed</code> directive to avoid runtime file dependencies</li>\n<li>Implement proper context cancellation for all Redis operations to prevent goroutine leaks</li>\n<li>Use <code>sync.RWMutex</code> for health checker state to allow concurrent reads</li>\n<li>Consider using <code>time.UnixNano()</code> for high-precision timestamps in rate limiting calculations</li>\n<li>Implement proper error wrapping with <code>fmt.Errorf(&quot;operation failed: %w&quot;, err)</code></li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing Redis backend integration, verify the following functionality:</p>\n<p><strong>Basic Connectivity Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start Redis locally or use Docker</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">docker</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#79B8FF\"> --name</span><span style=\"color:#9ECBFF\"> redis-test</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> 6379:6379</span><span style=\"color:#9ECBFF\"> redis:7</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test basic Redis operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/redis-test/main.go</span></span></code></pre></div>\n\n<p>Expected behavior: Connection established, Lua scripts loaded successfully, health checker reports healthy status.</p>\n<p><strong>Rate Limiting Functionality Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run integration test with Redis backend</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> ./internal/redis/...</span><span style=\"color:#79B8FF\"> -tags=integration</span></span></code></pre></div>\n\n<p>Expected behavior: Token bucket script executes correctly, sliding window counters increment properly, fallback triggers when Redis is stopped.</p>\n<p><strong>Performance Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Benchmark Redis vs local performance</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -bench=BenchmarkRedisRateLimit</span><span style=\"color:#9ECBFF\"> ./internal/redis/</span></span></code></pre></div>\n\n<p>Expected results: Redis operations complete in &lt;5ms p99, throughput &gt;1000 ops/sec per connection.</p>\n<p>Signs of problems and debugging steps:</p>\n<ul>\n<li><strong>Scripts not loading</strong>: Check Redis version (requires 2.6+), verify script syntax</li>\n<li><strong>Connection timeouts</strong>: Check network connectivity, Redis memory usage, connection pool size</li>\n<li><strong>Inconsistent results</strong>: Verify system clock synchronization, check for Redis clustering issues</li>\n</ul>\n<h2 id=\"consistent-hashing-and-sharding\">Consistent Hashing and Sharding</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 - Consistent Hashing &amp; Sharding</p>\n</blockquote>\n<p>The transition from single-node Redis to a distributed Redis cluster represents a fundamental scaling challenge in distributed rate limiting. While a single Redis instance can handle thousands of rate limit checks per second, production systems often require tens or hundreds of thousands of operations per second across millions of rate limit keys. This milestone transforms our centralized rate limiting system into a horizontally scalable distributed system that can grow by adding more Redis nodes while maintaining consistent performance and minimizing operational complexity during cluster topology changes.</p>\n<h3 id=\"mental-model-library-book-distribution\">Mental Model: Library Book Distribution</h3>\n<p>Think of distributing rate limit state across multiple Redis nodes like managing a university library system with multiple branch locations. In a traditional single-location library, all books are stored in one building, and patrons must visit that specific location to access any book. This works well for small collections, but as the collection grows to millions of books and thousands of daily visitors, the single location becomes overwhelmed.</p>\n<p>The solution is to distribute books across multiple branch libraries using a systematic approach. However, you cannot randomly scatter books across branches—patrons need a predictable way to find any specific book. A naive approach might be alphabetical distribution: books A-F go to Branch 1, G-M to Branch 2, and so on. But this creates problems when you need to add a new branch—suddenly you must physically move thousands of books to rebalance the collection, disrupting service for weeks.</p>\n<p><strong>Consistent hashing</strong> solves this problem elegantly. Instead of dividing the alphabet into fixed ranges, imagine arranging the branches in a circle based on their unique characteristics (like their postal codes). Each book is assigned to the first branch you encounter when walking clockwise from the book&#39;s hash position on the circle. When you add a new branch, you only need to move books from one adjacent branch to maintain balance. When a branch temporarily closes for maintenance, patrons are automatically redirected to the next available branch clockwise.</p>\n<p><strong>Hot key detection</strong> is like noticing that certain popular textbooks are being requested so frequently at one branch that students form long lines. The library system responds by placing copies of these popular books at multiple branches, reducing the load on any single location.</p>\n<p>This library analogy maps directly to our distributed rate limiting system:</p>\n<ul>\n<li><strong>Books</strong> → Rate limit keys (user:123, api:/login, ip:192.168.1.1)</li>\n<li><strong>Branch libraries</strong> → Redis nodes in the cluster</li>\n<li><strong>Book locations</strong> → Which Redis node stores each rate limit counter</li>\n<li><strong>Catalog lookup</strong> → Consistent hash function determining node assignment</li>\n<li><strong>Branch closure</strong> → Redis node failure requiring automatic failover</li>\n<li><strong>Popular textbooks</strong> → Hot keys that need replication across multiple nodes</li>\n<li><strong>Adding new branches</strong> → Scaling by adding Redis nodes with minimal data movement</li>\n</ul>\n<h3 id=\"consistent-hash-ring-design\">Consistent Hash Ring Design</h3>\n<p>The consistent hash ring forms the mathematical foundation for distributing rate limit keys across Redis nodes while minimizing redistribution during topology changes. Unlike traditional hash-based sharding where adding nodes requires rehashing all keys, consistent hashing ensures that only a small fraction of keys need to move when the cluster topology changes.</p>\n<h4 id=\"hash-ring-mathematics-and-virtual-nodes\">Hash Ring Mathematics and Virtual Nodes</h4>\n<p>The consistent hash ring maps both Redis nodes and rate limit keys onto a circular hash space, typically using SHA-1 or SHA-256 to produce a 160-bit or 256-bit hash space. The ring conceptually represents all possible hash values arranged in a circle, where the maximum hash value wraps around to zero.</p>\n<p>Each Redis node is assigned multiple positions on the ring called <strong>virtual nodes</strong> or <strong>vnodes</strong>. Virtual nodes solve the fundamental problem of uneven load distribution that occurs when physical nodes are hashed to random positions on the ring. Without virtual nodes, adding or removing a single physical node could create scenarios where one node handles 80% of the keys while others handle only 5%.</p>\n<p>Virtual nodes work by creating multiple hash positions for each physical Redis node using different hash inputs. For example, if Redis node <code>redis-1</code> at address <code>10.0.1.100:6379</code> uses 150 virtual nodes, we generate positions by hashing:</p>\n<ul>\n<li><code>redis-1:vnode:0</code> → hash position 0x1a2b3c4d...</li>\n<li><code>redis-1:vnode:1</code> → hash position 0x5e6f7g8h...</li>\n<li><code>redis-1:vnode:2</code> → hash position 0x9i0j1k2l...</li>\n<li>... continuing for all 150 virtual nodes</li>\n</ul>\n<p>This creates 150 different positions on the ring where <code>redis-1</code> is responsible for handling keys. The more virtual nodes per physical node, the more evenly distributed the load becomes, approaching perfect balance as the virtual node count increases.</p>\n<table>\n<thead>\n<tr>\n<th>Virtual Nodes Per Physical Node</th>\n<th>Expected Load Variance</th>\n<th>Memory Overhead</th>\n<th>Rebalancing Efficiency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>50</td>\n<td>±15% from perfect balance</td>\n<td>Low</td>\n<td>Good</td>\n</tr>\n<tr>\n<td>150</td>\n<td>±8% from perfect balance</td>\n<td>Medium</td>\n<td>Better</td>\n</tr>\n<tr>\n<td>500</td>\n<td>±3% from perfect balance</td>\n<td>High</td>\n<td>Excellent</td>\n</tr>\n<tr>\n<td>1000</td>\n<td>±2% from perfect balance</td>\n<td>Very High</td>\n<td>Excellent</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Virtual Node Count Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance load distribution accuracy against memory overhead and lookup performance</li>\n<li><strong>Options Considered</strong>: 50, 150, 500, or 1000 virtual nodes per physical node</li>\n<li><strong>Decision</strong>: Use 150 virtual nodes per physical node as the default</li>\n<li><strong>Rationale</strong>: Provides ±8% load variance which is acceptable for rate limiting workloads, while keeping memory overhead reasonable and maintaining fast lookup performance. Can be configured higher for clusters with extreme hot key problems.</li>\n<li><strong>Consequences</strong>: Enables good load balance with reasonable memory usage. Lookup time increases slightly due to larger ring structure, but remains O(log N) with binary search.</li>\n</ul>\n</blockquote>\n<h4 id=\"key-assignment-algorithm\">Key Assignment Algorithm</h4>\n<p>Rate limit keys are assigned to Redis nodes through a deterministic process that ensures any application instance can independently determine which node handles any specific key without central coordination. The algorithm follows these steps:</p>\n<ol>\n<li><p><strong>Hash the rate limit key</strong>: Apply the same hash function (SHA-256) to the complete rate limit key string. For example, the key <code>user:12345:api:/login:1m</code> produces a 256-bit hash value.</p>\n</li>\n<li><p><strong>Locate position on ring</strong>: The hash value represents a position on the circular hash space. Conceptually, this is like dropping a pin at a specific location on the ring.</p>\n</li>\n<li><p><strong>Find responsible virtual node</strong>: Walk clockwise from the key&#39;s position until encountering the first virtual node. This virtual node&#39;s physical Redis node is responsible for storing this key&#39;s rate limit state.</p>\n</li>\n<li><p><strong>Handle ring wrap-around</strong>: If no virtual node exists between the key&#39;s position and the maximum hash value, wrap around to the beginning of the ring and continue searching from zero.</p>\n</li>\n</ol>\n<p>The beauty of this algorithm lies in its consistency—every application instance performing the same calculation will always arrive at the same Redis node for any given key, without requiring centralized coordination or shared state.</p>\n<table>\n<thead>\n<tr>\n<th>Hash Ring Operation</th>\n<th>Time Complexity</th>\n<th>Space Complexity</th>\n<th>Consistency Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key lookup</td>\n<td>O(log V) where V = total virtual nodes</td>\n<td>O(V) for ring storage</td>\n<td>Deterministic - all nodes agree</td>\n</tr>\n<tr>\n<td>Node addition</td>\n<td>O(V/N) keys move where N = physical nodes</td>\n<td>O(V) ring restructure</td>\n<td>Affects only adjacent ranges</td>\n</tr>\n<tr>\n<td>Node removal</td>\n<td>O(V/N) keys move</td>\n<td>O(V) ring restructure</td>\n<td>Graceful failover to next node</td>\n</tr>\n<tr>\n<td>Ring rebalancing</td>\n<td>O(V log V) for sorting</td>\n<td>O(V) temporary storage</td>\n<td>Eventually consistent</td>\n</tr>\n</tbody></table>\n<h4 id=\"minimizing-redistribution-during-topology-changes\">Minimizing Redistribution During Topology Changes</h4>\n<p>The consistent hash ring&#39;s primary advantage becomes apparent during cluster topology changes. When a new Redis node joins the cluster, it only takes responsibility for keys that fall within specific ranges on the ring, rather than triggering a complete reshuffling of all keys.</p>\n<p><strong>Adding a new node</strong> follows this process:</p>\n<ol>\n<li><p><strong>Generate virtual node positions</strong>: The new physical node generates its virtual nodes at deterministic positions on the ring based on its identifier and virtual node indices.</p>\n</li>\n<li><p><strong>Identify affected ranges</strong>: For each virtual node of the new physical node, determine the range of keys that will move from the previously responsible node. This range spans from the new virtual node&#39;s position back to the previous virtual node in the clockwise direction.</p>\n</li>\n<li><p><strong>Coordinate data migration</strong>: The application instances coordinate with both the old and new Redis nodes to migrate rate limit state for keys within the affected ranges. This migration must maintain atomicity to prevent rate limit violations during the transition.</p>\n</li>\n<li><p><strong>Update routing tables</strong>: All application instances update their consistent hash ring structures to include the new node&#39;s virtual nodes, ensuring future requests route correctly.</p>\n</li>\n</ol>\n<p><strong>Removing a node</strong> (whether planned or due to failure) reverses this process:</p>\n<ol>\n<li><p><strong>Identify orphaned ranges</strong>: Determine which key ranges were handled by the departing node&#39;s virtual nodes.</p>\n</li>\n<li><p><strong>Reassign to next nodes</strong>: For each orphaned range, the responsibility transfers to the next virtual node clockwise on the ring.</p>\n</li>\n<li><p><strong>Migrate remaining state</strong>: If the departure was planned, migrate any remaining rate limit state to the newly responsible nodes. If the departure was due to failure, the rate limit state is lost, but the system continues operating with temporary accuracy degradation.</p>\n</li>\n<li><p><strong>Clean up routing tables</strong>: Remove the departed node&#39;s virtual nodes from all application instances&#39; hash ring structures.</p>\n</li>\n</ol>\n<p>The mathematical guarantee of consistent hashing is that adding or removing one node affects at most O(K/N) keys, where K is the total number of keys and N is the number of nodes. In practice, this means adding a fifth node to a four-node cluster will move approximately 20% of the keys, rather than the 100% that would move with traditional hash-based sharding.</p>\n<h3 id=\"hot-key-detection-and-rebalancing\">Hot Key Detection and Rebalancing</h3>\n<p>Even with perfect hash distribution, real-world traffic patterns create <strong>hot keys</strong>—rate limit keys that receive disproportionately high request volumes compared to the average. Hot keys can overwhelm individual Redis nodes while leaving others underutilized, creating performance bottlenecks that undermine the benefits of horizontal scaling.</p>\n<h4 id=\"hot-key-identification-mechanisms\">Hot Key Identification Mechanisms</h4>\n<p>Hot key detection operates through statistical analysis of request patterns across time windows, combined with cross-node comparison to identify keys that exceed normal distribution expectations. The detection system must balance accuracy against overhead, since monitoring every key individually would consume excessive resources.</p>\n<p><strong>Request frequency tracking</strong> maintains sliding window counters for key access patterns. Each application instance tracks the frequency of rate limit checks for individual keys over rolling time windows (typically 1-minute, 5-minute, and 15-minute windows). Keys that consistently appear in the top percentiles across multiple time windows become candidates for hot key classification.</p>\n<p><strong>Cross-node load comparison</strong> identifies keys that create uneven load distribution. The hot key detection system periodically samples key access frequencies across all Redis nodes and identifies keys where a single node handles a disproportionate share of the total cluster traffic. A key is classified as hot if it represents more than a configurable threshold (typically 2-5%) of any single node&#39;s request volume.</p>\n<p><strong>Adaptive thresholds</strong> prevent false positives during normal traffic variations. The detection system calculates dynamic thresholds based on overall cluster load patterns, ensuring that keys are only classified as hot when they create genuine bottlenecks rather than normal peak traffic.</p>\n<table>\n<thead>\n<tr>\n<th>Detection Method</th>\n<th>Accuracy</th>\n<th>Overhead</th>\n<th>Detection Latency</th>\n<th>False Positive Rate</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request frequency only</td>\n<td>Medium</td>\n<td>Low</td>\n<td>1-5 minutes</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Cross-node comparison</td>\n<td>High</td>\n<td>Medium</td>\n<td>2-10 minutes</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Combined approach</td>\n<td>Very High</td>\n<td>Medium-High</td>\n<td>1-5 minutes</td>\n<td>Very Low</td>\n</tr>\n<tr>\n<td>Real-time statistical analysis</td>\n<td>Excellent</td>\n<td>High</td>\n<td>30 seconds</td>\n<td>Very Low</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Hot Key Detection Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to identify problematic keys without adding significant overhead to normal operations</li>\n<li><strong>Options Considered</strong>: Request frequency tracking only, cross-node comparison only, combined approach, or real-time statistical analysis</li>\n<li><strong>Decision</strong>: Use combined approach with request frequency tracking and periodic cross-node comparison</li>\n<li><strong>Rationale</strong>: Provides high accuracy with reasonable overhead. Real-time analysis is too expensive for most workloads, while single-method approaches have too many false positives or negatives.</li>\n<li><strong>Consequences</strong>: Achieves good hot key detection with manageable overhead. May miss very short-lived hot keys (under 1 minute duration), but these rarely cause sustained performance problems.</li>\n</ul>\n</blockquote>\n<h4 id=\"hot-key-replication-strategy\">Hot Key Replication Strategy</h4>\n<p>Once hot keys are identified, the system must distribute their load across multiple Redis nodes while maintaining consistency and avoiding race conditions. Hot key replication creates multiple copies of a rate limit counter across different nodes, requiring careful coordination to prevent double-counting or lost updates.</p>\n<p><strong>Read replica distribution</strong> creates read-only copies of hot key state on multiple Redis nodes. When a hot key is detected, the system creates replicas on 2-3 additional nodes chosen to minimize impact on ring distribution. Read requests for rate limit previews can be served from any replica, distributing the query load. However, all write operations (actual rate limit checks that decrement counters) must still be directed to the primary node to maintain consistency.</p>\n<p><strong>Write load distribution</strong> splits hot key write operations across multiple nodes using key suffixing. Instead of storing all state for hot key <code>user:popular_user:api:/login:1m</code> on a single node, the system creates multiple suffixed keys:</p>\n<ul>\n<li><code>user:popular_user:api:/login:1m:shard:0</code></li>\n<li><code>user:popular_user:api:/login:1m:shard:1</code>  </li>\n<li><code>user:popular_user:api:/login:1m:shard:2</code></li>\n</ul>\n<p>Write operations are distributed across these sharded keys using consistent hashing on the client identifier or request timestamp. Rate limit checks aggregate state from all shards to determine the total usage.</p>\n<p><strong>Consistency maintenance</strong> ensures that replicated or sharded hot keys maintain accurate rate limit enforcement. For read replicas, the primary node periodically synchronizes state to replicas (typically every 10-30 seconds). For sharded writes, rate limit checks must query all shards and aggregate results, adding latency but distributing load.</p>\n<h4 id=\"automatic-rebalancing-triggers\">Automatic Rebalancing Triggers</h4>\n<p>The system automatically initiates rebalancing operations when hot key detection identifies sustained performance problems or when cluster topology changes create uneven load distribution. Rebalancing must be coordinated carefully to avoid disrupting active rate limiting operations.</p>\n<p><strong>Load threshold monitoring</strong> triggers rebalancing when key access patterns create sustained imbalance. If any Redis node consistently handles more than a configurable percentage (typically 40-50%) of total cluster requests for more than a sustained period (typically 10-15 minutes), the system initiates automatic rebalancing.</p>\n<p><strong>Cluster topology rebalancing</strong> activates after node additions or removals complete their initial data migration. Even though consistent hashing minimizes key movement, the addition or removal of nodes can still create scenarios where hot keys concentrate on a subset of nodes. The rebalancing system identifies these patterns and creates additional replicas or shards as needed.</p>\n<p><strong>Time-based rebalancing windows</strong> schedule major rebalancing operations during periods of lower traffic to minimize impact on active rate limiting. The system maintains historical traffic patterns and automatically schedules rebalancing during identified low-traffic windows, typically during off-peak hours.</p>\n<table>\n<thead>\n<tr>\n<th>Rebalancing Trigger</th>\n<th>Threshold</th>\n<th>Response Time</th>\n<th>Impact on Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sustained load imbalance</td>\n<td>&gt;50% requests on single node for &gt;15min</td>\n<td>5-10 minutes</td>\n<td>Low - gradual replica creation</td>\n</tr>\n<tr>\n<td>Hot key detection</td>\n<td>&gt;5% cluster requests for single key</td>\n<td>2-5 minutes</td>\n<td>Medium - immediate sharding</td>\n</tr>\n<tr>\n<td>Post-topology change</td>\n<td>After node add/remove completion</td>\n<td>1-2 hours</td>\n<td>Low - background optimization</td>\n</tr>\n<tr>\n<td>Scheduled maintenance</td>\n<td>Daily during off-peak hours</td>\n<td>Immediate</td>\n<td>Very Low - planned window</td>\n</tr>\n</tbody></table>\n<h3 id=\"node-health-and-failover\">Node Health and Failover</h3>\n<p>Redis node failures represent the most critical operational challenge in distributed rate limiting, since failure of a single node can immediately impact rate limit accuracy for thousands of keys. The health monitoring and failover system must detect failures quickly, route traffic away from failed nodes, and maintain system availability while minimizing rate limit violations during transitions.</p>\n<h4 id=\"health-check-implementation\">Health Check Implementation</h4>\n<p>Health checking monitors multiple indicators of Redis node availability and performance to distinguish between temporary network glitches and genuine node failures. The health check system must balance rapid failure detection against false positive alerts that could trigger unnecessary failovers.</p>\n<p><strong>Connection-level health checks</strong> verify basic network connectivity and Redis protocol responsiveness. Each application instance maintains persistent connections to all Redis nodes in the cluster and performs periodic ping operations (typically every 5-10 seconds). Failed ping operations increment failure counters, while successful operations reset counters and update last-seen timestamps.</p>\n<p><strong>Operation-level health checks</strong> monitor the success rate and latency of actual rate limiting operations rather than just connection availability. These checks perform lightweight rate limit operations on synthetic keys and measure both success rates and response times. Nodes that consistently return errors or exceed latency thresholds (typically 50-100ms for rate limit checks) are marked as degraded even if basic connectivity remains functional.</p>\n<p><strong>Memory and resource monitoring</strong> tracks Redis memory usage, CPU utilization, and key eviction rates to identify nodes approaching resource exhaustion. Redis nodes under memory pressure may start evicting rate limit keys or slowing response times significantly before completely failing. Early detection allows for graceful traffic redirection before performance degrades severely.</p>\n<table>\n<thead>\n<tr>\n<th>Health Check Type</th>\n<th>Frequency</th>\n<th>Failure Threshold</th>\n<th>Detection Time</th>\n<th>False Positive Rate</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic connectivity ping</td>\n<td>Every 5 seconds</td>\n<td>3 consecutive failures</td>\n<td>15-20 seconds</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Rate limit operation check</td>\n<td>Every 30 seconds</td>\n<td>5 failures in 2 minutes</td>\n<td>1-3 minutes</td>\n<td>Very Low</td>\n</tr>\n<tr>\n<td>Memory pressure monitoring</td>\n<td>Every 60 seconds</td>\n<td>&gt;90% memory usage</td>\n<td>2-5 minutes</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Latency threshold monitoring</td>\n<td>Continuous</td>\n<td>&gt;100ms for &gt;1 minute</td>\n<td>1-2 minutes</td>\n<td>Medium-High</td>\n</tr>\n</tbody></table>\n<h4 id=\"circuit-breaker-pattern-implementation\">Circuit Breaker Pattern Implementation</h4>\n<p>The circuit breaker pattern protects the rate limiting system from cascading failures by automatically stopping requests to failed Redis nodes and routing traffic to healthy alternatives. Circuit breakers prevent the system from repeatedly attempting operations against known-failed nodes, which would add latency and consume resources without providing value.</p>\n<p><strong>Circuit states</strong> define the operational mode for each Redis node connection:</p>\n<ul>\n<li><p><strong>Closed state</strong>: Normal operation where all requests are sent to the Redis node. Connection failures increment failure counters, but the circuit remains closed until failure thresholds are exceeded.</p>\n</li>\n<li><p><strong>Open state</strong>: All requests to the Redis node are immediately rejected without attempting connection. The circuit breaker routes traffic to alternative nodes or triggers local fallback behavior. The circuit remains open for a configurable timeout period (typically 30-60 seconds).</p>\n</li>\n<li><p><strong>Half-open state</strong>: After the timeout period expires, the circuit allows a limited number of test requests to determine if the Redis node has recovered. If test requests succeed, the circuit closes and normal operation resumes. If test requests fail, the circuit returns to the open state for another timeout period.</p>\n</li>\n</ul>\n<p><strong>Failure threshold configuration</strong> determines when circuits open based on error rates and timing patterns. Typical configurations open circuits after 5 consecutive failures or 50% error rate over a 2-minute sliding window. The system must balance rapid failure detection against temporary network issues that resolve quickly.</p>\n<p><strong>Recovery validation</strong> ensures that nodes are genuinely healthy before resuming full traffic. Half-open state test requests perform actual rate limiting operations rather than simple ping commands, verifying that the node can handle real workload before closing the circuit.</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Request Handling</th>\n<th>Failure Counting</th>\n<th>State Transition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>Send all requests to node</td>\n<td>Increment on failure, reset on success</td>\n<td>Open after threshold exceeded</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>Reject immediately, route to alternatives</td>\n<td>No requests sent</td>\n<td>Half-open after timeout</td>\n</tr>\n<tr>\n<td>Half-open</td>\n<td>Send limited test requests</td>\n<td>Evaluate test request results</td>\n<td>Closed on success, Open on failure</td>\n</tr>\n</tbody></table>\n<h4 id=\"automatic-failover-coordination\">Automatic Failover Coordination</h4>\n<p>When Redis nodes fail, the consistent hash ring must automatically redirect affected keys to healthy nodes while maintaining rate limit accuracy and avoiding split-brain scenarios where multiple nodes believe they are responsible for the same keys.</p>\n<p><strong>Immediate traffic redirection</strong> routes new rate limit requests away from failed nodes using the next available node clockwise on the consistent hash ring. This redirection happens automatically since each application instance independently detects node failures and updates its local hash ring state. No central coordination is required for basic traffic redirection.</p>\n<p><strong>State migration coordination</strong> attempts to preserve rate limit state from failed nodes when possible. If a node failure is detected early enough, the system may attempt to read current rate limit counter values and migrate them to the newly responsible nodes. However, this migration is best-effort only—the system prioritizes availability over perfect accuracy during failure scenarios.</p>\n<p><strong>Split-brain prevention</strong> ensures that when failed nodes recover, they do not create conflicting rate limit state. Recovered nodes must synchronize with the current cluster state and determine which key ranges they are currently responsible for according to the updated hash ring. Any rate limit state for keys that were reassigned during the failure must be discarded or merged carefully to prevent double-counting.</p>\n<blockquote>\n<p><strong>Decision: Failover Strategy Priority</strong></p>\n<ul>\n<li><strong>Context</strong>: Must balance rate limit accuracy against system availability during node failures</li>\n<li><strong>Options Considered</strong>: Perfect accuracy with downtime, immediate availability with temporary inaccuracy, or hybrid approach with best-effort state preservation</li>\n<li><strong>Decision</strong>: Prioritize availability with best-effort state preservation</li>\n<li><strong>Rationale</strong>: Rate limiting systems must remain operational during infrastructure failures. Temporary rate limit inaccuracy (allowing slightly more requests than configured limits) is preferable to complete service outage. Perfect accuracy is impossible to guarantee during arbitrary failure scenarios.</li>\n<li><strong>Consequences</strong>: Enables high availability during Redis failures at the cost of temporary rate limit violations. Requires careful monitoring and alerting to detect and respond to accuracy degradation.</li>\n</ul>\n</blockquote>\n<h4 id=\"recovery-and-reintegration-process\">Recovery and Reintegration Process</h4>\n<p>When failed Redis nodes recover and rejoin the cluster, they must be reintegrated carefully to avoid disrupting ongoing operations or creating inconsistent state. The recovery process coordinates between the recovered node, currently active nodes, and all application instances to restore normal hash ring operation.</p>\n<p><strong>Health validation</strong> verifies that recovered nodes are genuinely stable before accepting production traffic. Recovered nodes undergo extended health checking (typically 5-10 minutes of successful operations) before being marked as fully available. During this validation period, they may receive limited test traffic but are not included in production hash ring calculations.</p>\n<p><strong>State synchronization</strong> attempts to restore accurate rate limit state for keys that will return to the recovered node&#39;s responsibility. The system identifies key ranges that belonged to the recovered node before its failure and attempts to retrieve current counter values from the nodes that handled them during the failure. This synchronization is best-effort and may not be possible for all keys.</p>\n<p><strong>Gradual traffic migration</strong> slowly shifts rate limit operations back to recovered nodes rather than immediately resuming full traffic. This gradual approach prevents overwhelming recently recovered nodes and provides opportunity to detect any residual instability before full reintegration.</p>\n<p><strong>Consistency reconciliation</strong> resolves any conflicts between rate limit state on recovered nodes and current state managed by other nodes during the failure. In most cases, the current state from active nodes takes precedence, but the system may need to perform merging operations for keys that continued to receive updates on both the recovered node and its failover replacement.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Uneven Virtual Node Distribution</strong>\nMany implementations distribute virtual nodes unevenly across the hash ring, creating hot spots even with consistent hashing. This happens when virtual node generation uses poor hash functions or insufficient randomization. The symptom is persistent load imbalance despite having many virtual nodes. Fix this by using cryptographic hash functions (SHA-256) with proper salt values for virtual node generation, and validate distribution evenness during testing.</p>\n<p>⚠️ <strong>Pitfall: Hot Key Detection False Positives</strong>\nAggressive hot key detection can trigger unnecessary replication during normal traffic spikes, wasting resources and adding complexity. This occurs when detection thresholds are too low or time windows are too short. The symptom is frequent hot key alerts during peak hours for keys that don&#39;t actually create bottlenecks. Fix this by implementing adaptive thresholds based on overall cluster load and requiring sustained hot key patterns before triggering replication.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Hash Ring State During Failures</strong>\nApplication instances can maintain different views of hash ring topology during network partitions or rapid node failures, leading to rate limit checks being sent to wrong nodes. This manifests as sudden spikes in Redis errors or rate limit inaccuracy. Implement hash ring versioning and periodic synchronization to detect and resolve inconsistent states, with fallback to local rate limiting when uncertainty is detected.</p>\n<p>⚠️ <strong>Pitfall: Circuit Breaker Oscillation</strong>\nPoorly configured circuit breakers can oscillate rapidly between open and closed states when Redis nodes are intermittently failing, creating unstable performance. This happens when failure thresholds are too sensitive or recovery validation is insufficient. Symptoms include frequent circuit state changes and inconsistent response times. Fix this by implementing exponential backoff for circuit state changes and requiring sustained health before closing circuits.</p>\n<p>⚠️ <strong>Pitfall: State Loss During Node Recovery</strong>\nWhen failed nodes recover, their stale rate limit state can overwrite more recent state from failover nodes, causing incorrect rate limit calculations. This occurs when recovery processes don&#39;t properly synchronize state or determine key ownership. Implement proper state merging logic that prioritizes more recent timestamps and validates key ownership before accepting recovered state.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hash Function</td>\n<td><code>crypto/sha256</code> with hex encoding</td>\n<td><code>github.com/spaolacci/murmur3</code> for performance</td>\n</tr>\n<tr>\n<td>Virtual Node Management</td>\n<td>In-memory slice with binary search</td>\n<td><code>github.com/serialx/hashring</code> library</td>\n</tr>\n<tr>\n<td>Health Checking</td>\n<td>Simple ping with <code>go-redis</code></td>\n<td><code>github.com/sony/gobreaker</code> circuit breaker</td>\n</tr>\n<tr>\n<td>Node Discovery</td>\n<td>Static configuration file</td>\n<td><code>github.com/hashicorp/consul</code> for service discovery</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Basic counters in memory</td>\n<td><code>github.com/prometheus/client_golang</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  sharding/\n    ring.go                    ← consistent hash ring implementation\n    ring_test.go              ← ring distribution and rebalancing tests\n    virtual_nodes.go          ← virtual node management\n    hot_keys.go               ← hot key detection and replication\n    health_checker.go         ← Redis node health monitoring\n    circuit_breaker.go        ← circuit breaker pattern implementation\n  redis/\n    cluster_client.go         ← Redis cluster client wrapper\n    failover.go              ← automatic failover coordination\n    migration.go             ← key migration during rebalancing\n  config/\n    sharding_config.go        ← sharding configuration structures</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Consistent Hash Ring Infrastructure</strong> (complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> sharding</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha256</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HashRing represents a consistent hash ring with virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HashRing</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ring         []</span><span style=\"color:#F97583\">uint32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeWeights  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    virtualNodes </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHashRing creates a new consistent hash ring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHashRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">virtualNodes</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ring:         </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodes:        </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodeWeights:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        virtualNodes: virtualNodes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// hashKey generates a hash for a key or virtual node identifier</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">hashKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sha256.</span><span style=\"color:#B392F0\">Sum256</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(key))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(h[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">24</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(h[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(h[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(h[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddNode adds a physical node to the ring with virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">weight</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hr.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hr.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Remove existing virtual nodes for this physical node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hr.</span><span style=\"color:#B392F0\">removeNodeUnsafe</span><span style=\"color:#E1E4E8\">(nodeID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add virtual nodes for this physical node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> hr.virtualNodes</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">weight; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        virtualKey </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">:vnode:</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, nodeID, i)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hr.</span><span style=\"color:#B392F0\">hashKey</span><span style=\"color:#E1E4E8\">(virtualKey)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hr.ring </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(hr.ring, hash)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hr.nodes[hash] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nodeID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hr.nodeWeights[nodeID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> weight</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sort.</span><span style=\"color:#B392F0\">Slice</span><span style=\"color:#E1E4E8\">(hr.ring, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">j</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> hr.ring[i] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> hr.ring[j]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RemoveNode removes a physical node and all its virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RemoveNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hr.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hr.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hr.</span><span style=\"color:#B392F0\">removeNodeUnsafe</span><span style=\"color:#E1E4E8\">(nodeID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// removeNodeUnsafe removes a node without locking (internal helper)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">removeNodeUnsafe</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    newRing </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hr.ring))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, hash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hr.ring {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> hr.nodes[hash] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> nodeID {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            newRing </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(newRing, hash)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            delete</span><span style=\"color:#E1E4E8\">(hr.nodes, hash)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hr.ring </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> newRing</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    delete</span><span style=\"color:#E1E4E8\">(hr.nodeWeights, nodeID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNode returns the responsible node for a given key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hr.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hr.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(hr.ring) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hr.</span><span style=\"color:#B392F0\">hashKey</span><span style=\"color:#E1E4E8\">(key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    idx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sort.</span><span style=\"color:#B392F0\">Search</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hr.ring), </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> hr.ring[i] </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> hash</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> idx </span><span style=\"color:#F97583\">==</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(hr.ring) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        idx </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">  // wrap around to beginning</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hr.nodes[hr.ring[idx]], </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNodes returns N responsible nodes for a key (for replication)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNodes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hr.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hr.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(hr.ring) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> count </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hr.</span><span style=\"color:#B392F0\">hashKey</span><span style=\"color:#E1E4E8\">(key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    idx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sort.</span><span style=\"color:#B392F0\">Search</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hr.ring), </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> hr.ring[i] </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> hash</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> idx </span><span style=\"color:#F97583\">==</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(hr.ring) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        idx </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, count)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seen </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(result) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> count </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(seen) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(hr.nodeWeights) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodeID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hr.nodes[hr.ring[idx]]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">seen[nodeID] {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(result, nodeID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            seen[nodeID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        idx </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (idx </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">%</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(hr.ring)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Circuit Breaker Implementation</strong> (complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> sharding</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitClosed</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitOpen</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CircuitBreaker implements the circuit breaker pattern for Redis nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu                </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state            </span><span style=\"color:#B392F0\">CircuitState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureCount     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFailureTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nextRetryTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureThreshold </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recoveryTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    halfOpenMaxCalls </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    halfOpenCalls    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    halfOpenSuccesses </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCircuitBreaker creates a new circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCircuitBreaker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">failureThreshold</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">recoveryTimeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state:            CircuitClosed,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        failureThreshold: failureThreshold,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        recoveryTimeout:  recoveryTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        halfOpenMaxCalls: </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Execute runs a function with circuit breaker protection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Execute</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fn</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">cb.</span><span style=\"color:#B392F0\">allowRequest</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"circuit breaker is open\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> fn</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.</span><span style=\"color:#B392F0\">recordResult</span><span style=\"color:#E1E4E8\">(err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// allowRequest determines if a request should be allowed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">allowRequest</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> cb.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    now </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> cb.state {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> CircuitClosed:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> CircuitOpen:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">After</span><span style=\"color:#E1E4E8\">(cb.nextRetryTime) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.halfOpenCalls </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.halfOpenSuccesses </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> CircuitHalfOpen:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> cb.halfOpenCalls </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> cb.halfOpenMaxCalls</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// recordResult records the result of a request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recordResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">success</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> cb.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> cb.state {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> CircuitClosed:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> success {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.failureCount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.failureCount</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.lastFailureTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> cb.failureCount </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> cb.failureThreshold {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cb.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cb.nextRetryTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(cb.recoveryTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> CircuitHalfOpen:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.halfOpenCalls</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> success {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.halfOpenSuccesses</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> cb.halfOpenCalls </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> cb.halfOpenMaxCalls {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> cb.halfOpenSuccesses </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> cb.halfOpenMaxCalls {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cb.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitClosed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cb.failureCount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cb.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cb.nextRetryTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(cb.recoveryTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetState returns current circuit state (for monitoring)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetState</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">CircuitState</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> cb.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> cb.state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Hot Key Detection Implementation</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// HotKeyDetector monitors request patterns and identifies hot keys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HotKeyDetector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keyStats        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">KeyStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clusterStats    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config          </span><span style=\"color:#B392F0\">HotKeyConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    replicationChan </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// KeyStats tracks request frequency for individual keys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> KeyStats</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestCount1Min  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestCount5Min  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestCount15Min </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastUpdate        </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WindowBuckets     []</span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // Sliding window buckets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DetectHotKeys analyzes current key statistics and identifies hot keys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hkd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HotKeyDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DetectHotKeys</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get current cluster-wide request statistics to establish baseline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate dynamic threshold based on total cluster requests per minute</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Iterate through all tracked keys and check their request rates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Compare each key's rate against both absolute and relative thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: For keys exceeding thresholds, verify they exceed minimum duration requirement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return list of confirmed hot keys for replication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Hot key threshold = max(absolute_minimum, cluster_rps * hot_key_percentage)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordKeyAccess increments access counters for a specific rate limit key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hkd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HotKeyDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordKeyAccess</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get or create KeyStats struct for this key in thread-safe manner</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Update current minute bucket in sliding window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Increment counters for 1min, 5min, and 15min windows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update LastUpdate timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If key stats indicate potential hot key, trigger async detection check</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use atomic operations for counter updates to avoid lock contention</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReplicateHotKey creates replicas of hot key across multiple nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hkd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HotKeyDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ReplicateHotKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">replicationFactor</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Determine current responsible node for this key using hash ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Select additional nodes for replication using GetNodes(key, replicationFactor)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Read current rate limit state from primary node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create replica entries on selected nodes with read-only markers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update local routing table to include replica locations for read queries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Set up periodic synchronization from primary to replicas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Replicas should handle reads only; all writes go to primary node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Node Health Checker Implementation</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// HealthChecker monitors Redis node health and manages circuit breakers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthChecker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes           </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeHealth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreakers </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config          </span><span style=\"color:#B392F0\">HealthConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeHealth tracks health status for individual Redis nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeHealth</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastSeen         </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsecutiveFailures </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AverageLatency   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IsHealthy        </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryUsage      </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConnectionCount  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckNodeHealth performs comprehensive health check on a Redis node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckNodeHealth</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeHealth</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Perform basic connectivity ping to Redis node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute lightweight rate limit operation with synthetic key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Measure response latency and compare against thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Query Redis INFO command for memory usage and connection stats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update NodeHealth struct with current status and timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Determine overall health status based on all checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update circuit breaker state based on health check results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context.WithTimeout to prevent health checks from hanging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartHealthMonitoring begins periodic health checking for all nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartHealthMonitoring</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create ticker for periodic health checks (every 30 seconds)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each registered node, perform health check in separate goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Collect health check results and update node status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Trigger failover notifications for newly failed nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update hash ring to exclude failed nodes from routing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle context cancellation for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use errgroup to manage multiple concurrent health checks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleNodeFailure coordinates response to detected node failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleNodeFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Mark node as failed in health tracking structures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Open circuit breaker for this node to prevent further requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Remove node from consistent hash ring to stop routing new requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Identify keys that were handled by failed node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Notify key migration system to redirect affected keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Send alerts/notifications about node failure to monitoring systems</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Failure handling should be idempotent in case of multiple detection events</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h4>\n<p><strong>After implementing consistent hash ring</strong>:</p>\n<ul>\n<li>Run <code>go test ./internal/sharding/ring_test.go -v</code> to verify hash distribution</li>\n<li>Test with 4 nodes and 100,000 keys - each node should handle 20-30% of keys</li>\n<li>Add a 5th node - verify only ~20% of keys move to the new node</li>\n<li>Remove a node - verify keys redistribute to adjacent nodes only</li>\n</ul>\n<p><strong>After implementing hot key detection</strong>:</p>\n<ul>\n<li>Create test scenario with 1000 RPS to key &quot;user:popular&quot; and 10 RPS to other keys</li>\n<li>Monitor detection system - should identify &quot;user:popular&quot; as hot within 2 minutes</li>\n<li>Verify hot key replication creates read replicas on 2-3 additional nodes</li>\n<li>Test that read queries distribute across replicas while writes go to primary</li>\n</ul>\n<p><strong>After implementing health checking and failover</strong>:</p>\n<ul>\n<li>Start 3 Redis nodes and verify all show as healthy</li>\n<li>Stop one Redis node - should detect failure within 30-60 seconds</li>\n<li>Verify circuit breaker opens and traffic routes to remaining healthy nodes</li>\n<li>Restart failed node - should detect recovery and resume routing within 2-3 minutes</li>\n</ul>\n<p><strong>Signs something is wrong</strong>:</p>\n<ul>\n<li>Hash ring distribution variance &gt;20% indicates poor virtual node generation</li>\n<li>Hot key detection triggering for normal keys indicates thresholds too low</li>\n<li>Health checks showing false failures indicates network timeouts too aggressive</li>\n<li>Keys routing to wrong nodes during failures indicates hash ring inconsistency</li>\n</ul>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, 2, 3, 4, 5 - this section demonstrates how all components work together across rate limiting algorithms, multi-tier evaluation, Redis integration, sharding, and API management</p>\n</blockquote>\n<p>Understanding the interactions between components in a distributed rate limiting system is crucial for building a robust and performant implementation. This section explores the three primary data flows that define how the system operates: rate limit checking, configuration management, and metrics collection.</p>\n<h3 id=\"mental-model-the-air-traffic-control-system\">Mental Model: The Air Traffic Control System</h3>\n<p>Think of the distributed rate limiter as an air traffic control system managing aircraft (requests) across multiple airports (application instances) with a central coordination center (Redis cluster). When an aircraft requests landing clearance, the local tower (application instance) must coordinate with the central system to check runway capacity (rate limits), update the flight schedule (counters), and ensure safe separation (prevent overload). Configuration updates flow like weather advisories - broadcast from central meteorology (management API) to all towers simultaneously. Meanwhile, flight statistics (metrics) continuously stream back to central command for monitoring and decision-making.</p>\n<p>This analogy helps illustrate the critical coordination patterns: real-time decision making with shared state, configuration propagation across distributed nodes, and continuous monitoring of system health. The key insight is that each local decision requires global coordination, but the system must remain fast enough for real-time operation.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<h3 id=\"rate-limit-check-flow\">Rate Limit Check Flow</h3>\n<p>The rate limit check flow represents the most performance-critical path in the distributed rate limiting system. Every incoming request must be evaluated against potentially multiple rate limit rules, with the decision made in milliseconds while maintaining consistency across all application instances.</p>\n<h4 id=\"request-context-assembly\">Request Context Assembly</h4>\n<p>The rate limit check begins when an HTTP request arrives at any application instance. The <code>DistributedLimiter</code> component extracts relevant context information to construct a <code>RateLimitRequest</code> structure. This context assembly process is crucial because it determines which rate limit rules will be evaluated and how the request will be categorized.</p>\n<table>\n<thead>\n<tr>\n<th>Context Field</th>\n<th>Source</th>\n<th>Purpose</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>user_id</code></td>\n<td>Authentication header/JWT</td>\n<td>Per-user rate limiting</td>\n<td>&quot;user_12345&quot;</td>\n</tr>\n<tr>\n<td><code>ip_address</code></td>\n<td>Request remote address</td>\n<td>Per-IP rate limiting</td>\n<td>&quot;192.168.1.100&quot;</td>\n</tr>\n<tr>\n<td><code>api_endpoint</code></td>\n<td>Request path pattern</td>\n<td>Per-API rate limiting</td>\n<td>&quot;/api/v1/users&quot;</td>\n</tr>\n<tr>\n<td><code>user_agent</code></td>\n<td>HTTP header</td>\n<td>Bot detection/classification</td>\n<td>&quot;Mozilla/5.0...&quot;</td>\n</tr>\n<tr>\n<td><code>tokens</code></td>\n<td>Request payload size/type</td>\n<td>Resource-aware limiting</td>\n<td>1 or payload_size</td>\n</tr>\n</tbody></table>\n<p>The context assembly must handle edge cases like missing authentication (anonymous users), proxy forwarding (X-Forwarded-For headers), and API path normalization (removing parameters). The <code>KeyComposer</code> component uses this context to generate Redis keys that will be used for rate limit state storage.</p>\n<h4 id=\"multi-tier-rule-evaluation\">Multi-Tier Rule Evaluation</h4>\n<p>Once the request context is assembled, the <code>MultiTierLimiter</code> begins evaluating applicable rate limit rules. The system follows a short-circuit evaluation strategy, checking rules in priority order and stopping immediately when any limit is exceeded.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fmulti-tier-evaluation.svg\" alt=\"Multi-Tier Rate Limit Evaluation\"></p>\n<p>The tier evaluation follows this algorithmic sequence:</p>\n<ol>\n<li><p><strong>Rule Discovery</strong>: The <code>RuleManager</code> scans all configured rules and identifies those whose <code>key_pattern</code> matches the request context. Rules are sorted by priority (highest first) to ensure most specific limits are checked before general ones.</p>\n</li>\n<li><p><strong>Key Composition</strong>: For each matching rule, the <code>KeyComposer</code> generates a Redis key using the rule&#39;s pattern and request context. This involves pattern substitution and namespace prefixing to ensure key uniqueness across different rate limit dimensions.</p>\n</li>\n<li><p><strong>Tier Evaluation Loop</strong>: The system iterates through matching rules, performing rate limit checks for each tier:</p>\n<ul>\n<li>User-specific limits (highest priority)</li>\n<li>IP-address limits (medium priority)  </li>\n<li>API endpoint limits (medium priority)</li>\n<li>Global system limits (lowest priority)</li>\n</ul>\n</li>\n<li><p><strong>Short-Circuit Logic</strong>: If any rule evaluation returns <code>allowed: false</code>, the entire check immediately fails without evaluating remaining rules. This optimization reduces Redis operations and provides faster response times for requests that exceed limits.</p>\n</li>\n<li><p><strong>Result Aggregation</strong>: If all rules pass, the system returns the most restrictive result (lowest remaining count) to provide accurate rate limit headers in the response.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Evaluation Step</th>\n<th>Action</th>\n<th>Success Path</th>\n<th>Failure Path</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Discovery</td>\n<td>Find matching rules by pattern</td>\n<td>Continue to next step</td>\n<td>Return allowed=true (no rules)</td>\n</tr>\n<tr>\n<td>Key Composition</td>\n<td>Generate Redis keys</td>\n<td>Continue to tier evaluation</td>\n<td>Return error</td>\n</tr>\n<tr>\n<td>Per-User Check</td>\n<td>Check user-specific limits</td>\n<td>Continue to next tier</td>\n<td>Return allowed=false</td>\n</tr>\n<tr>\n<td>Per-IP Check</td>\n<td>Check IP-based limits</td>\n<td>Continue to next tier</td>\n<td>Return allowed=false</td>\n</tr>\n<tr>\n<td>Per-API Check</td>\n<td>Check endpoint limits</td>\n<td>Continue to next tier</td>\n<td>Return allowed=false</td>\n</tr>\n<tr>\n<td>Global Check</td>\n<td>Check system-wide limits</td>\n<td>Return allowed=true</td>\n<td>Return allowed=false</td>\n</tr>\n</tbody></table>\n<h4 id=\"redis-atomic-operations\">Redis Atomic Operations</h4>\n<p>The core of each individual rate limit check is an atomic Redis operation implemented as a Lua script. This atomicity is essential for maintaining accurate counters in a distributed environment where multiple application instances may be checking the same rate limit simultaneously.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Frate-check-sequence.svg\" alt=\"Rate Limit Check Sequence\"></p>\n<p>Each algorithm uses a specialized Lua script optimized for its specific requirements:</p>\n<p><strong>Token Bucket Script Flow:</strong></p>\n<ol>\n<li>Load current bucket state (<code>tokens</code>, <code>last_refill_time</code>) from Redis</li>\n<li>Calculate elapsed time since last refill and compute tokens to add</li>\n<li>Refill bucket up to capacity based on configured refill rate</li>\n<li>Check if requested tokens are available</li>\n<li>If available, subtract tokens and update state; if not, return current state</li>\n<li>Return result with remaining tokens and reset time</li>\n</ol>\n<p><strong>Sliding Window Counter Script Flow:</strong></p>\n<ol>\n<li>Calculate current time window and previous window boundaries</li>\n<li>Load counters for current and previous time windows</li>\n<li>Calculate weighted count based on time position within current window</li>\n<li>Check if adding this request would exceed the limit</li>\n<li>If within limit, increment current window counter</li>\n<li>Clean up expired window data to prevent memory leaks</li>\n<li>Return result with remaining capacity</li>\n</ol>\n<p><strong>Sliding Window Log Script Flow:</strong></p>\n<ol>\n<li>Remove expired timestamps from the request log</li>\n<li>Count remaining timestamps to determine current usage</li>\n<li>Check if adding this request would exceed the limit</li>\n<li>If within limit, add current timestamp to the log</li>\n<li>Return result with current usage and remaining capacity</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Script Operation</th>\n<th>Atomicity Requirement</th>\n<th>Performance Impact</th>\n<th>Memory Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket</td>\n<td>State read + refill + check + update</td>\n<td>Low (simple arithmetic)</td>\n<td>Fixed per key</td>\n</tr>\n<tr>\n<td>Sliding Counter</td>\n<td>Multi-window read + weighted calc + update</td>\n<td>Medium (window math)</td>\n<td>Fixed per window</td>\n</tr>\n<tr>\n<td>Sliding Log</td>\n<td>Log read + filter + count + append</td>\n<td>High (list operations)</td>\n<td>Grows with traffic</td>\n</tr>\n</tbody></table>\n<h4 id=\"local-fallback-handling\">Local Fallback Handling</h4>\n<p>When Redis becomes unavailable, the system must gracefully degrade to local rate limiting rather than failing open (allowing all requests) or closed (rejecting all requests). This graceful degradation maintains partial functionality while preserving system stability.</p>\n<p>The fallback detection and activation follows this sequence:</p>\n<ol>\n<li><p><strong>Failure Detection</strong>: Redis operations fail with connection timeout, network error, or other exceptions. The <code>CircuitBreaker</code> component tracks failure rates and automatically opens when thresholds are exceeded.</p>\n</li>\n<li><p><strong>Fallback Activation</strong>: The <code>DistributedLimiter</code> switches to its <code>localFallback</code> limiter instance, which maintains in-memory rate limit state using the same algorithms but with local scope.</p>\n</li>\n<li><p><strong>State Synchronization</strong>: When Redis connectivity is restored, the system must carefully synchronize local and distributed state to prevent double-counting or lost counts.</p>\n</li>\n<li><p><strong>Recovery Process</strong>: The circuit breaker gradually allows test requests through to verify Redis health before fully reopening.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Local fallback cannot maintain global accuracy, but it preserves system availability and prevents cascading failures. The trade-off between accuracy and availability is an explicit architectural choice.</p>\n</blockquote>\n<h4 id=\"response-header-generation\">Response Header Generation</h4>\n<p>The final step in the rate limit check flow is generating standard HTTP response headers that inform clients about their current rate limit status. These headers follow established conventions and enable clients to implement intelligent backoff strategies.</p>\n<table>\n<thead>\n<tr>\n<th>Header Name</th>\n<th>Value Format</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>X-RateLimit-Limit</code></td>\n<td>Integer</td>\n<td>Maximum requests in window</td>\n<td>&quot;1000&quot;</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Remaining</code></td>\n<td>Integer</td>\n<td>Requests left in current window</td>\n<td>&quot;247&quot;</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Reset</code></td>\n<td>Unix timestamp</td>\n<td>When window resets</td>\n<td>&quot;1699123456&quot;</td>\n</tr>\n<tr>\n<td><code>Retry-After</code></td>\n<td>Seconds</td>\n<td>How long to wait if limited</td>\n<td>&quot;60&quot;</td>\n</tr>\n</tbody></table>\n<h3 id=\"configuration-update-propagation\">Configuration Update Propagation</h3>\n<p>Configuration management in a distributed rate limiting system presents unique challenges: changes must be propagated to all application instances quickly and consistently, while the system continues processing requests without interruption.</p>\n<h4 id=\"mental-model-emergency-broadcast-system\">Mental Model: Emergency Broadcast System</h4>\n<p>Think of configuration updates like an emergency broadcast system. When new rate limit rules are created or modified (emergency announcement), they must reach every application instance (radio station) reliably and quickly. Each instance must validate the message authenticity (configuration schema), update its local understanding (rule cache), and begin operating under the new rules immediately. Failed deliveries are retried, and the system tracks which instances have received updates to ensure full coverage.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fconfiguration-propagation-flow.svg\" alt=\"Configuration Update Flow\"></p>\n<h4 id=\"configuration-source-and-validation\">Configuration Source and Validation</h4>\n<p>The configuration update process begins at the management API, which serves as the authoritative source for all rate limit rules. When administrators create, modify, or delete rate limit rules, the API performs comprehensive validation before persisting changes.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Check</th>\n<th>Purpose</th>\n<th>Failure Action</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Schema Validation</td>\n<td>Ensure required fields present</td>\n<td>Reject with 400 error</td>\n<td>Missing <code>limit</code> field</td>\n</tr>\n<tr>\n<td>Pattern Syntax</td>\n<td>Validate regex patterns</td>\n<td>Reject with 400 error</td>\n<td>Invalid regex: <code>[unclosed</code></td>\n</tr>\n<tr>\n<td>Limit Bounds</td>\n<td>Check reasonable limit values</td>\n<td>Reject with 400 error</td>\n<td>Negative rate limit</td>\n</tr>\n<tr>\n<td>Priority Conflicts</td>\n<td>Prevent overlapping priorities</td>\n<td>Reject with 409 error</td>\n<td>Two rules same priority</td>\n</tr>\n<tr>\n<td>Algorithm Support</td>\n<td>Verify algorithm exists</td>\n<td>Reject with 400 error</td>\n<td>Unknown algorithm name</td>\n</tr>\n</tbody></table>\n<p>The validation process uses a staged approach where syntactic validation occurs first (fast checks), followed by semantic validation (cross-rule consistency), and finally persistence validation (storage constraints). This ordering minimizes wasted work when validation fails.</p>\n<h4 id=\"redis-as-configuration-distribution-hub\">Redis as Configuration Distribution Hub</h4>\n<p>The system uses Redis not only for rate limit state storage but also as a configuration distribution mechanism. This design choice leverages Redis&#39;s existing high availability and clustering capabilities while providing real-time updates to all application instances.</p>\n<p>The configuration distribution follows a publish-subscribe pattern combined with persistent storage:</p>\n<ol>\n<li><p><strong>Persistent Storage</strong>: Rate limit rules are stored in Redis using hash structures, allowing atomic updates and version tracking.</p>\n</li>\n<li><p><strong>Change Notification</strong>: When rules are updated, the management API publishes a notification to a Redis channel announcing the change.</p>\n</li>\n<li><p><strong>Instance Subscription</strong>: All application instances subscribe to the configuration change channel and receive real-time notifications.</p>\n</li>\n<li><p><strong>Incremental Updates</strong>: Instances fetch only changed rules rather than reloading entire configurations, reducing network overhead and update latency.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Redis Structure</th>\n<th>Key Pattern</th>\n<th>Purpose</th>\n<th>Data Format</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Storage</td>\n<td><code>config:rules:{rule_id}</code></td>\n<td>Persistent rule data</td>\n<td>Hash with all rule fields</td>\n</tr>\n<tr>\n<td>Version Tracking</td>\n<td><code>config:version</code></td>\n<td>Change detection</td>\n<td>Integer timestamp</td>\n</tr>\n<tr>\n<td>Change Log</td>\n<td><code>config:changelog:{version}</code></td>\n<td>Audit trail</td>\n<td>JSON change description</td>\n</tr>\n<tr>\n<td>Heartbeat</td>\n<td><code>config:heartbeat:{instance_id}</code></td>\n<td>Instance health</td>\n<td>Timestamp + rule version</td>\n</tr>\n</tbody></table>\n<h4 id=\"application-instance-update-process\">Application Instance Update Process</h4>\n<p>Each application instance runs a configuration watcher component that maintains consistency between the central configuration store and local rule caches. This component must handle network failures, partial updates, and version conflicts gracefully.</p>\n<p>The update process for each application instance follows this sequence:</p>\n<ol>\n<li><p><strong>Change Detection</strong>: The instance receives a notification via Redis pub/sub or detects a version mismatch during periodic health checks.</p>\n</li>\n<li><p><strong>Version Comparison</strong>: The instance compares its local rule version against the central version to determine what updates are needed.</p>\n</li>\n<li><p><strong>Incremental Fetch</strong>: Only changed rules are fetched from Redis, using timestamps or version numbers to identify modifications.</p>\n</li>\n<li><p><strong>Atomic Local Update</strong>: The instance updates its in-memory rule cache atomically to prevent inconsistent state during rule evaluation.</p>\n</li>\n<li><p><strong>Validation and Rollback</strong>: After updating, the instance validates that all rules are correctly loaded and rolls back to the previous version if problems are detected.</p>\n</li>\n<li><p><strong>Heartbeat Update</strong>: The instance updates its heartbeat record in Redis to confirm successful configuration application.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Incremental vs Full Configuration Reload</strong></p>\n<ul>\n<li><strong>Context</strong>: Configuration changes need to propagate to potentially hundreds of application instances</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Full configuration reload on every change</li>\n<li>Incremental updates with change tracking</li>\n<li>Configuration push from central server</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Incremental updates with Redis pub/sub notification</li>\n<li><strong>Rationale</strong>: Minimizes network bandwidth, reduces update latency, and scales better with cluster size. Redis pub/sub provides reliable delivery with automatic retries.</li>\n<li><strong>Consequences</strong>: More complex change tracking logic, but significantly better performance and scalability.</li>\n</ul>\n</blockquote>\n<h4 id=\"configuration-validation-and-conflict-resolution\">Configuration Validation and Conflict Resolution</h4>\n<p>Distributed configuration management introduces the possibility of conflicts and inconsistencies that must be detected and resolved automatically. The system implements several mechanisms to ensure configuration consistency across all instances.</p>\n<table>\n<thead>\n<tr>\n<th>Conflict Type</th>\n<th>Detection Method</th>\n<th>Resolution Strategy</th>\n<th>Prevention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Version Skew</td>\n<td>Periodic heartbeat comparison</td>\n<td>Force full reload</td>\n<td>Timeout limits on updates</td>\n</tr>\n<tr>\n<td>Rule Priority Overlap</td>\n<td>Cross-rule validation</td>\n<td>Reject conflicting update</td>\n<td>Priority uniqueness check</td>\n</tr>\n<tr>\n<td>Pattern Ambiguity</td>\n<td>Pattern matching test</td>\n<td>Admin notification</td>\n<td>Test pattern coverage</td>\n</tr>\n<tr>\n<td>Resource Exhaustion</td>\n<td>Memory/performance monitoring</td>\n<td>Gradual rollout</td>\n<td>Capacity planning</td>\n</tr>\n</tbody></table>\n<p>The conflict resolution system operates on the principle of &quot;fail safely&quot; - when conflicts cannot be automatically resolved, the system maintains the last known good configuration while alerting administrators to the issue.</p>\n<h4 id=\"configuration-rollback-and-recovery\">Configuration Rollback and Recovery</h4>\n<p>The distributed nature of the system requires sophisticated rollback capabilities when configuration changes cause problems. The system maintains configuration history and provides mechanisms for rapid rollback across all instances.</p>\n<p>The rollback process works as follows:</p>\n<ol>\n<li><p><strong>Issue Detection</strong>: Monitoring systems detect increased error rates, performance degradation, or other problems following a configuration change.</p>\n</li>\n<li><p><strong>Rollback Trigger</strong>: Administrators or automated systems trigger a rollback to a previous configuration version.</p>\n</li>\n<li><p><strong>Coordinated Rollback</strong>: The management API publishes a rollback notification containing the target version number.</p>\n</li>\n<li><p><strong>Instance Rollback</strong>: Each application instance loads the specified historical configuration from its local cache or Redis backup.</p>\n</li>\n<li><p><strong>Verification</strong>: Instances report successful rollback via heartbeat updates, allowing centralized verification of rollback completion.</p>\n</li>\n</ol>\n<h3 id=\"metrics-collection-and-aggregation\">Metrics Collection and Aggregation</h3>\n<p>Comprehensive metrics collection is essential for operating a distributed rate limiting system effectively. The metrics system must capture detailed usage patterns, performance characteristics, and system health indicators while minimizing impact on the critical rate limiting path.</p>\n<h4 id=\"mental-model-hospital-vital-signs-monitoring\">Mental Model: Hospital Vital Signs Monitoring</h4>\n<p>Think of the metrics collection system like vital signs monitoring in a hospital. Each patient (rate limit key) has continuous monitoring of key indicators (request rates, rejection rates, latency). Nurses (application instances) collect readings regularly and report to a central monitoring station (metrics aggregator). Doctors (operators) use dashboards to spot trends, diagnose problems, and make treatment decisions. Critical alerts (threshold breaches) trigger immediate notifications, while long-term trends inform capacity planning and system optimization.</p>\n<h4 id=\"real-time-metrics-collection\">Real-Time Metrics Collection</h4>\n<p>The metrics collection system operates continuously alongside the rate limiting functionality, capturing both operational metrics (performance, errors) and business metrics (usage patterns, limit effectiveness). The collection must be designed for minimal performance impact while providing sufficient detail for troubleshooting and optimization.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Specific Metrics</th>\n<th>Collection Method</th>\n<th>Granularity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Metrics</td>\n<td><code>requests_total</code>, <code>requests_allowed</code>, <code>requests_denied</code></td>\n<td>Counter increment on each check</td>\n<td>Per rule, per instance</td>\n</tr>\n<tr>\n<td>Performance Metrics</td>\n<td><code>check_duration_ms</code>, <code>redis_latency_ms</code></td>\n<td>Histogram recording</td>\n<td>Per operation type</td>\n</tr>\n<tr>\n<td>System Health</td>\n<td><code>redis_connection_status</code>, <code>fallback_active</code></td>\n<td>Gauge sampling</td>\n<td>Per instance</td>\n</tr>\n<tr>\n<td>Usage Patterns</td>\n<td><code>top_keys_by_volume</code>, <code>limit_utilization_pct</code></td>\n<td>Periodic aggregation</td>\n<td>Per time window</td>\n</tr>\n</tbody></table>\n<p>Each application instance maintains local metric collectors that aggregate data over short time windows (typically 10-60 seconds) before transmitting to the central metrics system. This approach reduces network overhead while providing near-real-time visibility into system behavior.</p>\n<h4 id=\"hot-key-detection-and-analysis\">Hot Key Detection and Analysis</h4>\n<p>Hot key detection represents one of the most valuable capabilities of the metrics system. By identifying rate limit keys that receive disproportionate traffic, the system can automatically trigger replication, sharding adjustments, or capacity alerts.</p>\n<p>The <code>HotKeyDetector</code> component continuously analyzes request patterns using a multi-window approach:</p>\n<ol>\n<li><p><strong>Request Counting</strong>: Each application instance maintains counters for every rate limit key it processes, tracking request volumes over multiple time windows (1 minute, 5 minutes, 15 minutes).</p>\n</li>\n<li><p><strong>Statistical Analysis</strong>: The system calculates request rate percentiles across all keys to identify outliers that exceed configurable thresholds (e.g., 95th percentile).</p>\n</li>\n<li><p><strong>Trend Detection</strong>: Hot key detection considers not just absolute volumes but also growth rates to identify keys experiencing traffic spikes.</p>\n</li>\n<li><p><strong>Cluster-Wide Aggregation</strong>: Individual instance measurements are aggregated across the cluster to provide accurate global hot key identification.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Detection Window</th>\n<th>Purpose</th>\n<th>Threshold Type</th>\n<th>Action Triggered</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1 Minute</td>\n<td>Spike detection</td>\n<td>Absolute count &gt; P99</td>\n<td>Immediate replication</td>\n</tr>\n<tr>\n<td>5 Minutes</td>\n<td>Sustained load</td>\n<td>Rate increase &gt; 300%</td>\n<td>Load balancing adjustment</td>\n</tr>\n<tr>\n<td>15 Minutes</td>\n<td>Trend analysis</td>\n<td>Steady growth &gt; 50%</td>\n<td>Capacity planning alert</td>\n</tr>\n<tr>\n<td>1 Hour</td>\n<td>Baseline establishment</td>\n<td>Rolling average update</td>\n<td>Threshold recalibration</td>\n</tr>\n</tbody></table>\n<h4 id=\"dashboard-and-visualization\">Dashboard and Visualization</h4>\n<p>The real-time dashboard provides operators with comprehensive visibility into the rate limiting system&#39;s behavior. The dashboard must balance information density with usability, presenting critical information prominently while making detailed data easily accessible.</p>\n<p>The dashboard architecture uses WebSocket connections for real-time updates, with efficient data streaming that minimizes bandwidth usage. Key visualizations include:</p>\n<p><strong>System Overview Panel:</strong></p>\n<ul>\n<li>Total request rate across all instances</li>\n<li>Overall allow/deny ratio</li>\n<li>Active rate limit rules count</li>\n<li>Redis cluster health status</li>\n</ul>\n<p><strong>Top Keys Analysis:</strong></p>\n<ul>\n<li>Highest volume rate limit keys (by request count)</li>\n<li>Most restrictive keys (by denial rate)</li>\n<li>Hot keys detected in recent time windows</li>\n<li>Key growth trends over time</li>\n</ul>\n<p><strong>Performance Monitoring:</strong></p>\n<ul>\n<li>Rate limit check latency distribution</li>\n<li>Redis operation latency percentiles</li>\n<li>Circuit breaker status across instances</li>\n<li>Error rate trends by error type</li>\n</ul>\n<p><strong>Tier Analysis:</strong></p>\n<ul>\n<li>Effectiveness of different rate limit tiers</li>\n<li>Most frequently triggered rules</li>\n<li>Resource utilization by limit type</li>\n<li>Optimization recommendations</li>\n</ul>\n<h4 id=\"alerting-and-anomaly-detection\">Alerting and Anomaly Detection</h4>\n<p>The metrics system includes sophisticated alerting capabilities that can detect both immediate problems and developing issues before they impact users. The alerting system operates on multiple time horizons with different sensitivity levels.</p>\n<table>\n<thead>\n<tr>\n<th>Alert Type</th>\n<th>Detection Logic</th>\n<th>Severity</th>\n<th>Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High Error Rate</td>\n<td>Error rate &gt; 5% over 2 minutes</td>\n<td>Critical</td>\n<td>Immediate page</td>\n</tr>\n<tr>\n<td>Redis Unavailable</td>\n<td>Connection failures &gt; 50%</td>\n<td>Critical</td>\n<td>Immediate page</td>\n</tr>\n<tr>\n<td>Hot Key Spike</td>\n<td>Single key &gt; 10x normal rate</td>\n<td>Warning</td>\n<td>Auto-mitigation + alert</td>\n</tr>\n<tr>\n<td>Capacity Trend</td>\n<td>Usage growth &gt; 80% capacity</td>\n<td>Info</td>\n<td>Capacity planning notification</td>\n</tr>\n<tr>\n<td>Config Drift</td>\n<td>Instance config version skew</td>\n<td>Warning</td>\n<td>Auto-remediation attempt</td>\n</tr>\n</tbody></table>\n<p>The anomaly detection system uses statistical models to identify unusual patterns that might indicate attacks, misconfigurations, or system problems. These models adapt over time to account for normal traffic pattern evolution while maintaining sensitivity to genuine anomalies.</p>\n<h4 id=\"self-monitoring-rate-limits\">Self-Monitoring Rate Limits</h4>\n<p>A critical consideration for the metrics and management API is preventing the rate limiting system from overwhelming itself with monitoring traffic. The system implements self-monitoring rate limits to ensure that dashboard queries, metrics collection, and administrative operations don&#39;t interfere with primary rate limiting functionality.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The rate limiting system must protect itself from its own monitoring and management interfaces. This requires careful design of internal rate limits and circuit breakers.</p>\n</blockquote>\n<p>The self-monitoring system includes:</p>\n<p><strong>Dashboard Rate Limiting:</strong> WebSocket connections are rate limited to prevent a single dashboard user from overwhelming the metrics system with excessive query rates.</p>\n<p><strong>API Protection:</strong> Management API endpoints have their own rate limits to prevent configuration update storms or abusive administrative behavior.</p>\n<p><strong>Metrics Backpressure:</strong> When the metrics collection system becomes overwhelmed, it implements backpressure mechanisms to reduce collection frequency rather than dropping data.</p>\n<p><strong>Internal Circuit Breakers:</strong> Monitoring components include circuit breakers that disable non-essential metrics collection when the primary rate limiting system is under stress.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Race Conditions in Multi-Tier Evaluation</strong>\nMany implementations fail to properly coordinate between tier evaluations, leading to inconsistent state where one tier&#39;s counter is updated while another tier&#39;s update fails. This can result in &quot;phantom&quot; request counting where requests are partially counted across different tiers. The fix is to use two-phase operations: first check all tiers without updating, then update all tiers atomically, or implement compensating transactions for partial failures.</p>\n<p>⚠️ <strong>Pitfall: Configuration Update Ordering</strong>\nConfiguration updates can arrive at different instances in different orders due to network timing, causing temporary inconsistencies where the same request is evaluated differently by different instances. This is particularly dangerous when rule priorities change. The fix is to use version numbers and ensure all instances process configuration changes in the same order, potentially by using a consensus mechanism or designated configuration coordinator.</p>\n<p>⚠️ <strong>Pitfall: Metrics Collection Overwhelming Redis</strong>\nNaive metrics implementations can generate more Redis traffic than the actual rate limiting operations, especially with detailed per-key statistics. This can overwhelm Redis and degrade rate limiting performance. The solution is to batch metrics operations, use separate Redis instances for metrics vs rate limiting, and implement metrics sampling for high-volume keys.</p>\n<p>⚠️ <strong>Pitfall: Synchronous Configuration Updates Blocking Requests</strong>\nBlocking the request processing thread while updating configuration can cause significant latency spikes during configuration changes. The fix is to use asynchronous configuration updates with atomic cache swapping - load new configuration in the background and atomically replace the active configuration once fully validated.</p>\n<p>⚠️ <strong>Pitfall: Hardcoded Timeout Values</strong>\nUsing fixed timeout values for Redis operations doesn&#39;t account for varying network conditions or load. This can cause premature fallback activation or excessive blocking during temporary slowdowns. The solution is to implement adaptive timeouts based on recent latency percentiles and provide different timeout configurations for different operations (quick checks vs batch updates).</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation section provides complete infrastructure code and skeleton implementations for the core interaction patterns described above.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Client</td>\n<td><code>net/http</code> with connection pooling</td>\n<td><code>fasthttp</code> for high performance</td>\n</tr>\n<tr>\n<td>Configuration Storage</td>\n<td>Redis Hash structures</td>\n<td>etcd for stronger consistency</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Built-in counters with periodic export</td>\n<td>Prometheus client library</td>\n</tr>\n<tr>\n<td>Real-time Updates</td>\n<td>Redis Pub/Sub</td>\n<td>Apache Kafka for message ordering</td>\n</tr>\n<tr>\n<td>Dashboard Backend</td>\n<td>WebSocket with JSON</td>\n<td>gRPC with streaming</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure\">File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n├── flow/                           ← This component\n│   ├── coordinator.go             ← Main coordination logic\n│   ├── config_watcher.go          ← Configuration propagation\n│   ├── metrics_collector.go       ← Metrics collection and hot key detection\n│   └── health_checker.go          ← System health monitoring\n├── storage/                       ← Redis backend\n│   ├── redis_storage.go           ← Basic Redis operations\n│   └── lua_scripts.go             ← Atomic operation scripts\n└── api/                           ← Management interfaces\n    ├── handlers.go                ← REST API endpoints\n    └── websocket.go               ← Real-time dashboard connection</code></pre></div>\n\n<h4 id=\"request-flow-coordinator-infrastructure\">Request Flow Coordinator Infrastructure</h4>\n<p>This complete infrastructure handles request coordination across multiple rate limiting tiers with proper error handling and fallback logic.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package flow provides request coordination and data flow management</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// for distributed rate limiting operations across multiple tiers and backends.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> flow</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/rate-limiter/internal/config</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/rate-limiter/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FlowCoordinator manages the complete request flow from initial context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// assembly through multi-tier evaluation to response header generation.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FlowCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage        </span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keyComposer    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">KeyComposer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    algorithms     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localFallback  </span><span style=\"color:#B392F0\">Limiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metricsCollector </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsCollector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreaker </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu             </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    isHealthy      </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RequestContext holds all information needed for rate limit evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// extracted from the incoming HTTP request.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RequestContext</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"user_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IPAddress   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"ip_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    APIEndpoint </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"api_endpoint\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserAgent   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"user_agent\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Headers     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"headers\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tokens      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"tokens\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TierEvaluation represents the result of evaluating a single rate limit tier.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TierEvaluation</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RuleID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">               `json:\"rule_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TierName    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"tier_name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Result      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#9ECBFF\"> `json:\"result\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">        `json:\"duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RedisKey    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"redis_key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FlowResult contains the complete result of request flow processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// including all tier evaluations and final decision.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FlowResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Allowed       </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">               `json:\"allowed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TierResults   []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TierEvaluation</span><span style=\"color:#9ECBFF\">  `json:\"tier_results\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BlockingTier  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"blocking_tier,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalDuration </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">      `json:\"total_duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UsedFallback  </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">              `json:\"used_fallback\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Headers       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">  `json:\"headers\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">     `json:\"retry_after,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewFlowCoordinator creates a new coordinator with all required dependencies.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewFlowCoordinator</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    storage</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Storage</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    ruleManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    algorithms</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Algorithm</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    localFallback</span><span style=\"color:#B392F0\"> Limiter</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FlowCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">FlowCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:          storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ruleManager:      ruleManager,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        keyComposer:      config.</span><span style=\"color:#B392F0\">NewKeyComposer</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        algorithms:       algorithms,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        localFallback:    localFallback,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metricsCollector: </span><span style=\"color:#B392F0\">NewMetricsCollector</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        circuitBreaker:   </span><span style=\"color:#B392F0\">NewCircuitBreaker</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        isHealthy:        </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessRequest coordinates the complete flow from request context to final result.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FlowCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">reqCtx</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RequestContext</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FlowResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract and validate request context (IP, User, API endpoint)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get matching rules from rule manager sorted by priority</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check circuit breaker status - use fallback if open</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Evaluate each tier in priority order with short-circuit logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate standard HTTP headers for the response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Record metrics for request processing and hot key detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return comprehensive flow result with all evaluation details</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use fc.evaluateAllTiers() for the core multi-tier logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement short-circuit evaluation - stop on first denial</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Always record metrics even for failed/fallback requests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TODO: Implement ProcessRequest coordination logic\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evaluateAllTiers performs rate limit checking across all applicable tiers.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FlowCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluateAllTiers</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">reqCtx</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RequestContext</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rules</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TierEvaluation</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Iterate through rules in priority order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each rule, compose the appropriate Redis key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute the algorithm-specific rate limit check</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Collect timing and result information for each evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Implement short-circuit logic - return immediately on first denial</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle Redis failures gracefully with circuit breaker pattern</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Each algorithm (token bucket, sliding window) has different Redis operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use goroutines for parallel tier evaluation if all must be checked</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Distinguish between hard failures (errors) and soft failures (limits exceeded)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TODO: Implement multi-tier evaluation with short-circuit logic\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"configuration-watcher-infrastructure\">Configuration Watcher Infrastructure</h4>\n<p>Complete implementation of the configuration propagation system with Redis pub/sub and local caching.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ConfigurationWatcher manages real-time configuration updates using Redis pub/sub</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// and maintains local configuration cache with atomic updates.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ConfigurationWatcher</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redisClient    </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    changeChannel  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    subscription   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PubSub</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localVersion   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    updateCallback </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu             </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopChan       </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    healthTicker   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Ticker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewConfigurationWatcher creates a new watcher with Redis pub/sub subscription.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewConfigurationWatcher</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    redisClient</span><span style=\"color:#B392F0\"> redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    ruleManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    updateCallback</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConfigurationWatcher</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ConfigurationWatcher</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        redisClient:    redisClient,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ruleManager:    ruleManager,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        changeChannel:  </span><span style=\"color:#9ECBFF\">\"rate_limit:config:changes\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        updateCallback: updateCallback,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stopChan:       </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        healthTicker:   time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins listening for configuration changes and performing health checks.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cw </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConfigurationWatcher</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Subscribe to Redis configuration change channel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start background goroutine for processing change notifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start health check goroutine for periodic version comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Perform initial configuration load to sync current state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle subscription reconnection on Redis connection failures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use cw.processChangeNotifications() in a separate goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement exponential backoff for reconnection attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Log all configuration changes for audit trail</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TODO: Implement configuration watcher startup logic\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// processChangeNotifications handles incoming configuration change events.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cw </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConfigurationWatcher</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">processChangeNotifications</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Listen for messages on the Redis pub/sub channel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse change notification payload (rule ID, change type, version)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Fetch updated rule data from Redis configuration store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate new configuration for consistency and correctness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Atomically update local rule cache and notify callback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update local version number and heartbeat timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Handle different change types: CREATE, UPDATE, DELETE</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement rollback logic if validation fails</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use atomic operations to prevent inconsistent local state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TODO: Implement change notification processing\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// performHealthCheck compares local vs remote configuration versions.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cw </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConfigurationWatcher</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">performHealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Fetch current configuration version from Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compare with local version to detect drift</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If versions differ, trigger full configuration reload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update heartbeat record with instance status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log any version skew or synchronization issues</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use Redis GET on \"config:version\" key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement full reload as fallback for partial update failures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TODO: Implement configuration health checking\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"metrics-collection-infrastructure\">Metrics Collection Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// MetricsCollector aggregates rate limiting metrics and detects hot keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// across multiple time windows for real-time monitoring and alerting.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricsCollector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keyStats       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Map</span><span style=\"color:#6A737D\"> // map[string]*KeyStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    globalStats    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GlobalStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hotKeyDetector </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HotKeyDetector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exportChan     </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricsBatch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    windowSize     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu             </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricsBatch represents a collection of metrics ready for export.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricsBatch</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">               `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestMetrics  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RequestMetrics</span><span style=\"color:#9ECBFF\"> `json:\"request_metrics\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PerformanceMetrics </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMetrics</span><span style=\"color:#9ECBFF\">  `json:\"performance_metrics\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HotKeys        []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                `json:\"hot_keys\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SystemHealth   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemHealth</span><span style=\"color:#9ECBFF\">           `json:\"system_health\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RequestMetrics tracks request volume and outcomes for a specific rate limit key.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RequestMetrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalRequests    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `json:\"total_requests\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AllowedRequests  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `json:\"allowed_requests\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DeniedRequests   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `json:\"denied_requests\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorRequests    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\"> `json:\"error_requests\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastSeen         </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"last_seen\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMetricsCollector creates a metrics collector with configurable aggregation windows.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMetricsCollector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">windowSize</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exportChan</span><span style=\"color:#F97583\"> chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricsBatch</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsCollector</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MetricsCollector</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        globalStats:    </span><span style=\"color:#B392F0\">NewGlobalStats</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hotKeyDetector: </span><span style=\"color:#B392F0\">NewHotKeyDetector</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        exportChan:     exportChan,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        windowSize:     windowSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordRequest updates metrics for a single rate limit request.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Update key-specific request counters (total, allowed, denied)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Record request in hot key detector for trend analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update global performance metrics (latency histograms)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Track algorithm-specific metrics if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update last-seen timestamp for key activity tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sync.Map for concurrent access to key statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement lock-free counters using atomic operations where possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Consider sampling high-volume keys to reduce memory usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TODO: Implement request metrics recording\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExportMetrics produces a metrics batch for external consumption.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExportMetrics</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsBatch</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Aggregate all key-level metrics into exportable format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate performance percentiles and rates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Run hot key detection algorithm on recent data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Reset or rotate metrics windows to prevent unbounded growth</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Package everything into a comprehensive metrics batch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time-based bucketing for efficient percentile calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement memory limits to prevent metrics from consuming too much RAM</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TODO: Implement metrics export and aggregation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing the rate limit check flow:</p>\n<ul>\n<li>Run: <code>go test ./internal/flow/ -run TestRequestFlow</code></li>\n<li>Verify: Multi-tier evaluation works with short-circuit logic</li>\n<li>Test manually: Send requests that exceed different tier limits and confirm proper blocking</li>\n</ul>\n<p>After implementing configuration propagation:</p>\n<ul>\n<li>Run: <code>go test ./internal/flow/ -run TestConfigWatcher</code></li>\n<li>Verify: Configuration changes propagate to all instances within 5 seconds</li>\n<li>Test manually: Update a rule via API and confirm all instances receive the change</li>\n</ul>\n<p>After implementing metrics collection:</p>\n<ul>\n<li>Run: <code>go test ./internal/flow/ -run TestMetricsCollection</code></li>\n<li>Verify: Hot key detection identifies keys with &gt;10x normal traffic</li>\n<li>Test manually: Generate high traffic to specific keys and confirm dashboard updates</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inconsistent rate limiting</td>\n<td>Configuration version skew</td>\n<td>Check heartbeat timestamps in Redis</td>\n<td>Force configuration reload</td>\n</tr>\n<tr>\n<td>High latency on checks</td>\n<td>Redis connection pool exhaustion</td>\n<td>Monitor connection pool metrics</td>\n<td>Increase pool size or add connection timeout</td>\n</tr>\n<tr>\n<td>Missing metrics data</td>\n<td>Metrics export blocking</td>\n<td>Check exportChan buffer size</td>\n<td>Use buffered channel or async export</td>\n</tr>\n<tr>\n<td>False hot key detection</td>\n<td>Clock skew between instances</td>\n<td>Compare timestamps across instances</td>\n<td>Implement NTP sync or relative timing</td>\n</tr>\n</tbody></table>\n<h2 id=\"rate-limit-api-and-dashboard\">Rate Limit API and Dashboard</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 5 - Rate Limit API &amp; Dashboard</p>\n</blockquote>\n<p>The operational success of any distributed rate limiting system fundamentally depends on its management and observability capabilities. While the core algorithms and Redis backend provide the foundation for rate limiting functionality, the management API and dashboard serve as the critical interface between human operators and the distributed system. This section explores the design of a comprehensive rate limit management system that enables dynamic configuration updates, provides real-time visibility into system behavior, and maintains operational safety through self-protection mechanisms.</p>\n<h3 id=\"mental-model-the-air-traffic-control-system\">Mental Model: The Air Traffic Control System</h3>\n<p>Think of the rate limit API and dashboard as an air traffic control system for request traffic. Just as air traffic controllers need real-time radar displays showing aircraft positions, flight paths, and airport capacity, rate limit operators need dashboards showing current request rates, quota utilization, and system health across all tiers. The management API acts like the control tower radio system, allowing controllers to dynamically adjust flight patterns (rate limit rules) based on changing conditions without shutting down the airport (restarting services).</p>\n<p>The control tower must also protect itself - it can&#39;t allow unlimited radio chatter that would overwhelm the controllers&#39; ability to manage air traffic. Similarly, the rate limit management API must implement self-rate-limiting to prevent its own operations from overwhelming the very system it manages. This creates an interesting bootstrap problem: how do you rate limit the rate limiter without creating circular dependencies?</p>\n<h3 id=\"rate-limit-management-api\">Rate Limit Management API</h3>\n<p>The <strong>Rate Limit Management API</strong> serves as the primary interface for creating, reading, updating, and deleting rate limit rules across the entire distributed system. Unlike static configuration files that require service restarts, this API enables dynamic rule management with immediate propagation to all application instances through Redis pub/sub mechanisms.</p>\n<h4 id=\"api-design-philosophy\">API Design Philosophy</h4>\n<p>The management API follows REST principles with a strong emphasis on validation, versioning, and audit trails. Every rule change creates an audit log entry, enabling operators to understand how rate limiting behavior evolved over time. The API supports both immediate rule activation and scheduled deployments, allowing operators to prepare rate limit changes during low-traffic periods and activate them precisely when needed.</p>\n<blockquote>\n<p><strong>Decision: RESTful API with Resource-Based URLs</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to expose CRUD operations for rate limit rules with clear semantics and easy integration</li>\n<li><strong>Options Considered</strong>: REST API, GraphQL API, gRPC API</li>\n<li><strong>Decision</strong>: REST API with resource-based URLs following <code>/api/v1/rules/{id}</code> pattern</li>\n<li><strong>Rationale</strong>: REST provides familiar semantics for CRUD operations, excellent HTTP caching support, and broad client library support across languages</li>\n<li><strong>Consequences</strong>: Enables standard HTTP tooling and caching but requires multiple requests for complex operations</li>\n</ul>\n</blockquote>\n<p>The API resource structure centers around the <code>RateLimitRule</code> entity with hierarchical organization supporting rule grouping, inheritance, and override patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Endpoint</th>\n<th>Method</th>\n<th>Description</th>\n<th>Request Body</th>\n<th>Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>/api/v1/rules</code></td>\n<td>GET</td>\n<td>List all rate limit rules with filtering</td>\n<td>Query parameters</td>\n<td>Rule list with pagination</td>\n</tr>\n<tr>\n<td><code>/api/v1/rules</code></td>\n<td>POST</td>\n<td>Create new rate limit rule</td>\n<td><code>RateLimitRule</code> JSON</td>\n<td>Created rule with assigned ID</td>\n</tr>\n<tr>\n<td><code>/api/v1/rules/{id}</code></td>\n<td>GET</td>\n<td>Retrieve specific rule by ID</td>\n<td>None</td>\n<td>Complete <code>RateLimitRule</code> object</td>\n</tr>\n<tr>\n<td><code>/api/v1/rules/{id}</code></td>\n<td>PUT</td>\n<td>Update existing rule completely</td>\n<td>Complete <code>RateLimitRule</code></td>\n<td>Updated rule object</td>\n</tr>\n<tr>\n<td><code>/api/v1/rules/{id}</code></td>\n<td>PATCH</td>\n<td>Partial rule update</td>\n<td>Partial <code>RateLimitRule</code></td>\n<td>Updated rule object</td>\n</tr>\n<tr>\n<td><code>/api/v1/rules/{id}</code></td>\n<td>DELETE</td>\n<td>Remove rule and stop enforcement</td>\n<td>None</td>\n<td>Deletion confirmation</td>\n</tr>\n<tr>\n<td><code>/api/v1/rules/{id}/preview</code></td>\n<td>POST</td>\n<td>Test rule against sample requests</td>\n<td><code>RateLimitRequest</code> array</td>\n<td>Preview results without enforcement</td>\n</tr>\n<tr>\n<td><code>/api/v1/rules/{id}/reset</code></td>\n<td>POST</td>\n<td>Clear all counters for rule</td>\n<td>Optional key filter</td>\n<td>Reset confirmation</td>\n</tr>\n<tr>\n<td><code>/api/v1/rules/bulk</code></td>\n<td>POST</td>\n<td>Bulk create/update operations</td>\n<td>Rule array with operations</td>\n<td>Bulk operation results</td>\n</tr>\n</tbody></table>\n<h4 id=\"rule-validation-and-constraints\">Rule Validation and Constraints</h4>\n<p>The API implements comprehensive validation ensuring that rate limit rules are syntactically correct, semantically meaningful, and operationally safe before activation. Validation occurs at multiple levels: syntax validation checks JSON structure and data types, semantic validation ensures logical consistency between fields, and operational validation prevents rules that could destabilize the system.</p>\n<p><strong>Syntax Validation Rules:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Validation Rule</th>\n<th>Error Message</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>name</code></td>\n<td>Non-empty string, max 100 chars</td>\n<td>&quot;Rule name required and must be under 100 characters&quot;</td>\n</tr>\n<tr>\n<td><code>key_pattern</code></td>\n<td>Valid regex pattern</td>\n<td>&quot;Key pattern must be valid regular expression&quot;</td>\n</tr>\n<tr>\n<td><code>algorithm</code></td>\n<td>One of: token_bucket, sliding_window_counter, sliding_window_log</td>\n<td>&quot;Algorithm must be supported type&quot;</td>\n</tr>\n<tr>\n<td><code>limit</code></td>\n<td>Positive integer &gt; 0</td>\n<td>&quot;Rate limit must be positive integer&quot;</td>\n</tr>\n<tr>\n<td><code>window</code></td>\n<td>Duration between 1s and 24h</td>\n<td>&quot;Time window must be between 1 second and 24 hours&quot;</td>\n</tr>\n<tr>\n<td><code>burst_limit</code></td>\n<td>Optional, &gt;= limit if specified</td>\n<td>&quot;Burst limit must be &gt;= base limit when specified&quot;</td>\n</tr>\n<tr>\n<td><code>priority</code></td>\n<td>Integer between 1 and 100</td>\n<td>&quot;Priority must be between 1 (low) and 100 (high)&quot;</td>\n</tr>\n</tbody></table>\n<p><strong>Semantic Validation Rules:</strong></p>\n<p>The API performs semantic validation to catch logical inconsistencies that could lead to unexpected behavior. For example, a rule with <code>algorithm: &quot;token_bucket&quot;</code> must specify a <code>burst_limit</code> since token buckets are specifically designed for burst handling. Similarly, rules targeting the same key pattern cannot have conflicting priorities that would create ambiguous evaluation order.</p>\n<p><strong>Operational Safety Validation:</strong></p>\n<p>To prevent operators from accidentally creating rules that could destabilize the system, the API enforces operational safety constraints. Rules with extremely low limits (less than 10 requests per minute) trigger warnings and require explicit confirmation. Rules with very short time windows (less than 10 seconds) are flagged as potentially problematic for distributed consistency. The API also prevents creation of more than 1000 active rules per system to avoid overwhelming the Redis backend with excessive key space consumption.</p>\n<h4 id=\"rule-versioning-and-rollback\">Rule Versioning and Rollback</h4>\n<p>Every rule modification creates a new version while preserving the complete history of changes. This versioning system enables rapid rollback to previous configurations when new rules cause unexpected behavior. The versioning implementation stores rule snapshots in Redis with timestamp-based keys, enabling point-in-time recovery and audit trail reconstruction.</p>\n<table>\n<thead>\n<tr>\n<th>Version Operation</th>\n<th>Endpoint</th>\n<th>Description</th>\n<th>Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Create Version</td>\n<td>PUT/PATCH <code>/api/v1/rules/{id}</code></td>\n<td>Automatic version creation on modification</td>\n<td>New version stored, old version archived</td>\n</tr>\n<tr>\n<td>List Versions</td>\n<td>GET <code>/api/v1/rules/{id}/versions</code></td>\n<td>Retrieve complete version history</td>\n<td>No impact, read-only operation</td>\n</tr>\n<tr>\n<td>View Version</td>\n<td>GET <code>/api/v1/rules/{id}/versions/{version}</code></td>\n<td>Retrieve specific historical version</td>\n<td>No impact, read-only operation</td>\n</tr>\n<tr>\n<td>Rollback</td>\n<td>POST <code>/api/v1/rules/{id}/rollback</code></td>\n<td>Revert to specified previous version</td>\n<td>Immediate rule update, new version created</td>\n</tr>\n</tbody></table>\n<h4 id=\"configuration-propagation-mechanism\">Configuration Propagation Mechanism</h4>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fconfiguration-propagation-flow.svg\" alt=\"Configuration Update Flow\"></p>\n<p>When a rule is created or modified through the API, the change must propagate to all application instances running the distributed rate limiter. This propagation occurs through a multi-stage process designed to ensure consistency while minimizing latency and avoiding race conditions.</p>\n<p><strong>Propagation Stages:</strong></p>\n<ol>\n<li><strong>Validation and Persistence</strong>: The API validates the incoming rule change and persists it to Redis with a unique version number and timestamp</li>\n<li><strong>Change Notification</strong>: A change notification containing the rule ID and version is published to the Redis channel <code>rate_limit_config_changes</code></li>\n<li><strong>Instance Subscription</strong>: All application instances maintain Redis subscriptions to the configuration change channel</li>\n<li><strong>Version Comparison</strong>: Upon receiving a change notification, each instance compares the announced version with its local cache</li>\n<li><strong>Rule Retrieval</strong>: Instances with outdated versions fetch the complete updated rule from Redis</li>\n<li><strong>Local Cache Update</strong>: The instance updates its local rule cache and begins enforcing the new configuration</li>\n<li><strong>Acknowledgment</strong>: Each instance publishes an acknowledgment to confirm successful configuration update</li>\n</ol>\n<p>This propagation mechanism handles network partitions and instance failures gracefully. Instances that miss change notifications due to temporary disconnections perform periodic configuration health checks, comparing their local version numbers against Redis to detect missed updates. The system also implements exponential backoff retry logic when instances cannot reach Redis during configuration updates.</p>\n<blockquote>\n<p>The critical design insight here is that configuration propagation must be eventually consistent rather than strongly consistent. Temporary inconsistencies where different instances enforce slightly different rules for a few seconds are acceptable, but permanent inconsistencies where some instances never receive updates are not.</p>\n</blockquote>\n<h4 id=\"audit-trail-and-change-history\">Audit Trail and Change History</h4>\n<p>Every API operation creates detailed audit log entries capturing who made what changes when and why. These audit logs prove essential for troubleshooting unexpected rate limiting behavior and maintaining compliance with change management processes. The audit system records both successful operations and failed attempts, enabling security teams to detect unauthorized access attempts.</p>\n<table>\n<thead>\n<tr>\n<th>Audit Event Type</th>\n<th>Logged Information</th>\n<th>Retention Period</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Creation</td>\n<td>User ID, rule content, timestamp, source IP</td>\n<td>1 year</td>\n</tr>\n<tr>\n<td>Rule Modification</td>\n<td>User ID, old values, new values, change reason, timestamp</td>\n<td>1 year</td>\n</tr>\n<tr>\n<td>Rule Deletion</td>\n<td>User ID, deleted rule content, deletion reason, timestamp</td>\n<td>1 year</td>\n</tr>\n<tr>\n<td>Bulk Operations</td>\n<td>User ID, operation list, success/failure per rule, timestamp</td>\n<td>1 year</td>\n</tr>\n<tr>\n<td>Preview Requests</td>\n<td>User ID, rule ID, test scenarios, results, timestamp</td>\n<td>30 days</td>\n</tr>\n<tr>\n<td>Reset Operations</td>\n<td>User ID, rule ID, affected keys, timestamp</td>\n<td>90 days</td>\n</tr>\n<tr>\n<td>Authentication Failures</td>\n<td>Attempted user ID, source IP, failure reason, timestamp</td>\n<td>6 months</td>\n</tr>\n<tr>\n<td>Authorization Denials</td>\n<td>User ID, attempted operation, denial reason, timestamp</td>\n<td>6 months</td>\n</tr>\n</tbody></table>\n<h3 id=\"standard-rate-limit-headers\">Standard Rate Limit Headers</h3>\n<p>The distributed rate limiter implements standard HTTP headers following RFC 6585 and industry best practices to provide clients with actionable information about their rate limiting status. These headers enable clients to implement intelligent retry logic, display accurate quota information to users, and avoid unnecessarily aggressive request patterns that could trigger additional rate limiting.</p>\n<h4 id=\"header-implementation-strategy\">Header Implementation Strategy</h4>\n<p>The header implementation follows a tiered approach where different header sets are included based on the rate limiting result. Successful requests include current quota information, denied requests include retry guidance, and error scenarios include minimal diagnostic information to avoid information leakage.</p>\n<blockquote>\n<p><strong>Decision: RFC 6585 Compliance with Extensions</strong></p>\n<ul>\n<li><strong>Context</strong>: Need standardized headers for client rate limit awareness and retry logic</li>\n<li><strong>Options Considered</strong>: Custom headers, RFC 6585 standard, GitHub-style headers</li>\n<li><strong>Decision</strong>: Implement RFC 6585 with additional headers for enhanced client experience</li>\n<li><strong>Rationale</strong>: RFC compliance ensures broad client library support while extensions provide operational benefits</li>\n<li><strong>Consequences</strong>: Enables standard tooling but requires careful header size management to avoid HTTP limits</li>\n</ul>\n</blockquote>\n<p><strong>Standard Rate Limit Headers:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Header Name</th>\n<th>When Included</th>\n<th>Value Format</th>\n<th>Example</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>X-RateLimit-Limit</code></td>\n<td>All responses</td>\n<td>Integer</td>\n<td><code>1000</code></td>\n<td>Maximum requests allowed in current window</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Remaining</code></td>\n<td>All responses</td>\n<td>Integer</td>\n<td><code>742</code></td>\n<td>Requests remaining in current window</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Reset</code></td>\n<td>All responses</td>\n<td>Unix timestamp</td>\n<td><code>1640995200</code></td>\n<td>When the current window resets</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Window</code></td>\n<td>All responses</td>\n<td>Duration in seconds</td>\n<td><code>3600</code></td>\n<td>Length of rate limit window</td>\n</tr>\n<tr>\n<td><code>Retry-After</code></td>\n<td>429 responses only</td>\n<td>Seconds until retry</td>\n<td><code>45</code></td>\n<td>Minimum wait time before retry attempt</td>\n</tr>\n</tbody></table>\n<p><strong>Extended Headers for Enhanced Client Experience:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Header Name</th>\n<th>When Included</th>\n<th>Value Format</th>\n<th>Example</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>X-RateLimit-Policy</code></td>\n<td>All responses</td>\n<td>Algorithm identifier</td>\n<td><code>token_bucket</code></td>\n<td>Which rate limiting algorithm is active</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Scope</code></td>\n<td>All responses</td>\n<td>Scope identifier</td>\n<td><code>user:12345</code></td>\n<td>What entity the limit applies to</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Burst</code></td>\n<td>Token bucket responses</td>\n<td>Integer</td>\n<td><code>1500</code></td>\n<td>Maximum burst capacity available</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Tier</code></td>\n<td>Multi-tier responses</td>\n<td>Tier name</td>\n<td><code>per-user</code></td>\n<td>Which tier triggered the limit</td>\n</tr>\n</tbody></table>\n<h4 id=\"multi-tier-header-aggregation\">Multi-Tier Header Aggregation</h4>\n<p>When multiple rate limit tiers apply to a single request, the header generation logic must aggregate information from all applicable tiers to provide clients with actionable guidance. The aggregation follows a &quot;most restrictive wins&quot; principle where the tightest remaining quota determines the header values.</p>\n<p><strong>Tier Aggregation Logic:</strong></p>\n<ol>\n<li><strong>Collect All Applicable Limits</strong>: Gather rate limit results from user, IP, API, and global tiers</li>\n<li><strong>Identify Most Restrictive</strong>: Find the tier with the lowest <code>remaining/limit</code> ratio</li>\n<li><strong>Check for Denials</strong>: If any tier denies the request, use that tier&#39;s information for headers</li>\n<li><strong>Calculate Aggregate Reset</strong>: Use the earliest reset time across all tiers</li>\n<li><strong>Compose Scope Header</strong>: Create comma-separated list of all applicable scopes</li>\n</ol>\n<p>For example, a request that passes user-level limits (900/1000 remaining) but approaches IP-level limits (15/100 remaining) would generate headers reflecting the IP limit since it represents the most immediate constraint on future requests.</p>\n<h4 id=\"client-retry-guidance\">Client Retry Guidance</h4>\n<p>The <code>Retry-After</code> header calculation considers not just the immediate rate limit violation but also the broader context of multi-tier limits and algorithm-specific behavior. For token bucket algorithms, the retry time reflects the token refill rate. For sliding window algorithms, it considers both the window reset time and the distribution of recent requests.</p>\n<p><strong>Retry-After Calculation Algorithm:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Calculation Method</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket</td>\n<td><code>max(1, (tokens_needed - current_tokens) / refill_rate)</code></td>\n<td>Time to accumulate sufficient tokens</td>\n</tr>\n<tr>\n<td>Sliding Window Counter</td>\n<td><code>window_duration - time_since_window_start + jitter</code></td>\n<td>Wait for window boundary plus randomization</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n<td><code>oldest_request_timestamp + window_duration - current_time + jitter</code></td>\n<td>Wait for oldest request to age out</td>\n</tr>\n</tbody></table>\n<p>The retry calculation includes small random jitter (5-15% of the base retry time) to prevent thundering herd effects where many clients retry simultaneously after receiving identical <code>Retry-After</code> values.</p>\n<h3 id=\"real-time-dashboard-architecture\">Real-time Dashboard Architecture</h3>\n<p>The <strong>real-time dashboard</strong> provides operational visibility into rate limiting behavior across all tiers, algorithms, and time scales. Unlike traditional monitoring dashboards that display historical data, this dashboard focuses on current quota utilization, active patterns, and immediate operational decisions that rate limit administrators need to make.</p>\n<h4 id=\"mental-model-power-grid-control-room\">Mental Model: Power Grid Control Room</h4>\n<p>Think of the rate limiting dashboard like a power grid control room where operators monitor electricity demand across different regions and time scales. Just as grid operators need real-time visibility into current load, peak capacity, and demand forecasting to prevent blackouts, rate limit operators need current quota utilization, request patterns, and trend analysis to prevent service degradation. The dashboard must update continuously without overwhelming the underlying rate limiting system, just as grid monitoring cannot consume significant power itself.</p>\n<h4 id=\"dashboard-data-architecture\">Dashboard Data Architecture</h4>\n<p>The dashboard architecture separates data collection, aggregation, and presentation into distinct layers to ensure that dashboard operations never impact rate limiting performance. Data flows through a pipeline from individual rate limit checks through local aggregation, Redis-based consolidation, and finally WebSocket streaming to dashboard clients.</p>\n<p><strong>Data Flow Layers:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Layer</th>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Collection</td>\n<td><code>MetricsCollector</code></td>\n<td>Capture individual rate limit results</td>\n<td>Per request</td>\n</tr>\n<tr>\n<td>Local Aggregation</td>\n<td><code>MetricsBatch</code></td>\n<td>Combine metrics within single instance</td>\n<td>Every 5 seconds</td>\n</tr>\n<tr>\n<td>Redis Consolidation</td>\n<td>Redis Streams</td>\n<td>Merge metrics from all instances</td>\n<td>Every 10 seconds</td>\n</tr>\n<tr>\n<td>Dashboard Streaming</td>\n<td>WebSocket Server</td>\n<td>Push updates to dashboard clients</td>\n<td>Every 2 seconds</td>\n</tr>\n</tbody></table>\n<h4 id=\"real-time-metrics-collection\">Real-time Metrics Collection</h4>\n<p>The <code>MetricsCollector</code> component captures detailed information about every rate limit decision without impacting the critical path performance. Collection uses lock-free data structures and asynchronous batching to ensure that metrics gathering never blocks rate limit checks.</p>\n<p><strong>Collected Metric Categories:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Data Points</th>\n<th>Aggregation Method</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Counts</td>\n<td>Total, allowed, denied by rule</td>\n<td>Sum over time windows</td>\n<td>Quota utilization tracking</td>\n</tr>\n<tr>\n<td>Response Times</td>\n<td>P50, P95, P99 latency by tier</td>\n<td>Histogram bucketing</td>\n<td>Performance monitoring</td>\n</tr>\n<tr>\n<td>Algorithm Performance</td>\n<td>Token refill rates, window efficiency</td>\n<td>Moving averages</td>\n<td>Algorithm tuning guidance</td>\n</tr>\n<tr>\n<td>Error Rates</td>\n<td>Redis timeouts, fallback activation</td>\n<td>Count and rate calculation</td>\n<td>System health assessment</td>\n</tr>\n<tr>\n<td>Hot Key Detection</td>\n<td>Request distribution across keys</td>\n<td>Top-K tracking</td>\n<td>Load balancing insights</td>\n</tr>\n</tbody></table>\n<h4 id=\"websocket-based-real-time-updates\">WebSocket-Based Real-Time Updates</h4>\n<p>The dashboard uses WebSocket connections to stream real-time updates to client browsers without polling overhead. The WebSocket server implements intelligent update batching and client-specific filtering to ensure that each dashboard user receives only relevant updates without overwhelming their browser.</p>\n<p><strong>WebSocket Message Types:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Message Type</th>\n<th>Payload Structure</th>\n<th>Update Frequency</th>\n<th>Client Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>quota_update</code></td>\n<td><code>{rule_id, current, limit, remaining}</code></td>\n<td>Every 2 seconds</td>\n<td>Update quota gauges</td>\n</tr>\n<tr>\n<td><code>tier_status</code></td>\n<td><code>{tier_name, active_rules, total_requests}</code></td>\n<td>Every 5 seconds</td>\n<td>Refresh tier summary</td>\n</tr>\n<tr>\n<td><code>hot_keys</code></td>\n<td><code>{key, request_rate, trend}</code></td>\n<td>Every 10 seconds</td>\n<td>Update hot key list</td>\n</tr>\n<tr>\n<td><code>system_health</code></td>\n<td><code>{redis_status, fallback_rate, error_count}</code></td>\n<td>Every 5 seconds</td>\n<td>Update health indicators</td>\n</tr>\n<tr>\n<td><code>rule_change</code></td>\n<td><code>{operation, rule_id, new_config}</code></td>\n<td>Immediate</td>\n<td>Refresh rule display</td>\n</tr>\n</tbody></table>\n<h4 id=\"dashboard-user-interface-design\">Dashboard User Interface Design</h4>\n<p>The dashboard interface organizes information hierarchically from system overview through tier-specific views to individual rule analysis. Each view provides actionable information at the appropriate level of detail for different operational decisions.</p>\n<p><strong>Dashboard View Hierarchy:</strong></p>\n<ol>\n<li><strong>System Overview</strong>: High-level health, total request rates, and critical alerts across all tiers</li>\n<li><strong>Tier Dashboard</strong>: Detailed view of user, IP, API, or global tier with rule-by-rule breakdown</li>\n<li><strong>Rule Analysis</strong>: Deep dive into individual rule performance, quota utilization patterns, and historical trends</li>\n<li><strong>Hot Key Investigation</strong>: Real-time analysis of disproportionately accessed keys with geographic and temporal patterns</li>\n<li><strong>System Health</strong>: Redis cluster status, instance connectivity, and performance metrics</li>\n</ol>\n<h4 id=\"dashboard-performance-optimization\">Dashboard Performance Optimization</h4>\n<p>To ensure the dashboard remains responsive under high load conditions, several optimization strategies prevent dashboard operations from impacting rate limiting performance or overwhelming client browsers with excessive updates.</p>\n<blockquote>\n<p><strong>Decision: Adaptive Update Frequency Based on Activity Level</strong></p>\n<ul>\n<li><strong>Context</strong>: Dashboard updates must be frequent enough for operational decisions but not so frequent as to impact performance</li>\n<li><strong>Options Considered</strong>: Fixed update intervals, event-driven updates, adaptive frequency</li>\n<li><strong>Decision</strong>: Implement adaptive update frequency that increases during high activity and decreases during stable periods</li>\n<li><strong>Rationale</strong>: Provides real-time visibility when needed while conserving resources during normal operations</li>\n<li><strong>Consequences</strong>: More complex implementation but significantly better resource efficiency</li>\n</ul>\n</blockquote>\n<p><strong>Optimization Strategies:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Implementation</th>\n<th>Benefit</th>\n<th>Trade-off</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client-Side Filtering</td>\n<td>Dashboard clients specify interest filters</td>\n<td>Reduced bandwidth usage</td>\n<td>More complex client logic</td>\n</tr>\n<tr>\n<td>Update Batching</td>\n<td>Group multiple updates into single WebSocket messages</td>\n<td>Lower message overhead</td>\n<td>Slightly increased latency</td>\n</tr>\n<tr>\n<td>Adaptive Frequency</td>\n<td>Increase update rate during anomalies, decrease during stability</td>\n<td>Dynamic resource usage</td>\n<td>Complex triggering logic</td>\n</tr>\n<tr>\n<td>Data Compression</td>\n<td>Compress WebSocket payloads for large updates</td>\n<td>Reduced network usage</td>\n<td>CPU overhead for compression</td>\n</tr>\n<tr>\n<td>Local Caching</td>\n<td>Cache unchanged data on client side</td>\n<td>Minimal redundant updates</td>\n<td>Client-side memory usage</td>\n</tr>\n</tbody></table>\n<h3 id=\"self-rate-limiting-the-management-api\">Self-Rate-Limiting the Management API</h3>\n<p>The management API faces a unique challenge: it must protect itself from abuse without creating circular dependencies with the rate limiting system it manages. This creates a bootstrap problem where the rate limiter cannot rely on its own Redis backend for API protection since API operations might be necessary to fix Redis connectivity issues.</p>\n<h4 id=\"mental-model-emergency-services-communication\">Mental Model: Emergency Services Communication</h4>\n<p>Think of the management API like emergency services communication systems that must remain operational even when the systems they manage are failing. A fire department&#39;s radio system cannot depend on the electrical grid they might need to repair - it needs independent power and communication channels. Similarly, the rate limit management API needs independent protection mechanisms that don&#39;t depend on the distributed rate limiting infrastructure it controls.</p>\n<h4 id=\"independent-rate-limiting-strategy\">Independent Rate Limiting Strategy</h4>\n<p>The management API implements a lightweight, in-memory rate limiting system that operates independently of the main Redis-backed rate limiter. This independent system uses simpler algorithms and local state to provide basic protection without external dependencies.</p>\n<blockquote>\n<p><strong>Decision: Local Token Bucket for API Self-Protection</strong></p>\n<ul>\n<li><strong>Context</strong>: Management API needs rate limiting protection that doesn&#39;t depend on Redis or distributed state</li>\n<li><strong>Options Considered</strong>: Local token bucket, fixed window counters, dependency on main rate limiter</li>\n<li><strong>Decision</strong>: Implement local token bucket with per-client tracking using IP address + API key</li>\n<li><strong>Rationale</strong>: Token bucket provides burst handling for legitimate batch operations while preventing sustained abuse</li>\n<li><strong>Consequences</strong>: Protection is per-instance rather than cluster-wide, but maintains independence from Redis</li>\n</ul>\n</blockquote>\n<p><strong>Self-Rate-Limiting Architecture:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Implementation</th>\n<th>Responsibility</th>\n<th>Independence Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>APITokenBucket</code></td>\n<td>In-memory token bucket per client</td>\n<td>Request rate control</td>\n<td>Fully independent</td>\n</tr>\n<tr>\n<td><code>ClientIdentifier</code></td>\n<td>IP + API key hashing</td>\n<td>Client identification</td>\n<td>No external dependencies</td>\n</tr>\n<tr>\n<td><code>LocalRateLimiter</code></td>\n<td>Embedded rate limiter</td>\n<td>API call limiting</td>\n<td>Memory-only state</td>\n</tr>\n<tr>\n<td><code>OverrideManager</code></td>\n<td>Emergency bypass tokens</td>\n<td>Incident response</td>\n<td>File-based configuration</td>\n</tr>\n</tbody></table>\n<h4 id=\"api-specific-rate-limit-tiers\">API-Specific Rate Limit Tiers</h4>\n<p>The management API implements multiple tiers of rate limiting tailored to different operation types and client authentication levels. Read operations receive higher quotas than write operations, and authenticated API clients receive higher limits than unauthenticated requests.</p>\n<p><strong>API Rate Limit Tiers:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Authenticated Limit</th>\n<th>Unauthenticated Limit</th>\n<th>Burst Allowance</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Read Operations</td>\n<td>100 req/min</td>\n<td>20 req/min</td>\n<td>150% of base</td>\n<td>Dashboard updates need frequent reads</td>\n</tr>\n<tr>\n<td>Write Operations</td>\n<td>20 req/min</td>\n<td>5 req/min</td>\n<td>130% of base</td>\n<td>Rule changes should be deliberate</td>\n</tr>\n<tr>\n<td>Bulk Operations</td>\n<td>5 req/min</td>\n<td>0 req/min</td>\n<td>110% of base</td>\n<td>Bulk changes need careful consideration</td>\n</tr>\n<tr>\n<td>Preview Operations</td>\n<td>50 req/min</td>\n<td>10 req/min</td>\n<td>200% of base</td>\n<td>Testing scenarios may need rapid iteration</td>\n</tr>\n<tr>\n<td>Reset Operations</td>\n<td>10 req/min</td>\n<td>0 req/min</td>\n<td>100% of base</td>\n<td>Counter resets have immediate system impact</td>\n</tr>\n</tbody></table>\n<h4 id=\"emergency-override-mechanisms\">Emergency Override Mechanisms</h4>\n<p>During critical incidents where rate limiting rules may be causing service outages, operators need the ability to bypass normal API rate limits to implement emergency fixes. The override system provides this capability through multiple authentication factors and audit trails.</p>\n<p><strong>Override Activation Methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Override Type</th>\n<th>Activation Requirement</th>\n<th>Duration</th>\n<th>Audit Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Temporary Bypass</td>\n<td>Admin API key + incident ticket</td>\n<td>1 hour</td>\n<td>Full request logging</td>\n</tr>\n<tr>\n<td>Emergency Token</td>\n<td>Pre-generated token + manager approval</td>\n<td>30 minutes</td>\n<td>Real-time notification</td>\n</tr>\n<tr>\n<td>File-Based Override</td>\n<td>Server file system access</td>\n<td>Until file removal</td>\n<td>File system audit trail</td>\n</tr>\n<tr>\n<td>Multi-Admin Confirmation</td>\n<td>Two admin API keys simultaneously</td>\n<td>2 hours</td>\n<td>Multi-party audit logging</td>\n</tr>\n</tbody></table>\n<p>The emergency override system ensures that legitimate incident response can proceed quickly while maintaining strong audit trails and preventing abuse. Override usage triggers immediate notifications to security teams and creates detailed logs for post-incident review.</p>\n<h4 id=\"api-protection-without-circular-dependencies\">API Protection Without Circular Dependencies</h4>\n<p>To avoid circular dependencies where the management API rate limiting depends on the systems it manages, the implementation carefully isolates API protection from distributed state. The local rate limiter uses only in-memory data structures and local file system access for persistent configuration.</p>\n<p><strong>Dependency Isolation Strategies:</strong></p>\n<ol>\n<li><strong>Separate Algorithm Implementation</strong>: The API uses a simplified token bucket implementation that shares no code with the main distributed rate limiter</li>\n<li><strong>Local State Only</strong>: All API rate limiting state remains in local memory, never touching Redis or shared storage</li>\n<li><strong>Independent Configuration</strong>: API rate limits are configured through environment variables or local files, not through the main rule management system</li>\n<li><strong>Graceful Fallback</strong>: When local rate limiting fails, the API defaults to conservative limits rather than disabling protection</li>\n<li><strong>Health Check Separation</strong>: API health checks use direct Redis connectivity tests rather than going through the rate limiting system</li>\n</ol>\n<blockquote>\n<p>The fundamental design principle here is defense in depth - the management API must remain secure and operational even when every other component of the distributed rate limiting system is failing or misconfigured.</p>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Dashboard Overwhelming Rate Limiter with Monitoring Queries</strong></p>\n<p>A common mistake is implementing dashboard updates that query rate limiting state so frequently that the monitoring system becomes a significant load on the Redis backend. This creates a feedback loop where dashboard usage impacts rate limiting performance, which triggers more frequent dashboard updates to investigate the performance problem.</p>\n<p><strong>Why it&#39;s wrong</strong>: Dashboard queries compete with production rate limiting operations for Redis resources, potentially causing legitimate rate limit checks to fail or experience high latency.</p>\n<p><strong>How to fix</strong>: Implement async metrics collection where rate limiting operations push metrics to a separate data path rather than having the dashboard pull current state. Use Redis Streams or pub/sub to decouple dashboard data from operational state.</p>\n<p>⚠️ <strong>Pitfall: Configuration Changes Without Validation Rollback</strong></p>\n<p>When implementing dynamic configuration updates, developers often forget to implement automatic rollback when new configurations cause system instability. A rule change that accidentally sets all limits to 1 request per hour can cause immediate service outages.</p>\n<p><strong>Why it&#39;s wrong</strong>: Bad configurations can render the entire service unusable, and manual rollback takes time during which the service remains degraded.</p>\n<p><strong>How to fix</strong>: Implement automatic configuration validation that tests new rules against recent traffic patterns and automatically rolls back changes that cause denial rates to spike above configurable thresholds.</p>\n<p>⚠️ <strong>Pitfall: Management API Using Same Redis Instance as Rate Limiting</strong></p>\n<p>Some implementations use the same Redis instance for both rate limiting operations and management API data storage. This creates a single point of failure where Redis issues impact both operational rate limiting and the ability to fix configuration problems.</p>\n<p><strong>Why it&#39;s wrong</strong>: When Redis becomes unavailable, operators lose both rate limiting functionality and the ability to modify configuration to address the issue.</p>\n<p><strong>How to fix</strong>: Use separate Redis instances or different Redis databases for operational rate limiting and management data. Consider using a different storage backend entirely for management API data to ensure independence.</p>\n<p>⚠️ <strong>Pitfall: Rate Limit Headers Calculated After Request Processing</strong></p>\n<p>A subtle error occurs when rate limit headers are calculated after the main request processing rather than during the rate limit check. This can result in headers that don&#39;t reflect the actual rate limiting decision or show inconsistent quota information.</p>\n<p><strong>Why it&#39;s wrong</strong>: Clients receive misleading information about their rate limiting status, leading to incorrect retry behavior and poor user experience.</p>\n<p><strong>How to fix</strong>: Calculate all rate limit headers during the actual rate limit check operation and pass them through the request context to ensure header consistency with the rate limiting decision.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The rate limit API and dashboard implementation requires careful coordination between HTTP API handling, real-time data streaming, and independent rate limiting logic. This section provides complete implementation guidance for building these components.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP API Framework</td>\n<td><code>net/http</code> with <code>gorilla/mux</code> router</td>\n<td><code>gin-gonic/gin</code> with OpenAPI generation</td>\n</tr>\n<tr>\n<td>WebSocket Server</td>\n<td><code>gorilla/websocket</code> library</td>\n<td><code>Socket.io</code> with Redis adapter</td>\n</tr>\n<tr>\n<td>Real-time Metrics</td>\n<td>In-memory aggregation with periodic flush</td>\n<td>Redis Streams with consumer groups</td>\n</tr>\n<tr>\n<td>Dashboard Frontend</td>\n<td>Server-rendered HTML with vanilla JS</td>\n<td>React/Vue SPA with Chart.js visualization</td>\n</tr>\n<tr>\n<td>API Authentication</td>\n<td>JWT tokens with HMAC signing</td>\n<td>OAuth2 with RBAC authorization</td>\n</tr>\n<tr>\n<td>Configuration Storage</td>\n<td>JSON files with file watching</td>\n<td>etcd with distributed locking</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    rate-limiter/main.go           ← main rate limiting service\n    management-api/main.go         ← management API server\n    dashboard/main.go              ← dashboard web server\n  internal/\n    api/\n      handlers/\n        rules.go                   ← rate limit rule CRUD handlers\n        dashboard.go               ← dashboard data API handlers\n        websocket.go               ← real-time WebSocket server\n      middleware/\n        auth.go                    ← API authentication middleware\n        ratelimit.go               ← self-rate-limiting middleware\n        headers.go                 ← rate limit header injection\n      validation/\n        rules.go                   ← rule validation logic\n        constraints.go             ← operational safety constraints\n    dashboard/\n      metrics/\n        collector.go               ← metrics collection and aggregation\n        streaming.go               ← WebSocket streaming logic\n        batch.go                   ← metric batching and compression\n      ui/\n        templates/                 ← HTML templates for dashboard\n        static/                    ← CSS, JS, and image assets\n    config/\n      propagation.go               ← configuration change propagation\n      versioning.go                ← rule versioning and rollback\n      audit.go                     ← audit trail implementation\n  web/\n    dashboard/\n      index.html                   ← main dashboard interface\n      js/dashboard.js              ← dashboard JavaScript logic\n      css/styles.css               ← dashboard styling</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete API Rate Limiter (api/middleware/ratelimit.go)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> middleware</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// APIRateLimiter provides self-rate-limiting for management API</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> APIRateLimiter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clients     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanup     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limits      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">APILimits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// APILimits defines rate limits for different API operation types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> APILimits</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadLimit   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">         // requests per minute for read operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteLimit  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">         // requests per minute for write operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BulkLimit   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">         // requests per minute for bulk operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BurstRatio  </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\">       // burst allowance ratio (e.g., 1.5 = 150%)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TokenBucket implements simple in-memory token bucket for API protection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucket</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">     // maximum tokens</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">     // current tokens</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refillRate  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">     // tokens per minute</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastRefill  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\"> // last refill timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewAPIRateLimiter creates self-rate-limiter for management API</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewAPIRateLimiter</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limits </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">APILimits</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"authenticated\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadLimit:  </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteLimit: </span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            BulkLimit:  </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            BurstRatio: </span><span style=\"color:#79B8FF\">1.5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"unauthenticated\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadLimit:  </span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteLimit: </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            BulkLimit:  </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            BurstRatio: </span><span style=\"color:#79B8FF\">1.3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rl </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        clients: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cleanup: </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Minute,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        limits:  limits,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start background cleanup of inactive clients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">cleanupLoop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> rl</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Middleware returns HTTP middleware function for request rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Middleware</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Extract client identifier and operation type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            clientID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">getClientID</span><span style=\"color:#E1E4E8\">(r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            opType </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">getOperationType</span><span style=\"color:#E1E4E8\">(r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            authLevel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">getAuthLevel</span><span style=\"color:#E1E4E8\">(r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Check rate limit for this client and operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            allowed, remaining, resetTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">checkLimit</span><span style=\"color:#E1E4E8\">(clientID, opType, authLevel)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Add rate limit headers to response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-RateLimit-Limit\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, rl.</span><span style=\"color:#B392F0\">getLimit</span><span style=\"color:#E1E4E8\">(opType, authLevel)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-RateLimit-Remaining\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, remaining))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-RateLimit-Reset\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resetTime.</span><span style=\"color:#B392F0\">Unix</span><span style=\"color:#E1E4E8\">()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-RateLimit-Scope\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"api:</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, clientID))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">allowed {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // Calculate retry after based on refill rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                retryAfter </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">calculateRetryAfter</span><span style=\"color:#E1E4E8\">(opType, authLevel)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Retry-After\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">(retryAfter.</span><span style=\"color:#B392F0\">Seconds</span><span style=\"color:#E1E4E8\">())))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"API rate limit exceeded\"</span><span style=\"color:#E1E4E8\">, http.StatusTooManyRequests)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// checkLimit performs rate limiting check for client and operation type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkLimit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">clientID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">opType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">authLevel</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rl.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rl.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Get or create token bucket for this client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.clients[clientID]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        limit </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">getLimit</span><span style=\"color:#E1E4E8\">(opType, authLevel)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        burstRatio </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.limits[authLevel].BurstRatio</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        capacity </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(limit) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> burstRatio)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            capacity:   capacity,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tokens:     capacity,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            refillRate: limit,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lastRefill: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rl.clients[clientID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> bucket.</span><span style=\"color:#B392F0\">consume</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// consume attempts to consume tokens from bucket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">consume</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tokens</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> tb.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    now </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate tokens to add based on time elapsed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(tb.lastRefill)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokensToAdd </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(elapsed.</span><span style=\"color:#B392F0\">Minutes</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(tb.refillRate))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> tokensToAdd </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tb.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> min</span><span style=\"color:#E1E4E8\">(tb.capacity, tb.tokens </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> tokensToAdd)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tb.lastRefill </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> now</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate next reset time (when bucket will be full)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resetTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">((tb.capacity </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> tb.tokens) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> tb.refillRate) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> tb.tokens </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> tokens {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tb.tokens </span><span style=\"color:#F97583\">-=</span><span style=\"color:#E1E4E8\"> tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">, tb.tokens, resetTime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\">, tb.tokens, resetTime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Helper functions for client identification and operation classification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getClientID</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Try API key first, fall back to IP address</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> apiKey </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-API-Key\"</span><span style=\"color:#E1E4E8\">); apiKey </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key:</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, apiKey)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ip </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">extractIP</span><span style=\"color:#E1E4E8\">(r)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ip:</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, ip)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">extractIP</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check X-Forwarded-For header for proxy scenarios</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> xff </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-Forwarded-For\"</span><span style=\"color:#E1E4E8\">); xff </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parts </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Split</span><span style=\"color:#E1E4E8\">(xff, </span><span style=\"color:#9ECBFF\">\",\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(parts) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">TrimSpace</span><span style=\"color:#E1E4E8\">(parts[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Fall back to RemoteAddr</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ip, _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> net.</span><span style=\"color:#B392F0\">SplitHostPort</span><span style=\"color:#E1E4E8\">(r.RemoteAddr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> r.RemoteAddr</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ip</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getOperationType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> r.Method {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"GET\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"read\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"PUT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"PATCH\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"DELETE\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"write\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"POST\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(r.URL.Path, </span><span style=\"color:#9ECBFF\">\"/bulk\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"bulk\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"write\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"read\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getAuthLevel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-API-Key\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"authenticated\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"unauthenticated\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getLimit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">opType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">authLevel</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limits </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.limits[authLevel]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> opType {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"read\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> limits.ReadLimit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"write\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> limits.WriteLimit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"bulk\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> limits.BulkLimit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> limits.ReadLimit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">calculateRetryAfter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">opType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">authLevel</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limit </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rl.</span><span style=\"color:#B392F0\">getLimit</span><span style=\"color:#E1E4E8\">(opType, authLevel)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Time to get one token back</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">limit) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">cleanupLoop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(rl.cleanup)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ticker.C {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rl.</span><span style=\"color:#B392F0\">cleanupInactiveClients</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">APIRateLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">cleanupInactiveClients</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rl.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rl.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cutoff </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">rl.cleanup)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> clientID, bucket </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> rl.clients {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bucket.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastActivity </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bucket.lastRefill</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bucket.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> lastActivity.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(cutoff) {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            delete</span><span style=\"color:#E1E4E8\">(rl.clients, clientID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">a</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">b</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> b {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> a</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> b</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Complete WebSocket Streaming Server (dashboard/metrics/streaming.go)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> metrics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/websocket</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StreamingServer handles real-time metric streaming to dashboard clients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StreamingServer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clients     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clientsMu   </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    register    </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    unregister  </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    broadcast   </span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    collector   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsCollector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    upgrader    </span><span style=\"color:#B392F0\">websocket</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Upgrader</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Client represents a connected dashboard WebSocket client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Client</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">websocket</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Conn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    send      </span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filters   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">  // which metrics this client wants</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastSeen  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricUpdate represents a single metric update sent to clients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricUpdate</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"data\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewStreamingServer creates WebSocket server for real-time dashboard updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStreamingServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">collector</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricsCollector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        clients:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        register:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        unregister: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        broadcast:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        collector:  collector,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        upgrader: </span><span style=\"color:#B392F0\">websocket</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Upgrader</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            CheckOrigin: </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // In production, implement proper origin checking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins the streaming server event loop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">handleClients</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">streamMetrics</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleWebSocket upgrades HTTP connections to WebSocket for dashboard streaming</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleWebSocket</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.upgrader.</span><span style=\"color:#B392F0\">Upgrade</span><span style=\"color:#E1E4E8\">(w, r, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"WebSocket upgrade failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conn:     conn,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        send:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filters:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastSeen: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.register </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Handle client messages in separate goroutines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">readFromClient</span><span style=\"color:#E1E4E8\">(client)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">writeToClient</span><span style=\"color:#E1E4E8\">(client)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleClients</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> client </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">s.register:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.clientsMu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.clients[client] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.clientsMu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Send initial dashboard state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">sendInitialState</span><span style=\"color:#E1E4E8\">(client)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> client </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">s.unregister:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.clientsMu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> _, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.clients[client]; ok {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">                delete</span><span style=\"color:#E1E4E8\">(s.clients, client)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">                close</span><span style=\"color:#E1E4E8\">(client.send)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.clientsMu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> message </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">s.broadcast:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.clientsMu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> client </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> s.clients {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                case</span><span style=\"color:#E1E4E8\"> client.send </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> message:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Client send channel is full, disconnect</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">                    delete</span><span style=\"color:#E1E4E8\">(s.clients, client)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">                    close</span><span style=\"color:#E1E4E8\">(client.send)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.clientsMu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">streamMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Collect current metrics batch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            batch </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.collector.</span><span style=\"color:#B392F0\">ExportMetrics</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> batch </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Send different update types based on metric changes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">broadcastQuotaUpdates</span><span style=\"color:#E1E4E8\">(batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">broadcastTierStatus</span><span style=\"color:#E1E4E8\">(batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">broadcastHotKeys</span><span style=\"color:#E1E4E8\">(batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.</span><span style=\"color:#B392F0\">broadcastSystemHealth</span><span style=\"color:#E1E4E8\">(batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">broadcastQuotaUpdates</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricsBatch</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> ruleID, metrics </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> batch.RequestMetrics {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        update </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> MetricUpdate</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Type:      </span><span style=\"color:#9ECBFF\">\"quota_update\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timestamp: batch.Timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Data: </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"rule_id\"</span><span style=\"color:#E1E4E8\">:   ruleID,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"current\"</span><span style=\"color:#E1E4E8\">:   metrics.AllowedRequests,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"total\"</span><span style=\"color:#E1E4E8\">:     metrics.TotalRequests,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"denied\"</span><span style=\"color:#E1E4E8\">:    metrics.DeniedRequests,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"last_seen\"</span><span style=\"color:#E1E4E8\">: metrics.LastSeen,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.</span><span style=\"color:#B392F0\">broadcastUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">update)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">broadcastTierStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricsBatch</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Aggregate metrics by tier for tier status updates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tierStats </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> ruleID, metrics </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> batch.RequestMetrics {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Extract tier from rule ID (assuming format like \"user:tier_name\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tier </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> extractTierFromRuleID</span><span style=\"color:#E1E4E8\">(ruleID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tierStats[tier] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tierStats[tier] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"total_requests\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"active_rules\"</span><span style=\"color:#E1E4E8\">:   </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"avg_usage\"</span><span style=\"color:#E1E4E8\">:      </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tierStats[tier][</span><span style=\"color:#9ECBFF\">\"total_requests\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tierStats[tier][</span><span style=\"color:#9ECBFF\">\"total_requests\"</span><span style=\"color:#E1E4E8\">].(</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> metrics.TotalRequests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tierStats[tier][</span><span style=\"color:#9ECBFF\">\"active_rules\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tierStats[tier][</span><span style=\"color:#9ECBFF\">\"active_rules\"</span><span style=\"color:#E1E4E8\">].(</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> tierName, stats </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> tierStats {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        update </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> MetricUpdate</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Type:      </span><span style=\"color:#9ECBFF\">\"tier_status\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timestamp: batch.Timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Data: </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"tier_name\"</span><span style=\"color:#E1E4E8\">:      tierName,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"total_requests\"</span><span style=\"color:#E1E4E8\">: stats[</span><span style=\"color:#9ECBFF\">\"total_requests\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"active_rules\"</span><span style=\"color:#E1E4E8\">:   stats[</span><span style=\"color:#9ECBFF\">\"active_rules\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.</span><span style=\"color:#B392F0\">broadcastUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">update)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">broadcastHotKeys</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricsBatch</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    update </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> MetricUpdate</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Type:      </span><span style=\"color:#9ECBFF\">\"hot_keys\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: batch.Timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Data: </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"hot_keys\"</span><span style=\"color:#E1E4E8\">: batch.HotKeys,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.</span><span style=\"color:#B392F0\">broadcastUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">update)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">broadcastSystemHealth</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricsBatch</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    update </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> MetricUpdate</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Type:      </span><span style=\"color:#9ECBFF\">\"system_health\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: batch.Timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Data: </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"redis_healthy\"</span><span style=\"color:#E1E4E8\">:    batch.SystemHealth </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"performance\"</span><span style=\"color:#E1E4E8\">:      batch.PerformanceMetrics,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"error_rate\"</span><span style=\"color:#E1E4E8\">:       </span><span style=\"color:#B392F0\">calculateErrorRate</span><span style=\"color:#E1E4E8\">(batch),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.</span><span style=\"color:#B392F0\">broadcastUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">update)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">broadcastUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">update</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricUpdate</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(update)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to marshal metric update: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> s.broadcast </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> data:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Broadcast channel is full, drop update</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Dropped metric update - broadcast channel full\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">readFromClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">client</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.unregister </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client.conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client.conn.</span><span style=\"color:#B392F0\">SetReadDeadline</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client.conn.</span><span style=\"color:#B392F0\">SetPongHandler</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client.conn.</span><span style=\"color:#B392F0\">SetReadDeadline</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        var</span><span style=\"color:#E1E4E8\"> msg </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> client.conn.</span><span style=\"color:#B392F0\">ReadJSON</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">msg)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> websocket.</span><span style=\"color:#B392F0\">IsUnexpectedCloseError</span><span style=\"color:#E1E4E8\">(err, websocket.CloseGoingAway, websocket.CloseAbnormalClosure) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"WebSocket error: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Handle client filter updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> msgType, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> msg[</span><span style=\"color:#9ECBFF\">\"type\"</span><span style=\"color:#E1E4E8\">].(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">); ok </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> msgType </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"set_filters\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> filters, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> msg[</span><span style=\"color:#9ECBFF\">\"filters\"</span><span style=\"color:#E1E4E8\">].(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}); ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                client.filters </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> filter, enabled </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> filters {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> enabledBool, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> enabled.(</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        client.filters[filter] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> enabledBool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client.lastSeen </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">writeToClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">client</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">54</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client.conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> message, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">client.send:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            client.conn.</span><span style=\"color:#B392F0\">SetWriteDeadline</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                client.conn.</span><span style=\"color:#B392F0\">WriteMessage</span><span style=\"color:#E1E4E8\">(websocket.CloseMessage, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> client.conn.</span><span style=\"color:#B392F0\">WriteMessage</span><span style=\"color:#E1E4E8\">(websocket.TextMessage, message); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"WebSocket write error: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            client.conn.</span><span style=\"color:#B392F0\">SetWriteDeadline</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> client.conn.</span><span style=\"color:#B392F0\">WriteMessage</span><span style=\"color:#E1E4E8\">(websocket.PingMessage, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamingServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">sendInitialState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">client</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Send current dashboard state to newly connected client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.collector.</span><span style=\"color:#B392F0\">ExportMetrics</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> batch </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.</span><span style=\"color:#B392F0\">broadcastQuotaUpdates</span><span style=\"color:#E1E4E8\">(batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.</span><span style=\"color:#B392F0\">broadcastTierStatus</span><span style=\"color:#E1E4E8\">(batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.</span><span style=\"color:#B392F0\">broadcastSystemHealth</span><span style=\"color:#E1E4E8\">(batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Helper functions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> extractTierFromRuleID</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ruleID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Extract tier name from rule ID format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // This would depend on your rule ID naming convention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ruleID) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> ruleID[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">u</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"user\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ruleID) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> ruleID[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">i</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"ip\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ruleID) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> ruleID[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">a</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"api\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"global\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> calculateErrorRate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MetricsBatch</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> batch.PerformanceMetrics </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, metrics </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> batch.RequestMetrics {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> metrics.TotalRequests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errors </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> metrics.ErrorRequests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> total </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">/</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(total)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Rule Management Handler (api/handlers/rules.go)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> handlers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/mux</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RuleHandler</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validator   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">validation</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleValidator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    auditor     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">AuditLogger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateRule handles POST /api/v1/rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> rule </span><span style=\"color:#B392F0\">RateLimitRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Decode JSON request body into RateLimitRule struct</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate unique ID for new rule (use UUID or timestamp-based)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate rule using h.validator.ValidateRule()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check for conflicting rules with same key_pattern and priority</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Set rule creation metadata (created_at, version, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Save rule using h.ruleManager.SaveRule()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Log creation event using h.auditor.LogRuleCreation()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Trigger configuration propagation to all instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Return created rule with 201 status code</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use json.NewDecoder(r.Body).Decode() for request parsing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateRule handles PUT /api/v1/rules/{id}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract rule ID from URL path using mux.Vars(r)[\"id\"]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Load existing rule to preserve version history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Decode JSON request body into updated RateLimitRule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate updated rule ensuring operational safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Create new version entry preserving old configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update rule with incremented version number</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Save updated rule and publish change notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Log update event with before/after values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Return updated rule configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Version number should increment from existing rule.version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeleteRule handles DELETE /api/v1/rules/{id}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DeleteRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract rule ID from URL path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Load existing rule to ensure it exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check if rule can be safely deleted (no active dependencies)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Mark rule as deleted rather than removing (soft delete)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Clear all active rate limit counters for this rule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Publish rule deletion notification to all instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Log deletion event with rule configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return deletion confirmation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Soft delete preserves audit history while stopping enforcement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PreviewRule handles POST /api/v1/rules/{id}/preview</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">PreviewRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract rule ID and load rule configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Decode array of RateLimitRequest from request body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each test request, simulate rate limiting without updating counters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Use rate limiter Preview() method to get theoretical results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Collect timing information for performance analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return array of preview results with timing data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Log preview operation for audit purposes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Preview should never modify actual rate limit state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ResetRule handles POST /api/v1/rules/{id}/reset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ResetRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract rule ID and validate rule exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse optional key filter from request body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Identify all Redis keys associated with this rule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Clear counters using rate limiter Reset() method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle partial failures when some keys cannot be reset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Log reset operation with affected key count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return reset confirmation with operation summary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Key filter allows resetting specific users/IPs rather than all keys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ListRules handles GET /api/v1/rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ListRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse query parameters for filtering and pagination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Extract filters (enabled, algorithm, priority range)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Load rules matching filter criteria</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply sorting based on query parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Implement pagination using limit/offset or cursor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Calculate total count for pagination metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return paginated rule list with metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Support filters like ?enabled=true&#x26;algorithm=token_bucket&#x26;priority_min=50</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetRuleVersions handles GET /api/v1/rules/{id}/versions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetRuleVersions</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract rule ID from URL path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Load all historical versions of the rule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Sort versions by timestamp (newest first)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Include version metadata (created_by, created_at, change_reason)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply pagination if version history is large</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return version list with change summaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Each version should show what changed compared to previous version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RollbackRule handles POST /api/v1/rules/{id}/rollback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RollbackRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract rule ID and target version number from request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Load target version configuration from history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate that rollback target exists and is valid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create new version entry pointing to rolled-back configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Activate rolled-back configuration as current version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Clear any rate limit state that may be invalid after rollback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Publish configuration change notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Log rollback operation with justification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Return confirmation with new current version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Rollback creates a new version rather than deleting recent versions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Rate Limit Header Injection (api/middleware/headers.go)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> middleware</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitHeaders middleware injects standard rate limiting headers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitHeaders</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limiter </span><span style=\"color:#B392F0\">FlowCoordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRateLimitHeaders creates header injection middleware</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRateLimitHeaders</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">limiter</span><span style=\"color:#B392F0\"> FlowCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitHeaders</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RateLimitHeaders</span><span style=\"color:#E1E4E8\">{limiter: limiter}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Middleware returns HTTP middleware that adds rate limit headers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rlh </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitHeaders</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Middleware</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Extract request context (user_id, ip_address, api_endpoint)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Build RateLimitRequest from HTTP request data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Call limiter.Preview() to get current quota status without updating</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Extract most restrictive limit from multi-tier results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Calculate standard rate limit headers (X-RateLimit-*)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 6: Add extended headers for enhanced client experience</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 7: Handle multi-tier header aggregation following \"most restrictive wins\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 8: Add retry-after calculation for approaching limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 9: Set headers on response before calling next handler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Hint: Preview() gives quota info without consuming requests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// extractRequestContext builds rate limit context from HTTP request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rlh </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitHeaders</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">extractRequestContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RequestContext</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract user ID from authentication headers or JWT token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get client IP address handling proxy headers (X-Forwarded-For)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Determine API endpoint from request path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Extract user agent and other relevant headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Set request timestamp for consistent time-based calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return populated RequestContext</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check X-User-ID header, Authorization header, and X-Forwarded-For</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// calculateHeaders determines rate limit headers from flow result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rlh </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitHeaders</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">calculateHeaders</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">FlowResult</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Find most restrictive tier from result.TierResults</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Extract limit, remaining, and reset time from restrictive tier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Calculate retry-after for rate-limited requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Build standard headers map with X-RateLimit-* entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Add extended headers for algorithm and scope information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle edge case where no tiers were evaluated</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return complete headers map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Most restrictive = lowest remaining/limit ratio</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// findMostRestrictiveTier identifies tier with tightest remaining quota</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rlh </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitHeaders</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">findMostRestrictiveTier</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tiers</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TierEvaluation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TierEvaluation</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize with first tier if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through all tier results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Calculate remaining/limit ratio for each tier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Track tier with lowest ratio (most restrictive)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle ties by preferring higher priority tiers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return most restrictive tier evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Ratio of 0.1 (10% remaining) is more restrictive than 0.8 (80% remaining)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After API Implementation:</strong></p>\n<ul>\n<li>Start management API server: <code>go run cmd/management-api/main.go</code></li>\n<li>Create test rule: <code>curl -X POST http://localhost:8080/api/v1/rules -d &#39;{&quot;name&quot;:&quot;test&quot;,&quot;key_pattern&quot;:&quot;user:*&quot;,&quot;algorithm&quot;:&quot;token_bucket&quot;,&quot;limit&quot;:100,&quot;window&quot;:&quot;1m&quot;,&quot;burst_limit&quot;:150}&#39;</code></li>\n<li>Expected: Rule created with generated ID and version 1</li>\n<li>Verify: Rule appears in Redis and triggers configuration change notification</li>\n</ul>\n<p><strong>After Dashboard Implementation:</strong></p>\n<ul>\n<li>Start dashboard server: <code>go run cmd/dashboard/main.go</code>  </li>\n<li>Open browser to <code>http://localhost:3000</code></li>\n<li>Expected: Real-time dashboard showing quota utilization and system health</li>\n<li>Generate load: Run rate limiting requests and observe dashboard updates</li>\n<li>Verify: WebSocket connection established and metrics update every 2 seconds</li>\n</ul>\n<p><strong>After Self-Rate-Limiting Implementation:</strong></p>\n<ul>\n<li>Test API protection: Send 200 rapid requests to management API</li>\n<li>Expected: First 100 requests succeed, subsequent requests receive 429 status</li>\n<li>Check headers: Verify X-RateLimit-* headers present in all responses</li>\n<li>Test emergency override: Use admin API key to bypass normal limits</li>\n<li>Verify: Override allows higher request rates with audit log entries</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dashboard shows no data</td>\n<td>WebSocket connection failing</td>\n<td>Check browser dev tools for WebSocket errors</td>\n<td>Verify CORS settings and WebSocket endpoint</td>\n</tr>\n<tr>\n<td>Headers show wrong quota</td>\n<td>Multi-tier aggregation error</td>\n<td>Log tier results before header calculation</td>\n<td>Fix most-restrictive-tier selection logic</td>\n</tr>\n<tr>\n<td>Config changes not propagating</td>\n<td>Redis pub/sub subscription broken</td>\n<td>Check Redis logs for subscription errors</td>\n<td>Reconnect to Redis and restart subscription</td>\n</tr>\n<tr>\n<td>API rate limiting too aggressive</td>\n<td>Token bucket refill rate too low</td>\n<td>Check API rate limit configuration</td>\n<td>Increase refill rate or burst capacity</td>\n</tr>\n<tr>\n<td>Dashboard updates too slow</td>\n<td>Metrics collection batching issues</td>\n<td>Monitor metrics collector export frequency</td>\n<td>Reduce batching interval or increase update frequency</td>\n</tr>\n<tr>\n<td>Emergency override not working</td>\n<td>Override token validation failing</td>\n<td>Check API key validation and override logic</td>\n<td>Verify override token format and expiration</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 - Redis Backend Integration, Milestone 4 - Consistent Hashing &amp; Sharding, and foundational concepts applying to all milestones</p>\n</blockquote>\n<p>The robustness of a distributed rate limiting system fundamentally depends on how gracefully it handles failures, edge cases, and the inevitable inconsistencies that arise in distributed environments. Unlike monolithic applications where failures are typically binary (the application works or it doesn&#39;t), distributed systems must continue operating with partial functionality when components fail. This section examines the comprehensive failure scenarios, recovery strategies, and edge case handling that transform a basic distributed rate limiter into a production-ready system.</p>\n<h3 id=\"mental-model-the-emergency-response-network\">Mental Model: The Emergency Response Network</h3>\n<p>Think of distributed rate limiting like a city&#39;s emergency response network. When you call 911, the system must route your call to the right dispatcher, coordinate with multiple emergency services, and ensure help arrives even if some communication towers are down. The emergency network has multiple levels of fallback: if the primary dispatch center fails, backup centers take over; if radio communication fails, they use cellular; if all else fails, emergency vehicles operate with their last known instructions.</p>\n<p>Similarly, our distributed rate limiter must continue protecting your application even when Redis nodes fail, network partitions occur, or time synchronization drifts. The system degrades gracefully rather than failing completely, using local fallbacks when global coordination becomes impossible, just as emergency responders use local protocols when centralized coordination is unavailable.</p>\n<h3 id=\"redis-failure-scenarios\">Redis Failure Scenarios</h3>\n<p>Redis serves as the central nervous system of our distributed rate limiting infrastructure. When Redis becomes unavailable or unreliable, the entire system must adapt its behavior to maintain protection while avoiding cascading failures. Understanding and preparing for Redis failure modes represents one of the most critical aspects of building resilient distributed rate limiting.</p>\n<h4 id=\"connection-pool-exhaustion-and-recovery\">Connection Pool Exhaustion and Recovery</h4>\n<p>Connection pool exhaustion occurs when all available Redis connections become tied up in long-running operations, network timeouts, or blocked on slow Redis commands. This scenario often manifests gradually, with response times degrading before complete failure occurs.</p>\n<p>The <code>RedisStorage</code> component implements intelligent connection pool management that monitors connection health and automatically recovers from exhaustion scenarios. When connection attempts begin timing out, the system tracks failure rates and response times to detect degradation early.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Method</th>\n<th>Recovery Action</th>\n<th>Fallback Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pool exhaustion</td>\n<td>Connection timeout increase</td>\n<td>Close idle connections, expand pool temporarily</td>\n<td>Switch to local limiting</td>\n</tr>\n<tr>\n<td>Slow Redis responses</td>\n<td>Response time monitoring</td>\n<td>Circuit breaker activation</td>\n<td>Cache last known limits</td>\n</tr>\n<tr>\n<td>Network packet loss</td>\n<td>Retry failure patterns</td>\n<td>TCP connection reset</td>\n<td>Degrade to approximate limiting</td>\n</tr>\n<tr>\n<td>Redis memory pressure</td>\n<td>Error code analysis</td>\n<td>Reduce TTL on keys</td>\n<td>Local rate limiting only</td>\n</tr>\n</tbody></table>\n<p>The <code>CircuitBreaker</code> component provides the primary mechanism for detecting and responding to Redis degradation. It tracks success/failure ratios and response times, automatically switching to local fallback when Redis becomes unreliable.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Circuit Breaker State Machine:\n- Closed (normal): All requests go to Redis, track failure rate\n- Open (failed): All requests use local fallback, periodic health checks\n- Half-Open (testing): Limited requests test Redis recovery</code></pre></div>\n\n<blockquote>\n<p><strong>Decision: Circuit Breaker vs Retry Logic</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to handle Redis failures without overwhelming failed instances</li>\n<li><strong>Options Considered</strong>: Simple retry with exponential backoff, circuit breaker pattern, combination approach</li>\n<li><strong>Decision</strong>: Circuit breaker with limited retries for each state</li>\n<li><strong>Rationale</strong>: Circuit breakers prevent thundering herd problems and give failed Redis instances time to recover without constant retry pressure</li>\n<li><strong>Consequences</strong>: More complex state management but much better failure isolation and faster recovery detection</li>\n</ul>\n</blockquote>\n<h4 id=\"memory-pressure-and-eviction-handling\">Memory Pressure and Eviction Handling</h4>\n<p>Redis memory pressure creates particularly challenging scenarios because it can cause unpredictable key eviction, leading to rate limiting state loss. When Redis approaches its memory limit, it begins evicting keys according to its eviction policy, which may remove active rate limiting counters.</p>\n<p>The system implements several strategies to detect and handle memory-induced state loss. The <code>HealthChecker</code> component monitors Redis memory usage through the INFO command and adjusts rate limiting behavior when memory pressure is detected.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Condition</th>\n<th>Detection Threshold</th>\n<th>Automatic Response</th>\n<th>Operator Alert</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High usage</td>\n<td>&gt; 80% of max memory</td>\n<td>Reduce key TTLs by 50%</td>\n<td>Warning notification</td>\n</tr>\n<tr>\n<td>Critical pressure</td>\n<td>&gt; 95% of max memory</td>\n<td>Switch to local fallback</td>\n<td>Critical alert</td>\n</tr>\n<tr>\n<td>Eviction detected</td>\n<td>Keys disappearing unexpectedly</td>\n<td>Reset all affected counters</td>\n<td>Data loss warning</td>\n</tr>\n<tr>\n<td>Out of memory</td>\n<td>Redis refuses writes</td>\n<td>Full local fallback mode</td>\n<td>Emergency alert</td>\n</tr>\n</tbody></table>\n<p>When eviction is detected, the system faces a critical decision: should it assume evicted rate limiting keys represent exhausted quotas (deny requests) or available quotas (allow requests)? The safest approach biases toward protection, treating evicted keys as exhausted unless explicitly configured otherwise.</p>\n<h4 id=\"split-brain-and-network-partition-handling\">Split-Brain and Network Partition Handling</h4>\n<p>Network partitions can split the Redis cluster into multiple segments, each believing it represents the authoritative state. During partitions, different application instances may connect to different Redis nodes, leading to independent rate limiting decisions that violate global limits.</p>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Ffailure-handling-flow.svg\" alt=\"Redis Failure Handling Flow\"></p>\n<p>The <code>HashRing</code> component implements partition detection by monitoring which nodes remain reachable from each application instance. When a partition is detected, the system must choose between availability and consistency.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: During network partitions, we prioritize application availability over strict rate limiting accuracy. It&#39;s better to allow slightly more traffic than configured limits than to reject legitimate requests due to infrastructure failures.</p>\n</blockquote>\n<p>The partition handling algorithm follows these decision steps:</p>\n<ol>\n<li><strong>Detect partition</strong>: Monitor node reachability and cross-check with other application instances</li>\n<li><strong>Assess majority</strong>: Determine which partition contains the majority of Redis nodes</li>\n<li><strong>Minority partition response</strong>: Instances in minority partitions switch to local fallback</li>\n<li><strong>Majority partition response</strong>: Continue distributed limiting with reduced cluster</li>\n<li><strong>Partition healing</strong>: Gradually restore full distributed coordination as nodes reconnect</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Partition Scenario</th>\n<th>Instance Response</th>\n<th>Rate Limiting Behavior</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Isolated single node</td>\n<td>Switch to local fallback</td>\n<td>Per-instance limits only</td>\n<td>Rejoin when reachable</td>\n</tr>\n<tr>\n<td>Minority partition</td>\n<td>Degrade to local mode</td>\n<td>Conservative local limits</td>\n<td>Wait for majority contact</td>\n</tr>\n<tr>\n<td>Majority partition</td>\n<td>Continue distributed</td>\n<td>Adjust for lost capacity</td>\n<td>Gradually restore full state</td>\n</tr>\n<tr>\n<td>Complete isolation</td>\n<td>Full local fallback</td>\n<td>Emergency rate limits</td>\n<td>Manual intervention</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-corruption-and-inconsistency-detection\">Data Corruption and Inconsistency Detection</h4>\n<p>Redis data corruption can occur due to hardware failures, software bugs, or operational errors. Rate limiting counters may become corrupted, leading to wildly incorrect values that either completely block traffic or allow unlimited requests.</p>\n<p>The system implements data validation within Lua scripts to detect obviously corrupted values. Rate limiting counters that show impossible values (negative tokens, timestamps from the far future, counters exceeding configured limits by orders of magnitude) trigger automatic correction.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Data Validation Rules:\n- Token bucket tokens must be between 0 and configured capacity\n- Sliding window timestamps must be within reasonable time bounds\n- Counter values must not exceed limit * time_window_multiplier\n- State update timestamps must be monotonically increasing</code></pre></div>\n\n<h3 id=\"clock-skew-and-time-synchronization\">Clock Skew and Time Synchronization</h3>\n<p>Time represents the fundamental dimension for rate limiting, making clock synchronization critical for distributed coordination. When application instances and Redis nodes have different perceptions of current time, rate limiting windows become misaligned, leading to inaccurate quota tracking and unpredictable behavior.</p>\n<h4 id=\"detecting-and-measuring-clock-skew\">Detecting and Measuring Clock Skew</h4>\n<p>Clock skew detection requires comparing timestamps between distributed components while accounting for network latency. The <code>TimeProvider</code> component implements active clock skew measurement by periodically synchronizing with Redis server time and calculating drift.</p>\n<p>The detection algorithm works by:</p>\n<ol>\n<li><strong>Record local timestamp</strong> before Redis operation</li>\n<li><strong>Execute Redis TIME command</strong> to get server timestamp  </li>\n<li><strong>Record local timestamp</strong> after Redis response</li>\n<li><strong>Calculate network latency</strong> as (local_after - local_before) / 2</li>\n<li><strong>Determine skew</strong> as redis_time - local_time - estimated_latency</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Skew Magnitude</th>\n<th>Impact</th>\n<th>Detection Method</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&lt; 100ms</td>\n<td>Negligible</td>\n<td>Passive monitoring</td>\n<td>No action required</td>\n</tr>\n<tr>\n<td>100ms - 1s</td>\n<td>Boundary inaccuracy</td>\n<td>Active measurement</td>\n<td>Use Redis timestamps</td>\n</tr>\n<tr>\n<td>1s - 10s</td>\n<td>Window misalignment</td>\n<td>Continuous tracking</td>\n<td>Force Redis time sync</td>\n</tr>\n<tr>\n<td>&gt; 10s</td>\n<td>Severe inaccuracy</td>\n<td>Alert generation</td>\n<td>Manual time sync required</td>\n</tr>\n</tbody></table>\n<p>When significant skew is detected, the system can choose between several timestamp strategies. Using Redis server time ensures consistency but adds latency to every operation. Using local time with skew correction provides better performance but introduces complexity.</p>\n<blockquote>\n<p><strong>Decision: Redis Server Time vs Local Time Correction</strong></p>\n<ul>\n<li><strong>Context</strong>: Need consistent timestamps across distributed rate limiters with varying clock skew</li>\n<li><strong>Options Considered</strong>: Always use Redis TIME, local time with measured correction, hybrid approach based on skew magnitude</li>\n<li><strong>Decision</strong>: Hybrid approach - use local time when skew &lt; 500ms, Redis time when skew &gt; 500ms</li>\n<li><strong>Rationale</strong>: Balances accuracy requirements with performance impact; most systems have reasonable time sync</li>\n<li><strong>Consequences</strong>: Requires skew monitoring overhead but provides optimal performance/accuracy trade-off</li>\n</ul>\n</blockquote>\n<h4 id=\"window-boundary-synchronization\">Window Boundary Synchronization</h4>\n<p>Sliding window algorithms become particularly sensitive to clock skew at window boundaries. When different instances calculate window boundaries using different timestamps, the same request may fall into different time buckets, causing counter fragmentation and inaccurate limits.</p>\n<p>The <code>SlidingWindowCounter</code> implementation addresses boundary synchronization by quantizing all timestamps to common boundary points. Instead of using precise timestamps, the system rounds timestamps to the nearest window subdivision.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Boundary Synchronization Algorithm:\n1. Define window_size (e.g., 60 seconds) and bucket_count (e.g., 6 buckets)\n2. Calculate bucket_duration = window_size / bucket_count (10 seconds)\n3. Quantize timestamp: bucket_start = (timestamp / bucket_duration) * bucket_duration\n4. All instances use identical bucket_start times regardless of local clock skew</code></pre></div>\n\n<p>This quantization approach ensures that all distributed instances agree on bucket boundaries, even with moderate clock skew. The trade-off is slightly less precise timestamp handling in exchange for much better distributed consistency.</p>\n<h4 id=\"handling-timestamp-rollback\">Handling Timestamp Rollback</h4>\n<p>Timestamp rollback occurs when system administrators correct clock synchronization, causing time to appear to move backward. Rate limiting algorithms that depend on monotonically increasing timestamps can behave unpredictably when timestamps decrease.</p>\n<p>The <code>TokenBucket</code> algorithm handles rollback by detecting backward time movement and choosing appropriate responses:</p>\n<table>\n<thead>\n<tr>\n<th>Rollback Magnitude</th>\n<th>Detection Method</th>\n<th>Response Strategy</th>\n<th>State Adjustment</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&lt; 1 second</td>\n<td>Compare with last_refill_time</td>\n<td>Ignore update, use cached time</td>\n<td>No state change</td>\n</tr>\n<tr>\n<td>1-60 seconds</td>\n<td>Timestamp decrease detection</td>\n<td>Reset refill calculations</td>\n<td>Clear accumulated tokens</td>\n</tr>\n<tr>\n<td>&gt; 60 seconds</td>\n<td>Large backward jump</td>\n<td>Full state reset</td>\n<td>Start with empty bucket</td>\n</tr>\n<tr>\n<td>Clock sync correction</td>\n<td>NTP adjustment patterns</td>\n<td>Gradual time correction</td>\n<td>Smooth state transition</td>\n</tr>\n</tbody></table>\n<p>For sliding window algorithms, timestamp rollback requires careful handling to avoid double-counting requests. The system maintains a grace period where recent timestamps remain valid even if new timestamps appear earlier.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Timestamp Rollback</strong>\nMany implementations assume timestamps always increase, leading to broken rate limiting when system clocks are corrected. Always check for backward time movement and have a explicit strategy for handling it.</p>\n<h3 id=\"race-condition-prevention\">Race Condition Prevention</h3>\n<p>Distributed rate limiting inherently involves race conditions where multiple instances simultaneously check and update shared counters. Without proper coordination, these race conditions can lead to double-counting, lost updates, or quota violations that allow traffic beyond configured limits.</p>\n<h4 id=\"atomic-operations-with-lua-scripts\">Atomic Operations with Lua Scripts</h4>\n<p>Redis Lua scripts provide the foundation for atomic rate limiting operations by ensuring that check-and-update sequences execute without interruption. The <code>ExecuteLua</code> method encapsulates complex rate limiting logic in atomic scripts that eliminate race conditions.</p>\n<p>The token bucket Lua script demonstrates the atomic operation pattern:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Token Bucket Lua Script Logic:\n1. GET current token count and last refill timestamp\n2. Calculate tokens to add based on elapsed time and refill rate\n3. Update token count (capped at bucket capacity)\n4. IF requested tokens &lt;= available tokens THEN\n   5. Subtract requested tokens\n   6. SET new token count and timestamp\n   7. RETURN success with remaining tokens\n8. ELSE\n   9. SET updated token count (with refill but no consumption)\n   10. RETURN failure with retry delay</code></pre></div>\n\n<p>All state reads, calculations, and writes occur atomically within the single Lua script execution. This eliminates the race condition window that would exist with separate GET, calculate, and SET operations.</p>\n<table>\n<thead>\n<tr>\n<th>Race Condition Type</th>\n<th>Without Lua Scripts</th>\n<th>With Lua Scripts</th>\n<th>Prevention Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Read-modify-write</td>\n<td>Multiple updates lost</td>\n<td>Single atomic update</td>\n<td>Lua script atomicity</td>\n</tr>\n<tr>\n<td>Check-then-act</td>\n<td>State changes between check/act</td>\n<td>Atomic check-and-act</td>\n<td>Single script execution</td>\n</tr>\n<tr>\n<td>Counter increment</td>\n<td>Lost increment operations</td>\n<td>Atomic counter update</td>\n<td>Lua INCR operations</td>\n</tr>\n<tr>\n<td>Timestamp comparison</td>\n<td>Inconsistent time reads</td>\n<td>Single timestamp read</td>\n<td>Lua script locality</td>\n</tr>\n</tbody></table>\n<h4 id=\"optimistic-vs-pessimistic-concurrency\">Optimistic vs Pessimistic Concurrency</h4>\n<p>The system implements optimistic concurrency control, assuming that conflicts are rare and handling them through retry mechanisms rather than preventing them with locks. This approach provides better performance under normal conditions while gracefully handling conflicts when they occur.</p>\n<p>Optimistic concurrency in rate limiting works by:</p>\n<ol>\n<li><strong>Read current state</strong> without acquiring locks</li>\n<li><strong>Calculate new state</strong> based on read values</li>\n<li><strong>Attempt atomic update</strong> with conditional logic</li>\n<li><strong>Retry on conflict</strong> if state changed during calculation</li>\n<li><strong>Apply exponential backoff</strong> to prevent retry storms</li>\n</ol>\n<p>The <code>DistributedLimiter</code> component implements retry logic with jitter to prevent thundering herd problems when many instances simultaneously retry failed operations.</p>\n<blockquote>\n<p><strong>Decision: Optimistic vs Pessimistic Concurrency</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to handle concurrent rate limit checks across many application instances</li>\n<li><strong>Options Considered</strong>: Redis locks (pessimistic), optimistic retry with Lua scripts, hybrid locking for hot keys</li>\n<li><strong>Decision</strong>: Optimistic concurrency with exponential backoff retry</li>\n<li><strong>Rationale</strong>: Rate limiting operations are fast and conflicts are relatively rare; pessimistic locking adds significant overhead and can create deadlock risks</li>\n<li><strong>Consequences</strong>: Better performance under normal load but requires careful retry logic and backoff strategies</li>\n</ul>\n</blockquote>\n<h4 id=\"hot-key-conflict-resolution\">Hot Key Conflict Resolution</h4>\n<p>Hot keys create concentrated conflict points where many instances simultaneously attempt to update the same rate limiting counters. The <code>HotKeyDetector</code> identifies these high-contention keys and applies specialized conflict resolution strategies.</p>\n<p>When hot keys are detected, the system can apply several conflict reduction techniques:</p>\n<table>\n<thead>\n<tr>\n<th>Hot Key Strategy</th>\n<th>Mechanism</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key sharding</td>\n<td>Split single key into multiple sub-keys</td>\n<td>Reduces contention</td>\n<td>Complex aggregation logic</td>\n</tr>\n<tr>\n<td>Local caching</td>\n<td>Cache counter values briefly</td>\n<td>Fewer Redis operations</td>\n<td>Potential accuracy loss</td>\n</tr>\n<tr>\n<td>Retry backoff</td>\n<td>Exponential backoff with jitter</td>\n<td>Reduces retry storms</td>\n<td>Higher latency for some requests</td>\n</tr>\n<tr>\n<td>Priority queuing</td>\n<td>Process requests by priority</td>\n<td>Important requests succeed</td>\n<td>Complex queue management</td>\n</tr>\n</tbody></table>\n<p>The key sharding approach proves most effective for extremely hot keys, where a single rate limit key is split into multiple sub-keys and requests are randomly distributed across them. The trade-off is more complex Lua scripts that must aggregate across sub-keys to check total usage.</p>\n<h4 id=\"preventing-lost-updates-in-sliding-windows\">Preventing Lost Updates in Sliding Windows</h4>\n<p>Sliding window algorithms face particular challenges with concurrent updates because multiple bucket updates may occur simultaneously as time progresses. The <code>SlidingWindowLog</code> implementation prevents lost updates by using Redis list operations that are inherently atomic.</p>\n<p>The atomic sliding window update sequence:</p>\n<ol>\n<li><strong>LPUSH new timestamp</strong> to the request log list</li>\n<li><strong>ZREMRANGEBYSCORE</strong> to remove timestamps outside current window</li>\n<li><strong>ZCARD</strong> to count remaining timestamps in window</li>\n<li><strong>Compare count against limit</strong> and return result</li>\n</ol>\n<p>This sequence uses Redis&#39;s atomic list and sorted set operations to ensure that concurrent updates don&#39;t interfere with each other. Each step is atomic at the Redis level, and the overall sequence runs within a Lua script for complete atomicity.</p>\n<p>⚠️ <strong>Pitfall: Non-Atomic Window Updates</strong>\nAttempting to implement sliding windows with separate GET, filter, and SET operations creates race conditions where concurrent requests can cause lost updates or double-counting. Always use atomic operations for the complete window update sequence.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical implementation patterns for robust error handling and edge case management in distributed rate limiting systems.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time synchronization</td>\n<td>NTP client with periodic sync</td>\n<td>Hardware-assisted clock sync</td>\n</tr>\n<tr>\n<td>Circuit breaker</td>\n<td>Simple state machine</td>\n<td>Hystrix-style metrics-based breaker</td>\n</tr>\n<tr>\n<td>Health checking</td>\n<td>Basic ping/pong</td>\n<td>Comprehensive Redis INFO monitoring</td>\n</tr>\n<tr>\n<td>Clock skew detection</td>\n<td>Periodic Redis TIME calls</td>\n<td>Continuous drift measurement</td>\n</tr>\n<tr>\n<td>Retry logic</td>\n<td>Exponential backoff</td>\n<td>Adaptive backoff with jitter</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/resilience/\n  circuit_breaker.go          ← Circuit breaker implementation\n  circuit_breaker_test.go     ← Circuit breaker unit tests\n  health_checker.go           ← Redis health monitoring\n  time_provider.go            ← Clock skew detection and sync\n  retry_manager.go            ← Retry logic with backoff\n  fallback_limiter.go         ← Local fallback rate limiting\ninternal/redis/\n  failure_detector.go         ← Redis failure detection\n  connection_pool.go          ← Connection pool management\n  lua_scripts.go              ← Atomic Lua script execution</code></pre></div>\n\n<h4 id=\"circuit-breaker-infrastructure\">Circuit Breaker Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> resilience</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CircuitState represents the current state of the circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitClosed</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span><span style=\"color:#6A737D\">    // Normal operation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitOpen</span><span style=\"color:#6A737D\">                          // Rejecting all requests</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitHalfOpen</span><span style=\"color:#6A737D\">                     // Testing recovery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CircuitBreaker implements failure detection and graceful degradation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu                </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state             </span><span style=\"color:#B392F0\">CircuitState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureCount      </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFailureTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nextRetryTime     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureThreshold  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recoveryTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCircuitBreaker creates a circuit breaker with the specified configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCircuitBreaker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">failureThreshold</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">recoveryTimeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state:            CircuitClosed,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        failureThreshold: failureThreshold,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        recoveryTimeout:  recoveryTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Execute runs the provided function with circuit breaker protection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Execute</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fn</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check current circuit state and decide whether to allow execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If circuit is Open and recovery timeout hasn't elapsed, return error immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If circuit is Open and recovery timeout has elapsed, transition to HalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute the function and capture any error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update circuit state based on execution result (success/failure)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If in HalfOpen state, transition to Closed on success or Open on failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: If in Closed state, increment failure count on error and open if threshold exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use atomic operations or mutex for thread safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Track both failure count and failure rate over time windows</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement circuit breaker execution logic\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"health-checker-implementation\">Health Checker Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> resilience</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/redis/go-redis/v9</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeHealth represents the health status of a single Redis node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeHealth</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID              </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address             </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastSeen           </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsecutiveFailures </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AverageLatency     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IsHealthy          </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryUsage        </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConnectionCount    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthConfig configures health checking behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CheckInterval      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailureThreshold   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RecoveryThreshold  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LatencyThreshold   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryThreshold    </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthChecker monitors Redis cluster health and triggers failover actions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthChecker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes           </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeHealth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreakers </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config          </span><span style=\"color:#B392F0\">HealthConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckNodeHealth performs comprehensive health check on a Redis node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckNodeHealth</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeHealth</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create Redis client for the specified node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute PING command and measure response time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute INFO command to get memory usage and connection count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse INFO response to extract memory_used, memory_total, connected_clients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate memory usage percentage and compare against threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update NodeHealth struct with current metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Determine overall health status based on latency, memory, and recent failures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update consecutive failure count based on current check result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context timeout to avoid hanging on unresponsive nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Parse INFO command output to extract specific metrics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement comprehensive Redis node health checking\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleNodeFailure coordinates response to a detected node failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleNodeFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Mark node as unhealthy in the health map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Trigger circuit breaker for this node to prevent further requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Notify hash ring to remove node from active rotation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Log failure event with detailed diagnostic information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Schedule accelerated health checks to detect recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update metrics to track node failure for monitoring dashboard</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Coordinate with HashRing component to redistribute load</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement exponential backoff for recovery health checks</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement node failure response coordination\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"time-synchronization-and-clock-skew-detection\">Time Synchronization and Clock Skew Detection</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> resilience</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/redis/go-redis/v9</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeProvider handles clock synchronization and skew detection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeProvider</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redisClient   </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clockSkew     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastSync      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    syncInterval  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTimeProvider creates a time provider with Redis synchronization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTimeProvider</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">redisClient</span><span style=\"color:#B392F0\"> redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">syncInterval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeProvider</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TimeProvider</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        redisClient:  redisClient,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        syncInterval: syncInterval,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Now returns the current time adjusted for measured clock skew</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeProvider</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get current local time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if clock skew measurement is recent enough (within sync interval)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If skew measurement is stale, trigger background sync (don't block)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply measured clock skew correction to local time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return corrected timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Don't block on sync - return local time if Redis is unavailable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use atomic operations for reading clockSkew to avoid races</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement skew-corrected timestamp generation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MeasureClockSkew compares local time with Redis server time to detect skew</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeProvider</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">MeasureClockSkew</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Record local timestamp before Redis call</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute Redis TIME command to get server timestamp  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Record local timestamp after Redis response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse Redis TIME response (seconds, microseconds)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate network latency as (local_after - local_before) / 2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Calculate clock skew as redis_time - local_time - estimated_latency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update internal skew tracking with new measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return measured skew value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Redis TIME returns [seconds, microseconds] array</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Account for network latency in skew calculation</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement clock skew measurement against Redis server time\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"local-fallback-rate-limiter\">Local Fallback Rate Limiter</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> resilience</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FallbackLimiter provides local rate limiting when Redis is unavailable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FallbackLimiter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokenBuckets    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucketState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    windowCounters  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WindowCounterState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    defaultConfig   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanupInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TokenBucketState tracks local token bucket state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucketState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens          </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastRefillTime  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity        </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refillRate      </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WindowCounterState tracks local sliding window state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WindowCounterState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    requests    []</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    windowSize  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limit       </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewFallbackLimiter creates a local fallback rate limiter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewFallbackLimiter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">defaultConfig</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">TokenBucketConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FallbackLimiter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">FallbackLimiter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokenBuckets:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucketState</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        windowCounters:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WindowCounterState</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        defaultConfig:   defaultConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cleanupInterval: time.Minute </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckLocal performs rate limiting using local state only</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FallbackLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckLocal</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tokens</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitRule</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Determine which algorithm to use based on rule configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get or create local state for the specified key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply token bucket algorithm using local timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Calculate refill tokens based on elapsed time since last access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check if requested tokens are available in bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update local state if tokens are consumed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Calculate retry_after time if request is denied</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return RateLimitResult with local state information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use local time only - don't depend on Redis for timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement cleanup to prevent memory leaks from abandoned keys</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement local fallback rate limiting\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 3 Checkpoint - Redis Failure Handling:</strong>\nAfter implementing circuit breakers and local fallback:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test Redis failure scenarios</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/resilience/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/redis/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual testing:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 1. Start rate limiter with Redis backend</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 2. Send requests and verify normal operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 3. Stop Redis instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 4. Verify automatic fallback to local limiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 5. Restart Redis and verify recovery</span></span></code></pre></div>\n\n<p>Expected behavior: Rate limiting continues during Redis outage with degraded accuracy but maintained protection. Recovery should be automatic and seamless.</p>\n<p><strong>Milestone 4 Checkpoint - Clock Skew and Sharding:</strong>\nAfter implementing time synchronization and hot key handling:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test clock skew scenarios</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/resilience/...</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestClockSkew</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/sharding/...</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestHotKey</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual testing:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 1. Run multiple instances with deliberately skewed clocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 2. Verify rate limiting accuracy despite time differences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 3. Generate hot key traffic and verify conflict resolution</span></span></code></pre></div>\n\n<p>Expected behavior: Rate limiting accuracy within 5% even with moderate clock skew. Hot key detection should trigger within 30 seconds of elevated traffic.</p>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate limits too strict during Redis outage</td>\n<td>Local fallback using wrong limits</td>\n<td>Check fallback configuration and rule inheritance</td>\n<td>Configure appropriate local fallback limits</td>\n</tr>\n<tr>\n<td>Inconsistent rate limiting across instances</td>\n<td>Clock skew between nodes</td>\n<td>Compare timestamps in logs, measure skew</td>\n<td>Implement NTP sync and skew correction</td>\n</tr>\n<tr>\n<td>Circuit breaker stuck open</td>\n<td>Recovery timeout too long or health checks failing</td>\n<td>Check Redis connectivity and error patterns</td>\n<td>Tune recovery timeout and failure thresholds</td>\n</tr>\n<tr>\n<td>Rate limiting fails completely during network partition</td>\n<td>No fallback strategy implemented</td>\n<td>Check for local limiting during Redis failures</td>\n<td>Implement and test graceful degradation</td>\n</tr>\n<tr>\n<td>Hot keys causing Redis CPU spikes</td>\n<td>Too many concurrent updates to same keys</td>\n<td>Monitor Redis slow log and key access patterns</td>\n<td>Implement key sharding or request batching</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - testing strategies span rate limiting algorithms (Milestone 1), multi-tier evaluation (Milestone 2), Redis backend integration (Milestone 3), sharding and consistent hashing (Milestone 4), and API/dashboard functionality (Milestone 5)</p>\n</blockquote>\n<h3 id=\"mental-model-the-quality-assurance-laboratory-system\">Mental Model: The Quality Assurance Laboratory System</h3>\n<p>Think of testing a distributed rate limiter like running a comprehensive quality assurance laboratory for a pharmaceutical company. Just as a pharmaceutical lab has multiple testing phases - from testing individual compounds in isolation (unit testing), to testing drug interactions in controlled environments (integration testing), to testing complete treatment protocols in clinical trials (end-to-end testing), and finally to stress-testing drugs under extreme conditions (chaos testing) - our distributed rate limiter requires a multi-layered testing approach.</p>\n<p>The individual algorithms are like chemical compounds that must be tested for purity and effectiveness in isolation. The distributed system interactions are like drug combinations that must be tested for dangerous interactions. The milestone verification checkpoints are like clinical trial phases that must be passed before advancing to more complex testing. And chaos engineering is like testing medications under extreme stress conditions to ensure they remain safe and effective when patients are critically ill.</p>\n<p>This analogy helps us understand why each testing layer is essential - a drug that works perfectly in isolation might fail catastrophically when combined with other medications or under stress conditions, just as rate limiting algorithms that work perfectly locally might fail when distributed across multiple nodes or under high load.</p>\n<h3 id=\"algorithm-unit-testing\">Algorithm Unit Testing</h3>\n<p>The foundation of testing distributed rate limiting systems begins with rigorous verification of individual algorithms in complete isolation. Each rate limiting algorithm represents a mathematical model with precise behavioral expectations, and unit testing must validate both the happy path scenarios and the critical boundary conditions where algorithms typically break down.</p>\n<p><strong>Token Bucket Algorithm Testing Strategy</strong></p>\n<p>Token bucket testing requires careful verification of the refill mechanism, burst handling, and state transitions. The algorithm maintains an internal state consisting of current token count, last refill timestamp, capacity, and refill rate. Each of these state components must be tested independently and in combination.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Case</th>\n<th>Expected Behavior</th>\n<th>Boundary Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Refill Logic</td>\n<td>Initial state after creation</td>\n<td>Bucket starts at full capacity</td>\n<td>Time t=0 initialization</td>\n</tr>\n<tr>\n<td>Refill Logic</td>\n<td>Refill after exact window duration</td>\n<td>Tokens = min(capacity, current + refill_rate)</td>\n<td>Exact window boundary</td>\n</tr>\n<tr>\n<td>Refill Logic</td>\n<td>Partial refill with fractional time</td>\n<td>Proportional token addition</td>\n<td>Sub-second precision</td>\n</tr>\n<tr>\n<td>Burst Handling</td>\n<td>Request exactly at capacity</td>\n<td>All requests allowed instantly</td>\n<td>Full bucket depletion</td>\n</tr>\n<tr>\n<td>Burst Handling</td>\n<td>Request exceeding capacity</td>\n<td>Excess requests denied</td>\n<td>Overflow protection</td>\n</tr>\n<tr>\n<td>State Persistence</td>\n<td>Token count after partial consumption</td>\n<td>Accurate remaining count</td>\n<td>Fractional tokens</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Multiple simultaneous requests</td>\n<td>Atomic state updates</td>\n<td>Race condition prevention</td>\n</tr>\n<tr>\n<td>Time Manipulation</td>\n<td>Clock moving backwards</td>\n<td>Graceful degradation</td>\n<td>Clock skew scenarios</td>\n</tr>\n</tbody></table>\n<p>The token bucket algorithm testing must simulate time progression artificially to verify refill behavior without waiting for real time to elapse. This requires dependency injection of a <code>TimeProvider</code> interface that can be controlled during testing.</p>\n<p><strong>Sliding Window Counter Algorithm Testing Strategy</strong></p>\n<p>Sliding window counter testing focuses on bucket management, weight calculation accuracy, and window transition handling. The algorithm divides time into discrete buckets and maintains counters for each bucket, requiring precise weight calculation when the current time falls between bucket boundaries.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Case</th>\n<th>Expected Behavior</th>\n<th>Critical Edge Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bucket Creation</td>\n<td>First request in new window</td>\n<td>Initialize bucket with count=1</td>\n<td>Empty state initialization</td>\n</tr>\n<tr>\n<td>Bucket Transitions</td>\n<td>Request at exact window boundary</td>\n<td>Proper bucket rotation</td>\n<td>Precision timing</td>\n</tr>\n<tr>\n<td>Weight Calculation</td>\n<td>Request mid-bucket</td>\n<td>Accurate weighted sum</td>\n<td>Fractional weights</td>\n</tr>\n<tr>\n<td>Window Sliding</td>\n<td>Old bucket expiration</td>\n<td>Automatic cleanup</td>\n<td>Memory management</td>\n</tr>\n<tr>\n<td>Count Aggregation</td>\n<td>Multiple buckets active</td>\n<td>Correct total calculation</td>\n<td>Boundary spanning</td>\n</tr>\n<tr>\n<td>Precision Testing</td>\n<td>Sub-second request timing</td>\n<td>Nanosecond accuracy</td>\n<td>High-frequency requests</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>Long-running operation</td>\n<td>Bounded memory usage</td>\n<td>Garbage collection</td>\n</tr>\n</tbody></table>\n<p><strong>Sliding Window Log Algorithm Testing Strategy</strong></p>\n<p>Sliding window log testing requires validation of timestamp storage, log rotation, and memory management. Unlike counter-based approaches, the log algorithm stores individual request timestamps, making memory usage testing critical.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Case</th>\n<th>Expected Behavior</th>\n<th>Memory Consideration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Log Insertion</td>\n<td>First timestamp storage</td>\n<td>Accurate timestamp recording</td>\n<td>Initial allocation</td>\n</tr>\n<tr>\n<td>Log Rotation</td>\n<td>Timestamp expiration</td>\n<td>Automatic log cleanup</td>\n<td>Memory deallocation</td>\n</tr>\n<tr>\n<td>Binary Search</td>\n<td>Threshold calculation</td>\n<td>Efficient timestamp lookup</td>\n<td>O(log n) complexity</td>\n</tr>\n<tr>\n<td>Memory Bounds</td>\n<td>High request volume</td>\n<td>Controlled memory growth</td>\n<td>Maximum log size</td>\n</tr>\n<tr>\n<td>Timestamp Precision</td>\n<td>Nanosecond accuracy</td>\n<td>Precise timing storage</td>\n<td>Clock resolution</td>\n</tr>\n<tr>\n<td>Concurrent Logging</td>\n<td>Multiple simultaneous requests</td>\n<td>Thread-safe log updates</td>\n<td>Lock contention</td>\n</tr>\n</tbody></table>\n<p><strong>Algorithm Comparison and Performance Testing</strong></p>\n<p>Beyond individual algorithm testing, comprehensive benchmarks must compare algorithm performance characteristics under various load patterns. This comparison testing reveals the trade-offs between accuracy, memory usage, and computational overhead.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Memory Complexity</th>\n<th>Time Complexity</th>\n<th>Accuracy</th>\n<th>Burst Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket</td>\n<td>O(1) constant</td>\n<td>O(1) per request</td>\n<td>Exact for bursts</td>\n<td>Excellent</td>\n</tr>\n<tr>\n<td>Sliding Window Counter</td>\n<td>O(buckets)</td>\n<td>O(1) per request</td>\n<td>Approximate</td>\n<td>Good</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n<td>O(requests in window)</td>\n<td>O(log n) per request</td>\n<td>Exact</td>\n<td>Excellent</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: Algorithm testing must validate not just correctness but also performance characteristics under load. A token bucket that works correctly for 10 requests per second might fail catastrophically at 10,000 requests per second due to lock contention or memory allocation overhead.</p>\n</blockquote>\n<p><strong>Boundary Condition Testing Matrix</strong></p>\n<p>Rate limiting algorithms face their greatest challenges at boundary conditions where time windows transition, numeric limits approach maximum values, or system resources become constrained. Comprehensive boundary testing prevents production failures.</p>\n<table>\n<thead>\n<tr>\n<th>Boundary Type</th>\n<th>Test Scenario</th>\n<th>Algorithm Impact</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time Window Edges</td>\n<td>Request at exact window start</td>\n<td>Counter reset timing</td>\n<td>Millisecond precision testing</td>\n</tr>\n<tr>\n<td>Time Window Edges</td>\n<td>Request at exact window end</td>\n<td>Bucket rotation</td>\n<td>Boundary value analysis</td>\n</tr>\n<tr>\n<td>Numeric Limits</td>\n<td>Request count approaching int64 max</td>\n<td>Overflow protection</td>\n<td>Large value injection</td>\n</tr>\n<tr>\n<td>Numeric Limits</td>\n<td>Token count exceeding capacity</td>\n<td>Burst limitation</td>\n<td>Boundary value testing</td>\n</tr>\n<tr>\n<td>System Resources</td>\n<td>Memory pressure during operation</td>\n<td>Graceful degradation</td>\n<td>Resource constraint simulation</td>\n</tr>\n<tr>\n<td>Clock Anomalies</td>\n<td>System clock adjustment</td>\n<td>Time skew handling</td>\n<td>Clock manipulation testing</td>\n</tr>\n</tbody></table>\n<h3 id=\"distributed-integration-testing\">Distributed Integration Testing</h3>\n<p>Moving beyond individual algorithm testing, distributed integration testing validates the behavior of multiple rate limiter instances coordinating through Redis. This testing phase reveals issues that only emerge when multiple processes compete for shared resources and coordinate state changes.</p>\n<p><strong>Multi-Instance Coordination Testing</strong></p>\n<p>The core challenge in distributed rate limiting lies in ensuring accurate limit enforcement when multiple application instances simultaneously evaluate and update shared counters. Integration testing must simulate realistic deployment scenarios with multiple instances processing concurrent requests.</p>\n<p>The test environment requires multiple rate limiter instances configured to use the same Redis backend, with careful orchestration of request timing to trigger race conditions and coordination challenges. Each test scenario must verify that distributed behavior matches single-instance behavior within acceptable accuracy bounds.</p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Instance Count</th>\n<th>Request Pattern</th>\n<th>Expected Outcome</th>\n<th>Accuracy Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simultaneous Burst</td>\n<td>3 instances</td>\n<td>100 requests/instance instantly</td>\n<td>Total allowed ≤ limit + burst</td>\n<td>5% tolerance</td>\n</tr>\n<tr>\n<td>Gradual Ramp-up</td>\n<td>5 instances</td>\n<td>Linear increase over 60 seconds</td>\n<td>Smooth limit enforcement</td>\n<td>2% tolerance</td>\n</tr>\n<tr>\n<td>Coordinated Attack</td>\n<td>10 instances</td>\n<td>Synchronized request waves</td>\n<td>Consistent denial pattern</td>\n<td>1% tolerance</td>\n</tr>\n<tr>\n<td>Mixed Load Patterns</td>\n<td>8 instances</td>\n<td>Random request timing</td>\n<td>Fair resource allocation</td>\n<td>Statistical validation</td>\n</tr>\n</tbody></table>\n<p><strong>Redis Atomic Operation Testing</strong></p>\n<p>The correctness of distributed rate limiting fundamentally depends on atomic Redis operations implemented through Lua scripts. Integration testing must verify that these atomic operations maintain consistency under concurrent load from multiple instances.</p>\n<p>Testing atomic operations requires careful coordination of multiple Redis clients executing operations simultaneously and validating that the final state matches expectations regardless of execution order or timing.</p>\n<table>\n<thead>\n<tr>\n<th>Lua Script</th>\n<th>Concurrent Operations</th>\n<th>State Validation</th>\n<th>Atomicity Check</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket Check-and-Update</td>\n<td>50 simultaneous calls</td>\n<td>Token count accuracy</td>\n<td>No double-counting</td>\n</tr>\n<tr>\n<td>Sliding Window Increment</td>\n<td>100 concurrent increments</td>\n<td>Counter consistency</td>\n<td>No lost updates</td>\n</tr>\n<tr>\n<td>Multi-Tier Evaluation</td>\n<td>25 instances checking</td>\n<td>Tier precedence maintained</td>\n<td>Correct short-circuiting</td>\n</tr>\n<tr>\n<td>Hot Key Detection</td>\n<td>High-frequency access</td>\n<td>Access count accuracy</td>\n<td>No race conditions</td>\n</tr>\n</tbody></table>\n<p><strong>Network Partition Simulation</strong></p>\n<p>Distributed systems must handle network partitions gracefully, falling back to local rate limiting when Redis becomes unavailable. Integration testing must simulate various network failure scenarios and verify that fallback mechanisms activate correctly and recovery proceeds smoothly.</p>\n<p>Network partition testing requires controlled network manipulation to simulate Redis connectivity issues while maintaining test orchestration communication. The testing framework must distinguish between intentional network partitions and actual infrastructure failures.</p>\n<table>\n<thead>\n<tr>\n<th>Partition Scenario</th>\n<th>Duration</th>\n<th>Expected Behavior</th>\n<th>Recovery Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Complete Redis Loss</td>\n<td>30 seconds</td>\n<td>Local fallback activation</td>\n<td>Seamless transition back</td>\n</tr>\n<tr>\n<td>Intermittent Connectivity</td>\n<td>5-second cycles</td>\n<td>Circuit breaker activation</td>\n<td>Adaptive retry behavior</td>\n</tr>\n<tr>\n<td>Partial Node Failure</td>\n<td>60 seconds</td>\n<td>Hash ring rebalancing</td>\n<td>Minimal key redistribution</td>\n</tr>\n<tr>\n<td>Split-Brain Scenario</td>\n<td>Variable timing</td>\n<td>Consistent state maintenance</td>\n<td>No conflicting decisions</td>\n</tr>\n</tbody></table>\n<p><strong>Load Distribution and Sharding Testing</strong></p>\n<p>When using consistent hashing to distribute rate limiting state across multiple Redis nodes, integration testing must verify that load distributes evenly and that node additions or removals cause minimal disruption. This testing requires careful monitoring of key distribution patterns and performance characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Sharding Test</th>\n<th>Node Count</th>\n<th>Key Distribution</th>\n<th>Rebalancing Trigger</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Initial Distribution</td>\n<td>5 nodes</td>\n<td>1000 unique keys</td>\n<td>N/A</td>\n<td>±10% load variance</td>\n</tr>\n<tr>\n<td>Node Addition</td>\n<td>5→6 nodes</td>\n<td>Existing keys</td>\n<td>New node insertion</td>\n<td>&lt;20% key migration</td>\n</tr>\n<tr>\n<td>Node Removal</td>\n<td>6→5 nodes</td>\n<td>Existing keys</td>\n<td>Node failure</td>\n<td>&lt;25% key migration</td>\n</tr>\n<tr>\n<td>Hot Key Handling</td>\n<td>5 nodes</td>\n<td>80/20 access pattern</td>\n<td>Automatic detection</td>\n<td>Even response times</td>\n</tr>\n</tbody></table>\n<h3 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h3>\n<p>Each milestone in the distributed rate limiter development process requires specific verification checkpoints that confirm the implementation meets acceptance criteria before proceeding to more complex functionality. These checkpoints serve as quality gates preventing the accumulation of technical debt and ensuring solid foundations for subsequent development.</p>\n<p><strong>Milestone 1: Rate Limiting Algorithms Verification</strong></p>\n<p>The first milestone checkpoint focuses on verifying that individual rate limiting algorithms function correctly in isolation, handling edge cases appropriately, and meeting performance requirements under load.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm Component</th>\n<th>Verification Test</th>\n<th>Expected Result</th>\n<th>Performance Benchmark</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket Implementation</td>\n<td>Burst handling test: 1000 requests instantly</td>\n<td>≤ capacity requests allowed</td>\n<td>&lt; 1μs per operation</td>\n</tr>\n<tr>\n<td>Token Bucket Implementation</td>\n<td>Refill rate test: sustained load over 60 seconds</td>\n<td>Average rate = refill_rate</td>\n<td>Consistent latency</td>\n</tr>\n<tr>\n<td>Sliding Window Counter</td>\n<td>Boundary transition test: requests spanning window</td>\n<td>Smooth rate enforcement</td>\n<td>&lt; 500ns per operation</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n<td>Memory usage test: 1M requests over 1 hour</td>\n<td>Bounded memory growth</td>\n<td>Linear memory scaling</td>\n</tr>\n<tr>\n<td>Algorithm Comparison</td>\n<td>Accuracy benchmark: compare with exact counting</td>\n<td>Token bucket: exact, Counter: ~95%</td>\n<td>Measure overhead</td>\n</tr>\n</tbody></table>\n<p>The verification process requires implementing a comprehensive test suite that exercises each algorithm through multiple scenarios, measuring both correctness and performance characteristics. The test suite must generate synthetic load patterns that simulate real-world usage while maintaining precise control over timing and request distribution.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example checkpoint verification commands</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./pkg/algorithms/token_bucket/</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#79B8FF\"> -count=100</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./pkg/algorithms/sliding_window/</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -bench=.</span><span style=\"color:#79B8FF\"> -benchmem</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./pkg/algorithms/comparison/</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -timeout=300s</span></span></code></pre></div>\n\n<p><strong>Milestone 2: Multi-Tier Rate Limiting Verification</strong></p>\n<p>The second milestone verification ensures that hierarchical rate limiting works correctly with proper tier precedence, short-circuit evaluation, and accurate rule matching across user, IP, API, and global dimensions.</p>\n<table>\n<thead>\n<tr>\n<th>Multi-Tier Component</th>\n<th>Verification Test</th>\n<th>Expected Behavior</th>\n<th>Edge Case Coverage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tier Precedence</td>\n<td>User limit &lt; IP limit conflict</td>\n<td>User limit enforced</td>\n<td>Most restrictive wins</td>\n</tr>\n<tr>\n<td>Short-Circuit Evaluation</td>\n<td>Global limit reached first</td>\n<td>Skip remaining tiers</td>\n<td>Efficiency optimization</td>\n</tr>\n<tr>\n<td>Rule Pattern Matching</td>\n<td>Complex regex patterns</td>\n<td>Correct rule selection</td>\n<td>Pattern performance</td>\n</tr>\n<tr>\n<td>Quota Management</td>\n<td>Cross-tier usage tracking</td>\n<td>Accurate attribution</td>\n<td>No double-counting</td>\n</tr>\n</tbody></table>\n<p>The multi-tier verification process requires creating test scenarios with conflicting rate limits across different tiers and verifying that the system enforces the most restrictive applicable limit while maintaining performance efficiency.</p>\n<p><strong>Milestone 3: Redis Backend Integration Verification</strong></p>\n<p>The third milestone checkpoint validates that Redis integration maintains atomic operations, handles connection failures gracefully, and provides accurate distributed state synchronization across multiple application instances.</p>\n<table>\n<thead>\n<tr>\n<th>Redis Component</th>\n<th>Verification Test</th>\n<th>Expected Outcome</th>\n<th>Failure Scenario</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lua Script Atomicity</td>\n<td>100 concurrent check-and-update</td>\n<td>No race conditions</td>\n<td>Network latency</td>\n</tr>\n<tr>\n<td>Connection Pooling</td>\n<td>1000 simultaneous operations</td>\n<td>Connection reuse</td>\n<td>Pool exhaustion</td>\n</tr>\n<tr>\n<td>Graceful Degradation</td>\n<td>Redis unavailable</td>\n<td>Local fallback</td>\n<td>Service continuity</td>\n</tr>\n<tr>\n<td>State Synchronization</td>\n<td>Multi-instance coordination</td>\n<td>Consistent limits</td>\n<td>Clock skew tolerance</td>\n</tr>\n</tbody></table>\n<p>Redis integration verification requires deploying multiple rate limiter instances against a shared Redis backend and orchestrating load tests that verify distributed coordination accuracy under various failure conditions.</p>\n<p><strong>Milestone 4: Consistent Hashing and Sharding Verification</strong></p>\n<p>The fourth milestone verification confirms that consistent hashing distributes load evenly, minimizes key redistribution during topology changes, and handles hot key scenarios appropriately.</p>\n<table>\n<thead>\n<tr>\n<th>Sharding Component</th>\n<th>Verification Test</th>\n<th>Success Criteria</th>\n<th>Scalability Metric</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hash Ring Distribution</td>\n<td>10,000 keys across 5 nodes</td>\n<td>±15% load variance</td>\n<td>O(log n) lookup time</td>\n</tr>\n<tr>\n<td>Node Addition Impact</td>\n<td>Add 6th node to running system</td>\n<td>&lt;20% key migration</td>\n<td>Minimal disruption</td>\n</tr>\n<tr>\n<td>Hot Key Detection</td>\n<td>80/20 access pattern</td>\n<td>Automatic identification</td>\n<td>Response time consistency</td>\n</tr>\n<tr>\n<td>Failover Behavior</td>\n<td>Primary node failure</td>\n<td>Seamless redirection</td>\n<td>&lt;100ms recovery time</td>\n</tr>\n</tbody></table>\n<p>Sharding verification requires comprehensive load testing with realistic key distribution patterns and careful measurement of performance characteristics during topology changes.</p>\n<p><strong>Milestone 5: API and Dashboard Verification</strong></p>\n<p>The final milestone verification ensures that the management API functions correctly, the dashboard displays accurate real-time data, and rate limit headers conform to RFC standards.</p>\n<table>\n<thead>\n<tr>\n<th>API Component</th>\n<th>Verification Test</th>\n<th>Expected Result</th>\n<th>Integration Check</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CRUD Operations</td>\n<td>Create, read, update, delete rules</td>\n<td>Immediate effect</td>\n<td>Configuration propagation</td>\n</tr>\n<tr>\n<td>Rate Limit Headers</td>\n<td>X-RateLimit-* header presence</td>\n<td>RFC compliance</td>\n<td>Client compatibility</td>\n</tr>\n<tr>\n<td>Real-time Dashboard</td>\n<td>Live usage metrics</td>\n<td>&lt;5 second latency</td>\n<td>WebSocket stability</td>\n</tr>\n<tr>\n<td>Dynamic Configuration</td>\n<td>Rule updates without restart</td>\n<td>Zero downtime</td>\n<td>Gradual rollout</td>\n</tr>\n</tbody></table>\n<h3 id=\"chaos-and-failure-testing\">Chaos and Failure Testing</h3>\n<p>The robustness of a distributed rate limiter emerges most clearly under adverse conditions where components fail, networks partition, and system resources become constrained. Chaos engineering principles guide the systematic introduction of failures to validate recovery mechanisms and identify weaknesses in distributed coordination.</p>\n<p><strong>Redis Failure Scenario Testing</strong></p>\n<p>Redis failures represent the most critical failure mode for distributed rate limiting, as they eliminate the shared state coordination mechanism. Comprehensive failure testing must cover various Redis failure patterns and validate fallback behavior.</p>\n<p>The chaos testing framework must simulate realistic Redis failure scenarios including complete node failures, memory pressure leading to evictions, network partitions isolating Redis clusters, and performance degradation under load. Each failure scenario requires careful validation of both immediate response and recovery behavior.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Simulation Method</th>\n<th>Expected Behavior</th>\n<th>Recovery Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Complete Redis Outage</td>\n<td>Network blackhole</td>\n<td>Local fallback activation</td>\n<td>Automatic reconnection</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>Maxmemory + allkeys-lru</td>\n<td>Key eviction tolerance</td>\n<td>Performance degradation</td>\n</tr>\n<tr>\n<td>Network Partition</td>\n<td>Selective packet dropping</td>\n<td>Circuit breaker activation</td>\n<td>Split-brain prevention</td>\n</tr>\n<tr>\n<td>Performance Degradation</td>\n<td>Artificial latency injection</td>\n<td>Timeout handling</td>\n<td>Graceful degradation</td>\n</tr>\n<tr>\n<td>Partial Node Failure</td>\n<td>Single node termination</td>\n<td>Hash ring rebalancing</td>\n<td>Key redistribution</td>\n</tr>\n</tbody></table>\n<p><strong>Network Partition and Split-Brain Prevention</strong></p>\n<p>Network partitions represent particularly challenging failure scenarios where different parts of the distributed system lose connectivity with each other while maintaining connectivity with clients. The rate limiting system must handle these partitions without creating inconsistent states or allowing unlimited request flow.</p>\n<p>Network partition testing requires sophisticated network manipulation tools that can selectively block communication between specific components while maintaining test orchestration channels. The testing framework must validate that partitioned components make consistent decisions and that recovery procedures restore global consistency.</p>\n<table>\n<thead>\n<tr>\n<th>Partition Scenario</th>\n<th>Components Affected</th>\n<th>Decision Strategy</th>\n<th>Consistency Check</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Cluster Split</td>\n<td>Half of Redis nodes</td>\n<td>Majority quorum</td>\n<td>No split decisions</td>\n</tr>\n<tr>\n<td>Application Island</td>\n<td>Some app instances isolated</td>\n<td>Conservative limiting</td>\n<td>Err on restrictive side</td>\n</tr>\n<tr>\n<td>Cross-Region Partition</td>\n<td>Geographic separation</td>\n<td>Regional degradation</td>\n<td>Eventual consistency</td>\n</tr>\n<tr>\n<td>Cascading Failure</td>\n<td>Multiple component failures</td>\n<td>Progressive degradation</td>\n<td>Bounded blast radius</td>\n</tr>\n</tbody></table>\n<p><strong>High Load and Resource Exhaustion Testing</strong></p>\n<p>Distributed rate limiters must maintain accuracy and performance under extreme load conditions that stress both computational resources and network bandwidth. High load testing reveals performance bottlenecks, memory leaks, and coordination inefficiencies that only manifest under sustained pressure.</p>\n<p>Resource exhaustion testing requires careful load generation that can saturate different system resources independently - CPU usage through computational overhead, memory usage through state accumulation, network bandwidth through request volume, and disk I/O through logging and persistence operations.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Constraint</th>\n<th>Load Pattern</th>\n<th>System Response</th>\n<th>Performance Metric</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU Saturation</td>\n<td>High-frequency requests</td>\n<td>Graceful degradation</td>\n<td>Latency percentiles</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>Many unique keys</td>\n<td>Memory management</td>\n<td>GC frequency</td>\n</tr>\n<tr>\n<td>Network Bandwidth</td>\n<td>Large request payloads</td>\n<td>Request prioritization</td>\n<td>Throughput maintenance</td>\n</tr>\n<tr>\n<td>Redis Connection Pool</td>\n<td>Concurrent operations</td>\n<td>Connection queuing</td>\n<td>Pool utilization</td>\n</tr>\n</tbody></table>\n<p><strong>Clock Skew and Time Synchronization Testing</strong></p>\n<p>Distributed rate limiting algorithms depend critically on synchronized time across all participating nodes. Clock skew between application instances and Redis nodes can lead to inaccurate rate limit calculations, particularly for time-window-based algorithms.</p>\n<p>Clock skew testing requires artificially manipulating system clocks on different nodes and measuring the impact on rate limiting accuracy. The testing framework must validate that clock skew detection mechanisms function correctly and that synchronization procedures restore accuracy.</p>\n<table>\n<thead>\n<tr>\n<th>Clock Scenario</th>\n<th>Skew Magnitude</th>\n<th>Algorithm Impact</th>\n<th>Mitigation Effectiveness</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gradual Drift</td>\n<td>±1 second over 1 hour</td>\n<td>Sliding window accuracy</td>\n<td>NTP synchronization</td>\n</tr>\n<tr>\n<td>Sudden Jump</td>\n<td>+30 seconds instantly</td>\n<td>Token bucket disruption</td>\n<td>Skew detection</td>\n</tr>\n<tr>\n<td>Backwards Clock</td>\n<td>-10 seconds</td>\n<td>Timestamp confusion</td>\n<td>Monotonic time usage</td>\n</tr>\n<tr>\n<td>Different Zones</td>\n<td>Cross-timezone deployment</td>\n<td>UTC coordination</td>\n<td>Timezone normalization</td>\n</tr>\n</tbody></table>\n<p><strong>Cascading Failure Prevention Testing</strong></p>\n<p>The most dangerous failure scenarios in distributed systems involve cascading failures where the failure of one component triggers failures in dependent components, potentially leading to total system collapse. Rate limiting systems must include circuit breakers and bulkheads to prevent failure propagation.</p>\n<p>Cascading failure testing requires orchestrated failure injection across multiple system layers while monitoring the blast radius and recovery characteristics. The testing framework must validate that circuit breakers activate appropriately and that fallback mechanisms prevent total system failure.</p>\n<table>\n<thead>\n<tr>\n<th>Cascade Trigger</th>\n<th>Initial Failure</th>\n<th>Propagation Path</th>\n<th>Circuit Breaker Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Overload</td>\n<td>High latency</td>\n<td>Client timeout cascade</td>\n<td>Redis circuit opening</td>\n</tr>\n<tr>\n<td>App Instance Death</td>\n<td>Process crash</td>\n<td>Load redistribution</td>\n<td>Health check removal</td>\n</tr>\n<tr>\n<td>Network Congestion</td>\n<td>Packet loss</td>\n<td>Retry amplification</td>\n<td>Backpressure activation</td>\n</tr>\n<tr>\n<td>Dashboard Query Load</td>\n<td>Metrics overload</td>\n<td>Rate limiter impact</td>\n<td>Self-rate-limiting</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Testing Insight</strong>: Chaos testing must be continuous rather than periodic. Production systems face constant low-level failures and stress conditions that only become apparent through sustained chaos injection. A rate limiting system that passes a one-hour chaos test might fail catastrophically after running under minor stress for weeks.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides concrete tools and templates for building a comprehensive testing strategy that validates distributed rate limiting functionality across all development milestones.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Testing Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing</td>\n<td>Go testing package + testify</td>\n<td>Ginkgo BDD framework</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Custom goroutines</td>\n<td>Apache JMeter / k6</td>\n</tr>\n<tr>\n<td>Redis Simulation</td>\n<td>Embedded Redis (miniredis)</td>\n<td>Docker Compose Redis cluster</td>\n</tr>\n<tr>\n<td>Network Partition</td>\n<td>Manual connection dropping</td>\n<td>Chaos Mesh / Litmus</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Simple counters</td>\n<td>Prometheus + Grafana</td>\n</tr>\n<tr>\n<td>Test Orchestration</td>\n<td>Shell scripts</td>\n<td>Kubernetes Jobs</td>\n</tr>\n<tr>\n<td>Clock Manipulation</td>\n<td>time.Now() mocking</td>\n<td>libfaketime system-wide</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    chaos-test/main.go              ← Chaos engineering test runner\n    integration-test/main.go        ← Multi-instance integration tests\n    benchmark/main.go               ← Performance benchmark suite\n  \n  test/\n    unit/                           ← Algorithm unit tests\n      token_bucket_test.go\n      sliding_window_test.go\n      algorithms_benchmark_test.go\n    \n    integration/                    ← Distributed coordination tests\n      multi_instance_test.go\n      redis_atomic_test.go\n      failover_test.go\n    \n    chaos/                          ← Failure injection tests\n      redis_failure_test.go\n      network_partition_test.go\n      load_stress_test.go\n    \n    fixtures/                       ← Test data and configurations\n      rate_limit_rules.yaml\n      redis_cluster_config.yaml\n    \n    helpers/                        ← Testing utilities\n      redis_helper.go              ← Redis test setup/teardown\n      load_generator.go            ← Traffic generation utilities\n      time_helper.go               ← Clock manipulation helpers\n      metrics_collector.go         ← Test metrics collection\n  \n  pkg/\n    ratelimit/\n      *_test.go                    ← Unit tests alongside implementation\n  \n  scripts/\n    run_milestone_checks.sh        ← Automated milestone verification\n    setup_test_environment.sh     ← Test infrastructure setup\n    chaos_test_suite.sh           ← Comprehensive chaos testing</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code (COMPLETE, ready to use):</strong></p>\n<p><strong>Redis Test Helper - Complete Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/helpers/redis_helper.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> helpers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/alicebob/miniredis/v2</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/redis/go-redis/v9</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/testcontainers/testcontainers-go</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/testcontainers/testcontainers-go/wait</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisTestHelper manages Redis instances for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisTestHelper</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For unit tests - embedded Redis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    miniRedis </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">miniredis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Miniredis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For integration tests - real Redis containers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    containers </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">testcontainers</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Container</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clients    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRedisTestHelper creates a new Redis test helper</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisTestHelper</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisTestHelper</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RedisTestHelper</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        containers: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">testcontainers</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Container</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        clients:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartEmbeddedRedis starts a lightweight Redis for unit tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisTestHelper</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartEmbeddedRedis</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mini, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> miniredis.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to start miniredis: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.miniRedis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> mini</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#B392F0\">NewClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Options</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr: mini.</span><span style=\"color:#B392F0\">Addr</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DB:   </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanup </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mini.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> client, cleanup, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartRedisCluster starts a Redis cluster for integration tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisTestHelper</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartRedisCluster</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nodeCount</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> clients []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> cleanupFuncs []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> nodeCount; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        container, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> testcontainers.</span><span style=\"color:#B392F0\">GenericContainer</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#B392F0\">testcontainers</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">GenericContainerRequest</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ContainerRequest: </span><span style=\"color:#B392F0\">testcontainers</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ContainerRequest</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                Image:        </span><span style=\"color:#9ECBFF\">\"redis:7-alpine\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ExposedPorts: []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"6379/tcp\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                WaitingFor:   wait.</span><span style=\"color:#B392F0\">ForLog</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Ready to accept connections\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                Cmd:          []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"redis-server\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--appendonly\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"yes\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Started: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() { h.</span><span style=\"color:#B392F0\">cleanup</span><span style=\"color:#E1E4E8\">(cleanupFuncs) }, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        endpoint, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> container.</span><span style=\"color:#B392F0\">Endpoint</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() { h.</span><span style=\"color:#B392F0\">cleanup</span><span style=\"color:#E1E4E8\">(cleanupFuncs) }, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#B392F0\">NewClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Options</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Addr: endpoint,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        clients </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(clients, client)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        h.containers[fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, i)] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> container</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        h.clients[fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, i)] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cleanupFuncs </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(cleanupFuncs, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            client.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            container.</span><span style=\"color:#B392F0\">Terminate</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> clients, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() { h.</span><span style=\"color:#B392F0\">cleanup</span><span style=\"color:#E1E4E8\">(cleanupFuncs) }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SimulateNetworkPartition blocks network access to specific Redis nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisTestHelper</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SimulateNetworkPartition</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nodeIDs</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, nodeID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> nodeIDs {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> container, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> h.containers[nodeID]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Simulate network partition by pausing the container</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> container.</span><span style=\"color:#B392F0\">Terminate</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to partition node </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, nodeID, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisTestHelper</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">cleanup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">cleanupFuncs</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, cleanup </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> cleanupFuncs {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        cleanup</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Load Generator - Complete Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/helpers/load_generator.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> helpers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync/atomic</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/pkg/ratelimit</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadPattern defines different traffic generation patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LoadPattern</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ConstantLoad</span><span style=\"color:#B392F0\"> LoadPattern</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BurstLoad</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GradualRamp</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SpikeLoad</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RandomLoad</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadGeneratorConfig configures load generation parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LoadGeneratorConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Pattern          </span><span style=\"color:#B392F0\">LoadPattern</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestsPerSecond </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration         </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClientCount      </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    KeyPattern       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BurstSize        </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BurstInterval    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadGenerator generates synthetic traffic patterns for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LoadGenerator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config    </span><span style=\"color:#B392F0\">LoadGeneratorConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limiter   </span><span style=\"color:#B392F0\">ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DistributedLimiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    results   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadTestResults</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx       </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancelFn  </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadTestResults captures comprehensive load test metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LoadTestResults</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalRequests    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AllowedRequests  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DeniedRequests   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorRequests    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AverageLatency   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    P95Latency       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    P99Latency       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartTime        </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EndTime          </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LatencyHistogram []</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLoadGenerator creates a load generator with specified configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLoadGenerator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> LoadGeneratorConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limiter</span><span style=\"color:#B392F0\"> ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), config.Duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:  config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        limiter: limiter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">LoadTestResults</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            StartTime:        time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            LatencyHistogram: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, config.RequestsPerSecond</span><span style=\"color:#F97583\">*int</span><span style=\"color:#E1E4E8\">(config.Duration.</span><span style=\"color:#B392F0\">Seconds</span><span style=\"color:#E1E4E8\">())),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctx:      ctx,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cancelFn: cancel,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins load generation according to configured pattern</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadTestResults</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> lg.</span><span style=\"color:#B392F0\">cancelFn</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.results.StartTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> lg.config.Pattern {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> ConstantLoad:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lg.</span><span style=\"color:#B392F0\">generateConstantLoad</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> BurstLoad:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lg.</span><span style=\"color:#B392F0\">generateBurstLoad</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> GradualRamp:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lg.</span><span style=\"color:#B392F0\">generateGradualRamp</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> SpikeLoad:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lg.</span><span style=\"color:#B392F0\">generateSpikeLoad</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> RandomLoad:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lg.</span><span style=\"color:#B392F0\">generateRandomLoad</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.results.EndTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.</span><span style=\"color:#B392F0\">calculateStatistics</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> lg.results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// generateConstantLoad produces steady request rate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">generateConstantLoad</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    interval </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.Second </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(lg.config.RequestsPerSecond)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(interval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> wg </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">lg.ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            wg.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> lg.config.ClientCount; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    defer</span><span style=\"color:#E1E4E8\"> wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    lg.</span><span style=\"color:#B392F0\">executeRequest</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// generateBurstLoad produces periodic bursts of requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">generateBurstLoad</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    burstTicker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(lg.config.BurstInterval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> burstTicker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">lg.ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">burstTicker.C:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            var</span><span style=\"color:#E1E4E8\"> wg </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> lg.config.BurstSize; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    defer</span><span style=\"color:#E1E4E8\"> wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    lg.</span><span style=\"color:#B392F0\">executeRequest</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            wg.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// executeRequest performs a single rate limit check with timing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">executeRequest</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> lg.</span><span style=\"color:#B392F0\">generateKey</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        UserID:      fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"user_</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, rand.</span><span style=\"color:#B392F0\">Intn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        IPAddress:   fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"192.168.1.</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, rand.</span><span style=\"color:#B392F0\">Intn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">255</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        APIEndpoint: key,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Tokens:      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> lg.limiter.</span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(lg.ctx, req)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    latency </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.</span><span style=\"color:#B392F0\">recordResult</span><span style=\"color:#E1E4E8\">(result, err, latency)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// recordResult safely updates load test results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recordResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">latency</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.results.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> lg.results.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">lg.results.TotalRequests, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.results.LatencyHistogram </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(lg.results.LatencyHistogram, latency)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">lg.results.ErrorRequests, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> result.Allowed {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">lg.results.AllowedRequests, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">lg.results.DeniedRequests, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// generateKey creates test keys according to pattern</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">generateKey</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> lg.config.KeyPattern </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"api_endpoint_</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, rand.</span><span style=\"color:#B392F0\">Intn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(lg.config.KeyPattern, rand.</span><span style=\"color:#B392F0\">Intn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// calculateStatistics computes latency percentiles and averages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">calculateStatistics</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(lg.results.LatencyHistogram) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Sort latencies for percentile calculation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    latencies </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(lg.results.LatencyHistogram))</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(latencies, lg.results.LatencyHistogram)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sort.</span><span style=\"color:#B392F0\">Slice</span><span style=\"color:#E1E4E8\">(latencies, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">j</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> latencies[i] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> latencies[j]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate average</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> total </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, latency </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> latencies {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> latency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.results.AverageLatency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(latencies))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate percentiles</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p95Index </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.95</span><span style=\"color:#F97583\"> *</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(latencies)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p99Index </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.99</span><span style=\"color:#F97583\"> *</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(latencies)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.results.P95Latency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> latencies[p95Index]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lg.results.P99Latency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> latencies[p99Index]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Time Helper for Clock Manipulation - Complete Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/helpers/time_helper.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> helpers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MockTimeProvider implements controllable time for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockTimeProvider</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    currentTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clockSkew   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMockTimeProvider creates a controllable time provider</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMockTimeProvider</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">startTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeProvider</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MockTimeProvider</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        currentTime: startTime,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Now returns the current mock time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeProvider</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> m.currentTime.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(m.clockSkew)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AdvanceTime moves the mock clock forward</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeProvider</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AdvanceTime</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.currentTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> m.currentTime.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetClockSkew simulates time difference between nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeProvider</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetClockSkew</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">skew</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.clockSkew </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> skew</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetTime directly sets the current time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeProvider</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetTime</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.currentTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> t</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code (signature + TODOs only):</strong></p>\n<p><strong>Algorithm Unit Test Template:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/unit/token_bucket_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> unit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/stretchr/testify/assert</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/pkg/algorithms</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/test/helpers</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestTokenBucketBurstHandling validates burst capacity behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestTokenBucketBurstHandling</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create TokenBucket with capacity=10, refill_rate=5/sec</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute 10 requests instantly - all should be allowed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute 11th request - should be denied</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Wait 1 second for refill</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Execute 5 more requests - should be allowed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify remaining token count matches expected value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> algorithms</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TokenBucketConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Capacity:   </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        RefillRate: </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Window:     time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestTokenBucketRefillAccuracy validates precise refill timing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestTokenBucketRefillAccuracy</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create TokenBucket with capacity=100, refill_rate=10/sec</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Drain bucket completely (100 requests)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Advance time by 0.5 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify exactly 5 tokens available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Advance time by another 0.5 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify exactly 10 tokens available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Test fractional refills (0.1 second = 1 token)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeProvider </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> helpers.</span><span style=\"color:#B392F0\">NewMockTimeProvider</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestSlidingWindowBoundaryTransition validates accurate window transitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestSlidingWindowBoundaryTransition</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create SlidingWindowCounter with limit=100, window=1min, buckets=60</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Fill first 30 seconds with 50 requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Fill next 30 seconds with 50 requests  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Advance to 45 seconds - verify current count = 75 (weighted)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Advance to 60 seconds - verify count = 50 (second half only)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test precision at exact second boundaries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BenchmarkAlgorithmComparison compares performance characteristics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> BenchmarkAlgorithmComparison</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">b</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">B</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Set up TokenBucket, SlidingWindowCounter, SlidingWindowLog</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Benchmark each algorithm with same request pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Measure operations per second for each</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Measure memory allocation per operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Record results for comparison table</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Integration Test Template:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/integration/multi_instance_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> integration</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/pkg/ratelimit</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/test/helpers</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestDistributedCoordination validates multi-instance rate limiting accuracy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestDistributedCoordination</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start Redis cluster with 3 nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create 5 DistributedLimiter instances connected to same Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Configure rate limit: 100 requests/minute per API endpoint  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Generate 500 requests across all instances simultaneously</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify total allowed requests ≤ 105 (5% tolerance for race conditions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Wait for next window and verify reset behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Test with different request timing patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redisHelper </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> helpers.</span><span style=\"color:#B392F0\">NewRedisTestHelper</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestAtomicOperations validates Redis Lua script atomicity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestAtomicOperations</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Set up Redis with custom Lua scripts loaded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Launch 100 goroutines executing check-and-update simultaneously  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Each goroutine attempts to consume 1 token from bucket with capacity=50</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify exactly 50 operations succeed, 50 fail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify final bucket state is consistent (0 tokens remaining)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test with different algorithms and concurrency levels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestGracefulDegradation validates fallback behavior during Redis failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestGracefulDegradation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start rate limiter with Redis backend + local fallback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify normal operation with Redis available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Simulate Redis failure (network partition)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify automatic fallback to local limiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify degraded functionality still enforces limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Restore Redis and verify transition back to distributed mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Test various failure scenarios (partial failures, timeouts)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Chaos Test Template:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/chaos/redis_failure_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> chaos</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/test/helpers</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestRedisCompleteOutage validates behavior during total Redis failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestRedisCompleteOutage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Deploy 3 rate limiter instances with Redis cluster backend</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start load generation: 1000 req/sec across all instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: After 30 seconds, simulate complete Redis cluster failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify circuit breaker activates within 5 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify local fallback maintains some rate limiting (degraded)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: After 60 seconds, restore Redis cluster</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Verify recovery to normal operation within 30 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Measure accuracy degradation during outage period</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestNetworkPartitionSplitBrain validates split-brain prevention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestNetworkPartitionSplitBrain</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Set up 6-node Redis cluster + 4 rate limiter instances  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Partition network: isolate 2 Redis nodes + 2 app instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Generate traffic to both network partitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify minority partition fails safely (conservative limiting)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify majority partition continues normal operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Heal network partition after 120 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Verify convergence to consistent state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Measure request accuracy during partition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestClockSkewImpact validates time synchronization requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestClockSkewImpact</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Deploy rate limiters with controllable time providers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Introduce gradual clock skew: +1 second per minute drift</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Measure sliding window accuracy degradation over time  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test sudden clock jumps: +30 seconds, -10 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify skew detection mechanisms activate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test algorithm-specific impacts (token bucket vs sliding window)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Validate time sync recovery procedures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation goes here - learner fills this in</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>testing.TB</code> interface for helpers that work in both tests and benchmarks</li>\n<li>Leverage <code>testify/suite</code> for complex test setup/teardown with shared state</li>\n<li>Use <code>context.WithTimeout</code> extensively to prevent hanging integration tests  </li>\n<li>Implement test helpers that use <code>t.Cleanup()</code> for automatic resource cleanup</li>\n<li>Use <code>sync/atomic</code> for thread-safe counter operations in load tests</li>\n<li>Leverage <code>testing.Short()</code> to skip expensive tests during development</li>\n<li>Use <code>go test -race</code> consistently to detect race conditions</li>\n<li>Implement test fixtures with <code>embed</code> for configuration files</li>\n<li>Use <code>httptest.Server</code> for testing HTTP API endpoints</li>\n<li>Leverage <code>testcontainers-go</code> for realistic Redis cluster testing</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing each milestone&#39;s testing strategy:</p>\n<p><strong>Milestone 1 Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Verify algorithm implementations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./pkg/algorithms/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#79B8FF\"> -count=5</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./pkg/algorithms/...</span><span style=\"color:#79B8FF\"> -bench=.</span><span style=\"color:#79B8FF\"> -benchmem</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./test/unit/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -timeout=60s</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: All tests pass, benchmarks show expected performance characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Signs of problems: Race conditions, memory leaks, inconsistent results</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Verify multi-tier coordination</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./pkg/multitier/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -race</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./test/integration/multi_tier_test.go</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Proper tier precedence, short-circuit evaluation working</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Signs of problems: Wrong limits enforced, performance degradation</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Verify Redis integration</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./test/integration/redis_test.go</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">docker-compose</span><span style=\"color:#79B8FF\"> -f</span><span style=\"color:#9ECBFF\"> test/redis-cluster.yml</span><span style=\"color:#9ECBFF\"> up</span><span style=\"color:#79B8FF\"> -d</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./test/integration/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -timeout=300s</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Atomic operations, graceful degradation, connection pooling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Signs of problems: Race conditions, connection leaks, fallback failures</span></span></code></pre></div>\n\n<p><strong>G. Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Method</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tests pass individually but fail in parallel</td>\n<td>Race conditions in shared state</td>\n<td>Run with <code>-race</code> flag</td>\n<td>Add proper synchronization</td>\n</tr>\n<tr>\n<td>Integration tests hang indefinitely</td>\n<td>Missing context timeouts</td>\n<td>Check for blocking operations</td>\n<td>Add context.WithTimeout</td>\n</tr>\n<tr>\n<td>High memory usage during load tests</td>\n<td>Memory leaks in test harness</td>\n<td>Use <code>go test -memprofile</code></td>\n<td>Fix cleanup in test helpers</td>\n</tr>\n<tr>\n<td>Inconsistent rate limiting accuracy</td>\n<td>Clock skew or timing issues</td>\n<td>Log timestamps and compare</td>\n<td>Implement time synchronization</td>\n</tr>\n<tr>\n<td>Redis connection errors in tests</td>\n<td>Connection pool exhaustion</td>\n<td>Monitor Redis connection count</td>\n<td>Increase pool size or add cleanup</td>\n</tr>\n<tr>\n<td>Chaos tests produce false positives</td>\n<td>Insufficient failure simulation time</td>\n<td>Extend chaos duration</td>\n<td>Allow more time for failure detection</td>\n</tr>\n</tbody></table>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - debugging techniques span rate limiting algorithms (Milestone 1), multi-tier evaluation (Milestone 2), Redis backend integration (Milestone 3), consistent hashing and sharding (Milestone 4), and API design (Milestone 5).</p>\n</blockquote>\n<h3 id=\"mental-model-the-detective-investigation-framework\">Mental Model: The Detective Investigation Framework</h3>\n<p>Think of debugging a distributed rate limiter like conducting a complex criminal investigation across multiple crime scenes. Just as a detective must gather evidence from multiple locations, interview witnesses with different perspectives, and piece together a timeline of events, debugging distributed systems requires collecting logs from multiple nodes, understanding different component viewpoints, and reconstructing the sequence of operations that led to a problem.</p>\n<p>The detective starts with observable symptoms (the &quot;crime scene&quot;) - perhaps requests are being incorrectly allowed or denied, response times are slow, or counters seem inaccurate. Like fingerprints and DNA evidence, distributed systems leave traces in logs, metrics, and Redis state that tell the story of what happened. The challenge lies in correlating evidence across time zones (clock skew), dealing with unreliable witnesses (network partitions), and understanding that the timeline might be non-linear (asynchronous operations).</p>\n<p>A good detective follows a systematic approach: secure the scene (gather current state), collect evidence (logs and metrics), interview witnesses (check all nodes), analyze the timeline (sequence of operations), and test theories (reproduce the issue). Similarly, effective distributed system debugging requires structured approaches that account for the inherent complexity of multiple moving parts operating across network boundaries.</p>\n<h3 id=\"symptom-based-diagnosis-table\">Symptom-Based Diagnosis Table</h3>\n<p>The foundation of effective distributed rate limiting debugging lies in mapping observable symptoms to their likely root causes through systematic analysis. This diagnostic approach transforms the overwhelming complexity of distributed system failures into manageable investigation paths.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Observable Behavior</th>\n<th>Likely Root Causes</th>\n<th>Initial Diagnostic Steps</th>\n<th>Advanced Investigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Requests incorrectly allowed</strong></td>\n<td>Rate limit counters show usage below threshold but actual requests exceed configured limit</td>\n<td>Token bucket refill race condition, sliding window boundary condition, clock skew between nodes, Lua script logic error</td>\n<td>Check Redis TIME command vs local time, examine Lua script execution logs, verify algorithm parameters</td>\n<td>Enable request tracing with correlation IDs, compare counter values across Redis nodes, analyze time-series data for boundary transitions</td>\n</tr>\n<tr>\n<td><strong>Requests incorrectly denied</strong></td>\n<td>Rate limit counters exceed threshold but usage appears normal, legitimate traffic rejected</td>\n<td>Hot key concentration, Redis memory pressure, circuit breaker false positives, multi-tier evaluation precedence error</td>\n<td>Monitor Redis memory usage and eviction stats, check circuit breaker state, verify rule priority ordering</td>\n<td>Analyze key distribution across hash ring, examine tier evaluation logs, test rule pattern matching logic</td>\n</tr>\n<tr>\n<td><strong>Inconsistent behavior across instances</strong></td>\n<td>Same request allowed on one app instance but denied on another</td>\n<td>Local fallback active on some nodes, Redis connection issues, configuration propagation delay, stale rule cache</td>\n<td>Check Redis connectivity per node, verify configuration version across instances, examine local fallback activation logs</td>\n<td>Trace configuration update propagation, validate hash ring consistency, monitor network partitions</td>\n</tr>\n<tr>\n<td><strong>High latency in rate limit checks</strong></td>\n<td>Response times exceed acceptable thresholds consistently</td>\n<td>Redis cluster rebalancing, network latency to Redis, connection pool exhaustion, Lua script complexity</td>\n<td>Monitor Redis connection pool metrics, measure network round-trip time, profile Lua script execution duration</td>\n<td>Analyze request routing patterns, examine hot key impact on performance, trace connection lifecycle</td>\n</tr>\n<tr>\n<td><strong>Rate limit counters reset unexpectedly</strong></td>\n<td>Usage statistics drop to zero without administrative action</td>\n<td>Redis key expiration misconfiguration, manual counter reset, Redis failover with data loss, time provider synchronization issue</td>\n<td>Check Redis TTL settings, review administrative action logs, verify Redis persistence configuration</td>\n<td>Analyze Redis cluster topology changes, examine backup and recovery procedures, validate time synchronization</td>\n</tr>\n<tr>\n<td><strong>Memory usage growing unbounded</strong></td>\n<td>Redis memory consumption increases without corresponding traffic</td>\n<td>Sliding window log accumulation, connection leak, metrics collection retention, key namespace pollution</td>\n<td>Monitor Redis key count and memory per key, check connection pool statistics, analyze key patterns</td>\n<td>Profile memory allocation patterns, examine garbage collection behavior, trace key lifecycle management</td>\n</tr>\n<tr>\n<td><strong>Circuit breaker stuck open</strong></td>\n<td>All Redis requests rejected despite Redis being healthy</td>\n<td>Failure threshold too sensitive, health check misconfiguration, recovery timeout too long, cascading failure detection</td>\n<td>Verify Redis health via direct connection, check circuit breaker configuration parameters, review failure classification logic</td>\n<td>Analyze failure pattern leading to circuit opening, validate health check implementation, examine recovery criteria</td>\n</tr>\n<tr>\n<td><strong>Configuration changes not applied</strong></td>\n<td>Rule updates don&#39;t affect rate limiting behavior</td>\n<td>Redis pub/sub channel issues, configuration watcher stopped, rule validation failure, cache invalidation problem</td>\n<td>Check Redis pub/sub subscription status, verify configuration watcher process, examine rule validation logs</td>\n<td>Trace configuration update flow end-to-end, validate cache coherency mechanisms, analyze version control</td>\n</tr>\n<tr>\n<td><strong>Clock skew affecting time windows</strong></td>\n<td>Rate limiting behavior varies with server time differences</td>\n<td>NTP synchronization issues, Redis server time drift, timezone configuration mismatch, manual time adjustment</td>\n<td>Compare local time with Redis TIME command, check NTP status on all nodes, verify timezone settings</td>\n<td>Implement distributed time synchronization monitoring, analyze time drift patterns, validate time provider logic</td>\n</tr>\n<tr>\n<td><strong>Hot key performance degradation</strong></td>\n<td>Specific keys experience severe latency while others perform normally</td>\n<td>Uneven key distribution, single Redis instance overload, hash ring imbalance, algorithmic complexity</td>\n<td>Identify hot keys through Redis monitoring, analyze request distribution patterns, check hash ring virtual node allocation</td>\n<td>Implement hot key replication, analyze access pattern evolution, optimize key distribution strategy</td>\n</tr>\n</tbody></table>\n<h3 id=\"redis-specific-debugging-techniques\">Redis-Specific Debugging Techniques</h3>\n<p>Redis serves as the critical shared state store for distributed rate limiting, making Redis-specific debugging techniques essential for diagnosing and resolving issues. The challenge lies in Redis&#39;s single-threaded nature combined with distributed access patterns creating complex interactions between atomicity, performance, and consistency.</p>\n<p><strong>Redis CLI Diagnostic Commands</strong></p>\n<p>The Redis command-line interface provides powerful introspection capabilities for understanding rate limiting state and system health. These commands form the foundation of Redis debugging workflows.</p>\n<table>\n<thead>\n<tr>\n<th>Command Category</th>\n<th>Command</th>\n<th>Purpose</th>\n<th>Example Usage</th>\n<th>Interpretation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Time Synchronization</strong></td>\n<td><code>TIME</code></td>\n<td>Get Redis server timestamp</td>\n<td><code>redis-cli TIME</code></td>\n<td>Compare with local time to detect clock skew exceeding 100ms</td>\n</tr>\n<tr>\n<td><strong>Memory Analysis</strong></td>\n<td><code>MEMORY USAGE key</code></td>\n<td>Memory consumption per rate limit key</td>\n<td><code>MEMORY USAGE ratelimit:user:12345:api:/orders</code></td>\n<td>Identify memory-intensive keys causing Redis pressure</td>\n</tr>\n<tr>\n<td><strong>Key Inspection</strong></td>\n<td><code>TTL key</code></td>\n<td>Remaining time-to-live for rate limit counters</td>\n<td><code>TTL ratelimit:user:12345:token_bucket</code></td>\n<td>Verify expiration timing matches configured window duration</td>\n</tr>\n<tr>\n<td><strong>Script Debugging</strong></td>\n<td><code>EVAL script numkeys key [arg ...]</code></td>\n<td>Execute Lua script with debugging output</td>\n<td><code>EVAL &quot;return {KEYS[1], ARGV[1], redis.call(&#39;TIME&#39;)}&quot; 1 test_key 100</code></td>\n<td>Test Lua script logic with controlled inputs</td>\n</tr>\n<tr>\n<td><strong>Connection Monitoring</strong></td>\n<td><code>CLIENT LIST</code></td>\n<td>Active connection details</td>\n<td><code>CLIENT LIST TYPE normal</code></td>\n<td>Identify connection leaks and pool exhaustion</td>\n</tr>\n<tr>\n<td><strong>Performance Profiling</strong></td>\n<td><code>SLOWLOG GET count</code></td>\n<td>Recent slow operations</td>\n<td><code>SLOWLOG GET 10</code></td>\n<td>Detect rate limiting operations exceeding performance thresholds</td>\n</tr>\n<tr>\n<td><strong>Key Pattern Analysis</strong></td>\n<td><code>SCAN cursor MATCH pattern COUNT count</code></td>\n<td>Iterate through rate limit keys</td>\n<td><code>SCAN 0 MATCH ratelimit:user:* COUNT 1000</code></td>\n<td>Analyze key distribution and naming patterns</td>\n</tr>\n<tr>\n<td><strong>Memory Pressure</strong></td>\n<td><code>INFO memory</code></td>\n<td>Comprehensive memory statistics</td>\n<td><code>INFO memory</code></td>\n<td>Monitor memory usage, fragmentation, and eviction policies</td>\n</tr>\n<tr>\n<td><strong>Persistence Status</strong></td>\n<td><code>LASTSAVE</code></td>\n<td>Last successful save timestamp</td>\n<td><code>LASTSAVE</code></td>\n<td>Verify data persistence for rate limit state recovery</td>\n</tr>\n<tr>\n<td><strong>Cluster Health</strong></td>\n<td><code>CLUSTER NODES</code></td>\n<td>Cluster topology and node status</td>\n<td><code>CLUSTER NODES</code></td>\n<td>Diagnose Redis cluster partitions and failover status</td>\n</tr>\n</tbody></table>\n<p><strong>Lua Script Debugging Techniques</strong></p>\n<p>Rate limiting algorithms rely heavily on atomic Lua scripts for maintaining consistency. Debugging these scripts requires specialized approaches that account for Redis&#39;s execution environment constraints.</p>\n<p>The primary challenge in Lua script debugging lies in Redis&#39;s atomic execution model - scripts run to completion without interruption, making traditional debugging techniques ineffective. Instead, debugging relies on strategic logging, return value analysis, and controlled test environments.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Example debugging-enabled token bucket script with comprehensive logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> KEYS[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> capacity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(ARGV[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> refill_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(ARGV[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> requested_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(ARGV[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> window_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(ARGV[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Debug: Create execution trace for diagnostics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> debug_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">debug_info.</span><span style=\"color:#B392F0\">timestamp</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'TIME'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">debug_info.</span><span style=\"color:#B392F0\">key</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">debug_info.</span><span style=\"color:#B392F0\">inputs</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {capacity, refill_rate, requested_tokens, window_seconds}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Get current state with error handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> current_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'HMGET'</span><span style=\"color:#E1E4E8\">, key, </span><span style=\"color:#9ECBFF\">'tokens'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'last_refill'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(current_state[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> last_refill </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(current_state[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(debug_info.</span><span style=\"color:#B392F0\">timestamp</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">debug_info.</span><span style=\"color:#B392F0\">initial_state</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {tokens, last_refill}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Calculate refill amount with precision handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(debug_info.</span><span style=\"color:#B392F0\">timestamp</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> time_delta </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> math.max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, current_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> last_refill)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> refill_amount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> math.min</span><span style=\"color:#E1E4E8\">(capacity </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> tokens, time_delta </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> refill_rate </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> window_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> new_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> math.min</span><span style=\"color:#E1E4E8\">(capacity, tokens </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> refill_amount)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">debug_info.</span><span style=\"color:#B392F0\">refill_calculation</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {time_delta, refill_amount, new_tokens}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Make rate limiting decision</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> allowed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_tokens </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> requested_tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> final_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> allowed </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> (new_tokens </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> requested_tokens) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> new_tokens</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">debug_info.</span><span style=\"color:#B392F0\">decision</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {allowed, final_tokens}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Update state atomically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'HMSET'</span><span style=\"color:#E1E4E8\">, key, </span><span style=\"color:#9ECBFF\">'tokens'</span><span style=\"color:#E1E4E8\">, final_tokens, </span><span style=\"color:#9ECBFF\">'last_refill'</span><span style=\"color:#E1E4E8\">, current_time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'EXPIRE'</span><span style=\"color:#E1E4E8\">, key, window_seconds </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Return decision with comprehensive debugging information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowed </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> or</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    final_tokens,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> final_tokens, </span><span style=\"color:#6A737D\">-- remaining capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debug_info.</span><span style=\"color:#B392F0\">timestamp</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cjson.</span><span style=\"color:#79B8FF\">encode</span><span style=\"color:#E1E4E8\">(debug_info) </span><span style=\"color:#6A737D\">-- serialized debug information</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Redis Monitoring and Alerting Setup</strong></p>\n<p>Effective Redis monitoring for distributed rate limiting requires tracking both standard Redis metrics and rate limiting specific indicators. The monitoring system must detect performance degradation before it impacts user experience while providing sufficient detail for root cause analysis.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Key Metrics</th>\n<th>Alert Thresholds</th>\n<th>Diagnostic Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Memory Management</strong></td>\n<td><code>used_memory_rss</code>, <code>mem_fragmentation_ratio</code></td>\n<td>Memory usage &gt; 80%, fragmentation &gt; 2.0</td>\n<td>Indicates approaching capacity limits requiring scaling</td>\n</tr>\n<tr>\n<td><strong>Rate Limiting Performance</strong></td>\n<td>Lua script execution time, key access patterns</td>\n<td>Script duration &gt; 5ms, hot key concentration &gt; 10x average</td>\n<td>Identifies algorithmic inefficiencies and traffic patterns</td>\n</tr>\n<tr>\n<td><strong>Connection Health</strong></td>\n<td><code>connected_clients</code>, <code>blocked_clients</code></td>\n<td>Connections &gt; 80% of maxclients, blocked clients &gt; 0</td>\n<td>Reveals connection pool exhaustion and blocking operations</td>\n</tr>\n<tr>\n<td><strong>Persistence Status</strong></td>\n<td><code>last_save_time</code>, <code>changes_since_save</code></td>\n<td>No save in 10 minutes, unsaved changes &gt; 10000</td>\n<td>Ensures rate limit state durability for recovery scenarios</td>\n</tr>\n<tr>\n<td><strong>Cluster Coordination</strong></td>\n<td>Node availability, failover events</td>\n<td>Node down &gt; 30 seconds, frequent failovers</td>\n<td>Detects cluster instability affecting distributed consensus</td>\n</tr>\n</tbody></table>\n<h3 id=\"distributed-system-debugging\">Distributed System Debugging</h3>\n<p>Debugging distributed rate limiting systems requires sophisticated approaches that account for the fundamental challenges of multiple independent processes coordinating across network boundaries. Unlike single-node debugging where state is directly observable, distributed debugging demands correlation techniques that piece together a coherent picture from fragments scattered across multiple machines.</p>\n<p><strong>Correlation IDs and Request Tracing</strong></p>\n<p>The foundation of distributed debugging lies in correlation IDs - unique identifiers that follow requests through their entire journey across multiple system components. In distributed rate limiting, a single user request might trigger checks against multiple tiers, require Redis operations across several nodes, and involve fallback logic during failures.</p>\n<p>The <code>RequestContext</code> structure provides the framework for comprehensive request tracing throughout the rate limiting pipeline:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Tracing Purpose</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>UserID</code></td>\n<td><code>string</code></td>\n<td>Links requests to specific users across tiers</td>\n<td><code>user_12345</code></td>\n</tr>\n<tr>\n<td><code>IPAddress</code></td>\n<td><code>string</code></td>\n<td>Enables IP-based correlation and geo-analysis</td>\n<td><code>192.168.1.100</code></td>\n</tr>\n<tr>\n<td><code>APIEndpoint</code></td>\n<td><code>string</code></td>\n<td>Groups requests by functionality for pattern analysis</td>\n<td><code>/api/v1/orders</code></td>\n</tr>\n<tr>\n<td><code>Headers</code></td>\n<td><code>map[string]string</code></td>\n<td>Contains correlation IDs and request metadata</td>\n<td><code>{&quot;X-Trace-ID&quot;: &quot;trace_abc123&quot;, &quot;X-Request-ID&quot;: &quot;req_xyz789&quot;}</code></td>\n</tr>\n<tr>\n<td><code>Timestamp</code></td>\n<td><code>time.Time</code></td>\n<td>Enables temporal correlation across nodes with different clocks</td>\n<td><code>2023-10-15T14:30:45.123Z</code></td>\n</tr>\n</tbody></table>\n<p><strong>Distributed Tracing Implementation Strategy</strong></p>\n<p>Effective distributed tracing for rate limiting requires capturing decision points, performance bottlenecks, and error conditions at each system boundary. The tracing strategy must balance observability needs with performance overhead, particularly given rate limiting&#39;s position in the critical request path.</p>\n<blockquote>\n<p><strong>Decision: Structured Logging with Correlation IDs</strong></p>\n<ul>\n<li><strong>Context</strong>: Rate limiting checks occur on every API request, making tracing overhead critical to system performance</li>\n<li><strong>Options Considered</strong>: Full distributed tracing (Jaeger/Zipkin), structured logging with correlation IDs, custom event streaming</li>\n<li><strong>Decision</strong>: Structured logging with correlation IDs and sampling for detailed traces</li>\n<li><strong>Rationale</strong>: Provides necessary correlation capabilities without the performance overhead and complexity of full distributed tracing systems</li>\n<li><strong>Consequences</strong>: Enables request flow reconstruction while maintaining sub-millisecond overhead, but requires manual correlation for complex failure scenarios</li>\n</ul>\n</blockquote>\n<p>The <code>FlowCoordinator</code> serves as the central orchestration point where distributed tracing context is established and propagated:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FlowCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage         </span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keyComposer     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">KeyComposer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    algorithms      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localFallback   </span><span style=\"color:#B392F0\">Limiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metricsCollector </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsCollector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreaker  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Multi-Node Log Aggregation Patterns</strong></p>\n<p>Distributed rate limiting debugging requires aggregating logs from multiple application instances, Redis nodes, and monitoring systems into a coherent timeline. The challenge lies in correlating events across systems with different clock synchronization, log formats, and retention policies.</p>\n<table>\n<thead>\n<tr>\n<th>Log Source</th>\n<th>Information Provided</th>\n<th>Correlation Keys</th>\n<th>Retention Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Application Instances</strong></td>\n<td>Rate limit decisions, tier evaluations, algorithm execution</td>\n<td>Correlation ID, user ID, timestamp, instance ID</td>\n<td>7 days for debugging, 30 days for pattern analysis</td>\n</tr>\n<tr>\n<td><strong>Redis Cluster</strong></td>\n<td>Lua script execution, key access patterns, cluster topology changes</td>\n<td>Redis key, operation timestamp, node ID</td>\n<td>3 days for performance analysis, longer for capacity planning</td>\n</tr>\n<tr>\n<td><strong>Load Balancer</strong></td>\n<td>Request routing, instance health, traffic distribution</td>\n<td>Request ID, backend instance, response codes</td>\n<td>24 hours for routing analysis</td>\n</tr>\n<tr>\n<td><strong>Monitoring Systems</strong></td>\n<td>Performance metrics, alert triggers, system health indicators</td>\n<td>Metric timestamps, alert correlation IDs</td>\n<td>90 days for trend analysis and capacity planning</td>\n</tr>\n</tbody></table>\n<p><strong>Clock Skew Detection and Compensation</strong></p>\n<p>One of the most insidious debugging challenges in distributed rate limiting stems from clock skew between system components. When different nodes have slightly different time references, time-based rate limiting algorithms can produce inconsistent and confusing behavior that&#39;s difficult to diagnose without systematic time correlation.</p>\n<p>The <code>TimeProvider</code> component implements clock skew detection and compensation to ensure consistent time references across the distributed system:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Purpose</th>\n<th>Implementation Approach</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MeasureClockSkew(ctx context.Context) (time.Duration, error)</code></td>\n<td>Compare local time with Redis server time</td>\n<td>Execute Redis TIME command and calculate difference from local clock</td>\n<td>Retry on network errors, alert on skew &gt; 100ms</td>\n</tr>\n<tr>\n<td><code>Now() time.Time</code></td>\n<td>Provide skew-compensated current time</td>\n<td>Apply measured compensation to local time</td>\n<td>Fall back to local time on Redis unavailability</td>\n</tr>\n<tr>\n<td><code>SyncWithRedis(ctx context.Context) error</code></td>\n<td>Periodic synchronization with authoritative time source</td>\n<td>Background goroutine measuring skew every 30 seconds</td>\n<td>Circuit breaker on persistent sync failures</td>\n</tr>\n</tbody></table>\n<p><strong>Debugging Workflow for Common Scenarios</strong></p>\n<p>Effective distributed debugging follows systematic workflows that guide investigation from initial symptom observation through root cause identification and resolution verification. These workflows account for the complexity of distributed system interactions while providing actionable steps for developers at different experience levels.</p>\n<p><strong>Scenario 1: Inconsistent Rate Limiting Across Instances</strong></p>\n<p>When the same user experiences different rate limiting behavior depending on which application instance handles their request, the root cause typically lies in state synchronization, configuration propagation, or local fallback activation.</p>\n<p>Investigation Workflow:</p>\n<ol>\n<li><strong>Gather Request Context</strong>: Collect correlation IDs, timestamps, and instance identifiers for both the allowed and denied requests</li>\n<li><strong>Verify Configuration Consistency</strong>: Check that all instances have the same rate limiting rules and Redis configuration</li>\n<li><strong>Examine Redis Connectivity</strong>: Confirm all instances maintain healthy connections to the same Redis cluster</li>\n<li><strong>Analyze Local Fallback Status</strong>: Determine if any instances have activated local fallback due to Redis issues</li>\n<li><strong>Check Clock Synchronization</strong>: Measure time differences between instances and Redis to identify skew issues</li>\n<li><strong>Trace Key Composition</strong>: Verify that identical requests generate the same Redis keys across all instances</li>\n</ol>\n<p><strong>Scenario 2: Performance Degradation Under Load</strong></p>\n<p>When rate limiting performance degrades as traffic increases, the issue typically involves connection pool exhaustion, hot key concentration, or algorithmic complexity scaling problems.</p>\n<p>Investigation Workflow:</p>\n<ol>\n<li><strong>Baseline Performance Measurement</strong>: Establish normal response time distribution for rate limiting operations</li>\n<li><strong>Connection Pool Analysis</strong>: Monitor Redis connection pool utilization and blocking statistics</li>\n<li><strong>Hot Key Identification</strong>: Use Redis monitoring to identify keys receiving disproportionate traffic</li>\n<li><strong>Algorithm Performance Profiling</strong>: Measure Lua script execution times under different load conditions</li>\n<li><strong>Network Latency Assessment</strong>: Evaluate round-trip times to Redis cluster under load</li>\n<li><strong>Scaling Factor Analysis</strong>: Determine how performance degradation correlates with traffic volume</li>\n</ol>\n<p><strong>Distributed Debugging Tools and Techniques</strong></p>\n<table>\n<thead>\n<tr>\n<th>Tool Category</th>\n<th>Specific Tools</th>\n<th>Use Case</th>\n<th>Implementation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Log Aggregation</strong></td>\n<td>ELK Stack, Fluentd, Loki</td>\n<td>Centralized log collection and search</td>\n<td>Ship logs with correlation IDs, use structured JSON format</td>\n</tr>\n<tr>\n<td><strong>Metrics Collection</strong></td>\n<td>Prometheus, InfluxDB</td>\n<td>Performance monitoring and alerting</td>\n<td>Custom metrics for rate limiting operations, Redis health</td>\n</tr>\n<tr>\n<td><strong>Distributed Tracing</strong></td>\n<td>Jaeger, Zipkin (sampled)</td>\n<td>Complex request flow analysis</td>\n<td>1% sampling for detailed traces, correlation ID propagation</td>\n</tr>\n<tr>\n<td><strong>Redis Monitoring</strong></td>\n<td>RedisInsight, Redis monitoring commands</td>\n<td>Redis-specific debugging</td>\n<td>Real-time key inspection, memory analysis, performance profiling</td>\n</tr>\n<tr>\n<td><strong>Network Analysis</strong></td>\n<td>tcpdump, Wireshark, network monitoring</td>\n<td>Network-level debugging</td>\n<td>Packet capture for Redis communication, latency measurement</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations for Debugging Infrastructure</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Log Aggregation</td>\n<td>Local log files with correlation IDs</td>\n<td>ELK Stack with Fluentd collection</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Built-in Prometheus metrics</td>\n<td>Custom metrics with Grafana dashboards</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Structured logging with trace IDs</td>\n<td>Jaeger with sampling</td>\n</tr>\n<tr>\n<td>Redis Monitoring</td>\n<td>Redis CLI commands + scripts</td>\n<td>RedisInsight + custom monitoring</td>\n</tr>\n<tr>\n<td>Debugging Tools</td>\n<td>Go pprof + manual log analysis</td>\n<td>Comprehensive observability platform</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure for Debugging Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    debugging/\n      correlation/\n        trace_context.go      ← correlation ID management\n        request_context.go    ← request tracing context\n      metrics/\n        collector.go          ← metrics collection\n        redis_metrics.go      ← Redis-specific metrics\n      logging/\n        structured_logger.go  ← structured logging utilities\n        correlation_logger.go ← correlation-aware logging\n      diagnostics/\n        redis_diagnostics.go  ← Redis debugging utilities\n        cluster_diagnostics.go ← cluster health checking\n        symptom_analyzer.go   ← automated symptom analysis\n  cmd/debug/\n    redis-cli.go             ← Redis debugging CLI tool\n    cluster-health.go        ← cluster health checker\n    trace-analyzer.go        ← correlation ID trace analysis\n  scripts/\n    redis-debug.sh           ← Redis debugging scripts\n    log-correlation.py       ← log correlation utilities</code></pre></div>\n\n<p><strong>Correlation ID Management Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// CorrelationContext manages request correlation across distributed components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CorrelationContext</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TraceID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"trace_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"request_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UserID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"user_id,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"session_id,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metadata,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"start_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCorrelationContext creates a new correlation context with unique identifiers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCorrelationContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">userID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationContext</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Generate unique trace ID using UUID or similar</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create request ID for this specific request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Capture start timestamp for duration calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize metadata map for additional context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use crypto/rand for secure random ID generation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WithMetadata adds metadata to correlation context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationContext</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WithMetadata</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationContext</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add key-value pair to metadata map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return context for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Consider creating a copy to avoid mutation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PropagateToHeaders converts correlation context to HTTP headers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationContext</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">PropagateToHeaders</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Convert correlation context to HTTP headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Use standard header names (X-Trace-ID, X-Request-ID)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Serialize metadata as JSON if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Follow OpenTracing header conventions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Structured Logging with Correlation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// CorrelationLogger provides correlation-aware structured logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CorrelationLogger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">logrus</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCorrelationLogger creates a correlation-aware logger for a component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCorrelationLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationLogger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize logrus logger with JSON formatter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set appropriate log level from environment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Configure output destination (stdout/file)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Store component name for automatic field injection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use logrus.JSONFormatter for structured output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogRateLimitDecision logs rate limiting decisions with full context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationLogger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LogRateLimitDecision</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    ctx</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">CorrelationContext</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    redisKey</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create log entry with correlation fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add rate limiting specific fields (allowed, remaining, algorithm)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Include performance metrics (duration, Redis key)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Use appropriate log level based on result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Include all fields needed for debugging and analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogRedisOperation logs Redis operations for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationLogger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LogRedisOperation</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    ctx</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">CorrelationContext</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Log Redis operation with timing and error information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Include Redis key and operation type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Use different log levels for success vs error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add Redis node information if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This helps correlate application logs with Redis logs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Redis Debugging Utilities</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// RedisDebugger provides comprehensive Redis debugging capabilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisDebugger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationLogger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRedisDebugger creates Redis debugging utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisDebugger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">client</span><span style=\"color:#B392F0\"> redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisDebugger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize debugger with Redis client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create correlation logger for Redis operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set up health checking capabilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This will be used for manual debugging and automated diagnostics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DiagnoseKeyState provides comprehensive analysis of a rate limiting key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisDebugger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DiagnoseKeyState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">KeyDiagnostics</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Get current value and TTL for the key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Calculate memory usage for the key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check if key exists in all expected hash ring nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Measure access latency for the key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return comprehensive diagnostic information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use Redis MEMORY USAGE, TTL, and timing commands</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MeasureClusterHealth assesses overall Redis cluster health</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisDebugger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">MeasureClusterHealth</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterHealth</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check connectivity to all cluster nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Measure response times for each node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check memory usage and key distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Identify any failing or slow nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return overall cluster health assessment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use Redis CLUSTER commands and timing measurements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AnalyzePerformanceIssues identifies common Redis performance problems</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisDebugger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AnalyzePerformanceIssues</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceAnalysis</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check for slow operations using SLOWLOG</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Analyze memory fragmentation and usage patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Identify hot keys causing performance issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check connection pool utilization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return analysis with recommended actions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Combine multiple Redis diagnostic commands</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Debugging Data Structures</strong></p>\n<table>\n<thead>\n<tr>\n<th>Structure</th>\n<th>Fields</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>KeyDiagnostics</code></td>\n<td><code>Key string, Exists bool, TTL time.Duration, MemoryUsage int64, AccessLatency time.Duration, NodeLocations []string</code></td>\n<td>Complete diagnostic information for a rate limiting key</td>\n</tr>\n<tr>\n<td><code>ClusterHealth</code></td>\n<td><code>TotalNodes int, HealthyNodes int, NodeStatuses map[string]*NodeStatus, AverageLatency time.Duration, KeyDistribution map[string]int</code></td>\n<td>Overall Redis cluster health assessment</td>\n</tr>\n<tr>\n<td><code>PerformanceAnalysis</code></td>\n<td><code>SlowOperations []SlowOperation, MemoryIssues []string, HotKeys []string, RecommendedActions []string</code></td>\n<td>Performance problem analysis and recommendations</td>\n</tr>\n<tr>\n<td><code>NodeStatus</code></td>\n<td><code>Address string, Reachable bool, Latency time.Duration, MemoryUsage float64, Role string, LastError error</code></td>\n<td>Individual Redis node status information</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone Checkpoints</strong></p>\n<p><strong>Checkpoint 1: Basic Debugging Infrastructure (After Milestone 1)</strong></p>\n<ul>\n<li>Command: <code>go test ./internal/debugging/... -v</code></li>\n<li>Expected: All debugging utilities compile and basic tests pass</li>\n<li>Manual verification: Generate correlation IDs, verify structured logging output includes correlation fields</li>\n<li>Signs of issues: Missing correlation fields in logs, UUID generation errors</li>\n</ul>\n<p><strong>Checkpoint 2: Redis Debugging Tools (After Milestone 3)</strong></p>\n<ul>\n<li>Command: <code>./cmd/debug/redis-cli -operation=diagnose -key=test_key</code></li>\n<li>Expected: Comprehensive key diagnostics including memory usage, TTL, and access latency</li>\n<li>Manual verification: Connect to Redis manually and compare diagnostic output with manual commands</li>\n<li>Signs of issues: Timeout errors, missing Redis connectivity, incorrect key analysis</li>\n</ul>\n<p><strong>Checkpoint 3: Distributed Correlation (After Milestone 4)</strong></p>\n<ul>\n<li>Command: Start multiple application instances and trace a request across them using correlation IDs</li>\n<li>Expected: Same correlation ID appears in logs from all instances handling the request</li>\n<li>Manual verification: Search logs for correlation ID, verify complete request flow is captured</li>\n<li>Signs of issues: Missing correlation in some instances, broken correlation chain</li>\n</ul>\n<p><strong>Checkpoint 4: Performance Debugging (After Milestone 5)</strong></p>\n<ul>\n<li>Command: <code>go test -bench=. -memprofile=mem.prof -cpuprofile=cpu.prof</code></li>\n<li>Expected: Performance profiles show debugging overhead &lt; 1% of total execution time</li>\n<li>Manual verification: Run load test with debugging enabled, verify minimal performance impact</li>\n<li>Signs of issues: High debugging overhead, memory leaks in correlation tracking</li>\n</ul>\n<p><strong>Common Debugging Pitfalls and Solutions</strong></p>\n<p>⚠️ <strong>Pitfall: Correlation ID Not Propagated</strong>\nMany developers forget to propagate correlation IDs through all system boundaries, leading to broken trace chains. This happens when middleware doesn&#39;t extract IDs from headers or when background processes don&#39;t inherit correlation context. Solution: Create explicit correlation context types and ensure every system boundary explicitly handles correlation propagation.</p>\n<p>⚠️ <strong>Pitfall: Clock Skew Ignored in Debugging</strong>\nTime-based debugging assumes synchronized clocks, but distributed systems often have clock skew that makes event ordering confusing. Log timestamps from different nodes can&#39;t be directly compared without accounting for skew. Solution: Always include a time synchronization check in debugging workflows and normalize timestamps to a common reference.</p>\n<p>⚠️ <strong>Pitfall: Debugging Overhead in Production</strong>\nVerbose debugging significantly impacts rate limiting performance when left enabled in production. Detailed logging and tracing can double response times. Solution: Implement sampling for detailed traces (1-5% of requests) and use structured logging with configurable verbosity levels.</p>\n<p>⚠️ <strong>Pitfall: Redis Debug Commands in Production</strong>\nRunning Redis debugging commands like SLOWLOG or MEMORY USAGE during high load can impact Redis performance. These commands can block Redis briefly while gathering information. Solution: Use Redis replicas for debugging commands when possible, and limit debug command frequency during peak traffic.</p>\n<p>⚠️ <strong>Pitfall: Incomplete Error Context</strong>\nWhen rate limiting errors occur, developers often log the immediate error without sufficient context about the request, Redis state, or system conditions. This makes root cause analysis nearly impossible. Solution: Always include correlation context, request details, Redis key information, and system state in error logs.</p>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Beyond current implementation scope - these extensions represent advanced capabilities that could be added after completing all five core milestones</p>\n</blockquote>\n<p>The distributed rate limiter we&#39;ve designed provides a solid foundation for production rate limiting needs. However, the landscape of distributed systems continues to evolve, presenting new challenges and opportunities. This section explores three major extensions that would significantly enhance the system&#39;s capabilities: adaptive rate limiting that responds dynamically to system conditions, geographic distribution for global scale applications, and seamless integration with modern service mesh architectures.</p>\n<p>These extensions represent natural evolution paths as organizations scale their infrastructure and face increasingly complex operational requirements. Each extension builds upon the core architecture while introducing sophisticated new capabilities that address real-world operational challenges.</p>\n<h3 id=\"adaptive-rate-limiting\">Adaptive Rate Limiting</h3>\n<p><strong>Mental Model: Smart Traffic Light System</strong></p>\n<p>Think of adaptive rate limiting like a smart traffic management system in a busy city. Traditional traffic lights operate on fixed timers regardless of actual traffic conditions - they might keep cars waiting at a red light even when no cross traffic exists. Smart traffic lights, however, use sensors to detect real traffic patterns and adjust timing dynamically. Similarly, traditional rate limiting uses fixed thresholds regardless of system health, while adaptive rate limiting monitors system performance and adjusts limits based on actual capacity and response times.</p>\n<p>Adaptive rate limiting represents a fundamental shift from static resource protection to dynamic capacity management. Instead of enforcing predetermined limits regardless of system state, an adaptive system continuously monitors performance indicators and automatically adjusts rate limits to maintain optimal system health while maximizing throughput.</p>\n<p>The core principle behind adaptive rate limiting involves establishing a feedback loop between system performance metrics and rate limit thresholds. When the system operates well below capacity with fast response times, rate limits can be relaxed to allow more traffic. Conversely, when response times increase or error rates climb, the system tightens limits to prevent cascade failures.</p>\n<blockquote>\n<p><strong>Decision: Feedback Loop Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Static rate limits either waste capacity during low load or fail to protect during unexpected traffic spikes</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>External monitoring system that updates rate limit rules via API</li>\n<li>Built-in adaptive controller that adjusts limits based on local metrics</li>\n<li>Machine learning-based predictor that forecasts optimal limits</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Built-in adaptive controller with configurable feedback algorithms</li>\n<li><strong>Rationale</strong>: External systems introduce latency and complexity, while ML approaches require significant data and expertise. Built-in controllers provide real-time adaptation with predictable behavior.</li>\n<li><strong>Consequences</strong>: Enables automatic scaling of rate limits but requires careful tuning to prevent oscillations</li>\n</ul>\n</blockquote>\n<h4 id=\"adaptive-algorithm-design\">Adaptive Algorithm Design</h4>\n<p>The adaptive rate limiting system operates through several interconnected components that monitor system health, calculate optimal limits, and gradually adjust thresholds to prevent shock changes that could destabilize the system.</p>\n<p><strong>Performance Metrics Collection</strong></p>\n<p>The foundation of adaptive rate limiting lies in comprehensive performance monitoring. The system must collect metrics across multiple dimensions to understand system health holistically.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Specific Metrics</th>\n<th>Collection Method</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Response Time</td>\n<td>P50, P95, P99 latency</td>\n<td>Request timing instrumentation</td>\n<td>Every 10 seconds</td>\n</tr>\n<tr>\n<td>Error Rate</td>\n<td>5xx errors, timeout rate</td>\n<td>Response status tracking</td>\n<td>Every 10 seconds</td>\n</tr>\n<tr>\n<td>System Resources</td>\n<td>CPU utilization, memory usage</td>\n<td>OS metrics collection</td>\n<td>Every 30 seconds</td>\n</tr>\n<tr>\n<td>Queue Depth</td>\n<td>Pending requests, worker utilization</td>\n<td>Application metrics</td>\n<td>Every 5 seconds</td>\n</tr>\n<tr>\n<td>Downstream Health</td>\n<td>Dependency response times</td>\n<td>Service mesh metrics</td>\n<td>Every 15 seconds</td>\n</tr>\n</tbody></table>\n<p>The metrics collection system must balance accuracy with overhead. High-frequency collection provides better responsiveness but consumes system resources. The adaptive controller uses exponential smoothing to reduce noise while maintaining sensitivity to genuine performance changes.</p>\n<p><strong>Control Algorithm Implementation</strong></p>\n<p>The adaptive control algorithm implements a PID (Proportional-Integral-Derivative) controller that adjusts rate limits based on the difference between target and actual performance metrics.</p>\n<p>The proportional component responds to current performance deviation from targets. If response times exceed the target by 20%, the proportional controller immediately reduces rate limits proportionally. The integral component addresses sustained deviations by accumulating error over time, ensuring persistent performance issues result in continued limit adjustments. The derivative component anticipates future performance by observing the rate of change, helping prevent overshoot when conditions improve rapidly.</p>\n<ol>\n<li>The controller samples current performance metrics every adjustment interval (typically 30-60 seconds)</li>\n<li>It calculates the error between actual performance and target thresholds for each monitored metric</li>\n<li>The proportional component computes immediate adjustment based on current error magnitude</li>\n<li>The integral component accumulates historical error to address persistent issues</li>\n<li>The derivative component considers error rate of change to anticipate trends</li>\n<li>All components combine to produce a rate limit adjustment factor</li>\n<li>The system applies adjustment gradually over multiple intervals to prevent shock changes</li>\n<li>Safety bounds prevent adjustments beyond configured minimum and maximum limits</li>\n</ol>\n<p><strong>Oscillation Prevention</strong></p>\n<p>One of the primary challenges in adaptive systems involves preventing oscillations where limits bounce between high and low values. This occurs when the system overreacts to performance changes, creating instability rather than improvement.</p>\n<p>The system implements several oscillation prevention mechanisms:</p>\n<p><strong>Hysteresis</strong>: Different thresholds trigger increases versus decreases in rate limits. Response times must drop below 80% of the target to increase limits but must exceed 120% to decrease them. This prevents constant adjustments around the threshold.</p>\n<p><strong>Rate of Change Limits</strong>: Adjustments cannot exceed 10% per interval regardless of error magnitude. Large performance changes trigger multiple small adjustments rather than dramatic swings.</p>\n<p><strong>Adjustment History</strong>: The controller tracks recent adjustments and reduces sensitivity when frequent changes occur. If limits have been adjusted more than three times in the past hour, subsequent changes require larger error magnitudes.</p>\n<p><strong>Warmup Periods</strong>: After any adjustment, the system waits for performance metrics to stabilize before considering additional changes. This prevents cascading adjustments based on transient effects.</p>\n<h4 id=\"integration-with-existing-architecture\">Integration with Existing Architecture</h4>\n<p>Adaptive rate limiting integrates into the existing distributed rate limiter architecture through several key extension points that maintain backward compatibility while adding dynamic capabilities.</p>\n<p>The <code>AdaptiveController</code> component sits between the metrics collection system and the rate limit rule management, automatically updating rule thresholds based on performance feedback.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Interface</th>\n<th>Responsibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MetricsCollector</td>\n<td><code>CollectPerformanceMetrics() *PerformanceSnapshot</code></td>\n<td>Gather system performance indicators</td>\n</tr>\n<tr>\n<td>AdaptiveController</td>\n<td><code>UpdateLimits(ctx context.Context, metrics *PerformanceSnapshot) error</code></td>\n<td>Calculate optimal rate limits</td>\n</tr>\n<tr>\n<td>ControlAlgorithm</td>\n<td><code>CalculateAdjustment(current, target PerformanceMetrics) float64</code></td>\n<td>PID control algorithm implementation</td>\n</tr>\n<tr>\n<td>SafetyBounds</td>\n<td><code>ValidateAdjustment(currentLimit, proposedLimit int64) int64</code></td>\n<td>Prevent dangerous limit changes</td>\n</tr>\n</tbody></table>\n<p>The adaptive system extends the existing <code>RateLimitRule</code> structure with adaptive configuration fields while maintaining compatibility with static rules.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>adaptive_enabled</td>\n<td>bool</td>\n<td>Whether this rule participates in adaptive adjustment</td>\n</tr>\n<tr>\n<td>target_p99_latency</td>\n<td>time.Duration</td>\n<td>Maximum acceptable P99 response time</td>\n</tr>\n<tr>\n<td>target_error_rate</td>\n<td>float64</td>\n<td>Maximum acceptable error rate percentage</td>\n</tr>\n<tr>\n<td>min_limit</td>\n<td>int64</td>\n<td>Absolute minimum requests allowed per window</td>\n</tr>\n<tr>\n<td>max_limit</td>\n<td>int64</td>\n<td>Absolute maximum requests allowed per window</td>\n</tr>\n<tr>\n<td>adjustment_sensitivity</td>\n<td>float64</td>\n<td>Multiplier for control algorithm responsiveness</td>\n</tr>\n</tbody></table>\n<h4 id=\"common-pitfalls-in-adaptive-systems\">Common Pitfalls in Adaptive Systems</h4>\n<p>⚠️ <strong>Pitfall: Rapid Oscillation</strong>\nMany adaptive implementations suffer from oscillation where limits bounce rapidly between high and low values. This occurs when the system overreacts to temporary performance changes without considering adjustment history. The fix involves implementing hysteresis with different thresholds for increasing versus decreasing limits, and rate-limiting the frequency of adjustments.</p>\n<p>⚠️ <strong>Pitfall: Cascade Failures During Traffic Spikes</strong>\nWhen traffic suddenly increases, response times may spike temporarily even if the system can handle the load. Poorly tuned adaptive systems may aggressively reduce limits, creating artificial scarcity that prevents the system from recovering. The solution requires distinguishing between temporary load spikes and genuine capacity problems by considering multiple metrics simultaneously.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Downstream Dependencies</strong>\nAdaptive systems that only monitor local performance may miss downstream bottlenecks. When a database becomes slow, the adaptive system might increase local rate limits, worsening the downstream problem. Comprehensive adaptive systems must monitor the health of all critical dependencies and adjust limits based on the weakest link in the chain.</p>\n<h3 id=\"geographic-distribution\">Geographic Distribution</h3>\n<p><strong>Mental Model: Global Banking Network</strong></p>\n<p>Consider how major banks operate across different countries and time zones. Each branch maintains local cash reserves for immediate customer needs, but they&#39;re all connected to regional centers that can transfer funds when needed. Customer account balances must be consistent globally - you can&#39;t withdraw the same money from ATMs in New York and London simultaneously. However, the system tolerates brief inconsistencies during transfers as long as the final state is correct. Geographic rate limiting works similarly, with local enforcement for immediate decisions and eventual consistency for global accuracy.</p>\n<p>Geographic distribution extends the distributed rate limiter to operate across multiple geographic regions, each potentially containing multiple data centers. This architecture addresses the needs of global applications that serve users worldwide while maintaining rate limiting accuracy across regions.</p>\n<p>The fundamental challenge in geographic distribution lies in balancing consistency with performance. Users expect sub-100ms response times regardless of their location, but maintaining perfect global consistency for rate limits would require cross-region network calls that add hundreds of milliseconds of latency.</p>\n<p>The solution involves a hybrid approach combining local enforcement with asynchronous global reconciliation. Each region maintains local rate limit state that enables immediate decisions, while background processes synchronize state across regions to maintain global accuracy over time.</p>\n<h4 id=\"multi-region-architecture-design\">Multi-Region Architecture Design</h4>\n<p>The geographic distribution architecture consists of three primary layers: regional clusters, global coordination, and conflict resolution mechanisms.</p>\n<p><strong>Regional Cluster Organization</strong></p>\n<p>Each geographic region operates as an independent rate limiting cluster with complete functionality for serving local traffic. Regional clusters contain their own Redis shards, application instances, and management APIs.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Regional Instance</th>\n<th>Global Coordination</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Cluster</td>\n<td>3-5 nodes per region</td>\n<td>Cross-region replication streams</td>\n</tr>\n<tr>\n<td>Application Instances</td>\n<td>Serve local traffic only</td>\n<td>Participate in global state sync</td>\n</tr>\n<tr>\n<td>Management API</td>\n<td>Regional configuration</td>\n<td>Global rule propagation</td>\n</tr>\n<tr>\n<td>Monitoring Dashboard</td>\n<td>Regional metrics</td>\n<td>Aggregated global view</td>\n</tr>\n</tbody></table>\n<p>Regional clusters implement the complete distributed rate limiting architecture described in previous sections. They can operate independently even when network connectivity to other regions fails, ensuring local users experience no service degradation during network partitions.</p>\n<p><strong>Global State Synchronization</strong></p>\n<p>Global state synchronization operates through asynchronous replication streams that propagate rate limit usage information between regions. Unlike traditional database replication, rate limiting synchronization focuses on aggregate consumption rather than individual request tracking.</p>\n<p>The synchronization protocol operates on a periodic basis (typically every 30-60 seconds) where each region reports its usage statistics to other regions:</p>\n<ol>\n<li>Each regional cluster aggregates rate limit usage across all local keys and time windows</li>\n<li>Usage reports include consumed tokens, timestamp ranges, and region identifiers</li>\n<li>Reports transmit to other regions through reliable message queues or direct Redis streams</li>\n<li>Receiving regions incorporate remote usage into their local enforcement decisions</li>\n<li>Conflict resolution algorithms handle cases where global usage exceeds configured limits</li>\n<li>Corrective actions redistribute quota or temporarily tighten local enforcement</li>\n</ol>\n<p><strong>Eventual Consistency Model</strong></p>\n<p>The geographic distribution system operates under an eventual consistency model where rate limits may temporarily exceed global thresholds during network partitions or high cross-region latency, but converge to correct enforcement as synchronization catches up.</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Guarantee</th>\n<th>Local Enforcement</th>\n<th>Global Convergence</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Strong Consistency</td>\n<td>Not achievable without unacceptable latency</td>\n<td>Not suitable for user-facing responses</td>\n</tr>\n<tr>\n<td>Eventual Consistency</td>\n<td>Immediate decisions based on local + cached remote state</td>\n<td>Global limits enforced within sync interval</td>\n</tr>\n<tr>\n<td>Bounded Staleness</td>\n<td>Local decisions with maximum staleness bounds</td>\n<td>Guarantees global limit violations stay within bounds</td>\n</tr>\n</tbody></table>\n<p>The bounded staleness approach provides the best balance for most applications. Local enforcement uses recently synchronized global state (maximum 2-3 minutes stale) combined with current local usage to make decisions. This ensures global violations remain bounded even during extended network partitions.</p>\n<h4 id=\"cross-region-synchronization-protocol\">Cross-Region Synchronization Protocol</h4>\n<p>The synchronization protocol ensures efficient propagation of rate limiting state while handling network failures, region outages, and conflicting updates from multiple sources.</p>\n<p><strong>Incremental State Transfer</strong></p>\n<p>Rather than synchronizing complete rate limiting state, the protocol transmits incremental updates focusing on consumed quota within specific time windows.</p>\n<table>\n<thead>\n<tr>\n<th>Message Type</th>\n<th>Contents</th>\n<th>Frequency</th>\n<th>Size</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Usage Update</td>\n<td>Key, consumed tokens, window, timestamp</td>\n<td>Every 60 seconds</td>\n<td>&lt;1KB per key</td>\n</tr>\n<tr>\n<td>Heartbeat</td>\n<td>Region health, sync lag, active keys</td>\n<td>Every 30 seconds</td>\n<td>&lt;100 bytes</td>\n</tr>\n<tr>\n<td>Quota Adjustment</td>\n<td>Key, new global limit, reason</td>\n<td>On configuration change</td>\n<td>&lt;500 bytes</td>\n</tr>\n<tr>\n<td>Conflict Resolution</td>\n<td>Key, authoritative state, reconciliation</td>\n<td>When conflicts detected</td>\n<td>&lt;2KB</td>\n</tr>\n</tbody></table>\n<p>Usage updates contain aggregated consumption data rather than individual request logs. For a key with 1000 requests in the last minute, the system sends a single update indicating &quot;1000 tokens consumed&quot; rather than 1000 individual records.</p>\n<p><strong>Conflict Detection and Resolution</strong></p>\n<p>Conflicts arise when the sum of regional usage reports exceeds the configured global limit for a key. This can occur during network partitions where regions operate independently or during traffic spikes that exceed synchronization frequency.</p>\n<p>The conflict resolution protocol implements a priority-based approach:</p>\n<ol>\n<li><strong>Detection Phase</strong>: Each region compares local + remote usage against global limits during sync processing</li>\n<li><strong>Reporting Phase</strong>: Regions experiencing conflicts broadcast conflict notifications to all other regions</li>\n<li><strong>Priority Resolution</strong>: Regions use deterministic priority rules (timestamp, region ID) to determine authoritative state</li>\n<li><strong>Reconciliation Phase</strong>: Non-authoritative regions adjust their local enforcement to compensate for over-consumption</li>\n<li><strong>Recovery Phase</strong>: Gradual restoration of normal enforcement as usage patterns stabilize</li>\n</ol>\n<p><strong>Network Partition Handling</strong></p>\n<p>Network partitions between regions represent one of the most challenging scenarios for geographic distribution. The system must continue serving users in each region while preventing excessive global limit violations.</p>\n<p>During partition detection (missed heartbeats, failed sync operations), each region transitions to conservative enforcement mode:</p>\n<ul>\n<li>Local rate limits reduce by a safety margin (typically 20-30%) to compensate for unknown remote usage</li>\n<li>Cached remote state continues informing decisions but with increasing skepticism over time</li>\n<li>Critical keys (those approaching limits) receive more aggressive local enforcement</li>\n<li>Non-critical keys maintain normal enforcement to avoid unnecessary user impact</li>\n</ul>\n<p>When partitions heal, regions exchange complete state snapshots to reconcile any conflicts that occurred during isolation. The reconciliation process prioritizes preserving user experience over perfect quota enforcement, typically allowing temporary over-consumption rather than retroactively blocking users.</p>\n<h4 id=\"common-pitfalls-in-geographic-distribution\">Common Pitfalls in Geographic Distribution</h4>\n<p>⚠️ <strong>Pitfall: Ignoring Network Latency Variations</strong>\nMany geographic distribution implementations assume consistent network latency between regions. In reality, trans-Pacific links may have 300ms latency while trans-Atlantic links have 150ms. This variation affects synchronization timing and can cause some regions to operate with staler data than others. The solution involves adaptive sync intervals based on measured network latency and prioritizing synchronization between high-latency region pairs.</p>\n<p>⚠️ <strong>Pitfall: Global Limits That Are Too Restrictive</strong>\nSetting global limits equal to the sum of regional capacity ignores cross-regional traffic distribution. If Europe has 40% of global users but only 25% of quota, European users will experience unnecessary blocking while other regions have unused capacity. Effective geographic distribution requires quota allocation that matches actual traffic distribution with automatic rebalancing capabilities.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Partition Tolerance Testing</strong>\nGeographic distribution systems often work perfectly during testing with reliable networks but fail catastrophically during real network partitions. Comprehensive testing must simulate various partition scenarios: complete isolation, asymmetric partitions where region A can reach B but not vice versa, and flapping connections that repeatedly partition and heal.</p>\n<h3 id=\"service-mesh-integration\">Service Mesh Integration</h3>\n<p><strong>Mental Model: Airport Security Integration</strong></p>\n<p>Consider how security checkpoints integrate into airport operations. Rather than requiring every airline to implement their own security protocols, airports provide centralized security infrastructure that all flights use transparently. Passengers don&#39;t need to understand different security systems - they simply pass through standardized checkpoints. Similarly, service mesh integration allows applications to benefit from rate limiting without implementing rate limiting logic themselves. The mesh intercepts traffic transparently and applies policies consistently across all services.</p>\n<p>Service mesh integration represents a paradigm shift from application-embedded rate limiting to infrastructure-level policy enforcement. Instead of each service implementing its own rate limiting logic, the service mesh (Istio, Linkerd, or Consul Connect) intercepts network traffic and applies rate limiting policies transparently.</p>\n<p>This approach offers significant operational advantages: consistent policy enforcement across all services, centralized configuration management, and the ability to apply rate limiting to legacy applications without code changes. However, it also introduces new complexity in policy management and debugging distributed policy enforcement.</p>\n<h4 id=\"envoy-proxy-integration-design\">Envoy Proxy Integration Design</h4>\n<p>Most service meshes use Envoy proxy as their data plane component, making Envoy integration the primary path for service mesh rate limiting support. Envoy provides several extension points for implementing custom rate limiting logic.</p>\n<p><strong>Rate Limiting Filter Architecture</strong></p>\n<p>Envoy implements rate limiting through HTTP filters that intercept requests before they reach upstream services. The distributed rate limiter integrates as an external authorization service that Envoy queries for each request.</p>\n<table>\n<thead>\n<tr>\n<th>Envoy Component</th>\n<th>Integration Point</th>\n<th>Responsibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Connection Manager</td>\n<td>Rate Limit Filter</td>\n<td>Intercepts requests and queries rate limiting service</td>\n</tr>\n<tr>\n<td>Cluster Manager</td>\n<td>Rate Limit Service Cluster</td>\n<td>Manages connections to rate limiting service instances</td>\n</tr>\n<tr>\n<td>Admin Interface</td>\n<td>Statistics and Health</td>\n<td>Exposes rate limiting metrics and health status</td>\n</tr>\n<tr>\n<td>Configuration API</td>\n<td>Dynamic Updates</td>\n<td>Receives rate limiting policy updates</td>\n</tr>\n</tbody></table>\n<p>The integration follows Envoy&#39;s external authorization pattern where the proxy sends request context to the rate limiting service and waits for an allow/deny decision. This design maintains clean separation between traffic proxying and policy enforcement.</p>\n<p><strong>Request Context Extraction</strong></p>\n<p>The service mesh integration must extract relevant rate limiting context from HTTP requests and connection metadata. Unlike application-level integration where context is readily available, proxy-level integration must infer context from network-level information.</p>\n<table>\n<thead>\n<tr>\n<th>Context Source</th>\n<th>Available Information</th>\n<th>Rate Limiting Application</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Headers</td>\n<td>User-Agent, Authorization, Custom headers</td>\n<td>User identification, API versioning</td>\n</tr>\n<tr>\n<td>Connection Metadata</td>\n<td>Source IP, Destination service</td>\n<td>IP-based limiting, service quotas</td>\n</tr>\n<tr>\n<td>TLS Certificate</td>\n<td>Client certificate CN, SAN</td>\n<td>Certificate-based user identification</td>\n</tr>\n<tr>\n<td>Request Path</td>\n<td>URL path, query parameters</td>\n<td>API endpoint classification</td>\n</tr>\n<tr>\n<td>Service Labels</td>\n<td>Kubernetes labels, service mesh metadata</td>\n<td>Service-to-service rate limiting</td>\n</tr>\n</tbody></table>\n<p>The context extraction process operates through configurable rules that map network-level information to rate limiting dimensions. For example, a rule might extract user ID from the &quot;X-User-ID&quot; header and map it to per-user rate limiting, while another rule uses the destination service name for global service rate limiting.</p>\n<p><strong>Policy Configuration Integration</strong></p>\n<p>Service mesh policy configuration differs significantly from application-level configuration. Instead of managing rate limiting rules through dedicated APIs, policies integrate with service mesh configuration systems like Istio&#39;s VirtualService and DestinationRule resources.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example Istio integration (for illustration - actual implementation in code section)</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">apiVersion</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">networking.istio.io/v1alpha3</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">kind</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">VirtualService</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">metadata</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">api-rate-limiting</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">spec</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  hosts</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#9ECBFF\">api.example.com</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  http</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">match</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    - </span><span style=\"color:#85E89D\">uri</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">        prefix</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">/api/v1</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    route</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    - </span><span style=\"color:#85E89D\">destination</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">        host</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">api-service</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    rateLimiting</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      algorithm</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">token_bucket</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      limit</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1000</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      window</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">60s</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      dimensions</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#85E89D\">header</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"x-user-id\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">        tier</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">user</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#85E89D\">source_ip</span><span style=\"color:#E1E4E8\">: {}</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">        tier</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">ip</span></span></code></pre></div>\n\n<p>This approach provides several advantages over standalone rate limiting services: configuration lives alongside routing rules, policies automatically apply to matching traffic, and service mesh tools provide policy validation and deployment workflows.</p>\n<h4 id=\"transparent-policy-enforcement\">Transparent Policy Enforcement</h4>\n<p>Transparent policy enforcement ensures applications receive rate limiting protection without requiring code changes or awareness of rate limiting infrastructure. This transparency extends to multiple aspects of the system: request handling, error responses, and monitoring integration.</p>\n<p><strong>Request Flow Transparency</strong></p>\n<p>From the application&#39;s perspective, rate limiting enforcement appears as natural network behavior rather than explicit policy enforcement. Applications see either successful request delivery or standard HTTP error responses - they don&#39;t need special handling for rate limiting scenarios.</p>\n<p>The transparency requirement affects several design decisions:</p>\n<ol>\n<li><strong>Response Headers</strong>: Rate limiting information (remaining quota, retry timing) appears in standard HTTP headers that applications can optionally consume</li>\n<li><strong>Error Codes</strong>: Rate limiting uses standard HTTP 429 (Too Many Requests) responses rather than custom error formats</li>\n<li><strong>Latency Impact</strong>: Rate limiting decisions complete within microseconds to avoid noticeable request latency</li>\n<li><strong>Circuit Breaker Integration</strong>: When rate limiting services fail, traffic passes through normally rather than blocking</li>\n</ol>\n<p><strong>Policy Inheritance and Precedence</strong></p>\n<p>Service mesh environments often have complex service hierarchies with multiple applicable policies. The rate limiting system must resolve policy conflicts and inheritance in predictable ways.</p>\n<table>\n<thead>\n<tr>\n<th>Policy Scope</th>\n<th>Precedence Level</th>\n<th>Example Application</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global Mesh</td>\n<td>Lowest</td>\n<td>Default rate limiting for all services</td>\n</tr>\n<tr>\n<td>Namespace</td>\n<td>Medium</td>\n<td>Rate limiting for all services in a namespace</td>\n</tr>\n<tr>\n<td>Service</td>\n<td>High</td>\n<td>Rate limiting specific to one service</td>\n</tr>\n<tr>\n<td>Route</td>\n<td>Highest</td>\n<td>Rate limiting for specific API endpoints</td>\n</tr>\n</tbody></table>\n<p>Higher precedence policies override lower precedence ones, but the system also supports policy composition where multiple policies can apply to the same request. For example, a global policy might set overall limits while a service-specific policy adds burst handling.</p>\n<p><strong>Error Response Customization</strong></p>\n<p>Different services may require different error response formats when rate limiting blocks requests. API services might expect JSON error responses, while web applications might prefer HTML error pages with user-friendly messages.</p>\n<p>The service mesh integration provides configurable error response templates that can adapt to service requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Service Type</th>\n<th>Response Format</th>\n<th>Headers</th>\n<th>Body Content</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>REST API</td>\n<td>JSON</td>\n<td>application/json</td>\n<td>{&quot;error&quot;: &quot;rate_limit_exceeded&quot;, &quot;retry_after&quot;: 60}</td>\n</tr>\n<tr>\n<td>GraphQL</td>\n<td>JSON</td>\n<td>application/json</td>\n<td>{&quot;errors&quot;: [{&quot;message&quot;: &quot;Rate limit exceeded&quot;}]}</td>\n</tr>\n<tr>\n<td>Web Service</td>\n<td>HTML</td>\n<td>text/html</td>\n<td>User-friendly error page with retry guidance</td>\n</tr>\n<tr>\n<td>gRPC</td>\n<td>Status</td>\n<td>application/grpc</td>\n<td>RESOURCE_EXHAUSTED with retry metadata</td>\n</tr>\n</tbody></table>\n<h4 id=\"observability-and-debugging\">Observability and Debugging</h4>\n<p>Service mesh integration introduces additional complexity in observability since rate limiting decisions occur in infrastructure rather than application code. Comprehensive observability becomes critical for debugging policy enforcement and understanding traffic patterns.</p>\n<p><strong>Distributed Tracing Integration</strong></p>\n<p>Rate limiting decisions must integrate with distributed tracing systems (Jaeger, Zipkin) to provide visibility into policy enforcement within request traces. Each rate limiting decision appears as a span within the overall request trace.</p>\n<table>\n<thead>\n<tr>\n<th>Trace Information</th>\n<th>Content</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Span Name</td>\n<td>&quot;rate_limiting_check&quot;</td>\n<td>Identifies rate limiting operations in traces</td>\n</tr>\n<tr>\n<td>Span Tags</td>\n<td>rule_id, algorithm, tier, decision</td>\n<td>Provides context for rate limiting decisions</td>\n</tr>\n<tr>\n<td>Span Logs</td>\n<td>quota_remaining, rule_evaluation_time</td>\n<td>Detailed information for debugging</td>\n</tr>\n<tr>\n<td>Span Status</td>\n<td>OK (allowed) or ERROR (denied)</td>\n<td>Quick visual indication of decision</td>\n</tr>\n</tbody></table>\n<p>The tracing integration helps operators understand why specific requests were rate limited and identify patterns in rate limiting decisions across different services.</p>\n<p><strong>Metrics Aggregation Across Services</strong></p>\n<p>Unlike application-embedded rate limiting where each service has its own metrics, service mesh rate limiting must aggregate metrics across all services while maintaining the ability to drill down into specific service behavior.</p>\n<p>The metrics system provides multiple aggregation levels:</p>\n<ul>\n<li><strong>Mesh-wide</strong>: Total requests, rate limiting decisions, error rates across all services</li>\n<li><strong>Service-level</strong>: Rate limiting metrics per service, including top rate limited endpoints</li>\n<li><strong>Policy-level</strong>: Effectiveness metrics for each rate limiting policy</li>\n<li><strong>Client-level</strong>: Rate limiting behavior per client across all services they access</li>\n</ul>\n<p>These aggregated metrics enable both high-level mesh monitoring and detailed debugging of specific rate limiting issues.</p>\n<h4 id=\"common-pitfalls-in-service-mesh-integration\">Common Pitfalls in Service Mesh Integration</h4>\n<p>⚠️ <strong>Pitfall: Configuration Complexity</strong>\nService mesh rate limiting can quickly become complex with multiple policy layers, inheritance rules, and service-specific configurations. Teams often create contradictory policies or policies that interact in unexpected ways. The solution involves policy validation tools that check for conflicts and comprehensive testing of policy interactions before deployment.</p>\n<p>⚠️ <strong>Pitfall: Performance Impact on Critical Path</strong>\nEvery request must wait for rate limiting decisions, making performance critical. Poorly optimized integrations can add tens of milliseconds to request latency, violating SLA requirements. High-performance integration requires connection pooling, efficient serialization, and aggressive caching of rate limiting decisions.</p>\n<p>⚠️ <strong>Pitfall: Debugging Distributed Policies</strong>\nWhen rate limiting occurs in the service mesh, applications may not have visibility into why requests fail. This makes debugging customer issues extremely difficult. Comprehensive logging, tracing integration, and clear error messages become essential for operational success.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The future extensions represent advanced capabilities that build upon the core distributed rate limiter foundation. Each extension requires careful architectural decisions and implementation strategies to maintain system reliability while adding sophisticated new features.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Area</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Adaptive Control</td>\n<td>Basic PID controller with manual tuning</td>\n<td>Machine learning-based limit prediction with auto-tuning</td>\n</tr>\n<tr>\n<td>Geographic Sync</td>\n<td>Direct Redis replication streams</td>\n<td>Apache Kafka-based event streaming with exactly-once semantics</td>\n</tr>\n<tr>\n<td>Service Mesh</td>\n<td>Envoy HTTP filter with REST API calls</td>\n<td>Native Envoy gRPC integration with connection multiplexing</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Prometheus metrics with periodic scraping</td>\n<td>OpenTelemetry with real-time streaming to observability platforms</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>YAML files with manual deployment</td>\n<td>GitOps workflow with policy validation and gradual rollouts</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<p>Future extensions integrate into the existing codebase through new packages that maintain clear separation of concerns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>distributed-rate-limiter/\n  internal/\n    adaptive/              ← adaptive rate limiting\n      controller.go        ← PID control algorithm\n      metrics.go          ← performance metrics collection\n      safety.go           ← oscillation prevention\n    geographic/           ← multi-region distribution\n      sync.go             ← cross-region synchronization\n      partition.go        ← network partition handling\n      conflict.go         ← conflict resolution\n    servicemesh/          ← service mesh integration\n      envoy/              ← Envoy proxy integration\n        filter.go         ← HTTP filter implementation\n        config.go         ← policy configuration\n      istio/              ← Istio-specific integrations\n        policies.go       ← VirtualService integration\n      observability.go    ← tracing and metrics\n    extensions/           ← shared extension utilities\n      timeutil.go         ← time synchronization utilities\n      configutil.go      ← configuration validation</code></pre></div>\n\n<h4 id=\"adaptive-rate-limiting-starter-code\">Adaptive Rate Limiting Starter Code</h4>\n<p>The adaptive controller provides the foundation for dynamic rate limit adjustment based on system performance metrics.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package adaptive provides dynamic rate limit adjustment based on system performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> adaptive</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/distributed-rate-limiter/internal/config</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/distributed-rate-limiter/internal/metrics</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PerformanceMetrics represents current system performance indicators</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PerformanceMetrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    P50Latency      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"p50_latency\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    P95Latency      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"p95_latency\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    P99Latency      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"p99_latency\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorRate       </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">       `json:\"error_rate\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CPUUtilization  </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">       `json:\"cpu_utilization\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryUsage     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">       `json:\"memory_usage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueueDepth      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"queue_depth\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AdaptiveConfig defines configuration for adaptive rate limiting behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AdaptiveConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled             </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `yaml:\"enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AdjustmentInterval  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"adjustment_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TargetP99Latency   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"target_p99_latency\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TargetErrorRate    </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">       `yaml:\"target_error_rate\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxAdjustmentRate  </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">       `yaml:\"max_adjustment_rate\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProportionalGain   </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">       `yaml:\"proportional_gain\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IntegralGain       </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">       `yaml:\"integral_gain\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DerivativeGain     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">       `yaml:\"derivative_gain\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PIDController implements proportional-integral-derivative control for rate limit adjustment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PIDController</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config          </span><span style=\"color:#B392F0\">AdaptiveConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    previousError   </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    integralError   </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastUpdate      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    adjustmentHistory []</span><span style=\"color:#B392F0\">AdjustmentRecord</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AdjustmentRecord tracks historical rate limit adjustments for oscillation prevention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AdjustmentRecord</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PreviousLimit   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"previous_limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NewLimit        </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"new_limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AdjustmentRatio </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"adjustment_ratio\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TriggerReason   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"trigger_reason\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AdaptiveController manages dynamic rate limit adjustments based on system performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AdaptiveController</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu                </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config            </span><span style=\"color:#B392F0\">AdaptiveConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pidController     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PIDController</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metricsCollector  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">metrics</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Collector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    adjustmentTimer   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Timer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx               </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancelFn          </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewAdaptiveController creates a new adaptive rate limiting controller</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewAdaptiveController</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> AdaptiveConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ruleManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                          metricsCollector</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">metrics</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Collector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AdaptiveController</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize PID controller with configured gains</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set up adjustment timer based on configured interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create cancellable context for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize adjustment history tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins adaptive rate limit adjustment processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ac </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AdaptiveController</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start background goroutine for periodic adjustment evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set up metrics collection integration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize baseline performance measurements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Begin adjustment timer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully shuts down adaptive processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ac </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AdaptiveController</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Cancel background processing context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Stop adjustment timer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Flush any pending adjustments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EvaluateAdjustments analyzes current performance and determines necessary rate limit changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ac </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AdaptiveController</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EvaluateAdjustments</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Collect current performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compare against target thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Calculate PID controller output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Apply safety bounds and oscillation prevention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Update rate limit rules if adjustment needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Record adjustment in history</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CalculatePIDOutput computes rate limit adjustment using PID control algorithm</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PIDController</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CalculatePIDOutput</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">current</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">target</span><span style=\"color:#B392F0\"> PerformanceMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Calculate error between current and target performance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compute proportional term based on current error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Update and compute integral term based on accumulated error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compute derivative term based on error rate of change</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Combine P, I, and D terms with configured gains</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Apply output limits to prevent excessive adjustments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"geographic-distribution-starter-code\">Geographic Distribution Starter Code</h4>\n<p>The geographic synchronization system enables rate limit state sharing across multiple regions with eventual consistency guarantees.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package geographic provides multi-region rate limiting with eventual consistency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> geographic</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/go-redis/redis/v8</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/distributed-rate-limiter/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegionConfig defines configuration for a geographic region</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RegionConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RegionID        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `yaml:\"region_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RedisAddresses  []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"redis_addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SyncInterval    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"sync_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PeerRegions     []</span><span style=\"color:#B392F0\">PeerRegion</span><span style=\"color:#9ECBFF\"> `yaml:\"peer_regions\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SyncTimeout     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"sync_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConflictResolution </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"conflict_resolution\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PeerRegion defines connection information for remote regions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PeerRegion</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RegionID    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"region_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SyncAddress </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"sync_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Priority    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">    `yaml:\"priority\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UsageReport represents rate limit consumption data for cross-region synchronization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UsageReport</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RegionID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                    `json:\"region_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">                 `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WindowStart   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">                 `json:\"window_start\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WindowEnd     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">                 `json:\"window_end\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    KeyUsage      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">          `json:\"key_usage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Sequence      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                     `json:\"sequence\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ConflictReport indicates detected inconsistencies in global rate limit state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ConflictReport</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                    `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GlobalLimit   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                     `json:\"global_limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReportedUsage </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">          `json:\"reported_usage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalUsage    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                     `json:\"total_usage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Excess        </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                     `json:\"excess\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Resolution    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                    `json:\"resolution\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">                 `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GeographicSyncManager coordinates rate limiting state across multiple regions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GeographicSyncManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config          </span><span style=\"color:#B392F0\">RegionConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localStorage    </span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    peerConnections </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    usageBuffer     []</span><span style=\"color:#B392F0\">UsageReport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conflictResolver </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConflictResolver</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    syncTicker      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Ticker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx             </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancelFn        </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ConflictResolver handles inconsistencies in global rate limit state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ConflictResolver</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resolutionStrategy </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priorityMap     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conflictHistory []</span><span style=\"color:#B392F0\">ConflictReport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewGeographicSyncManager creates a new geographic synchronization manager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewGeographicSyncManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> RegionConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">localStorage</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Storage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GeographicSyncManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize peer region connections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create conflict resolver with configured strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set up usage report buffering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize sync ticker with configured interval</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins cross-region synchronization processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">gsm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GeographicSyncManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Establish connections to all peer regions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start background sync processing goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Begin periodic usage report generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start conflict detection and resolution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SynchronizeUsage sends local usage reports to peer regions and processes incoming reports</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">gsm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GeographicSyncManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SynchronizeUsage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Generate usage report from local Redis state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Send usage report to all peer regions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Receive and process usage reports from peers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Detect conflicts between reported usage and global limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Trigger conflict resolution if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessPeerReport incorporates usage information from a remote region</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">gsm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GeographicSyncManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessPeerReport</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">report</span><span style=\"color:#B392F0\"> UsageReport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate report authenticity and sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Merge remote usage into local global state view</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check for conflicts with global rate limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Update local enforcement based on global usage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DetectConflicts identifies cases where global usage exceeds configured limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConflictResolver</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DetectConflicts</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">reports</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">UsageReport</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                          globalLimits</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">ConflictReport</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Sum reported usage across all regions for each key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compare total usage against configured global limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Identify keys where usage exceeds limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Calculate excess amount and affected regions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ResolveConflict applies conflict resolution strategy to restore global consistency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConflictResolver</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ResolveConflict</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">conflict</span><span style=\"color:#B392F0\"> ConflictReport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Apply configured resolution strategy (priority-based, proportional reduction, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Calculate corrective actions for each region</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Send adjustment commands to affected regions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Record conflict resolution in history</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"service-mesh-integration-starter-code\">Service Mesh Integration Starter Code</h4>\n<p>The service mesh integration provides transparent rate limiting through Envoy proxy filters and Istio policy integration.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package servicemesh provides transparent rate limiting through service mesh integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> servicemesh</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    envoy_service_ratelimit_v3 </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#B392F0\">github.com/envoyproxy/go-control-plane/envoy/service/ratelimit/v3</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">google.golang.org/grpc</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/distributed-rate-limiter/internal/ratelimit</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EnvoyRateLimitService implements Envoy's RateLimitService interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> EnvoyRateLimitService</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limiter           </span><span style=\"color:#B392F0\">ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DistributedLimiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    contextExtractor  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RequestContextExtractor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    policyManager     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PolicyManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metricsCollector  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ServiceMeshMetrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RequestContextExtractor extracts rate limiting context from Envoy request metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RequestContextExtractor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    extractionRules   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">ExtractionRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    defaultContext    </span><span style=\"color:#B392F0\">DefaultContextConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExtractionRule defines how to extract rate limiting dimensions from requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ExtractionRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Dimension     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"dimension\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Source        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"source\"`</span><span style=\"color:#6A737D\">        // header, path, query, metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"key\"`</span><span style=\"color:#6A737D\">           // header name, path pattern, etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Transformer   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"transformer\"`</span><span style=\"color:#6A737D\">   // regex, hash, lookup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DefaultValue  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"default_value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PolicyManager handles service mesh rate limiting policy configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PolicyManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    policies        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ServiceMeshPolicy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    istioClient     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">istio</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    configWatcher   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConfigWatcher</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ServiceMeshPolicy represents rate limiting policy in service mesh context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ServiceMeshPolicy</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name            </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                    `yaml:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Namespace       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                    `yaml:\"namespace\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Services        []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                  `yaml:\"services\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Routes          []</span><span style=\"color:#B392F0\">RouteMatch</span><span style=\"color:#9ECBFF\">              `yaml:\"routes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RateLimiting    </span><span style=\"color:#B392F0\">RateLimitingSpec</span><span style=\"color:#9ECBFF\">          `yaml:\"rate_limiting\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Priority        </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">                       `yaml:\"priority\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled         </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">                      `yaml:\"enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RouteMatch defines traffic matching criteria for policy application</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RouteMatch</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PathRegex       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"path_regex\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Methods         []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `yaml:\"methods\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Headers         </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"headers\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryParams     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"query_params\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitingSpec defines rate limiting configuration within service mesh policy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitingSpec</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                    `yaml:\"algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Limit           </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                     `yaml:\"limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window          </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">             `yaml:\"window\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Dimensions      []</span><span style=\"color:#B392F0\">RateLimitDimension</span><span style=\"color:#9ECBFF\">      `yaml:\"dimensions\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorResponse   </span><span style=\"color:#B392F0\">ErrorResponseConfig</span><span style=\"color:#9ECBFF\">       `yaml:\"error_response\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateLimitDimension defines a dimension for rate limiting (user, IP, service, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateLimitDimension</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name            </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Extractor       </span><span style=\"color:#B392F0\">ExtractionRule</span><span style=\"color:#9ECBFF\">    `yaml:\"extractor\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tier            </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"tier\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ErrorResponseConfig customizes error responses when rate limits are exceeded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ErrorResponseConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StatusCode      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">               `yaml:\"status_code\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ContentType     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"content_type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Body            </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"body\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Headers         </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"headers\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewEnvoyRateLimitService creates a new Envoy rate limiting service</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewEnvoyRateLimitService</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">limiter</span><span style=\"color:#B392F0\"> ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EnvoyRateLimitService</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize request context extractor with default rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create policy manager with Istio client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set up service mesh metrics collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ShouldRateLimit implements Envoy's rate limiting service interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">erls </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EnvoyRateLimitService</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ShouldRateLimit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                                  req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">envoy_service_ratelimit_v3</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                                                  *</span><span style=\"color:#B392F0\">envoy_service_ratelimit_v3</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Extract request context from Envoy metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Find matching service mesh policies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Evaluate rate limiting rules for matched policies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Convert rate limiting result to Envoy response format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Include appropriate response headers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExtractRequestContext converts Envoy request metadata to rate limiting context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rce </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RequestContextExtractor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExtractRequestContext</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">envoy_service_ratelimit_v3</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RequestContext</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Iterate through extraction rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Extract values from request headers, path, query parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Apply transformers (regex, hashing, lookups)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Build RequestContext with extracted dimensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FindMatchingPolicies identifies service mesh policies that apply to the current request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PolicyManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">FindMatchingPolicies</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                             reqCtx</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RequestContext</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ServiceMeshPolicy</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Match request against service selectors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Match request against route patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Apply policy precedence rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return ordered list of applicable policies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ConvertToEnvoyResponse transforms rate limiting result into Envoy response format</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ConvertToEnvoyResponse</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">FlowResult</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                          errorConfig</span><span style=\"color:#B392F0\"> ErrorResponseConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">envoy_service_ratelimit_v3</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimitResponse</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set response status based on rate limiting decision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add rate limiting headers (X-RateLimit-Limit, X-RateLimit-Remaining)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Configure error response if rate limit exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Include retry-after header with appropriate timing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Adaptive Rate Limiting Checkpoint:</strong>\nAfter implementing adaptive rate limiting, run load tests with varying traffic patterns. You should observe rate limits automatically adjusting based on system performance. Monitor the adjustment history to ensure the system doesn&#39;t oscillate - limits should converge to stable values that maintain target performance levels.</p>\n<p><strong>Geographic Distribution Checkpoint:</strong>\nDeploy the rate limiter across two simulated regions with controlled network latency between them. Generate traffic in both regions and verify that global rate limits are eventually enforced even when regional limits are exceeded. Test network partition scenarios by blocking cross-region communication and confirm that each region continues operating with conservative local enforcement.</p>\n<p><strong>Service Mesh Integration Checkpoint:</strong>\nDeploy the rate limiter as an Envoy filter in a test service mesh. Configure rate limiting policies through Istio VirtualService resources and verify that applications receive rate limiting transparently without code changes. Check that rate limiting decisions appear correctly in distributed traces and that error responses match configured formats.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this glossary provides definitions for technical terms used across rate limiting algorithms, multi-tier evaluation, Redis backend integration, sharding, and API design throughout the distributed rate limiting system</p>\n</blockquote>\n<p>The distributed rate limiting system introduces numerous technical concepts spanning algorithm design, distributed systems, data storage, and operational management. This glossary provides comprehensive definitions of all terms, acronyms, data structures, and domain concepts used throughout the design document. Understanding these definitions is crucial for implementing, operating, and extending the rate limiting system effectively.</p>\n<h3 id=\"mental-model-the-technical-reference-library\">Mental Model: The Technical Reference Library</h3>\n<p>Think of this glossary as the technical reference library for our distributed rate limiting &quot;city.&quot; Just as a city has specialized terminology for its infrastructure (traffic signals, utility grids, zoning laws), our distributed system has its own vocabulary. Each term represents a specific concept, component, or operation that engineers use to communicate precisely about the system&#39;s behavior. Like a reference library, this glossary organizes knowledge so that anyone working with the system can quickly understand unfamiliar concepts and use consistent terminology when discussing designs, implementing features, or troubleshooting issues.</p>\n<h3 id=\"core-system-concepts\">Core System Concepts</h3>\n<p><strong>Distributed Rate Limiting</strong>: The coordination of request quota enforcement across multiple application instances using shared state storage. Unlike local rate limiting where each instance operates independently, distributed rate limiting ensures that the combined traffic from all instances respects global quotas. This requires atomic operations, clock synchronization, and graceful degradation strategies.</p>\n<p><strong>Local Fallback</strong>: The system&#39;s ability to switch to per-instance rate limiting when shared storage becomes unavailable. During fallback mode, each application instance enforces rate limits using only local state, accepting that global quotas may be exceeded but maintaining basic protection. The system automatically returns to distributed mode when shared storage connectivity is restored.</p>\n<p><strong>Graceful Degradation</strong>: The system&#39;s capacity to maintain reduced functionality during component failures rather than completely failing. For rate limiting, this means continuing to provide protection (albeit with reduced accuracy) when Redis is unavailable, configuration updates fail, or individual nodes become unreachable.</p>\n<p><strong>Atomic Operations</strong>: Indivisible check-and-update operations that prevent race conditions in distributed rate limiting. These operations ensure that reading a counter, comparing it to a limit, and incrementing it (if allowed) happen as a single unit, preventing the double-spending problem where multiple instances might simultaneously approve requests that should collectively exceed the limit.</p>\n<p><strong>Circuit Breaker</strong>: A failure detection and prevention mechanism that protects against cascading failures by monitoring error rates and temporarily blocking operations to failing components. In rate limiting, circuit breakers prevent the system from repeatedly attempting Redis operations that are likely to fail, reducing latency and resource consumption during outages.</p>\n<h3 id=\"rate-limiting-algorithm-terms\">Rate Limiting Algorithm Terms</h3>\n<p><strong>Token Bucket</strong>: A rate limiting algorithm that allows controlled bursts above sustained rates by maintaining a bucket of tokens that refill at a steady rate. Requests consume tokens from the bucket; when the bucket is empty, additional requests are denied until tokens are replenished. The bucket capacity determines the maximum burst size, while the refill rate controls the sustained throughput.</p>\n<p><strong>Sliding Window Counter</strong>: A memory-efficient approximate rate limiting algorithm that divides time into fixed buckets and weights recent buckets more heavily than older ones. This approach provides good accuracy while using constant memory per rate limit key, making it suitable for high-scale deployments where memory usage must be bounded.</p>\n<p><strong>Sliding Window Log</strong>: A precise rate limiting algorithm that stores individual request timestamps to make exact decisions about whether new requests should be allowed. While providing perfect accuracy, this approach requires O(n) memory per key where n is the number of requests in the time window, making it suitable for scenarios where precision is more important than memory efficiency.</p>\n<p><strong>Burst Handling</strong>: The system&#39;s capability to allow short traffic spikes above sustained rate limits without rejecting requests. Burst handling recognizes that real-world traffic is often bursty rather than perfectly uniform, and well-behaved clients should be able to send requests in bursts as long as their long-term average respects the configured limits.</p>\n<p><strong>Refill Rate</strong>: The steady rate at which tokens are added to a token bucket, typically expressed as tokens per second. The refill rate determines the sustained throughput allowed by the rate limiter - over long periods, the average request rate cannot exceed the refill rate regardless of the bucket capacity.</p>\n<p><strong>Boundary Condition</strong>: Edge cases where rate limiting algorithms face their greatest challenges, particularly during time window transitions. For example, a sliding window counter must carefully handle the transition from one time bucket to the next to avoid losing track of requests or allowing double the configured limit during boundary periods.</p>\n<h3 id=\"multi-tier-rate-limiting-concepts\">Multi-Tier Rate Limiting Concepts</h3>\n<p><strong>Multi-Tier Rate Limiting</strong>: A hierarchical system of rate limits that enforces quotas across multiple dimensions simultaneously, such as per-user, per-IP, per-API, and global limits. Each tier represents a different scope of protection, and requests must pass all applicable tiers to be allowed.</p>\n<p><strong>Tier Precedence Hierarchy</strong>: The ordered evaluation system for multiple rate limit tiers, typically proceeding from most specific (per-user) to most general (global). The hierarchy determines which limits are checked first and how conflicts between tiers are resolved.</p>\n<p><strong>Short-Circuit Evaluation</strong>: An optimization strategy that stops tier evaluation immediately when any rate limit is exceeded, avoiding unnecessary computation and reducing latency. Once a request is denied by one tier, there is no need to check remaining tiers since the request will be rejected regardless.</p>\n<p><strong>Rule Pattern Matching</strong>: The process of identifying which rate limit rules apply to a specific request based on the request&#39;s context (user ID, IP address, API endpoint, etc.). Pattern matching uses regular expressions or exact string matching to determine which configured rules should be evaluated.</p>\n<p><strong>Key Composition</strong>: The systematic construction of Redis keys for rate limit storage by combining rule patterns with request context. Proper key composition ensures that rate limits are applied to the correct scope (e.g., &quot;user:12345:api:/orders&quot; for a per-user rate limit on the orders endpoint).</p>\n<h3 id=\"distributed-systems-and-consistency\">Distributed Systems and Consistency</h3>\n<p><strong>Consistent Hashing</strong>: A distributed data placement strategy that minimizes data movement when nodes are added or removed from a cluster. In rate limiting, consistent hashing ensures that rate limit keys are distributed evenly across Redis nodes and that most keys remain on the same node during topology changes.</p>\n<p><strong>Virtual Nodes</strong>: Multiple hash positions assigned to each physical node in a consistent hash ring to improve load balancing. Virtual nodes ensure that when a physical node fails, its load is distributed across multiple remaining nodes rather than overwhelming a single neighbor.</p>\n<p><strong>Hash Ring</strong>: The circular hash space used in consistent hashing where keys and nodes are placed based on their hash values. The ring topology ensures that each key is assigned to the first node found by walking clockwise from the key&#39;s position.</p>\n<p><strong>Hot Key Detection</strong>: The process of identifying rate limit keys that receive disproportionately high access compared to others. Hot keys can overwhelm individual Redis nodes and require special handling such as replication or redistribution to maintain system performance.</p>\n<p><strong>Split-Brain Prevention</strong>: Strategies to ensure consistent cluster state during network partitions that might divide the cluster into multiple segments. Split-brain scenarios can lead to inconsistent rate limit decisions if different cluster segments allow requests that should collectively exceed limits.</p>\n<p><strong>Clock Skew</strong>: Time differences between distributed system nodes that can affect time-based rate limiting algorithms. Clock skew can cause requests to be incorrectly categorized in time windows or lead to inconsistent token bucket refill timing across different application instances.</p>\n<h3 id=\"redis-and-storage-concepts\">Redis and Storage Concepts</h3>\n<p><strong>Connection Pooling</strong>: The practice of maintaining a reusable set of Redis connections to improve performance and resource utilization. Connection pools reduce the overhead of establishing new connections for each operation while limiting the total number of connections to prevent overwhelming Redis servers.</p>\n<p><strong>Lua Scripting</strong>: Redis&#39;s capability to execute Lua scripts atomically on the server side, ensuring that complex operations involving multiple Redis commands are performed without interruption. In rate limiting, Lua scripts implement atomic check-and-update operations that prevent race conditions.</p>\n<p><strong>Failover</strong>: The automatic redirection of traffic from failed Redis nodes to healthy replicas or alternative nodes. Failover mechanisms ensure that rate limiting continues to function even when individual Redis instances become unavailable due to hardware failures or network issues.</p>\n<p><strong>Rebalancing</strong>: The process of redistributing rate limit state across Redis nodes to maintain optimal performance and load distribution. Rebalancing occurs when nodes are added or removed from the cluster or when hot key detection indicates uneven load distribution.</p>\n<p><strong>Configuration Propagation</strong>: The distribution of rate limit rule changes from the management API to all application instances. Propagation ensures that all instances use consistent rate limiting rules without requiring service restarts or manual updates.</p>\n<h3 id=\"testing-and-quality-assurance\">Testing and Quality Assurance</h3>\n<p><strong>Algorithm Unit Testing</strong>: Isolated testing of individual rate limiting algorithms to verify correctness and boundary condition handling. Unit tests validate that algorithms behave correctly under various scenarios including burst traffic, time window boundaries, and edge cases like exactly hitting rate limits.</p>\n<p><strong>Distributed Integration Testing</strong>: Multi-instance testing scenarios that verify cluster-wide rate limiting behavior works correctly across multiple application instances and Redis nodes. These tests validate that distributed coordination functions properly and that race conditions are prevented.</p>\n<p><strong>Milestone Verification Checkpoints</strong>: Quality gates that confirm implementation meets acceptance criteria before advancing to subsequent development phases. Each checkpoint includes specific tests to run and expected behaviors to verify, ensuring systematic progress toward project completion.</p>\n<p><strong>Chaos and Failure Testing</strong>: Systematic failure injection to validate recovery mechanisms and identify system weaknesses. Chaos testing includes simulating Redis failures, network partitions, clock skew, and other adverse conditions to verify that the system continues operating correctly.</p>\n<p><strong>Load Pattern</strong>: Synthetic traffic generation strategies for testing system behavior under different types of load. Patterns include constant load (steady rate), burst load (periodic spikes), gradual ramp (increasing rate), spike load (sudden increases), and random load (variable timing).</p>\n<p><strong>Network Partition Simulation</strong>: Controlled isolation of system components to test split-brain scenarios and validate that the system maintains consistency during network failures. Partition testing ensures that conflicting rate limit decisions don&#39;t occur when cluster segments can&#39;t communicate.</p>\n<h3 id=\"monitoring-and-operations\">Monitoring and Operations</h3>\n<p><strong>Correlation IDs</strong>: Unique identifiers that follow requests through distributed system components, enabling tracing of request flow across multiple services and facilitating debugging of complex interactions. Correlation IDs help operators understand how rate limiting decisions relate to specific user requests.</p>\n<p><strong>Structured Logging</strong>: Logging practices that use consistent formats and fields to enable automated analysis and monitoring. Structured logs facilitate debugging by providing machine-readable information about rate limiting decisions, Redis operations, and system performance.</p>\n<p><strong>Distributed Tracing</strong>: The practice of tracking request flow across multiple system components to understand performance characteristics and identify bottlenecks. Tracing helps operators understand the latency contribution of different rate limiting operations.</p>\n<p><strong>Symptom-Based Diagnosis</strong>: A troubleshooting methodology that maps observable problems to likely root causes through systematic analysis. This approach helps operators quickly identify the source of rate limiting issues based on symptoms like high latency, incorrect decisions, or system errors.</p>\n<p><strong>Metrics Aggregation</strong>: The collection and combination of usage statistics from multiple application instances for centralized monitoring and alerting. Aggregation provides global visibility into rate limiting effectiveness and system health.</p>\n<h3 id=\"system-architecture-components\">System Architecture Components</h3>\n<p>This section defines the key data structures and interfaces that comprise the distributed rate limiting system, organized by their primary responsibilities and relationships.</p>\n<h4 id=\"core-rate-limiting-types\">Core Rate Limiting Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RateLimitRule</code></td>\n<td>Defines rate limiting policies</td>\n<td>Contains pattern matching, algorithm selection, and limit configuration</td>\n</tr>\n<tr>\n<td><code>RateLimitRequest</code></td>\n<td>Represents incoming requests for rate limit evaluation</td>\n<td>Encapsulates user context, IP address, API endpoint, and token requirements</td>\n</tr>\n<tr>\n<td><code>RateLimitResult</code></td>\n<td>Provides rate limiting decision and metadata</td>\n<td>Includes allow/deny decision, remaining quota, and retry timing information</td>\n</tr>\n<tr>\n<td><code>RequestContext</code></td>\n<td>Extended request information with correlation data</td>\n<td>Adds correlation IDs, headers, and timestamp for distributed tracing</td>\n</tr>\n</tbody></table>\n<p>The <code>RateLimitRule</code> structure serves as the foundation for policy definition, containing fields for pattern matching (<code>key_pattern</code>), algorithm selection (<code>algorithm</code>), limit values (<code>limit</code>, <code>burst_limit</code>), time windows (<code>window</code>), and operational controls (<code>enabled</code>, <code>priority</code>). The priority field enables conflict resolution when multiple rules match the same request.</p>\n<p>The <code>RateLimitRequest</code> captures the essential information needed to make rate limiting decisions, including user identification (<code>user_id</code>), network identification (<code>ip_address</code>), resource identification (<code>api_endpoint</code>), client identification (<code>user_agent</code>), and resource consumption (<code>tokens</code>). This structure provides the input for key composition and rule matching.</p>\n<p>The <code>RateLimitResult</code> communicates rate limiting decisions back to applications, indicating whether the request is allowed (<code>allowed</code>), how much quota remains (<code>remaining</code>), when the client should retry if denied (<code>retry_after</code>), when limits reset (<code>reset_time</code>), which rule caused the decision (<code>rule_id</code>), and which algorithm was used (<code>algorithm</code>).</p>\n<h4 id=\"algorithm-implementation-types\">Algorithm Implementation Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TokenBucketConfig</code></td>\n<td>Configuration for token bucket algorithm</td>\n<td>Specifies capacity, refill rate, and time window</td>\n</tr>\n<tr>\n<td><code>TokenBucketState</code></td>\n<td>Runtime state for token bucket instances</td>\n<td>Tracks current tokens, last refill time, and configuration</td>\n</tr>\n<tr>\n<td><code>SlidingWindowConfig</code></td>\n<td>Configuration for sliding window algorithms</td>\n<td>Defines limit, window size, and bucket count for approximation</td>\n</tr>\n<tr>\n<td><code>SlidingWindowCounter</code></td>\n<td>Implementation of sliding window counter algorithm</td>\n<td>Provides memory-efficient approximate rate limiting</td>\n</tr>\n<tr>\n<td><code>SlidingWindowLog</code></td>\n<td>Implementation of sliding window log algorithm</td>\n<td>Provides precise rate limiting with timestamp tracking</td>\n</tr>\n</tbody></table>\n<p>The token bucket implementation uses <code>TokenBucketConfig</code> to define the bucket capacity (maximum burst), refill rate (sustained throughput), and time window for rate calculations. The <code>TokenBucketState</code> maintains runtime information including current token count, last refill timestamp, and configuration references for atomic updates.</p>\n<p>Sliding window algorithms share a common configuration structure (<code>SlidingWindowConfig</code>) but differ in their implementation approaches. The counter variant approximates sliding windows using fixed time buckets with weighted averaging, while the log variant maintains precise timestamps for exact calculations.</p>\n<h4 id=\"multi-tier-evaluation-types\">Multi-Tier Evaluation Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MultiTierLimiter</code></td>\n<td>Coordinates evaluation across multiple rate limit tiers</td>\n<td>Manages rule matching, tier ordering, and short-circuit evaluation</td>\n</tr>\n<tr>\n<td><code>TierEvaluation</code></td>\n<td>Records results from individual tier evaluation</td>\n<td>Captures rule ID, tier name, result, duration, and algorithm used</td>\n</tr>\n<tr>\n<td><code>FlowResult</code></td>\n<td>Comprehensive result from multi-tier evaluation</td>\n<td>Aggregates tier results, identifies blocking tier, and provides response headers</td>\n</tr>\n<tr>\n<td><code>KeyComposer</code></td>\n<td>Generates Redis keys from rule patterns and request context</td>\n<td>Maintains pattern cache and handles key generation for different tiers</td>\n</tr>\n</tbody></table>\n<p>The <code>MultiTierLimiter</code> orchestrates the evaluation of multiple rate limiting rules against a single request. It maintains references to storage backends, rule management, key composition, algorithm implementations, and fallback mechanisms. The design enables efficient evaluation through rule pre-filtering and short-circuit logic.</p>\n<p><code>TierEvaluation</code> structures capture detailed information about each tier&#39;s evaluation, enabling detailed logging, metrics collection, and debugging. The <code>FlowResult</code> aggregates these individual evaluations into a comprehensive response that applications can use for decision-making and client communication.</p>\n<h4 id=\"distributed-storage-types\">Distributed Storage Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RedisStorage</code></td>\n<td>Redis backend implementation for distributed state</td>\n<td>Manages connections, executes Lua scripts, handles failover</td>\n</tr>\n<tr>\n<td><code>RedisConfig</code></td>\n<td>Configuration for Redis connections and behavior</td>\n<td>Specifies addresses, authentication, timeouts, and pool settings</td>\n</tr>\n<tr>\n<td><code>DistributedLimiter</code></td>\n<td>Main entry point for distributed rate limiting</td>\n<td>Coordinates storage, rule management, and fallback strategies</td>\n</tr>\n<tr>\n<td><code>FallbackLimiter</code></td>\n<td>Local-only rate limiting for degraded operation</td>\n<td>Maintains local state when Redis is unavailable</td>\n</tr>\n</tbody></table>\n<p>The <code>RedisStorage</code> type encapsulates all Redis interactions, providing a clean interface for atomic operations while handling connection management, script execution, and error recovery. The design isolates Redis-specific concerns from algorithm logic, enabling easier testing and potential backend substitution.</p>\n<p><code>DistributedLimiter</code> serves as the primary interface for applications, coordinating between Redis storage, rule management, and local fallback. This design ensures that applications have a consistent interface regardless of whether the system is operating in distributed or fallback mode.</p>\n<h4 id=\"consistent-hashing-and-clustering-types\">Consistent Hashing and Clustering Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HashRing</code></td>\n<td>Implements consistent hashing for key distribution</td>\n<td>Manages virtual nodes, handles topology changes, minimizes redistribution</td>\n</tr>\n<tr>\n<td><code>CircuitBreaker</code></td>\n<td>Prevents cascading failures through intelligent failure detection</td>\n<td>Tracks failure rates, implements state machine, provides fallback paths</td>\n</tr>\n<tr>\n<td><code>HealthChecker</code></td>\n<td>Monitors Redis node health and coordinates responses</td>\n<td>Performs health checks, manages circuit breakers, handles failover</td>\n</tr>\n<tr>\n<td><code>HotKeyDetector</code></td>\n<td>Identifies disproportionately accessed rate limit keys</td>\n<td>Analyzes access patterns, triggers replication, enables load balancing</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/rate-limiter-distributed/architecture-doc/asset?path=diagrams%2Fredis-cluster-topology.svg\" alt=\"Redis Cluster and Consistent Hashing\"></p>\n<p>The <code>HashRing</code> implementation uses virtual nodes to ensure even distribution of rate limit keys across Redis instances. The virtual node approach prevents hotspots that could occur if keys were distributed only based on physical nodes, especially in small clusters.</p>\n<p><code>CircuitBreaker</code> provides protection against cascading failures by implementing a state machine with closed (normal operation), open (all requests rejected), and half-open (limited test requests allowed) states. This pattern prevents the system from overwhelming failing components while allowing for automatic recovery.</p>\n<h4 id=\"configuration-and-management-types\">Configuration and Management Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RuleManager</code></td>\n<td>Manages rate limit rule lifecycle</td>\n<td>Handles rule loading, validation, caching, and change notification</td>\n</tr>\n<tr>\n<td><code>ConfigurationWatcher</code></td>\n<td>Monitors for configuration changes</td>\n<td>Subscribes to Redis updates, maintains version consistency</td>\n</tr>\n<tr>\n<td><code>FlowCoordinator</code></td>\n<td>Orchestrates complete request processing</td>\n<td>Coordinates all components for end-to-end request handling</td>\n</tr>\n<tr>\n<td><code>MetricsCollector</code></td>\n<td>Aggregates usage statistics and performance data</td>\n<td>Collects metrics, detects patterns, provides monitoring data</td>\n</tr>\n</tbody></table>\n<p>The <code>RuleManager</code> centralizes all rule-related operations, providing interfaces for loading rules from configuration files, validating rule syntax and logic, maintaining rule caches for performance, and notifying other components of rule changes.</p>\n<p><code>ConfigurationWatcher</code> implements the observer pattern for configuration changes, using Redis pub/sub mechanisms to receive notifications when rules are updated. This design ensures that all application instances receive configuration updates promptly without requiring polling.</p>\n<h4 id=\"time-and-synchronization-types\">Time and Synchronization Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TimeProvider</code></td>\n<td>Abstracts time operations for testing and synchronization</td>\n<td>Provides consistent time across distributed components</td>\n</tr>\n<tr>\n<td><code>MockTimeProvider</code></td>\n<td>Controllable time provider for testing</td>\n<td>Enables deterministic testing of time-based algorithms</td>\n</tr>\n<tr>\n<td><code>CorrelationContext</code></td>\n<td>Provides request correlation for distributed tracing</td>\n<td>Maintains correlation IDs and metadata across component boundaries</td>\n</tr>\n<tr>\n<td><code>CorrelationLogger</code></td>\n<td>Structured logging with correlation information</td>\n<td>Enables efficient debugging and request tracing</td>\n</tr>\n</tbody></table>\n<p>The <code>TimeProvider</code> abstraction enables consistent time handling across the distributed system while supporting testing scenarios. The interface provides methods for measuring clock skew relative to Redis servers and adjusting local timestamps accordingly.</p>\n<p><code>CorrelationContext</code> facilitates debugging and monitoring by providing unique identifiers that follow requests through the system. This context includes trace IDs, request IDs, user IDs, session IDs, and arbitrary metadata that helps operators understand request flow.</p>\n<h4 id=\"testing-and-quality-assurance-types\">Testing and Quality Assurance Types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RedisTestHelper</code></td>\n<td>Utilities for Redis testing scenarios</td>\n<td>Manages embedded Redis, clusters, network simulation</td>\n</tr>\n<tr>\n<td><code>LoadGenerator</code></td>\n<td>Generates synthetic traffic for performance testing</td>\n<td>Supports various load patterns and measures system response</td>\n</tr>\n<tr>\n<td><code>LoadTestResults</code></td>\n<td>Aggregates load testing metrics and analysis</td>\n<td>Captures latency, throughput, and accuracy measurements</td>\n</tr>\n<tr>\n<td><code>LoadGeneratorConfig</code></td>\n<td>Configuration for load testing scenarios</td>\n<td>Defines traffic patterns, duration, and client behavior</td>\n</tr>\n</tbody></table>\n<p>The <code>RedisTestHelper</code> provides utilities for creating test environments including embedded Redis instances, Redis clusters, and network partition simulation. This tooling enables comprehensive testing without requiring external Redis infrastructure.</p>\n<p><code>LoadGenerator</code> supports multiple traffic patterns including constant load (steady request rate), burst load (periodic spikes), gradual ramp (linearly increasing rate), spike load (sudden increases), and random load (variable timing). Each pattern tests different aspects of the rate limiting system&#39;s behavior.</p>\n<h3 id=\"algorithm-specific-constants\">Algorithm-Specific Constants</h3>\n<p>The system defines several algorithm identifiers and configuration constants that standardize behavior across implementations:</p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Value</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ALGORITHM_TOKEN_BUCKET</code></td>\n<td>&quot;token_bucket&quot;</td>\n<td>Identifier for token bucket algorithm selection</td>\n</tr>\n<tr>\n<td><code>ALGORITHM_SLIDING_WINDOW_LOG</code></td>\n<td>&quot;sliding_window_log&quot;</td>\n<td>Identifier for precise sliding window algorithm</td>\n</tr>\n<tr>\n<td><code>ALGORITHM_SLIDING_COUNTER</code></td>\n<td>&quot;sliding_window_counter&quot;</td>\n<td>Identifier for approximate sliding window algorithm</td>\n</tr>\n<tr>\n<td><code>DEFAULT_POOL_SIZE</code></td>\n<td>10</td>\n<td>Default number of Redis connections per pool</td>\n</tr>\n<tr>\n<td><code>DEFAULT_TIMEOUT</code></td>\n<td>5ms</td>\n<td>Default timeout for Redis operations</td>\n</tr>\n</tbody></table>\n<p>Priority constants enable rule precedence configuration:</p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Value</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PRIORITY_HIGH</code></td>\n<td>100</td>\n<td>High priority for critical rate limiting rules</td>\n</tr>\n<tr>\n<td><code>PRIORITY_LOW</code></td>\n<td>1</td>\n<td>Low priority for general rate limiting rules</td>\n</tr>\n</tbody></table>\n<p>Circuit breaker states manage failure handling:</p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Purpose</th>\n<th>Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CircuitClosed</code></td>\n<td>Normal operation state</td>\n<td>All requests are processed normally</td>\n</tr>\n<tr>\n<td><code>CircuitOpen</code></td>\n<td>All requests rejected state</td>\n<td>Requests fail fast without attempting operations</td>\n</tr>\n<tr>\n<td><code>CircuitHalfOpen</code></td>\n<td>Limited test requests allowed state</td>\n<td>Some requests are tested to determine if service has recovered</td>\n</tr>\n</tbody></table>\n<h3 id=\"interface-definitions\">Interface Definitions</h3>\n<p>The system defines several key interfaces that enable modularity and testing. These interfaces abstract the essential operations while allowing for multiple implementations.</p>\n<h4 id=\"core-rate-limiting-interfaces\">Core Rate Limiting Interfaces</h4>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Check</code></td>\n<td><code>ctx context.Context, req RateLimitRequest</code></td>\n<td><code>*RateLimitResult, error</code></td>\n<td>Performs rate limit check and updates counters atomically</td>\n</tr>\n<tr>\n<td><code>Preview</code></td>\n<td><code>ctx context.Context, req RateLimitRequest</code></td>\n<td><code>*RateLimitResult, error</code></td>\n<td>Checks rate limit status without updating counters</td>\n</tr>\n<tr>\n<td><code>Reset</code></td>\n<td><code>ctx context.Context, req RateLimitRequest</code></td>\n<td><code>error</code></td>\n<td>Clears rate limit counters for the specified request context</td>\n</tr>\n</tbody></table>\n<p>The <code>Check</code> method represents the primary rate limiting operation, atomically evaluating whether a request should be allowed and updating counters if so. The <code>Preview</code> method enables applications to check rate limit status without consuming quota, useful for displaying remaining limits to users.</p>\n<h4 id=\"storage-backend-interfaces\">Storage Backend Interfaces</h4>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CheckAndUpdate</code></td>\n<td><code>ctx context.Context, key string, limit int64, window time.Duration</code></td>\n<td><code>bool, int64, time.Time, error</code></td>\n<td>Atomically checks and updates rate limit counters</td>\n</tr>\n<tr>\n<td><code>ExecuteLua</code></td>\n<td><code>ctx context.Context, script string, keys []string, args []interface{}</code></td>\n<td><code>interface{}, error</code></td>\n<td>Executes Redis Lua script atomically</td>\n</tr>\n<tr>\n<td><code>GetNode</code></td>\n<td><code>key string</code></td>\n<td><code>string, bool</code></td>\n<td>Returns responsible node for key in sharded setup</td>\n</tr>\n</tbody></table>\n<p>The storage interface abstracts Redis operations to enable testing with mock backends and potential future support for alternative storage systems. The <code>ExecuteLua</code> method enables atomic operations by running Lua scripts on Redis servers.</p>\n<h4 id=\"rule-management-interfaces\">Rule Management Interfaces</h4>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>LoadRules</code></td>\n<td><code>configPath string</code></td>\n<td><code>error</code></td>\n<td>Loads rate limit rules from configuration file</td>\n</tr>\n<tr>\n<td><code>GetMatchingRules</code></td>\n<td><code>userID, ipAddress, apiEndpoint string</code></td>\n<td><code>[]*RateLimitRule</code></td>\n<td>Returns rules matching request context</td>\n</tr>\n<tr>\n<td><code>ComposeKey</code></td>\n<td><code>rule *RateLimitRule, req *RateLimitRequest</code></td>\n<td><code>string, error</code></td>\n<td>Generates Redis key from rule pattern and request context</td>\n</tr>\n</tbody></table>\n<p>Rule management interfaces enable dynamic rule loading and efficient rule matching. The <code>GetMatchingRules</code> method filters the complete rule set to only those applicable to a specific request, optimizing evaluation performance.</p>\n<h3 id=\"error-handling-and-state-management\">Error Handling and State Management</h3>\n<p>The system defines comprehensive error handling strategies and state management approaches to ensure reliable operation under various failure conditions.</p>\n<h4 id=\"error-categories\">Error Categories</h4>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Connection Failures</td>\n<td>Connection timeout or network errors</td>\n<td>Automatic failover to backup Redis nodes or local fallback</td>\n</tr>\n<tr>\n<td>Clock Skew Exceeding Tolerance</td>\n<td>Time synchronization checks</td>\n<td>Log warnings and adjust calculations using measured skew</td>\n</tr>\n<tr>\n<td>Configuration Parsing Errors</td>\n<td>Schema validation during rule loading</td>\n<td>Reject invalid configuration and continue with previous valid rules</td>\n</tr>\n<tr>\n<td>Hot Key Overload</td>\n<td>Access pattern analysis</td>\n<td>Replicate hot keys across multiple nodes or apply specialized handling</td>\n</tr>\n<tr>\n<td>Lua Script Execution Failures</td>\n<td>Redis error responses</td>\n<td>Retry with exponential backoff or fall back to local limiting</td>\n</tr>\n</tbody></table>\n<h4 id=\"state-machine-transitions\">State Machine Transitions</h4>\n<p>Circuit breaker state management follows a well-defined state machine to provide predictable failure handling:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Action Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CircuitClosed</code></td>\n<td>Failure threshold exceeded</td>\n<td><code>CircuitOpen</code></td>\n<td>Begin rejecting all requests immediately</td>\n</tr>\n<tr>\n<td><code>CircuitOpen</code></td>\n<td>Recovery timeout elapsed</td>\n<td><code>CircuitHalfOpen</code></td>\n<td>Allow limited test requests to probe service health</td>\n</tr>\n<tr>\n<td><code>CircuitHalfOpen</code></td>\n<td>Test request succeeds</td>\n<td><code>CircuitClosed</code></td>\n<td>Resume normal operation with full request processing</td>\n</tr>\n<tr>\n<td><code>CircuitHalfOpen</code></td>\n<td>Test request fails</td>\n<td><code>CircuitOpen</code></td>\n<td>Return to rejecting all requests and reset recovery timer</td>\n</tr>\n</tbody></table>\n<p>This state machine prevents cascading failures by quickly detecting problems and avoiding unnecessary load on failing components while providing automatic recovery when services become healthy again.</p>\n<h3 id=\"performance-and-scalability-considerations\">Performance and Scalability Considerations</h3>\n<p>The distributed rate limiting system incorporates several design patterns and optimizations to achieve high performance and horizontal scalability.</p>\n<h4 id=\"memory-usage-optimization\">Memory Usage Optimization</h4>\n<p>Different rate limiting algorithms have distinct memory characteristics that affect their suitability for different use cases:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Memory Per Key</th>\n<th>Accuracy</th>\n<th>Best Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket</td>\n<td>O(1) - constant</td>\n<td>Exact for burst, approximate for sustained</td>\n<td>General-purpose rate limiting with burst support</td>\n</tr>\n<tr>\n<td>Sliding Window Counter</td>\n<td>O(1) - constant</td>\n<td>Approximate (can allow 2x limit at boundaries)</td>\n<td>High-scale deployments requiring memory efficiency</td>\n</tr>\n<tr>\n<td>Sliding Window Log</td>\n<td>O(n) - proportional to requests</td>\n<td>Exact</td>\n<td>Low-to-medium scale deployments requiring precision</td>\n</tr>\n</tbody></table>\n<p>The constant memory algorithms (token bucket and sliding window counter) enable the system to support millions of rate-limited keys without running out of memory, making them suitable for large-scale deployments.</p>\n<h4 id=\"latency-optimization-strategies\">Latency Optimization Strategies</h4>\n<p>The system employs several strategies to minimize rate limiting latency:</p>\n<table>\n<thead>\n<tr>\n<th>Optimization</th>\n<th>Technique</th>\n<th>Benefit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Connection Pooling</td>\n<td>Reuse Redis connections across requests</td>\n<td>Eliminates connection establishment overhead</td>\n</tr>\n<tr>\n<td>Lua Script Preloading</td>\n<td>Cache compiled Lua scripts in Redis</td>\n<td>Reduces script compilation time for each request</td>\n</tr>\n<tr>\n<td>Short-Circuit Evaluation</td>\n<td>Stop checking tiers after first denial</td>\n<td>Reduces computation and network calls</td>\n</tr>\n<tr>\n<td>Local Rule Caching</td>\n<td>Cache rule matching results locally</td>\n<td>Eliminates repeated pattern matching computation</td>\n</tr>\n<tr>\n<td>Asynchronous Metrics Collection</td>\n<td>Decouple metrics from request path</td>\n<td>Prevents metrics overhead from affecting latency</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-implementation-patterns\">Common Implementation Patterns</h3>\n<p>Several design patterns appear throughout the distributed rate limiting system, providing consistency and reliability across different components.</p>\n<h4 id=\"observer-pattern-for-configuration-updates\">Observer Pattern for Configuration Updates</h4>\n<p>The system uses the observer pattern to propagate configuration changes from the management API to all application instances. The <code>ConfigurationWatcher</code> subscribes to Redis pub/sub channels, receives change notifications, and triggers local rule reloading. This pattern ensures that configuration updates are applied consistently across the cluster without requiring service restarts.</p>\n<h4 id=\"strategy-pattern-for-algorithm-selection\">Strategy Pattern for Algorithm Selection</h4>\n<p>Rate limiting algorithms are implemented using the strategy pattern, enabling runtime selection based on rule configuration. Each algorithm implements a common interface while providing different trade-offs between accuracy, memory usage, and performance. This design allows the same infrastructure to support multiple algorithms simultaneously.</p>\n<h4 id=\"circuit-breaker-pattern-for-failure-isolation\">Circuit Breaker Pattern for Failure Isolation</h4>\n<p>Circuit breakers provide failure isolation by monitoring error rates and preventing requests to failing components. The pattern includes timeout mechanisms, retry logic, and automatic recovery detection. This approach prevents cascading failures and improves overall system resilience.</p>\n<h4 id=\"template-method-pattern-for-request-processing\">Template Method Pattern for Request Processing</h4>\n<p>The request processing flow uses the template method pattern to standardize the steps while allowing customization for different scenarios. The flow includes request validation, rule matching, tier evaluation, storage operations, and response generation. Each step can be customized while maintaining consistent overall behavior.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for implementing the distributed rate limiting system, including technology recommendations, code organization, and development best practices.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Storage Backend</td>\n<td>Redis single instance with persistence</td>\n<td>Redis Cluster with replication</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>YAML files with file watching</td>\n<td>Redis-backed configuration with pub/sub updates</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Prometheus metrics with Grafana</td>\n<td>Distributed tracing with Jaeger plus metrics</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Embedded Redis with unit tests</td>\n<td>Docker Compose with integration tests</td>\n</tr>\n<tr>\n<td>Deployment</td>\n<td>Single process with Redis connection</td>\n<td>Kubernetes with Redis Operator</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<p>The following directory structure organizes the codebase into logical modules that align with the system&#39;s architectural components:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>distributed-rate-limiter/\n├── cmd/\n│   ├── server/main.go              # HTTP API server entry point\n│   └── cli/main.go                 # Command-line management tools\n├── internal/\n│   ├── algorithms/\n│   │   ├── token_bucket.go         # Token bucket implementation\n│   │   ├── sliding_window.go       # Sliding window implementations\n│   │   └── algorithm_test.go       # Algorithm unit tests\n│   ├── storage/\n│   │   ├── redis.go                # Redis storage implementation\n│   │   ├── fallback.go             # Local fallback storage\n│   │   └── storage_test.go         # Storage integration tests\n│   ├── config/\n│   │   ├── rule_manager.go         # Rule loading and management\n│   │   ├── watcher.go              # Configuration change monitoring\n│   │   └── config_test.go          # Configuration tests\n│   ├── ratelimit/\n│   │   ├── distributed_limiter.go  # Main rate limiting logic\n│   │   ├── multi_tier.go           # Multi-tier evaluation\n│   │   └── ratelimit_test.go       # Core logic tests\n│   ├── cluster/\n│   │   ├── hash_ring.go            # Consistent hashing implementation\n│   │   ├── health_checker.go       # Node health monitoring\n│   │   └── cluster_test.go         # Clustering tests\n│   └── api/\n│       ├── handlers.go             # HTTP request handlers\n│       ├── middleware.go           # Rate limiting middleware\n│       └── api_test.go             # API integration tests\n├── pkg/\n│   ├── metrics/\n│   │   ├── collector.go            # Metrics collection interfaces\n│   │   └── prometheus.go           # Prometheus metrics implementation\n│   └── logging/\n│       ├── correlation.go          # Correlation context and logging\n│       └── structured.go           # Structured logging utilities\n├── test/\n│   ├── fixtures/\n│   │   ├── rules.yaml              # Test rate limiting rules\n│   │   └── config.yaml             # Test configuration\n│   ├── helpers/\n│   │   ├── redis_helper.go         # Redis testing utilities\n│   │   └── load_generator.go       # Load testing tools\n│   └── integration/\n│       ├── distributed_test.go     # Multi-instance integration tests\n│       └── chaos_test.go           # Failure injection tests\n├── configs/\n│   ├── rules.yaml                  # Production rate limiting rules\n│   └── server.yaml                 # Server configuration\n├── scripts/\n│   ├── lua/\n│   │   ├── token_bucket.lua        # Token bucket Lua script\n│   │   └── sliding_window.lua      # Sliding window Lua script\n│   └── deploy/\n│       ├── docker-compose.yaml     # Development environment\n│       └── kubernetes/             # Kubernetes deployment manifests\n└── docs/\n    ├── api.md                      # API documentation\n    ├── algorithms.md               # Algorithm documentation\n    └── operations.md               # Operational procedures</code></pre></div>\n\n<p>This structure separates concerns clearly while maintaining logical grouping. The <code>internal/</code> directory contains private implementation details, while <code>pkg/</code> contains reusable packages that could be imported by other projects.</p>\n<h4 id=\"core-interface-implementation\">Core Interface Implementation</h4>\n<p>The following skeleton provides the foundation for implementing the main rate limiting interface:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package ratelimit provides distributed rate limiting capabilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ratelimit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DistributedLimiter coordinates rate limiting across multiple application instances</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DistributedLimiter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage       </span><span style=\"color:#B392F0\">Storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localFallback </span><span style=\"color:#B392F0\">Limiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    algorithms    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keyComposer   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">KeyComposer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">metrics</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Collector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewDistributedLimiter creates a new distributed rate limiter instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDistributedLimiter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> Storage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ruleManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize algorithm map with token bucket, sliding window implementations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create key composer for generating Redis keys from rule patterns  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize local fallback limiter for degraded operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Set up metrics collector for monitoring and debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return configured DistributedLimiter instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check performs rate limit evaluation and updates counters atomically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> RateLimitRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateLimitResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract correlation context for distributed tracing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get matching rules based on request context (user, IP, API endpoint)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Evaluate all applicable tiers with short-circuit logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each tier, compose Redis key and select appropriate algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Execute rate limit check using selected algorithm and storage backend</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If any tier denies request, return denial with retry-after timing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: If all tiers pass, return success with remaining quota information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Handle Redis failures by falling back to local rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Record metrics for monitoring and hot key detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Log rate limiting decision with correlation context for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"redis-storage-implementation-skeleton\">Redis Storage Implementation Skeleton</h4>\n<p>The Redis storage backend requires careful implementation of atomic operations and connection management:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package storage provides distributed storage backends for rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/go-redis/redis/v8</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisStorage implements distributed rate limiting storage using Redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisStorage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#B392F0\">RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scripts </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Script</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreaker </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRedisStorage creates Redis storage with connection pooling and health checking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRedisStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create Redis universal client with cluster or sentinel support</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Configure connection pool with appropriate size and timeouts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Load and register Lua scripts for atomic rate limit operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Initialize circuit breaker for failure protection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Perform initial connectivity test and return configured storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckAndUpdate atomically checks rate limit and updates counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckAndUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check circuit breaker state before attempting Redis operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Select appropriate Lua script based on algorithm requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Prepare script arguments including current timestamp and window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute Lua script with proper error handling and retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse script result to determine allow/deny decision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Calculate remaining quota and reset time from Redis response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update circuit breaker state based on operation success/failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return rate limiting decision with metadata for client response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"load-testing-and-validation-tools\">Load Testing and Validation Tools</h4>\n<p>Comprehensive testing requires tools for validating distributed behavior under various conditions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package testhelpers provides utilities for testing distributed rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> testhelpers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadGenerator produces synthetic traffic for testing rate limiting behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LoadGenerator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config    </span><span style=\"color:#B392F0\">LoadGeneratorConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limiter   </span><span style=\"color:#B392F0\">ratelimit</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DistributedLimiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    results   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadTestResults</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx       </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancelFn  </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins load generation according to configured pattern</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadTestResults</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize results tracking with thread-safe counters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create context with cancellation for stopping load generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Launch worker goroutines based on configured client count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Coordinate traffic pattern (constant, burst, ramp, spike, random)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Execute rate limit checks with latency measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Record results including success/failure counts and timing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Monitor for test completion or cancellation signal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Aggregate final results and return comprehensive report</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// generateConstantLoad produces steady request rate for baseline testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">generateConstantLoad</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate inter-request delay for desired requests per second</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create ticker for maintaining steady request rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute requests in loop until context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Maintain request timing accuracy despite processing variations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Record each request result with timestamp for analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"debugging-and-troubleshooting-utilities\">Debugging and Troubleshooting Utilities</h4>\n<p>Debugging distributed rate limiting requires specialized tools for analyzing Redis state and request flow:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package debugging provides utilities for troubleshooting distributed rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debugging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RedisDebugger provides Redis-specific debugging capabilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RedisDebugger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#B392F0\">redis</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UniversalClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorrelationLogger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DiagnoseKeyState provides comprehensive analysis of rate limiting key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisDebugger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DiagnoseKeyState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">KeyDiagnostics</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if key exists in Redis and retrieve TTL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Measure memory usage for the key using MEMORY USAGE command</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Determine which Redis node(s) contain the key in cluster setup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Measure access latency by performing test operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Analyze key access patterns from Redis slow log if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return comprehensive diagnostics for troubleshooting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MeasureClusterHealth assesses overall Redis cluster health and performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RedisDebugger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">MeasureClusterHealth</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterHealth</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Enumerate all Redis nodes in cluster configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check connectivity and responsiveness for each node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Measure latency and memory usage across all nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Analyze key distribution balance using CLUSTER NODES</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Identify potential hotspots or performance issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return cluster health report with recommendations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h4>\n<p>After implementing each major component, verify functionality using these checkpoints:</p>\n<p><strong>Milestone 1 - Algorithm Implementation:</strong></p>\n<ul>\n<li>Run <code>go test ./internal/algorithms/...</code> to verify algorithm correctness</li>\n<li>Execute load tests with different traffic patterns to validate burst handling</li>\n<li>Test boundary conditions at window transitions and capacity limits</li>\n<li>Verify token bucket allows configured burst size followed by steady rate</li>\n<li>Confirm sliding window algorithms respect configured request limits</li>\n</ul>\n<p><strong>Milestone 2 - Multi-Tier Evaluation:</strong></p>\n<ul>\n<li>Test per-user, per-IP, and global rate limits simultaneously</li>\n<li>Verify short-circuit evaluation stops at first tier denial</li>\n<li>Validate rule precedence handling when multiple rules match</li>\n<li>Test burst allowance behavior across different tiers</li>\n<li>Confirm API endpoint rate limiting works correctly</li>\n</ul>\n<p><strong>Milestone 3 - Redis Integration:</strong></p>\n<ul>\n<li>Start Redis instance and verify Lua script execution</li>\n<li>Test atomic check-and-update operations under concurrent load</li>\n<li>Simulate Redis failures and verify graceful degradation to local fallback</li>\n<li>Validate connection pooling reduces latency compared to per-request connections</li>\n<li>Test Redis cluster configuration with multiple nodes</li>\n</ul>\n<p><strong>Milestone 4 - Consistent Hashing:</strong></p>\n<ul>\n<li>Add and remove Redis nodes while monitoring key redistribution</li>\n<li>Generate synthetic hot keys and verify detection and replication</li>\n<li>Test load balancing across nodes with different key distributions</li>\n<li>Simulate node failures and verify automatic failover behavior</li>\n<li>Validate virtual nodes improve distribution compared to simple hashing</li>\n</ul>\n<p><strong>Milestone 5 - Management API:</strong></p>\n<ul>\n<li>Create, update, and delete rate limit rules through REST API</li>\n<li>Verify configuration changes propagate to all application instances</li>\n<li>Test real-time dashboard shows current usage and remaining quotas</li>\n<li>Validate rate limit headers in HTTP responses match RFC standards</li>\n<li>Ensure management API itself is protected by rate limiting</li>\n</ul>\n<p>Each checkpoint should include specific commands to run, expected outputs, and signs that indicate proper functionality or potential issues requiring investigation.</p>\n","toc":[{"level":1,"text":"Distributed Rate Limiter: Design Document","id":"distributed-rate-limiter-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Nightclub Bouncer System","id":"mental-model-the-nightclub-bouncer-system"},{"level":3,"text":"Existing Approaches Comparison","id":"existing-approaches-comparison"},{"level":4,"text":"Local vs Distributed Rate Limiting","id":"local-vs-distributed-rate-limiting"},{"level":4,"text":"In-Memory vs Persistent Storage","id":"in-memory-vs-persistent-storage"},{"level":4,"text":"Algorithm Complexity vs Accuracy Trade-offs","id":"algorithm-complexity-vs-accuracy-trade-offs"},{"level":4,"text":"Fallback Strategy Comparison","id":"fallback-strategy-comparison"},{"level":3,"text":"Common Pitfalls in Distributed Rate Limiting","id":"common-pitfalls-in-distributed-rate-limiting"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Algorithm Skeleton","id":"core-algorithm-skeleton"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoint: Basic Rate Limiter","id":"milestone-checkpoint-basic-rate-limiter"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Mental Model: The Traffic Control Center","id":"mental-model-the-traffic-control-center"},{"level":3,"text":"Functional Goals","id":"functional-goals"},{"level":4,"text":"Core Rate Limiting Capabilities","id":"core-rate-limiting-capabilities"},{"level":4,"text":"Dynamic Configuration Management","id":"dynamic-configuration-management"},{"level":4,"text":"Atomic Operations and Consistency","id":"atomic-operations-and-consistency"},{"level":4,"text":"State Persistence and Recovery","id":"state-persistence-and-recovery"},{"level":3,"text":"Non-Functional Goals","id":"non-functional-goals"},{"level":4,"text":"Performance Requirements","id":"performance-requirements"},{"level":4,"text":"Reliability and Availability","id":"reliability-and-availability"},{"level":4,"text":"Scalability Requirements","id":"scalability-requirements"},{"level":4,"text":"Monitoring and Observability","id":"monitoring-and-observability"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":4,"text":"Advanced Algorithm Features","id":"advanced-algorithm-features"},{"level":4,"text":"Enterprise Integration Features","id":"enterprise-integration-features"},{"level":4,"text":"Operational Complexity Features","id":"operational-complexity-features"},{"level":4,"text":"Performance and Scale Limitations","id":"performance-and-scale-limitations"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Project Structure","id":"project-structure"},{"level":4,"text":"Core Configuration Structures","id":"core-configuration-structures"},{"level":4,"text":"Rate Limit Rule Definition","id":"rate-limit-rule-definition"},{"level":4,"text":"Essential Constants","id":"essential-constants"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Overview","id":"component-overview"},{"level":4,"text":"DistributedLimiter Component","id":"distributedlimiter-component"},{"level":4,"text":"Storage Backend Components","id":"storage-backend-components"},{"level":4,"text":"Algorithm Implementation Components","id":"algorithm-implementation-components"},{"level":4,"text":"RuleManager Component","id":"rulemanager-component"},{"level":4,"text":"ConsistentHashRing Component","id":"consistenthashring-component"},{"level":4,"text":"Metrics and Monitoring Components","id":"metrics-and-monitoring-components"},{"level":3,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Module Dependency Guidelines","id":"module-dependency-guidelines"},{"level":4,"text":"Configuration Management Structure","id":"configuration-management-structure"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure Setup","id":"recommended-file-structure-setup"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Component Skeletons","id":"core-component-skeletons"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Rate Limit Rule Structure","id":"rate-limit-rule-structure"},{"level":3,"text":"Redis Data Structures","id":"redis-data-structures"},{"level":3,"text":"Redis Key Expiration Strategy","id":"redis-key-expiration-strategy"},{"level":3,"text":"Metrics and Monitoring Data","id":"metrics-and-monitoring-data"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Core Data Structure Definitions","id":"core-data-structure-definitions"},{"level":4,"text":"Redis Storage Implementation Skeleton","id":"redis-storage-implementation-skeleton"},{"level":4,"text":"Rule Management Implementation Skeleton","id":"rule-management-implementation-skeleton"},{"level":4,"text":"Metrics Collection Implementation Skeleton","id":"metrics-collection-implementation-skeleton"},{"level":4,"text":"Configuration File Example","id":"configuration-file-example"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Rate Limiting Algorithms","id":"rate-limiting-algorithms"},{"level":3,"text":"Mental Model: Water Flow Control Systems","id":"mental-model-water-flow-control-systems"},{"level":3,"text":"Token Bucket Algorithm Design","id":"token-bucket-algorithm-design"},{"level":3,"text":"Sliding Window Algorithms Design","id":"sliding-window-algorithms-design"},{"level":3,"text":"Algorithm Selection ADR","id":"algorithm-selection-adr"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Multi-Tier Rate Limiting","id":"multi-tier-rate-limiting"},{"level":3,"text":"Mental Model: Cascading Security Checkpoints","id":"mental-model-cascading-security-checkpoints"},{"level":3,"text":"Tier Evaluation Strategy","id":"tier-evaluation-strategy"},{"level":4,"text":"Tier Evaluation Algorithm Details","id":"tier-evaluation-algorithm-details"},{"level":4,"text":"Common Pitfalls in Tier Evaluation","id":"common-pitfalls-in-tier-evaluation"},{"level":3,"text":"Rate Limit Key Composition","id":"rate-limit-key-composition"},{"level":4,"text":"Key Composition Implementation Strategy","id":"key-composition-implementation-strategy"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Redis Backend Integration","id":"redis-backend-integration"},{"level":3,"text":"Mental Model: Bank Transaction Processing","id":"mental-model-bank-transaction-processing"},{"level":3,"text":"Lua Script Design","id":"lua-script-design"},{"level":3,"text":"Connection Pool Management","id":"connection-pool-management"},{"level":3,"text":"Graceful Degradation Strategy","id":"graceful-degradation-strategy"},{"level":3,"text":"Redis Backend ADR","id":"redis-backend-adr"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Consistent Hashing and Sharding","id":"consistent-hashing-and-sharding"},{"level":3,"text":"Mental Model: Library Book Distribution","id":"mental-model-library-book-distribution"},{"level":3,"text":"Consistent Hash Ring Design","id":"consistent-hash-ring-design"},{"level":4,"text":"Hash Ring Mathematics and Virtual Nodes","id":"hash-ring-mathematics-and-virtual-nodes"},{"level":4,"text":"Key Assignment Algorithm","id":"key-assignment-algorithm"},{"level":4,"text":"Minimizing Redistribution During Topology Changes","id":"minimizing-redistribution-during-topology-changes"},{"level":3,"text":"Hot Key Detection and Rebalancing","id":"hot-key-detection-and-rebalancing"},{"level":4,"text":"Hot Key Identification Mechanisms","id":"hot-key-identification-mechanisms"},{"level":4,"text":"Hot Key Replication Strategy","id":"hot-key-replication-strategy"},{"level":4,"text":"Automatic Rebalancing Triggers","id":"automatic-rebalancing-triggers"},{"level":3,"text":"Node Health and Failover","id":"node-health-and-failover"},{"level":4,"text":"Health Check Implementation","id":"health-check-implementation"},{"level":4,"text":"Circuit Breaker Pattern Implementation","id":"circuit-breaker-pattern-implementation"},{"level":4,"text":"Automatic Failover Coordination","id":"automatic-failover-coordination"},{"level":4,"text":"Recovery and Reintegration Process","id":"recovery-and-reintegration-process"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Mental Model: The Air Traffic Control System","id":"mental-model-the-air-traffic-control-system"},{"level":3,"text":"Rate Limit Check Flow","id":"rate-limit-check-flow"},{"level":4,"text":"Request Context Assembly","id":"request-context-assembly"},{"level":4,"text":"Multi-Tier Rule Evaluation","id":"multi-tier-rule-evaluation"},{"level":4,"text":"Redis Atomic Operations","id":"redis-atomic-operations"},{"level":4,"text":"Local Fallback Handling","id":"local-fallback-handling"},{"level":4,"text":"Response Header Generation","id":"response-header-generation"},{"level":3,"text":"Configuration Update Propagation","id":"configuration-update-propagation"},{"level":4,"text":"Mental Model: Emergency Broadcast System","id":"mental-model-emergency-broadcast-system"},{"level":4,"text":"Configuration Source and Validation","id":"configuration-source-and-validation"},{"level":4,"text":"Redis as Configuration Distribution Hub","id":"redis-as-configuration-distribution-hub"},{"level":4,"text":"Application Instance Update Process","id":"application-instance-update-process"},{"level":4,"text":"Configuration Validation and Conflict Resolution","id":"configuration-validation-and-conflict-resolution"},{"level":4,"text":"Configuration Rollback and Recovery","id":"configuration-rollback-and-recovery"},{"level":3,"text":"Metrics Collection and Aggregation","id":"metrics-collection-and-aggregation"},{"level":4,"text":"Mental Model: Hospital Vital Signs Monitoring","id":"mental-model-hospital-vital-signs-monitoring"},{"level":4,"text":"Real-Time Metrics Collection","id":"real-time-metrics-collection"},{"level":4,"text":"Hot Key Detection and Analysis","id":"hot-key-detection-and-analysis"},{"level":4,"text":"Dashboard and Visualization","id":"dashboard-and-visualization"},{"level":4,"text":"Alerting and Anomaly Detection","id":"alerting-and-anomaly-detection"},{"level":4,"text":"Self-Monitoring Rate Limits","id":"self-monitoring-rate-limits"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure","id":"file-structure"},{"level":4,"text":"Request Flow Coordinator Infrastructure","id":"request-flow-coordinator-infrastructure"},{"level":4,"text":"Configuration Watcher Infrastructure","id":"configuration-watcher-infrastructure"},{"level":4,"text":"Metrics Collection Infrastructure","id":"metrics-collection-infrastructure"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Rate Limit API and Dashboard","id":"rate-limit-api-and-dashboard"},{"level":3,"text":"Mental Model: The Air Traffic Control System","id":"mental-model-the-air-traffic-control-system"},{"level":3,"text":"Rate Limit Management API","id":"rate-limit-management-api"},{"level":4,"text":"API Design Philosophy","id":"api-design-philosophy"},{"level":4,"text":"Rule Validation and Constraints","id":"rule-validation-and-constraints"},{"level":4,"text":"Rule Versioning and Rollback","id":"rule-versioning-and-rollback"},{"level":4,"text":"Configuration Propagation Mechanism","id":"configuration-propagation-mechanism"},{"level":4,"text":"Audit Trail and Change History","id":"audit-trail-and-change-history"},{"level":3,"text":"Standard Rate Limit Headers","id":"standard-rate-limit-headers"},{"level":4,"text":"Header Implementation Strategy","id":"header-implementation-strategy"},{"level":4,"text":"Multi-Tier Header Aggregation","id":"multi-tier-header-aggregation"},{"level":4,"text":"Client Retry Guidance","id":"client-retry-guidance"},{"level":3,"text":"Real-time Dashboard Architecture","id":"real-time-dashboard-architecture"},{"level":4,"text":"Mental Model: Power Grid Control Room","id":"mental-model-power-grid-control-room"},{"level":4,"text":"Dashboard Data Architecture","id":"dashboard-data-architecture"},{"level":4,"text":"Real-time Metrics Collection","id":"real-time-metrics-collection"},{"level":4,"text":"WebSocket-Based Real-Time Updates","id":"websocket-based-real-time-updates"},{"level":4,"text":"Dashboard User Interface Design","id":"dashboard-user-interface-design"},{"level":4,"text":"Dashboard Performance Optimization","id":"dashboard-performance-optimization"},{"level":3,"text":"Self-Rate-Limiting the Management API","id":"self-rate-limiting-the-management-api"},{"level":4,"text":"Mental Model: Emergency Services Communication","id":"mental-model-emergency-services-communication"},{"level":4,"text":"Independent Rate Limiting Strategy","id":"independent-rate-limiting-strategy"},{"level":4,"text":"API-Specific Rate Limit Tiers","id":"api-specific-rate-limit-tiers"},{"level":4,"text":"Emergency Override Mechanisms","id":"emergency-override-mechanisms"},{"level":4,"text":"API Protection Without Circular Dependencies","id":"api-protection-without-circular-dependencies"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Mental Model: The Emergency Response Network","id":"mental-model-the-emergency-response-network"},{"level":3,"text":"Redis Failure Scenarios","id":"redis-failure-scenarios"},{"level":4,"text":"Connection Pool Exhaustion and Recovery","id":"connection-pool-exhaustion-and-recovery"},{"level":4,"text":"Memory Pressure and Eviction Handling","id":"memory-pressure-and-eviction-handling"},{"level":4,"text":"Split-Brain and Network Partition Handling","id":"split-brain-and-network-partition-handling"},{"level":4,"text":"Data Corruption and Inconsistency Detection","id":"data-corruption-and-inconsistency-detection"},{"level":3,"text":"Clock Skew and Time Synchronization","id":"clock-skew-and-time-synchronization"},{"level":4,"text":"Detecting and Measuring Clock Skew","id":"detecting-and-measuring-clock-skew"},{"level":4,"text":"Window Boundary Synchronization","id":"window-boundary-synchronization"},{"level":4,"text":"Handling Timestamp Rollback","id":"handling-timestamp-rollback"},{"level":3,"text":"Race Condition Prevention","id":"race-condition-prevention"},{"level":4,"text":"Atomic Operations with Lua Scripts","id":"atomic-operations-with-lua-scripts"},{"level":4,"text":"Optimistic vs Pessimistic Concurrency","id":"optimistic-vs-pessimistic-concurrency"},{"level":4,"text":"Hot Key Conflict Resolution","id":"hot-key-conflict-resolution"},{"level":4,"text":"Preventing Lost Updates in Sliding Windows","id":"preventing-lost-updates-in-sliding-windows"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Circuit Breaker Infrastructure","id":"circuit-breaker-infrastructure"},{"level":4,"text":"Health Checker Implementation","id":"health-checker-implementation"},{"level":4,"text":"Time Synchronization and Clock Skew Detection","id":"time-synchronization-and-clock-skew-detection"},{"level":4,"text":"Local Fallback Rate Limiter","id":"local-fallback-rate-limiter"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Mental Model: The Quality Assurance Laboratory System","id":"mental-model-the-quality-assurance-laboratory-system"},{"level":3,"text":"Algorithm Unit Testing","id":"algorithm-unit-testing"},{"level":3,"text":"Distributed Integration Testing","id":"distributed-integration-testing"},{"level":3,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"},{"level":3,"text":"Chaos and Failure Testing","id":"chaos-and-failure-testing"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: The Detective Investigation Framework","id":"mental-model-the-detective-investigation-framework"},{"level":3,"text":"Symptom-Based Diagnosis Table","id":"symptom-based-diagnosis-table"},{"level":3,"text":"Redis-Specific Debugging Techniques","id":"redis-specific-debugging-techniques"},{"level":3,"text":"Distributed System Debugging","id":"distributed-system-debugging"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Adaptive Rate Limiting","id":"adaptive-rate-limiting"},{"level":4,"text":"Adaptive Algorithm Design","id":"adaptive-algorithm-design"},{"level":4,"text":"Integration with Existing Architecture","id":"integration-with-existing-architecture"},{"level":4,"text":"Common Pitfalls in Adaptive Systems","id":"common-pitfalls-in-adaptive-systems"},{"level":3,"text":"Geographic Distribution","id":"geographic-distribution"},{"level":4,"text":"Multi-Region Architecture Design","id":"multi-region-architecture-design"},{"level":4,"text":"Cross-Region Synchronization Protocol","id":"cross-region-synchronization-protocol"},{"level":4,"text":"Common Pitfalls in Geographic Distribution","id":"common-pitfalls-in-geographic-distribution"},{"level":3,"text":"Service Mesh Integration","id":"service-mesh-integration"},{"level":4,"text":"Envoy Proxy Integration Design","id":"envoy-proxy-integration-design"},{"level":4,"text":"Transparent Policy Enforcement","id":"transparent-policy-enforcement"},{"level":4,"text":"Observability and Debugging","id":"observability-and-debugging"},{"level":4,"text":"Common Pitfalls in Service Mesh Integration","id":"common-pitfalls-in-service-mesh-integration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Adaptive Rate Limiting Starter Code","id":"adaptive-rate-limiting-starter-code"},{"level":4,"text":"Geographic Distribution Starter Code","id":"geographic-distribution-starter-code"},{"level":4,"text":"Service Mesh Integration Starter Code","id":"service-mesh-integration-starter-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mental Model: The Technical Reference Library","id":"mental-model-the-technical-reference-library"},{"level":3,"text":"Core System Concepts","id":"core-system-concepts"},{"level":3,"text":"Rate Limiting Algorithm Terms","id":"rate-limiting-algorithm-terms"},{"level":3,"text":"Multi-Tier Rate Limiting Concepts","id":"multi-tier-rate-limiting-concepts"},{"level":3,"text":"Distributed Systems and Consistency","id":"distributed-systems-and-consistency"},{"level":3,"text":"Redis and Storage Concepts","id":"redis-and-storage-concepts"},{"level":3,"text":"Testing and Quality Assurance","id":"testing-and-quality-assurance"},{"level":3,"text":"Monitoring and Operations","id":"monitoring-and-operations"},{"level":3,"text":"System Architecture Components","id":"system-architecture-components"},{"level":4,"text":"Core Rate Limiting Types","id":"core-rate-limiting-types"},{"level":4,"text":"Algorithm Implementation Types","id":"algorithm-implementation-types"},{"level":4,"text":"Multi-Tier Evaluation Types","id":"multi-tier-evaluation-types"},{"level":4,"text":"Distributed Storage Types","id":"distributed-storage-types"},{"level":4,"text":"Consistent Hashing and Clustering Types","id":"consistent-hashing-and-clustering-types"},{"level":4,"text":"Configuration and Management Types","id":"configuration-and-management-types"},{"level":4,"text":"Time and Synchronization Types","id":"time-and-synchronization-types"},{"level":4,"text":"Testing and Quality Assurance Types","id":"testing-and-quality-assurance-types"},{"level":3,"text":"Algorithm-Specific Constants","id":"algorithm-specific-constants"},{"level":3,"text":"Interface Definitions","id":"interface-definitions"},{"level":4,"text":"Core Rate Limiting Interfaces","id":"core-rate-limiting-interfaces"},{"level":4,"text":"Storage Backend Interfaces","id":"storage-backend-interfaces"},{"level":4,"text":"Rule Management Interfaces","id":"rule-management-interfaces"},{"level":3,"text":"Error Handling and State Management","id":"error-handling-and-state-management"},{"level":4,"text":"Error Categories","id":"error-categories"},{"level":4,"text":"State Machine Transitions","id":"state-machine-transitions"},{"level":3,"text":"Performance and Scalability Considerations","id":"performance-and-scalability-considerations"},{"level":4,"text":"Memory Usage Optimization","id":"memory-usage-optimization"},{"level":4,"text":"Latency Optimization Strategies","id":"latency-optimization-strategies"},{"level":3,"text":"Common Implementation Patterns","id":"common-implementation-patterns"},{"level":4,"text":"Observer Pattern for Configuration Updates","id":"observer-pattern-for-configuration-updates"},{"level":4,"text":"Strategy Pattern for Algorithm Selection","id":"strategy-pattern-for-algorithm-selection"},{"level":4,"text":"Circuit Breaker Pattern for Failure Isolation","id":"circuit-breaker-pattern-for-failure-isolation"},{"level":4,"text":"Template Method Pattern for Request Processing","id":"template-method-pattern-for-request-processing"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Core Interface Implementation","id":"core-interface-implementation"},{"level":4,"text":"Redis Storage Implementation Skeleton","id":"redis-storage-implementation-skeleton"},{"level":4,"text":"Load Testing and Validation Tools","id":"load-testing-and-validation-tools"},{"level":4,"text":"Debugging and Troubleshooting Utilities","id":"debugging-and-troubleshooting-utilities"},{"level":4,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"}],"title":"Distributed Rate Limiter: Design Document","markdown":"# Distributed Rate Limiter: Design Document\n\n\n## Overview\n\nA distributed rate limiting system that enforces request quotas across multiple application instances using Redis as a shared state store. The key challenge is maintaining accurate rate limits in a distributed environment while handling Redis failures, clock skew, and achieving high performance through sharding.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones - this foundational understanding applies to rate limiting algorithms, multi-tier systems, Redis integration, sharding, and API design.\n\n### Mental Model: The Nightclub Bouncer System\n\nThink of distributed rate limiting like managing capacity across a chain of popular nightclubs during New Year's Eve. Each nightclub has a fire safety capacity limit, but customers don't just visit one location - they might hop between venues throughout the night. The challenge is ensuring that the combined crowd across all locations doesn't exceed what the fire department allows, while also managing individual venue limits and VIP customer quotas.\n\nIn a traditional single-venue scenario, you'd have one bouncer with a mechanical clicker counter at the door. Every person entering gets counted up, every person leaving gets counted down. The bouncer always knows exactly how many people are inside and can make instant decisions. This represents **local rate limiting** - one application instance tracking its own request counts in memory.\n\nBut when you expand to multiple venues (distributed application instances), the problem becomes exponentially more complex. Now you need to coordinate between multiple bouncers across the city. If your fire department limit is 1000 people total across all venues, and Bouncer Alice at Club Downtown has seen 400 people enter while Bouncer Bob at Club Uptown has seen 450 people, what happens when someone approaches Bouncer Charlie at Club Westside? Charlie needs to know the current total (850) before deciding whether to allow the 851st person in.\n\nThe naive approach would be to have each bouncer call every other bouncer before making a decision: \"Hey Alice, how many people do you have? Hey Bob, what's your count?\" This creates several problems. First, it's slow - every admission decision requires multiple phone calls. Second, it's fragile - if Alice doesn't answer her phone, does that mean Charlie should assume zero people at Downtown? Third, it creates race conditions - while Charlie is making his phone calls, Alice might admit 50 more people, making Charlie's decision based on stale data.\n\nThe **distributed rate limiting solution** introduces a **central coordination system** (Redis) that acts like a real-time dispatch center. Instead of bouncers calling each other, they all report to and check with dispatch. When someone wants to enter Charlie's venue, Charlie radios dispatch: \"Request to admit one person.\" Dispatch has the real-time total across all venues and responds: \"Approved, new total is 851\" or \"Denied, at capacity.\" This provides several critical capabilities:\n\n**Atomic Decision Making**: Dispatch can check the current count and increment it in a single atomic operation, preventing the race condition where two bouncers simultaneously think they're admitting the 1000th person when they're actually admitting the 999th and 1000th.\n\n**Global State Visibility**: Every bouncer gets decisions based on the true global state, not stale local information or incomplete peer-to-peer communication.\n\n**Hierarchical Limits**: Dispatch can enforce multiple types of limits simultaneously. Maybe the fire department allows 1000 total people, but Club Downtown has a structural limit of 300, and VIP customers get guaranteed access to 50 spots regardless of the general admission count.\n\n**Resilience Through Fallback**: If the radio system goes down, bouncers can fall back to local-only decisions. They might occasionally exceed the global limit during the outage, but the venues remain operational rather than shutting down entirely.\n\nHowever, this coordination comes with new challenges that don't exist in the single-venue scenario:\n\n**Network Latency**: Radio calls to dispatch take time. In high-traffic periods, the delay between \"request to admit\" and \"approved/denied\" could cause customer frustration.\n\n**Central Point of Failure**: If dispatch goes offline, all venues are affected. The system needs fallback strategies and redundancy.\n\n**Clock Synchronization**: If Club Downtown's clocks are 5 minutes fast, their \"hourly\" limits might reset at different times than Club Uptown's, creating windows where the global limit can be exceeded.\n\n**Hot Spot Management**: If a celebrity shows up at Club Downtown and generates massive traffic, that venue might overwhelm dispatch with admission requests, slowing down decisions for all other venues.\n\nThis nightclub analogy maps directly to distributed rate limiting concepts:\n\n- **Venues** = Application instances\n- **Bouncers** = Rate limiter components within each instance\n- **Customers** = Incoming HTTP requests\n- **Fire department capacity** = Global rate limits\n- **Venue structural limits** = Per-instance or per-API limits\n- **VIP quotas** = Per-user rate limits\n- **Dispatch center** = Redis cluster\n- **Radio communication** = Network calls to Redis\n- **Mechanical clickers** = In-memory counters for local fallback\n- **Clock synchronization** = Time-based window alignment across nodes\n\n### Existing Approaches Comparison\n\nUnderstanding the landscape of rate limiting approaches helps illuminate why distributed rate limiting requires careful design decisions. Each approach represents different trade-offs between accuracy, performance, complexity, and resilience.\n\n#### Local vs Distributed Rate Limiting\n\n| Aspect | Local Rate Limiting | Distributed Rate Limiting |\n|--------|-------------------|-------------------------|\n| **State Storage** | In-memory within each application instance | Shared external store (Redis, database) |\n| **Decision Latency** | Microseconds (memory access) | Milliseconds (network + storage access) |\n| **Accuracy** | Perfect for single-instance traffic | Perfect for cluster-wide traffic |\n| **Failure Behavior** | Independent failures per instance | Coordinated failures across cluster |\n| **Implementation Complexity** | Simple hash maps and timers | Atomic operations, consensus, fallback logic |\n| **Resource Usage** | Minimal CPU and memory overhead | Network bandwidth, Redis memory, connection pools |\n| **Scaling Characteristics** | Limits scale with instance count | Limits remain constant regardless of instance count |\n\n**Local rate limiting** excels in scenarios where each application instance handles completely independent traffic or where approximate limiting is acceptable. For example, if you have 10 application instances and want to limit each user to 100 requests per hour, local limiting would give each user 1000 requests per hour cluster-wide (100 per instance). This might be acceptable for rough abuse prevention but fails for precise quota enforcement or protecting downstream services with hard capacity limits.\n\n**Distributed rate limiting** becomes essential when you need precise control over cluster-wide request rates. Consider a payment processing API where the downstream banking service can only handle 1000 transactions per minute total. With local limiting, you'd need to divide this quota across instances (100 per instance if you have 10 instances), but this creates problems: if traffic is unevenly distributed, some instances might exhaust their quota while others remain idle, leading to artificial throttling despite available global capacity.\n\n> **Key Insight**: The choice between local and distributed rate limiting fundamentally depends on whether your limiting goal is per-instance abuse prevention (local) or cluster-wide resource protection (distributed).\n\n#### In-Memory vs Persistent Storage\n\n| Aspect | In-Memory Storage | Persistent Storage (Redis) |\n|--------|------------------|---------------------------|\n| **Performance** | Fastest (no I/O) | Fast (network + memory access) |\n| **Durability** | Lost on process restart | Survives restarts and failures |\n| **Sharing** | Cannot share between instances | Natural sharing across cluster |\n| **Memory Usage** | Grows with active rate limit keys | Centralized memory usage |\n| **Consistency** | Eventually consistent across instances | Strongly consistent with atomic operations |\n| **Operational Complexity** | No external dependencies | Requires Redis cluster management |\n\n**In-memory storage** using hash maps or similar structures provides the lowest latency for rate limiting decisions. Popular libraries like Google's `golang.org/x/time/rate` or Java's Guava RateLimiter implement sophisticated algorithms entirely in memory. However, this approach has fundamental limitations in distributed systems:\n\n- **State Isolation**: Each instance maintains separate counters, making precise global limits impossible\n- **Cold Start Problems**: New instances start with empty rate limit state, potentially allowing bursts that exceed intended limits\n- **Restart Penalties**: Process restarts reset all counters, effectively giving users fresh quota allocations\n\n**Persistent storage** through Redis or similar systems enables true distributed coordination but introduces new complexities:\n\n- **Network Partitions**: What happens when an application instance can reach Redis but other instances cannot?\n- **Redis Failures**: How do you maintain availability when the coordination layer fails?\n- **Data Consistency**: Ensuring that concurrent updates from multiple instances don't corrupt rate limit counters\n\n> **Decision: Redis as Coordination Layer**\n> - **Context**: Need for atomic operations, high availability, and performance in distributed rate limiting\n> - **Options Considered**: \n>   1. Database-backed counters with transactions\n>   2. Redis with Lua scripts for atomicity\n>   3. Distributed consensus systems (etcd, Consul)\n> - **Decision**: Redis with Lua scripts\n> - **Rationale**: Redis provides microsecond-latency atomic operations through Lua scripts, built-in expiration for time-window management, and mature high-availability clustering. Database transactions add unnecessary overhead for simple counter operations, while consensus systems optimize for different use cases (configuration management) rather than high-throughput counting.\n> - **Consequences**: Enables precise distributed rate limiting with excellent performance, but requires Redis operational expertise and introduces dependency on Redis availability.\n\n#### Algorithm Complexity vs Accuracy Trade-offs\n\nDifferent rate limiting algorithms make different trade-offs between implementation complexity, memory usage, and limiting accuracy:\n\n| Algorithm | Memory Usage | Accuracy | Burst Handling | Implementation Complexity |\n|-----------|-------------|----------|----------------|--------------------------|\n| **Fixed Window Counter** | O(1) per key | Poor (2x limit possible) | Allows full limit per window | Simple |\n| **Sliding Window Log** | O(n) per key (n=requests) | Perfect | Precise burst control | Moderate |\n| **Sliding Window Counter** | O(k) per key (k=sub-windows) | Good (configurable accuracy) | Smooth burst handling | Moderate |\n| **Token Bucket** | O(1) per key | Good (allows configured burst) | Explicit burst capacity | Simple |\n| **Leaky Bucket** | O(n) per key (n=queued requests) | Perfect (no bursts) | No burst allowance | Complex |\n\n**Fixed Window Counter** represents the simplest approach: reset a counter every time period (e.g., every minute). This creates the \"boundary problem\" where a user could make 1000 requests in the last second of one minute and another 1000 requests in the first second of the next minute, effectively achieving 2000 requests per minute despite a 1000/minute limit.\n\n**Sliding Window Log** maintains a timestamp for every request within the current window. This provides perfect accuracy but consumes memory proportional to the request rate, making it expensive for high-traffic scenarios.\n\n**Token Bucket** offers a middle ground by modeling rate limits as a bucket that starts full of tokens, loses tokens with each request, and refills at a steady rate. This naturally handles bursts (empty the bucket quickly) while enforcing long-term rate limits (bucket refill rate).\n\n> **Architecture Decision**: The distributed rate limiter implements multiple algorithms to handle different use cases. Token bucket for APIs that benefit from burst allowance, sliding window counter for smooth rate enforcement, and sliding window log for scenarios requiring perfect accuracy despite higher memory costs.\n\n#### Fallback Strategy Comparison\n\nWhen the coordination layer (Redis) becomes unavailable, different fallback strategies offer different trade-offs:\n\n| Fallback Strategy | Availability | Accuracy During Outage | Recovery Behavior |\n|------------------|-------------|----------------------|------------------|\n| **Fail Open** | 100% (no limiting) | 0% (unlimited requests) | Immediate return to normal |\n| **Fail Closed** | 0% (reject all) | 100% (no limit violations) | Immediate return to normal |\n| **Local Fallback** | High (degraded limiting) | Poor (per-instance limits) | Gradual convergence |\n| **Circuit Breaker** | Variable (configurable) | Variable (configurable) | Staged recovery testing |\n\n**Fail Open** prioritizes availability by allowing all requests through during Redis outages. This suits scenarios where brief periods of unlimited access are preferable to service disruption, such as content delivery or social media APIs.\n\n**Fail Closed** prioritizes protection by rejecting requests during outages. This suits scenarios where exceeding limits could cause cascading failures, such as payment processing or database write APIs.\n\n**Local Fallback** attempts to maintain degraded rate limiting by switching to per-instance limits during outages. Each instance divides the global limit by the number of instances and enforces this reduced limit locally. This provides some protection while maintaining availability, but can lead to under-utilization if traffic is unevenly distributed.\n\n**Circuit Breaker Pattern** implements intelligent failure detection and recovery testing. Rather than immediately resuming full Redis usage when connectivity returns, the circuit breaker gradually increases traffic to Redis while monitoring for continued failures.\n\n> **Decision: Graceful Degradation with Local Fallback**\n> - **Context**: Need to balance availability with protection during Redis outages\n> - **Options Considered**: Fail open, fail closed, local fallback with circuit breaker\n> - **Decision**: Local fallback with gradual recovery\n> - **Rationale**: Maintains both availability and some level of protection during outages. Circuit breaker prevents thundering herd problems when Redis recovers. Local fallback provides predictable behavior that operations teams can reason about.\n> - **Consequences**: Enables continued operation during Redis outages with degraded but predictable rate limiting. Requires careful configuration of per-instance quotas and circuit breaker thresholds.\n\n### Common Pitfalls in Distributed Rate Limiting\n\n⚠️ **Pitfall: Assuming Network Calls Are Instantaneous**\n\nMany developers initially design distributed rate limiters as if Redis calls have zero latency. They write code that makes synchronous Redis calls in the request path without considering timeout handling, connection pooling, or retry logic. Under load, this creates cascading delays where rate limiting decisions become the bottleneck rather than the protection mechanism.\n\nThe fix requires treating every Redis interaction as a potentially slow network operation with explicit timeouts, connection reuse, and circuit breaker patterns. Rate limiting checks should typically complete within 1-2 milliseconds; anything slower suggests architectural problems.\n\n⚠️ **Pitfall: Ignoring Clock Skew Between Instances**\n\nTime-based rate limiting algorithms assume synchronized clocks across all application instances. In practice, server clocks can drift by seconds or minutes, causing time window boundaries to misalign. This creates windows where users can exceed limits by making requests to instances with fast clocks just before window boundaries and slow clocks just after boundaries.\n\nThe solution involves using Redis server time for all time-based calculations rather than application instance time, or implementing clock synchronization monitoring with alerts when drift exceeds acceptable thresholds.\n\n⚠️ **Pitfall: Not Handling Redis Memory Pressure**\n\nRedis operates as an in-memory database, and rate limiting can generate enormous numbers of keys (user IDs, IP addresses, API endpoints combined with time windows). Without proper key expiration and memory management, Redis can run out of memory, causing either data eviction or service failures.\n\nThe fix requires careful key naming with TTL management, monitoring Redis memory usage, and implementing key cleanup strategies for inactive rate limit entries. Every rate limit key should have an explicit expiration time, typically 2x the rate limit window duration.\n\n### Implementation Guidance\n\nThis section provides concrete technology recommendations and starter code for building the distributed rate limiter foundation.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Redis Client** | `go-redis/redis/v9` (Redis client) | `go-redis/redis/v9` with cluster support |\n| **HTTP Framework** | `net/http` with middleware | `gin-gonic/gin` or `gorilla/mux` |\n| **Configuration** | Environment variables + `os.Getenv` | `viper` configuration management |\n| **Metrics** | `expvar` for basic metrics | `prometheus/client_golang` |\n| **Logging** | `log/slog` (Go 1.21+) | `uber-go/zap` for structured logging |\n| **Testing** | `testing` + `testcontainers` for Redis | `stretchr/testify` + test containers |\n\n#### Recommended Project Structure\n\n```\ndistributed-rate-limiter/\n├── cmd/\n│   ├── server/main.go              ← HTTP server with rate limiting middleware\n│   └── dashboard/main.go           ← Management dashboard server\n├── internal/\n│   ├── ratelimit/                  ← Core rate limiting logic\n│   │   ├── limiter.go             ← Main rate limiter interface\n│   │   ├── algorithms/            ← Rate limiting algorithm implementations\n│   │   │   ├── token_bucket.go    ← Token bucket algorithm\n│   │   │   ├── sliding_window.go  ← Sliding window algorithms\n│   │   │   └── algorithm.go       ← Common algorithm interface\n│   │   ├── storage/               ← Storage backends\n│   │   │   ├── redis.go          ← Redis backend implementation\n│   │   │   ├── local.go          ← Local fallback storage\n│   │   │   └── storage.go        ← Storage interface\n│   │   └── config/               ← Configuration and rule management\n│   │       ├── rules.go          ← Rate limit rule definitions\n│   │       └── manager.go        ← Dynamic configuration management\n│   ├── api/                      ← REST API for management\n│   │   ├── handlers.go           ← HTTP handlers for CRUD operations\n│   │   └── middleware.go         ← Rate limiting middleware\n│   ├── dashboard/                ← Real-time dashboard components\n│   │   ├── websocket.go          ← WebSocket handlers for real-time updates\n│   │   └── metrics.go            ← Metrics collection and aggregation\n│   └── sharding/                 ← Consistent hashing and sharding\n│       ├── consistent_hash.go    ← Consistent hash ring implementation\n│       └── node_manager.go       ← Redis node health and management\n├── pkg/                          ← Public interfaces (if needed)\n├── configs/                      ← Configuration files\n│   └── rate_limits.yaml         ← Default rate limit rules\n├── scripts/                      ← Deployment and utility scripts\n│   ├── redis_setup.sh           ← Redis cluster setup script\n│   └── load_test.sh             ← Load testing script\n├── docs/                         ← Additional documentation\n└── docker-compose.yml            ← Local development environment\n```\n\n#### Infrastructure Starter Code\n\n**Redis Connection Manager** (`internal/ratelimit/storage/redis.go`):\n\n```go\npackage storage\n\nimport (\n    \"context\"\n    \"time\"\n    \"github.com/redis/go-redis/v9\"\n)\n\n// RedisConfig holds Redis connection configuration\ntype RedisConfig struct {\n    Addresses    []string      `json:\"addresses\"`\n    Password     string        `json:\"password\"`\n    DB           int           `json:\"db\"`\n    PoolSize     int           `json:\"pool_size\"`\n    ReadTimeout  time.Duration `json:\"read_timeout\"`\n    WriteTimeout time.Duration `json:\"write_timeout\"`\n    DialTimeout  time.Duration `json:\"dial_timeout\"`\n}\n\n// RedisStorage implements the Storage interface using Redis\ntype RedisStorage struct {\n    client redis.UniversalClient\n    config RedisConfig\n}\n\n// NewRedisStorage creates a new Redis storage backend with connection pooling\nfunc NewRedisStorage(config RedisConfig) (*RedisStorage, error) {\n    // TODO: Create Redis universal client (handles both single instance and cluster)\n    // TODO: Configure connection pool with proper timeouts\n    // TODO: Test connectivity with ping command\n    // TODO: Return configured storage instance\n}\n\n// CheckAndUpdate atomically checks current count and updates if limit allows\nfunc (r *RedisStorage) CheckAndUpdate(ctx context.Context, key string, limit int64, window time.Duration) (allowed bool, remaining int64, resetTime time.Time, err error) {\n    // TODO: This will be implemented with Lua scripts in Redis Backend Integration section\n    // For now, return placeholder values for basic connectivity testing\n    return true, limit-1, time.Now().Add(window), nil\n}\n```\n\n**Rate Limit Rule Configuration** (`internal/ratelimit/config/rules.go`):\n\n```go\npackage config\n\nimport (\n    \"time\"\n)\n\n// RateLimitRule defines a single rate limiting rule\ntype RateLimitRule struct {\n    ID          string        `json:\"id\" yaml:\"id\"`\n    Name        string        `json:\"name\" yaml:\"name\"`\n    KeyPattern  string        `json:\"key_pattern\" yaml:\"key_pattern\"`   // e.g., \"user:{user_id}\", \"ip:{ip_address}\"\n    Algorithm   string        `json:\"algorithm\" yaml:\"algorithm\"`        // \"token_bucket\", \"sliding_window_counter\", etc.\n    Limit       int64         `json:\"limit\" yaml:\"limit\"`               // Number of requests allowed\n    Window      time.Duration `json:\"window\" yaml:\"window\"`             // Time window for the limit\n    BurstLimit  int64         `json:\"burst_limit,omitempty\" yaml:\"burst_limit,omitempty\"` // For token bucket\n    Enabled     bool          `json:\"enabled\" yaml:\"enabled\"`\n    Priority    int           `json:\"priority\" yaml:\"priority\"`         // Higher priority rules checked first\n    CreatedAt   time.Time     `json:\"created_at\" yaml:\"created_at\"`\n    UpdatedAt   time.Time     `json:\"updated_at\" yaml:\"updated_at\"`\n}\n\n// RuleManager handles loading and updating rate limit rules\ntype RuleManager struct {\n    rules map[string]*RateLimitRule\n    // TODO: Add mutex for concurrent access\n    // TODO: Add file watcher for dynamic updates\n    // TODO: Add Redis pub/sub for distributed rule updates\n}\n\n// LoadRules loads rate limit rules from configuration file\nfunc (rm *RuleManager) LoadRules(configPath string) error {\n    // TODO: Read YAML configuration file\n    // TODO: Parse rules and validate configuration\n    // TODO: Store rules in memory with indexing by key pattern\n    // TODO: Set up file watcher for automatic reloading\n}\n\n// GetMatchingRules returns all rules that match the given request context\nfunc (rm *RuleManager) GetMatchingRules(userID, ipAddress, apiEndpoint string) []*RateLimitRule {\n    // TODO: Match request attributes against key patterns\n    // TODO: Return rules sorted by priority (highest first)\n    // TODO: Handle wildcard patterns and parameter substitution\n}\n```\n\n**Basic Rate Limiter Interface** (`internal/ratelimit/limiter.go`):\n\n```go\npackage ratelimit\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// RateLimitResult contains the result of a rate limit check\ntype RateLimitResult struct {\n    Allowed     bool          `json:\"allowed\"`\n    Remaining   int64         `json:\"remaining\"`\n    RetryAfter  time.Duration `json:\"retry_after,omitempty\"`\n    ResetTime   time.Time     `json:\"reset_time\"`\n    RuleID      string        `json:\"rule_id\"`\n    Algorithm   string        `json:\"algorithm\"`\n}\n\n// RateLimitRequest contains parameters for a rate limit check\ntype RateLimitRequest struct {\n    UserID      string `json:\"user_id,omitempty\"`\n    IPAddress   string `json:\"ip_address,omitempty\"`\n    APIEndpoint string `json:\"api_endpoint,omitempty\"`\n    UserAgent   string `json:\"user_agent,omitempty\"`\n    Tokens      int64  `json:\"tokens,omitempty\"` // Number of tokens to consume (default: 1)\n}\n\n// Limiter is the main interface for rate limiting operations\ntype Limiter interface {\n    // Check performs a rate limit check and updates counters if allowed\n    Check(ctx context.Context, req RateLimitRequest) (*RateLimitResult, error)\n    \n    // Preview checks rate limit status without updating counters\n    Preview(ctx context.Context, req RateLimitRequest) (*RateLimitResult, error)\n    \n    // Reset clears rate limit counters for the given request pattern\n    Reset(ctx context.Context, req RateLimitRequest) error\n}\n\n// DistributedLimiter implements the Limiter interface with Redis backend\ntype DistributedLimiter struct {\n    storage     Storage\n    ruleManager *config.RuleManager\n    localFallback Limiter // Used when Redis is unavailable\n}\n\n// NewDistributedLimiter creates a new distributed rate limiter\nfunc NewDistributedLimiter(storage Storage, ruleManager *config.RuleManager) *DistributedLimiter {\n    return &DistributedLimiter{\n        storage:     storage,\n        ruleManager: ruleManager,\n        // TODO: Initialize local fallback limiter\n    }\n}\n\n// Check performs distributed rate limiting with multi-tier evaluation\nfunc (dl *DistributedLimiter) Check(ctx context.Context, req RateLimitRequest) (*RateLimitResult, error) {\n    // TODO: Get matching rules from rule manager\n    // TODO: Evaluate rules in priority order with short-circuit logic\n    // TODO: For each rule, generate Redis key and check limit\n    // TODO: Return first rule that denies the request, or allow if all pass\n    // TODO: Handle Redis failures with local fallback\n}\n```\n\n#### Core Algorithm Skeleton\n\n**Token Bucket Algorithm** (`internal/ratelimit/algorithms/token_bucket.go`):\n\n```go\npackage algorithms\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// TokenBucketConfig defines parameters for token bucket algorithm\ntype TokenBucketConfig struct {\n    Capacity   int64         `json:\"capacity\"`    // Maximum tokens in bucket\n    RefillRate int64         `json:\"refill_rate\"` // Tokens added per second\n    Window     time.Duration `json:\"window\"`      // Window for rate calculation\n}\n\n// TokenBucketState represents current state stored in Redis\ntype TokenBucketState struct {\n    Tokens        int64     `json:\"tokens\"`\n    LastRefillTime int64    `json:\"last_refill_time\"` // Unix nanoseconds\n}\n\n// TokenBucket implements token bucket rate limiting algorithm\ntype TokenBucket struct {\n    config TokenBucketConfig\n    storage Storage\n}\n\n// CheckAndUpdate performs atomic check-and-update for token bucket algorithm\nfunc (tb *TokenBucket) CheckAndUpdate(ctx context.Context, key string, tokensRequested int64) (allowed bool, remaining int64, resetTime time.Time, err error) {\n    // TODO 1: Calculate current time in nanoseconds for precision\n    // TODO 2: Execute Lua script in Redis for atomic check-and-update:\n    //   - Get current bucket state (tokens, last_refill_time)\n    //   - Calculate tokens to add based on time elapsed and refill rate\n    //   - Add tokens to bucket, capping at capacity\n    //   - If sufficient tokens available, subtract requested tokens and allow\n    //   - If insufficient tokens, deny and calculate retry-after time\n    //   - Update bucket state with new token count and refill time\n    //   - Return result with remaining tokens and reset time\n    // TODO 3: Handle Redis errors with appropriate fallback strategy\n    // TODO 4: Parse Lua script response and construct result object\n    \n    // Placeholder implementation for initial testing\n    return true, tb.config.Capacity - tokensRequested, time.Now().Add(tb.config.Window), nil\n}\n```\n\n#### Language-Specific Implementation Hints\n\n**Redis Lua Script Execution in Go**:\n```go\n// Use redis.NewScript() to precompile Lua scripts for better performance\nscript := redis.NewScript(`\n    -- Your Lua script here\n    return {allowed, remaining, reset_time}\n`)\n\n// Execute with automatic retry and connection management\nresult, err := script.Run(ctx, redisClient, []string{key}, arg1, arg2).Result()\n```\n\n**Time Handling for Rate Limiting**:\n```go\n// Always use UTC to avoid timezone issues across distributed instances\nnow := time.Now().UTC()\n\n// Use UnixNano() for high-precision timestamps in Redis\ntimestamp := now.UnixNano()\n\n// Calculate time windows with proper boundary alignment\nwindowStart := now.Truncate(windowDuration)\nwindowEnd := windowStart.Add(windowDuration)\n```\n\n**Context and Timeout Management**:\n```go\n// Set reasonable timeouts for Redis operations (1-2ms typical)\nctx, cancel := context.WithTimeout(context.Background(), 5*time.Millisecond)\ndefer cancel()\n\n// Always check for context cancellation in long-running operations\nselect {\ncase <-ctx.Done():\n    return nil, ctx.Err()\ndefault:\n    // Continue with Redis operation\n}\n```\n\n#### Milestone Checkpoint: Basic Rate Limiter\n\nAfter implementing the foundational components, verify the system with these checkpoints:\n\n**Test 1: Redis Connectivity**\n```bash\ngo run cmd/server/main.go\n# Expected: Server starts without errors, connects to Redis\n# Check: Redis logs show successful connections from Go client\n```\n\n**Test 2: Basic Rate Limiting**\n```bash\n# Send requests to test endpoint\nfor i in {1..10}; do\n  curl -H \"X-User-ID: test-user\" http://localhost:8080/api/test\ndone\n# Expected: First N requests succeed, subsequent requests return 429 Too Many Requests\n# Check: Response includes X-RateLimit-* headers with correct values\n```\n\n**Test 3: Configuration Loading**\n```bash\n# Modify configs/rate_limits.yaml and restart server\n# Expected: New rules take effect without code changes\n# Check: Different endpoints have different rate limits as configured\n```\n\n**Common Issues and Debugging**:\n- **\"Connection refused\"**: Ensure Redis is running on expected port (6379)\n- **\"WRONGTYPE Operation\"**: Redis key collision with existing data, use `FLUSHDB` to clear\n- **\"Context deadline exceeded\"**: Increase Redis operation timeouts or check network latency\n- **\"Rate limit headers missing\"**: Ensure middleware is properly installed in HTTP handler chain\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones - this foundational understanding applies to rate limiting algorithms, multi-tier systems, Redis integration, sharding, and API design throughout the project.\n\n### Mental Model: The Traffic Control Center\n\nThink of our distributed rate limiter as a **city-wide traffic control center** coordinating traffic lights across an entire metropolitan area. Each traffic light (application instance) needs to make real-time decisions about allowing vehicles (requests) to pass through intersections (API endpoints). However, unlike independent traffic lights that only consider local conditions, our traffic control center must coordinate globally to prevent citywide congestion.\n\nThe traffic control center maintains a **shared understanding** of traffic flow across all intersections. When a major event creates a surge of vehicles heading downtown, every traffic light needs to know about the overall traffic load, not just what's happening at their local intersection. Some intersections might need stricter controls (per-user limits), while major highways require global coordination (system-wide limits). The control center must continue functioning even when communication to some traffic lights is temporarily disrupted (graceful degradation).\n\nThis analogy captures the essential challenge: **local decisions with global coordination**. Each application instance must make millisecond decisions about request acceptance while maintaining awareness of system-wide resource consumption patterns.\n\n### Functional Goals\n\nThe distributed rate limiter must provide comprehensive request quota enforcement across multiple application instances with precise control over different limiting strategies. These capabilities form the foundation for protecting system resources while maintaining fair access for legitimate users.\n\n#### Core Rate Limiting Capabilities\n\nOur system must support multiple **rate limiting algorithms** with different behavioral characteristics. The token bucket algorithm provides burst handling capabilities, allowing short periods of activity above the sustained rate while preventing long-term abuse. Users can temporarily exceed their baseline quota during legitimate usage spikes, but cannot sustain high request rates indefinitely. The sliding window counter algorithm offers memory-efficient approximate limiting with configurable accuracy trade-offs. This approach reduces memory overhead compared to exact tracking while maintaining reasonable accuracy for most use cases. The sliding window log algorithm provides precise request tracking with exact compliance checking, storing individual request timestamps to enable perfect accuracy at the cost of increased memory usage.\n\n| Algorithm | Accuracy | Memory Usage | Burst Handling | Use Case |\n|-----------|----------|--------------|----------------|----------|\n| Token Bucket | Good | Low | Excellent | API endpoints with bursty traffic |\n| Sliding Window Counter | Good | Low | Limited | High-traffic endpoints requiring efficiency |\n| Sliding Window Log | Perfect | High | None | Critical endpoints requiring exact limits |\n\nThe system must implement **multi-tier rate limiting** with hierarchical enforcement across different dimensions. Per-user limits prevent individual users from consuming excessive resources, with different quotas based on subscription tiers or usage patterns. Per-IP limits protect against unauthenticated abuse and distributed attacks, providing a safety net when user identification is unavailable or compromised. Per-API endpoint limits ensure that no single API can overwhelm system resources, with different thresholds based on endpoint complexity and resource requirements. Global system limits provide the ultimate protection against total system overload, aggregating usage across all users, IPs, and endpoints.\n\nThe tier evaluation must follow a **short-circuit strategy** where exceeding any tier immediately blocks the request without evaluating remaining tiers. This approach minimizes computational overhead while ensuring the most restrictive applicable limit takes precedence. The system must support configurable priority ordering, allowing administrators to specify whether user limits should be checked before IP limits or vice versa based on their specific threat model.\n\n#### Dynamic Configuration Management\n\nRate limit rules must be **dynamically configurable** without requiring application restarts or deployments. The `RuleManager` must support real-time rule updates, additions, and deletions through the management API. Rule changes must propagate to all application instances within a configurable time window, typically under 30 seconds for non-emergency changes and under 5 seconds for emergency rate limit adjustments.\n\n| Configuration Operation | Target Latency | Consistency Model | Rollback Support |\n|------------------------|----------------|-------------------|------------------|\n| Rule Creation | < 30 seconds | Eventually consistent | Full rollback |\n| Rule Modification | < 30 seconds | Eventually consistent | Previous version restore |\n| Emergency Rate Limit | < 5 seconds | Strong consistency | Manual override |\n| Rule Deletion | < 60 seconds | Eventually consistent | Soft delete with restore |\n\nThe system must support **rule pattern matching** using flexible key patterns that can incorporate user IDs, IP addresses, API endpoints, and custom attributes. Pattern matching should support wildcards, regular expressions, and hierarchical matching to enable sophisticated routing of requests to appropriate rate limit rules.\n\n#### Atomic Operations and Consistency\n\nAll rate limit checks must be **atomic operations** that prevent race conditions between checking current usage and updating counters. The `CheckAndUpdate` method must implement check-and-increment as a single atomic operation, ensuring that concurrent requests cannot bypass limits by checking usage simultaneously before any updates occur.\n\nThe system must maintain **eventual consistency** across the distributed cluster while providing strong consistency for individual rate limit decisions. When a request is approved and counted against a limit, that decision must be immediately reflected in subsequent rate limit checks for the same key, even under high concurrency.\n\n#### State Persistence and Recovery\n\nRate limit state must survive individual application instance failures and restarts. The Redis backend must maintain all rate limiting counters, token bucket states, and sliding window data with appropriate expiration policies to prevent indefinite memory growth.\n\nThe system must support **state recovery** mechanisms for reconstructing local fallback state from Redis data when application instances restart. This capability ensures that transitioning between Redis-backed and local fallback modes maintains reasonable accuracy rather than resetting all limits to their initial values.\n\n### Non-Functional Goals\n\nThe distributed rate limiter must meet stringent performance, reliability, and scalability requirements while maintaining operational simplicity and cost-effectiveness.\n\n#### Performance Requirements\n\nRate limit checks must complete with **sub-5 millisecond latency** for the 95th percentile under normal load conditions. This requirement ensures that rate limiting does not become a bottleneck in request processing pipelines. The system must support at least **100,000 rate limit checks per second per application instance** while maintaining this latency target.\n\n| Performance Metric | Target | Measurement Method |\n|-------------------|--------|-------------------|\n| P50 Latency | < 1ms | Request-response time for CheckAndUpdate |\n| P95 Latency | < 5ms | 95th percentile across all rate limit algorithms |\n| P99 Latency | < 10ms | Including Redis network round trips |\n| Throughput | 100K ops/sec | Per application instance sustained load |\n| Memory Usage | < 100MB | Per application instance excluding Redis |\n\nThe Redis integration must implement **connection pooling** with configurable pool sizes to minimize connection establishment overhead. Connection reuse must be balanced with connection health monitoring to prevent using stale or failed connections that would increase error rates.\n\n#### Reliability and Availability\n\nThe system must achieve **99.9% availability** for rate limiting decisions, measured as successful rate limit checks divided by total check attempts. This requirement must be met even during Redis node failures, network partitions, and planned maintenance activities.\n\n**Graceful degradation** must ensure that rate limiting continues functioning when Redis is unavailable, falling back to local per-instance limiting with reduced accuracy. The fallback mode should maintain at least 80% of the intended rate limiting effectiveness while preserving system stability.\n\n| Failure Scenario | Recovery Time | Degraded Capability | Maintained Capability |\n|------------------|---------------|--------------------|--------------------|\n| Single Redis Node | < 1 second | Cross-instance coordination | Per-instance limiting |\n| Redis Cluster | < 30 seconds | Global rate limits | Local fallback limits |\n| Network Partition | < 5 seconds | Distributed coordination | Independent operation |\n| Application Restart | < 10 seconds | Warm cache state | Cold start with Redis sync |\n\nThe system must implement **circuit breaker patterns** for Redis operations, automatically switching to local fallback mode when Redis error rates exceed configurable thresholds. Circuit breaker state must be shared across application instances to prevent cascading failures.\n\n#### Scalability Requirements\n\nThe system must **horizontally scale** to support at least 1000 application instances sharing rate limit state through the Redis backend. This scaling must be achieved through consistent hashing and sharding strategies that distribute load evenly across Redis nodes.\n\n**Hot key detection** must identify when specific rate limit keys experience disproportionate access patterns and automatically implement mitigation strategies such as key replication or local caching. The system should handle scenarios where a small number of users or API endpoints generate the majority of rate limit checks.\n\nRedis cluster scaling must support **dynamic node addition and removal** with minimal disruption to ongoing rate limiting operations. Consistent hashing with virtual nodes must minimize key redistribution when cluster topology changes occur.\n\n#### Monitoring and Observability\n\nThe system must provide comprehensive metrics for **rate limiting effectiveness, performance, and resource usage**. Metrics must be exported in Prometheus format with appropriate labels for filtering and aggregation across different dimensions.\n\n| Metric Category | Key Metrics | Labels |\n|----------------|-------------|---------|\n| Rate Limiting | Requests allowed/denied, limit utilization | algorithm, tier, rule_id, endpoint |\n| Performance | Check latency, throughput, error rate | instance, redis_node, operation |\n| Resource Usage | Redis memory, connection pool size, CPU | node, algorithm, key_pattern |\n| Health | Circuit breaker state, fallback mode | instance, failure_type, recovery_time |\n\nReal-time dashboards must display current rate limit utilization across all tiers and provide alerting when usage approaches configured thresholds. Historical data must be retained for capacity planning and pattern analysis.\n\n### Explicit Non-Goals\n\nTo maintain project scope and complexity at an intermediate level, several advanced features are explicitly excluded from this implementation. These non-goals help focus development efforts on core distributed rate limiting concepts while avoiding enterprise-grade complexity.\n\n#### Advanced Algorithm Features\n\n**Adaptive rate limiting** that automatically adjusts limits based on system load, response times, or external signals is not included. While adaptive limiting can provide superior resource utilization, it introduces significant complexity around feedback loops, oscillation prevention, and parameter tuning that would distract from learning core distributed systems concepts.\n\n**Machine learning-based anomaly detection** for identifying suspicious traffic patterns or predicting optimal rate limits is excluded. Such features require expertise in ML model training, feature engineering, and online learning systems that exceed the scope of an intermediate distributed systems project.\n\n**Geographic distribution** with multi-region rate limiting coordination is not implemented. While global rate limiting across continents presents interesting challenges around latency, consistency, and network partitions, it requires infrastructure complexity beyond the Redis cluster approach used here.\n\n#### Enterprise Integration Features\n\n**Authentication and authorization** for the rate limiting system itself is simplified. Production systems require sophisticated access controls, API key management, and integration with enterprise identity providers, but these concerns are orthogonal to rate limiting algorithms and distributed coordination.\n\n| Excluded Feature | Rationale | Alternative Approach |\n|-----------------|-----------|---------------------|\n| OAuth2/JWT Integration | Authentication complexity exceeds scope | Simple API key validation |\n| RBAC for Rate Limit Rules | Authorization logic unrelated to core learning | Basic admin/read-only roles |\n| Audit Logging | Compliance features beyond technical focus | Basic operation logging |\n| Encryption at Rest | Security implementation complexity | Redis AUTH password only |\n\n**Service mesh integration** with Istio, Envoy, or similar platforms is not provided. While transparent rate limiting through sidecar proxies offers operational advantages, it requires understanding service mesh architectures, Envoy filter development, and Kubernetes operators that distract from rate limiting fundamentals.\n\n**Database persistence** for rate limit rules and historical data is excluded in favor of file-based configuration and Redis storage. Database integration introduces schema design, migration management, and ORM complexity without teaching additional rate limiting concepts.\n\n#### Operational Complexity Features\n\n**Multi-tenancy** with strict isolation between different customer environments is not implemented. True multi-tenancy requires namespace isolation, resource quotas, and security boundaries that significantly complicate the architecture without adding educational value for rate limiting concepts.\n\n**Automatic scaling and provisioning** of the Redis cluster based on load patterns is excluded. While auto-scaling is valuable for production systems, it requires infrastructure automation, monitoring thresholds, and capacity planning logic that exceeds the project scope.\n\n**Disaster recovery and backup/restore** capabilities are simplified. Production systems require point-in-time recovery, cross-region replication, and automated failover procedures that involve significant operational complexity beyond rate limiting algorithms.\n\n> **Design Principle: Learning-Focused Scope**\n> These non-goals ensure that learners can focus on mastering distributed rate limiting concepts without being overwhelmed by peripheral enterprise features. Each excluded feature represents a potential future enhancement once core concepts are solidified.\n\n#### Performance and Scale Limitations\n\nThe system is designed for **medium-scale deployments** rather than hyperscale environments. Supporting millions of unique rate limit keys, petabytes of historical data, or tens of thousands of application instances would require specialized data structures, storage engines, and coordination protocols that exceed intermediate complexity.\n\n**Real-time analytics and complex aggregations** over rate limiting data are limited to basic usage metrics. Advanced analytics like percentile calculations, time-series forecasting, or correlation analysis with business metrics require specialized analytics databases and query engines.\n\n| Scale Limitation | Design Target | Beyond Scope |\n|-----------------|---------------|--------------|\n| Application Instances | 1,000 instances | 10,000+ instances |\n| Rate Limit Keys | 1M active keys | 100M+ keys |\n| Redis Cluster Size | 10-20 nodes | 100+ nodes |\n| Historical Retention | 30 days metrics | Long-term data warehouse |\n\nThese limitations ensure that the implementation remains comprehensible and deployable on modest infrastructure while teaching all essential distributed rate limiting concepts. Organizations requiring hyperscale capabilities can use this implementation as a foundation for understanding the principles before adopting specialized commercial solutions.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Redis Client | `go-redis/redis` with basic connection pooling | `go-redis/redis` with cluster support and sentinel |\n| Configuration | YAML files with `gopkg.in/yaml.v3` | etcd with watch-based dynamic updates |\n| Metrics | Prometheus `prometheus/client_golang` | Custom metrics with multiple exporters |\n| HTTP Framework | Standard `net/http` with `gorilla/mux` | Gin or Echo with middleware support |\n| Logging | `logrus` or `zap` structured logging | OpenTelemetry with distributed tracing |\n| Testing | Standard `testing` package with testify | Property-based testing with `gopter` |\n\nFor this intermediate-level project, we recommend the simple options to maintain focus on rate limiting concepts rather than framework complexity. The advanced options can be adopted later as operational requirements grow.\n\n#### Project Structure\n\n```\ndistributed-rate-limiter/\n├── cmd/\n│   ├── server/main.go              ← HTTP API server entry point\n│   ├── cli/main.go                 ← Command-line management tool\n│   └── dashboard/main.go           ← Real-time dashboard server\n├── internal/\n│   ├── limiter/                    ← Core rate limiting logic\n│   │   ├── distributed.go          ← DistributedLimiter implementation\n│   │   ├── algorithms/             ← Rate limiting algorithms\n│   │   │   ├── token_bucket.go     ← TokenBucket implementation\n│   │   │   ├── sliding_window.go   ← Sliding window algorithms\n│   │   │   └── interface.go        ← Common algorithm interface\n│   │   └── fallback.go             ← Local fallback limiter\n│   ├── storage/                    ← Redis backend integration\n│   │   ├── redis.go                ← RedisStorage implementation\n│   │   ├── scripts.go              ← Lua script management\n│   │   └── sharding.go             ← Consistent hashing logic\n│   ├── config/                     ← Configuration management\n│   │   ├── rules.go                ← RuleManager implementation\n│   │   ├── loader.go               ← Configuration file loading\n│   │   └── validation.go           ← Rule validation logic\n│   ├── api/                        ← HTTP API handlers\n│   │   ├── handlers.go             ← Rate limit check endpoints\n│   │   ├── management.go           ← Rule management endpoints\n│   │   └── middleware.go           ← Rate limit headers middleware\n│   └── metrics/                    ← Monitoring and metrics\n│       ├── collector.go            ← Prometheus metrics\n│       └── dashboard.go            ← Real-time dashboard data\n├── configs/\n│   ├── rules.yaml                  ← Rate limit rule definitions\n│   └── redis.yaml                  ← Redis cluster configuration\n├── scripts/\n│   └── lua/                        ← Redis Lua scripts\n│       ├── token_bucket.lua        ← Token bucket algorithm\n│       ├── sliding_window.lua      ← Sliding window algorithm\n│       └── utils.lua               ← Common script utilities\n└── tests/\n    ├── integration/                ← Multi-instance integration tests\n    ├── chaos/                      ← Chaos engineering tests\n    └── benchmarks/                 ← Performance benchmarks\n```\n\nThis structure separates concerns clearly while maintaining the `internal/` convention for non-exported packages. Each package has a single responsibility and minimal dependencies on other internal packages.\n\n#### Core Configuration Structures\n\n```go\n// RedisConfig defines connection parameters for Redis cluster integration\ntype RedisConfig struct {\n    Addresses     []string      `yaml:\"addresses\"`     // Redis cluster node addresses\n    Password      string        `yaml:\"password\"`      // AUTH password for Redis\n    DB            int           `yaml:\"db\"`            // Database number (0-15)\n    PoolSize      int           `yaml:\"pool_size\"`     // Connection pool size per node\n    ReadTimeout   time.Duration `yaml:\"read_timeout\"`  // Socket read timeout\n    WriteTimeout  time.Duration `yaml:\"write_timeout\"` // Socket write timeout\n    DialTimeout   time.Duration `yaml:\"dial_timeout\"`  // Connection establishment timeout\n}\n\n// NewRedisConfig creates a RedisConfig with sensible defaults\nfunc NewRedisConfig() RedisConfig {\n    return RedisConfig{\n        Addresses:    []string{\"localhost:6379\"},\n        Password:     \"\",\n        DB:           0,\n        PoolSize:     DEFAULT_POOL_SIZE,\n        ReadTimeout:  DEFAULT_TIMEOUT,\n        WriteTimeout: DEFAULT_TIMEOUT,\n        DialTimeout:  DEFAULT_TIMEOUT,\n    }\n}\n```\n\n#### Rate Limit Rule Definition\n\n```go\n// RateLimitRule defines a rate limiting policy that can be applied to requests\ntype RateLimitRule struct {\n    ID          string        `yaml:\"id\" json:\"id\"`                    // Unique rule identifier\n    Name        string        `yaml:\"name\" json:\"name\"`                // Human-readable rule name\n    KeyPattern  string        `yaml:\"key_pattern\" json:\"key_pattern\"`  // Pattern for matching requests\n    Algorithm   string        `yaml:\"algorithm\" json:\"algorithm\"`      // Algorithm: \"token_bucket\", \"sliding_window_counter\", \"sliding_window_log\"\n    Limit       int64         `yaml:\"limit\" json:\"limit\"`              // Maximum requests per window\n    Window      time.Duration `yaml:\"window\" json:\"window\"`            // Time window duration\n    BurstLimit  int64         `yaml:\"burst_limit\" json:\"burst_limit\"`  // Maximum burst size (token bucket only)\n    Enabled     bool          `yaml:\"enabled\" json:\"enabled\"`          // Whether rule is active\n    Priority    int           `yaml:\"priority\" json:\"priority\"`        // Evaluation priority (higher = first)\n    CreatedAt   time.Time     `yaml:\"created_at\" json:\"created_at\"`    // Rule creation timestamp\n    UpdatedAt   time.Time     `yaml:\"updated_at\" json:\"updated_at\"`    // Last modification timestamp\n}\n```\n\n#### Essential Constants\n\n```go\nconst (\n    // Redis connection pool configuration\n    DEFAULT_POOL_SIZE = 10               // Connections per Redis node\n    DEFAULT_TIMEOUT   = 5 * time.Millisecond  // Redis operation timeout\n    \n    // Rule priority levels for common use cases\n    PRIORITY_HIGH = 100                  // Emergency rate limits\n    PRIORITY_LOW  = 1                   // Default background limits\n    \n    // Rate limiting algorithms\n    ALGORITHM_TOKEN_BUCKET       = \"token_bucket\"\n    ALGORITHM_SLIDING_WINDOW_LOG = \"sliding_window_log\"\n    ALGORITHM_SLIDING_COUNTER    = \"sliding_window_counter\"\n    \n    // Rate limit header names\n    HEADER_LIMIT_REMAINING = \"X-RateLimit-Remaining\"\n    HEADER_LIMIT_RESET     = \"X-RateLimit-Reset\"\n    HEADER_RETRY_AFTER     = \"Retry-After\"\n)\n```\n\n#### Milestone Checkpoints\n\n**After completing Milestone 1 (Rate Limiting Algorithms):**\n- Run `go test ./internal/limiter/algorithms/...` - all algorithm tests should pass\n- Create a simple CLI tool that can test each algorithm with configurable parameters\n- Verify that token bucket allows bursts above sustained rate but prevents long-term abuse\n- Test sliding window algorithms with requests clustered at window boundaries\n- Expected behavior: Token bucket should allow 10 requests immediately if capacity=10, then throttle to refill rate\n\n**After completing Milestone 2 (Multi-tier Rate Limiting):**\n- Run integration test with simultaneous per-user and global limits\n- Verify that the most restrictive limit takes precedence using short-circuit evaluation\n- Test rule pattern matching with wildcards and regular expressions\n- Expected behavior: Request matching both user limit (100/hour) and global limit (1000/hour) should be limited by user quota first\n\n**After completing Milestone 3 (Redis Backend Integration):**\n- Start Redis locally and run `go test ./internal/storage/...`\n- Test graceful degradation by stopping Redis mid-test and verifying local fallback\n- Verify that concurrent requests don't bypass limits using atomic Lua scripts\n- Expected behavior: Two application instances should coordinate limits correctly through Redis\n\n**After completing Milestone 4 (Consistent Hashing & Sharding):**\n- Deploy Redis cluster with 3 nodes and test key distribution\n- Add/remove Redis nodes and verify minimal key redistribution\n- Test hot key detection with skewed request patterns\n- Expected behavior: Keys should be evenly distributed, with <10% redistribution when adding nodes\n\n**After completing Milestone 5 (Rate Limit API & Dashboard):**\n- Start the HTTP API server and test rule management endpoints with curl\n- Verify rate limit headers are included in all API responses\n- Test the real-time dashboard updates as rate limits are consumed\n- Expected behavior: Dashboard should show live usage percentages updating every second\n\nEach milestone should build incrementally, with later milestones reusing and extending earlier implementations rather than replacing them.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** All milestones - this architectural foundation enables rate limiting algorithms (Milestone 1), multi-tier evaluation (Milestone 2), Redis backend integration (Milestone 3), consistent hashing and sharding (Milestone 4), and API management (Milestone 5).\n\nThe distributed rate limiter requires careful coordination between multiple components to maintain consistent quota enforcement across application instances while handling failures gracefully. Think of this system like a **coordinated nightclub security operation** - instead of just one bouncer at the door counting patrons, you have multiple entry points (application instances) that must communicate with a central dispatch system (Redis) to maintain an accurate headcount and enforce capacity limits consistently across all entrances.\n\nThis mental model captures the core challenge: each application instance acts as an independent bouncer checking IDs and counting entries, but they all must coordinate through a shared communication system to ensure the overall venue doesn't exceed capacity. When the radio system goes down (Redis failure), each bouncer falls back to local counting with reduced accuracy, but the doors stay open.\n\nThe architecture balances several competing concerns through a layered design. At the foundation, Redis provides atomic operations and cluster-wide state sharing. Above that, the rate limiting algorithms implement different quota enforcement strategies with varying accuracy and performance trade-offs. The multi-tier evaluation layer applies hierarchical limits across user, IP, API, and global dimensions. Finally, the management layer provides dynamic configuration and real-time monitoring capabilities.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n> **Key Architectural Principle**: The system maintains functionality even during partial failures through graceful degradation. Each component can operate in a reduced-capability mode when dependencies are unavailable, preventing cascading failures while maintaining core rate limiting capabilities.\n\n### Component Overview\n\nThe distributed rate limiter consists of seven primary components, each with distinct responsibilities and failure modes. Understanding these components and their interactions is crucial for implementing a robust system that handles the complexities of distributed quota enforcement.\n\n#### DistributedLimiter Component\n\nThe `DistributedLimiter` serves as the orchestration layer, coordinating multi-tier rate limit evaluation and managing fallback strategies during Redis failures. This component implements the primary `Check()` method that application code calls to verify whether a request should be allowed or rejected.\n\n| Responsibility | Description | Failure Behavior |\n|---------------|-------------|------------------|\n| Request Orchestration | Coordinates multi-tier rate limit checks with short-circuit evaluation | Continues with remaining tiers if one fails |\n| Algorithm Selection | Routes requests to appropriate algorithm implementation based on rule configuration | Falls back to token bucket if specified algorithm unavailable |\n| Fallback Coordination | Switches to local rate limiting when Redis becomes unavailable | Maintains reduced accuracy with per-instance limits |\n| Result Aggregation | Combines results from multiple tiers into final allow/deny decision | Uses most restrictive result from successful tier checks |\n\nThe `DistributedLimiter` maintains no persistent state itself - it acts purely as a coordinator that delegates to storage backends and algorithm implementations. This stateless design ensures that multiple application instances can run identical rate limiting logic without coordination overhead.\n\n> **Decision: Stateless Coordinator Design**\n> - **Context**: Need to run identical rate limiting logic across multiple application instances\n> - **Options Considered**: Stateful coordinator with instance coordination, stateless coordinator with shared storage, leader-election based coordination\n> - **Decision**: Stateless coordinator with all persistent state in Redis\n> - **Rationale**: Eliminates complex coordination between application instances, simplifies deployment and scaling, enables any instance to handle any request\n> - **Consequences**: All state must be externalized to Redis, requires careful atomic operation design, enables horizontal scaling without instance affinity\n\n#### Storage Backend Components\n\nThe storage layer abstracts persistent state management through a common `Storage` interface, with `RedisStorage` as the primary implementation and local storage options for fallback scenarios.\n\n**RedisStorage Component**\n\n`RedisStorage` implements distributed state management using Redis cluster operations with atomic Lua scripts. This component handles the complexity of coordinating rate limit state across multiple application instances while maintaining consistency guarantees.\n\n| Feature | Implementation | Consistency Guarantee |\n|---------|----------------|---------------------|\n| Atomic Operations | Lua scripts for check-and-update | Strong consistency within single Redis node |\n| Connection Pooling | Redis universal client with configurable pool size | Automatic failover between cluster nodes |\n| Cluster Support | Consistent hashing for key distribution | Eventually consistent across cluster during splits |\n| Health Monitoring | Periodic ping with circuit breaker logic | Automatic fallback when health check fails |\n\nThe Redis integration uses Lua scripts to ensure atomicity of complex operations like token bucket refill-and-consume cycles. Without atomic operations, race conditions between multiple application instances could lead to incorrect quota enforcement - imagine two bouncers at different doors both thinking they're letting in the \"last\" patron simultaneously.\n\n**Local Storage Fallback**\n\nWhen Redis becomes unavailable, the system maintains functionality through local storage implementations that provide per-instance rate limiting. While accuracy is reduced (each instance enforces limits independently), the system continues protecting against abuse rather than failing open.\n\n| Storage Type | Use Case | Accuracy Trade-off |\n|--------------|----------|-------------------|\n| In-Memory Map | Development and fallback | Lost on process restart |\n| Local Database | Persistent fallback | Per-instance limits only |\n| File-based Storage | Simple persistence | High latency, suitable for low-traffic fallback |\n\n#### Algorithm Implementation Components\n\nEach rate limiting algorithm is implemented as a separate component that works with the storage abstraction. This design allows mixing different algorithms for different use cases within the same deployment.\n\n**TokenBucket Component**\n\nThe `TokenBucket` implementation manages refill rates and burst capacity using atomic compare-and-swap operations in Redis. The algorithm maintains a bucket state with current token count and last refill timestamp.\n\n| State Field | Purpose | Update Frequency |\n|-------------|---------|-----------------|\n| tokens | Current available tokens | Every request (decremented) and refill interval |\n| last_refill_time | Nanosecond timestamp of last refill | Every refill calculation |\n| capacity | Maximum bucket size | Configuration only |\n| refill_rate | Tokens added per time window | Configuration only |\n\nThe token bucket algorithm allows controlled bursts above the sustained rate, making it ideal for APIs that need to handle occasional spikes while maintaining average throughput limits. The refill calculation happens atomically with token consumption to prevent race conditions.\n\n**SlidingWindow Implementations**\n\nBoth sliding window counter and sliding window log algorithms track request patterns over time windows, but with different memory and accuracy trade-offs.\n\n| Algorithm | Memory Usage | Accuracy | Best For |\n|-----------|--------------|----------|----------|\n| Sliding Window Counter | O(1) per key | Approximate (up to 2x limit at boundaries) | High-traffic APIs needing memory efficiency |\n| Sliding Window Log | O(requests) per key | Exact | Critical APIs requiring precise enforcement |\n\n#### RuleManager Component\n\nThe `RuleManager` handles dynamic configuration of rate limiting rules, supporting real-time updates without service restarts. Rules are organized by priority and matching patterns to enable complex hierarchical rate limiting scenarios.\n\n| Rule Matching | Pattern Type | Example | Priority Handling |\n|---------------|--------------|---------|------------------|\n| User ID | Exact match | `user:12345` | Higher priority overrides lower |\n| IP Address | CIDR blocks | `192.168.1.0/24` | More specific patterns win |\n| API Endpoint | Path patterns | `/api/v1/upload/*` | Endpoint-specific before global |\n| User Agent | Regex patterns | `bot\\|crawler` | Pattern complexity affects performance |\n\nThe rule evaluation engine uses short-circuit evaluation to minimize Redis operations - once a rate limit is exceeded, evaluation stops immediately rather than checking remaining tiers.\n\n> **Decision: Priority-Based Rule Evaluation**\n> - **Context**: Need to handle overlapping rate limit rules for the same request\n> - **Options Considered**: First-match wins, most restrictive wins, priority-based evaluation\n> - **Decision**: Priority-based evaluation with configurable rule precedence\n> - **Rationale**: Provides predictable behavior for complex rule sets, allows override patterns for special cases, enables debugging through explicit rule ordering\n> - **Consequences**: Requires careful priority assignment, evaluation order affects performance, enables sophisticated rate limiting policies\n\n#### ConsistentHashRing Component\n\nFor horizontally scaled Redis deployments, the `ConsistentHashRing` distributes rate limiting keys across multiple Redis nodes while minimizing redistribution during topology changes.\n\n| Hash Ring Feature | Purpose | Implementation |\n|-------------------|---------|----------------|\n| Virtual Nodes | Uniform distribution | 100-500 virtual nodes per physical node |\n| Key Placement | Consistent node assignment | SHA-256 hash of rate limit key |\n| Node Addition | Minimal key movement | Only keys between old and new node positions move |\n| Hot Key Detection | Load balancing | Monitor request frequency per key |\n\nThe consistent hashing approach ensures that adding or removing Redis nodes only affects a small fraction of keys, maintaining cache locality and avoiding thundering herd problems during topology changes.\n\n#### Metrics and Monitoring Components\n\nReal-time observability is crucial for distributed rate limiting since quota violations may indicate either legitimate traffic spikes or attack patterns.\n\n**MetricsCollector Component**\n\n| Metric Category | Examples | Collection Method |\n|----------------|----------|------------------|\n| Rate Limit Decisions | Allowed/denied counts per rule | In-process counters |\n| Algorithm Performance | Token bucket refill latency | Histogram metrics |\n| Storage Operations | Redis operation latency/errors | Redis client middleware |\n| Hot Key Detection | Request frequency distribution | Sliding window counters |\n\n**Dashboard Components**\n\nThe real-time dashboard requires efficient data streaming to avoid overwhelming the rate limiting system with monitoring queries.\n\n| Dashboard Feature | Data Source | Update Frequency |\n|-------------------|-------------|------------------|\n| Current Usage Gauges | Redis state queries | 1 second intervals |\n| Historical Trends | Time-series metrics | 10 second aggregation |\n| Rule Configuration | RuleManager API | On-demand with WebSocket push |\n| Health Status | Component health checks | 5 second intervals |\n\n> **Critical Design Insight**: The monitoring system must not become a bottleneck or single point of failure for the rate limiting system. Dashboard queries are rate-limited and use read replicas where possible to avoid affecting production rate limiting performance.\n\n### Recommended Module Structure\n\nA well-organized module structure enables parallel development of different components while maintaining clear dependency boundaries. The structure follows domain-driven design principles with infrastructure concerns separated from business logic.\n\n```\ndistributed-rate-limiter/\n├── cmd/\n│   ├── server/                    # HTTP API server\n│   │   └── main.go               # Server entry point with configuration\n│   ├── cli/                      # Management CLI tool\n│   │   └── main.go               # CLI for rule management and testing\n│   └── dashboard/                # Dashboard web server\n│       └── main.go               # Dashboard with WebSocket endpoints\n├── internal/\n│   ├── limiter/                  # Core rate limiting logic\n│   │   ├── limiter.go           # DistributedLimiter implementation\n│   │   ├── limiter_test.go      # Multi-instance integration tests\n│   │   ├── request.go           # RateLimitRequest and RateLimitResult\n│   │   └── interface.go         # Limiter interface definition\n│   ├── algorithms/               # Rate limiting algorithm implementations\n│   │   ├── tokenbucket/         # Token bucket algorithm\n│   │   │   ├── tokenbucket.go   # TokenBucket component\n│   │   │   ├── tokenbucket_test.go # Algorithm correctness tests\n│   │   │   └── state.go         # TokenBucketState and TokenBucketConfig\n│   │   ├── slidingwindow/       # Sliding window algorithms\n│   │   │   ├── counter.go       # Sliding window counter implementation\n│   │   │   ├── log.go           # Sliding window log implementation\n│   │   │   └── window_test.go   # Boundary condition tests\n│   │   └── interface.go         # Common algorithm interface\n│   ├── storage/                  # Storage backend implementations\n│   │   ├── redis/               # Redis backend\n│   │   │   ├── storage.go       # RedisStorage implementation\n│   │   │   ├── scripts.go       # Lua scripts for atomic operations\n│   │   │   ├── pool.go          # Connection pool management\n│   │   │   ├── health.go        # Health checking and circuit breaker\n│   │   │   └── redis_test.go    # Redis integration tests\n│   │   ├── local/               # Local storage fallback\n│   │   │   ├── memory.go        # In-memory map storage\n│   │   │   └── file.go          # File-based persistence\n│   │   └── interface.go         # Storage interface definition\n│   ├── config/                   # Configuration management\n│   │   ├── rules.go             # RuleManager implementation\n│   │   ├── loader.go            # Configuration file loading\n│   │   ├── validation.go        # Rule validation logic\n│   │   └── config_test.go       # Rule matching tests\n│   ├── sharding/                 # Consistent hashing and node management\n│   │   ├── hasher.go            # ConsistentHashRing implementation\n│   │   ├── nodes.go             # Node health and topology management\n│   │   ├── rebalancer.go        # Hot key detection and rebalancing\n│   │   └── sharding_test.go     # Hash distribution tests\n│   ├── metrics/                  # Observability and monitoring\n│   │   ├── collector.go         # MetricsCollector component\n│   │   ├── dashboard.go         # Dashboard data aggregation\n│   │   └── alerts.go            # Alerting logic for quota violations\n│   ├── api/                      # HTTP API handlers\n│   │   ├── handlers/            # Request handlers\n│   │   │   ├── ratelimit.go     # Rate limit check endpoints\n│   │   │   ├── rules.go         # Rule management CRUD endpoints\n│   │   │   └── metrics.go       # Metrics and dashboard endpoints\n│   │   ├── middleware/          # HTTP middleware\n│   │   │   ├── headers.go       # Rate limit header injection\n│   │   │   ├── logging.go       # Request logging and tracing\n│   │   │   └── recovery.go      # Panic recovery middleware\n│   │   └── server.go            # HTTP server setup and routing\n│   └── util/                     # Shared utilities\n│       ├── time.go              # Time utilities for clock skew handling\n│       ├── hash.go              # Consistent hashing utilities\n│       └── testing.go           # Test helpers for integration tests\n├── pkg/                          # Public API packages\n│   ├── client/                  # Go client library\n│   │   ├── client.go            # HTTP client for rate limit checks\n│   │   └── client_test.go       # Client integration tests\n│   └── types/                   # Shared type definitions\n│       ├── rules.go             # RateLimitRule and related types\n│       ├── results.go           # RateLimitResult and error types\n│       └── config.go            # Configuration structures\n├── configs/                      # Configuration file examples\n│   ├── rules.yaml               # Example rate limiting rules\n│   ├── redis.yaml               # Redis cluster configuration\n│   └── server.yaml              # Server configuration\n├── scripts/                      # Deployment and development scripts\n│   ├── setup-redis.sh           # Redis cluster setup script\n│   ├── load-test.sh             # Rate limiter load testing\n│   └── migrate-rules.sh         # Rule migration utilities\n├── deployments/                  # Kubernetes and Docker configurations\n│   ├── k8s/                     # Kubernetes manifests\n│   └── docker/                  # Docker compose configurations\n└── docs/                         # Additional documentation\n    ├── algorithms.md            # Algorithm comparison and selection guide\n    ├── deployment.md            # Production deployment guide\n    └── troubleshooting.md       # Common issues and solutions\n```\n\n#### Module Dependency Guidelines\n\nThe module structure enforces clear dependency boundaries to prevent circular imports and enable independent testing of components.\n\n| Layer | Allowed Dependencies | Prohibited Dependencies |\n|-------|---------------------|------------------------|\n| `cmd/` packages | Any internal package | None - these are entry points |\n| `internal/limiter/` | `algorithms/`, `storage/`, `config/`, `metrics/` | `api/`, `cmd/` - business logic independent of transport |\n| `internal/algorithms/` | `storage/` interface only | Concrete storage implementations |\n| `internal/storage/` | `util/` only | Algorithm or limiter packages |\n| `internal/config/` | `util/`, `pkg/types/` | Storage or algorithm implementations |\n| `internal/api/` | `limiter/`, `config/`, `metrics/` | Storage implementations directly |\n\n> **Decision: Hexagonal Architecture with Interface Boundaries**\n> - **Context**: Need to enable parallel development and independent testing of components\n> - **Options Considered**: Layered architecture, microservices with RPC, hexagonal architecture with interfaces\n> - **Decision**: Hexagonal architecture with storage and algorithm abstractions\n> - **Rationale**: Enables testing without Redis dependencies, supports multiple storage backends, allows algorithm experimentation without changing core logic\n> - **Consequences**: Requires discipline to maintain interface boundaries, adds abstraction overhead, enables comprehensive unit testing\n\n#### Configuration Management Structure\n\nRate limiting rules require dynamic updates without service restarts, necessitating a structured approach to configuration management.\n\n| Configuration Type | File Location | Update Method | Validation |\n|-------------------|---------------|---------------|------------|\n| Rate Limit Rules | `configs/rules.yaml` | HTTP API with immediate propagation | Schema validation + rule conflict detection |\n| Redis Configuration | `configs/redis.yaml` | Environment variables + config file | Connection testing during startup |\n| Server Settings | `configs/server.yaml` | Environment variables only | Port availability and permission checks |\n| Algorithm Parameters | Embedded in rules | Rule update API | Algorithm-specific parameter validation |\n\nThe configuration system supports environment variable overrides for deployment-specific settings while maintaining rule definitions in version-controlled YAML files.\n\n### Common Pitfalls\n\nUnderstanding common mistakes in distributed rate limiter architecture helps avoid subtle bugs that only manifest under high load or failure conditions.\n\n⚠️ **Pitfall: Circular Dependencies Between Components**\n\nMany implementations create circular import cycles by having storage components depend on algorithm implementations while algorithms depend on storage interfaces. This typically happens when trying to implement algorithm-specific optimizations in the storage layer.\n\n**Why it's wrong**: Circular dependencies prevent independent testing and make the codebase fragile to changes. They also indicate that separation of concerns is violated.\n\n**How to fix**: Use dependency inversion with interfaces. Storage components should only know about generic operations, while algorithms implement their logic using storage interface methods.\n\n⚠️ **Pitfall: Blocking Operations in Request Path**\n\nSynchronous Redis operations in the request handling path can cause cascading failures when Redis latency spikes. This is especially problematic when checking multiple rate limit tiers sequentially.\n\n**Why it's wrong**: A slow Redis query blocks the entire request, leading to thread pool exhaustion and service unavailability even when Redis is only temporarily slow.\n\n**How to fix**: Use timeouts for all Redis operations, implement circuit breaker patterns, and provide local fallback that can execute when Redis operations timeout.\n\n⚠️ **Pitfall: Inconsistent Clock Sources**\n\nUsing different time sources (system clock vs Redis TIME command) across components leads to inconsistent rate limit window calculations and can cause requests to be incorrectly allowed or denied.\n\n**Why it's wrong**: Clock skew between application instances and Redis nodes causes time-based calculations to diverge, leading to unpredictable rate limiting behavior.\n\n**How to fix**: Use a consistent time source strategy - either always use Redis TIME command for distributed consistency, or ensure NTP synchronization across all nodes and use local clocks consistently.\n\n⚠️ **Pitfall: Missing Graceful Degradation Strategy**\n\nFailing hard when Redis is unavailable means rate limiting becomes a single point of failure that can take down the entire application.\n\n**Why it's wrong**: Rate limiting is a protection mechanism, not a core business function. Its unavailability should not prevent the application from functioning.\n\n**How to fix**: Implement local fallback storage that provides reduced-accuracy rate limiting when Redis is unavailable. Design the system to \"fail open\" with monitoring rather than \"fail closed\" and break the application.\n\n### Implementation Guidance\n\nThis section provides the foundational code structure and infrastructure components needed to implement the distributed rate limiter architecture. The code focuses on providing complete, working infrastructure that learners can build upon rather than implementing the core rate limiting algorithms themselves.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|----------------|\n| Redis Client | `github.com/go-redis/redis/v8` with single instance | `github.com/go-redis/redis/v8` with cluster support |\n| HTTP Framework | Standard `net/http` with custom routing | `github.com/gin-gonic/gin` for middleware ecosystem |\n| Configuration | YAML files with `gopkg.in/yaml.v3` | `github.com/spf13/viper` for multiple formats |\n| Metrics | Simple counters with `expvar` | `github.com/prometheus/client_golang` |\n| Logging | Standard `log` package | `github.com/sirupsen/logrus` with structured logging |\n| Testing | Standard `testing` package | `github.com/stretchr/testify` for assertions |\n\n#### Recommended File Structure Setup\n\nStart by creating the basic directory structure and go module:\n\n```bash\nmkdir distributed-rate-limiter\ncd distributed-rate-limiter\ngo mod init github.com/yourusername/distributed-rate-limiter\n```\n\nCreate the core directories:\n```bash\nmkdir -p {cmd/{server,cli,dashboard},internal/{limiter,algorithms/{tokenbucket,slidingwindow},storage/{redis,local},config,sharding,metrics,api/{handlers,middleware},util},pkg/{client,types},configs,scripts,deployments/{k8s,docker},docs}\n```\n\n#### Infrastructure Starter Code\n\n**Complete Redis Configuration and Connection Management**\n\n`internal/storage/redis/config.go`:\n```go\npackage redis\n\nimport (\n    \"context\"\n    \"crypto/tls\"\n    \"time\"\n    \"github.com/go-redis/redis/v8\"\n)\n\n// RedisConfig holds all Redis connection and operation parameters\ntype RedisConfig struct {\n    // Addresses contains Redis node addresses for cluster or single instance\n    Addresses []string `yaml:\"addresses\" json:\"addresses\"`\n    \n    // Password for Redis authentication\n    Password string `yaml:\"password\" json:\"password\"`\n    \n    // DB database number (ignored in cluster mode)\n    DB int `yaml:\"db\" json:\"db\"`\n    \n    // PoolSize maximum number of socket connections\n    PoolSize int `yaml:\"pool_size\" json:\"pool_size\"`\n    \n    // ReadTimeout for socket reads\n    ReadTimeout time.Duration `yaml:\"read_timeout\" json:\"read_timeout\"`\n    \n    // WriteTimeout for socket writes\n    WriteTimeout time.Duration `yaml:\"write_timeout\" json:\"write_timeout\"`\n    \n    // DialTimeout for establishing new connections\n    DialTimeout time.Duration `yaml:\"dial_timeout\" json:\"dial_timeout\"`\n    \n    // EnableTLS enables TLS encryption\n    EnableTLS bool `yaml:\"enable_tls\" json:\"enable_tls\"`\n    \n    // TLSConfig for custom TLS settings\n    TLSConfig *tls.Config `yaml:\"-\" json:\"-\"`\n}\n\n// DefaultRedisConfig returns configuration with sensible defaults\nfunc DefaultRedisConfig() RedisConfig {\n    return RedisConfig{\n        Addresses:    []string{\"localhost:6379\"},\n        Password:     \"\",\n        DB:           0,\n        PoolSize:     DEFAULT_POOL_SIZE,\n        ReadTimeout:  DEFAULT_TIMEOUT,\n        WriteTimeout: DEFAULT_TIMEOUT,\n        DialTimeout:  DEFAULT_TIMEOUT,\n        EnableTLS:    false,\n    }\n}\n\n// Validate checks configuration parameters for correctness\nfunc (c RedisConfig) Validate() error {\n    if len(c.Addresses) == 0 {\n        return errors.New(\"at least one Redis address required\")\n    }\n    \n    if c.PoolSize <= 0 {\n        return errors.New(\"pool_size must be positive\")\n    }\n    \n    if c.ReadTimeout <= 0 || c.WriteTimeout <= 0 || c.DialTimeout <= 0 {\n        return errors.New(\"all timeout values must be positive\")\n    }\n    \n    return nil\n}\n```\n\n**Complete Redis Storage Implementation with Health Checking**\n\n`internal/storage/redis/storage.go`:\n```go\npackage redis\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/go-redis/redis/v8\"\n)\n\n// RedisStorage implements distributed rate limiting storage using Redis\ntype RedisStorage struct {\n    client redis.UniversalClient\n    config RedisConfig\n    \n    // Health checking\n    healthy    bool\n    healthLock sync.RWMutex\n    lastCheck  time.Time\n    \n    // Circuit breaker state\n    failureCount int\n    lastFailure  time.Time\n}\n\n// NewRedisStorage creates Redis storage with connection pooling and health monitoring\nfunc NewRedisStorage(config RedisConfig) (*RedisStorage, error) {\n    if err := config.Validate(); err != nil {\n        return nil, fmt.Errorf(\"invalid Redis configuration: %w\", err)\n    }\n    \n    // Create universal client (works with both single instance and cluster)\n    var client redis.UniversalClient\n    \n    if len(config.Addresses) == 1 {\n        // Single instance mode\n        client = redis.NewClient(&redis.Options{\n            Addr:         config.Addresses[0],\n            Password:     config.Password,\n            DB:           config.DB,\n            PoolSize:     config.PoolSize,\n            ReadTimeout:  config.ReadTimeout,\n            WriteTimeout: config.WriteTimeout,\n            DialTimeout:  config.DialTimeout,\n            TLSConfig:    config.TLSConfig,\n        })\n    } else {\n        // Cluster mode\n        client = redis.NewClusterClient(&redis.ClusterOptions{\n            Addrs:        config.Addresses,\n            Password:     config.Password,\n            PoolSize:     config.PoolSize,\n            ReadTimeout:  config.ReadTimeout,\n            WriteTimeout: config.WriteTimeout,\n            DialTimeout:  config.DialTimeout,\n            TLSConfig:    config.TLSConfig,\n        })\n    }\n    \n    storage := &RedisStorage{\n        client:  client,\n        config:  config,\n        healthy: true,\n    }\n    \n    // Verify connection\n    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n    defer cancel()\n    \n    if err := storage.client.Ping(ctx).Err(); err != nil {\n        return nil, fmt.Errorf(\"failed to connect to Redis: %w\", err)\n    }\n    \n    // Start health checking goroutine\n    go storage.healthChecker()\n    \n    return storage, nil\n}\n\n// healthChecker runs periodic health checks in background\nfunc (r *RedisStorage) healthChecker() {\n    ticker := time.NewTicker(5 * time.Second)\n    defer ticker.Stop()\n    \n    for range ticker.C {\n        ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\n        \n        err := r.client.Ping(ctx).Err()\n        cancel()\n        \n        r.healthLock.Lock()\n        r.lastCheck = time.Now()\n        if err != nil {\n            r.healthy = false\n            r.failureCount++\n            r.lastFailure = time.Now()\n        } else {\n            r.healthy = true\n            r.failureCount = 0\n        }\n        r.healthLock.Unlock()\n    }\n}\n\n// IsHealthy returns current health status\nfunc (r *RedisStorage) IsHealthy() bool {\n    r.healthLock.RLock()\n    defer r.healthLock.RUnlock()\n    return r.healthy\n}\n\n// Close closes Redis connections\nfunc (r *RedisStorage) Close() error {\n    return r.client.Close()\n}\n```\n\n**Core Type Definitions**\n\n`pkg/types/rules.go`:\n```go\npackage types\n\nimport (\n    \"time\"\n)\n\n// RateLimitRule defines a rate limiting rule with all configuration parameters\ntype RateLimitRule struct {\n    // ID uniquely identifies this rule\n    ID string `yaml:\"id\" json:\"id\"`\n    \n    // Name human-readable rule name\n    Name string `yaml:\"name\" json:\"name\"`\n    \n    // KeyPattern defines which requests this rule applies to\n    // Examples: \"user:*\", \"ip:192.168.1.*\", \"api:/upload/*\"\n    KeyPattern string `yaml:\"key_pattern\" json:\"key_pattern\"`\n    \n    // Algorithm specifies which rate limiting algorithm to use\n    Algorithm string `yaml:\"algorithm\" json:\"algorithm\"`\n    \n    // Limit maximum number of requests allowed in the time window\n    Limit int64 `yaml:\"limit\" json:\"limit\"`\n    \n    // Window time duration for the rate limit\n    Window time.Duration `yaml:\"window\" json:\"window\"`\n    \n    // BurstLimit allows temporary bursts above the sustained limit (token bucket only)\n    BurstLimit int64 `yaml:\"burst_limit\" json:\"burst_limit\"`\n    \n    // Enabled whether this rule is currently active\n    Enabled bool `yaml:\"enabled\" json:\"enabled\"`\n    \n    // Priority determines evaluation order (higher numbers evaluated first)\n    Priority int `yaml:\"priority\" json:\"priority\"`\n    \n    // CreatedAt timestamp when rule was created\n    CreatedAt time.Time `yaml:\"created_at\" json:\"created_at\"`\n    \n    // UpdatedAt timestamp when rule was last modified\n    UpdatedAt time.Time `yaml:\"updated_at\" json:\"updated_at\"`\n}\n\n// RateLimitRequest contains all context needed to evaluate rate limits\ntype RateLimitRequest struct {\n    // UserID for authenticated user rate limiting\n    UserID string `json:\"user_id\"`\n    \n    // IPAddress for IP-based rate limiting\n    IPAddress string `json:\"ip_address\"`\n    \n    // APIEndpoint for per-endpoint rate limiting\n    APIEndpoint string `json:\"api_endpoint\"`\n    \n    // UserAgent for bot detection and user-agent-based limiting\n    UserAgent string `json:\"user_agent\"`\n    \n    // Tokens number of tokens to consume (default 1)\n    Tokens int64 `json:\"tokens\"`\n}\n\n// RateLimitResult contains the decision and metadata from a rate limit check\ntype RateLimitResult struct {\n    // Allowed whether the request should be permitted\n    Allowed bool `json:\"allowed\"`\n    \n    // Remaining number of requests remaining in current window\n    Remaining int64 `json:\"remaining\"`\n    \n    // RetryAfter duration to wait before next allowed request\n    RetryAfter time.Duration `json:\"retry_after\"`\n    \n    // ResetTime when the rate limit window resets\n    ResetTime time.Time `json:\"reset_time\"`\n    \n    // RuleID which rule triggered this result\n    RuleID string `json:\"rule_id\"`\n    \n    // Algorithm which algorithm was used\n    Algorithm string `json:\"algorithm\"`\n}\n```\n\n**Storage Interface Definition**\n\n`internal/storage/interface.go`:\n```go\npackage storage\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// Storage defines the interface for rate limiting storage backends\ntype Storage interface {\n    // CheckAndUpdate atomically checks current count and updates if under limit\n    // Returns: allowed, remaining, resetTime, error\n    CheckAndUpdate(ctx context.Context, key string, limit int64, window time.Duration) (bool, int64, time.Time, error)\n    \n    // Preview checks current status without updating counters\n    Preview(ctx context.Context, key string, limit int64, window time.Duration) (bool, int64, time.Time, error)\n    \n    // Reset clears all counters for a key\n    Reset(ctx context.Context, key string) error\n    \n    // IsHealthy returns whether storage backend is available\n    IsHealthy() bool\n    \n    // Close releases resources\n    Close() error\n}\n```\n\n#### Core Component Skeletons\n\n**Rate Limiter Interface and Core Types**\n\n`internal/limiter/interface.go`:\n```go\npackage limiter\n\nimport (\n    \"context\"\n    \n    \"github.com/yourusername/distributed-rate-limiter/pkg/types\"\n)\n\n// Limiter defines the interface for rate limiting implementations\ntype Limiter interface {\n    // Check performs rate limit evaluation and updates counters\n    Check(ctx context.Context, req types.RateLimitRequest) (*types.RateLimitResult, error)\n    \n    // Preview checks rate limit status without updating counters  \n    Preview(ctx context.Context, req types.RateLimitRequest) (*types.RateLimitResult, error)\n    \n    // Reset clears rate limit counters for a request context\n    Reset(ctx context.Context, req types.RateLimitRequest) error\n}\n```\n\n**Main DistributedLimiter Component (Core Logic Skeleton)**\n\n`internal/limiter/limiter.go`:\n```go\npackage limiter\n\nimport (\n    \"context\"\n    \"fmt\"\n    \n    \"github.com/yourusername/distributed-rate-limiter/internal/config\"\n    \"github.com/yourusername/distributed-rate-limiter/internal/storage\"\n    \"github.com/yourusername/distributed-rate-limiter/pkg/types\"\n)\n\n// DistributedLimiter coordinates rate limiting across multiple tiers with fallback\ntype DistributedLimiter struct {\n    storage       storage.Storage\n    ruleManager   *config.RuleManager\n    localFallback Limiter\n}\n\n// NewDistributedLimiter creates distributed rate limiter instance\nfunc NewDistributedLimiter(storage storage.Storage, ruleManager *config.RuleManager) *DistributedLimiter {\n    // TODO: Create local fallback limiter for when Redis is unavailable\n    // TODO: Initialize metrics collection\n    \n    return &DistributedLimiter{\n        storage:     storage,\n        ruleManager: ruleManager,\n        // localFallback: localLimiter,\n    }\n}\n\n// Check performs multi-tier rate limit evaluation with short-circuit logic\nfunc (d *DistributedLimiter) Check(ctx context.Context, req types.RateLimitRequest) (*types.RateLimitResult, error) {\n    // TODO 1: Check if storage backend is healthy, switch to fallback if not\n    // TODO 2: Get matching rules from rule manager based on request context\n    // TODO 3: Sort rules by priority (highest priority first)\n    // TODO 4: Evaluate each rule using short-circuit logic:\n    //         - Stop immediately if any rule denies the request\n    //         - Track the most restrictive remaining count across all rules\n    // TODO 5: For each rule that applies:\n    //         - Generate appropriate Redis key (user:123, ip:1.2.3.4, etc.)\n    //         - Call storage.CheckAndUpdate with rule's limit and window\n    //         - If denied, return immediately with rule details\n    // TODO 6: If all rules pass, return allowed=true with most restrictive remaining count\n    // TODO 7: Handle storage errors by falling back to local limiter\n    // TODO 8: Update metrics for allowed/denied decisions\n    \n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// Preview checks rate limit status without updating counters\nfunc (d *DistributedLimiter) Preview(ctx context.Context, req types.RateLimitRequest) (*types.RateLimitResult, error) {\n    // TODO 1: Same logic as Check() but call storage.Preview instead of CheckAndUpdate\n    // TODO 2: Return most restrictive result across all matching rules\n    \n    return nil, fmt.Errorf(\"not implemented\")  \n}\n\n// Reset clears rate limit counters for a request\nfunc (d *DistributedLimiter) Reset(ctx context.Context, req types.RateLimitRequest) error {\n    // TODO 1: Get matching rules for the request\n    // TODO 2: For each rule, generate Redis key and call storage.Reset\n    // TODO 3: Handle partial failures gracefully\n    \n    return fmt.Errorf(\"not implemented\")\n}\n```\n\n**Configuration Constants**\n\n`internal/storage/redis/constants.go`:\n```go\npackage redis\n\nimport \"time\"\n\nconst (\n    // DEFAULT_POOL_SIZE maximum Redis connections per instance\n    DEFAULT_POOL_SIZE = 10\n    \n    // DEFAULT_TIMEOUT for Redis operations\n    DEFAULT_TIMEOUT = 5 * time.Millisecond\n    \n    // Circuit breaker thresholds\n    MAX_FAILURE_COUNT = 5\n    CIRCUIT_OPEN_TIME = 30 * time.Second\n)\n```\n\n`internal/config/constants.go`:\n```go\npackage config\n\nconst (\n    // Priority levels for rule evaluation order\n    PRIORITY_HIGH = 100\n    PRIORITY_MEDIUM = 50  \n    PRIORITY_LOW = 1\n    \n    // Algorithm identifiers\n    ALGORITHM_TOKEN_BUCKET = \"token_bucket\"\n    ALGORITHM_SLIDING_WINDOW_LOG = \"sliding_window_log\" \n    ALGORITHM_SLIDING_COUNTER = \"sliding_window_counter\"\n)\n```\n\n#### Language-Specific Implementation Hints\n\n**Redis Lua Script Development**\n\nWhen implementing atomic operations, Redis Lua scripts ensure consistency across distributed instances:\n\n- Use `redis.call('TIME')` within Lua scripts to get consistent timestamps\n- Store nanosecond timestamps as integers to avoid floating point precision issues  \n- Return multiple values from Lua scripts to minimize round-trips: `return {allowed, remaining, reset_time}`\n- Test Lua scripts thoroughly - Redis script errors are harder to debug than Go code\n\n**Go Context and Timeout Handling**\n\n- Always pass context with timeouts to Redis operations: `ctx, cancel := context.WithTimeout(parent, 50*time.Millisecond)`\n- Use `context.WithCancel` for health checking goroutines to enable graceful shutdown\n- Check `ctx.Done()` in long-running operations to respect cancellation\n- Wrap Redis errors with `fmt.Errorf(\"redis operation failed: %w\", err)` for better error tracking\n\n**Concurrent Map Access**\n\nThe rule manager needs thread-safe access patterns:\n\n- Use `sync.RWMutex` for rule maps - most operations are reads\n- Hold read locks for the minimum time necessary: get rule list, release lock, then process\n- Use `sync.Map` for metrics counters that are updated frequently from multiple goroutines\n- Avoid nested locking - always acquire locks in the same order to prevent deadlocks\n\n#### Milestone Checkpoints\n\n**After completing the architecture setup:**\n\n1. **Connectivity Test**: Run `go test ./internal/storage/redis/...` - should successfully connect to Redis and perform basic operations\n2. **Rule Loading Test**: Create a test YAML file with sample rules and verify `RuleManager.LoadRules()` parses correctly\n3. **Interface Compliance**: Run `go build ./...` - all interface implementations should compile without errors\n4. **Health Check Test**: Start Redis, verify health status is `true`, stop Redis, verify it changes to `false` within 10 seconds\n\n**Expected behavior verification:**\n- `RedisStorage.IsHealthy()` should return `true` when Redis is running\n- Rule manager should load rules from YAML and match patterns correctly\n- DistributedLimiter creation should not panic when given valid storage and rule manager\n\n**Common setup issues:**\n- Redis connection failures: Check Redis is running on expected port with `redis-cli ping`\n- Module import errors: Ensure `go mod tidy` has been run and all dependencies are available\n- Configuration validation errors: Verify YAML syntax and required fields are present\n\n\n## Data Model\n\n> **Milestone(s):** All milestones - this data model foundation enables rate limiting algorithms (Milestone 1), multi-tier evaluation (Milestone 2), Redis backend integration (Milestone 3), sharding (Milestone 4), and API design (Milestone 5).\n\nThe data model serves as the foundation for distributed rate limiting by defining how rate limit rules are configured, how state is tracked across multiple application instances, and how metrics are collected for monitoring and alerting. Think of this as the blueprint that architects use before constructing a building - every component needs to understand the same data structures to work together effectively.\n\n![Data Model Relationships](./diagrams/data-model.svg)\n\nThe data model addresses three critical challenges in distributed rate limiting. First, it must represent complex rate limit rules that can apply to multiple dimensions (user, IP, API endpoint) with different algorithms and priorities. Second, it must define how rate limiting state is stored in Redis with atomic operations to prevent race conditions across multiple application instances. Third, it must capture metrics and monitoring data that provide visibility into system behavior and enable proactive capacity management.\n\n### Rate Limit Rule Structure\n\nRate limit rules define the policies that govern request admission across different dimensions and tiers. Think of rate limit rules as the security protocols at different checkpoints in an airport - each checkpoint has specific criteria (first class vs economy, domestic vs international) and enforcement mechanisms (document check, metal detector, baggage scan) that determine whether a passenger can proceed.\n\nThe rule structure must support multi-dimensional matching, where a single HTTP request might be subject to multiple overlapping rules based on user identity, source IP address, API endpoint, and global system capacity. This creates a hierarchical evaluation system where rules are applied in priority order, with higher priority rules taking precedence over lower priority ones.\n\n> **Decision: Unified Rule Structure with Pattern Matching**\n> - **Context**: Rate limits need to apply across multiple dimensions (user, IP, API) with different algorithms and priorities\n> - **Options Considered**: Separate rule types per dimension vs unified rule structure vs hardcoded tiers\n> - **Decision**: Single unified `RateLimitRule` structure with pattern-based key matching\n> - **Rationale**: Unified structure reduces complexity, enables flexible rule composition, and simplifies rule management API\n> - **Consequences**: More flexible but requires careful pattern design to avoid rule conflicts and performance issues\n\nThe `RateLimitRule` structure captures all necessary configuration for rate limit enforcement:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `id` | string | Unique identifier for the rule, used for updates and deletion |\n| `name` | string | Human-readable name for the rule, displayed in dashboard and logs |\n| `key_pattern` | string | Pattern for matching rate limit keys (supports wildcards and templating) |\n| `algorithm` | string | Rate limiting algorithm identifier (`token_bucket`, `sliding_window_counter`, `sliding_window_log`) |\n| `limit` | int64 | Maximum number of requests allowed within the time window |\n| `window` | time.Duration | Time window duration for rate limit evaluation |\n| `burst_limit` | int64 | Maximum burst capacity for token bucket algorithm (ignored for other algorithms) |\n| `enabled` | bool | Whether this rule is actively enforced |\n| `priority` | int | Rule evaluation priority (higher numbers evaluated first) |\n| `created_at` | time.Time | Timestamp when rule was created |\n| `updated_at` | time.Time | Timestamp when rule was last modified |\n\nThe `key_pattern` field uses a templating system that allows rules to match multiple rate limit dimensions. For example, a pattern like `user:{user_id}:api:/v1/upload` creates rate limit keys specific to both user identity and API endpoint. This enables fine-grained control where a user might have different rate limits for different API operations.\n\n> **Critical Design Insight**: The pattern-based approach allows a single rule to generate different Redis keys based on request context, enabling both specific limits (per-user API limits) and aggregate limits (global API limits) using the same rule structure.\n\nThe `priority` field determines rule evaluation order during multi-tier rate limiting. Rules with higher priority values are evaluated first, allowing system-wide protections (priority 100) to take precedence over user-specific limits (priority 50). This prevents lower-priority rules from consuming system resources when higher-priority limits are already exceeded.\n\n**Common Rule Configuration Patterns:**\n\n| Pattern Type | Key Pattern | Example Usage |\n|-------------|-------------|---------------|\n| Per-User Global | `user:{user_id}` | 1000 requests per hour per user across all APIs |\n| Per-User API | `user:{user_id}:api:{api_endpoint}` | 100 requests per minute for upload API per user |\n| Per-IP | `ip:{ip_address}` | 500 requests per hour per IP address |\n| Global API | `api:{api_endpoint}` | 10000 requests per minute for upload API across all users |\n| Global System | `global` | 100000 requests per minute system-wide |\n\n### Redis Data Structures\n\nRedis serves as the distributed state store for rate limiting counters and algorithm-specific state. The data structures must support atomic check-and-update operations to prevent race conditions when multiple application instances evaluate the same rate limit key simultaneously. Think of Redis as a high-speed synchronized ledger that multiple bank tellers can access simultaneously, with built-in mechanisms to ensure no two tellers can modify the same account balance at exactly the same time.\n\n> **Decision: Algorithm-Specific Redis Key Schemas**\n> - **Context**: Different rate limiting algorithms require different state representations in Redis\n> - **Options Considered**: Unified state format vs algorithm-specific schemas vs separate Redis databases\n> - **Decision**: Algorithm-specific key schemas with consistent naming conventions\n> - **Rationale**: Optimizes Redis operations for each algorithm while maintaining operational simplicity\n> - **Consequences**: More complex key management but better performance and clearer debugging\n\nEach rate limiting algorithm requires different data structures in Redis to maintain its state effectively:\n\n**Token Bucket Algorithm Redis Schema:**\n\n| Key Format | Data Type | Fields | Description |\n|------------|----------|--------|-------------|\n| `tb:{rule_id}:{key}:state` | Redis Hash | `tokens`, `last_refill_time` | Current token count and last refill timestamp |\n| `tb:{rule_id}:{key}:config` | Redis Hash | `capacity`, `refill_rate`, `window` | Algorithm configuration parameters |\n\nThe token bucket state uses a Redis hash to store both the current token count and the timestamp of the last refill operation. This enables the atomic Lua script to calculate how many tokens should be added based on elapsed time and then update both the token count and timestamp in a single atomic operation.\n\n**Sliding Window Counter Algorithm Redis Schema:**\n\n| Key Format | Data Type | Fields | Description |\n|------------|----------|--------|-------------|\n| `swc:{rule_id}:{key}:{bucket_id}` | Redis String | counter value | Request count for specific time bucket |\n| `swc:{rule_id}:{key}:meta` | Redis Hash | `window_size`, `bucket_size`, `last_cleanup` | Algorithm parameters and maintenance info |\n\nThe sliding window counter divides the time window into smaller buckets, with each bucket stored as a separate Redis key. This allows expired buckets to be cleaned up automatically using Redis TTL, while active buckets can be summed to get the current window count. The bucket ID is calculated as `floor(current_timestamp / bucket_size)`.\n\n**Sliding Window Log Algorithm Redis Schema:**\n\n| Key Format | Data Type | Fields | Description |\n|------------|----------|--------|-------------|\n| `swl:{rule_id}:{key}:log` | Redis Sorted Set | score=timestamp, member=request_id | Individual request timestamps |\n| `swl:{rule_id}:{key}:count` | Redis String | current count | Cached count for performance optimization |\n\nThe sliding window log uses a Redis sorted set where the score is the request timestamp and the member is a unique request identifier. This allows efficient range queries to count requests within the current time window and automatic cleanup of expired entries using `ZREMRANGEBYSCORE`.\n\n**Cross-Algorithm Metadata Schema:**\n\n| Key Format | Data Type | Fields | Description |\n|------------|----------|--------|-------------|\n| `rl:rules:{rule_id}` | Redis Hash | serialized `RateLimitRule` | Rule configuration for distributed access |\n| `rl:metrics:{key}:{timestamp}` | Redis Hash | `requests`, `allowed`, `denied`, `algorithm` | Aggregated metrics for monitoring |\n| `rl:health:{node_id}` | Redis String | timestamp | Node health heartbeat |\n\nThe metadata keys enable configuration distribution and health monitoring across the distributed rate limiting system. Rule configurations are cached in Redis so that any application instance can access the current rule set without requiring a central configuration service.\n\n### Redis Key Expiration Strategy\n\nAll rate limiting keys use Redis TTL (Time To Live) to prevent memory leaks and automatically clean up obsolete state. The TTL values are set based on the rate limit window duration plus a safety buffer:\n\n| Algorithm | TTL Formula | Rationale |\n|-----------|------------|-----------|\n| Token Bucket | `2 * window_duration` | Allows for clock skew and delayed cleanup |\n| Sliding Window Counter | `bucket_duration + window_duration` | Ensures all buckets in window remain available |\n| Sliding Window Log | `window_duration + 1 minute` | Prevents log growth while handling delayed requests |\n\n### Metrics and Monitoring Data\n\nComprehensive metrics collection enables operators to understand system behavior, detect anomalies, and plan capacity upgrades. The metrics system captures both real-time operational data and historical trends for analysis. Think of this as the instrument panel in an aircraft cockpit - pilots need both immediate readings (altitude, speed) and trend information (fuel consumption over time) to make informed decisions.\n\nThe metrics data model supports multiple aggregation levels to balance storage efficiency with query flexibility. Raw metrics are collected at high granularity for immediate operational needs, while aggregated metrics provide efficient historical analysis.\n\n**Real-Time Metrics Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `timestamp` | int64 | Unix timestamp in milliseconds for precise ordering |\n| `rule_id` | string | Rate limit rule that generated this metric |\n| `key` | string | Rate limit key (may be hashed for privacy) |\n| `algorithm` | string | Rate limiting algorithm used |\n| `requests` | int64 | Number of requests evaluated |\n| `allowed` | int64 | Number of requests allowed |\n| `denied` | int64 | Number of requests denied |\n| `remaining_quota` | int64 | Remaining capacity in current window |\n| `window_reset_time` | int64 | When current window resets (Unix timestamp) |\n| `node_id` | string | Application instance that recorded this metric |\n\n**Aggregated Metrics Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `time_bucket` | string | Time bucket identifier (e.g., \"2024-01-15T14:30:00Z\") |\n| `bucket_size` | string | Aggregation interval (\"1m\", \"5m\", \"1h\", \"1d\") |\n| `dimensions` | map[string]string | Aggregation dimensions (rule_id, algorithm, etc.) |\n| `total_requests` | int64 | Sum of all requests in time bucket |\n| `total_allowed` | int64 | Sum of allowed requests |\n| `total_denied` | int64 | Sum of denied requests |\n| `avg_remaining_quota` | float64 | Average remaining quota across measurements |\n| `min_remaining_quota` | int64 | Minimum remaining quota observed |\n| `max_remaining_quota` | int64 | Maximum remaining quota observed |\n| `unique_keys` | int64 | Number of distinct rate limit keys active |\n\n**System Health Metrics Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `node_id` | string | Application instance identifier |\n| `timestamp` | int64 | Metric collection timestamp |\n| `redis_latency_p50` | float64 | 50th percentile Redis operation latency (milliseconds) |\n| `redis_latency_p99` | float64 | 99th percentile Redis operation latency (milliseconds) |\n| `redis_errors` | int64 | Number of Redis operation errors |\n| `local_fallback_active` | bool | Whether local fallback is currently active |\n| `rules_loaded` | int64 | Number of rate limit rules currently loaded |\n| `active_keys` | int64 | Number of rate limit keys with recent activity |\n| `memory_usage_bytes` | int64 | Application memory usage |\n| `cpu_usage_percent` | float64 | Application CPU usage percentage |\n\n> **Decision: Hierarchical Metric Aggregation**\n> - **Context**: Need both real-time monitoring and historical trend analysis with storage efficiency\n> - **Options Considered**: Store only raw metrics vs pre-aggregate everything vs hierarchical aggregation\n> - **Decision**: Hierarchical aggregation with multiple time granularities\n> - **Rationale**: Balances query performance, storage efficiency, and operational flexibility\n> - **Consequences**: More complex metric processing but enables both real-time dashboards and historical analysis\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Redis Key Explosion**\nWhen designing Redis key schemas, it's tempting to create highly granular keys for every possible combination of dimensions. However, this can lead to millions of keys in Redis, consuming excessive memory and degrading performance. Instead, use consistent key patterns with appropriate TTL values and consider using hashed key suffixes for high-cardinality dimensions like user IDs.\n\n⚠️ **Pitfall: Inconsistent Rule Priority Handling**\nRule priorities must be consistently interpreted across all application instances. A common mistake is using different priority comparison logic (ascending vs descending) in different components, leading to inconsistent rate limit enforcement. Always document priority semantics clearly and use constants like `PRIORITY_HIGH` and `PRIORITY_LOW` to make ordering explicit.\n\n⚠️ **Pitfall: Missing Rule Validation**\nRate limit rules should be validated when loaded to prevent runtime errors. Common validation failures include negative limits, zero or negative time windows, invalid algorithm names, and circular rule dependencies. Implement comprehensive validation with clear error messages to prevent misconfigured rules from reaching production.\n\n⚠️ **Pitfall: Metric Storage Overflow**\nWithout proper aggregation and cleanup, metrics can consume more storage than the actual application data. Implement time-based partitioning for metrics storage and automatically delete old partitions. Use sampling for high-frequency metrics and focus detailed collection on anomalous events.\n\n### Implementation Guidance\n\nThis implementation guidance provides the concrete data structures and Redis integration patterns needed to build the distributed rate limiting data model.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Configuration Format | YAML files with validation | etcd/Consul with dynamic updates |\n| Redis Client | `go-redis/redis` with connection pooling | `go-redis/redis` with cluster support |\n| Metrics Storage | In-memory aggregation with periodic export | ClickHouse/InfluxDB for time series |\n| Rule Validation | JSON Schema validation | Custom validation with dependency analysis |\n\n#### Recommended Module Structure\n\n```\ninternal/\n  config/\n    rule.go              ← RateLimitRule definition and validation\n    loader.go            ← Rule loading from YAML/JSON\n    manager.go           ← RuleManager with pattern matching\n  storage/\n    redis.go             ← RedisStorage implementation\n    interface.go         ← Storage interface definition  \n    lua_scripts.go       ← Embedded Lua scripts for atomic operations\n  metrics/\n    collector.go         ← Metrics collection and aggregation\n    types.go            ← Metric data structures\n    exporter.go         ← Export metrics to external systems\n  models/\n    types.go            ← Core data type definitions\n```\n\n#### Core Data Structure Definitions\n\n```go\npackage models\n\nimport (\n    \"time\"\n    \"github.com/go-redis/redis/v8\"\n)\n\n// RedisConfig holds Redis connection and behavior configuration\ntype RedisConfig struct {\n    Addresses    []string      `yaml:\"addresses\" json:\"addresses\"`\n    Password     string        `yaml:\"password\" json:\"password\"`\n    DB           int          `yaml:\"db\" json:\"db\"`\n    PoolSize     int          `yaml:\"pool_size\" json:\"pool_size\"`\n    ReadTimeout  time.Duration `yaml:\"read_timeout\" json:\"read_timeout\"`\n    WriteTimeout time.Duration `yaml:\"write_timeout\" json:\"write_timeout\"`\n    DialTimeout  time.Duration `yaml:\"dial_timeout\" json:\"dial_timeout\"`\n}\n\n// RedisStorage provides Redis-backed rate limiting state storage\ntype RedisStorage struct {\n    client redis.UniversalClient\n    config RedisConfig\n}\n\n// RateLimitRule defines a rate limiting policy with matching criteria and limits\ntype RateLimitRule struct {\n    ID          string        `yaml:\"id\" json:\"id\"`\n    Name        string        `yaml:\"name\" json:\"name\"`\n    KeyPattern  string        `yaml:\"key_pattern\" json:\"key_pattern\"`\n    Algorithm   string        `yaml:\"algorithm\" json:\"algorithm\"`\n    Limit       int64         `yaml:\"limit\" json:\"limit\"`\n    Window      time.Duration `yaml:\"window\" json:\"window\"`\n    BurstLimit  int64         `yaml:\"burst_limit\" json:\"burst_limit\"`\n    Enabled     bool          `yaml:\"enabled\" json:\"enabled\"`\n    Priority    int           `yaml:\"priority\" json:\"priority\"`\n    CreatedAt   time.Time     `yaml:\"created_at\" json:\"created_at\"`\n    UpdatedAt   time.Time     `yaml:\"updated_at\" json:\"updated_at\"`\n}\n\n// RuleManager manages rate limit rules with pattern matching and priority sorting\ntype RuleManager struct {\n    rules map[string]*RateLimitRule\n}\n\n// RateLimitRequest contains context information for rate limit evaluation\ntype RateLimitRequest struct {\n    UserID      string `json:\"user_id\"`\n    IPAddress   string `json:\"ip_address\"`\n    APIEndpoint string `json:\"api_endpoint\"`\n    UserAgent   string `json:\"user_agent\"`\n    Tokens      int64  `json:\"tokens\"`\n}\n\n// RateLimitResult contains the outcome of rate limit evaluation\ntype RateLimitResult struct {\n    Allowed    bool          `json:\"allowed\"`\n    Remaining  int64         `json:\"remaining\"`\n    RetryAfter time.Duration `json:\"retry_after\"`\n    ResetTime  time.Time     `json:\"reset_time\"`\n    RuleID     string        `json:\"rule_id\"`\n    Algorithm  string        `json:\"algorithm\"`\n}\n```\n\n#### Redis Storage Implementation Skeleton\n\n```go\npackage storage\n\n// NewRedisStorage creates a Redis storage instance with connection pooling\nfunc NewRedisStorage(config RedisConfig) (*RedisStorage, error) {\n    // TODO 1: Validate config parameters (addresses not empty, positive timeouts)\n    // TODO 2: Create Redis universal client with cluster support\n    // TODO 3: Test connection with PING command\n    // TODO 4: Load Lua scripts into Redis for atomic operations\n    // TODO 5: Return configured RedisStorage instance\n}\n\n// CheckAndUpdate atomically checks current usage and updates counters\nfunc (r *RedisStorage) CheckAndUpdate(ctx context.Context, key string, limit int64, window time.Duration) (bool, int64, time.Time, error) {\n    // TODO 1: Determine algorithm from key prefix\n    // TODO 2: Select appropriate Lua script for algorithm\n    // TODO 3: Prepare script arguments (key, limit, window, current_timestamp)\n    // TODO 4: Execute Lua script atomically in Redis\n    // TODO 5: Parse script response (allowed, remaining, reset_time)\n    // TODO 6: Handle Redis errors with circuit breaker pattern\n    // TODO 7: Return rate limit decision with metadata\n}\n```\n\n#### Rule Management Implementation Skeleton\n\n```go\npackage config\n\n// LoadRules loads rate limit rules from YAML configuration file\nfunc (rm *RuleManager) LoadRules(configPath string) error {\n    // TODO 1: Read YAML file from configPath\n    // TODO 2: Parse YAML into slice of RateLimitRule structs\n    // TODO 3: Validate each rule (positive limits, valid algorithms, pattern syntax)\n    // TODO 4: Check for rule ID conflicts\n    // TODO 5: Sort rules by priority (highest first)\n    // TODO 6: Update rm.rules map atomically\n    // TODO 7: Log successful rule loading with count\n}\n\n// GetMatchingRules returns rules that match the request context\nfunc (rm *RuleManager) GetMatchingRules(userID, ipAddress, apiEndpoint string) []*RateLimitRule {\n    // TODO 1: Create request context map with userID, ipAddress, apiEndpoint\n    // TODO 2: Iterate through rules in priority order\n    // TODO 3: For each rule, expand key_pattern template with request context\n    // TODO 4: Check if expanded pattern matches request (wildcards, exact match)\n    // TODO 5: Add matching rules to result slice\n    // TODO 6: Return rules sorted by priority (highest first)\n    // Hint: Use text/template package for pattern expansion\n}\n```\n\n#### Metrics Collection Implementation Skeleton\n\n```go\npackage metrics\n\n// CollectMetrics gathers rate limiting metrics for monitoring and alerting\nfunc (c *MetricsCollector) CollectMetrics(result *RateLimitResult, duration time.Duration) {\n    // TODO 1: Extract timestamp and dimensions from result\n    // TODO 2: Create RealTimeMetric struct with current values\n    // TODO 3: Add to in-memory buffer with timestamp-based key\n    // TODO 4: Update running counters (total requests, allowed, denied)\n    // TODO 5: Check if aggregation interval has passed\n    // TODO 6: If interval passed, compute aggregated metrics and export\n    // TODO 7: Clean up old metrics from memory buffer\n}\n```\n\n#### Configuration File Example\n\n```yaml\n# Example rate limit rules configuration\nrules:\n  - id: \"user-global\"\n    name: \"Per-User Global Limit\"\n    key_pattern: \"user:{user_id}\"\n    algorithm: \"token_bucket\"\n    limit: 1000\n    window: \"1h\"\n    burst_limit: 50\n    enabled: true\n    priority: 50\n\n  - id: \"upload-api\"\n    name: \"Upload API Limit\"\n    key_pattern: \"api:/v1/upload\"\n    algorithm: \"sliding_window_counter\"\n    limit: 10000\n    window: \"1m\"\n    enabled: true\n    priority: 100\n\n  - id: \"ip-limit\"\n    name: \"Per-IP Address Limit\"\n    key_pattern: \"ip:{ip_address}\"\n    algorithm: \"sliding_window_log\"\n    limit: 500\n    window: \"1h\"\n    enabled: true\n    priority: 75\n```\n\n#### Milestone Checkpoints\n\n**After implementing data structures:**\n- Run `go test ./internal/models/...` - all type definitions should compile without errors\n- Test rule validation with invalid configurations - should return specific error messages\n- Verify Redis key generation matches expected patterns for each algorithm\n\n**After implementing Redis storage:**\n- Run `go test ./internal/storage/...` - Redis operations should be atomic and consistent\n- Test with Redis cluster to verify consistent hashing works correctly\n- Verify Lua scripts execute atomically even under high concurrency\n\n**After implementing rule management:**\n- Load sample configuration file - should parse all rules and sort by priority\n- Test pattern matching with various request contexts - should match correct rules\n- Verify rule updates propagate to all application instances within configured interval\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Rules not loading | YAML syntax error | Check application logs for parsing errors | Validate YAML syntax and required fields |\n| Redis keys growing infinitely | Missing TTL on keys | Use `redis-cli` to check key expiration | Set appropriate TTL in Lua scripts |\n| Inconsistent rate limiting | Clock skew between nodes | Compare timestamps in Redis vs application logs | Use Redis TIME command for consistent timestamps |\n| High memory usage | Too many metric data points | Monitor metrics buffer size | Implement time-based cleanup and aggregation |\n| Pattern matching fails | Template expansion error | Log expanded patterns vs expected keys | Debug template syntax and variable names |\n\n\n## Rate Limiting Algorithms\n\n> **Milestone(s):** Milestone 1 - Rate Limiting Algorithms\n\nThe heart of any distributed rate limiting system lies in its algorithms for tracking and enforcing request quotas over time. While the distributed coordination and Redis integration add complexity, the fundamental challenge remains: how do we accurately measure request rates and make allow/deny decisions in real-time? This section explores three complementary algorithms that form the foundation of our rate limiting system, each with distinct characteristics that make them suitable for different use cases and performance requirements.\n\n### Mental Model: Water Flow Control Systems\n\nBefore diving into the technical details of rate limiting algorithms, it's helpful to think about water flow control systems that we encounter in everyday life. Each rate limiting algorithm maps naturally to a different type of water control mechanism, helping us understand their behaviors and trade-offs intuitively.\n\n**Token Bucket as a Water Storage Tank**: Imagine a water storage tank with a fixed capacity that receives water at a steady rate from a source pipe. When you need water (make a request), you can take it instantly from the tank if water is available. The key insight is that the tank allows you to consume water faster than the refill rate for short periods - you can drain the entire tank quickly if needed, but then you must wait for it to refill at the steady rate. This mirrors how token bucket allows controlled bursts above the sustained rate while preventing long-term overconsumption.\n\n**Sliding Window Counter as a Dam with Spillways**: Picture a dam with multiple spillway gates that open and close on a schedule. Every minute, one gate opens to release exactly 1000 gallons, while simultaneously another gate closes. You're measuring the total flow by looking at all currently open gates. This approximates the flow rate over the last hour by dividing it into discrete time buckets. It's memory-efficient (you only track a few bucket counts) but imprecise at bucket boundaries - if all gates opened at the start of their window, you might see 2000 gallons flow in one minute even though the average is 1000 per minute.\n\n**Sliding Window Log as Individual Raindrop Tracking**: Imagine tracking every individual raindrop that falls on your roof, recording the exact timestamp of each drop. To know the rainfall rate in the last hour, you count all drops within the 60-minute window ending right now. This gives you perfect accuracy - you know exactly how many drops fell in any time period. However, during a heavy downpour, you must remember millions of individual timestamps, making this approach memory-intensive but perfectly precise.\n\nThese water analogies capture the essential trade-offs: token bucket provides controlled bursting, sliding window counter offers memory efficiency with some accuracy loss, and sliding window log delivers perfect accuracy at the cost of memory usage. Understanding these mental models helps us choose the right algorithm for different scenarios in our distributed rate limiting system.\n\n![Token Bucket State Machine](./diagrams/algorithm-state-machine.svg)\n\n### Token Bucket Algorithm Design\n\nThe **token bucket algorithm** serves as our primary rate limiting mechanism because it naturally handles the most common real-world requirement: allowing controlled bursts while maintaining a sustainable long-term rate. This algorithm maintains a bucket that holds a fixed number of tokens, with new tokens added at a steady rate. Each request consumes one or more tokens, and requests are denied when insufficient tokens remain.\n\nThe algorithm operates on two fundamental parameters that define its behavior. The **bucket capacity** determines the maximum burst size - how many requests can be processed instantly when the system has been idle. The **refill rate** controls the sustained throughput - how many requests per second the system allows over extended periods. These parameters work together to provide both responsiveness and protection.\n\n**Token Bucket State Management**\n\nOur token bucket implementation maintains state through a `TokenBucketState` structure that tracks the current token count and the timestamp of the last refill operation. This design enables efficient state updates by calculating how many tokens should be added based on elapsed time since the last refill, avoiding the need for background processes or timers.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `tokens` | `int64` | Current number of tokens available in the bucket |\n| `last_refill_time` | `int64` | Unix timestamp in nanoseconds of the last refill calculation |\n\nThe state transitions follow a predictable pattern that maps to our water storage tank analogy. When the bucket is **full**, incoming tokens are discarded (the tank overflows), and requests are immediately approved until tokens are exhausted. During **partial fill** states, each request decreases the token count, and the refill calculation determines how many tokens to add based on elapsed time. When **empty**, requests must wait for tokens to be replenished through the steady refill process.\n\n**Atomic Token Bucket Operations**\n\nThe core challenge in distributed token bucket implementation lies in ensuring atomic check-and-update operations. In a single-threaded environment, checking the current token count and updating it can be separate operations. However, in a distributed system with multiple application instances, we must prevent race conditions where two instances simultaneously check the same token count and both approve requests that should collectively exceed the limit.\n\nOur solution employs Redis Lua scripts to guarantee atomicity. The script performs the following operations as a single indivisible unit:\n\n1. **Calculate elapsed time** since the last refill by comparing the current timestamp with `last_refill_time` stored in Redis\n2. **Compute tokens to add** by multiplying elapsed time by the refill rate, respecting the maximum bucket capacity\n3. **Update the token count** with newly calculated tokens, ensuring it never exceeds the configured capacity\n4. **Check token sufficiency** by comparing the requested token amount with the available tokens\n5. **Deduct tokens if sufficient** and update both the token count and last refill timestamp\n6. **Return the decision** along with remaining tokens and time until next token availability\n\nThis atomic operation prevents the classic race condition where multiple instances read the same token count, each conclude sufficient tokens exist, and all approve requests that collectively exceed the limit.\n\n**Burst Handling and Refill Logic**\n\nThe token bucket's burst handling capability distinguishes it from simpler rate limiting approaches. When a system has been idle, the bucket accumulates tokens up to its full capacity, enabling it to handle sudden traffic spikes without rejecting requests. This behavior closely mimics real-world usage patterns where traffic often arrives in bursts rather than smooth, evenly-distributed streams.\n\nThe refill logic implements a **continuous refill model** rather than periodic batch refills. Instead of adding tokens every second through a background process, we calculate the exact number of tokens that should have been added based on elapsed time whenever a rate limit check occurs. This approach eliminates the need for background workers and ensures consistent behavior regardless of request timing patterns.\n\nFor example, consider a token bucket configured with 100 tokens capacity and 50 tokens per second refill rate. If the bucket starts full and receives 100 requests instantly, all are approved and the bucket becomes empty. Subsequent requests must wait, but after 2 seconds of no activity, the bucket will have accumulated 100 tokens again (2 seconds × 50 tokens/second), ready for another burst.\n\n**Configuration and Tuning Parameters**\n\nToken bucket behavior is controlled through the `TokenBucketConfig` structure, which encapsulates all necessary parameters for proper operation:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `capacity` | `int64` | Maximum number of tokens the bucket can hold (burst size) |\n| `refill_rate` | `int64` | Number of tokens added per second (sustained rate) |\n| `window` | `time.Duration` | Time window for rate calculations (typically 1 second) |\n\nThe relationship between these parameters determines the algorithm's characteristics. A high capacity relative to refill rate creates a \"bursty\" system that can handle large traffic spikes but takes longer to recover. A low capacity relative to refill rate creates a \"smooth\" system that provides steady throughput with limited burst capability.\n\n**Token Bucket Implementation Architecture**\n\nThe `TokenBucket` type encapsulates the algorithm logic and integrates with our distributed storage layer:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `Check` | `ctx context.Context, key string, tokens int64` | `*RateLimitResult, error` | Atomically checks and updates token bucket state |\n| `Preview` | `ctx context.Context, key string, tokens int64` | `*RateLimitResult, error` | Returns current state without consuming tokens |\n| `Reset` | `ctx context.Context, key string` | `error` | Resets bucket to full capacity |\n| `GetState` | `ctx context.Context, key string` | `*TokenBucketState, error` | Returns current bucket state for monitoring |\n\nThe `Check` method serves as the primary interface for rate limiting decisions. It accepts a context for timeout control, a key identifying the specific rate limit bucket, and the number of tokens requested. The method returns a `RateLimitResult` containing the decision, remaining tokens, and timing information needed for proper HTTP response headers.\n\n> **Design Insight**: Token bucket's strength lies in its intuitive burst behavior that matches real-world traffic patterns. Unlike algorithms that strictly enforce per-second limits, token bucket allows natural traffic spikes while preventing sustained overload. This makes it ideal for user-facing APIs where occasional bursts are acceptable but sustained abuse must be prevented.\n\n### Sliding Window Algorithms Design\n\nWhile token bucket algorithms excel at handling burst traffic patterns, sliding window algorithms provide more predictable rate limiting behavior by measuring request rates over precise time windows. Our system implements two sliding window variants: **sliding window counter** for memory efficiency and **sliding window log** for perfect accuracy. Understanding their trade-offs helps us select the appropriate algorithm based on specific rate limiting requirements.\n\n**Sliding Window Counter Algorithm**\n\nThe sliding window counter algorithm divides time into fixed-size buckets and maintains a count of requests in each bucket. To determine the current rate, it examines all buckets within the sliding window and aggregates their counts. This approach provides memory efficiency by storing only bucket counts rather than individual request timestamps, making it suitable for high-traffic scenarios where memory usage must be controlled.\n\nOur implementation maintains a configurable number of sub-windows within each rate limiting window. For example, a 60-second rate limit window might be divided into 12 sub-windows of 5 seconds each. This granularity affects both memory usage and accuracy - more sub-windows provide better accuracy but consume more memory.\n\nThe algorithm tracks the following data for each rate limit key:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `window_start` | `int64` | Unix timestamp of the current window start time |\n| `bucket_counts` | `[]int64` | Array of request counts for each sub-window bucket |\n| `bucket_timestamps` | `[]int64` | Array of timestamps for each bucket to handle expiration |\n| `total_count` | `int64` | Cached total count across all buckets for efficiency |\n\nWhen processing a rate limit check, the algorithm follows these steps:\n\n1. **Calculate current bucket index** based on the current timestamp and bucket duration\n2. **Expire old buckets** by zeroing counts for buckets outside the sliding window\n3. **Update current bucket** by incrementing the count for the active time bucket\n4. **Aggregate total count** across all non-expired buckets within the window\n5. **Compare against limit** and return the appropriate decision with remaining quota\n\nThe key advantage of this approach lies in its **bounded memory usage**. Regardless of request volume, each rate limit key consumes memory proportional to the number of sub-windows rather than the number of requests. A system processing millions of requests per second still only stores a few dozen bucket counts per rate limit key.\n\nHowever, this efficiency comes with an accuracy trade-off known as the **boundary condition problem**. Consider a rate limit of 1000 requests per minute using 5-second buckets. If 1000 requests arrive in the first second of minute 1 and another 1000 requests arrive in the first second of minute 2, the algorithm might allow both bursts because they fall in different minute-windows, even though 2000 requests occurred within a 61-second period.\n\n**Sliding Window Log Algorithm**\n\nThe sliding window log algorithm eliminates the boundary condition problem by maintaining exact timestamps for each request within the sliding window. When evaluating a rate limit, it counts all requests that occurred within the precise time window ending at the current moment, providing perfect accuracy regardless of request timing patterns.\n\nThis algorithm stores request timestamps in a time-ordered data structure:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `request_timestamps` | `[]int64` | Sorted array of Unix timestamps for requests in the window |\n| `window_duration` | `int64` | Duration of the sliding window in nanoseconds |\n| `last_cleanup` | `int64` | Timestamp of last cleanup operation to remove expired entries |\n\nThe request evaluation process provides exact rate calculations:\n\n1. **Remove expired timestamps** older than the current window by scanning from the beginning of the array\n2. **Count remaining timestamps** to determine current request count within the window\n3. **Compare against rate limit** to make the allow/deny decision\n4. **Add current timestamp** if the request is allowed\n5. **Return result** with exact remaining quota and reset time\n\nThis precision comes at a significant memory cost. During traffic spikes, the algorithm must store individual timestamps for every request within the window duration. A system allowing 10,000 requests per minute must maintain up to 10,000 timestamps in memory per rate limit key. For applications with thousands of rate-limited keys, this memory usage can become prohibitive.\n\n**Algorithm Accuracy Comparison**\n\nTo understand the practical implications of each algorithm's accuracy characteristics, consider how they handle edge cases:\n\n| Scenario | Token Bucket Behavior | Sliding Window Counter | Sliding Window Log |\n|----------|----------------------|------------------------|-------------------|\n| Burst at window boundary | Allows up to full capacity instantly | May allow up to 2× limit at boundaries | Perfect accuracy, never exceeds limit |\n| Steady traffic | Enforces long-term average rate | Enforces average with small variations | Enforces exact rate continuously |\n| Irregular patterns | Smooths bursts through token accumulation | Approximates rate with bucket granularity | Measures exact rate regardless of pattern |\n| Memory usage | O(1) per key | O(buckets) per key | O(requests in window) per key |\n| Computational cost | O(1) per request | O(1) per request | O(log n) per request for cleanup |\n\n**Sliding Window Implementation Strategy**\n\nBoth sliding window algorithms integrate with our Redis backend through specialized Lua scripts that ensure atomic operations. The scripts handle the complex logic of timestamp management, expiration, and count aggregation while maintaining consistency across multiple application instances.\n\nFor sliding window counter, the Lua script manages bucket rotation and count aggregation:\n\n```lua\n-- Sliding window counter operations happen atomically in Redis\n-- 1. Calculate current bucket based on timestamp\n-- 2. Expire old buckets outside the window  \n-- 3. Increment current bucket count\n-- 4. Sum all active bucket counts\n-- 5. Return decision and remaining quota\n```\n\nFor sliding window log, the script manages timestamp arrays with careful memory cleanup:\n\n```lua\n-- Sliding window log operations happen atomically in Redis\n-- 1. Remove timestamps older than window duration\n-- 2. Count remaining timestamps\n-- 3. Add new timestamp if under limit\n-- 4. Return exact remaining quota\n```\n\n> **Performance Consideration**: Sliding window log's memory usage can grow large during traffic spikes, making it unsuitable for high-volume systems without careful memory management. However, its perfect accuracy makes it ideal for strict rate limiting scenarios where boundary conditions cannot be tolerated, such as payment processing or security-sensitive APIs.\n\n**Hybrid Sliding Window Approach**\n\nFor scenarios requiring both memory efficiency and improved accuracy, our system supports a hybrid approach that combines sliding window counter granularity with boundary condition detection. This algorithm uses fine-grained buckets (1-second buckets for 60-second windows) and applies smoothing logic to reduce boundary effects.\n\nThe hybrid algorithm applies a **weighted calculation** that considers partial bucket contributions when evaluating rate limits near window boundaries. Instead of simply summing bucket counts, it calculates what portion of each bucket falls within the exact sliding window and weights the counts accordingly.\n\nThis approach provides significantly better accuracy than standard sliding window counter while maintaining bounded memory usage. The computational overhead increases slightly due to weighted calculations, but remains much more efficient than maintaining individual request timestamps.\n\n### Algorithm Selection ADR\n\nChoosing the appropriate rate limiting algorithm significantly impacts system performance, accuracy, and operational characteristics. This decision affects memory usage, computational overhead, user experience, and the types of traffic patterns the system can handle effectively.\n\n> **Decision: Multi-Algorithm Support with Configurable Selection**\n> - **Context**: Different rate limiting scenarios have conflicting requirements. API endpoints serving user traffic need burst handling for good UX, while security-sensitive endpoints require strict accuracy. High-volume systems need memory efficiency, while low-volume critical systems can afford precision overhead.\n> - **Options Considered**: Single algorithm for simplicity, token bucket only for burst handling, sliding window log only for accuracy, configurable algorithm selection per rule\n> - **Decision**: Implement all three algorithms with per-rule configuration, defaulting to token bucket for most use cases\n> - **Rationale**: Different endpoints have fundamentally different requirements that cannot be satisfied by a single algorithm. The additional complexity is justified by the flexibility to optimize each rate limit rule for its specific constraints and requirements.\n> - **Consequences**: Increases implementation complexity and testing surface area, but enables optimal algorithm selection for each use case, improving both performance and user experience across different scenarios.\n\n**Algorithm Selection Decision Matrix**\n\nThe choice of rate limiting algorithm should be based on specific requirements and constraints:\n\n| Algorithm | Memory Usage | Accuracy | Burst Handling | Best Use Cases |\n|-----------|-------------|----------|----------------|----------------|\n| Token Bucket | O(1) per key | Good for sustained rate | Excellent - natural bursting | User-facing APIs, general rate limiting |\n| Sliding Window Counter | O(buckets) per key | Good with boundary effects | Limited - depends on bucket size | High-volume systems, memory-constrained environments |\n| Sliding Window Log | O(requests) per key | Perfect | None - strict enforcement | Security APIs, payment processing, strict SLA enforcement |\n\n**Configuration Recommendations by Scenario**\n\nBased on common distributed system patterns, we recommend the following algorithm selections:\n\n**User-Facing API Endpoints** should use token bucket with generous burst capacity. Users often interact with applications in bursts - opening a page might trigger multiple API calls simultaneously. Token bucket naturally accommodates this pattern while preventing sustained abuse.\n\nRecommended configuration:\n- Algorithm: `ALGORITHM_TOKEN_BUCKET`\n- Capacity: 3-5× the sustained rate (allows reasonable bursts)\n- Refill rate: Target sustained requests per second\n- Example: 100 requests/minute with 20-token capacity allows 20 instant requests followed by ~1.67 requests/second\n\n**Security-Sensitive Endpoints** should use sliding window log for perfect accuracy. Authentication attempts, password resets, and payment operations cannot tolerate the boundary condition issues that might allow double the intended rate limit.\n\nRecommended configuration:\n- Algorithm: `ALGORITHM_SLIDING_WINDOW_LOG`\n- Window: Short duration to limit memory usage (5-15 minutes)\n- Limit: Conservative values with no burst allowance\n- Example: 5 login attempts per 15 minutes with exact enforcement\n\n**High-Volume Internal APIs** should use sliding window counter with fine-grained buckets. Service-to-service communication often has predictable patterns and can tolerate slight accuracy variations in exchange for memory efficiency.\n\nRecommended configuration:\n- Algorithm: `ALGORITHM_SLIDING_COUNTER`\n- Buckets: 10-20 sub-windows for reasonable accuracy\n- Window: Match service SLA requirements\n- Example: 10,000 requests/minute divided into 12 five-second buckets\n\n**Global Rate Limits** protecting overall system capacity should use sliding window counter with coarse granularity. These limits serve as circuit breakers preventing total system overload, where approximate enforcement is acceptable for memory efficiency.\n\n**Algorithm Performance Characteristics**\n\nUnderstanding the computational and memory overhead of each algorithm helps in capacity planning:\n\n| Metric | Token Bucket | Sliding Window Counter | Sliding Window Log |\n|---------|-------------|----------------------|-------------------|\n| Redis operations per check | 1 Lua script execution | 1 Lua script execution | 1 Lua script execution |\n| Memory per key (idle) | ~32 bytes | ~64 bytes + (8 × buckets) | ~64 bytes |\n| Memory per key (active) | ~32 bytes | ~64 bytes + (8 × buckets) | ~64 bytes + (8 × requests in window) |\n| CPU overhead | Minimal | Low | Moderate (timestamp cleanup) |\n| Network overhead | Low | Low | Higher (larger data structures) |\n\n**Migration Strategy Between Algorithms**\n\nSystems may need to change algorithms as requirements evolve. Our design supports algorithm migration through versioned rate limit rules:\n\n1. **Gradual Rollout**: Deploy new rules with different algorithms alongside existing rules\n2. **A/B Testing**: Route percentage of traffic to new algorithm for validation\n3. **State Transition**: Both algorithms can coexist during migration periods\n4. **Monitoring**: Compare accuracy, performance, and user experience metrics\n5. **Cutover**: Switch traffic once new algorithm proves stable and effective\n\n> **Common Pitfall**: Choosing sliding window log for high-volume endpoints without considering memory implications. A popular API endpoint processing 100,000 requests per minute with a 5-minute window could require storing 500,000 timestamps per rate limit key. With multiple rate limit dimensions (per-user, per-IP, global), memory usage can quickly become unsustainable.\n\nThe flexibility to select appropriate algorithms per rate limit rule enables optimization for diverse requirements within a single system, providing both the performance needed for high-volume endpoints and the accuracy required for security-sensitive operations.\n\n### Implementation Guidance\n\nThis implementation guidance focuses on building the core rate limiting algorithms that serve as the foundation for our distributed rate limiting system. The algorithms themselves are the primary learning objective, while Redis integration and HTTP handling serve as supporting infrastructure.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Time Handling | `time.Now()` with Unix timestamps | High-resolution timestamps with `time.Now().UnixNano()` |\n| Math Operations | Standard int64 arithmetic | `math` package for precision calculations |\n| Configuration | Hard-coded constants | YAML configuration with validation |\n| Testing | Basic unit tests | Property-based testing with random inputs |\n\n**Recommended File Structure**\n\n```\ninternal/algorithms/\n  token_bucket.go          ← Token bucket algorithm implementation\n  token_bucket_test.go     ← Unit tests for token bucket\n  sliding_window.go        ← Sliding window counter and log algorithms  \n  sliding_window_test.go   ← Unit tests for sliding windows\n  types.go                 ← Common algorithm types and interfaces\n  config.go                ← Algorithm configuration structures\n  testdata/\n    algorithm_configs.yaml ← Test configuration files\n```\n\n**Infrastructure Starter Code**\n\nThe following infrastructure code handles time management, configuration, and basic Redis operations, allowing you to focus on implementing the algorithm logic:\n\n```go\npackage algorithms\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// Storage interface abstracts the underlying storage mechanism\n// This allows algorithms to work with Redis, in-memory, or other backends\ntype Storage interface {\n    // Lua script execution for atomic operations\n    ExecuteLua(ctx context.Context, script string, keys []string, args []interface{}) (interface{}, error)\n    \n    // Simple get/set operations for non-atomic operations\n    Get(ctx context.Context, key string) (string, error)\n    Set(ctx context.Context, key string, value string, expiration time.Duration) error\n    \n    // Delete operations for cleanup\n    Delete(ctx context.Context, keys ...string) error\n}\n\n// TimeProvider allows mocking time in tests\ntype TimeProvider interface {\n    Now() time.Time\n}\n\ntype RealTimeProvider struct{}\n\nfunc (r RealTimeProvider) Now() time.Time {\n    return time.Now()\n}\n\n// Configuration structures\ntype TokenBucketConfig struct {\n    Capacity   int64         `yaml:\"capacity\"`\n    RefillRate int64         `yaml:\"refill_rate\"`  \n    Window     time.Duration `yaml:\"window\"`\n}\n\ntype SlidingWindowConfig struct {\n    Limit      int64         `yaml:\"limit\"`\n    Window     time.Duration `yaml:\"window\"`\n    Buckets    int           `yaml:\"buckets\"`    // For counter algorithm\n}\n\n// Result structures  \ntype RateLimitResult struct {\n    Allowed     bool          `json:\"allowed\"`\n    Remaining   int64         `json:\"remaining\"`\n    RetryAfter  time.Duration `json:\"retry_after\"`\n    ResetTime   time.Time     `json:\"reset_time\"`\n    RuleID      string        `json:\"rule_id\"`\n    Algorithm   string        `json:\"algorithm\"`\n}\n\n// State structures for Redis storage\ntype TokenBucketState struct {\n    Tokens         int64 `json:\"tokens\"`\n    LastRefillTime int64 `json:\"last_refill_time\"`\n}\n```\n\n**Core Algorithm Skeleton Code**\n\n**Token Bucket Implementation Skeleton:**\n\n```go\npackage algorithms\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"math\"\n)\n\ntype TokenBucket struct {\n    config       TokenBucketConfig\n    storage      Storage\n    timeProvider TimeProvider\n}\n\nfunc NewTokenBucket(config TokenBucketConfig, storage Storage) *TokenBucket {\n    return &TokenBucket{\n        config:       config,\n        storage:      storage,\n        timeProvider: RealTimeProvider{},\n    }\n}\n\n// Check performs atomic token bucket rate limiting check\nfunc (tb *TokenBucket) Check(ctx context.Context, key string, tokensRequested int64) (*RateLimitResult, error) {\n    // TODO 1: Get current timestamp in nanoseconds using tb.timeProvider.Now().UnixNano()\n    // TODO 2: Execute Redis Lua script with key, timestamp, and configuration parameters\n    // TODO 3: Parse Lua script result containing allowed flag, remaining tokens, and next refill time\n    // TODO 4: Build RateLimitResult with appropriate retry_after and reset_time values\n    // TODO 5: Handle Redis errors by returning error for upstream handling\n    \n    // Lua script for atomic token bucket operations\n    luaScript := `\n        -- TODO 6: Parse input parameters (key timestamp, tokens_requested, capacity, refill_rate, window_ns)\n        -- TODO 7: Get current state from Redis or initialize if doesn't exist  \n        -- TODO 8: Calculate tokens to add based on elapsed time since last refill\n        -- TODO 9: Update token count (new_tokens = min(current + added, capacity))\n        -- TODO 10: Check if sufficient tokens available for request\n        -- TODO 11: If allowed, subtract requested tokens and update state in Redis\n        -- TODO 12: Return result array with [allowed, remaining_tokens, next_refill_timestamp]\n    `\n    \n    // Hint: Use tb.storage.ExecuteLua() with keys=[key] and args=[timestamp, tokensRequested, tb.config.Capacity, tb.config.RefillRate, tb.config.Window.Nanoseconds()]\n    // Hint: Redis returns []interface{} that you need to type assert to []interface{} then individual elements\n    // Hint: Calculate retry_after as time until next token becomes available\n}\n\n// Preview checks current state without consuming tokens\nfunc (tb *TokenBucket) Preview(ctx context.Context, key string) (*RateLimitResult, error) {\n    // TODO 13: Similar to Check() but with tokensRequested = 0\n    // TODO 14: Lua script should not subtract tokens, only return current state\n    // Hint: Reuse most logic from Check() but don't modify state\n}\n\n// Reset clears the bucket state, effectively filling it to capacity\nfunc (tb *TokenBucket) Reset(ctx context.Context, key string) error {\n    // TODO 15: Delete the Redis key to reset state\n    // TODO 16: Handle Redis errors appropriately\n    // Hint: Use tb.storage.Delete(ctx, key)\n}\n```\n\n**Sliding Window Counter Implementation Skeleton:**\n\n```go\ntype SlidingWindowCounter struct {\n    config       SlidingWindowConfig\n    storage      Storage  \n    timeProvider TimeProvider\n}\n\nfunc NewSlidingWindowCounter(config SlidingWindowConfig, storage Storage) *SlidingWindowCounter {\n    return &SlidingWindowCounter{\n        config:       config,\n        storage:      storage,\n        timeProvider: RealTimeProvider{},\n    }\n}\n\n// Check performs atomic sliding window counter rate limiting\nfunc (swc *SlidingWindowCounter) Check(ctx context.Context, key string, increment int64) (*RateLimitResult, error) {\n    // TODO 17: Calculate current timestamp and bucket duration\n    // TODO 18: Execute Lua script for atomic bucket operations\n    // TODO 19: Parse results and build RateLimitResult\n    \n    luaScript := `\n        -- TODO 20: Calculate current bucket index from timestamp\n        -- TODO 21: Load existing bucket counts from Redis hash\n        -- TODO 22: Expire buckets outside the sliding window\n        -- TODO 23: Increment current bucket count  \n        -- TODO 24: Sum all active bucket counts\n        -- TODO 25: Compare total against limit and return result\n    `\n    \n    // Hint: Use Redis hash to store bucket counts with bucket index as field name\n    // Hint: Calculate bucket index as (timestamp / bucket_duration) % num_buckets\n    // Hint: Window contains last N buckets, expire older ones\n}\n```\n\n**Sliding Window Log Implementation Skeleton:**\n\n```go\ntype SlidingWindowLog struct {\n    config       SlidingWindowConfig\n    storage      Storage\n    timeProvider TimeProvider\n}\n\n// Check performs atomic sliding window log rate limiting\nfunc (swl *SlidingWindowLog) Check(ctx context.Context, key string, increment int64) (*RateLimitResult, error) {\n    // TODO 26: Get current timestamp for window calculations\n    // TODO 27: Execute Lua script for atomic timestamp operations\n    // TODO 28: Handle memory cleanup to prevent unbounded growth\n    \n    luaScript := `\n        -- TODO 29: Load current timestamp list from Redis sorted set\n        -- TODO 30: Remove timestamps older than window duration\n        -- TODO 31: Count remaining timestamps in window\n        -- TODO 32: If under limit, add current timestamp to set\n        -- TODO 33: Return decision with exact remaining count\n    `\n    \n    // Hint: Use Redis sorted set (ZSET) with timestamps as scores for efficient range operations\n    // Hint: Use ZREMRANGEBYSCORE to remove old timestamps atomically\n    // Hint: Use ZCARD to count current timestamps in set\n}\n```\n\n**Algorithm Factory and Selection Logic:**\n\n```go\n// AlgorithmFactory creates algorithm instances based on configuration\ntype AlgorithmFactory struct {\n    storage Storage\n}\n\nfunc (af *AlgorithmFactory) CreateAlgorithm(algorithmType string, config interface{}) (RateLimitingAlgorithm, error) {\n    // TODO 34: Switch on algorithmType (ALGORITHM_TOKEN_BUCKET, etc.)\n    // TODO 35: Cast config to appropriate type and validate parameters\n    // TODO 36: Return appropriate algorithm instance or error\n    // Hint: Use type switches for config casting: config.(*TokenBucketConfig)\n}\n\n// Common interface for all algorithms\ntype RateLimitingAlgorithm interface {\n    Check(ctx context.Context, key string, tokens int64) (*RateLimitResult, error)\n    Preview(ctx context.Context, key string) (*RateLimitResult, error)  \n    Reset(ctx context.Context, key string) error\n}\n```\n\n**Language-Specific Hints**\n\n- Use `time.Now().UnixNano()` for high-precision timestamps to avoid race conditions with multiple requests in the same millisecond\n- Redis Lua scripts return `[]interface{}` that require type assertion: `result[0].(int64)`\n- Handle Redis connection errors gracefully - rate limiting failures should not crash the application\n- Use `math.Min()` for token calculations to prevent overflow: `math.Min(float64(current + added), float64(capacity))`\n- Validate configuration parameters at startup to fail fast on invalid settings\n\n**Milestone Checkpoint**\n\nAfter implementing the rate limiting algorithms, verify your implementation:\n\n**Testing Commands:**\n```bash\ngo test ./internal/algorithms/... -v\ngo test ./internal/algorithms/... -race  # Check for race conditions\ngo test ./internal/algorithms/... -bench=. # Performance benchmarks\n```\n\n**Expected Behavior:**\n- Token bucket allows bursts up to capacity, then enforces sustained rate\n- Sliding window counter approximates rate with small boundary effects  \n- Sliding window log provides exact rate enforcement with higher memory usage\n- All algorithms handle concurrent access safely through atomic Lua scripts\n\n**Manual Verification:**\n- Create token bucket with 10 capacity, 1/second refill rate\n- Make 10 instant requests → all should succeed\n- Make 11th request immediately → should be denied\n- Wait 5 seconds, make 5 requests → all should succeed\n- Monitor Redis keys to verify state is properly maintained\n\n**Debugging Signs:**\n- \"Token count negative\" → Race condition in Lua script logic\n- \"Memory usage growing unbounded\" → Sliding window log not cleaning up old timestamps  \n- \"Inconsistent rate limiting\" → Time synchronization issues between instances\n- \"Redis timeout errors\" → Connection pool exhaustion or network issues\n\nThe algorithms implemented in this milestone form the foundation for all subsequent distributed rate limiting functionality. Focus on correctness and atomicity - performance optimizations can be added later once the core logic is solid.\n\n\n## Multi-Tier Rate Limiting\n\n> **Milestone(s):** Milestone 2 - Multi-tier Rate Limiting\n\nThe power of distributed rate limiting extends far beyond simple request counting. Real-world applications require sophisticated hierarchical controls that operate across multiple dimensions simultaneously - protecting individual users from abuse, preventing single IP addresses from overwhelming the system, ensuring fair API resource allocation, and maintaining global system capacity. This multi-tier approach transforms rate limiting from a binary gate into an intelligent traffic management system that can adapt to complex usage patterns while maintaining predictable system performance.\n\n### Mental Model: Cascading Security Checkpoints\n\nThink of multi-tier rate limiting like airport security checkpoints, where passengers must pass through multiple stages of screening before reaching their destination. Each checkpoint serves a different purpose and operates independently, but they work together to ensure overall security and flow management.\n\nAt the document check station, each individual passenger (user) has personal limits - they can only travel on their specific ticket with their allocated baggage allowance. This represents **per-user rate limits** that prevent individual accounts from consuming excessive resources through either malicious behavior or buggy client code.\n\nThe security scanner represents **per-IP rate limits**, where the physical location (IP address) has throughput constraints regardless of how many different passengers (users) might be coming from that location. A single corporate NAT gateway might have hundreds of employees behind it, but the checkpoint can only process a certain number of people per minute from any single entry point.\n\nThe gate area introduces **per-API rate limits**, where different flight destinations (API endpoints) have different capacity constraints. The international terminal might handle fewer but more complex departures, while domestic gates process higher volumes of simpler operations. Each API endpoint has distinct resource requirements and thus different rate limiting policies.\n\nFinally, the airport itself has **global rate limits** - a maximum number of passengers it can process per hour across all gates, regardless of individual tickets or origins. Even if every individual checkpoint is under capacity, the airport's overall throughput ceiling prevents system-wide congestion that would affect everyone.\n\nThe critical insight is that passengers must successfully pass through ALL checkpoints in sequence. Being approved at document check doesn't guarantee passage through security, and having space at your gate doesn't matter if the airport has reached its global capacity limit. However, once any checkpoint rejects a passenger, there's no need to continue checking the remaining stations - this **short-circuit evaluation** saves resources and provides faster feedback.\n\nJust as airport security adapts to different threat levels and passenger volumes, multi-tier rate limiting systems must handle varying load patterns, user behaviors, and attack scenarios while maintaining fair resource allocation across all dimensions.\n\n![Multi-Tier Rate Limit Evaluation](./diagrams/multi-tier-evaluation.svg)\n\n### Tier Evaluation Strategy\n\nMulti-tier rate limiting requires a sophisticated evaluation strategy that balances accuracy, performance, and resource consumption. The system must check multiple rate limit dimensions efficiently while avoiding unnecessary computation when limits are already exceeded. The evaluation algorithm determines not just whether a request should be allowed, but which specific limits are constraining the user and how much capacity remains across all tiers.\n\nThe **tier precedence hierarchy** establishes the order in which rate limits are evaluated, typically flowing from most specific to most general: per-user limits, per-IP limits, per-API limits, and finally global limits. This ordering reflects both the logical dependency relationships and the performance characteristics of different limit types. User-specific limits are typically the most restrictive and fastest to evaluate since they operate on a single key, while global limits require aggregation across many keys and are computationally expensive.\n\nHowever, tier precedence alone is insufficient - the system must also consider **limit severity** and **evaluation cost** when determining the optimal checking sequence. A per-user limit that allows 1000 requests per hour should be checked before a global limit that allows 10 million requests per hour, even if the global limit is technically \"higher\" in the hierarchy. Similarly, expensive operations like sliding window log evaluations should be deferred until cheaper approximations (like sliding window counters) have passed.\n\nThe **short-circuit evaluation algorithm** implements this strategy through a prioritized sequence of checks:\n\n1. **Load applicable rules** for the request context by matching the user ID, IP address, and API endpoint against the rule patterns stored in the `RuleManager`. This initial filtering reduces the evaluation set from potentially thousands of rules to a manageable subset of 3-10 relevant limits.\n\n2. **Sort rules by priority and cost** to establish the evaluation order. Rules with higher priority values (indicating more restrictive or important limits) are checked first, with tie-breaking based on computational cost estimates for each algorithm type.\n\n3. **Evaluate each rule in sequence** using the appropriate rate limiting algorithm. For each rule, extract the required tokens from the request, construct the Redis key using the rule's key pattern, and perform the atomic check-and-update operation.\n\n4. **Short-circuit on first failure** - as soon as any rule denies the request, immediately return a rejection response without evaluating remaining rules. The response includes the specific rule ID, remaining capacity, and retry-after timing from the failing rule.\n\n5. **Aggregate success results** - if all rules pass, combine the results to determine the most restrictive remaining capacity and earliest reset time across all checked tiers. This aggregate information populates the rate limit headers in the successful response.\n\nThe evaluation strategy must also handle **rule conflicts** where multiple rules apply to the same request dimension but specify different limits or algorithms. The system resolves conflicts by giving precedence to rules with higher priority values, with the most restrictive rule taking effect when priorities are equal.\n\n**Rule matching** uses pattern-based key composition to determine which rules apply to each request. A rule with key pattern `user:{user_id}:api:orders` matches requests from any user to the orders API endpoint, while `user:premium_user_*:api:*` applies broader limits to all premium users across all endpoints. The pattern matching system supports wildcards, prefix matching, and parameterized substitution to create flexible rule targeting.\n\nThe evaluation strategy incorporates **performance optimizations** to minimize latency and Redis load. Rule results are cached locally for short periods (typically 100-500 milliseconds) to avoid redundant checks for burst traffic patterns. The system also maintains separate connections to Redis for different rule priorities, allowing high-priority user limits to bypass congestion from expensive global limit calculations.\n\n**Error handling during evaluation** follows a fail-open strategy by default - if a specific rule cannot be evaluated due to Redis connectivity issues, the system logs the failure and continues checking remaining rules. Only if all applicable rules fail to evaluate does the system fall back to local rate limiting or (in strict security mode) deny the request entirely.\n\n> **Design Insight**: The tier evaluation strategy serves as the nervous system of the distributed rate limiter, coordinating decisions across multiple dimensions while maintaining sub-millisecond response times. The short-circuit approach not only improves performance but also provides clearer user feedback by identifying the specific limit that constrained their request rather than an ambiguous \"rate limited\" message.\n\n#### Tier Evaluation Algorithm Details\n\nThe core tier evaluation algorithm operates through a series of well-defined phases that transform a raw request into a comprehensive rate limiting decision:\n\n| Phase | Input | Processing | Output | Performance Impact |\n|-------|--------|------------|---------|-------------------|\n| Rule Discovery | `RateLimitRequest` with user/IP/API context | Pattern matching against `RuleManager` rule set | Filtered list of applicable `RateLimitRule` objects | O(log n) with indexed patterns |\n| Rule Prioritization | List of applicable rules | Sort by priority field, then by algorithm cost | Ordered evaluation sequence | O(k log k) where k is rule count |\n| Sequential Evaluation | Ordered rule list + request tokens | Redis-backed algorithm execution per rule | First failure or aggregated success | O(k) Redis operations, short-circuits |\n| Result Aggregation | Individual rule results | Compute most restrictive limits and earliest reset | Final `RateLimitResult` with headers | O(k) in-memory computation |\n\nThe **rule discovery phase** leverages efficient pattern matching to avoid evaluating irrelevant rules. The `RuleManager` maintains separate indices for user patterns, IP patterns, and API patterns, allowing the system to quickly identify candidate rules without scanning the entire rule set. For a request from user \"user123\" to IP \"192.168.1.10\" accessing \"/api/orders\", the discovery process queries each index independently and takes the intersection of matching rules.\n\n**Rule prioritization** considers both the explicit priority field and implicit algorithm costs when determining evaluation order:\n\n| Algorithm Type | Relative Cost | Reason | Evaluation Priority |\n|----------------|---------------|---------|-------------------|\n| `ALGORITHM_TOKEN_BUCKET` | Low | Single Redis key, simple arithmetic | Check first |\n| `ALGORITHM_SLIDING_COUNTER` | Medium | Multiple bucket keys, time-based logic | Check second |\n| `ALGORITHM_SLIDING_WINDOW_LOG` | High | List operations, timestamp management | Check last |\n\n**Sequential evaluation** implements the short-circuit logic with careful attention to atomicity and consistency. Each rule evaluation calls the `CheckAndUpdate` method on the appropriate algorithm implementation, passing the constructed Redis key and required token count. The evaluation immediately terminates on the first rule that returns `allowed: false`, capturing the specific failure details for client feedback.\n\n**Result aggregation** becomes necessary only when all individual rule checks succeed. The aggregation process identifies the most restrictive remaining capacity across all checked rules, calculates the earliest reset time when any limit will refresh, and determines the appropriate retry-after value for rate limit headers.\n\n#### Common Pitfalls in Tier Evaluation\n\n⚠️ **Pitfall: Evaluating All Tiers Even After Failure**\n\nA common mistake in multi-tier rate limiting implementations is continuing to evaluate all applicable rules even after one has already denied the request. This occurs when developers implement rule checking as a validation pipeline that collects all failures rather than a gate system that stops at the first barrier.\n\n```\n// WRONG: Evaluating all rules regardless of failures\nresults := make([]*RateLimitResult, 0)\nfor _, rule := range applicableRules {\n    result := evaluateRule(rule, request)  \n    results = append(results, result)\n}\n// Then checking if any failed...\n```\n\nThis approach wastes Redis operations, increases response latency, and provides confusing user feedback since the client receives information about multiple limit violations when only the first one is actionable. The correct approach uses immediate return on first failure:\n\n```\n// CORRECT: Short-circuit evaluation\nfor _, rule := range applicableRules {\n    result := evaluateRule(rule, request)\n    if !result.allowed {\n        return result  // Immediate failure, no further evaluation\n    }\n}\n```\n\n⚠️ **Pitfall: Inconsistent Rule Priority Handling**\n\nAnother frequent error involves inconsistent interpretation of priority values across different parts of the system. Some implementations treat higher numeric values as higher priority while others use the reverse convention, leading to rules being evaluated in the wrong order.\n\nThe problem becomes especially acute when priority ties need resolution - without consistent tie-breaking logic, the same request might be evaluated differently across application instances, leading to unpredictable rate limiting behavior. Always use explicit priority ordering with well-defined tie-breaking rules based on secondary criteria like rule creation time or alphabetical rule ID ordering.\n\n⚠️ **Pitfall: Ignoring Rule Pattern Overlap**\n\nOverlapping rule patterns can create unexpected interactions where multiple rules apply to the same request dimension but specify conflicting limits. For example, a general rule `user:*:api:*` allowing 1000 requests per hour might conflict with a specific rule `user:premium_*:api:orders` allowing 5000 requests per hour for premium users accessing the orders endpoint.\n\nWithout proper conflict resolution, the system might apply the more restrictive limit even when a more specific, permissive rule should take precedence. Always implement explicit precedence rules that favor more specific patterns and higher priority values when resolving overlaps.\n\n### Rate Limit Key Composition\n\nThe foundation of effective multi-tier rate limiting lies in **Redis key composition** - the systematic construction of unique identifiers that organize rate limit state across different dimensions, time windows, and algorithms. Key composition determines not only how rate limiting data is stored and retrieved, but also how efficiently the system can scale, how clearly administrators can debug issues, and how reliably the system maintains consistency under load.\n\nEffective key composition balances several competing requirements: keys must be **unique** to prevent collisions between different rate limits, **predictable** to enable debugging and monitoring, **efficient** to minimize Redis memory usage, and **hierarchical** to support operations like bulk deletion or pattern-based queries. The key structure also influences Redis clustering behavior, as keys determine which Redis node stores each rate limit counter.\n\nThe **base key structure** follows a hierarchical pattern that encodes the rate limit dimension, resource identifier, time window, and algorithm type:\n\n```\nratelimit:{dimension}:{resource}:{algorithm}:{window}:{additional_context}\n```\n\nThis structure provides natural groupings that align with Redis operations and administrative needs. The `ratelimit:` prefix enables easy identification of rate limiting keys in mixed-use Redis instances. The dimension component (`user`, `ip`, `api`, `global`) creates logical separation between different rate limit types. The resource identifier specifies the exact entity being limited, while algorithm and window components ensure that different rate limiting approaches for the same resource don't interfere.\n\n**User-scoped rate limit keys** incorporate the user identifier and any relevant context that affects the user's rate limit tier:\n\n| Key Pattern | Example | Use Case | Considerations |\n|-------------|---------|----------|---------------|\n| `ratelimit:user:{user_id}:token_bucket:{window}` | `ratelimit:user:user123:token_bucket:3600` | Per-user global limits | Simple, efficient, scales linearly |\n| `ratelimit:user:{user_id}:api:{endpoint}:sliding_counter:{window}` | `ratelimit:user:user123:api:orders:sliding_counter:3600` | Per-user API endpoint limits | More granular, higher memory usage |\n| `ratelimit:user:{user_tier}:{user_id}:global:token_bucket:{window}` | `ratelimit:user:premium:user123:global:token_bucket:3600` | Tier-aware user limits | Enables tier-specific limit policies |\n\nThe user ID component typically uses the application's native user identifier (database primary key, UUID, or username) rather than derived values, ensuring consistency across application restarts and reducing the likelihood of key collisions during user management operations.\n\n**IP-scoped rate limit keys** must handle the complexities of network addressing, proxy scenarios, and IPv4/IPv6 compatibility:\n\n| Key Pattern | Example | Use Case | Special Handling |\n|-------------|---------|----------|-----------------|\n| `ratelimit:ip:{ip_address}:sliding_log:{window}` | `ratelimit:ip:192.168.1.10:sliding_log:3600` | Direct IP rate limiting | IPv6 colon escaping required |\n| `ratelimit:ip:{ip_subnet}:token_bucket:{window}` | `ratelimit:ip:192.168.1.0_24:token_bucket:3600` | Subnet-based rate limiting | Subnet calculation for each request |\n| `ratelimit:ip:{ip_hash}:api:{endpoint}:sliding_counter:{window}` | `ratelimit:ip:a1b2c3d4:api:upload:sliding_counter:300` | Privacy-preserving IP limits | Hash consistency across instances |\n\nIP address handling requires careful consideration of IPv6 compatibility, as IPv6 addresses contain colons that conflict with Redis key delimiters. The system typically normalizes IPv6 addresses to a canonical form and replaces colons with underscores or uses base64 encoding for the address component.\n\n**API-scoped rate limit keys** organize limits by endpoint, HTTP method, and resource type to provide fine-grained control over different API operations:\n\n| Key Pattern | Example | Use Case | Granularity Trade-offs |\n|-------------|---------|----------|----------------------|\n| `ratelimit:api:{endpoint}:sliding_counter:{window}` | `ratelimit:api:orders:sliding_counter:3600` | Endpoint-level rate limiting | Coarse-grained, efficient |\n| `ratelimit:api:{method}:{endpoint}:token_bucket:{window}` | `ratelimit:api:POST:orders:token_bucket:3600` | Method-specific rate limiting | Distinguishes read/write operations |\n| `ratelimit:api:{service}:{version}:{endpoint}:sliding_log:{window}` | `ratelimit:api:orders:v2:create:sliding_log:300` | Versioned API rate limiting | Supports API evolution and migration |\n\nAPI endpoint normalization presents significant challenges in key composition, as URLs may contain variable path parameters, query strings, and other dynamic elements. The system typically applies endpoint normalization rules that replace path parameters with placeholders (`/orders/123` becomes `/orders/{id}`) and ignore query parameters unless they're explicitly included in the rate limiting policy.\n\n**Global rate limit keys** aggregate usage across all users, IPs, and endpoints to enforce system-wide capacity constraints:\n\n| Key Pattern | Example | Use Case | Aggregation Method |\n|-------------|---------|----------|-------------------|\n| `ratelimit:global:requests:sliding_counter:{window}` | `ratelimit:global:requests:sliding_counter:60` | Total request rate limiting | Simple counter increment |\n| `ratelimit:global:{resource_type}:token_bucket:{window}` | `ratelimit:global:database_writes:token_bucket:3600` | Resource-specific global limits | Categorized request counting |\n| `ratelimit:global:{region}:api:{endpoint}:sliding_log:{window}` | `ratelimit:global:us-west:api:search:sliding_log:300` | Regional global rate limiting | Geographic request distribution |\n\nGlobal rate limits present unique challenges in distributed systems, as they require coordination across all application instances and potentially multiple Redis nodes. The key composition must support efficient aggregation while avoiding hot-spotting on a single Redis key.\n\n**Time window encoding** within rate limit keys determines how algorithm implementations track temporal boundaries and handle window transitions:\n\n| Algorithm Type | Time Window Encoding | Example Component | Window Alignment |\n|----------------|---------------------|-------------------|------------------|\n| `ALGORITHM_TOKEN_BUCKET` | Window duration in seconds | `:3600` | Floating window based on first request |\n| `ALGORITHM_SLIDING_COUNTER` | Current time bucket + duration | `:1609459200:3600` | Fixed time bucket boundaries |\n| `ALGORITHM_SLIDING_WINDOW_LOG` | Window duration only | `:3600` | Sliding based on request timestamps |\n\nThe time window encoding affects both key uniqueness and Redis memory usage patterns. Sliding window counter algorithms generate multiple keys per time window (one per sub-bucket), while token bucket algorithms typically use a single key with embedded timestamp data.\n\n**Redis cluster considerations** influence key composition decisions to ensure optimal data distribution and avoid hot-spotting. The Redis cluster uses CRC16 hashing of the key to determine node assignment, meaning that keys with similar prefixes may cluster on the same node:\n\n> **Design Insight**: Effective key composition serves as the addressing system for distributed rate limiting state, determining not just where data lives but how efficiently it can be accessed, updated, and managed. The key structure becomes the foundation for Redis clustering, monitoring queries, and administrative operations.\n\n#### Key Composition Implementation Strategy\n\nThe `RateLimitRule` structure includes a `key_pattern` field that serves as a template for generating actual Redis keys based on request context. The pattern uses placeholder syntax to inject dynamic values:\n\n| Pattern Component | Placeholder Syntax | Example Pattern | Generated Key |\n|-------------------|-------------------|-----------------|---------------|\n| User ID | `{user_id}` | `ratelimit:user:{user_id}:api:orders` | `ratelimit:user:user123:api:orders` |\n| IP Address | `{ip_address}` | `ratelimit:ip:{ip_address}:global` | `ratelimit:ip:192.168.1.10:global` |\n| API Endpoint | `{api_endpoint}` | `ratelimit:api:{api_endpoint}:sliding_counter` | `ratelimit:api:orders:sliding_counter` |\n| Algorithm Type | `{algorithm}` | `ratelimit:user:{user_id}:{algorithm}` | `ratelimit:user:user123:token_bucket` |\n| Time Window | `{window}` | `ratelimit:global:requests:{window}` | `ratelimit:global:requests:3600` |\n\nThe key composition system includes **validation logic** to ensure generated keys meet Redis requirements and avoid common pitfalls:\n\n- **Length validation**: Redis keys are limited to 512 MB, but practical considerations limit keys to under 250 characters for optimal performance\n- **Character validation**: Keys avoid problematic characters like spaces, newlines, and Redis command separators\n- **Collision detection**: The system validates that different rule patterns cannot generate identical keys under normal operation\n- **Cluster compatibility**: Keys are structured to distribute evenly across Redis cluster nodes\n\n**Key lifecycle management** handles the creation, updates, and cleanup of rate limiting keys as rules change and time windows expire:\n\n1. **Key creation** occurs dynamically when the first request matching a rule arrives, with initial values set according to the algorithm type\n2. **Key updates** happen atomically through Lua scripts that ensure consistency during concurrent access\n3. **Key expiration** uses Redis TTL mechanisms to automatically clean up expired time windows and reduce memory usage\n4. **Key migration** handles rule changes that affect key patterns, typically through background processes that move data to new key structures\n\nThe key composition strategy also supports **administrative operations** like bulk key deletion, usage reporting, and debugging through predictable key patterns that enable efficient Redis pattern matching and iteration.\n\n![Data Model Relationships](./diagrams/data-model.svg)\n\n![Rate Limit Check Sequence](./diagrams/rate-check-sequence.svg)\n\n### Implementation Guidance\n\nMulti-tier rate limiting requires careful orchestration of rule management, key composition, and evaluation logic to achieve both correctness and performance. The implementation bridges the conceptual design with practical Redis operations while handling the complexities of distributed coordination and failure scenarios.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Rule Storage | YAML configuration files with file watching | Redis-based rule storage with pub/sub updates |\n| Pattern Matching | Simple string templates with Go text/template | Regex-based patterns with named capture groups |\n| Key Composition | String concatenation with validation | Template engine with type-safe placeholders |\n| Tier Coordination | Sequential rule evaluation with early termination | Parallel rule evaluation with context cancellation |\n| Result Caching | In-memory LRU cache with TTL | Redis-based shared cache across instances |\n\n#### Recommended Module Structure\n\n```\ninternal/\n  limiter/\n    multi_tier.go           ← Main multi-tier limiter implementation\n    rule_manager.go         ← Rule loading and matching logic\n    key_composer.go         ← Redis key composition utilities\n    tier_evaluator.go       ← Sequential evaluation with short-circuiting\n    result_aggregator.go    ← Success result combination logic\n  config/\n    rules.yaml             ← Rate limit rule definitions\n    rule_loader.go         ← YAML rule parsing and validation\n    rule_watcher.go        ← File change detection for rule updates\n  storage/\n    redis_operations.go    ← Atomic Redis operations for rule evaluation\n```\n\n#### Infrastructure Starter Code\n\n**Rule Configuration Structure (config/rules.yaml):**\n```yaml\n# Complete working rule configuration\nrules:\n  - id: \"user_global_limit\"\n    name: \"Per-user global rate limit\"\n    key_pattern: \"ratelimit:user:{user_id}:global:token_bucket:{window}\"\n    algorithm: \"token_bucket\"\n    limit: 1000\n    window: \"1h\"\n    burst_limit: 1200\n    priority: 100\n    enabled: true\n    \n  - id: \"ip_burst_protection\"\n    name: \"Per-IP burst protection\"\n    key_pattern: \"ratelimit:ip:{ip_address}:burst:sliding_counter:{window}\"\n    algorithm: \"sliding_window_counter\"\n    limit: 100\n    window: \"1m\"\n    priority: 200\n    enabled: true\n    \n  - id: \"api_endpoint_limit\"\n    name: \"Per-API endpoint rate limit\"\n    key_pattern: \"ratelimit:api:{api_endpoint}:requests:sliding_log:{window}\"\n    algorithm: \"sliding_window_log\"\n    limit: 10000\n    window: \"1h\"\n    priority: 50\n    enabled: true\n```\n\n**Rule Loader Implementation (config/rule_loader.go):**\n```go\npackage config\n\nimport (\n    \"fmt\"\n    \"os\"\n    \"time\"\n    \"gopkg.in/yaml.v2\"\n)\n\n// Complete rule loading with validation\ntype RuleConfig struct {\n    Rules []*RateLimitRule `yaml:\"rules\"`\n}\n\n// LoadRules loads and validates rate limit rules from YAML configuration\nfunc LoadRules(configPath string) ([]*RateLimitRule, error) {\n    data, err := os.ReadFile(configPath)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to read rule config: %w\", err)\n    }\n    \n    var config RuleConfig\n    if err := yaml.Unmarshal(data, &config); err != nil {\n        return nil, fmt.Errorf(\"failed to parse rule config: %w\", err)\n    }\n    \n    // Validate and normalize rules\n    for _, rule := range config.Rules {\n        if err := validateRule(rule); err != nil {\n            return nil, fmt.Errorf(\"invalid rule %s: %w\", rule.ID, err)\n        }\n        \n        // Parse window duration\n        if duration, err := time.ParseDuration(rule.WindowStr); err != nil {\n            return nil, fmt.Errorf(\"invalid window duration for rule %s: %w\", rule.ID, err)\n        } else {\n            rule.Window = duration\n        }\n        \n        // Set timestamps\n        rule.CreatedAt = time.Now()\n        rule.UpdatedAt = time.Now()\n    }\n    \n    return config.Rules, nil\n}\n\nfunc validateRule(rule *RateLimitRule) error {\n    if rule.ID == \"\" {\n        return fmt.Errorf(\"rule ID is required\")\n    }\n    if rule.KeyPattern == \"\" {\n        return fmt.Errorf(\"key pattern is required\")\n    }\n    if rule.Limit <= 0 {\n        return fmt.Errorf(\"limit must be positive\")\n    }\n    \n    // Validate algorithm\n    validAlgorithms := map[string]bool{\n        ALGORITHM_TOKEN_BUCKET:         true,\n        ALGORITHM_SLIDING_WINDOW_LOG:   true,\n        ALGORITHM_SLIDING_COUNTER:      true,\n    }\n    if !validAlgorithms[rule.Algorithm] {\n        return fmt.Errorf(\"unsupported algorithm: %s\", rule.Algorithm)\n    }\n    \n    return nil\n}\n```\n\n**Key Composer Implementation (limiter/key_composer.go):**\n```go\npackage limiter\n\nimport (\n    \"fmt\"\n    \"net\"\n    \"regexp\"\n    \"strings\"\n    \"time\"\n)\n\n// KeyComposer handles Redis key generation from rule patterns\ntype KeyComposer struct {\n    patternCache map[string]*regexp.Regexp\n}\n\n// NewKeyComposer creates a key composer with pattern caching\nfunc NewKeyComposer() *KeyComposer {\n    return &KeyComposer{\n        patternCache: make(map[string]*regexp.Regexp),\n    }\n}\n\n// ComposeKey generates Redis key from rule pattern and request context\nfunc (kc *KeyComposer) ComposeKey(rule *RateLimitRule, req *RateLimitRequest) (string, error) {\n    key := rule.KeyPattern\n    \n    // Replace standard placeholders\n    replacements := map[string]string{\n        \"{user_id}\":      normalizeUserID(req.UserID),\n        \"{ip_address}\":   normalizeIPAddress(req.IPAddress),\n        \"{api_endpoint}\": normalizeAPIEndpoint(req.APIEndpoint),\n        \"{algorithm}\":    rule.Algorithm,\n        \"{window}\":       fmt.Sprintf(\"%d\", int64(rule.Window.Seconds())),\n    }\n    \n    for placeholder, value := range replacements {\n        key = strings.ReplaceAll(key, placeholder, value)\n    }\n    \n    // Validate final key\n    if err := validateRedisKey(key); err != nil {\n        return \"\", fmt.Errorf(\"invalid Redis key generated: %w\", err)\n    }\n    \n    return key, nil\n}\n\n// Utility functions for key normalization\nfunc normalizeUserID(userID string) string {\n    if userID == \"\" {\n        return \"anonymous\"\n    }\n    // Remove problematic characters\n    normalized := regexp.MustCompile(`[^a-zA-Z0-9_-]`).ReplaceAllString(userID, \"_\")\n    if len(normalized) > 64 {\n        normalized = normalized[:64]\n    }\n    return normalized\n}\n\nfunc normalizeIPAddress(ipAddr string) string {\n    ip := net.ParseIP(ipAddr)\n    if ip == nil {\n        return \"invalid_ip\"\n    }\n    \n    // Handle IPv6 - replace colons with underscores\n    if strings.Contains(ipAddr, \":\") {\n        return strings.ReplaceAll(ip.String(), \":\", \"_\")\n    }\n    \n    return ip.String()\n}\n\nfunc normalizeAPIEndpoint(endpoint string) string {\n    // Remove leading slash\n    normalized := strings.TrimPrefix(endpoint, \"/\")\n    \n    // Replace path separators with underscores\n    normalized = strings.ReplaceAll(normalized, \"/\", \"_\")\n    \n    // Replace query parameters and fragments\n    if idx := strings.Index(normalized, \"?\"); idx != -1 {\n        normalized = normalized[:idx]\n    }\n    if idx := strings.Index(normalized, \"#\"); idx != -1 {\n        normalized = normalized[:idx]\n    }\n    \n    // Limit length\n    if len(normalized) > 100 {\n        normalized = normalized[:100]\n    }\n    \n    return normalized\n}\n\nfunc validateRedisKey(key string) error {\n    if len(key) == 0 {\n        return fmt.Errorf(\"key cannot be empty\")\n    }\n    if len(key) > 250 {\n        return fmt.Errorf(\"key too long: %d characters\", len(key))\n    }\n    \n    // Check for problematic characters\n    if strings.ContainsAny(key, \" \\n\\r\\t\") {\n        return fmt.Errorf(\"key contains whitespace characters\")\n    }\n    \n    return nil\n}\n```\n\n#### Core Logic Skeleton\n\n**Multi-Tier Limiter Implementation (limiter/multi_tier.go):**\n```go\npackage limiter\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sort\"\n    \"time\"\n)\n\n// MultiTierLimiter implements hierarchical rate limiting across multiple dimensions\ntype MultiTierLimiter struct {\n    storage      Storage\n    ruleManager  *RuleManager\n    keyComposer  *KeyComposer\n    algorithms   map[string]Algorithm\n    localFallback Limiter\n}\n\n// NewMultiTierLimiter creates a multi-tier rate limiter\nfunc NewMultiTierLimiter(storage Storage, ruleManager *RuleManager) *MultiTierLimiter {\n    return &MultiTierLimiter{\n        storage:     storage,\n        ruleManager: ruleManager,\n        keyComposer: NewKeyComposer(),\n        algorithms:  make(map[string]Algorithm),\n    }\n}\n\n// Check performs multi-tier rate limit evaluation with short-circuit logic\nfunc (mtl *MultiTierLimiter) Check(ctx context.Context, req *RateLimitRequest) (*RateLimitResult, error) {\n    // TODO 1: Get all applicable rules for this request using ruleManager.GetMatchingRules()\n    // Hint: Pass req.UserID, req.IPAddress, req.APIEndpoint to find matching patterns\n    \n    // TODO 2: Sort rules by priority (higher priority first) and algorithm cost\n    // Hint: Use sort.Slice with custom comparison function checking rule.Priority\n    \n    // TODO 3: Evaluate each rule in sequence with short-circuit on first failure\n    // Hint: Call evaluateRule() for each rule, return immediately if result.Allowed == false\n    \n    // TODO 4: If all rules pass, aggregate the most restrictive limits\n    // Hint: Find minimum remaining capacity and earliest reset time across all results\n    \n    // TODO 5: Handle Redis failures by falling back to local rate limiting\n    // Hint: Check error types and call mtl.localFallback.Check() on storage errors\n    \n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// evaluateRule performs rate limit check for a single rule\nfunc (mtl *MultiTierLimiter) evaluateRule(ctx context.Context, rule *RateLimitRule, req *RateLimitRequest) (*RateLimitResult, error) {\n    // TODO 1: Generate Redis key using keyComposer.ComposeKey(rule, req)\n    \n    // TODO 2: Get the appropriate algorithm implementation for rule.Algorithm\n    // Hint: Look up in mtl.algorithms map, return error if not found\n    \n    // TODO 3: Call algorithm.Check() with the generated key and required tokens\n    // Hint: Use req.Tokens for token count, handle context cancellation\n    \n    // TODO 4: Populate result with rule metadata (rule ID, algorithm type)\n    \n    // TODO 5: Apply burst limit adjustments if rule has burst_limit configured\n    // Hint: For token bucket, allow temporary exceeding of base limit up to burst_limit\n    \n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// Preview checks rate limit status without updating counters\nfunc (mtl *MultiTierLimiter) Preview(ctx context.Context, req *RateLimitRequest) (*RateLimitResult, error) {\n    // TODO 1: Similar to Check() but call Preview() on individual algorithms\n    // TODO 2: Return aggregate status without modifying any counters\n    return nil, fmt.Errorf(\"not implemented\")\n}\n```\n\n**Rule Manager Implementation (limiter/rule_manager.go):**\n```go\npackage limiter\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"regexp\"\n    \"strings\"\n    \"sync\"\n)\n\n// RuleManager handles rule storage, matching, and updates\ntype RuleManager struct {\n    rules       map[string]*RateLimitRule\n    userIndex   map[string][]*RateLimitRule\n    ipIndex     map[string][]*RateLimitRule  \n    apiIndex    map[string][]*RateLimitRule\n    globalRules []*RateLimitRule\n    mutex       sync.RWMutex\n}\n\n// NewRuleManager creates a rule manager with indexing\nfunc NewRuleManager() *RuleManager {\n    return &RuleManager{\n        rules:     make(map[string]*RateLimitRule),\n        userIndex: make(map[string][]*RateLimitRule),\n        ipIndex:   make(map[string][]*RateLimitRule),\n        apiIndex:  make(map[string][]*RateLimitRule),\n    }\n}\n\n// GetMatchingRules returns all rules applicable to the given request context\nfunc (rm *RuleManager) GetMatchingRules(userID, ipAddress, apiEndpoint string) []*RateLimitRule {\n    rm.mutex.RLock()\n    defer rm.mutex.RUnlock()\n    \n    // TODO 1: Check user-specific rules by looking up userID in userIndex\n    // TODO 2: Check IP-specific rules by looking up ipAddress in ipIndex  \n    // TODO 3: Check API-specific rules by looking up apiEndpoint in apiIndex\n    // TODO 4: Always include global rules from globalRules slice\n    // TODO 5: Deduplicate rules that match multiple patterns\n    // TODO 6: Filter out disabled rules (rule.Enabled == false)\n    // Hint: Use map[string]bool to track seen rule IDs and avoid duplicates\n    \n    return nil\n}\n\n// LoadRules loads rules from configuration and rebuilds indices\nfunc (rm *RuleManager) LoadRules(configPath string) error {\n    rm.mutex.Lock()\n    defer rm.mutex.Unlock()\n    \n    // TODO 1: Call config.LoadRules() to parse YAML configuration\n    // TODO 2: Clear existing rules and indices\n    // TODO 3: Rebuild rule indices by pattern type\n    // TODO 4: Validate that no two rules generate conflicting Redis keys\n    \n    return fmt.Errorf(\"not implemented\")\n}\n\n// buildIndices creates lookup indices for efficient rule matching\nfunc (rm *RuleManager) buildIndices() {\n    // TODO 1: Clear all existing indices\n    // TODO 2: Iterate through all rules and categorize by key pattern\n    // TODO 3: Rules with {user_id} patterns go into userIndex\n    // TODO 4: Rules with {ip_address} patterns go into ipIndex\n    // TODO 5: Rules with {api_endpoint} patterns go into apiIndex\n    // TODO 6: Rules matching all requests go into globalRules\n    // Hint: Use strings.Contains() to detect pattern placeholders\n}\n```\n\n#### Milestone Checkpoint\n\nAfter implementing multi-tier rate limiting, verify the following behaviors:\n\n**Test Command**: \n```bash\ngo test ./internal/limiter/... -v -run TestMultiTier\n```\n\n**Expected Behavior**:\n1. **Rule Loading**: Configuration loads successfully with validation errors for malformed rules\n2. **Pattern Matching**: Rules correctly match requests based on user ID, IP address, and API endpoint patterns\n3. **Short-Circuit Evaluation**: Evaluation stops immediately when the first rule denies a request\n4. **Priority Ordering**: Higher priority rules are evaluated before lower priority rules\n5. **Result Aggregation**: Successful requests return the most restrictive remaining capacity across all checked rules\n\n**Manual Testing**:\n```bash\n# Test per-user rate limiting\ncurl -H \"X-User-ID: testuser\" http://localhost:8080/api/orders\n# Should show X-RateLimit-Remaining header\n\n# Test IP rate limiting  \nfor i in {1..10}; do curl -H \"X-Forwarded-For: 192.168.1.100\" http://localhost:8080/api/search; done\n# Should eventually return 429 Too Many Requests\n\n# Test API endpoint limits\ncurl http://localhost:8080/api/upload  # Different limits than /api/orders\n```\n\n**Signs of Issues**:\n- Rules not matching expected requests → Check pattern normalization logic\n- All rules being evaluated despite failures → Verify short-circuit implementation  \n- Inconsistent rate limiting across instances → Check Redis key composition\n- Poor performance under load → Profile rule matching and Redis operations\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| Rules not triggering | Pattern matching failure | Check generated Redis keys in logs | Fix key normalization functions |\n| Inconsistent rate limiting | Rule priority conflicts | Review rule evaluation order | Implement explicit priority tie-breaking |\n| High Redis CPU usage | Inefficient Lua scripts | Monitor Redis SLOWLOG | Optimize atomic operations |\n| Memory leaks | Key expiration not working | Check Redis key TTLs | Implement proper cleanup |\n\n\n## Redis Backend Integration\n\n> **Milestone(s):** Milestone 3 - Redis Backend Integration\n\nThe transition from local rate limiting to distributed rate limiting represents one of the most critical architectural decisions in building scalable systems. While local rate limiting provides excellent performance and simplicity, it falls apart the moment you deploy multiple application instances. Each instance operates with its own view of request counts, leading to effective limits that are N times higher than intended, where N is the number of instances. Redis backend integration solves this fundamental problem by providing a shared state store that all application instances can access atomically, ensuring that rate limits are enforced accurately across the entire cluster.\n\n### Mental Model: Bank Transaction Processing\n\nUnderstanding atomic check-and-update operations in distributed rate limiting becomes much clearer when we think about how banks handle account transactions. Consider what happens when you try to withdraw money from an ATM. The bank doesn't simply check your balance and then subtract the withdrawal amount in two separate operations—that would create a race condition where multiple ATMs could simultaneously check the same balance and approve withdrawals that collectively exceed your available funds.\n\nInstead, banks use atomic transactions that combine the balance check and deduction into a single, indivisible operation. The ATM sends a request to the bank's central system saying \"if account 12345 has at least $100, subtract $100 and tell me the new balance.\" This atomic check-and-update ensures that no matter how many ATMs are processing transactions simultaneously, the account balance remains consistent and never goes below zero (assuming no overdraft protection).\n\nRate limiting with Redis follows exactly the same pattern. When an application instance wants to allow a request, it can't simply check the current count in Redis and then increment it in a separate operation. Between the check and the increment, another instance might have processed requests that push the count over the limit. Instead, we need atomic check-and-update operations that say \"if the current request count is below the limit, increment the count and tell me if the request is allowed.\"\n\nThis banking analogy extends to other aspects of our Redis integration. Just as banks have redundant systems and backup procedures for when the main transaction system fails, our rate limiter needs graceful degradation strategies when Redis becomes unavailable. And just as banks use connection pooling to efficiently handle thousands of simultaneous ATM transactions, our Redis integration uses connection pooling to manage the network resources efficiently across multiple application instances.\n\nThe key insight here is that distributed rate limiting isn't just about storing counters in a shared location—it's about ensuring that the fundamental check-and-update operation remains atomic even when performed by multiple processes across a network. This atomicity requirement drives every aspect of our Redis integration design, from Lua script implementation to connection management strategies.\n\n### Lua Script Design\n\nRedis Lua scripts provide the atomicity guarantees we need for distributed rate limiting by ensuring that complex multi-step operations execute without interruption from other clients. Unlike regular Redis commands that might be interleaved with operations from other connections, Lua scripts run atomically within Redis, providing the equivalent of database transactions for our rate limiting logic.\n\nThe fundamental challenge in implementing rate limiting algorithms as Lua scripts lies in translating the conceptual algorithms we designed earlier into Redis operations that efficiently manipulate the underlying data structures. Each algorithm has different storage requirements and update patterns, requiring carefully crafted scripts that balance correctness, performance, and memory usage.\n\n**Token Bucket Lua Script Design**\n\nThe token bucket algorithm requires maintaining two pieces of state: the current token count and the timestamp of the last refill operation. Our Lua script must atomically read these values, calculate how many tokens should be added based on elapsed time, update the bucket state, and determine whether the request can be allowed.\n\n| Script Operation | Redis Commands Used | Purpose | Complexity Consideration |\n|-----------------|--------------------| --------|-------------------------|\n| Read current state | `HMGET key tokens last_refill` | Retrieve bucket state | Single round-trip, O(1) |\n| Calculate elapsed time | Lua math operations | Determine refill amount | Time precision handling |\n| Refill tokens | Lua math, bounded by capacity | Update token count | Prevent integer overflow |\n| Check allowance | Compare tokens to request | Rate limit decision | Handle burst scenarios |\n| Update state | `HMSET key tokens new_tokens last_refill now` | Persist new state | Atomic state update |\n| Set expiration | `EXPIRE key window_seconds` | Cleanup old buckets | Memory management |\n\nThe token bucket script must handle several edge cases that make the implementation more complex than a simple counter increment. Time calculations require careful handling of integer precision to avoid drift over long periods. The refill calculation must prevent token counts from exceeding the bucket capacity while handling cases where the elapsed time is so large that multiple full refills should have occurred.\n\n**Sliding Window Counter Lua Script Design**\n\nSliding window counter scripts operate on a hash structure where each field represents a time bucket and its value contains the request count for that bucket. The script must determine the current bucket, increment the appropriate counter, calculate the total count across all buckets within the window, and clean up expired buckets to prevent unbounded memory growth.\n\n| Script Phase | Operations | Redis Commands | Error Handling |\n|-------------|-----------|----------------|----------------|\n| Bucket identification | Calculate current bucket ID from timestamp | Lua math operations | Handle clock skew |\n| Current bucket update | Increment counter for current bucket | `HINCRBY key bucket_id tokens` | Initialize if missing |\n| Window calculation | Sum counts across active buckets | `HMGET key bucket1 bucket2 ...` | Handle missing buckets |\n| Expired cleanup | Remove buckets outside window | `HDEL key expired_bucket1 ...` | Batch deletions |\n| Rate limit decision | Compare total to limit | Lua comparison | Return detailed result |\n| Expiration management | Set key TTL based on window | `EXPIRE key ttl_seconds` | Prevent memory leaks |\n\nThe sliding window counter script faces the challenge of maintaining accuracy while managing memory efficiently. The number of buckets affects both accuracy and memory usage—more buckets provide smoother rate limiting but require more Redis memory and script execution time. The script must balance these trade-offs while ensuring that bucket cleanup doesn't interfere with active rate limiting decisions.\n\n**Sliding Window Log Lua Script Design**\n\nThe sliding window log approach stores individual request timestamps, providing the highest accuracy at the cost of memory usage proportional to the request rate. The Lua script must add new timestamps to a Redis list or sorted set, remove expired timestamps, count remaining timestamps, and make the rate limiting decision—all atomically.\n\n| Implementation Choice | Redis Structure | Script Operations | Trade-offs |\n|----------------------|-----------------|-------------------|-----------|\n| Sorted Set approach | `ZADD key timestamp uuid` | Add, remove by score, count | Memory efficient, complex cleanup |\n| List approach | `LPUSH key timestamp` | Push, trim, count | Simple operations, less precise cleanup |\n| Hybrid approach | Sorted set with periodic cleanup | Add new, batch remove old | Best of both, complex logic |\n\nThe sorted set implementation provides the most precise sliding window behavior because it can efficiently remove timestamps that fall outside the window using `ZREMRANGEBYSCORE`. However, it requires generating unique scores for timestamps that might be identical, typically by appending a random component or sequence number.\n\n> **Design Insight**: The choice between Redis data structures for sliding window log has profound implications for memory usage patterns. Sorted sets provide O(log N) operations but require 16 bytes overhead per entry, while lists provide O(1) append but only O(N) cleanup. For high-traffic keys, this memory difference can determine whether the rate limiter scales economically.\n\n**Script Error Handling and Return Values**\n\nAll Lua scripts must return consistent, structured results that allow the calling application to make informed decisions about request handling and error recovery. The script return format needs to convey not just whether the request is allowed, but also the current state information needed for rate limit headers and monitoring.\n\n| Return Field | Type | Purpose | Example Value |\n|-------------|------|---------|---------------|\n| `allowed` | Boolean | Whether request should proceed | `1` (allowed) or `0` (denied) |\n| `remaining` | Integer | Tokens/requests remaining in window | `45` |\n| `reset_time` | Integer | Unix timestamp when limit resets | `1699123456` |\n| `retry_after` | Integer | Seconds to wait before retrying | `30` |\n| `current_count` | Integer | Current usage within window | `55` |\n| `error` | String | Error message if script failed | `nil` or error description |\n\nScript error handling must distinguish between Redis operational errors (like out of memory) and rate limiting logic errors (like invalid parameters). Operational errors should typically cause the script to return an error result, triggering fallback behavior in the application. Logic errors should return a deny result to fail safely.\n\n> **Architecture Decision: Lua Script Deployment Strategy**\n> - **Context**: Lua scripts can be embedded in application code or loaded into Redis once and called by SHA hash\n> - **Options Considered**: \n>   1. Embed scripts in application code and use `EVAL` for each call\n>   2. Load scripts at startup using `SCRIPT LOAD` and call via `EVALSHA`\n>   3. Hybrid approach with fallback from `EVALSHA` to `EVAL` if script not cached\n> - **Decision**: Use hybrid approach with `EVALSHA` primary and `EVAL` fallback\n> - **Rationale**: Provides performance benefits of cached scripts while handling Redis restarts gracefully. Network bandwidth savings significant for complex scripts.\n> - **Consequences**: Requires script loading logic at startup and error handling for cache misses, but eliminates script transmission overhead for normal operations\n\n### Connection Pool Management\n\nEffective connection pool management forms the backbone of reliable Redis integration, determining both the performance characteristics and failure resilience of the distributed rate limiter. Unlike simple client-server applications where connection management can be relatively straightforward, a distributed rate limiter must handle high concurrency, varying load patterns, and network failures while maintaining low latency for every rate limit decision.\n\nThe fundamental challenge lies in balancing connection resource usage against performance and reliability requirements. Too few connections create bottlenecks during traffic spikes, leading to increased latency and potential timeouts. Too many connections waste system resources and can overwhelm Redis servers. The optimal pool size depends on factors that change dynamically: request volume, Redis response times, network latency, and the concurrency patterns of the application using the rate limiter.\n\n**Connection Pool Sizing Strategy**\n\nConnection pool sizing requires understanding both the mathematical relationships between throughput, latency, and concurrency, and the practical constraints of Redis server capacity and network resources. The pool must accommodate not just average load but also traffic bursts that are common in rate limiting scenarios—after all, rate limiters are most critical exactly when traffic spikes occur.\n\n| Pool Sizing Factor | Calculation Method | Typical Value Range | Monitoring Metric |\n|--------------------|-------------------|--------------------|--------------------|\n| Base pool size | `(average_rps * avg_latency_ms) / 1000` | 5-20 connections | Connection utilization % |\n| Burst capacity | `base_size * burst_multiplier` | 2x-5x base size | Peak concurrent connections |\n| Redis server limit | Server max connections / number of app instances | Varies by Redis config | Server connection count |\n| Network overhead | Account for connection setup/teardown cost | 10-20% buffer | Connection churn rate |\n\nThe connection pool must implement intelligent sizing that adapts to observed load patterns while respecting hard limits imposed by Redis server configuration and network infrastructure. Static pool sizing often leads to either resource waste during low traffic or bottlenecks during high traffic.\n\n**Health Checking and Circuit Breaker Integration**\n\nConnection health checking in a rate limiting context goes beyond simple ping/pong tests because rate limiting requires not just connectivity but also acceptable response times. A Redis connection that takes 5 seconds to respond is effectively unusable for rate limiting, even though it's technically healthy from a connectivity perspective.\n\n| Health Check Type | Test Method | Success Criteria | Failure Response |\n|------------------|-------------|-------------------|------------------|\n| Connectivity check | `PING` command | Response within 100ms | Mark connection unhealthy |\n| Performance check | Simple `INCR` operation | Response within 50ms | Reduce connection priority |\n| Functionality check | Execute minimal rate limit script | Correct result within 100ms | Flag script issues |\n| Memory pressure check | `INFO memory` command | Used memory < 90% | Trigger degradation mode |\n\nThe circuit breaker pattern becomes essential when Redis experiences problems that manifest as slow responses rather than complete failures. A Redis server under memory pressure might accept connections and even respond to commands, but with latencies that make rate limiting ineffective. The circuit breaker must detect these degraded performance conditions and trigger fallback behavior before the entire rate limiting system becomes unresponsive.\n\n**Connection Lifecycle Management**\n\nManaging the lifecycle of Redis connections involves more than simple creation and destruction—it requires handling the various states a connection can be in and the transitions between those states. Connections in a rate limiting system experience different usage patterns than typical application database connections, with potentially bursty traffic and stringent latency requirements.\n\n| Connection State | Characteristics | Transition Triggers | Pool Management Actions |\n|-----------------|----------------|--------------------|-----------------------|\n| Available | Idle, ready for use | Request arrives | Assign to request |\n| Active | Executing Redis command | Command completion | Return to available pool |\n| Degraded | Slow but functional | High latency detected | Mark for replacement |\n| Failed | Connection error occurred | Network/Redis error | Remove and create new |\n| Draining | Being retired gracefully | Pool size reduction | Complete current ops, close |\n\nConnection lifecycle management must handle the reality that Redis connections can fail in various ways—network timeouts, Redis server restarts, memory pressure causing slow responses, or even subtle issues like clock skew affecting time-based operations. The pool manager needs strategies for detecting each type of failure and responding appropriately without causing cascading failures in the rate limiting system.\n\n**Retry Logic and Backoff Strategies**\n\nRetry logic for Redis operations in rate limiting systems requires careful design because failed rate limit checks can't simply be retried indefinitely—the request being rate limited is waiting for a decision, and excessive retry delays defeat the purpose of rate limiting. The retry strategy must balance reliability against latency while avoiding retry storms that could worsen Redis server problems.\n\n| Failure Type | Retry Strategy | Backoff Pattern | Max Retry Time |\n|-------------|---------------|-----------------|----------------|\n| Network timeout | 2-3 retries | Linear: 10ms, 20ms, 30ms | 100ms total |\n| Connection refused | 1 retry with new connection | No backoff | 50ms total |\n| Redis overload | No immediate retry | Exponential for background | Trigger fallback |\n| Script error | 1 retry, then fallback | No backoff | 50ms total |\n\nThe retry logic must integrate with the fallback strategy—if Redis operations are failing frequently enough to trigger retries, the system should consider switching to local fallback mode rather than continuing to hammer the failing Redis infrastructure. This requires monitoring retry rates and making intelligent decisions about when distributed rate limiting is more harmful than helpful.\n\n> **Design Insight**: Connection pool management in distributed rate limiting differs fundamentally from typical database connection pools because rate limiting decisions are latency-critical and failure-sensitive. A database query can be retried or delayed, but a rate limiting decision that takes too long effectively becomes an allow decision, potentially compromising the entire rate limiting scheme.\n\n### Graceful Degradation Strategy\n\nGraceful degradation represents one of the most critical aspects of distributed rate limiting design, as it determines how the system behaves when the shared state store becomes unavailable. The challenge lies in maintaining some level of rate limiting effectiveness while avoiding complete service disruption, requiring careful balance between protection and availability.\n\nWhen Redis becomes unavailable, the distributed rate limiter faces a fundamental choice: fail open (allow all requests) or fail closed (deny all requests). Neither option is ideal—failing open provides no rate limiting protection and could lead to system overload, while failing closed effectively creates a denial-of-service condition. The graceful degradation strategy must provide a third option that preserves some rate limiting capability while maintaining service availability.\n\n**Local Fallback Implementation Strategy**\n\nLocal fallback involves each application instance switching to per-instance rate limiting when the shared Redis backend becomes unavailable. This approach provides continued protection against abuse while maintaining service availability, though with reduced accuracy compared to true distributed rate limiting.\n\nThe key insight is that local rate limiting with adjusted limits can approximate distributed rate limiting behavior. If the distributed system normally allows 1000 requests per minute across 10 application instances, each instance can implement local rate limiting at 100 requests per minute during fallback mode. This provides similar protection levels, though with less accurate enforcement and potential for slightly higher actual limits due to uneven traffic distribution.\n\n| Fallback Aspect | Local Implementation | Accuracy Impact | Mitigation Strategy |\n|-----------------|---------------------|-----------------|-------------------|\n| Rate limit scaling | Divide distributed limit by instance count | Uneven traffic causes over/under limiting | Use dynamic instance discovery |\n| State isolation | Each instance tracks separately | No cross-instance coordination | Monitor aggregate metrics |\n| Rule synchronization | Use last known good configuration | Rules may become stale | Cache rules with TTL |\n| Metrics collection | Local counters only | Missing distributed view | Aggregate in monitoring system |\n\nThe local fallback implementation must handle the transition periods carefully—when Redis becomes available again, instances shouldn't immediately switch back to distributed mode, as this could cause thundering herd problems. Instead, a gradual transition with health checking ensures stable operation.\n\n**Failure Detection and Mode Switching**\n\nAccurate failure detection determines how quickly the system can switch to fallback mode and how effectively it can detect recovery. The failure detection strategy must distinguish between temporary network hiccups that should be retried and genuine Redis failures that require fallback activation.\n\n| Detection Signal | Threshold | Confidence Level | Action Triggered |\n|-----------------|-----------|------------------|------------------|\n| Connection timeout | 3 consecutive failures | High | Immediate fallback |\n| Slow response | >500ms for 10 requests | Medium | Gradual degradation |\n| Redis memory errors | Any OOM response | High | Immediate fallback |\n| Script execution errors | 5 in 60 seconds | Medium | Disable scripts, use simple commands |\n| Network partitions | No response for 30s | High | Full fallback mode |\n\nMode switching must be implemented with hysteresis to prevent oscillation between distributed and local modes. The criteria for entering fallback mode should be more sensitive than the criteria for returning to distributed mode, ensuring that the system doesn't constantly switch back and forth during marginal conditions.\n\n**Rate Limit Accuracy During Degradation**\n\nDuring fallback mode, rate limiting accuracy degrades in predictable ways that must be understood and monitored. The degradation patterns help operations teams understand the current protection level and make informed decisions about additional protective measures.\n\n| Degradation Scenario | Accuracy Impact | Burst Behavior | Recommended Monitoring |\n|---------------------|-----------------|----------------|------------------------|\n| Even traffic distribution | 90-95% of intended limit | Normal burst handling | Per-instance rate metrics |\n| Uneven traffic (80/20 split) | 60-120% of intended limit | Some instances allow full bursts | Traffic distribution monitoring |\n| Single hot instance | Up to 200% of intended limit | Full burst on hot instance | Instance-level alerting |\n| Partial Redis availability | Mixed accuracy across instances | Inconsistent burst behavior | Redis connectivity per instance |\n\nUnderstanding these accuracy patterns allows the system to provide meaningful rate limit headers even during degradation. Clients can receive information about the current rate limiting state and adjust their behavior accordingly.\n\n**Recovery and State Synchronization**\n\nRecovery from fallback mode requires careful orchestration to prevent thundering herd effects and ensure smooth transition back to distributed operation. The challenge lies in synchronizing state between the local fallback counters and the Redis-based distributed state without causing sudden changes in rate limiting behavior.\n\n| Recovery Phase | Actions | Validation | Rollback Criteria |\n|---------------|---------|------------|-------------------|\n| Redis availability confirmation | Health checks pass for 60s | Script execution successful | Any health check failure |\n| Gradual transition start | 10% of requests use Redis | Compare local vs distributed decisions | >50% decision disagreement |\n| Transition scaling | Increase to 50%, then 90% | Monitor error rates | Redis error rate >1% |\n| Full distributed mode | 100% requests use Redis | Full functionality restored | Sustained high error rate |\n\nThe state synchronization strategy must handle the reality that local counters during fallback mode may not accurately represent what the distributed state should be. Rather than trying to perfectly synchronize state, the recovery process should focus on ensuring that the transition doesn't create sudden changes in rate limiting behavior that could surprise clients or cause traffic spikes.\n\n> **Architecture Decision: Fallback Trigger Sensitivity**\n> - **Context**: Need to balance between false positives (unnecessary fallbacks) and false negatives (delayed fallback during real failures)\n> - **Options Considered**: \n>   1. Conservative: Only fallback on complete Redis failure\n>   2. Aggressive: Fallback on any performance degradation\n>   3. Adaptive: Adjust sensitivity based on recent failure patterns\n> - **Decision**: Adaptive approach with configurable base sensitivity\n> - **Rationale**: Different deployments have different tolerance for degraded Redis performance. Some can handle 200ms Redis responses, others need <50ms for effective rate limiting.\n> - **Consequences**: Requires more complex configuration and monitoring, but provides better operational flexibility and fewer false positive fallbacks\n\n### Redis Backend ADR\n\nThe choice of backend storage for distributed rate limiting state represents one of the most fundamental architectural decisions in the system, affecting everything from performance characteristics to operational complexity. This decision impacts not just the immediate implementation but also long-term scalability, operational procedures, and integration patterns with existing infrastructure.\n\n> **Architecture Decision: Redis as Primary Backend Storage**\n> - **Context**: Need shared storage for rate limiting state that supports atomic operations, high performance, and horizontal scaling. Must handle thousands of rate limit checks per second across multiple application instances with sub-10ms latency requirements.\n> - **Options Considered**:\n>   1. Redis with Lua scripts for atomic operations\n>   2. etcd with compare-and-swap operations for consistency\n>   3. PostgreSQL with advisory locks and ACID transactions\n>   4. DynamoDB with conditional writes and TTL\n>   5. Cassandra with lightweight transactions\n> - **Decision**: Redis with Lua scripts as primary backend, with etcd as alternative for environments requiring strong consistency\n> - **Rationale**: Redis provides the optimal combination of performance (sub-millisecond operations), atomic operation support (Lua scripts), memory efficiency for time-series data, and operational maturity for high-traffic systems.\n> - **Consequences**: Requires Redis operational expertise, introduces eventual consistency considerations during network partitions, but provides excellent performance and horizontal scaling capabilities.\n\n**Detailed Options Analysis**\n\nThe storage backend decision required extensive analysis of how each option handles the specific requirements of distributed rate limiting, particularly around atomic operations, performance characteristics, and operational complexity.\n\n| Storage Option | Atomic Operations | Typical Latency | Memory Efficiency | Operational Complexity | Horizontal Scaling |\n|---------------|-------------------|-----------------|-------------------|-----------------------|-------------------|\n| Redis + Lua | Lua scripts | 0.1-1ms | Excellent | Medium | Hash-based sharding |\n| etcd | Compare-and-swap | 1-5ms | Good | High | Raft consensus |\n| PostgreSQL | ACID transactions | 5-20ms | Poor for counters | High | Read replicas only |\n| DynamoDB | Conditional writes | 10-50ms | Good | Low | Automatic |\n| Cassandra | Lightweight transactions | 5-15ms | Good | Very High | Excellent |\n\nRedis emerged as the optimal choice primarily due to its combination of performance and atomic operation support. Lua scripts provide true atomicity for complex rate limiting algorithms, while Redis's in-memory architecture delivers the low latency required for inline rate limiting decisions. The memory efficiency for storing counters and timestamps makes Redis particularly well-suited for the access patterns typical in rate limiting workloads.\n\n**etcd Comparison and Use Cases**\n\netcd represents the primary alternative to Redis, particularly in environments where strong consistency requirements outweigh performance considerations. Understanding when to choose etcd over Redis helps inform deployment decisions and architectural trade-offs.\n\n| Consideration | Redis Advantages | etcd Advantages |\n|--------------|------------------|-----------------|\n| Consistency model | Eventual consistency, faster | Strong consistency, slower |\n| Performance | Sub-millisecond operations | Millisecond operations |\n| Operational complexity | Familiar to most teams | Requires Raft understanding |\n| Failure handling | Manual failover or Redis Sentinel | Automatic leader election |\n| Multi-datacenter | Complex setup | Native support |\n| Kubernetes integration | Requires external setup | Often pre-installed |\n\netcd becomes the preferred choice in environments where rate limiting must integrate with existing Kubernetes control plane infrastructure, or where strong consistency requirements justify the performance trade-offs. For example, financial systems might prefer etcd's consistency guarantees even at the cost of higher latency.\n\n**PostgreSQL and Traditional Database Analysis**\n\nTraditional relational databases like PostgreSQL initially seem attractive for rate limiting because they provide strong consistency guarantees and familiar operational models. However, deeper analysis reveals fundamental mismatches with rate limiting requirements.\n\nThe primary challenge with PostgreSQL for rate limiting lies in the access patterns. Rate limiting requires high-frequency updates to counter values with minimal read complexity—essentially the inverse of typical web application database usage. PostgreSQL's MVCC (Multi-Version Concurrency Control) system creates overhead for high-frequency counter updates, and the persistence guarantees designed for critical business data become unnecessary overhead for rate limiting state.\n\n| Database Limitation | Impact on Rate Limiting | Mitigation Cost |\n|--------------------|------------------------|----------------|\n| MVCC overhead for updates | Higher CPU usage for counters | Requires more database capacity |\n| Disk I/O for durability | Slower response times | SSD required, higher costs |\n| Connection overhead | Fewer concurrent rate checks | Larger connection pools needed |\n| Limited atomic operations | Complex application logic | More application complexity |\n\nPostgreSQL remains viable for rate limiting in environments where extreme consistency requirements justify the performance costs, or where existing PostgreSQL expertise and infrastructure make operational complexity the primary concern.\n\n**Cloud-Native Options Analysis**\n\nCloud-native storage options like DynamoDB offer compelling operational simplicity but introduce different trade-offs around performance predictability and cost scaling.\n\nDynamoDB's conditional write operations provide the atomicity needed for rate limiting, and automatic scaling eliminates capacity planning concerns. However, the performance characteristics vary significantly based on provisioned capacity and hot key patterns common in rate limiting workloads. The pricing model also creates challenges for high-traffic rate limiting where the cost can become substantial.\n\n| Cloud Option | Operational Burden | Performance Predictability | Cost Scaling | Lock-in Risk |\n|-------------|-------------------|---------------------------|--------------|-------------|\n| DynamoDB | Very Low | Variable | High at scale | High |\n| Cloud Redis | Low | Predictable | Moderate | Medium |\n| Cloud SQL | Medium | Predictable | Low | Low |\n\nThe choice between cloud-native and self-managed options often depends more on organizational factors than technical requirements. Teams with strong infrastructure automation capabilities may prefer self-managed Redis for cost control and performance predictability, while teams prioritizing operational simplicity may accept the trade-offs of managed cloud services.\n\n**Future Migration Considerations**\n\nThe storage backend decision should consider not just current requirements but also likely evolution paths. Rate limiting systems often start simple and grow more sophisticated over time, requiring migration strategies that minimize service disruption.\n\nRedis provides good migration paths to other storage options because its simple key-value model can be replicated in most other systems. The atomic operation patterns established using Lua scripts can be translated to stored procedures in databases or conditional operations in other NoSQL systems.\n\nThe modular storage interface design allows for backend migration without changing the core rate limiting logic. This architectural separation enables gradual migration strategies where different rate limiting tiers or different types of keys can use different storage backends during transition periods.\n\n> **Design Insight**: The storage backend choice for distributed rate limiting differs significantly from typical application data storage decisions. Rate limiting requires high write throughput with simple access patterns, making it more similar to metrics collection or event streaming workloads than traditional CRUD operations. This fundamental difference in access patterns explains why Redis often outperforms traditional databases for this specific use case.\n\n### Implementation Guidance\n\nThis implementation guidance focuses on building production-ready Redis integration for distributed rate limiting, with emphasis on the connection management, atomic operations, and graceful degradation patterns that separate robust systems from fragile prototypes.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Redis Client | `go-redis/redis/v8` with basic clustering | `go-redis/redis/v8` with Redis Sentinel for HA |\n| Connection Pooling | Built-in go-redis connection pool | Custom pool with circuit breaker integration |\n| Lua Script Management | Embedded scripts with EVALSHA fallback | External script files with hot reload |\n| Health Checking | Simple PING commands | Comprehensive health with performance metrics |\n| Configuration Management | Static YAML configuration | Dynamic config with Redis pub/sub updates |\n| Monitoring Integration | Basic metrics collection | Prometheus metrics with custom collectors |\n\nFor production deployments, the advanced options provide the reliability and observability needed to debug issues under load. The simple options work well for development and testing environments.\n\n**B. Recommended Module Structure**\n\n```\ninternal/redis/\n├── client.go              ← Redis client wrapper with pooling\n├── scripts/\n│   ├── token_bucket.lua    ← Token bucket Lua script\n│   ├── sliding_counter.lua ← Sliding window counter script\n│   └── sliding_log.lua     ← Sliding window log script\n├── storage.go             ← RedisStorage implementation\n├── health.go              ← Health checking and circuit breaker\n├── fallback.go            ← Local fallback implementation\n└── config.go              ← Redis configuration structures\n\ninternal/scripts/\n├── loader.go              ← Lua script loading and caching\n└── registry.go           ← Script SHA management\n```\n\nThis structure separates Redis-specific concerns from the core rate limiting algorithms while providing clear boundaries for testing and future backend alternatives.\n\n**C. Infrastructure Starter Code**\n\n**Redis Configuration and Client Setup**\n\n```go\npackage redis\n\nimport (\n    \"context\"\n    \"time\"\n    \n    \"github.com/go-redis/redis/v8\"\n)\n\ntype RedisConfig struct {\n    Addresses      []string      `yaml:\"addresses\"`\n    Password       string        `yaml:\"password\"`\n    DB            int           `yaml:\"db\"`\n    PoolSize      int           `yaml:\"pool_size\"`\n    ReadTimeout   time.Duration `yaml:\"read_timeout\"`\n    WriteTimeout  time.Duration `yaml:\"write_timeout\"`\n    DialTimeout   time.Duration `yaml:\"dial_timeout\"`\n}\n\nfunc NewRedisStorage(config RedisConfig) (*RedisStorage, error) {\n    var client redis.UniversalClient\n    \n    if len(config.Addresses) == 1 {\n        client = redis.NewClient(&redis.Options{\n            Addr:         config.Addresses[0],\n            Password:     config.Password,\n            DB:           config.DB,\n            PoolSize:     config.PoolSize,\n            ReadTimeout:  config.ReadTimeout,\n            WriteTimeout: config.WriteTimeout,\n            DialTimeout:  config.DialTimeout,\n        })\n    } else {\n        client = redis.NewClusterClient(&redis.ClusterOptions{\n            Addrs:        config.Addresses,\n            Password:     config.Password,\n            PoolSize:     config.PoolSize,\n            ReadTimeout:  config.ReadTimeout,\n            WriteTimeout: config.WriteTimeout,\n            DialTimeout:  config.DialTimeout,\n        })\n    }\n    \n    storage := &RedisStorage{\n        client: client,\n        config: config,\n        scripts: make(map[string]string),\n    }\n    \n    // Load Lua scripts on startup\n    if err := storage.loadScripts(); err != nil {\n        return nil, fmt.Errorf(\"failed to load scripts: %w\", err)\n    }\n    \n    return storage, nil\n}\n```\n\n**Lua Script Management**\n\n```go\npackage redis\n\nimport (\n    _ \"embed\"\n    \"crypto/sha1\"\n    \"fmt\"\n)\n\n//go:embed scripts/token_bucket.lua\nvar tokenBucketScript string\n\n//go:embed scripts/sliding_counter.lua\nvar slidingCounterScript string\n\n//go:embed scripts/sliding_log.lua\nvar slidingLogScript string\n\ntype RedisStorage struct {\n    client  redis.UniversalClient\n    config  RedisConfig\n    scripts map[string]string // script name -> SHA hash\n}\n\nfunc (r *RedisStorage) loadScripts() error {\n    scripts := map[string]string{\n        \"token_bucket\":    tokenBucketScript,\n        \"sliding_counter\": slidingCounterScript,\n        \"sliding_log\":     slidingLogScript,\n    }\n    \n    for name, script := range scripts {\n        sha, err := r.client.ScriptLoad(context.Background(), script).Result()\n        if err != nil {\n            return fmt.Errorf(\"failed to load script %s: %w\", name, err)\n        }\n        r.scripts[name] = sha\n    }\n    \n    return nil\n}\n\nfunc (r *RedisStorage) ExecuteLua(ctx context.Context, script string, keys []string, args []interface{}) (interface{}, error) {\n    sha, exists := r.scripts[script]\n    if !exists {\n        return nil, fmt.Errorf(\"script %s not found\", script)\n    }\n    \n    // Try EVALSHA first for performance\n    result, err := r.client.EvalSha(ctx, sha, keys, args...).Result()\n    if err != nil {\n        // If script not in cache, fall back to EVAL\n        if err.Error() == \"NOSCRIPT No matching script. Please use EVAL.\" {\n            result, err = r.client.Eval(ctx, r.getScriptContent(script), keys, args...).Result()\n        }\n    }\n    \n    return result, err\n}\n```\n\n**Health Checking with Circuit Breaker**\n\n```go\npackage redis\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\ntype HealthChecker struct {\n    storage         *RedisStorage\n    mu              sync.RWMutex\n    healthy         bool\n    lastCheck       time.Time\n    failureCount    int\n    circuitOpen     bool\n    nextRetryTime   time.Time\n}\n\nfunc NewHealthChecker(storage *RedisStorage) *HealthChecker {\n    hc := &HealthChecker{\n        storage: storage,\n        healthy: true,\n    }\n    \n    go hc.startHealthChecking()\n    return hc\n}\n\nfunc (hc *HealthChecker) IsHealthy() bool {\n    hc.mu.RLock()\n    defer hc.mu.RUnlock()\n    return hc.healthy && !hc.circuitOpen\n}\n\nfunc (hc *HealthChecker) startHealthChecking() {\n    ticker := time.NewTicker(5 * time.Second)\n    defer ticker.Stop()\n    \n    for range ticker.C {\n        hc.performHealthCheck()\n    }\n}\n\nfunc (hc *HealthChecker) performHealthCheck() {\n    ctx, cancel := context.WithTimeout(context.Background(), time.Second)\n    defer cancel()\n    \n    start := time.Now()\n    err := hc.storage.client.Ping(ctx).Err()\n    latency := time.Since(start)\n    \n    hc.mu.Lock()\n    defer hc.mu.Unlock()\n    \n    if err != nil || latency > 100*time.Millisecond {\n        hc.failureCount++\n        if hc.failureCount >= 3 {\n            hc.healthy = false\n            hc.circuitOpen = true\n            hc.nextRetryTime = time.Now().Add(30 * time.Second)\n        }\n    } else {\n        hc.failureCount = 0\n        hc.healthy = true\n        if time.Now().After(hc.nextRetryTime) {\n            hc.circuitOpen = false\n        }\n    }\n    \n    hc.lastCheck = time.Now()\n}\n```\n\n**D. Core Logic Skeleton Code**\n\n**Token Bucket Redis Implementation**\n\n```go\nfunc (r *RedisStorage) CheckAndUpdate(ctx context.Context, key string, limit int64, window time.Duration) (bool, int64, time.Time, error) {\n    // TODO 1: Prepare Redis key for token bucket (prefix + key)\n    // TODO 2: Get current timestamp in nanoseconds for precise timing\n    // TODO 3: Calculate refill rate as tokens per nanosecond (limit / window)\n    // TODO 4: Execute token bucket Lua script with key, current time, limit, refill rate, requested tokens\n    // TODO 5: Parse script result - [allowed, remaining_tokens, reset_time]\n    // TODO 6: Convert reset_time from nanoseconds to time.Time\n    // TODO 7: Return whether allowed, remaining tokens, and reset time\n    // Hint: Use time.Now().UnixNano() for high precision timestamps\n    // Hint: Script should handle token refill calculation atomically\n}\n```\n\n**Sliding Window Counter Implementation**\n\n```go\nfunc (r *RedisStorage) CheckSlidingWindow(ctx context.Context, key string, limit int64, window time.Duration, buckets int) (*RateLimitResult, error) {\n    // TODO 1: Calculate current bucket ID based on current time and bucket size\n    // TODO 2: Determine which buckets fall within the current window\n    // TODO 3: Execute sliding counter Lua script with key, current bucket, window buckets, limit\n    // TODO 4: Parse script result - [allowed, current_count, window_total, oldest_bucket]\n    // TODO 5: Calculate remaining quota (limit - window_total)\n    // TODO 6: Calculate reset time (when oldest bucket expires)\n    // TODO 7: Return structured RateLimitResult with all computed values\n    // Hint: Bucket size = window / buckets, bucket ID = current_time / bucket_size\n    // Hint: Script should clean up expired buckets to prevent memory growth\n}\n```\n\n**Graceful Degradation Manager**\n\n```go\nfunc (d *DistributedLimiter) Check(ctx context.Context, req RateLimitRequest) (*RateLimitResult, error) {\n    // TODO 1: Check if Redis backend is healthy using health checker\n    // TODO 2: If healthy, attempt Redis-based rate limiting with timeout\n    // TODO 3: If Redis fails or times out, record failure for circuit breaker\n    // TODO 4: On Redis failure, switch to local fallback with adjusted limits\n    // TODO 5: Scale down limits for local mode (divide by estimated instance count)\n    // TODO 6: Include fallback mode indicator in result for monitoring\n    // TODO 7: Implement retry logic with exponential backoff for Redis recovery\n    // Hint: Use context.WithTimeout for Redis operations to prevent hanging\n    // Hint: Local limits should be distributed_limit / instance_count\n}\n```\n\n**E. Language-Specific Hints for Go**\n\n- Use `go-redis/redis/v8` for Redis connectivity with built-in connection pooling\n- Embed Lua scripts using `//go:embed` directive to avoid runtime file dependencies\n- Implement proper context cancellation for all Redis operations to prevent goroutine leaks\n- Use `sync.RWMutex` for health checker state to allow concurrent reads\n- Consider using `time.UnixNano()` for high-precision timestamps in rate limiting calculations\n- Implement proper error wrapping with `fmt.Errorf(\"operation failed: %w\", err)`\n\n**F. Milestone Checkpoint**\n\nAfter implementing Redis backend integration, verify the following functionality:\n\n**Basic Connectivity Test:**\n```bash\n# Start Redis locally or use Docker\ndocker run -d --name redis-test -p 6379:6379 redis:7\n\n# Test basic Redis operations\ngo run cmd/redis-test/main.go\n```\n\nExpected behavior: Connection established, Lua scripts loaded successfully, health checker reports healthy status.\n\n**Rate Limiting Functionality Test:**\n```bash\n# Run integration test with Redis backend\ngo test -v ./internal/redis/... -tags=integration\n```\n\nExpected behavior: Token bucket script executes correctly, sliding window counters increment properly, fallback triggers when Redis is stopped.\n\n**Performance Verification:**\n```bash\n# Benchmark Redis vs local performance\ngo test -bench=BenchmarkRedisRateLimit ./internal/redis/\n```\n\nExpected results: Redis operations complete in <5ms p99, throughput >1000 ops/sec per connection.\n\nSigns of problems and debugging steps:\n- **Scripts not loading**: Check Redis version (requires 2.6+), verify script syntax\n- **Connection timeouts**: Check network connectivity, Redis memory usage, connection pool size\n- **Inconsistent results**: Verify system clock synchronization, check for Redis clustering issues\n\n\n## Consistent Hashing and Sharding\n\n> **Milestone(s):** Milestone 4 - Consistent Hashing & Sharding\n\nThe transition from single-node Redis to a distributed Redis cluster represents a fundamental scaling challenge in distributed rate limiting. While a single Redis instance can handle thousands of rate limit checks per second, production systems often require tens or hundreds of thousands of operations per second across millions of rate limit keys. This milestone transforms our centralized rate limiting system into a horizontally scalable distributed system that can grow by adding more Redis nodes while maintaining consistent performance and minimizing operational complexity during cluster topology changes.\n\n### Mental Model: Library Book Distribution\n\nThink of distributing rate limit state across multiple Redis nodes like managing a university library system with multiple branch locations. In a traditional single-location library, all books are stored in one building, and patrons must visit that specific location to access any book. This works well for small collections, but as the collection grows to millions of books and thousands of daily visitors, the single location becomes overwhelmed.\n\nThe solution is to distribute books across multiple branch libraries using a systematic approach. However, you cannot randomly scatter books across branches—patrons need a predictable way to find any specific book. A naive approach might be alphabetical distribution: books A-F go to Branch 1, G-M to Branch 2, and so on. But this creates problems when you need to add a new branch—suddenly you must physically move thousands of books to rebalance the collection, disrupting service for weeks.\n\n**Consistent hashing** solves this problem elegantly. Instead of dividing the alphabet into fixed ranges, imagine arranging the branches in a circle based on their unique characteristics (like their postal codes). Each book is assigned to the first branch you encounter when walking clockwise from the book's hash position on the circle. When you add a new branch, you only need to move books from one adjacent branch to maintain balance. When a branch temporarily closes for maintenance, patrons are automatically redirected to the next available branch clockwise.\n\n**Hot key detection** is like noticing that certain popular textbooks are being requested so frequently at one branch that students form long lines. The library system responds by placing copies of these popular books at multiple branches, reducing the load on any single location.\n\nThis library analogy maps directly to our distributed rate limiting system:\n\n- **Books** → Rate limit keys (user:123, api:/login, ip:192.168.1.1)\n- **Branch libraries** → Redis nodes in the cluster\n- **Book locations** → Which Redis node stores each rate limit counter\n- **Catalog lookup** → Consistent hash function determining node assignment\n- **Branch closure** → Redis node failure requiring automatic failover\n- **Popular textbooks** → Hot keys that need replication across multiple nodes\n- **Adding new branches** → Scaling by adding Redis nodes with minimal data movement\n\n### Consistent Hash Ring Design\n\nThe consistent hash ring forms the mathematical foundation for distributing rate limit keys across Redis nodes while minimizing redistribution during topology changes. Unlike traditional hash-based sharding where adding nodes requires rehashing all keys, consistent hashing ensures that only a small fraction of keys need to move when the cluster topology changes.\n\n#### Hash Ring Mathematics and Virtual Nodes\n\nThe consistent hash ring maps both Redis nodes and rate limit keys onto a circular hash space, typically using SHA-1 or SHA-256 to produce a 160-bit or 256-bit hash space. The ring conceptually represents all possible hash values arranged in a circle, where the maximum hash value wraps around to zero.\n\nEach Redis node is assigned multiple positions on the ring called **virtual nodes** or **vnodes**. Virtual nodes solve the fundamental problem of uneven load distribution that occurs when physical nodes are hashed to random positions on the ring. Without virtual nodes, adding or removing a single physical node could create scenarios where one node handles 80% of the keys while others handle only 5%.\n\nVirtual nodes work by creating multiple hash positions for each physical Redis node using different hash inputs. For example, if Redis node `redis-1` at address `10.0.1.100:6379` uses 150 virtual nodes, we generate positions by hashing:\n- `redis-1:vnode:0` → hash position 0x1a2b3c4d...\n- `redis-1:vnode:1` → hash position 0x5e6f7g8h...\n- `redis-1:vnode:2` → hash position 0x9i0j1k2l...\n- ... continuing for all 150 virtual nodes\n\nThis creates 150 different positions on the ring where `redis-1` is responsible for handling keys. The more virtual nodes per physical node, the more evenly distributed the load becomes, approaching perfect balance as the virtual node count increases.\n\n| Virtual Nodes Per Physical Node | Expected Load Variance | Memory Overhead | Rebalancing Efficiency |\n|--------------------------------|----------------------|----------------|----------------------|\n| 50 | ±15% from perfect balance | Low | Good |\n| 150 | ±8% from perfect balance | Medium | Better |\n| 500 | ±3% from perfect balance | High | Excellent |\n| 1000 | ±2% from perfect balance | Very High | Excellent |\n\n> **Decision: Virtual Node Count Selection**\n> - **Context**: Need to balance load distribution accuracy against memory overhead and lookup performance\n> - **Options Considered**: 50, 150, 500, or 1000 virtual nodes per physical node\n> - **Decision**: Use 150 virtual nodes per physical node as the default\n> - **Rationale**: Provides ±8% load variance which is acceptable for rate limiting workloads, while keeping memory overhead reasonable and maintaining fast lookup performance. Can be configured higher for clusters with extreme hot key problems.\n> - **Consequences**: Enables good load balance with reasonable memory usage. Lookup time increases slightly due to larger ring structure, but remains O(log N) with binary search.\n\n#### Key Assignment Algorithm\n\nRate limit keys are assigned to Redis nodes through a deterministic process that ensures any application instance can independently determine which node handles any specific key without central coordination. The algorithm follows these steps:\n\n1. **Hash the rate limit key**: Apply the same hash function (SHA-256) to the complete rate limit key string. For example, the key `user:12345:api:/login:1m` produces a 256-bit hash value.\n\n2. **Locate position on ring**: The hash value represents a position on the circular hash space. Conceptually, this is like dropping a pin at a specific location on the ring.\n\n3. **Find responsible virtual node**: Walk clockwise from the key's position until encountering the first virtual node. This virtual node's physical Redis node is responsible for storing this key's rate limit state.\n\n4. **Handle ring wrap-around**: If no virtual node exists between the key's position and the maximum hash value, wrap around to the beginning of the ring and continue searching from zero.\n\nThe beauty of this algorithm lies in its consistency—every application instance performing the same calculation will always arrive at the same Redis node for any given key, without requiring centralized coordination or shared state.\n\n| Hash Ring Operation | Time Complexity | Space Complexity | Consistency Guarantee |\n|-------------------|----------------|-----------------|---------------------|\n| Key lookup | O(log V) where V = total virtual nodes | O(V) for ring storage | Deterministic - all nodes agree |\n| Node addition | O(V/N) keys move where N = physical nodes | O(V) ring restructure | Affects only adjacent ranges |\n| Node removal | O(V/N) keys move | O(V) ring restructure | Graceful failover to next node |\n| Ring rebalancing | O(V log V) for sorting | O(V) temporary storage | Eventually consistent |\n\n#### Minimizing Redistribution During Topology Changes\n\nThe consistent hash ring's primary advantage becomes apparent during cluster topology changes. When a new Redis node joins the cluster, it only takes responsibility for keys that fall within specific ranges on the ring, rather than triggering a complete reshuffling of all keys.\n\n**Adding a new node** follows this process:\n\n1. **Generate virtual node positions**: The new physical node generates its virtual nodes at deterministic positions on the ring based on its identifier and virtual node indices.\n\n2. **Identify affected ranges**: For each virtual node of the new physical node, determine the range of keys that will move from the previously responsible node. This range spans from the new virtual node's position back to the previous virtual node in the clockwise direction.\n\n3. **Coordinate data migration**: The application instances coordinate with both the old and new Redis nodes to migrate rate limit state for keys within the affected ranges. This migration must maintain atomicity to prevent rate limit violations during the transition.\n\n4. **Update routing tables**: All application instances update their consistent hash ring structures to include the new node's virtual nodes, ensuring future requests route correctly.\n\n**Removing a node** (whether planned or due to failure) reverses this process:\n\n1. **Identify orphaned ranges**: Determine which key ranges were handled by the departing node's virtual nodes.\n\n2. **Reassign to next nodes**: For each orphaned range, the responsibility transfers to the next virtual node clockwise on the ring.\n\n3. **Migrate remaining state**: If the departure was planned, migrate any remaining rate limit state to the newly responsible nodes. If the departure was due to failure, the rate limit state is lost, but the system continues operating with temporary accuracy degradation.\n\n4. **Clean up routing tables**: Remove the departed node's virtual nodes from all application instances' hash ring structures.\n\nThe mathematical guarantee of consistent hashing is that adding or removing one node affects at most O(K/N) keys, where K is the total number of keys and N is the number of nodes. In practice, this means adding a fifth node to a four-node cluster will move approximately 20% of the keys, rather than the 100% that would move with traditional hash-based sharding.\n\n### Hot Key Detection and Rebalancing\n\nEven with perfect hash distribution, real-world traffic patterns create **hot keys**—rate limit keys that receive disproportionately high request volumes compared to the average. Hot keys can overwhelm individual Redis nodes while leaving others underutilized, creating performance bottlenecks that undermine the benefits of horizontal scaling.\n\n#### Hot Key Identification Mechanisms\n\nHot key detection operates through statistical analysis of request patterns across time windows, combined with cross-node comparison to identify keys that exceed normal distribution expectations. The detection system must balance accuracy against overhead, since monitoring every key individually would consume excessive resources.\n\n**Request frequency tracking** maintains sliding window counters for key access patterns. Each application instance tracks the frequency of rate limit checks for individual keys over rolling time windows (typically 1-minute, 5-minute, and 15-minute windows). Keys that consistently appear in the top percentiles across multiple time windows become candidates for hot key classification.\n\n**Cross-node load comparison** identifies keys that create uneven load distribution. The hot key detection system periodically samples key access frequencies across all Redis nodes and identifies keys where a single node handles a disproportionate share of the total cluster traffic. A key is classified as hot if it represents more than a configurable threshold (typically 2-5%) of any single node's request volume.\n\n**Adaptive thresholds** prevent false positives during normal traffic variations. The detection system calculates dynamic thresholds based on overall cluster load patterns, ensuring that keys are only classified as hot when they create genuine bottlenecks rather than normal peak traffic.\n\n| Detection Method | Accuracy | Overhead | Detection Latency | False Positive Rate |\n|-----------------|----------|----------|------------------|-------------------|\n| Request frequency only | Medium | Low | 1-5 minutes | Medium |\n| Cross-node comparison | High | Medium | 2-10 minutes | Low |\n| Combined approach | Very High | Medium-High | 1-5 minutes | Very Low |\n| Real-time statistical analysis | Excellent | High | 30 seconds | Very Low |\n\n> **Decision: Hot Key Detection Strategy**\n> - **Context**: Need to identify problematic keys without adding significant overhead to normal operations\n> - **Options Considered**: Request frequency tracking only, cross-node comparison only, combined approach, or real-time statistical analysis\n> - **Decision**: Use combined approach with request frequency tracking and periodic cross-node comparison\n> - **Rationale**: Provides high accuracy with reasonable overhead. Real-time analysis is too expensive for most workloads, while single-method approaches have too many false positives or negatives.\n> - **Consequences**: Achieves good hot key detection with manageable overhead. May miss very short-lived hot keys (under 1 minute duration), but these rarely cause sustained performance problems.\n\n#### Hot Key Replication Strategy\n\nOnce hot keys are identified, the system must distribute their load across multiple Redis nodes while maintaining consistency and avoiding race conditions. Hot key replication creates multiple copies of a rate limit counter across different nodes, requiring careful coordination to prevent double-counting or lost updates.\n\n**Read replica distribution** creates read-only copies of hot key state on multiple Redis nodes. When a hot key is detected, the system creates replicas on 2-3 additional nodes chosen to minimize impact on ring distribution. Read requests for rate limit previews can be served from any replica, distributing the query load. However, all write operations (actual rate limit checks that decrement counters) must still be directed to the primary node to maintain consistency.\n\n**Write load distribution** splits hot key write operations across multiple nodes using key suffixing. Instead of storing all state for hot key `user:popular_user:api:/login:1m` on a single node, the system creates multiple suffixed keys:\n- `user:popular_user:api:/login:1m:shard:0`\n- `user:popular_user:api:/login:1m:shard:1`  \n- `user:popular_user:api:/login:1m:shard:2`\n\nWrite operations are distributed across these sharded keys using consistent hashing on the client identifier or request timestamp. Rate limit checks aggregate state from all shards to determine the total usage.\n\n**Consistency maintenance** ensures that replicated or sharded hot keys maintain accurate rate limit enforcement. For read replicas, the primary node periodically synchronizes state to replicas (typically every 10-30 seconds). For sharded writes, rate limit checks must query all shards and aggregate results, adding latency but distributing load.\n\n#### Automatic Rebalancing Triggers\n\nThe system automatically initiates rebalancing operations when hot key detection identifies sustained performance problems or when cluster topology changes create uneven load distribution. Rebalancing must be coordinated carefully to avoid disrupting active rate limiting operations.\n\n**Load threshold monitoring** triggers rebalancing when key access patterns create sustained imbalance. If any Redis node consistently handles more than a configurable percentage (typically 40-50%) of total cluster requests for more than a sustained period (typically 10-15 minutes), the system initiates automatic rebalancing.\n\n**Cluster topology rebalancing** activates after node additions or removals complete their initial data migration. Even though consistent hashing minimizes key movement, the addition or removal of nodes can still create scenarios where hot keys concentrate on a subset of nodes. The rebalancing system identifies these patterns and creates additional replicas or shards as needed.\n\n**Time-based rebalancing windows** schedule major rebalancing operations during periods of lower traffic to minimize impact on active rate limiting. The system maintains historical traffic patterns and automatically schedules rebalancing during identified low-traffic windows, typically during off-peak hours.\n\n| Rebalancing Trigger | Threshold | Response Time | Impact on Performance |\n|--------------------|-----------|---------------|---------------------|\n| Sustained load imbalance | >50% requests on single node for >15min | 5-10 minutes | Low - gradual replica creation |\n| Hot key detection | >5% cluster requests for single key | 2-5 minutes | Medium - immediate sharding |\n| Post-topology change | After node add/remove completion | 1-2 hours | Low - background optimization |\n| Scheduled maintenance | Daily during off-peak hours | Immediate | Very Low - planned window |\n\n### Node Health and Failover\n\nRedis node failures represent the most critical operational challenge in distributed rate limiting, since failure of a single node can immediately impact rate limit accuracy for thousands of keys. The health monitoring and failover system must detect failures quickly, route traffic away from failed nodes, and maintain system availability while minimizing rate limit violations during transitions.\n\n#### Health Check Implementation\n\nHealth checking monitors multiple indicators of Redis node availability and performance to distinguish between temporary network glitches and genuine node failures. The health check system must balance rapid failure detection against false positive alerts that could trigger unnecessary failovers.\n\n**Connection-level health checks** verify basic network connectivity and Redis protocol responsiveness. Each application instance maintains persistent connections to all Redis nodes in the cluster and performs periodic ping operations (typically every 5-10 seconds). Failed ping operations increment failure counters, while successful operations reset counters and update last-seen timestamps.\n\n**Operation-level health checks** monitor the success rate and latency of actual rate limiting operations rather than just connection availability. These checks perform lightweight rate limit operations on synthetic keys and measure both success rates and response times. Nodes that consistently return errors or exceed latency thresholds (typically 50-100ms for rate limit checks) are marked as degraded even if basic connectivity remains functional.\n\n**Memory and resource monitoring** tracks Redis memory usage, CPU utilization, and key eviction rates to identify nodes approaching resource exhaustion. Redis nodes under memory pressure may start evicting rate limit keys or slowing response times significantly before completely failing. Early detection allows for graceful traffic redirection before performance degrades severely.\n\n| Health Check Type | Frequency | Failure Threshold | Detection Time | False Positive Rate |\n|------------------|-----------|------------------|----------------|-------------------|\n| Basic connectivity ping | Every 5 seconds | 3 consecutive failures | 15-20 seconds | Low |\n| Rate limit operation check | Every 30 seconds | 5 failures in 2 minutes | 1-3 minutes | Very Low |\n| Memory pressure monitoring | Every 60 seconds | >90% memory usage | 2-5 minutes | Medium |\n| Latency threshold monitoring | Continuous | >100ms for >1 minute | 1-2 minutes | Medium-High |\n\n#### Circuit Breaker Pattern Implementation\n\nThe circuit breaker pattern protects the rate limiting system from cascading failures by automatically stopping requests to failed Redis nodes and routing traffic to healthy alternatives. Circuit breakers prevent the system from repeatedly attempting operations against known-failed nodes, which would add latency and consume resources without providing value.\n\n**Circuit states** define the operational mode for each Redis node connection:\n\n- **Closed state**: Normal operation where all requests are sent to the Redis node. Connection failures increment failure counters, but the circuit remains closed until failure thresholds are exceeded.\n\n- **Open state**: All requests to the Redis node are immediately rejected without attempting connection. The circuit breaker routes traffic to alternative nodes or triggers local fallback behavior. The circuit remains open for a configurable timeout period (typically 30-60 seconds).\n\n- **Half-open state**: After the timeout period expires, the circuit allows a limited number of test requests to determine if the Redis node has recovered. If test requests succeed, the circuit closes and normal operation resumes. If test requests fail, the circuit returns to the open state for another timeout period.\n\n**Failure threshold configuration** determines when circuits open based on error rates and timing patterns. Typical configurations open circuits after 5 consecutive failures or 50% error rate over a 2-minute sliding window. The system must balance rapid failure detection against temporary network issues that resolve quickly.\n\n**Recovery validation** ensures that nodes are genuinely healthy before resuming full traffic. Half-open state test requests perform actual rate limiting operations rather than simple ping commands, verifying that the node can handle real workload before closing the circuit.\n\n| Circuit State | Request Handling | Failure Counting | State Transition |\n|--------------|-----------------|------------------|------------------|\n| Closed | Send all requests to node | Increment on failure, reset on success | Open after threshold exceeded |\n| Open | Reject immediately, route to alternatives | No requests sent | Half-open after timeout |\n| Half-open | Send limited test requests | Evaluate test request results | Closed on success, Open on failure |\n\n#### Automatic Failover Coordination\n\nWhen Redis nodes fail, the consistent hash ring must automatically redirect affected keys to healthy nodes while maintaining rate limit accuracy and avoiding split-brain scenarios where multiple nodes believe they are responsible for the same keys.\n\n**Immediate traffic redirection** routes new rate limit requests away from failed nodes using the next available node clockwise on the consistent hash ring. This redirection happens automatically since each application instance independently detects node failures and updates its local hash ring state. No central coordination is required for basic traffic redirection.\n\n**State migration coordination** attempts to preserve rate limit state from failed nodes when possible. If a node failure is detected early enough, the system may attempt to read current rate limit counter values and migrate them to the newly responsible nodes. However, this migration is best-effort only—the system prioritizes availability over perfect accuracy during failure scenarios.\n\n**Split-brain prevention** ensures that when failed nodes recover, they do not create conflicting rate limit state. Recovered nodes must synchronize with the current cluster state and determine which key ranges they are currently responsible for according to the updated hash ring. Any rate limit state for keys that were reassigned during the failure must be discarded or merged carefully to prevent double-counting.\n\n> **Decision: Failover Strategy Priority**\n> - **Context**: Must balance rate limit accuracy against system availability during node failures\n> - **Options Considered**: Perfect accuracy with downtime, immediate availability with temporary inaccuracy, or hybrid approach with best-effort state preservation\n> - **Decision**: Prioritize availability with best-effort state preservation\n> - **Rationale**: Rate limiting systems must remain operational during infrastructure failures. Temporary rate limit inaccuracy (allowing slightly more requests than configured limits) is preferable to complete service outage. Perfect accuracy is impossible to guarantee during arbitrary failure scenarios.\n> - **Consequences**: Enables high availability during Redis failures at the cost of temporary rate limit violations. Requires careful monitoring and alerting to detect and respond to accuracy degradation.\n\n#### Recovery and Reintegration Process\n\nWhen failed Redis nodes recover and rejoin the cluster, they must be reintegrated carefully to avoid disrupting ongoing operations or creating inconsistent state. The recovery process coordinates between the recovered node, currently active nodes, and all application instances to restore normal hash ring operation.\n\n**Health validation** verifies that recovered nodes are genuinely stable before accepting production traffic. Recovered nodes undergo extended health checking (typically 5-10 minutes of successful operations) before being marked as fully available. During this validation period, they may receive limited test traffic but are not included in production hash ring calculations.\n\n**State synchronization** attempts to restore accurate rate limit state for keys that will return to the recovered node's responsibility. The system identifies key ranges that belonged to the recovered node before its failure and attempts to retrieve current counter values from the nodes that handled them during the failure. This synchronization is best-effort and may not be possible for all keys.\n\n**Gradual traffic migration** slowly shifts rate limit operations back to recovered nodes rather than immediately resuming full traffic. This gradual approach prevents overwhelming recently recovered nodes and provides opportunity to detect any residual instability before full reintegration.\n\n**Consistency reconciliation** resolves any conflicts between rate limit state on recovered nodes and current state managed by other nodes during the failure. In most cases, the current state from active nodes takes precedence, but the system may need to perform merging operations for keys that continued to receive updates on both the recovered node and its failover replacement.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Uneven Virtual Node Distribution**\nMany implementations distribute virtual nodes unevenly across the hash ring, creating hot spots even with consistent hashing. This happens when virtual node generation uses poor hash functions or insufficient randomization. The symptom is persistent load imbalance despite having many virtual nodes. Fix this by using cryptographic hash functions (SHA-256) with proper salt values for virtual node generation, and validate distribution evenness during testing.\n\n⚠️ **Pitfall: Hot Key Detection False Positives**\nAggressive hot key detection can trigger unnecessary replication during normal traffic spikes, wasting resources and adding complexity. This occurs when detection thresholds are too low or time windows are too short. The symptom is frequent hot key alerts during peak hours for keys that don't actually create bottlenecks. Fix this by implementing adaptive thresholds based on overall cluster load and requiring sustained hot key patterns before triggering replication.\n\n⚠️ **Pitfall: Inconsistent Hash Ring State During Failures**\nApplication instances can maintain different views of hash ring topology during network partitions or rapid node failures, leading to rate limit checks being sent to wrong nodes. This manifests as sudden spikes in Redis errors or rate limit inaccuracy. Implement hash ring versioning and periodic synchronization to detect and resolve inconsistent states, with fallback to local rate limiting when uncertainty is detected.\n\n⚠️ **Pitfall: Circuit Breaker Oscillation**\nPoorly configured circuit breakers can oscillate rapidly between open and closed states when Redis nodes are intermittently failing, creating unstable performance. This happens when failure thresholds are too sensitive or recovery validation is insufficient. Symptoms include frequent circuit state changes and inconsistent response times. Fix this by implementing exponential backoff for circuit state changes and requiring sustained health before closing circuits.\n\n⚠️ **Pitfall: State Loss During Node Recovery**\nWhen failed nodes recover, their stale rate limit state can overwrite more recent state from failover nodes, causing incorrect rate limit calculations. This occurs when recovery processes don't properly synchronize state or determine key ownership. Implement proper state merging logic that prioritizes more recent timestamps and validates key ownership before accepting recovered state.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Hash Function | `crypto/sha256` with hex encoding | `github.com/spaolacci/murmur3` for performance |\n| Virtual Node Management | In-memory slice with binary search | `github.com/serialx/hashring` library |\n| Health Checking | Simple ping with `go-redis` | `github.com/sony/gobreaker` circuit breaker |\n| Node Discovery | Static configuration file | `github.com/hashicorp/consul` for service discovery |\n| Metrics Collection | Basic counters in memory | `github.com/prometheus/client_golang` |\n\n#### Recommended File Structure\n\n```\ninternal/\n  sharding/\n    ring.go                    ← consistent hash ring implementation\n    ring_test.go              ← ring distribution and rebalancing tests\n    virtual_nodes.go          ← virtual node management\n    hot_keys.go               ← hot key detection and replication\n    health_checker.go         ← Redis node health monitoring\n    circuit_breaker.go        ← circuit breaker pattern implementation\n  redis/\n    cluster_client.go         ← Redis cluster client wrapper\n    failover.go              ← automatic failover coordination\n    migration.go             ← key migration during rebalancing\n  config/\n    sharding_config.go        ← sharding configuration structures\n```\n\n#### Infrastructure Starter Code\n\n**Consistent Hash Ring Infrastructure** (complete implementation):\n\n```go\npackage sharding\n\nimport (\n    \"crypto/sha256\"\n    \"fmt\"\n    \"sort\"\n    \"sync\"\n)\n\n// HashRing represents a consistent hash ring with virtual nodes\ntype HashRing struct {\n    mu           sync.RWMutex\n    ring         []uint32\n    nodes        map[uint32]string\n    nodeWeights  map[string]int\n    virtualNodes int\n}\n\n// NewHashRing creates a new consistent hash ring\nfunc NewHashRing(virtualNodes int) *HashRing {\n    return &HashRing{\n        ring:         make([]uint32, 0),\n        nodes:        make(map[uint32]string),\n        nodeWeights:  make(map[string]int),\n        virtualNodes: virtualNodes,\n    }\n}\n\n// hashKey generates a hash for a key or virtual node identifier\nfunc (hr *HashRing) hashKey(key string) uint32 {\n    h := sha256.Sum256([]byte(key))\n    return uint32(h[0])<<24 | uint32(h[1])<<16 | uint32(h[2])<<8 | uint32(h[3])\n}\n\n// AddNode adds a physical node to the ring with virtual nodes\nfunc (hr *HashRing) AddNode(nodeID string, weight int) {\n    hr.mu.Lock()\n    defer hr.mu.Unlock()\n    \n    // Remove existing virtual nodes for this physical node\n    hr.removeNodeUnsafe(nodeID)\n    \n    // Add virtual nodes for this physical node\n    for i := 0; i < hr.virtualNodes*weight; i++ {\n        virtualKey := fmt.Sprintf(\"%s:vnode:%d\", nodeID, i)\n        hash := hr.hashKey(virtualKey)\n        hr.ring = append(hr.ring, hash)\n        hr.nodes[hash] = nodeID\n    }\n    \n    hr.nodeWeights[nodeID] = weight\n    sort.Slice(hr.ring, func(i, j int) bool {\n        return hr.ring[i] < hr.ring[j]\n    })\n}\n\n// RemoveNode removes a physical node and all its virtual nodes\nfunc (hr *HashRing) RemoveNode(nodeID string) {\n    hr.mu.Lock()\n    defer hr.mu.Unlock()\n    hr.removeNodeUnsafe(nodeID)\n}\n\n// removeNodeUnsafe removes a node without locking (internal helper)\nfunc (hr *HashRing) removeNodeUnsafe(nodeID string) {\n    newRing := make([]uint32, 0, len(hr.ring))\n    for _, hash := range hr.ring {\n        if hr.nodes[hash] != nodeID {\n            newRing = append(newRing, hash)\n        } else {\n            delete(hr.nodes, hash)\n        }\n    }\n    hr.ring = newRing\n    delete(hr.nodeWeights, nodeID)\n}\n\n// GetNode returns the responsible node for a given key\nfunc (hr *HashRing) GetNode(key string) (string, bool) {\n    hr.mu.RLock()\n    defer hr.mu.RUnlock()\n    \n    if len(hr.ring) == 0 {\n        return \"\", false\n    }\n    \n    hash := hr.hashKey(key)\n    idx := sort.Search(len(hr.ring), func(i int) bool {\n        return hr.ring[i] >= hash\n    })\n    \n    if idx == len(hr.ring) {\n        idx = 0  // wrap around to beginning\n    }\n    \n    return hr.nodes[hr.ring[idx]], true\n}\n\n// GetNodes returns N responsible nodes for a key (for replication)\nfunc (hr *HashRing) GetNodes(key string, count int) []string {\n    hr.mu.RLock()\n    defer hr.mu.RUnlock()\n    \n    if len(hr.ring) == 0 || count <= 0 {\n        return nil\n    }\n    \n    hash := hr.hashKey(key)\n    idx := sort.Search(len(hr.ring), func(i int) bool {\n        return hr.ring[i] >= hash\n    })\n    \n    if idx == len(hr.ring) {\n        idx = 0\n    }\n    \n    result := make([]string, 0, count)\n    seen := make(map[string]bool)\n    \n    for len(result) < count && len(seen) < len(hr.nodeWeights) {\n        nodeID := hr.nodes[hr.ring[idx]]\n        if !seen[nodeID] {\n            result = append(result, nodeID)\n            seen[nodeID] = true\n        }\n        idx = (idx + 1) % len(hr.ring)\n    }\n    \n    return result\n}\n```\n\n**Circuit Breaker Implementation** (complete implementation):\n\n```go\npackage sharding\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\ntype CircuitState int\n\nconst (\n    CircuitClosed CircuitState = iota\n    CircuitOpen\n    CircuitHalfOpen\n)\n\n// CircuitBreaker implements the circuit breaker pattern for Redis nodes\ntype CircuitBreaker struct {\n    mu                sync.RWMutex\n    state            CircuitState\n    failureCount     int\n    lastFailureTime  time.Time\n    nextRetryTime    time.Time\n    \n    // Configuration\n    failureThreshold int\n    recoveryTimeout  time.Duration\n    halfOpenMaxCalls int\n    halfOpenCalls    int\n    halfOpenSuccesses int\n}\n\n// NewCircuitBreaker creates a new circuit breaker\nfunc NewCircuitBreaker(failureThreshold int, recoveryTimeout time.Duration) *CircuitBreaker {\n    return &CircuitBreaker{\n        state:            CircuitClosed,\n        failureThreshold: failureThreshold,\n        recoveryTimeout:  recoveryTimeout,\n        halfOpenMaxCalls: 3,\n    }\n}\n\n// Execute runs a function with circuit breaker protection\nfunc (cb *CircuitBreaker) Execute(ctx context.Context, fn func() error) error {\n    if !cb.allowRequest() {\n        return fmt.Errorf(\"circuit breaker is open\")\n    }\n    \n    err := fn()\n    cb.recordResult(err == nil)\n    return err\n}\n\n// allowRequest determines if a request should be allowed\nfunc (cb *CircuitBreaker) allowRequest() bool {\n    cb.mu.Lock()\n    defer cb.mu.Unlock()\n    \n    now := time.Now()\n    \n    switch cb.state {\n    case CircuitClosed:\n        return true\n    case CircuitOpen:\n        if now.After(cb.nextRetryTime) {\n            cb.state = CircuitHalfOpen\n            cb.halfOpenCalls = 0\n            cb.halfOpenSuccesses = 0\n            return true\n        }\n        return false\n    case CircuitHalfOpen:\n        return cb.halfOpenCalls < cb.halfOpenMaxCalls\n    default:\n        return false\n    }\n}\n\n// recordResult records the result of a request\nfunc (cb *CircuitBreaker) recordResult(success bool) {\n    cb.mu.Lock()\n    defer cb.mu.Unlock()\n    \n    switch cb.state {\n    case CircuitClosed:\n        if success {\n            cb.failureCount = 0\n        } else {\n            cb.failureCount++\n            cb.lastFailureTime = time.Now()\n            if cb.failureCount >= cb.failureThreshold {\n                cb.state = CircuitOpen\n                cb.nextRetryTime = time.Now().Add(cb.recoveryTimeout)\n            }\n        }\n    case CircuitHalfOpen:\n        cb.halfOpenCalls++\n        if success {\n            cb.halfOpenSuccesses++\n        }\n        \n        if cb.halfOpenCalls >= cb.halfOpenMaxCalls {\n            if cb.halfOpenSuccesses == cb.halfOpenMaxCalls {\n                cb.state = CircuitClosed\n                cb.failureCount = 0\n            } else {\n                cb.state = CircuitOpen\n                cb.nextRetryTime = time.Now().Add(cb.recoveryTimeout)\n            }\n        }\n    }\n}\n\n// GetState returns current circuit state (for monitoring)\nfunc (cb *CircuitBreaker) GetState() CircuitState {\n    cb.mu.RLock()\n    defer cb.mu.RUnlock()\n    return cb.state\n}\n```\n\n#### Core Logic Skeleton Code\n\n**Hot Key Detection Implementation**:\n\n```go\n// HotKeyDetector monitors request patterns and identifies hot keys\ntype HotKeyDetector struct {\n    mu              sync.RWMutex\n    keyStats        map[string]*KeyStats\n    clusterStats    *ClusterStats\n    config          HotKeyConfig\n    replicationChan chan string\n}\n\n// KeyStats tracks request frequency for individual keys\ntype KeyStats struct {\n    RequestCount1Min  int64\n    RequestCount5Min  int64\n    RequestCount15Min int64\n    LastUpdate        time.Time\n    WindowBuckets     []int64  // Sliding window buckets\n}\n\n// DetectHotKeys analyzes current key statistics and identifies hot keys\nfunc (hkd *HotKeyDetector) DetectHotKeys() []string {\n    // TODO 1: Get current cluster-wide request statistics to establish baseline\n    // TODO 2: Calculate dynamic threshold based on total cluster requests per minute\n    // TODO 3: Iterate through all tracked keys and check their request rates\n    // TODO 4: Compare each key's rate against both absolute and relative thresholds\n    // TODO 5: For keys exceeding thresholds, verify they exceed minimum duration requirement\n    // TODO 6: Return list of confirmed hot keys for replication\n    // Hint: Hot key threshold = max(absolute_minimum, cluster_rps * hot_key_percentage)\n    return nil\n}\n\n// RecordKeyAccess increments access counters for a specific rate limit key\nfunc (hkd *HotKeyDetector) RecordKeyAccess(key string) {\n    // TODO 1: Get or create KeyStats struct for this key in thread-safe manner\n    // TODO 2: Update current minute bucket in sliding window\n    // TODO 3: Increment counters for 1min, 5min, and 15min windows\n    // TODO 4: Update LastUpdate timestamp\n    // TODO 5: If key stats indicate potential hot key, trigger async detection check\n    // Hint: Use atomic operations for counter updates to avoid lock contention\n}\n\n// ReplicateHotKey creates replicas of hot key across multiple nodes\nfunc (hkd *HotKeyDetector) ReplicateHotKey(key string, replicationFactor int) error {\n    // TODO 1: Determine current responsible node for this key using hash ring\n    // TODO 2: Select additional nodes for replication using GetNodes(key, replicationFactor)\n    // TODO 3: Read current rate limit state from primary node\n    // TODO 4: Create replica entries on selected nodes with read-only markers\n    // TODO 5: Update local routing table to include replica locations for read queries\n    // TODO 6: Set up periodic synchronization from primary to replicas\n    // Hint: Replicas should handle reads only; all writes go to primary node\n    return nil\n}\n```\n\n**Node Health Checker Implementation**:\n\n```go\n// HealthChecker monitors Redis node health and manages circuit breakers\ntype HealthChecker struct {\n    mu              sync.RWMutex\n    nodes           map[string]*NodeHealth\n    circuitBreakers map[string]*CircuitBreaker\n    config          HealthConfig\n}\n\n// NodeHealth tracks health status for individual Redis nodes\ntype NodeHealth struct {\n    NodeID           string\n    Address          string\n    LastSeen         time.Time\n    ConsecutiveFailures int\n    AverageLatency   time.Duration\n    IsHealthy        bool\n    MemoryUsage      float64\n    ConnectionCount  int\n}\n\n// CheckNodeHealth performs comprehensive health check on a Redis node\nfunc (hc *HealthChecker) CheckNodeHealth(ctx context.Context, nodeID string) (*NodeHealth, error) {\n    // TODO 1: Perform basic connectivity ping to Redis node\n    // TODO 2: Execute lightweight rate limit operation with synthetic key\n    // TODO 3: Measure response latency and compare against thresholds\n    // TODO 4: Query Redis INFO command for memory usage and connection stats\n    // TODO 5: Update NodeHealth struct with current status and timestamps\n    // TODO 6: Determine overall health status based on all checks\n    // TODO 7: Update circuit breaker state based on health check results\n    // Hint: Use context.WithTimeout to prevent health checks from hanging\n    return nil, nil\n}\n\n// StartHealthMonitoring begins periodic health checking for all nodes\nfunc (hc *HealthChecker) StartHealthMonitoring(ctx context.Context) {\n    // TODO 1: Create ticker for periodic health checks (every 30 seconds)\n    // TODO 2: For each registered node, perform health check in separate goroutine\n    // TODO 3: Collect health check results and update node status\n    // TODO 4: Trigger failover notifications for newly failed nodes\n    // TODO 5: Update hash ring to exclude failed nodes from routing\n    // TODO 6: Handle context cancellation for graceful shutdown\n    // Hint: Use errgroup to manage multiple concurrent health checks\n}\n\n// HandleNodeFailure coordinates response to detected node failure\nfunc (hc *HealthChecker) HandleNodeFailure(nodeID string) error {\n    // TODO 1: Mark node as failed in health tracking structures\n    // TODO 2: Open circuit breaker for this node to prevent further requests\n    // TODO 3: Remove node from consistent hash ring to stop routing new requests\n    // TODO 4: Identify keys that were handled by failed node\n    // TODO 5: Notify key migration system to redirect affected keys\n    // TODO 6: Send alerts/notifications about node failure to monitoring systems\n    // Hint: Failure handling should be idempotent in case of multiple detection events\n    return nil\n}\n```\n\n#### Milestone Verification Checkpoints\n\n**After implementing consistent hash ring**:\n- Run `go test ./internal/sharding/ring_test.go -v` to verify hash distribution\n- Test with 4 nodes and 100,000 keys - each node should handle 20-30% of keys\n- Add a 5th node - verify only ~20% of keys move to the new node\n- Remove a node - verify keys redistribute to adjacent nodes only\n\n**After implementing hot key detection**:\n- Create test scenario with 1000 RPS to key \"user:popular\" and 10 RPS to other keys\n- Monitor detection system - should identify \"user:popular\" as hot within 2 minutes\n- Verify hot key replication creates read replicas on 2-3 additional nodes\n- Test that read queries distribute across replicas while writes go to primary\n\n**After implementing health checking and failover**:\n- Start 3 Redis nodes and verify all show as healthy\n- Stop one Redis node - should detect failure within 30-60 seconds\n- Verify circuit breaker opens and traffic routes to remaining healthy nodes\n- Restart failed node - should detect recovery and resume routing within 2-3 minutes\n\n**Signs something is wrong**:\n- Hash ring distribution variance >20% indicates poor virtual node generation\n- Hot key detection triggering for normal keys indicates thresholds too low\n- Health checks showing false failures indicates network timeouts too aggressive\n- Keys routing to wrong nodes during failures indicates hash ring inconsistency\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** Milestone 1, 2, 3, 4, 5 - this section demonstrates how all components work together across rate limiting algorithms, multi-tier evaluation, Redis integration, sharding, and API management\n\nUnderstanding the interactions between components in a distributed rate limiting system is crucial for building a robust and performant implementation. This section explores the three primary data flows that define how the system operates: rate limit checking, configuration management, and metrics collection.\n\n### Mental Model: The Air Traffic Control System\n\nThink of the distributed rate limiter as an air traffic control system managing aircraft (requests) across multiple airports (application instances) with a central coordination center (Redis cluster). When an aircraft requests landing clearance, the local tower (application instance) must coordinate with the central system to check runway capacity (rate limits), update the flight schedule (counters), and ensure safe separation (prevent overload). Configuration updates flow like weather advisories - broadcast from central meteorology (management API) to all towers simultaneously. Meanwhile, flight statistics (metrics) continuously stream back to central command for monitoring and decision-making.\n\nThis analogy helps illustrate the critical coordination patterns: real-time decision making with shared state, configuration propagation across distributed nodes, and continuous monitoring of system health. The key insight is that each local decision requires global coordination, but the system must remain fast enough for real-time operation.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n### Rate Limit Check Flow\n\nThe rate limit check flow represents the most performance-critical path in the distributed rate limiting system. Every incoming request must be evaluated against potentially multiple rate limit rules, with the decision made in milliseconds while maintaining consistency across all application instances.\n\n#### Request Context Assembly\n\nThe rate limit check begins when an HTTP request arrives at any application instance. The `DistributedLimiter` component extracts relevant context information to construct a `RateLimitRequest` structure. This context assembly process is crucial because it determines which rate limit rules will be evaluated and how the request will be categorized.\n\n| Context Field | Source | Purpose | Example Value |\n|---------------|--------|---------|---------------|\n| `user_id` | Authentication header/JWT | Per-user rate limiting | \"user_12345\" |\n| `ip_address` | Request remote address | Per-IP rate limiting | \"192.168.1.100\" |\n| `api_endpoint` | Request path pattern | Per-API rate limiting | \"/api/v1/users\" |\n| `user_agent` | HTTP header | Bot detection/classification | \"Mozilla/5.0...\" |\n| `tokens` | Request payload size/type | Resource-aware limiting | 1 or payload_size |\n\nThe context assembly must handle edge cases like missing authentication (anonymous users), proxy forwarding (X-Forwarded-For headers), and API path normalization (removing parameters). The `KeyComposer` component uses this context to generate Redis keys that will be used for rate limit state storage.\n\n#### Multi-Tier Rule Evaluation\n\nOnce the request context is assembled, the `MultiTierLimiter` begins evaluating applicable rate limit rules. The system follows a short-circuit evaluation strategy, checking rules in priority order and stopping immediately when any limit is exceeded.\n\n![Multi-Tier Rate Limit Evaluation](./diagrams/multi-tier-evaluation.svg)\n\nThe tier evaluation follows this algorithmic sequence:\n\n1. **Rule Discovery**: The `RuleManager` scans all configured rules and identifies those whose `key_pattern` matches the request context. Rules are sorted by priority (highest first) to ensure most specific limits are checked before general ones.\n\n2. **Key Composition**: For each matching rule, the `KeyComposer` generates a Redis key using the rule's pattern and request context. This involves pattern substitution and namespace prefixing to ensure key uniqueness across different rate limit dimensions.\n\n3. **Tier Evaluation Loop**: The system iterates through matching rules, performing rate limit checks for each tier:\n   - User-specific limits (highest priority)\n   - IP-address limits (medium priority)  \n   - API endpoint limits (medium priority)\n   - Global system limits (lowest priority)\n\n4. **Short-Circuit Logic**: If any rule evaluation returns `allowed: false`, the entire check immediately fails without evaluating remaining rules. This optimization reduces Redis operations and provides faster response times for requests that exceed limits.\n\n5. **Result Aggregation**: If all rules pass, the system returns the most restrictive result (lowest remaining count) to provide accurate rate limit headers in the response.\n\n| Evaluation Step | Action | Success Path | Failure Path |\n|-----------------|--------|--------------|--------------|\n| Rule Discovery | Find matching rules by pattern | Continue to next step | Return allowed=true (no rules) |\n| Key Composition | Generate Redis keys | Continue to tier evaluation | Return error |\n| Per-User Check | Check user-specific limits | Continue to next tier | Return allowed=false |\n| Per-IP Check | Check IP-based limits | Continue to next tier | Return allowed=false |\n| Per-API Check | Check endpoint limits | Continue to next tier | Return allowed=false |\n| Global Check | Check system-wide limits | Return allowed=true | Return allowed=false |\n\n#### Redis Atomic Operations\n\nThe core of each individual rate limit check is an atomic Redis operation implemented as a Lua script. This atomicity is essential for maintaining accurate counters in a distributed environment where multiple application instances may be checking the same rate limit simultaneously.\n\n![Rate Limit Check Sequence](./diagrams/rate-check-sequence.svg)\n\nEach algorithm uses a specialized Lua script optimized for its specific requirements:\n\n**Token Bucket Script Flow:**\n1. Load current bucket state (`tokens`, `last_refill_time`) from Redis\n2. Calculate elapsed time since last refill and compute tokens to add\n3. Refill bucket up to capacity based on configured refill rate\n4. Check if requested tokens are available\n5. If available, subtract tokens and update state; if not, return current state\n6. Return result with remaining tokens and reset time\n\n**Sliding Window Counter Script Flow:**\n1. Calculate current time window and previous window boundaries\n2. Load counters for current and previous time windows\n3. Calculate weighted count based on time position within current window\n4. Check if adding this request would exceed the limit\n5. If within limit, increment current window counter\n6. Clean up expired window data to prevent memory leaks\n7. Return result with remaining capacity\n\n**Sliding Window Log Script Flow:**\n1. Remove expired timestamps from the request log\n2. Count remaining timestamps to determine current usage\n3. Check if adding this request would exceed the limit\n4. If within limit, add current timestamp to the log\n5. Return result with current usage and remaining capacity\n\n| Script Operation | Atomicity Requirement | Performance Impact | Memory Usage |\n|-------------------|----------------------|-------------------|---------------|\n| Token Bucket | State read + refill + check + update | Low (simple arithmetic) | Fixed per key |\n| Sliding Counter | Multi-window read + weighted calc + update | Medium (window math) | Fixed per window |\n| Sliding Log | Log read + filter + count + append | High (list operations) | Grows with traffic |\n\n#### Local Fallback Handling\n\nWhen Redis becomes unavailable, the system must gracefully degrade to local rate limiting rather than failing open (allowing all requests) or closed (rejecting all requests). This graceful degradation maintains partial functionality while preserving system stability.\n\nThe fallback detection and activation follows this sequence:\n\n1. **Failure Detection**: Redis operations fail with connection timeout, network error, or other exceptions. The `CircuitBreaker` component tracks failure rates and automatically opens when thresholds are exceeded.\n\n2. **Fallback Activation**: The `DistributedLimiter` switches to its `localFallback` limiter instance, which maintains in-memory rate limit state using the same algorithms but with local scope.\n\n3. **State Synchronization**: When Redis connectivity is restored, the system must carefully synchronize local and distributed state to prevent double-counting or lost counts.\n\n4. **Recovery Process**: The circuit breaker gradually allows test requests through to verify Redis health before fully reopening.\n\n> **Critical Design Insight**: Local fallback cannot maintain global accuracy, but it preserves system availability and prevents cascading failures. The trade-off between accuracy and availability is an explicit architectural choice.\n\n#### Response Header Generation\n\nThe final step in the rate limit check flow is generating standard HTTP response headers that inform clients about their current rate limit status. These headers follow established conventions and enable clients to implement intelligent backoff strategies.\n\n| Header Name | Value Format | Purpose | Example |\n|-------------|--------------|---------|---------|\n| `X-RateLimit-Limit` | Integer | Maximum requests in window | \"1000\" |\n| `X-RateLimit-Remaining` | Integer | Requests left in current window | \"247\" |\n| `X-RateLimit-Reset` | Unix timestamp | When window resets | \"1699123456\" |\n| `Retry-After` | Seconds | How long to wait if limited | \"60\" |\n\n### Configuration Update Propagation\n\nConfiguration management in a distributed rate limiting system presents unique challenges: changes must be propagated to all application instances quickly and consistently, while the system continues processing requests without interruption.\n\n#### Mental Model: Emergency Broadcast System\n\nThink of configuration updates like an emergency broadcast system. When new rate limit rules are created or modified (emergency announcement), they must reach every application instance (radio station) reliably and quickly. Each instance must validate the message authenticity (configuration schema), update its local understanding (rule cache), and begin operating under the new rules immediately. Failed deliveries are retried, and the system tracks which instances have received updates to ensure full coverage.\n\n![Configuration Update Flow](./diagrams/configuration-propagation-flow.svg)\n\n#### Configuration Source and Validation\n\nThe configuration update process begins at the management API, which serves as the authoritative source for all rate limit rules. When administrators create, modify, or delete rate limit rules, the API performs comprehensive validation before persisting changes.\n\n| Validation Check | Purpose | Failure Action | Example |\n|------------------|---------|----------------|---------|\n| Schema Validation | Ensure required fields present | Reject with 400 error | Missing `limit` field |\n| Pattern Syntax | Validate regex patterns | Reject with 400 error | Invalid regex: `[unclosed` |\n| Limit Bounds | Check reasonable limit values | Reject with 400 error | Negative rate limit |\n| Priority Conflicts | Prevent overlapping priorities | Reject with 409 error | Two rules same priority |\n| Algorithm Support | Verify algorithm exists | Reject with 400 error | Unknown algorithm name |\n\nThe validation process uses a staged approach where syntactic validation occurs first (fast checks), followed by semantic validation (cross-rule consistency), and finally persistence validation (storage constraints). This ordering minimizes wasted work when validation fails.\n\n#### Redis as Configuration Distribution Hub\n\nThe system uses Redis not only for rate limit state storage but also as a configuration distribution mechanism. This design choice leverages Redis's existing high availability and clustering capabilities while providing real-time updates to all application instances.\n\nThe configuration distribution follows a publish-subscribe pattern combined with persistent storage:\n\n1. **Persistent Storage**: Rate limit rules are stored in Redis using hash structures, allowing atomic updates and version tracking.\n\n2. **Change Notification**: When rules are updated, the management API publishes a notification to a Redis channel announcing the change.\n\n3. **Instance Subscription**: All application instances subscribe to the configuration change channel and receive real-time notifications.\n\n4. **Incremental Updates**: Instances fetch only changed rules rather than reloading entire configurations, reducing network overhead and update latency.\n\n| Redis Structure | Key Pattern | Purpose | Data Format |\n|-----------------|-------------|---------|-------------|\n| Rule Storage | `config:rules:{rule_id}` | Persistent rule data | Hash with all rule fields |\n| Version Tracking | `config:version` | Change detection | Integer timestamp |\n| Change Log | `config:changelog:{version}` | Audit trail | JSON change description |\n| Heartbeat | `config:heartbeat:{instance_id}` | Instance health | Timestamp + rule version |\n\n#### Application Instance Update Process\n\nEach application instance runs a configuration watcher component that maintains consistency between the central configuration store and local rule caches. This component must handle network failures, partial updates, and version conflicts gracefully.\n\nThe update process for each application instance follows this sequence:\n\n1. **Change Detection**: The instance receives a notification via Redis pub/sub or detects a version mismatch during periodic health checks.\n\n2. **Version Comparison**: The instance compares its local rule version against the central version to determine what updates are needed.\n\n3. **Incremental Fetch**: Only changed rules are fetched from Redis, using timestamps or version numbers to identify modifications.\n\n4. **Atomic Local Update**: The instance updates its in-memory rule cache atomically to prevent inconsistent state during rule evaluation.\n\n5. **Validation and Rollback**: After updating, the instance validates that all rules are correctly loaded and rolls back to the previous version if problems are detected.\n\n6. **Heartbeat Update**: The instance updates its heartbeat record in Redis to confirm successful configuration application.\n\n> **Decision: Incremental vs Full Configuration Reload**\n> - **Context**: Configuration changes need to propagate to potentially hundreds of application instances\n> - **Options Considered**: \n>   1. Full configuration reload on every change\n>   2. Incremental updates with change tracking\n>   3. Configuration push from central server\n> - **Decision**: Incremental updates with Redis pub/sub notification\n> - **Rationale**: Minimizes network bandwidth, reduces update latency, and scales better with cluster size. Redis pub/sub provides reliable delivery with automatic retries.\n> - **Consequences**: More complex change tracking logic, but significantly better performance and scalability.\n\n#### Configuration Validation and Conflict Resolution\n\nDistributed configuration management introduces the possibility of conflicts and inconsistencies that must be detected and resolved automatically. The system implements several mechanisms to ensure configuration consistency across all instances.\n\n| Conflict Type | Detection Method | Resolution Strategy | Prevention |\n|---------------|------------------|-------------------|------------|\n| Version Skew | Periodic heartbeat comparison | Force full reload | Timeout limits on updates |\n| Rule Priority Overlap | Cross-rule validation | Reject conflicting update | Priority uniqueness check |\n| Pattern Ambiguity | Pattern matching test | Admin notification | Test pattern coverage |\n| Resource Exhaustion | Memory/performance monitoring | Gradual rollout | Capacity planning |\n\nThe conflict resolution system operates on the principle of \"fail safely\" - when conflicts cannot be automatically resolved, the system maintains the last known good configuration while alerting administrators to the issue.\n\n#### Configuration Rollback and Recovery\n\nThe distributed nature of the system requires sophisticated rollback capabilities when configuration changes cause problems. The system maintains configuration history and provides mechanisms for rapid rollback across all instances.\n\nThe rollback process works as follows:\n\n1. **Issue Detection**: Monitoring systems detect increased error rates, performance degradation, or other problems following a configuration change.\n\n2. **Rollback Trigger**: Administrators or automated systems trigger a rollback to a previous configuration version.\n\n3. **Coordinated Rollback**: The management API publishes a rollback notification containing the target version number.\n\n4. **Instance Rollback**: Each application instance loads the specified historical configuration from its local cache or Redis backup.\n\n5. **Verification**: Instances report successful rollback via heartbeat updates, allowing centralized verification of rollback completion.\n\n### Metrics Collection and Aggregation\n\nComprehensive metrics collection is essential for operating a distributed rate limiting system effectively. The metrics system must capture detailed usage patterns, performance characteristics, and system health indicators while minimizing impact on the critical rate limiting path.\n\n#### Mental Model: Hospital Vital Signs Monitoring\n\nThink of the metrics collection system like vital signs monitoring in a hospital. Each patient (rate limit key) has continuous monitoring of key indicators (request rates, rejection rates, latency). Nurses (application instances) collect readings regularly and report to a central monitoring station (metrics aggregator). Doctors (operators) use dashboards to spot trends, diagnose problems, and make treatment decisions. Critical alerts (threshold breaches) trigger immediate notifications, while long-term trends inform capacity planning and system optimization.\n\n#### Real-Time Metrics Collection\n\nThe metrics collection system operates continuously alongside the rate limiting functionality, capturing both operational metrics (performance, errors) and business metrics (usage patterns, limit effectiveness). The collection must be designed for minimal performance impact while providing sufficient detail for troubleshooting and optimization.\n\n| Metric Category | Specific Metrics | Collection Method | Granularity |\n|-----------------|------------------|-------------------|-------------|\n| Request Metrics | `requests_total`, `requests_allowed`, `requests_denied` | Counter increment on each check | Per rule, per instance |\n| Performance Metrics | `check_duration_ms`, `redis_latency_ms` | Histogram recording | Per operation type |\n| System Health | `redis_connection_status`, `fallback_active` | Gauge sampling | Per instance |\n| Usage Patterns | `top_keys_by_volume`, `limit_utilization_pct` | Periodic aggregation | Per time window |\n\nEach application instance maintains local metric collectors that aggregate data over short time windows (typically 10-60 seconds) before transmitting to the central metrics system. This approach reduces network overhead while providing near-real-time visibility into system behavior.\n\n#### Hot Key Detection and Analysis\n\nHot key detection represents one of the most valuable capabilities of the metrics system. By identifying rate limit keys that receive disproportionate traffic, the system can automatically trigger replication, sharding adjustments, or capacity alerts.\n\nThe `HotKeyDetector` component continuously analyzes request patterns using a multi-window approach:\n\n1. **Request Counting**: Each application instance maintains counters for every rate limit key it processes, tracking request volumes over multiple time windows (1 minute, 5 minutes, 15 minutes).\n\n2. **Statistical Analysis**: The system calculates request rate percentiles across all keys to identify outliers that exceed configurable thresholds (e.g., 95th percentile).\n\n3. **Trend Detection**: Hot key detection considers not just absolute volumes but also growth rates to identify keys experiencing traffic spikes.\n\n4. **Cluster-Wide Aggregation**: Individual instance measurements are aggregated across the cluster to provide accurate global hot key identification.\n\n| Detection Window | Purpose | Threshold Type | Action Triggered |\n|------------------|---------|----------------|------------------|\n| 1 Minute | Spike detection | Absolute count > P99 | Immediate replication |\n| 5 Minutes | Sustained load | Rate increase > 300% | Load balancing adjustment |\n| 15 Minutes | Trend analysis | Steady growth > 50% | Capacity planning alert |\n| 1 Hour | Baseline establishment | Rolling average update | Threshold recalibration |\n\n#### Dashboard and Visualization\n\nThe real-time dashboard provides operators with comprehensive visibility into the rate limiting system's behavior. The dashboard must balance information density with usability, presenting critical information prominently while making detailed data easily accessible.\n\nThe dashboard architecture uses WebSocket connections for real-time updates, with efficient data streaming that minimizes bandwidth usage. Key visualizations include:\n\n**System Overview Panel:**\n- Total request rate across all instances\n- Overall allow/deny ratio\n- Active rate limit rules count\n- Redis cluster health status\n\n**Top Keys Analysis:**\n- Highest volume rate limit keys (by request count)\n- Most restrictive keys (by denial rate)\n- Hot keys detected in recent time windows\n- Key growth trends over time\n\n**Performance Monitoring:**\n- Rate limit check latency distribution\n- Redis operation latency percentiles\n- Circuit breaker status across instances\n- Error rate trends by error type\n\n**Tier Analysis:**\n- Effectiveness of different rate limit tiers\n- Most frequently triggered rules\n- Resource utilization by limit type\n- Optimization recommendations\n\n#### Alerting and Anomaly Detection\n\nThe metrics system includes sophisticated alerting capabilities that can detect both immediate problems and developing issues before they impact users. The alerting system operates on multiple time horizons with different sensitivity levels.\n\n| Alert Type | Detection Logic | Severity | Response |\n|------------|-----------------|----------|----------|\n| High Error Rate | Error rate > 5% over 2 minutes | Critical | Immediate page |\n| Redis Unavailable | Connection failures > 50% | Critical | Immediate page |\n| Hot Key Spike | Single key > 10x normal rate | Warning | Auto-mitigation + alert |\n| Capacity Trend | Usage growth > 80% capacity | Info | Capacity planning notification |\n| Config Drift | Instance config version skew | Warning | Auto-remediation attempt |\n\nThe anomaly detection system uses statistical models to identify unusual patterns that might indicate attacks, misconfigurations, or system problems. These models adapt over time to account for normal traffic pattern evolution while maintaining sensitivity to genuine anomalies.\n\n#### Self-Monitoring Rate Limits\n\nA critical consideration for the metrics and management API is preventing the rate limiting system from overwhelming itself with monitoring traffic. The system implements self-monitoring rate limits to ensure that dashboard queries, metrics collection, and administrative operations don't interfere with primary rate limiting functionality.\n\n> **Critical Design Insight**: The rate limiting system must protect itself from its own monitoring and management interfaces. This requires careful design of internal rate limits and circuit breakers.\n\nThe self-monitoring system includes:\n\n**Dashboard Rate Limiting:** WebSocket connections are rate limited to prevent a single dashboard user from overwhelming the metrics system with excessive query rates.\n\n**API Protection:** Management API endpoints have their own rate limits to prevent configuration update storms or abusive administrative behavior.\n\n**Metrics Backpressure:** When the metrics collection system becomes overwhelmed, it implements backpressure mechanisms to reduce collection frequency rather than dropping data.\n\n**Internal Circuit Breakers:** Monitoring components include circuit breakers that disable non-essential metrics collection when the primary rate limiting system is under stress.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Race Conditions in Multi-Tier Evaluation**\nMany implementations fail to properly coordinate between tier evaluations, leading to inconsistent state where one tier's counter is updated while another tier's update fails. This can result in \"phantom\" request counting where requests are partially counted across different tiers. The fix is to use two-phase operations: first check all tiers without updating, then update all tiers atomically, or implement compensating transactions for partial failures.\n\n⚠️ **Pitfall: Configuration Update Ordering**\nConfiguration updates can arrive at different instances in different orders due to network timing, causing temporary inconsistencies where the same request is evaluated differently by different instances. This is particularly dangerous when rule priorities change. The fix is to use version numbers and ensure all instances process configuration changes in the same order, potentially by using a consensus mechanism or designated configuration coordinator.\n\n⚠️ **Pitfall: Metrics Collection Overwhelming Redis**\nNaive metrics implementations can generate more Redis traffic than the actual rate limiting operations, especially with detailed per-key statistics. This can overwhelm Redis and degrade rate limiting performance. The solution is to batch metrics operations, use separate Redis instances for metrics vs rate limiting, and implement metrics sampling for high-volume keys.\n\n⚠️ **Pitfall: Synchronous Configuration Updates Blocking Requests**\nBlocking the request processing thread while updating configuration can cause significant latency spikes during configuration changes. The fix is to use asynchronous configuration updates with atomic cache swapping - load new configuration in the background and atomically replace the active configuration once fully validated.\n\n⚠️ **Pitfall: Hardcoded Timeout Values**\nUsing fixed timeout values for Redis operations doesn't account for varying network conditions or load. This can cause premature fallback activation or excessive blocking during temporary slowdowns. The solution is to implement adaptive timeouts based on recent latency percentiles and provide different timeout configurations for different operations (quick checks vs batch updates).\n\n### Implementation Guidance\n\nThis implementation section provides complete infrastructure code and skeleton implementations for the core interaction patterns described above.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Client | `net/http` with connection pooling | `fasthttp` for high performance |\n| Configuration Storage | Redis Hash structures | etcd for stronger consistency |\n| Metrics Collection | Built-in counters with periodic export | Prometheus client library |\n| Real-time Updates | Redis Pub/Sub | Apache Kafka for message ordering |\n| Dashboard Backend | WebSocket with JSON | gRPC with streaming |\n\n#### File Structure\n\n```\ninternal/\n├── flow/                           ← This component\n│   ├── coordinator.go             ← Main coordination logic\n│   ├── config_watcher.go          ← Configuration propagation\n│   ├── metrics_collector.go       ← Metrics collection and hot key detection\n│   └── health_checker.go          ← System health monitoring\n├── storage/                       ← Redis backend\n│   ├── redis_storage.go           ← Basic Redis operations\n│   └── lua_scripts.go             ← Atomic operation scripts\n└── api/                           ← Management interfaces\n    ├── handlers.go                ← REST API endpoints\n    └── websocket.go               ← Real-time dashboard connection\n```\n\n#### Request Flow Coordinator Infrastructure\n\nThis complete infrastructure handles request coordination across multiple rate limiting tiers with proper error handling and fallback logic.\n\n```go\n// Package flow provides request coordination and data flow management\n// for distributed rate limiting operations across multiple tiers and backends.\npackage flow\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sort\"\n    \"sync\"\n    \"time\"\n\n    \"github.com/rate-limiter/internal/config\"\n    \"github.com/rate-limiter/internal/storage\"\n)\n\n// FlowCoordinator manages the complete request flow from initial context\n// assembly through multi-tier evaluation to response header generation.\ntype FlowCoordinator struct {\n    storage        storage.Storage\n    ruleManager    *config.RuleManager\n    keyComposer    *config.KeyComposer\n    algorithms     map[string]Algorithm\n    localFallback  Limiter\n    metricsCollector *MetricsCollector\n    circuitBreaker *CircuitBreaker\n    mu             sync.RWMutex\n    isHealthy      bool\n}\n\n// RequestContext holds all information needed for rate limit evaluation\n// extracted from the incoming HTTP request.\ntype RequestContext struct {\n    UserID      string            `json:\"user_id\"`\n    IPAddress   string            `json:\"ip_address\"`\n    APIEndpoint string            `json:\"api_endpoint\"`\n    UserAgent   string            `json:\"user_agent\"`\n    Headers     map[string]string `json:\"headers\"`\n    Tokens      int64             `json:\"tokens\"`\n    Timestamp   time.Time         `json:\"timestamp\"`\n}\n\n// TierEvaluation represents the result of evaluating a single rate limit tier.\ntype TierEvaluation struct {\n    RuleID      string               `json:\"rule_id\"`\n    TierName    string              `json:\"tier_name\"`\n    Result      *storage.RateLimitResult `json:\"result\"`\n    Duration    time.Duration        `json:\"duration\"`\n    Algorithm   string              `json:\"algorithm\"`\n    RedisKey    string              `json:\"redis_key\"`\n}\n\n// FlowResult contains the complete result of request flow processing\n// including all tier evaluations and final decision.\ntype FlowResult struct {\n    Allowed       bool               `json:\"allowed\"`\n    TierResults   []*TierEvaluation  `json:\"tier_results\"`\n    BlockingTier  string            `json:\"blocking_tier,omitempty\"`\n    TotalDuration time.Duration      `json:\"total_duration\"`\n    UsedFallback  bool              `json:\"used_fallback\"`\n    Headers       map[string]string  `json:\"headers\"`\n    RetryAfter    time.Duration     `json:\"retry_after,omitempty\"`\n}\n\n// NewFlowCoordinator creates a new coordinator with all required dependencies.\nfunc NewFlowCoordinator(\n    storage storage.Storage,\n    ruleManager *config.RuleManager,\n    algorithms map[string]Algorithm,\n    localFallback Limiter,\n) *FlowCoordinator {\n    return &FlowCoordinator{\n        storage:          storage,\n        ruleManager:      ruleManager,\n        keyComposer:      config.NewKeyComposer(),\n        algorithms:       algorithms,\n        localFallback:    localFallback,\n        metricsCollector: NewMetricsCollector(),\n        circuitBreaker:   NewCircuitBreaker(),\n        isHealthy:        true,\n    }\n}\n\n// ProcessRequest coordinates the complete flow from request context to final result.\nfunc (fc *FlowCoordinator) ProcessRequest(ctx context.Context, reqCtx *RequestContext) (*FlowResult, error) {\n    startTime := time.Now()\n    \n    // TODO 1: Extract and validate request context (IP, User, API endpoint)\n    // TODO 2: Get matching rules from rule manager sorted by priority\n    // TODO 3: Check circuit breaker status - use fallback if open\n    // TODO 4: Evaluate each tier in priority order with short-circuit logic\n    // TODO 5: Generate standard HTTP headers for the response\n    // TODO 6: Record metrics for request processing and hot key detection\n    // TODO 7: Return comprehensive flow result with all evaluation details\n    \n    // Hint: Use fc.evaluateAllTiers() for the core multi-tier logic\n    // Hint: Implement short-circuit evaluation - stop on first denial\n    // Hint: Always record metrics even for failed/fallback requests\n    \n    panic(\"TODO: Implement ProcessRequest coordination logic\")\n}\n\n// evaluateAllTiers performs rate limit checking across all applicable tiers.\nfunc (fc *FlowCoordinator) evaluateAllTiers(ctx context.Context, reqCtx *RequestContext, rules []*config.RateLimitRule) ([]*TierEvaluation, error) {\n    // TODO 1: Iterate through rules in priority order\n    // TODO 2: For each rule, compose the appropriate Redis key\n    // TODO 3: Execute the algorithm-specific rate limit check\n    // TODO 4: Collect timing and result information for each evaluation\n    // TODO 5: Implement short-circuit logic - return immediately on first denial\n    // TODO 6: Handle Redis failures gracefully with circuit breaker pattern\n    \n    // Hint: Each algorithm (token bucket, sliding window) has different Redis operations\n    // Hint: Use goroutines for parallel tier evaluation if all must be checked\n    // Hint: Distinguish between hard failures (errors) and soft failures (limits exceeded)\n    \n    panic(\"TODO: Implement multi-tier evaluation with short-circuit logic\")\n}\n```\n\n#### Configuration Watcher Infrastructure\n\nComplete implementation of the configuration propagation system with Redis pub/sub and local caching.\n\n```go\n// ConfigurationWatcher manages real-time configuration updates using Redis pub/sub\n// and maintains local configuration cache with atomic updates.\ntype ConfigurationWatcher struct {\n    redisClient    redis.UniversalClient\n    ruleManager    *config.RuleManager\n    changeChannel  string\n    subscription   *redis.PubSub\n    localVersion   int64\n    updateCallback func([]*config.RateLimitRule)\n    mu             sync.RWMutex\n    stopChan       chan struct{}\n    healthTicker   *time.Ticker\n}\n\n// NewConfigurationWatcher creates a new watcher with Redis pub/sub subscription.\nfunc NewConfigurationWatcher(\n    redisClient redis.UniversalClient,\n    ruleManager *config.RuleManager,\n    updateCallback func([]*config.RateLimitRule),\n) *ConfigurationWatcher {\n    return &ConfigurationWatcher{\n        redisClient:    redisClient,\n        ruleManager:    ruleManager,\n        changeChannel:  \"rate_limit:config:changes\",\n        updateCallback: updateCallback,\n        stopChan:       make(chan struct{}),\n        healthTicker:   time.NewTicker(30 * time.Second),\n    }\n}\n\n// Start begins listening for configuration changes and performing health checks.\nfunc (cw *ConfigurationWatcher) Start(ctx context.Context) error {\n    // TODO 1: Subscribe to Redis configuration change channel\n    // TODO 2: Start background goroutine for processing change notifications\n    // TODO 3: Start health check goroutine for periodic version comparison\n    // TODO 4: Perform initial configuration load to sync current state\n    // TODO 5: Handle subscription reconnection on Redis connection failures\n    \n    // Hint: Use cw.processChangeNotifications() in a separate goroutine\n    // Hint: Implement exponential backoff for reconnection attempts\n    // Hint: Log all configuration changes for audit trail\n    \n    panic(\"TODO: Implement configuration watcher startup logic\")\n}\n\n// processChangeNotifications handles incoming configuration change events.\nfunc (cw *ConfigurationWatcher) processChangeNotifications() {\n    // TODO 1: Listen for messages on the Redis pub/sub channel\n    // TODO 2: Parse change notification payload (rule ID, change type, version)\n    // TODO 3: Fetch updated rule data from Redis configuration store\n    // TODO 4: Validate new configuration for consistency and correctness\n    // TODO 5: Atomically update local rule cache and notify callback\n    // TODO 6: Update local version number and heartbeat timestamp\n    \n    // Hint: Handle different change types: CREATE, UPDATE, DELETE\n    // Hint: Implement rollback logic if validation fails\n    // Hint: Use atomic operations to prevent inconsistent local state\n    \n    panic(\"TODO: Implement change notification processing\")\n}\n\n// performHealthCheck compares local vs remote configuration versions.\nfunc (cw *ConfigurationWatcher) performHealthCheck(ctx context.Context) error {\n    // TODO 1: Fetch current configuration version from Redis\n    // TODO 2: Compare with local version to detect drift\n    // TODO 3: If versions differ, trigger full configuration reload\n    // TODO 4: Update heartbeat record with instance status\n    // TODO 5: Log any version skew or synchronization issues\n    \n    // Hint: Use Redis GET on \"config:version\" key\n    // Hint: Implement full reload as fallback for partial update failures\n    \n    panic(\"TODO: Implement configuration health checking\")\n}\n```\n\n#### Metrics Collection Infrastructure\n\n```go\n// MetricsCollector aggregates rate limiting metrics and detects hot keys\n// across multiple time windows for real-time monitoring and alerting.\ntype MetricsCollector struct {\n    keyStats       sync.Map // map[string]*KeyStats\n    globalStats    *GlobalStats\n    hotKeyDetector *HotKeyDetector\n    exportChan     chan *MetricsBatch\n    windowSize     time.Duration\n    mu             sync.RWMutex\n}\n\n// MetricsBatch represents a collection of metrics ready for export.\ntype MetricsBatch struct {\n    Timestamp       time.Time               `json:\"timestamp\"`\n    RequestMetrics  map[string]*RequestMetrics `json:\"request_metrics\"`\n    PerformanceMetrics *PerformanceMetrics  `json:\"performance_metrics\"`\n    HotKeys        []string                `json:\"hot_keys\"`\n    SystemHealth   *SystemHealth           `json:\"system_health\"`\n}\n\n// RequestMetrics tracks request volume and outcomes for a specific rate limit key.\ntype RequestMetrics struct {\n    TotalRequests    int64 `json:\"total_requests\"`\n    AllowedRequests  int64 `json:\"allowed_requests\"`\n    DeniedRequests   int64 `json:\"denied_requests\"`\n    ErrorRequests    int64 `json:\"error_requests\"`\n    LastSeen         time.Time `json:\"last_seen\"`\n}\n\n// NewMetricsCollector creates a metrics collector with configurable aggregation windows.\nfunc NewMetricsCollector(windowSize time.Duration, exportChan chan *MetricsBatch) *MetricsCollector {\n    return &MetricsCollector{\n        globalStats:    NewGlobalStats(),\n        hotKeyDetector: NewHotKeyDetector(),\n        exportChan:     exportChan,\n        windowSize:     windowSize,\n    }\n}\n\n// RecordRequest updates metrics for a single rate limit request.\nfunc (mc *MetricsCollector) RecordRequest(key string, result *storage.RateLimitResult, duration time.Duration) {\n    // TODO 1: Update key-specific request counters (total, allowed, denied)\n    // TODO 2: Record request in hot key detector for trend analysis\n    // TODO 3: Update global performance metrics (latency histograms)\n    // TODO 4: Track algorithm-specific metrics if available\n    // TODO 5: Update last-seen timestamp for key activity tracking\n    \n    // Hint: Use sync.Map for concurrent access to key statistics\n    // Hint: Implement lock-free counters using atomic operations where possible\n    // Hint: Consider sampling high-volume keys to reduce memory usage\n    \n    panic(\"TODO: Implement request metrics recording\")\n}\n\n// ExportMetrics produces a metrics batch for external consumption.\nfunc (mc *MetricsCollector) ExportMetrics() *MetricsBatch {\n    // TODO 1: Aggregate all key-level metrics into exportable format\n    // TODO 2: Calculate performance percentiles and rates\n    // TODO 3: Run hot key detection algorithm on recent data\n    // TODO 4: Reset or rotate metrics windows to prevent unbounded growth\n    // TODO 5: Package everything into a comprehensive metrics batch\n    \n    // Hint: Use time-based bucketing for efficient percentile calculation\n    // Hint: Implement memory limits to prevent metrics from consuming too much RAM\n    \n    panic(\"TODO: Implement metrics export and aggregation\")\n}\n```\n\n#### Milestone Checkpoints\n\nAfter implementing the rate limit check flow:\n- Run: `go test ./internal/flow/ -run TestRequestFlow`\n- Verify: Multi-tier evaluation works with short-circuit logic\n- Test manually: Send requests that exceed different tier limits and confirm proper blocking\n\nAfter implementing configuration propagation:\n- Run: `go test ./internal/flow/ -run TestConfigWatcher`\n- Verify: Configuration changes propagate to all instances within 5 seconds\n- Test manually: Update a rule via API and confirm all instances receive the change\n\nAfter implementing metrics collection:\n- Run: `go test ./internal/flow/ -run TestMetricsCollection`\n- Verify: Hot key detection identifies keys with >10x normal traffic\n- Test manually: Generate high traffic to specific keys and confirm dashboard updates\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|-------------|-----------|-----|\n| Inconsistent rate limiting | Configuration version skew | Check heartbeat timestamps in Redis | Force configuration reload |\n| High latency on checks | Redis connection pool exhaustion | Monitor connection pool metrics | Increase pool size or add connection timeout |\n| Missing metrics data | Metrics export blocking | Check exportChan buffer size | Use buffered channel or async export |\n| False hot key detection | Clock skew between instances | Compare timestamps across instances | Implement NTP sync or relative timing |\n\n\n## Rate Limit API and Dashboard\n\n> **Milestone(s):** Milestone 5 - Rate Limit API & Dashboard\n\nThe operational success of any distributed rate limiting system fundamentally depends on its management and observability capabilities. While the core algorithms and Redis backend provide the foundation for rate limiting functionality, the management API and dashboard serve as the critical interface between human operators and the distributed system. This section explores the design of a comprehensive rate limit management system that enables dynamic configuration updates, provides real-time visibility into system behavior, and maintains operational safety through self-protection mechanisms.\n\n### Mental Model: The Air Traffic Control System\n\nThink of the rate limit API and dashboard as an air traffic control system for request traffic. Just as air traffic controllers need real-time radar displays showing aircraft positions, flight paths, and airport capacity, rate limit operators need dashboards showing current request rates, quota utilization, and system health across all tiers. The management API acts like the control tower radio system, allowing controllers to dynamically adjust flight patterns (rate limit rules) based on changing conditions without shutting down the airport (restarting services).\n\nThe control tower must also protect itself - it can't allow unlimited radio chatter that would overwhelm the controllers' ability to manage air traffic. Similarly, the rate limit management API must implement self-rate-limiting to prevent its own operations from overwhelming the very system it manages. This creates an interesting bootstrap problem: how do you rate limit the rate limiter without creating circular dependencies?\n\n### Rate Limit Management API\n\nThe **Rate Limit Management API** serves as the primary interface for creating, reading, updating, and deleting rate limit rules across the entire distributed system. Unlike static configuration files that require service restarts, this API enables dynamic rule management with immediate propagation to all application instances through Redis pub/sub mechanisms.\n\n#### API Design Philosophy\n\nThe management API follows REST principles with a strong emphasis on validation, versioning, and audit trails. Every rule change creates an audit log entry, enabling operators to understand how rate limiting behavior evolved over time. The API supports both immediate rule activation and scheduled deployments, allowing operators to prepare rate limit changes during low-traffic periods and activate them precisely when needed.\n\n> **Decision: RESTful API with Resource-Based URLs**\n> - **Context**: Need to expose CRUD operations for rate limit rules with clear semantics and easy integration\n> - **Options Considered**: REST API, GraphQL API, gRPC API\n> - **Decision**: REST API with resource-based URLs following `/api/v1/rules/{id}` pattern\n> - **Rationale**: REST provides familiar semantics for CRUD operations, excellent HTTP caching support, and broad client library support across languages\n> - **Consequences**: Enables standard HTTP tooling and caching but requires multiple requests for complex operations\n\nThe API resource structure centers around the `RateLimitRule` entity with hierarchical organization supporting rule grouping, inheritance, and override patterns:\n\n| Endpoint | Method | Description | Request Body | Response |\n|----------|--------|-------------|--------------|----------|\n| `/api/v1/rules` | GET | List all rate limit rules with filtering | Query parameters | Rule list with pagination |\n| `/api/v1/rules` | POST | Create new rate limit rule | `RateLimitRule` JSON | Created rule with assigned ID |\n| `/api/v1/rules/{id}` | GET | Retrieve specific rule by ID | None | Complete `RateLimitRule` object |\n| `/api/v1/rules/{id}` | PUT | Update existing rule completely | Complete `RateLimitRule` | Updated rule object |\n| `/api/v1/rules/{id}` | PATCH | Partial rule update | Partial `RateLimitRule` | Updated rule object |\n| `/api/v1/rules/{id}` | DELETE | Remove rule and stop enforcement | None | Deletion confirmation |\n| `/api/v1/rules/{id}/preview` | POST | Test rule against sample requests | `RateLimitRequest` array | Preview results without enforcement |\n| `/api/v1/rules/{id}/reset` | POST | Clear all counters for rule | Optional key filter | Reset confirmation |\n| `/api/v1/rules/bulk` | POST | Bulk create/update operations | Rule array with operations | Bulk operation results |\n\n#### Rule Validation and Constraints\n\nThe API implements comprehensive validation ensuring that rate limit rules are syntactically correct, semantically meaningful, and operationally safe before activation. Validation occurs at multiple levels: syntax validation checks JSON structure and data types, semantic validation ensures logical consistency between fields, and operational validation prevents rules that could destabilize the system.\n\n**Syntax Validation Rules:**\n\n| Field | Validation Rule | Error Message |\n|-------|-----------------|---------------|\n| `name` | Non-empty string, max 100 chars | \"Rule name required and must be under 100 characters\" |\n| `key_pattern` | Valid regex pattern | \"Key pattern must be valid regular expression\" |\n| `algorithm` | One of: token_bucket, sliding_window_counter, sliding_window_log | \"Algorithm must be supported type\" |\n| `limit` | Positive integer > 0 | \"Rate limit must be positive integer\" |\n| `window` | Duration between 1s and 24h | \"Time window must be between 1 second and 24 hours\" |\n| `burst_limit` | Optional, >= limit if specified | \"Burst limit must be >= base limit when specified\" |\n| `priority` | Integer between 1 and 100 | \"Priority must be between 1 (low) and 100 (high)\" |\n\n**Semantic Validation Rules:**\n\nThe API performs semantic validation to catch logical inconsistencies that could lead to unexpected behavior. For example, a rule with `algorithm: \"token_bucket\"` must specify a `burst_limit` since token buckets are specifically designed for burst handling. Similarly, rules targeting the same key pattern cannot have conflicting priorities that would create ambiguous evaluation order.\n\n**Operational Safety Validation:**\n\nTo prevent operators from accidentally creating rules that could destabilize the system, the API enforces operational safety constraints. Rules with extremely low limits (less than 10 requests per minute) trigger warnings and require explicit confirmation. Rules with very short time windows (less than 10 seconds) are flagged as potentially problematic for distributed consistency. The API also prevents creation of more than 1000 active rules per system to avoid overwhelming the Redis backend with excessive key space consumption.\n\n#### Rule Versioning and Rollback\n\nEvery rule modification creates a new version while preserving the complete history of changes. This versioning system enables rapid rollback to previous configurations when new rules cause unexpected behavior. The versioning implementation stores rule snapshots in Redis with timestamp-based keys, enabling point-in-time recovery and audit trail reconstruction.\n\n| Version Operation | Endpoint | Description | Impact |\n|------------------|----------|-------------|--------|\n| Create Version | PUT/PATCH `/api/v1/rules/{id}` | Automatic version creation on modification | New version stored, old version archived |\n| List Versions | GET `/api/v1/rules/{id}/versions` | Retrieve complete version history | No impact, read-only operation |\n| View Version | GET `/api/v1/rules/{id}/versions/{version}` | Retrieve specific historical version | No impact, read-only operation |\n| Rollback | POST `/api/v1/rules/{id}/rollback` | Revert to specified previous version | Immediate rule update, new version created |\n\n#### Configuration Propagation Mechanism\n\n![Configuration Update Flow](./diagrams/configuration-propagation-flow.svg)\n\nWhen a rule is created or modified through the API, the change must propagate to all application instances running the distributed rate limiter. This propagation occurs through a multi-stage process designed to ensure consistency while minimizing latency and avoiding race conditions.\n\n**Propagation Stages:**\n\n1. **Validation and Persistence**: The API validates the incoming rule change and persists it to Redis with a unique version number and timestamp\n2. **Change Notification**: A change notification containing the rule ID and version is published to the Redis channel `rate_limit_config_changes`\n3. **Instance Subscription**: All application instances maintain Redis subscriptions to the configuration change channel\n4. **Version Comparison**: Upon receiving a change notification, each instance compares the announced version with its local cache\n5. **Rule Retrieval**: Instances with outdated versions fetch the complete updated rule from Redis\n6. **Local Cache Update**: The instance updates its local rule cache and begins enforcing the new configuration\n7. **Acknowledgment**: Each instance publishes an acknowledgment to confirm successful configuration update\n\nThis propagation mechanism handles network partitions and instance failures gracefully. Instances that miss change notifications due to temporary disconnections perform periodic configuration health checks, comparing their local version numbers against Redis to detect missed updates. The system also implements exponential backoff retry logic when instances cannot reach Redis during configuration updates.\n\n> The critical design insight here is that configuration propagation must be eventually consistent rather than strongly consistent. Temporary inconsistencies where different instances enforce slightly different rules for a few seconds are acceptable, but permanent inconsistencies where some instances never receive updates are not.\n\n#### Audit Trail and Change History\n\nEvery API operation creates detailed audit log entries capturing who made what changes when and why. These audit logs prove essential for troubleshooting unexpected rate limiting behavior and maintaining compliance with change management processes. The audit system records both successful operations and failed attempts, enabling security teams to detect unauthorized access attempts.\n\n| Audit Event Type | Logged Information | Retention Period |\n|------------------|-------------------|------------------|\n| Rule Creation | User ID, rule content, timestamp, source IP | 1 year |\n| Rule Modification | User ID, old values, new values, change reason, timestamp | 1 year |\n| Rule Deletion | User ID, deleted rule content, deletion reason, timestamp | 1 year |\n| Bulk Operations | User ID, operation list, success/failure per rule, timestamp | 1 year |\n| Preview Requests | User ID, rule ID, test scenarios, results, timestamp | 30 days |\n| Reset Operations | User ID, rule ID, affected keys, timestamp | 90 days |\n| Authentication Failures | Attempted user ID, source IP, failure reason, timestamp | 6 months |\n| Authorization Denials | User ID, attempted operation, denial reason, timestamp | 6 months |\n\n### Standard Rate Limit Headers\n\nThe distributed rate limiter implements standard HTTP headers following RFC 6585 and industry best practices to provide clients with actionable information about their rate limiting status. These headers enable clients to implement intelligent retry logic, display accurate quota information to users, and avoid unnecessarily aggressive request patterns that could trigger additional rate limiting.\n\n#### Header Implementation Strategy\n\nThe header implementation follows a tiered approach where different header sets are included based on the rate limiting result. Successful requests include current quota information, denied requests include retry guidance, and error scenarios include minimal diagnostic information to avoid information leakage.\n\n> **Decision: RFC 6585 Compliance with Extensions**\n> - **Context**: Need standardized headers for client rate limit awareness and retry logic\n> - **Options Considered**: Custom headers, RFC 6585 standard, GitHub-style headers\n> - **Decision**: Implement RFC 6585 with additional headers for enhanced client experience\n> - **Rationale**: RFC compliance ensures broad client library support while extensions provide operational benefits\n> - **Consequences**: Enables standard tooling but requires careful header size management to avoid HTTP limits\n\n**Standard Rate Limit Headers:**\n\n| Header Name | When Included | Value Format | Example | Purpose |\n|-------------|---------------|---------------|---------|---------|\n| `X-RateLimit-Limit` | All responses | Integer | `1000` | Maximum requests allowed in current window |\n| `X-RateLimit-Remaining` | All responses | Integer | `742` | Requests remaining in current window |\n| `X-RateLimit-Reset` | All responses | Unix timestamp | `1640995200` | When the current window resets |\n| `X-RateLimit-Window` | All responses | Duration in seconds | `3600` | Length of rate limit window |\n| `Retry-After` | 429 responses only | Seconds until retry | `45` | Minimum wait time before retry attempt |\n\n**Extended Headers for Enhanced Client Experience:**\n\n| Header Name | When Included | Value Format | Example | Purpose |\n|-------------|---------------|---------------|---------|---------|\n| `X-RateLimit-Policy` | All responses | Algorithm identifier | `token_bucket` | Which rate limiting algorithm is active |\n| `X-RateLimit-Scope` | All responses | Scope identifier | `user:12345` | What entity the limit applies to |\n| `X-RateLimit-Burst` | Token bucket responses | Integer | `1500` | Maximum burst capacity available |\n| `X-RateLimit-Tier` | Multi-tier responses | Tier name | `per-user` | Which tier triggered the limit |\n\n#### Multi-Tier Header Aggregation\n\nWhen multiple rate limit tiers apply to a single request, the header generation logic must aggregate information from all applicable tiers to provide clients with actionable guidance. The aggregation follows a \"most restrictive wins\" principle where the tightest remaining quota determines the header values.\n\n**Tier Aggregation Logic:**\n\n1. **Collect All Applicable Limits**: Gather rate limit results from user, IP, API, and global tiers\n2. **Identify Most Restrictive**: Find the tier with the lowest `remaining/limit` ratio\n3. **Check for Denials**: If any tier denies the request, use that tier's information for headers\n4. **Calculate Aggregate Reset**: Use the earliest reset time across all tiers\n5. **Compose Scope Header**: Create comma-separated list of all applicable scopes\n\nFor example, a request that passes user-level limits (900/1000 remaining) but approaches IP-level limits (15/100 remaining) would generate headers reflecting the IP limit since it represents the most immediate constraint on future requests.\n\n#### Client Retry Guidance\n\nThe `Retry-After` header calculation considers not just the immediate rate limit violation but also the broader context of multi-tier limits and algorithm-specific behavior. For token bucket algorithms, the retry time reflects the token refill rate. For sliding window algorithms, it considers both the window reset time and the distribution of recent requests.\n\n**Retry-After Calculation Algorithm:**\n\n| Algorithm | Calculation Method | Rationale |\n|-----------|-------------------|-----------|\n| Token Bucket | `max(1, (tokens_needed - current_tokens) / refill_rate)` | Time to accumulate sufficient tokens |\n| Sliding Window Counter | `window_duration - time_since_window_start + jitter` | Wait for window boundary plus randomization |\n| Sliding Window Log | `oldest_request_timestamp + window_duration - current_time + jitter` | Wait for oldest request to age out |\n\nThe retry calculation includes small random jitter (5-15% of the base retry time) to prevent thundering herd effects where many clients retry simultaneously after receiving identical `Retry-After` values.\n\n### Real-time Dashboard Architecture\n\nThe **real-time dashboard** provides operational visibility into rate limiting behavior across all tiers, algorithms, and time scales. Unlike traditional monitoring dashboards that display historical data, this dashboard focuses on current quota utilization, active patterns, and immediate operational decisions that rate limit administrators need to make.\n\n#### Mental Model: Power Grid Control Room\n\nThink of the rate limiting dashboard like a power grid control room where operators monitor electricity demand across different regions and time scales. Just as grid operators need real-time visibility into current load, peak capacity, and demand forecasting to prevent blackouts, rate limit operators need current quota utilization, request patterns, and trend analysis to prevent service degradation. The dashboard must update continuously without overwhelming the underlying rate limiting system, just as grid monitoring cannot consume significant power itself.\n\n#### Dashboard Data Architecture\n\nThe dashboard architecture separates data collection, aggregation, and presentation into distinct layers to ensure that dashboard operations never impact rate limiting performance. Data flows through a pipeline from individual rate limit checks through local aggregation, Redis-based consolidation, and finally WebSocket streaming to dashboard clients.\n\n**Data Flow Layers:**\n\n| Layer | Component | Responsibility | Update Frequency |\n|-------|-----------|----------------|------------------|\n| Collection | `MetricsCollector` | Capture individual rate limit results | Per request |\n| Local Aggregation | `MetricsBatch` | Combine metrics within single instance | Every 5 seconds |\n| Redis Consolidation | Redis Streams | Merge metrics from all instances | Every 10 seconds |\n| Dashboard Streaming | WebSocket Server | Push updates to dashboard clients | Every 2 seconds |\n\n#### Real-time Metrics Collection\n\nThe `MetricsCollector` component captures detailed information about every rate limit decision without impacting the critical path performance. Collection uses lock-free data structures and asynchronous batching to ensure that metrics gathering never blocks rate limit checks.\n\n**Collected Metric Categories:**\n\n| Metric Category | Data Points | Aggregation Method | Purpose |\n|-----------------|-------------|-------------------|---------|\n| Request Counts | Total, allowed, denied by rule | Sum over time windows | Quota utilization tracking |\n| Response Times | P50, P95, P99 latency by tier | Histogram bucketing | Performance monitoring |\n| Algorithm Performance | Token refill rates, window efficiency | Moving averages | Algorithm tuning guidance |\n| Error Rates | Redis timeouts, fallback activation | Count and rate calculation | System health assessment |\n| Hot Key Detection | Request distribution across keys | Top-K tracking | Load balancing insights |\n\n#### WebSocket-Based Real-Time Updates\n\nThe dashboard uses WebSocket connections to stream real-time updates to client browsers without polling overhead. The WebSocket server implements intelligent update batching and client-specific filtering to ensure that each dashboard user receives only relevant updates without overwhelming their browser.\n\n**WebSocket Message Types:**\n\n| Message Type | Payload Structure | Update Frequency | Client Response |\n|--------------|------------------|------------------|-----------------|\n| `quota_update` | `{rule_id, current, limit, remaining}` | Every 2 seconds | Update quota gauges |\n| `tier_status` | `{tier_name, active_rules, total_requests}` | Every 5 seconds | Refresh tier summary |\n| `hot_keys` | `{key, request_rate, trend}` | Every 10 seconds | Update hot key list |\n| `system_health` | `{redis_status, fallback_rate, error_count}` | Every 5 seconds | Update health indicators |\n| `rule_change` | `{operation, rule_id, new_config}` | Immediate | Refresh rule display |\n\n#### Dashboard User Interface Design\n\nThe dashboard interface organizes information hierarchically from system overview through tier-specific views to individual rule analysis. Each view provides actionable information at the appropriate level of detail for different operational decisions.\n\n**Dashboard View Hierarchy:**\n\n1. **System Overview**: High-level health, total request rates, and critical alerts across all tiers\n2. **Tier Dashboard**: Detailed view of user, IP, API, or global tier with rule-by-rule breakdown\n3. **Rule Analysis**: Deep dive into individual rule performance, quota utilization patterns, and historical trends\n4. **Hot Key Investigation**: Real-time analysis of disproportionately accessed keys with geographic and temporal patterns\n5. **System Health**: Redis cluster status, instance connectivity, and performance metrics\n\n#### Dashboard Performance Optimization\n\nTo ensure the dashboard remains responsive under high load conditions, several optimization strategies prevent dashboard operations from impacting rate limiting performance or overwhelming client browsers with excessive updates.\n\n> **Decision: Adaptive Update Frequency Based on Activity Level**\n> - **Context**: Dashboard updates must be frequent enough for operational decisions but not so frequent as to impact performance\n> - **Options Considered**: Fixed update intervals, event-driven updates, adaptive frequency\n> - **Decision**: Implement adaptive update frequency that increases during high activity and decreases during stable periods\n> - **Rationale**: Provides real-time visibility when needed while conserving resources during normal operations\n> - **Consequences**: More complex implementation but significantly better resource efficiency\n\n**Optimization Strategies:**\n\n| Strategy | Implementation | Benefit | Trade-off |\n|----------|---------------|---------|-----------|\n| Client-Side Filtering | Dashboard clients specify interest filters | Reduced bandwidth usage | More complex client logic |\n| Update Batching | Group multiple updates into single WebSocket messages | Lower message overhead | Slightly increased latency |\n| Adaptive Frequency | Increase update rate during anomalies, decrease during stability | Dynamic resource usage | Complex triggering logic |\n| Data Compression | Compress WebSocket payloads for large updates | Reduced network usage | CPU overhead for compression |\n| Local Caching | Cache unchanged data on client side | Minimal redundant updates | Client-side memory usage |\n\n### Self-Rate-Limiting the Management API\n\nThe management API faces a unique challenge: it must protect itself from abuse without creating circular dependencies with the rate limiting system it manages. This creates a bootstrap problem where the rate limiter cannot rely on its own Redis backend for API protection since API operations might be necessary to fix Redis connectivity issues.\n\n#### Mental Model: Emergency Services Communication\n\nThink of the management API like emergency services communication systems that must remain operational even when the systems they manage are failing. A fire department's radio system cannot depend on the electrical grid they might need to repair - it needs independent power and communication channels. Similarly, the rate limit management API needs independent protection mechanisms that don't depend on the distributed rate limiting infrastructure it controls.\n\n#### Independent Rate Limiting Strategy\n\nThe management API implements a lightweight, in-memory rate limiting system that operates independently of the main Redis-backed rate limiter. This independent system uses simpler algorithms and local state to provide basic protection without external dependencies.\n\n> **Decision: Local Token Bucket for API Self-Protection**\n> - **Context**: Management API needs rate limiting protection that doesn't depend on Redis or distributed state\n> - **Options Considered**: Local token bucket, fixed window counters, dependency on main rate limiter\n> - **Decision**: Implement local token bucket with per-client tracking using IP address + API key\n> - **Rationale**: Token bucket provides burst handling for legitimate batch operations while preventing sustained abuse\n> - **Consequences**: Protection is per-instance rather than cluster-wide, but maintains independence from Redis\n\n**Self-Rate-Limiting Architecture:**\n\n| Component | Implementation | Responsibility | Independence Level |\n|-----------|---------------|---------------|-------------------|\n| `APITokenBucket` | In-memory token bucket per client | Request rate control | Fully independent |\n| `ClientIdentifier` | IP + API key hashing | Client identification | No external dependencies |\n| `LocalRateLimiter` | Embedded rate limiter | API call limiting | Memory-only state |\n| `OverrideManager` | Emergency bypass tokens | Incident response | File-based configuration |\n\n#### API-Specific Rate Limit Tiers\n\nThe management API implements multiple tiers of rate limiting tailored to different operation types and client authentication levels. Read operations receive higher quotas than write operations, and authenticated API clients receive higher limits than unauthenticated requests.\n\n**API Rate Limit Tiers:**\n\n| Operation Type | Authenticated Limit | Unauthenticated Limit | Burst Allowance | Rationale |\n|----------------|-------------------|---------------------|-----------------|-----------|\n| Read Operations | 100 req/min | 20 req/min | 150% of base | Dashboard updates need frequent reads |\n| Write Operations | 20 req/min | 5 req/min | 130% of base | Rule changes should be deliberate |\n| Bulk Operations | 5 req/min | 0 req/min | 110% of base | Bulk changes need careful consideration |\n| Preview Operations | 50 req/min | 10 req/min | 200% of base | Testing scenarios may need rapid iteration |\n| Reset Operations | 10 req/min | 0 req/min | 100% of base | Counter resets have immediate system impact |\n\n#### Emergency Override Mechanisms\n\nDuring critical incidents where rate limiting rules may be causing service outages, operators need the ability to bypass normal API rate limits to implement emergency fixes. The override system provides this capability through multiple authentication factors and audit trails.\n\n**Override Activation Methods:**\n\n| Override Type | Activation Requirement | Duration | Audit Level |\n|---------------|----------------------|----------|-------------|\n| Temporary Bypass | Admin API key + incident ticket | 1 hour | Full request logging |\n| Emergency Token | Pre-generated token + manager approval | 30 minutes | Real-time notification |\n| File-Based Override | Server file system access | Until file removal | File system audit trail |\n| Multi-Admin Confirmation | Two admin API keys simultaneously | 2 hours | Multi-party audit logging |\n\nThe emergency override system ensures that legitimate incident response can proceed quickly while maintaining strong audit trails and preventing abuse. Override usage triggers immediate notifications to security teams and creates detailed logs for post-incident review.\n\n#### API Protection Without Circular Dependencies\n\nTo avoid circular dependencies where the management API rate limiting depends on the systems it manages, the implementation carefully isolates API protection from distributed state. The local rate limiter uses only in-memory data structures and local file system access for persistent configuration.\n\n**Dependency Isolation Strategies:**\n\n1. **Separate Algorithm Implementation**: The API uses a simplified token bucket implementation that shares no code with the main distributed rate limiter\n2. **Local State Only**: All API rate limiting state remains in local memory, never touching Redis or shared storage\n3. **Independent Configuration**: API rate limits are configured through environment variables or local files, not through the main rule management system\n4. **Graceful Fallback**: When local rate limiting fails, the API defaults to conservative limits rather than disabling protection\n5. **Health Check Separation**: API health checks use direct Redis connectivity tests rather than going through the rate limiting system\n\n> The fundamental design principle here is defense in depth - the management API must remain secure and operational even when every other component of the distributed rate limiting system is failing or misconfigured.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Dashboard Overwhelming Rate Limiter with Monitoring Queries**\n\nA common mistake is implementing dashboard updates that query rate limiting state so frequently that the monitoring system becomes a significant load on the Redis backend. This creates a feedback loop where dashboard usage impacts rate limiting performance, which triggers more frequent dashboard updates to investigate the performance problem.\n\n**Why it's wrong**: Dashboard queries compete with production rate limiting operations for Redis resources, potentially causing legitimate rate limit checks to fail or experience high latency.\n\n**How to fix**: Implement async metrics collection where rate limiting operations push metrics to a separate data path rather than having the dashboard pull current state. Use Redis Streams or pub/sub to decouple dashboard data from operational state.\n\n⚠️ **Pitfall: Configuration Changes Without Validation Rollback**\n\nWhen implementing dynamic configuration updates, developers often forget to implement automatic rollback when new configurations cause system instability. A rule change that accidentally sets all limits to 1 request per hour can cause immediate service outages.\n\n**Why it's wrong**: Bad configurations can render the entire service unusable, and manual rollback takes time during which the service remains degraded.\n\n**How to fix**: Implement automatic configuration validation that tests new rules against recent traffic patterns and automatically rolls back changes that cause denial rates to spike above configurable thresholds.\n\n⚠️ **Pitfall: Management API Using Same Redis Instance as Rate Limiting**\n\nSome implementations use the same Redis instance for both rate limiting operations and management API data storage. This creates a single point of failure where Redis issues impact both operational rate limiting and the ability to fix configuration problems.\n\n**Why it's wrong**: When Redis becomes unavailable, operators lose both rate limiting functionality and the ability to modify configuration to address the issue.\n\n**How to fix**: Use separate Redis instances or different Redis databases for operational rate limiting and management data. Consider using a different storage backend entirely for management API data to ensure independence.\n\n⚠️ **Pitfall: Rate Limit Headers Calculated After Request Processing**\n\nA subtle error occurs when rate limit headers are calculated after the main request processing rather than during the rate limit check. This can result in headers that don't reflect the actual rate limiting decision or show inconsistent quota information.\n\n**Why it's wrong**: Clients receive misleading information about their rate limiting status, leading to incorrect retry behavior and poor user experience.\n\n**How to fix**: Calculate all rate limit headers during the actual rate limit check operation and pass them through the request context to ensure header consistency with the rate limiting decision.\n\n### Implementation Guidance\n\nThe rate limit API and dashboard implementation requires careful coordination between HTTP API handling, real-time data streaming, and independent rate limiting logic. This section provides complete implementation guidance for building these components.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP API Framework | `net/http` with `gorilla/mux` router | `gin-gonic/gin` with OpenAPI generation |\n| WebSocket Server | `gorilla/websocket` library | `Socket.io` with Redis adapter |\n| Real-time Metrics | In-memory aggregation with periodic flush | Redis Streams with consumer groups |\n| Dashboard Frontend | Server-rendered HTML with vanilla JS | React/Vue SPA with Chart.js visualization |\n| API Authentication | JWT tokens with HMAC signing | OAuth2 with RBAC authorization |\n| Configuration Storage | JSON files with file watching | etcd with distributed locking |\n\n#### Recommended Module Structure\n\n```\nproject-root/\n  cmd/\n    rate-limiter/main.go           ← main rate limiting service\n    management-api/main.go         ← management API server\n    dashboard/main.go              ← dashboard web server\n  internal/\n    api/\n      handlers/\n        rules.go                   ← rate limit rule CRUD handlers\n        dashboard.go               ← dashboard data API handlers\n        websocket.go               ← real-time WebSocket server\n      middleware/\n        auth.go                    ← API authentication middleware\n        ratelimit.go               ← self-rate-limiting middleware\n        headers.go                 ← rate limit header injection\n      validation/\n        rules.go                   ← rule validation logic\n        constraints.go             ← operational safety constraints\n    dashboard/\n      metrics/\n        collector.go               ← metrics collection and aggregation\n        streaming.go               ← WebSocket streaming logic\n        batch.go                   ← metric batching and compression\n      ui/\n        templates/                 ← HTML templates for dashboard\n        static/                    ← CSS, JS, and image assets\n    config/\n      propagation.go               ← configuration change propagation\n      versioning.go                ← rule versioning and rollback\n      audit.go                     ← audit trail implementation\n  web/\n    dashboard/\n      index.html                   ← main dashboard interface\n      js/dashboard.js              ← dashboard JavaScript logic\n      css/styles.css               ← dashboard styling\n```\n\n#### Infrastructure Starter Code\n\n**Complete API Rate Limiter (api/middleware/ratelimit.go)**:\n\n```go\npackage middleware\n\nimport (\n    \"fmt\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n    \"net\"\n    \"strings\"\n)\n\n// APIRateLimiter provides self-rate-limiting for management API\ntype APIRateLimiter struct {\n    mu          sync.RWMutex\n    clients     map[string]*TokenBucket\n    cleanup     time.Duration\n    limits      map[string]APILimits\n}\n\n// APILimits defines rate limits for different API operation types\ntype APILimits struct {\n    ReadLimit   int64         // requests per minute for read operations\n    WriteLimit  int64         // requests per minute for write operations\n    BulkLimit   int64         // requests per minute for bulk operations\n    BurstRatio  float64       // burst allowance ratio (e.g., 1.5 = 150%)\n}\n\n// TokenBucket implements simple in-memory token bucket for API protection\ntype TokenBucket struct {\n    capacity    int64     // maximum tokens\n    tokens      int64     // current tokens\n    refillRate  int64     // tokens per minute\n    lastRefill  time.Time // last refill timestamp\n    mu          sync.Mutex\n}\n\n// NewAPIRateLimiter creates self-rate-limiter for management API\nfunc NewAPIRateLimiter() *APIRateLimiter {\n    limits := map[string]APILimits{\n        \"authenticated\": {\n            ReadLimit:  100,\n            WriteLimit: 20,\n            BulkLimit:  5,\n            BurstRatio: 1.5,\n        },\n        \"unauthenticated\": {\n            ReadLimit:  20,\n            WriteLimit: 5,\n            BulkLimit:  0,\n            BurstRatio: 1.3,\n        },\n    }\n    \n    rl := &APIRateLimiter{\n        clients: make(map[string]*TokenBucket),\n        cleanup: 5 * time.Minute,\n        limits:  limits,\n    }\n    \n    // Start background cleanup of inactive clients\n    go rl.cleanupLoop()\n    \n    return rl\n}\n\n// Middleware returns HTTP middleware function for request rate limiting\nfunc (rl *APIRateLimiter) Middleware() func(http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            // Extract client identifier and operation type\n            clientID := rl.getClientID(r)\n            opType := rl.getOperationType(r)\n            authLevel := rl.getAuthLevel(r)\n            \n            // Check rate limit for this client and operation\n            allowed, remaining, resetTime := rl.checkLimit(clientID, opType, authLevel)\n            \n            // Add rate limit headers to response\n            w.Header().Set(\"X-RateLimit-Limit\", fmt.Sprintf(\"%d\", rl.getLimit(opType, authLevel)))\n            w.Header().Set(\"X-RateLimit-Remaining\", fmt.Sprintf(\"%d\", remaining))\n            w.Header().Set(\"X-RateLimit-Reset\", fmt.Sprintf(\"%d\", resetTime.Unix()))\n            w.Header().Set(\"X-RateLimit-Scope\", fmt.Sprintf(\"api:%s\", clientID))\n            \n            if !allowed {\n                // Calculate retry after based on refill rate\n                retryAfter := rl.calculateRetryAfter(opType, authLevel)\n                w.Header().Set(\"Retry-After\", fmt.Sprintf(\"%d\", int(retryAfter.Seconds())))\n                \n                http.Error(w, \"API rate limit exceeded\", http.StatusTooManyRequests)\n                return\n            }\n            \n            next.ServeHTTP(w, r)\n        })\n    }\n}\n\n// checkLimit performs rate limiting check for client and operation type\nfunc (rl *APIRateLimiter) checkLimit(clientID, opType, authLevel string) (bool, int64, time.Time) {\n    rl.mu.Lock()\n    defer rl.mu.Unlock()\n    \n    // Get or create token bucket for this client\n    bucket, exists := rl.clients[clientID]\n    if !exists {\n        limit := rl.getLimit(opType, authLevel)\n        burstRatio := rl.limits[authLevel].BurstRatio\n        capacity := int64(float64(limit) * burstRatio)\n        \n        bucket = &TokenBucket{\n            capacity:   capacity,\n            tokens:     capacity,\n            refillRate: limit,\n            lastRefill: time.Now(),\n        }\n        rl.clients[clientID] = bucket\n    }\n    \n    return bucket.consume(1)\n}\n\n// consume attempts to consume tokens from bucket\nfunc (tb *TokenBucket) consume(tokens int64) (bool, int64, time.Time) {\n    tb.mu.Lock()\n    defer tb.mu.Unlock()\n    \n    now := time.Now()\n    \n    // Calculate tokens to add based on time elapsed\n    elapsed := now.Sub(tb.lastRefill)\n    tokensToAdd := int64(elapsed.Minutes() * float64(tb.refillRate))\n    \n    if tokensToAdd > 0 {\n        tb.tokens = min(tb.capacity, tb.tokens + tokensToAdd)\n        tb.lastRefill = now\n    }\n    \n    // Calculate next reset time (when bucket will be full)\n    resetTime := now.Add(time.Duration((tb.capacity - tb.tokens) * 60 / tb.refillRate) * time.Second)\n    \n    if tb.tokens >= tokens {\n        tb.tokens -= tokens\n        return true, tb.tokens, resetTime\n    }\n    \n    return false, tb.tokens, resetTime\n}\n\n// Helper functions for client identification and operation classification\nfunc (rl *APIRateLimiter) getClientID(r *http.Request) string {\n    // Try API key first, fall back to IP address\n    if apiKey := r.Header.Get(\"X-API-Key\"); apiKey != \"\" {\n        return fmt.Sprintf(\"key:%s\", apiKey)\n    }\n    \n    ip := rl.extractIP(r)\n    return fmt.Sprintf(\"ip:%s\", ip)\n}\n\nfunc (rl *APIRateLimiter) extractIP(r *http.Request) string {\n    // Check X-Forwarded-For header for proxy scenarios\n    if xff := r.Header.Get(\"X-Forwarded-For\"); xff != \"\" {\n        parts := strings.Split(xff, \",\")\n        if len(parts) > 0 {\n            return strings.TrimSpace(parts[0])\n        }\n    }\n    \n    // Fall back to RemoteAddr\n    ip, _, err := net.SplitHostPort(r.RemoteAddr)\n    if err != nil {\n        return r.RemoteAddr\n    }\n    return ip\n}\n\nfunc (rl *APIRateLimiter) getOperationType(r *http.Request) string {\n    switch r.Method {\n    case \"GET\":\n        return \"read\"\n    case \"PUT\", \"PATCH\", \"DELETE\":\n        return \"write\"\n    case \"POST\":\n        if strings.Contains(r.URL.Path, \"/bulk\") {\n            return \"bulk\"\n        }\n        return \"write\"\n    default:\n        return \"read\"\n    }\n}\n\nfunc (rl *APIRateLimiter) getAuthLevel(r *http.Request) string {\n    if r.Header.Get(\"X-API-Key\") != \"\" {\n        return \"authenticated\"\n    }\n    return \"unauthenticated\"\n}\n\nfunc (rl *APIRateLimiter) getLimit(opType, authLevel string) int64 {\n    limits := rl.limits[authLevel]\n    switch opType {\n    case \"read\":\n        return limits.ReadLimit\n    case \"write\":\n        return limits.WriteLimit\n    case \"bulk\":\n        return limits.BulkLimit\n    default:\n        return limits.ReadLimit\n    }\n}\n\nfunc (rl *APIRateLimiter) calculateRetryAfter(opType, authLevel string) time.Duration {\n    limit := rl.getLimit(opType, authLevel)\n    // Time to get one token back\n    return time.Duration(60/limit) * time.Second\n}\n\nfunc (rl *APIRateLimiter) cleanupLoop() {\n    ticker := time.NewTicker(rl.cleanup)\n    defer ticker.Stop()\n    \n    for range ticker.C {\n        rl.cleanupInactiveClients()\n    }\n}\n\nfunc (rl *APIRateLimiter) cleanupInactiveClients() {\n    rl.mu.Lock()\n    defer rl.mu.Unlock()\n    \n    cutoff := time.Now().Add(-rl.cleanup)\n    for clientID, bucket := range rl.clients {\n        bucket.mu.Lock()\n        lastActivity := bucket.lastRefill\n        bucket.mu.Unlock()\n        \n        if lastActivity.Before(cutoff) {\n            delete(rl.clients, clientID)\n        }\n    }\n}\n\nfunc min(a, b int64) int64 {\n    if a < b {\n        return a\n    }\n    return b\n}\n```\n\n**Complete WebSocket Streaming Server (dashboard/metrics/streaming.go)**:\n\n```go\npackage metrics\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"log\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/gorilla/websocket\"\n)\n\n// StreamingServer handles real-time metric streaming to dashboard clients\ntype StreamingServer struct {\n    clients     map[*Client]bool\n    clientsMu   sync.RWMutex\n    register    chan *Client\n    unregister  chan *Client\n    broadcast   chan []byte\n    collector   *MetricsCollector\n    upgrader    websocket.Upgrader\n}\n\n// Client represents a connected dashboard WebSocket client\ntype Client struct {\n    conn      *websocket.Conn\n    send      chan []byte\n    filters   map[string]bool  // which metrics this client wants\n    lastSeen  time.Time\n}\n\n// MetricUpdate represents a single metric update sent to clients\ntype MetricUpdate struct {\n    Type      string                 `json:\"type\"`\n    Timestamp time.Time              `json:\"timestamp\"`\n    Data      map[string]interface{} `json:\"data\"`\n}\n\n// NewStreamingServer creates WebSocket server for real-time dashboard updates\nfunc NewStreamingServer(collector *MetricsCollector) *StreamingServer {\n    return &StreamingServer{\n        clients:    make(map[*Client]bool),\n        register:   make(chan *Client),\n        unregister: make(chan *Client),\n        broadcast:  make(chan []byte, 256),\n        collector:  collector,\n        upgrader: websocket.Upgrader{\n            CheckOrigin: func(r *http.Request) bool {\n                // In production, implement proper origin checking\n                return true\n            },\n        },\n    }\n}\n\n// Start begins the streaming server event loop\nfunc (s *StreamingServer) Start(ctx context.Context) {\n    go s.handleClients()\n    go s.streamMetrics(ctx)\n}\n\n// HandleWebSocket upgrades HTTP connections to WebSocket for dashboard streaming\nfunc (s *StreamingServer) HandleWebSocket(w http.ResponseWriter, r *http.Request) {\n    conn, err := s.upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        log.Printf(\"WebSocket upgrade failed: %v\", err)\n        return\n    }\n    \n    client := &Client{\n        conn:     conn,\n        send:     make(chan []byte, 256),\n        filters:  make(map[string]bool),\n        lastSeen: time.Now(),\n    }\n    \n    s.register <- client\n    \n    // Handle client messages in separate goroutines\n    go s.readFromClient(client)\n    go s.writeToClient(client)\n}\n\nfunc (s *StreamingServer) handleClients() {\n    for {\n        select {\n        case client := <-s.register:\n            s.clientsMu.Lock()\n            s.clients[client] = true\n            s.clientsMu.Unlock()\n            \n            // Send initial dashboard state\n            s.sendInitialState(client)\n            \n        case client := <-s.unregister:\n            s.clientsMu.Lock()\n            if _, ok := s.clients[client]; ok {\n                delete(s.clients, client)\n                close(client.send)\n            }\n            s.clientsMu.Unlock()\n            \n        case message := <-s.broadcast:\n            s.clientsMu.RLock()\n            for client := range s.clients {\n                select {\n                case client.send <- message:\n                default:\n                    // Client send channel is full, disconnect\n                    delete(s.clients, client)\n                    close(client.send)\n                }\n            }\n            s.clientsMu.RUnlock()\n        }\n    }\n}\n\nfunc (s *StreamingServer) streamMetrics(ctx context.Context) {\n    ticker := time.NewTicker(2 * time.Second)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case <-ctx.Done():\n            return\n            \n        case <-ticker.C:\n            // Collect current metrics batch\n            batch := s.collector.ExportMetrics()\n            if batch == nil {\n                continue\n            }\n            \n            // Send different update types based on metric changes\n            s.broadcastQuotaUpdates(batch)\n            s.broadcastTierStatus(batch)\n            s.broadcastHotKeys(batch)\n            s.broadcastSystemHealth(batch)\n        }\n    }\n}\n\nfunc (s *StreamingServer) broadcastQuotaUpdates(batch *MetricsBatch) {\n    for ruleID, metrics := range batch.RequestMetrics {\n        update := MetricUpdate{\n            Type:      \"quota_update\",\n            Timestamp: batch.Timestamp,\n            Data: map[string]interface{}{\n                \"rule_id\":   ruleID,\n                \"current\":   metrics.AllowedRequests,\n                \"total\":     metrics.TotalRequests,\n                \"denied\":    metrics.DeniedRequests,\n                \"last_seen\": metrics.LastSeen,\n            },\n        }\n        \n        s.broadcastUpdate(&update)\n    }\n}\n\nfunc (s *StreamingServer) broadcastTierStatus(batch *MetricsBatch) {\n    // Aggregate metrics by tier for tier status updates\n    tierStats := make(map[string]map[string]interface{})\n    \n    for ruleID, metrics := range batch.RequestMetrics {\n        // Extract tier from rule ID (assuming format like \"user:tier_name\")\n        tier := extractTierFromRuleID(ruleID)\n        \n        if tierStats[tier] == nil {\n            tierStats[tier] = map[string]interface{}{\n                \"total_requests\": int64(0),\n                \"active_rules\":   0,\n                \"avg_usage\":      0.0,\n            }\n        }\n        \n        tierStats[tier][\"total_requests\"] = tierStats[tier][\"total_requests\"].(int64) + metrics.TotalRequests\n        tierStats[tier][\"active_rules\"] = tierStats[tier][\"active_rules\"].(int) + 1\n    }\n    \n    for tierName, stats := range tierStats {\n        update := MetricUpdate{\n            Type:      \"tier_status\",\n            Timestamp: batch.Timestamp,\n            Data: map[string]interface{}{\n                \"tier_name\":      tierName,\n                \"total_requests\": stats[\"total_requests\"],\n                \"active_rules\":   stats[\"active_rules\"],\n            },\n        }\n        \n        s.broadcastUpdate(&update)\n    }\n}\n\nfunc (s *StreamingServer) broadcastHotKeys(batch *MetricsBatch) {\n    update := MetricUpdate{\n        Type:      \"hot_keys\",\n        Timestamp: batch.Timestamp,\n        Data: map[string]interface{}{\n            \"hot_keys\": batch.HotKeys,\n        },\n    }\n    \n    s.broadcastUpdate(&update)\n}\n\nfunc (s *StreamingServer) broadcastSystemHealth(batch *MetricsBatch) {\n    update := MetricUpdate{\n        Type:      \"system_health\",\n        Timestamp: batch.Timestamp,\n        Data: map[string]interface{}{\n            \"redis_healthy\":    batch.SystemHealth != nil,\n            \"performance\":      batch.PerformanceMetrics,\n            \"error_rate\":       calculateErrorRate(batch),\n        },\n    }\n    \n    s.broadcastUpdate(&update)\n}\n\nfunc (s *StreamingServer) broadcastUpdate(update *MetricUpdate) {\n    data, err := json.Marshal(update)\n    if err != nil {\n        log.Printf(\"Failed to marshal metric update: %v\", err)\n        return\n    }\n    \n    select {\n    case s.broadcast <- data:\n    default:\n        // Broadcast channel is full, drop update\n        log.Printf(\"Dropped metric update - broadcast channel full\")\n    }\n}\n\nfunc (s *StreamingServer) readFromClient(client *Client) {\n    defer func() {\n        s.unregister <- client\n        client.conn.Close()\n    }()\n    \n    client.conn.SetReadDeadline(time.Now().Add(60 * time.Second))\n    client.conn.SetPongHandler(func(string) error {\n        client.conn.SetReadDeadline(time.Now().Add(60 * time.Second))\n        return nil\n    })\n    \n    for {\n        var msg map[string]interface{}\n        err := client.conn.ReadJSON(&msg)\n        if err != nil {\n            if websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway, websocket.CloseAbnormalClosure) {\n                log.Printf(\"WebSocket error: %v\", err)\n            }\n            break\n        }\n        \n        // Handle client filter updates\n        if msgType, ok := msg[\"type\"].(string); ok && msgType == \"set_filters\" {\n            if filters, ok := msg[\"filters\"].(map[string]interface{}); ok {\n                client.filters = make(map[string]bool)\n                for filter, enabled := range filters {\n                    if enabledBool, ok := enabled.(bool); ok {\n                        client.filters[filter] = enabledBool\n                    }\n                }\n            }\n        }\n        \n        client.lastSeen = time.Now()\n    }\n}\n\nfunc (s *StreamingServer) writeToClient(client *Client) {\n    ticker := time.NewTicker(54 * time.Second)\n    defer func() {\n        ticker.Stop()\n        client.conn.Close()\n    }()\n    \n    for {\n        select {\n        case message, ok := <-client.send:\n            client.conn.SetWriteDeadline(time.Now().Add(10 * time.Second))\n            if !ok {\n                client.conn.WriteMessage(websocket.CloseMessage, []byte{})\n                return\n            }\n            \n            if err := client.conn.WriteMessage(websocket.TextMessage, message); err != nil {\n                log.Printf(\"WebSocket write error: %v\", err)\n                return\n            }\n            \n        case <-ticker.C:\n            client.conn.SetWriteDeadline(time.Now().Add(10 * time.Second))\n            if err := client.conn.WriteMessage(websocket.PingMessage, nil); err != nil {\n                return\n            }\n        }\n    }\n}\n\nfunc (s *StreamingServer) sendInitialState(client *Client) {\n    // Send current dashboard state to newly connected client\n    batch := s.collector.ExportMetrics()\n    if batch != nil {\n        s.broadcastQuotaUpdates(batch)\n        s.broadcastTierStatus(batch)\n        s.broadcastSystemHealth(batch)\n    }\n}\n\n// Helper functions\nfunc extractTierFromRuleID(ruleID string) string {\n    // Extract tier name from rule ID format\n    // This would depend on your rule ID naming convention\n    if len(ruleID) > 0 && ruleID[0] == 'u' {\n        return \"user\"\n    } else if len(ruleID) > 0 && ruleID[0] == 'i' {\n        return \"ip\"\n    } else if len(ruleID) > 0 && ruleID[0] == 'a' {\n        return \"api\"\n    }\n    return \"global\"\n}\n\nfunc calculateErrorRate(batch *MetricsBatch) float64 {\n    if batch.PerformanceMetrics == nil {\n        return 0.0\n    }\n    \n    total := int64(0)\n    errors := int64(0)\n    \n    for _, metrics := range batch.RequestMetrics {\n        total += metrics.TotalRequests\n        errors += metrics.ErrorRequests\n    }\n    \n    if total == 0 {\n        return 0.0\n    }\n    \n    return float64(errors) / float64(total)\n}\n```\n\n#### Core Logic Skeleton\n\n**Rule Management Handler (api/handlers/rules.go)**:\n\n```go\npackage handlers\n\nimport (\n    \"encoding/json\"\n    \"net/http\"\n    \"strconv\"\n    \"time\"\n    \n    \"github.com/gorilla/mux\"\n)\n\ntype RuleHandler struct {\n    ruleManager *config.RuleManager\n    validator   *validation.RuleValidator\n    auditor     *config.AuditLogger\n}\n\n// CreateRule handles POST /api/v1/rules\nfunc (h *RuleHandler) CreateRule(w http.ResponseWriter, r *http.Request) {\n    var rule RateLimitRule\n    \n    // TODO 1: Decode JSON request body into RateLimitRule struct\n    // TODO 2: Generate unique ID for new rule (use UUID or timestamp-based)\n    // TODO 3: Validate rule using h.validator.ValidateRule()\n    // TODO 4: Check for conflicting rules with same key_pattern and priority\n    // TODO 5: Set rule creation metadata (created_at, version, etc.)\n    // TODO 6: Save rule using h.ruleManager.SaveRule()\n    // TODO 7: Log creation event using h.auditor.LogRuleCreation()\n    // TODO 8: Trigger configuration propagation to all instances\n    // TODO 9: Return created rule with 201 status code\n    // Hint: Use json.NewDecoder(r.Body).Decode() for request parsing\n}\n\n// UpdateRule handles PUT /api/v1/rules/{id}\nfunc (h *RuleHandler) UpdateRule(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract rule ID from URL path using mux.Vars(r)[\"id\"]\n    // TODO 2: Load existing rule to preserve version history\n    // TODO 3: Decode JSON request body into updated RateLimitRule\n    // TODO 4: Validate updated rule ensuring operational safety\n    // TODO 5: Create new version entry preserving old configuration\n    // TODO 6: Update rule with incremented version number\n    // TODO 7: Save updated rule and publish change notification\n    // TODO 8: Log update event with before/after values\n    // TODO 9: Return updated rule configuration\n    // Hint: Version number should increment from existing rule.version\n}\n\n// DeleteRule handles DELETE /api/v1/rules/{id}\nfunc (h *RuleHandler) DeleteRule(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract rule ID from URL path\n    // TODO 2: Load existing rule to ensure it exists\n    // TODO 3: Check if rule can be safely deleted (no active dependencies)\n    // TODO 4: Mark rule as deleted rather than removing (soft delete)\n    // TODO 5: Clear all active rate limit counters for this rule\n    // TODO 6: Publish rule deletion notification to all instances\n    // TODO 7: Log deletion event with rule configuration\n    // TODO 8: Return deletion confirmation\n    // Hint: Soft delete preserves audit history while stopping enforcement\n}\n\n// PreviewRule handles POST /api/v1/rules/{id}/preview\nfunc (h *RuleHandler) PreviewRule(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract rule ID and load rule configuration\n    // TODO 2: Decode array of RateLimitRequest from request body\n    // TODO 3: For each test request, simulate rate limiting without updating counters\n    // TODO 4: Use rate limiter Preview() method to get theoretical results\n    // TODO 5: Collect timing information for performance analysis\n    // TODO 6: Return array of preview results with timing data\n    // TODO 7: Log preview operation for audit purposes\n    // Hint: Preview should never modify actual rate limit state\n}\n\n// ResetRule handles POST /api/v1/rules/{id}/reset\nfunc (h *RuleHandler) ResetRule(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract rule ID and validate rule exists\n    // TODO 2: Parse optional key filter from request body\n    // TODO 3: Identify all Redis keys associated with this rule\n    // TODO 4: Clear counters using rate limiter Reset() method\n    // TODO 5: Handle partial failures when some keys cannot be reset\n    // TODO 6: Log reset operation with affected key count\n    // TODO 7: Return reset confirmation with operation summary\n    // Hint: Key filter allows resetting specific users/IPs rather than all keys\n}\n\n// ListRules handles GET /api/v1/rules\nfunc (h *RuleHandler) ListRules(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Parse query parameters for filtering and pagination\n    // TODO 2: Extract filters (enabled, algorithm, priority range)\n    // TODO 3: Load rules matching filter criteria\n    // TODO 4: Apply sorting based on query parameters\n    // TODO 5: Implement pagination using limit/offset or cursor\n    // TODO 6: Calculate total count for pagination metadata\n    // TODO 7: Return paginated rule list with metadata\n    // Hint: Support filters like ?enabled=true&algorithm=token_bucket&priority_min=50\n}\n\n// GetRuleVersions handles GET /api/v1/rules/{id}/versions\nfunc (h *RuleHandler) GetRuleVersions(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract rule ID from URL path\n    // TODO 2: Load all historical versions of the rule\n    // TODO 3: Sort versions by timestamp (newest first)\n    // TODO 4: Include version metadata (created_by, created_at, change_reason)\n    // TODO 5: Apply pagination if version history is large\n    // TODO 6: Return version list with change summaries\n    // Hint: Each version should show what changed compared to previous version\n}\n\n// RollbackRule handles POST /api/v1/rules/{id}/rollback\nfunc (h *RuleHandler) RollbackRule(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract rule ID and target version number from request\n    // TODO 2: Load target version configuration from history\n    // TODO 3: Validate that rollback target exists and is valid\n    // TODO 4: Create new version entry pointing to rolled-back configuration\n    // TODO 5: Activate rolled-back configuration as current version\n    // TODO 6: Clear any rate limit state that may be invalid after rollback\n    // TODO 7: Publish configuration change notification\n    // TODO 8: Log rollback operation with justification\n    // TODO 9: Return confirmation with new current version\n    // Hint: Rollback creates a new version rather than deleting recent versions\n}\n```\n\n**Rate Limit Header Injection (api/middleware/headers.go)**:\n\n```go\npackage middleware\n\nimport (\n    \"fmt\"\n    \"net/http\"\n    \"strconv\"\n    \"time\"\n)\n\n// RateLimitHeaders middleware injects standard rate limiting headers\ntype RateLimitHeaders struct {\n    limiter FlowCoordinator\n}\n\n// NewRateLimitHeaders creates header injection middleware\nfunc NewRateLimitHeaders(limiter FlowCoordinator) *RateLimitHeaders {\n    return &RateLimitHeaders{limiter: limiter}\n}\n\n// Middleware returns HTTP middleware that adds rate limit headers\nfunc (rlh *RateLimitHeaders) Middleware() func(http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            // TODO 1: Extract request context (user_id, ip_address, api_endpoint)\n            // TODO 2: Build RateLimitRequest from HTTP request data\n            // TODO 3: Call limiter.Preview() to get current quota status without updating\n            // TODO 4: Extract most restrictive limit from multi-tier results\n            // TODO 5: Calculate standard rate limit headers (X-RateLimit-*)\n            // TODO 6: Add extended headers for enhanced client experience\n            // TODO 7: Handle multi-tier header aggregation following \"most restrictive wins\"\n            // TODO 8: Add retry-after calculation for approaching limits\n            // TODO 9: Set headers on response before calling next handler\n            // Hint: Preview() gives quota info without consuming requests\n            \n            next.ServeHTTP(w, r)\n        })\n    }\n}\n\n// extractRequestContext builds rate limit context from HTTP request\nfunc (rlh *RateLimitHeaders) extractRequestContext(r *http.Request) *RequestContext {\n    // TODO 1: Extract user ID from authentication headers or JWT token\n    // TODO 2: Get client IP address handling proxy headers (X-Forwarded-For)\n    // TODO 3: Determine API endpoint from request path\n    // TODO 4: Extract user agent and other relevant headers\n    // TODO 5: Set request timestamp for consistent time-based calculations\n    // TODO 6: Return populated RequestContext\n    // Hint: Check X-User-ID header, Authorization header, and X-Forwarded-For\n}\n\n// calculateHeaders determines rate limit headers from flow result\nfunc (rlh *RateLimitHeaders) calculateHeaders(result *FlowResult) map[string]string {\n    // TODO 1: Find most restrictive tier from result.TierResults\n    // TODO 2: Extract limit, remaining, and reset time from restrictive tier\n    // TODO 3: Calculate retry-after for rate-limited requests\n    // TODO 4: Build standard headers map with X-RateLimit-* entries\n    // TODO 5: Add extended headers for algorithm and scope information\n    // TODO 6: Handle edge case where no tiers were evaluated\n    // TODO 7: Return complete headers map\n    // Hint: Most restrictive = lowest remaining/limit ratio\n}\n\n// findMostRestrictiveTier identifies tier with tightest remaining quota\nfunc (rlh *RateLimitHeaders) findMostRestrictiveTier(tiers []*TierEvaluation) *TierEvaluation {\n    // TODO 1: Initialize with first tier if available\n    // TODO 2: Iterate through all tier results\n    // TODO 3: Calculate remaining/limit ratio for each tier\n    // TODO 4: Track tier with lowest ratio (most restrictive)\n    // TODO 5: Handle ties by preferring higher priority tiers\n    // TODO 6: Return most restrictive tier evaluation\n    // Hint: Ratio of 0.1 (10% remaining) is more restrictive than 0.8 (80% remaining)\n}\n```\n\n#### Milestone Checkpoints\n\n**After API Implementation:**\n- Start management API server: `go run cmd/management-api/main.go`\n- Create test rule: `curl -X POST http://localhost:8080/api/v1/rules -d '{\"name\":\"test\",\"key_pattern\":\"user:*\",\"algorithm\":\"token_bucket\",\"limit\":100,\"window\":\"1m\",\"burst_limit\":150}'`\n- Expected: Rule created with generated ID and version 1\n- Verify: Rule appears in Redis and triggers configuration change notification\n\n**After Dashboard Implementation:**\n- Start dashboard server: `go run cmd/dashboard/main.go`  \n- Open browser to `http://localhost:3000`\n- Expected: Real-time dashboard showing quota utilization and system health\n- Generate load: Run rate limiting requests and observe dashboard updates\n- Verify: WebSocket connection established and metrics update every 2 seconds\n\n**After Self-Rate-Limiting Implementation:**\n- Test API protection: Send 200 rapid requests to management API\n- Expected: First 100 requests succeed, subsequent requests receive 429 status\n- Check headers: Verify X-RateLimit-* headers present in all responses\n- Test emergency override: Use admin API key to bypass normal limits\n- Verify: Override allows higher request rates with audit log entries\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Dashboard shows no data | WebSocket connection failing | Check browser dev tools for WebSocket errors | Verify CORS settings and WebSocket endpoint |\n| Headers show wrong quota | Multi-tier aggregation error | Log tier results before header calculation | Fix most-restrictive-tier selection logic |\n| Config changes not propagating | Redis pub/sub subscription broken | Check Redis logs for subscription errors | Reconnect to Redis and restart subscription |\n| API rate limiting too aggressive | Token bucket refill rate too low | Check API rate limit configuration | Increase refill rate or burst capacity |\n| Dashboard updates too slow | Metrics collection batching issues | Monitor metrics collector export frequency | Reduce batching interval or increase update frequency |\n| Emergency override not working | Override token validation failing | Check API key validation and override logic | Verify override token format and expiration |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** Milestone 3 - Redis Backend Integration, Milestone 4 - Consistent Hashing & Sharding, and foundational concepts applying to all milestones\n\nThe robustness of a distributed rate limiting system fundamentally depends on how gracefully it handles failures, edge cases, and the inevitable inconsistencies that arise in distributed environments. Unlike monolithic applications where failures are typically binary (the application works or it doesn't), distributed systems must continue operating with partial functionality when components fail. This section examines the comprehensive failure scenarios, recovery strategies, and edge case handling that transform a basic distributed rate limiter into a production-ready system.\n\n### Mental Model: The Emergency Response Network\n\nThink of distributed rate limiting like a city's emergency response network. When you call 911, the system must route your call to the right dispatcher, coordinate with multiple emergency services, and ensure help arrives even if some communication towers are down. The emergency network has multiple levels of fallback: if the primary dispatch center fails, backup centers take over; if radio communication fails, they use cellular; if all else fails, emergency vehicles operate with their last known instructions.\n\nSimilarly, our distributed rate limiter must continue protecting your application even when Redis nodes fail, network partitions occur, or time synchronization drifts. The system degrades gracefully rather than failing completely, using local fallbacks when global coordination becomes impossible, just as emergency responders use local protocols when centralized coordination is unavailable.\n\n### Redis Failure Scenarios\n\nRedis serves as the central nervous system of our distributed rate limiting infrastructure. When Redis becomes unavailable or unreliable, the entire system must adapt its behavior to maintain protection while avoiding cascading failures. Understanding and preparing for Redis failure modes represents one of the most critical aspects of building resilient distributed rate limiting.\n\n#### Connection Pool Exhaustion and Recovery\n\nConnection pool exhaustion occurs when all available Redis connections become tied up in long-running operations, network timeouts, or blocked on slow Redis commands. This scenario often manifests gradually, with response times degrading before complete failure occurs.\n\nThe `RedisStorage` component implements intelligent connection pool management that monitors connection health and automatically recovers from exhaustion scenarios. When connection attempts begin timing out, the system tracks failure rates and response times to detect degradation early.\n\n| Failure Mode | Detection Method | Recovery Action | Fallback Strategy |\n|--------------|------------------|------------------|-------------------|\n| Pool exhaustion | Connection timeout increase | Close idle connections, expand pool temporarily | Switch to local limiting |\n| Slow Redis responses | Response time monitoring | Circuit breaker activation | Cache last known limits |\n| Network packet loss | Retry failure patterns | TCP connection reset | Degrade to approximate limiting |\n| Redis memory pressure | Error code analysis | Reduce TTL on keys | Local rate limiting only |\n\nThe `CircuitBreaker` component provides the primary mechanism for detecting and responding to Redis degradation. It tracks success/failure ratios and response times, automatically switching to local fallback when Redis becomes unreliable.\n\n```\nCircuit Breaker State Machine:\n- Closed (normal): All requests go to Redis, track failure rate\n- Open (failed): All requests use local fallback, periodic health checks\n- Half-Open (testing): Limited requests test Redis recovery\n```\n\n> **Decision: Circuit Breaker vs Retry Logic**\n> - **Context**: Need to handle Redis failures without overwhelming failed instances\n> - **Options Considered**: Simple retry with exponential backoff, circuit breaker pattern, combination approach\n> - **Decision**: Circuit breaker with limited retries for each state\n> - **Rationale**: Circuit breakers prevent thundering herd problems and give failed Redis instances time to recover without constant retry pressure\n> - **Consequences**: More complex state management but much better failure isolation and faster recovery detection\n\n#### Memory Pressure and Eviction Handling\n\nRedis memory pressure creates particularly challenging scenarios because it can cause unpredictable key eviction, leading to rate limiting state loss. When Redis approaches its memory limit, it begins evicting keys according to its eviction policy, which may remove active rate limiting counters.\n\nThe system implements several strategies to detect and handle memory-induced state loss. The `HealthChecker` component monitors Redis memory usage through the INFO command and adjusts rate limiting behavior when memory pressure is detected.\n\n| Memory Condition | Detection Threshold | Automatic Response | Operator Alert |\n|------------------|---------------------|---------------------|----------------|\n| High usage | > 80% of max memory | Reduce key TTLs by 50% | Warning notification |\n| Critical pressure | > 95% of max memory | Switch to local fallback | Critical alert |\n| Eviction detected | Keys disappearing unexpectedly | Reset all affected counters | Data loss warning |\n| Out of memory | Redis refuses writes | Full local fallback mode | Emergency alert |\n\nWhen eviction is detected, the system faces a critical decision: should it assume evicted rate limiting keys represent exhausted quotas (deny requests) or available quotas (allow requests)? The safest approach biases toward protection, treating evicted keys as exhausted unless explicitly configured otherwise.\n\n#### Split-Brain and Network Partition Handling\n\nNetwork partitions can split the Redis cluster into multiple segments, each believing it represents the authoritative state. During partitions, different application instances may connect to different Redis nodes, leading to independent rate limiting decisions that violate global limits.\n\n![Redis Failure Handling Flow](./diagrams/failure-handling-flow.svg)\n\nThe `HashRing` component implements partition detection by monitoring which nodes remain reachable from each application instance. When a partition is detected, the system must choose between availability and consistency.\n\n> **Key Design Insight**: During network partitions, we prioritize application availability over strict rate limiting accuracy. It's better to allow slightly more traffic than configured limits than to reject legitimate requests due to infrastructure failures.\n\nThe partition handling algorithm follows these decision steps:\n\n1. **Detect partition**: Monitor node reachability and cross-check with other application instances\n2. **Assess majority**: Determine which partition contains the majority of Redis nodes\n3. **Minority partition response**: Instances in minority partitions switch to local fallback\n4. **Majority partition response**: Continue distributed limiting with reduced cluster\n5. **Partition healing**: Gradually restore full distributed coordination as nodes reconnect\n\n| Partition Scenario | Instance Response | Rate Limiting Behavior | Recovery Action |\n|-------------------|-------------------|-------------------------|-----------------|\n| Isolated single node | Switch to local fallback | Per-instance limits only | Rejoin when reachable |\n| Minority partition | Degrade to local mode | Conservative local limits | Wait for majority contact |\n| Majority partition | Continue distributed | Adjust for lost capacity | Gradually restore full state |\n| Complete isolation | Full local fallback | Emergency rate limits | Manual intervention |\n\n#### Data Corruption and Inconsistency Detection\n\nRedis data corruption can occur due to hardware failures, software bugs, or operational errors. Rate limiting counters may become corrupted, leading to wildly incorrect values that either completely block traffic or allow unlimited requests.\n\nThe system implements data validation within Lua scripts to detect obviously corrupted values. Rate limiting counters that show impossible values (negative tokens, timestamps from the far future, counters exceeding configured limits by orders of magnitude) trigger automatic correction.\n\n```\nData Validation Rules:\n- Token bucket tokens must be between 0 and configured capacity\n- Sliding window timestamps must be within reasonable time bounds\n- Counter values must not exceed limit * time_window_multiplier\n- State update timestamps must be monotonically increasing\n```\n\n### Clock Skew and Time Synchronization\n\nTime represents the fundamental dimension for rate limiting, making clock synchronization critical for distributed coordination. When application instances and Redis nodes have different perceptions of current time, rate limiting windows become misaligned, leading to inaccurate quota tracking and unpredictable behavior.\n\n#### Detecting and Measuring Clock Skew\n\nClock skew detection requires comparing timestamps between distributed components while accounting for network latency. The `TimeProvider` component implements active clock skew measurement by periodically synchronizing with Redis server time and calculating drift.\n\nThe detection algorithm works by:\n\n1. **Record local timestamp** before Redis operation\n2. **Execute Redis TIME command** to get server timestamp  \n3. **Record local timestamp** after Redis response\n4. **Calculate network latency** as (local_after - local_before) / 2\n5. **Determine skew** as redis_time - local_time - estimated_latency\n\n| Skew Magnitude | Impact | Detection Method | Mitigation Strategy |\n|----------------|---------|------------------|---------------------|\n| < 100ms | Negligible | Passive monitoring | No action required |\n| 100ms - 1s | Boundary inaccuracy | Active measurement | Use Redis timestamps |\n| 1s - 10s | Window misalignment | Continuous tracking | Force Redis time sync |\n| > 10s | Severe inaccuracy | Alert generation | Manual time sync required |\n\nWhen significant skew is detected, the system can choose between several timestamp strategies. Using Redis server time ensures consistency but adds latency to every operation. Using local time with skew correction provides better performance but introduces complexity.\n\n> **Decision: Redis Server Time vs Local Time Correction**\n> - **Context**: Need consistent timestamps across distributed rate limiters with varying clock skew\n> - **Options Considered**: Always use Redis TIME, local time with measured correction, hybrid approach based on skew magnitude\n> - **Decision**: Hybrid approach - use local time when skew < 500ms, Redis time when skew > 500ms\n> - **Rationale**: Balances accuracy requirements with performance impact; most systems have reasonable time sync\n> - **Consequences**: Requires skew monitoring overhead but provides optimal performance/accuracy trade-off\n\n#### Window Boundary Synchronization\n\nSliding window algorithms become particularly sensitive to clock skew at window boundaries. When different instances calculate window boundaries using different timestamps, the same request may fall into different time buckets, causing counter fragmentation and inaccurate limits.\n\nThe `SlidingWindowCounter` implementation addresses boundary synchronization by quantizing all timestamps to common boundary points. Instead of using precise timestamps, the system rounds timestamps to the nearest window subdivision.\n\n```\nBoundary Synchronization Algorithm:\n1. Define window_size (e.g., 60 seconds) and bucket_count (e.g., 6 buckets)\n2. Calculate bucket_duration = window_size / bucket_count (10 seconds)\n3. Quantize timestamp: bucket_start = (timestamp / bucket_duration) * bucket_duration\n4. All instances use identical bucket_start times regardless of local clock skew\n```\n\nThis quantization approach ensures that all distributed instances agree on bucket boundaries, even with moderate clock skew. The trade-off is slightly less precise timestamp handling in exchange for much better distributed consistency.\n\n#### Handling Timestamp Rollback\n\nTimestamp rollback occurs when system administrators correct clock synchronization, causing time to appear to move backward. Rate limiting algorithms that depend on monotonically increasing timestamps can behave unpredictably when timestamps decrease.\n\nThe `TokenBucket` algorithm handles rollback by detecting backward time movement and choosing appropriate responses:\n\n| Rollback Magnitude | Detection Method | Response Strategy | State Adjustment |\n|-------------------|------------------|-------------------|------------------|\n| < 1 second | Compare with last_refill_time | Ignore update, use cached time | No state change |\n| 1-60 seconds | Timestamp decrease detection | Reset refill calculations | Clear accumulated tokens |\n| > 60 seconds | Large backward jump | Full state reset | Start with empty bucket |\n| Clock sync correction | NTP adjustment patterns | Gradual time correction | Smooth state transition |\n\nFor sliding window algorithms, timestamp rollback requires careful handling to avoid double-counting requests. The system maintains a grace period where recent timestamps remain valid even if new timestamps appear earlier.\n\n⚠️ **Pitfall: Ignoring Timestamp Rollback**\nMany implementations assume timestamps always increase, leading to broken rate limiting when system clocks are corrected. Always check for backward time movement and have a explicit strategy for handling it.\n\n### Race Condition Prevention\n\nDistributed rate limiting inherently involves race conditions where multiple instances simultaneously check and update shared counters. Without proper coordination, these race conditions can lead to double-counting, lost updates, or quota violations that allow traffic beyond configured limits.\n\n#### Atomic Operations with Lua Scripts\n\nRedis Lua scripts provide the foundation for atomic rate limiting operations by ensuring that check-and-update sequences execute without interruption. The `ExecuteLua` method encapsulates complex rate limiting logic in atomic scripts that eliminate race conditions.\n\nThe token bucket Lua script demonstrates the atomic operation pattern:\n\n```\nToken Bucket Lua Script Logic:\n1. GET current token count and last refill timestamp\n2. Calculate tokens to add based on elapsed time and refill rate\n3. Update token count (capped at bucket capacity)\n4. IF requested tokens <= available tokens THEN\n   5. Subtract requested tokens\n   6. SET new token count and timestamp\n   7. RETURN success with remaining tokens\n8. ELSE\n   9. SET updated token count (with refill but no consumption)\n   10. RETURN failure with retry delay\n```\n\nAll state reads, calculations, and writes occur atomically within the single Lua script execution. This eliminates the race condition window that would exist with separate GET, calculate, and SET operations.\n\n| Race Condition Type | Without Lua Scripts | With Lua Scripts | Prevention Method |\n|---------------------|---------------------|------------------|-------------------|\n| Read-modify-write | Multiple updates lost | Single atomic update | Lua script atomicity |\n| Check-then-act | State changes between check/act | Atomic check-and-act | Single script execution |\n| Counter increment | Lost increment operations | Atomic counter update | Lua INCR operations |\n| Timestamp comparison | Inconsistent time reads | Single timestamp read | Lua script locality |\n\n#### Optimistic vs Pessimistic Concurrency\n\nThe system implements optimistic concurrency control, assuming that conflicts are rare and handling them through retry mechanisms rather than preventing them with locks. This approach provides better performance under normal conditions while gracefully handling conflicts when they occur.\n\nOptimistic concurrency in rate limiting works by:\n\n1. **Read current state** without acquiring locks\n2. **Calculate new state** based on read values\n3. **Attempt atomic update** with conditional logic\n4. **Retry on conflict** if state changed during calculation\n5. **Apply exponential backoff** to prevent retry storms\n\nThe `DistributedLimiter` component implements retry logic with jitter to prevent thundering herd problems when many instances simultaneously retry failed operations.\n\n> **Decision: Optimistic vs Pessimistic Concurrency**\n> - **Context**: Need to handle concurrent rate limit checks across many application instances\n> - **Options Considered**: Redis locks (pessimistic), optimistic retry with Lua scripts, hybrid locking for hot keys\n> - **Decision**: Optimistic concurrency with exponential backoff retry\n> - **Rationale**: Rate limiting operations are fast and conflicts are relatively rare; pessimistic locking adds significant overhead and can create deadlock risks\n> - **Consequences**: Better performance under normal load but requires careful retry logic and backoff strategies\n\n#### Hot Key Conflict Resolution\n\nHot keys create concentrated conflict points where many instances simultaneously attempt to update the same rate limiting counters. The `HotKeyDetector` identifies these high-contention keys and applies specialized conflict resolution strategies.\n\nWhen hot keys are detected, the system can apply several conflict reduction techniques:\n\n| Hot Key Strategy | Mechanism | Advantages | Disadvantages |\n|------------------|-----------|------------|---------------|\n| Key sharding | Split single key into multiple sub-keys | Reduces contention | Complex aggregation logic |\n| Local caching | Cache counter values briefly | Fewer Redis operations | Potential accuracy loss |\n| Retry backoff | Exponential backoff with jitter | Reduces retry storms | Higher latency for some requests |\n| Priority queuing | Process requests by priority | Important requests succeed | Complex queue management |\n\nThe key sharding approach proves most effective for extremely hot keys, where a single rate limit key is split into multiple sub-keys and requests are randomly distributed across them. The trade-off is more complex Lua scripts that must aggregate across sub-keys to check total usage.\n\n#### Preventing Lost Updates in Sliding Windows\n\nSliding window algorithms face particular challenges with concurrent updates because multiple bucket updates may occur simultaneously as time progresses. The `SlidingWindowLog` implementation prevents lost updates by using Redis list operations that are inherently atomic.\n\nThe atomic sliding window update sequence:\n\n1. **LPUSH new timestamp** to the request log list\n2. **ZREMRANGEBYSCORE** to remove timestamps outside current window\n3. **ZCARD** to count remaining timestamps in window\n4. **Compare count against limit** and return result\n\nThis sequence uses Redis's atomic list and sorted set operations to ensure that concurrent updates don't interfere with each other. Each step is atomic at the Redis level, and the overall sequence runs within a Lua script for complete atomicity.\n\n⚠️ **Pitfall: Non-Atomic Window Updates**\nAttempting to implement sliding windows with separate GET, filter, and SET operations creates race conditions where concurrent requests can cause lost updates or double-counting. Always use atomic operations for the complete window update sequence.\n\n### Implementation Guidance\n\nThis section provides practical implementation patterns for robust error handling and edge case management in distributed rate limiting systems.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Time synchronization | NTP client with periodic sync | Hardware-assisted clock sync |\n| Circuit breaker | Simple state machine | Hystrix-style metrics-based breaker |\n| Health checking | Basic ping/pong | Comprehensive Redis INFO monitoring |\n| Clock skew detection | Periodic Redis TIME calls | Continuous drift measurement |\n| Retry logic | Exponential backoff | Adaptive backoff with jitter |\n\n#### Recommended Module Structure\n\n```\ninternal/resilience/\n  circuit_breaker.go          ← Circuit breaker implementation\n  circuit_breaker_test.go     ← Circuit breaker unit tests\n  health_checker.go           ← Redis health monitoring\n  time_provider.go            ← Clock skew detection and sync\n  retry_manager.go            ← Retry logic with backoff\n  fallback_limiter.go         ← Local fallback rate limiting\ninternal/redis/\n  failure_detector.go         ← Redis failure detection\n  connection_pool.go          ← Connection pool management\n  lua_scripts.go              ← Atomic Lua script execution\n```\n\n#### Circuit Breaker Infrastructure\n\n```go\npackage resilience\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// CircuitState represents the current state of the circuit breaker\ntype CircuitState int\n\nconst (\n    CircuitClosed CircuitState = iota    // Normal operation\n    CircuitOpen                          // Rejecting all requests\n    CircuitHalfOpen                     // Testing recovery\n)\n\n// CircuitBreaker implements failure detection and graceful degradation\ntype CircuitBreaker struct {\n    mu                sync.RWMutex\n    state             CircuitState\n    failureCount      int\n    lastFailureTime   time.Time\n    nextRetryTime     time.Time\n    failureThreshold  int\n    recoveryTimeout   time.Duration\n}\n\n// NewCircuitBreaker creates a circuit breaker with the specified configuration\nfunc NewCircuitBreaker(failureThreshold int, recoveryTimeout time.Duration) *CircuitBreaker {\n    return &CircuitBreaker{\n        state:            CircuitClosed,\n        failureThreshold: failureThreshold,\n        recoveryTimeout:  recoveryTimeout,\n    }\n}\n\n// Execute runs the provided function with circuit breaker protection\nfunc (cb *CircuitBreaker) Execute(ctx context.Context, fn func() error) error {\n    // TODO 1: Check current circuit state and decide whether to allow execution\n    // TODO 2: If circuit is Open and recovery timeout hasn't elapsed, return error immediately\n    // TODO 3: If circuit is Open and recovery timeout has elapsed, transition to HalfOpen\n    // TODO 4: Execute the function and capture any error\n    // TODO 5: Update circuit state based on execution result (success/failure)\n    // TODO 6: If in HalfOpen state, transition to Closed on success or Open on failure\n    // TODO 7: If in Closed state, increment failure count on error and open if threshold exceeded\n    // Hint: Use atomic operations or mutex for thread safety\n    // Hint: Track both failure count and failure rate over time windows\n    panic(\"implement circuit breaker execution logic\")\n}\n```\n\n#### Health Checker Implementation\n\n```go\npackage resilience\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n    \"github.com/redis/go-redis/v9\"\n)\n\n// NodeHealth represents the health status of a single Redis node\ntype NodeHealth struct {\n    NodeID              string\n    Address             string\n    LastSeen           time.Time\n    ConsecutiveFailures int\n    AverageLatency     time.Duration\n    IsHealthy          bool\n    MemoryUsage        float64\n    ConnectionCount    int\n}\n\n// HealthConfig configures health checking behavior\ntype HealthConfig struct {\n    CheckInterval      time.Duration\n    FailureThreshold   int\n    RecoveryThreshold  int\n    LatencyThreshold   time.Duration\n    MemoryThreshold    float64\n}\n\n// HealthChecker monitors Redis cluster health and triggers failover actions\ntype HealthChecker struct {\n    mu              sync.RWMutex\n    nodes           map[string]*NodeHealth\n    circuitBreakers map[string]*CircuitBreaker\n    config          HealthConfig\n}\n\n// CheckNodeHealth performs comprehensive health check on a Redis node\nfunc (hc *HealthChecker) CheckNodeHealth(ctx context.Context, nodeID string) (*NodeHealth, error) {\n    // TODO 1: Create Redis client for the specified node\n    // TODO 2: Execute PING command and measure response time\n    // TODO 3: Execute INFO command to get memory usage and connection count\n    // TODO 4: Parse INFO response to extract memory_used, memory_total, connected_clients\n    // TODO 5: Calculate memory usage percentage and compare against threshold\n    // TODO 6: Update NodeHealth struct with current metrics\n    // TODO 7: Determine overall health status based on latency, memory, and recent failures\n    // TODO 8: Update consecutive failure count based on current check result\n    // Hint: Use context timeout to avoid hanging on unresponsive nodes\n    // Hint: Parse INFO command output to extract specific metrics\n    panic(\"implement comprehensive Redis node health checking\")\n}\n\n// HandleNodeFailure coordinates response to a detected node failure\nfunc (hc *HealthChecker) HandleNodeFailure(nodeID string) error {\n    // TODO 1: Mark node as unhealthy in the health map\n    // TODO 2: Trigger circuit breaker for this node to prevent further requests\n    // TODO 3: Notify hash ring to remove node from active rotation\n    // TODO 4: Log failure event with detailed diagnostic information\n    // TODO 5: Schedule accelerated health checks to detect recovery\n    // TODO 6: Update metrics to track node failure for monitoring dashboard\n    // Hint: Coordinate with HashRing component to redistribute load\n    // Hint: Implement exponential backoff for recovery health checks\n    panic(\"implement node failure response coordination\")\n}\n```\n\n#### Time Synchronization and Clock Skew Detection\n\n```go\npackage resilience\n\nimport (\n    \"context\"\n    \"math\"\n    \"sync\"\n    \"time\"\n    \"github.com/redis/go-redis/v9\"\n)\n\n// TimeProvider handles clock synchronization and skew detection\ntype TimeProvider struct {\n    mu            sync.RWMutex\n    redisClient   redis.UniversalClient\n    clockSkew     time.Duration\n    lastSync      time.Time\n    syncInterval  time.Duration\n}\n\n// NewTimeProvider creates a time provider with Redis synchronization\nfunc NewTimeProvider(redisClient redis.UniversalClient, syncInterval time.Duration) *TimeProvider {\n    return &TimeProvider{\n        redisClient:  redisClient,\n        syncInterval: syncInterval,\n    }\n}\n\n// Now returns the current time adjusted for measured clock skew\nfunc (tp *TimeProvider) Now() time.Time {\n    // TODO 1: Get current local time\n    // TODO 2: Check if clock skew measurement is recent enough (within sync interval)\n    // TODO 3: If skew measurement is stale, trigger background sync (don't block)\n    // TODO 4: Apply measured clock skew correction to local time\n    // TODO 5: Return corrected timestamp\n    // Hint: Don't block on sync - return local time if Redis is unavailable\n    // Hint: Use atomic operations for reading clockSkew to avoid races\n    panic(\"implement skew-corrected timestamp generation\")\n}\n\n// MeasureClockSkew compares local time with Redis server time to detect skew\nfunc (tp *TimeProvider) MeasureClockSkew(ctx context.Context) (time.Duration, error) {\n    // TODO 1: Record local timestamp before Redis call\n    // TODO 2: Execute Redis TIME command to get server timestamp  \n    // TODO 3: Record local timestamp after Redis response\n    // TODO 4: Parse Redis TIME response (seconds, microseconds)\n    // TODO 5: Calculate network latency as (local_after - local_before) / 2\n    // TODO 6: Calculate clock skew as redis_time - local_time - estimated_latency\n    // TODO 7: Update internal skew tracking with new measurement\n    // TODO 8: Return measured skew value\n    // Hint: Redis TIME returns [seconds, microseconds] array\n    // Hint: Account for network latency in skew calculation\n    panic(\"implement clock skew measurement against Redis server time\")\n}\n```\n\n#### Local Fallback Rate Limiter\n\n```go\npackage resilience\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// FallbackLimiter provides local rate limiting when Redis is unavailable\ntype FallbackLimiter struct {\n    mu              sync.RWMutex\n    tokenBuckets    map[string]*TokenBucketState\n    windowCounters  map[string]*WindowCounterState\n    defaultConfig   *TokenBucketConfig\n    cleanupInterval time.Duration\n}\n\n// TokenBucketState tracks local token bucket state\ntype TokenBucketState struct {\n    tokens          int64\n    lastRefillTime  int64\n    capacity        int64\n    refillRate      int64\n}\n\n// WindowCounterState tracks local sliding window state\ntype WindowCounterState struct {\n    requests    []time.Time\n    windowSize  time.Duration\n    limit       int64\n}\n\n// NewFallbackLimiter creates a local fallback rate limiter\nfunc NewFallbackLimiter(defaultConfig *TokenBucketConfig) *FallbackLimiter {\n    return &FallbackLimiter{\n        tokenBuckets:    make(map[string]*TokenBucketState),\n        windowCounters:  make(map[string]*WindowCounterState),\n        defaultConfig:   defaultConfig,\n        cleanupInterval: time.Minute * 5,\n    }\n}\n\n// CheckLocal performs rate limiting using local state only\nfunc (fl *FallbackLimiter) CheckLocal(ctx context.Context, key string, tokens int64, rule *RateLimitRule) (*RateLimitResult, error) {\n    // TODO 1: Determine which algorithm to use based on rule configuration\n    // TODO 2: Get or create local state for the specified key\n    // TODO 3: Apply token bucket algorithm using local timestamps\n    // TODO 4: Calculate refill tokens based on elapsed time since last access\n    // TODO 5: Check if requested tokens are available in bucket\n    // TODO 6: Update local state if tokens are consumed\n    // TODO 7: Calculate retry_after time if request is denied\n    // TODO 8: Return RateLimitResult with local state information\n    // Hint: Use local time only - don't depend on Redis for timestamps\n    // Hint: Implement cleanup to prevent memory leaks from abandoned keys\n    panic(\"implement local fallback rate limiting\")\n}\n```\n\n#### Milestone Checkpoints\n\n**Milestone 3 Checkpoint - Redis Failure Handling:**\nAfter implementing circuit breakers and local fallback:\n```bash\n# Test Redis failure scenarios\ngo test ./internal/resilience/... -v\ngo test ./internal/redis/... -v\n\n# Manual testing:\n# 1. Start rate limiter with Redis backend\n# 2. Send requests and verify normal operation\n# 3. Stop Redis instance\n# 4. Verify automatic fallback to local limiting\n# 5. Restart Redis and verify recovery\n```\n\nExpected behavior: Rate limiting continues during Redis outage with degraded accuracy but maintained protection. Recovery should be automatic and seamless.\n\n**Milestone 4 Checkpoint - Clock Skew and Sharding:**\nAfter implementing time synchronization and hot key handling:\n```bash\n# Test clock skew scenarios\ngo test ./internal/resilience/... -run TestClockSkew\ngo test ./internal/sharding/... -run TestHotKey\n\n# Manual testing:\n# 1. Run multiple instances with deliberately skewed clocks\n# 2. Verify rate limiting accuracy despite time differences\n# 3. Generate hot key traffic and verify conflict resolution\n```\n\nExpected behavior: Rate limiting accuracy within 5% even with moderate clock skew. Hot key detection should trigger within 30 seconds of elevated traffic.\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| Rate limits too strict during Redis outage | Local fallback using wrong limits | Check fallback configuration and rule inheritance | Configure appropriate local fallback limits |\n| Inconsistent rate limiting across instances | Clock skew between nodes | Compare timestamps in logs, measure skew | Implement NTP sync and skew correction |\n| Circuit breaker stuck open | Recovery timeout too long or health checks failing | Check Redis connectivity and error patterns | Tune recovery timeout and failure thresholds |\n| Rate limiting fails completely during network partition | No fallback strategy implemented | Check for local limiting during Redis failures | Implement and test graceful degradation |\n| Hot keys causing Redis CPU spikes | Too many concurrent updates to same keys | Monitor Redis slow log and key access patterns | Implement key sharding or request batching |\n\n\n## Testing Strategy\n\n> **Milestone(s):** All milestones - testing strategies span rate limiting algorithms (Milestone 1), multi-tier evaluation (Milestone 2), Redis backend integration (Milestone 3), sharding and consistent hashing (Milestone 4), and API/dashboard functionality (Milestone 5)\n\n### Mental Model: The Quality Assurance Laboratory System\n\nThink of testing a distributed rate limiter like running a comprehensive quality assurance laboratory for a pharmaceutical company. Just as a pharmaceutical lab has multiple testing phases - from testing individual compounds in isolation (unit testing), to testing drug interactions in controlled environments (integration testing), to testing complete treatment protocols in clinical trials (end-to-end testing), and finally to stress-testing drugs under extreme conditions (chaos testing) - our distributed rate limiter requires a multi-layered testing approach.\n\nThe individual algorithms are like chemical compounds that must be tested for purity and effectiveness in isolation. The distributed system interactions are like drug combinations that must be tested for dangerous interactions. The milestone verification checkpoints are like clinical trial phases that must be passed before advancing to more complex testing. And chaos engineering is like testing medications under extreme stress conditions to ensure they remain safe and effective when patients are critically ill.\n\nThis analogy helps us understand why each testing layer is essential - a drug that works perfectly in isolation might fail catastrophically when combined with other medications or under stress conditions, just as rate limiting algorithms that work perfectly locally might fail when distributed across multiple nodes or under high load.\n\n### Algorithm Unit Testing\n\nThe foundation of testing distributed rate limiting systems begins with rigorous verification of individual algorithms in complete isolation. Each rate limiting algorithm represents a mathematical model with precise behavioral expectations, and unit testing must validate both the happy path scenarios and the critical boundary conditions where algorithms typically break down.\n\n**Token Bucket Algorithm Testing Strategy**\n\nToken bucket testing requires careful verification of the refill mechanism, burst handling, and state transitions. The algorithm maintains an internal state consisting of current token count, last refill timestamp, capacity, and refill rate. Each of these state components must be tested independently and in combination.\n\n| Test Category | Test Case | Expected Behavior | Boundary Condition |\n|---------------|-----------|-------------------|-------------------|\n| Refill Logic | Initial state after creation | Bucket starts at full capacity | Time t=0 initialization |\n| Refill Logic | Refill after exact window duration | Tokens = min(capacity, current + refill_rate) | Exact window boundary |\n| Refill Logic | Partial refill with fractional time | Proportional token addition | Sub-second precision |\n| Burst Handling | Request exactly at capacity | All requests allowed instantly | Full bucket depletion |\n| Burst Handling | Request exceeding capacity | Excess requests denied | Overflow protection |\n| State Persistence | Token count after partial consumption | Accurate remaining count | Fractional tokens |\n| Concurrent Access | Multiple simultaneous requests | Atomic state updates | Race condition prevention |\n| Time Manipulation | Clock moving backwards | Graceful degradation | Clock skew scenarios |\n\nThe token bucket algorithm testing must simulate time progression artificially to verify refill behavior without waiting for real time to elapse. This requires dependency injection of a `TimeProvider` interface that can be controlled during testing.\n\n**Sliding Window Counter Algorithm Testing Strategy**\n\nSliding window counter testing focuses on bucket management, weight calculation accuracy, and window transition handling. The algorithm divides time into discrete buckets and maintains counters for each bucket, requiring precise weight calculation when the current time falls between bucket boundaries.\n\n| Test Category | Test Case | Expected Behavior | Critical Edge Case |\n|---------------|-----------|-------------------|-------------------|\n| Bucket Creation | First request in new window | Initialize bucket with count=1 | Empty state initialization |\n| Bucket Transitions | Request at exact window boundary | Proper bucket rotation | Precision timing |\n| Weight Calculation | Request mid-bucket | Accurate weighted sum | Fractional weights |\n| Window Sliding | Old bucket expiration | Automatic cleanup | Memory management |\n| Count Aggregation | Multiple buckets active | Correct total calculation | Boundary spanning |\n| Precision Testing | Sub-second request timing | Nanosecond accuracy | High-frequency requests |\n| Memory Pressure | Long-running operation | Bounded memory usage | Garbage collection |\n\n**Sliding Window Log Algorithm Testing Strategy**\n\nSliding window log testing requires validation of timestamp storage, log rotation, and memory management. Unlike counter-based approaches, the log algorithm stores individual request timestamps, making memory usage testing critical.\n\n| Test Category | Test Case | Expected Behavior | Memory Consideration |\n|---------------|-----------|-------------------|---------------------|\n| Log Insertion | First timestamp storage | Accurate timestamp recording | Initial allocation |\n| Log Rotation | Timestamp expiration | Automatic log cleanup | Memory deallocation |\n| Binary Search | Threshold calculation | Efficient timestamp lookup | O(log n) complexity |\n| Memory Bounds | High request volume | Controlled memory growth | Maximum log size |\n| Timestamp Precision | Nanosecond accuracy | Precise timing storage | Clock resolution |\n| Concurrent Logging | Multiple simultaneous requests | Thread-safe log updates | Lock contention |\n\n**Algorithm Comparison and Performance Testing**\n\nBeyond individual algorithm testing, comprehensive benchmarks must compare algorithm performance characteristics under various load patterns. This comparison testing reveals the trade-offs between accuracy, memory usage, and computational overhead.\n\n| Algorithm | Memory Complexity | Time Complexity | Accuracy | Burst Handling |\n|-----------|------------------|-----------------|-----------|---------------|\n| Token Bucket | O(1) constant | O(1) per request | Exact for bursts | Excellent |\n| Sliding Window Counter | O(buckets) | O(1) per request | Approximate | Good |\n| Sliding Window Log | O(requests in window) | O(log n) per request | Exact | Excellent |\n\n> **Design Insight**: Algorithm testing must validate not just correctness but also performance characteristics under load. A token bucket that works correctly for 10 requests per second might fail catastrophically at 10,000 requests per second due to lock contention or memory allocation overhead.\n\n**Boundary Condition Testing Matrix**\n\nRate limiting algorithms face their greatest challenges at boundary conditions where time windows transition, numeric limits approach maximum values, or system resources become constrained. Comprehensive boundary testing prevents production failures.\n\n| Boundary Type | Test Scenario | Algorithm Impact | Validation Method |\n|---------------|---------------|------------------|-------------------|\n| Time Window Edges | Request at exact window start | Counter reset timing | Millisecond precision testing |\n| Time Window Edges | Request at exact window end | Bucket rotation | Boundary value analysis |\n| Numeric Limits | Request count approaching int64 max | Overflow protection | Large value injection |\n| Numeric Limits | Token count exceeding capacity | Burst limitation | Boundary value testing |\n| System Resources | Memory pressure during operation | Graceful degradation | Resource constraint simulation |\n| Clock Anomalies | System clock adjustment | Time skew handling | Clock manipulation testing |\n\n### Distributed Integration Testing\n\nMoving beyond individual algorithm testing, distributed integration testing validates the behavior of multiple rate limiter instances coordinating through Redis. This testing phase reveals issues that only emerge when multiple processes compete for shared resources and coordinate state changes.\n\n**Multi-Instance Coordination Testing**\n\nThe core challenge in distributed rate limiting lies in ensuring accurate limit enforcement when multiple application instances simultaneously evaluate and update shared counters. Integration testing must simulate realistic deployment scenarios with multiple instances processing concurrent requests.\n\nThe test environment requires multiple rate limiter instances configured to use the same Redis backend, with careful orchestration of request timing to trigger race conditions and coordination challenges. Each test scenario must verify that distributed behavior matches single-instance behavior within acceptable accuracy bounds.\n\n| Test Scenario | Instance Count | Request Pattern | Expected Outcome | Accuracy Threshold |\n|---------------|---------------|-----------------|------------------|-------------------|\n| Simultaneous Burst | 3 instances | 100 requests/instance instantly | Total allowed ≤ limit + burst | 5% tolerance |\n| Gradual Ramp-up | 5 instances | Linear increase over 60 seconds | Smooth limit enforcement | 2% tolerance |\n| Coordinated Attack | 10 instances | Synchronized request waves | Consistent denial pattern | 1% tolerance |\n| Mixed Load Patterns | 8 instances | Random request timing | Fair resource allocation | Statistical validation |\n\n**Redis Atomic Operation Testing**\n\nThe correctness of distributed rate limiting fundamentally depends on atomic Redis operations implemented through Lua scripts. Integration testing must verify that these atomic operations maintain consistency under concurrent load from multiple instances.\n\nTesting atomic operations requires careful coordination of multiple Redis clients executing operations simultaneously and validating that the final state matches expectations regardless of execution order or timing.\n\n| Lua Script | Concurrent Operations | State Validation | Atomicity Check |\n|------------|----------------------|------------------|-----------------|\n| Token Bucket Check-and-Update | 50 simultaneous calls | Token count accuracy | No double-counting |\n| Sliding Window Increment | 100 concurrent increments | Counter consistency | No lost updates |\n| Multi-Tier Evaluation | 25 instances checking | Tier precedence maintained | Correct short-circuiting |\n| Hot Key Detection | High-frequency access | Access count accuracy | No race conditions |\n\n**Network Partition Simulation**\n\nDistributed systems must handle network partitions gracefully, falling back to local rate limiting when Redis becomes unavailable. Integration testing must simulate various network failure scenarios and verify that fallback mechanisms activate correctly and recovery proceeds smoothly.\n\nNetwork partition testing requires controlled network manipulation to simulate Redis connectivity issues while maintaining test orchestration communication. The testing framework must distinguish between intentional network partitions and actual infrastructure failures.\n\n| Partition Scenario | Duration | Expected Behavior | Recovery Validation |\n|-------------------|----------|-------------------|-------------------|\n| Complete Redis Loss | 30 seconds | Local fallback activation | Seamless transition back |\n| Intermittent Connectivity | 5-second cycles | Circuit breaker activation | Adaptive retry behavior |\n| Partial Node Failure | 60 seconds | Hash ring rebalancing | Minimal key redistribution |\n| Split-Brain Scenario | Variable timing | Consistent state maintenance | No conflicting decisions |\n\n**Load Distribution and Sharding Testing**\n\nWhen using consistent hashing to distribute rate limiting state across multiple Redis nodes, integration testing must verify that load distributes evenly and that node additions or removals cause minimal disruption. This testing requires careful monitoring of key distribution patterns and performance characteristics.\n\n| Sharding Test | Node Count | Key Distribution | Rebalancing Trigger | Success Criteria |\n|---------------|------------|------------------|-------------------|------------------|\n| Initial Distribution | 5 nodes | 1000 unique keys | N/A | ±10% load variance |\n| Node Addition | 5→6 nodes | Existing keys | New node insertion | <20% key migration |\n| Node Removal | 6→5 nodes | Existing keys | Node failure | <25% key migration |\n| Hot Key Handling | 5 nodes | 80/20 access pattern | Automatic detection | Even response times |\n\n### Milestone Verification Checkpoints\n\nEach milestone in the distributed rate limiter development process requires specific verification checkpoints that confirm the implementation meets acceptance criteria before proceeding to more complex functionality. These checkpoints serve as quality gates preventing the accumulation of technical debt and ensuring solid foundations for subsequent development.\n\n**Milestone 1: Rate Limiting Algorithms Verification**\n\nThe first milestone checkpoint focuses on verifying that individual rate limiting algorithms function correctly in isolation, handling edge cases appropriately, and meeting performance requirements under load.\n\n| Algorithm Component | Verification Test | Expected Result | Performance Benchmark |\n|-------------------|------------------|-----------------|----------------------|\n| Token Bucket Implementation | Burst handling test: 1000 requests instantly | ≤ capacity requests allowed | < 1μs per operation |\n| Token Bucket Implementation | Refill rate test: sustained load over 60 seconds | Average rate = refill_rate | Consistent latency |\n| Sliding Window Counter | Boundary transition test: requests spanning window | Smooth rate enforcement | < 500ns per operation |\n| Sliding Window Log | Memory usage test: 1M requests over 1 hour | Bounded memory growth | Linear memory scaling |\n| Algorithm Comparison | Accuracy benchmark: compare with exact counting | Token bucket: exact, Counter: ~95% | Measure overhead |\n\nThe verification process requires implementing a comprehensive test suite that exercises each algorithm through multiple scenarios, measuring both correctness and performance characteristics. The test suite must generate synthetic load patterns that simulate real-world usage while maintaining precise control over timing and request distribution.\n\n```bash\n# Example checkpoint verification commands\ngo test ./pkg/algorithms/token_bucket/ -v -race -count=100\ngo test ./pkg/algorithms/sliding_window/ -v -bench=. -benchmem\ngo test ./pkg/algorithms/comparison/ -v -timeout=300s\n```\n\n**Milestone 2: Multi-Tier Rate Limiting Verification**\n\nThe second milestone verification ensures that hierarchical rate limiting works correctly with proper tier precedence, short-circuit evaluation, and accurate rule matching across user, IP, API, and global dimensions.\n\n| Multi-Tier Component | Verification Test | Expected Behavior | Edge Case Coverage |\n|---------------------|------------------|-------------------|--------------------|\n| Tier Precedence | User limit < IP limit conflict | User limit enforced | Most restrictive wins |\n| Short-Circuit Evaluation | Global limit reached first | Skip remaining tiers | Efficiency optimization |\n| Rule Pattern Matching | Complex regex patterns | Correct rule selection | Pattern performance |\n| Quota Management | Cross-tier usage tracking | Accurate attribution | No double-counting |\n\nThe multi-tier verification process requires creating test scenarios with conflicting rate limits across different tiers and verifying that the system enforces the most restrictive applicable limit while maintaining performance efficiency.\n\n**Milestone 3: Redis Backend Integration Verification**\n\nThe third milestone checkpoint validates that Redis integration maintains atomic operations, handles connection failures gracefully, and provides accurate distributed state synchronization across multiple application instances.\n\n| Redis Component | Verification Test | Expected Outcome | Failure Scenario |\n|-----------------|------------------|------------------|------------------|\n| Lua Script Atomicity | 100 concurrent check-and-update | No race conditions | Network latency |\n| Connection Pooling | 1000 simultaneous operations | Connection reuse | Pool exhaustion |\n| Graceful Degradation | Redis unavailable | Local fallback | Service continuity |\n| State Synchronization | Multi-instance coordination | Consistent limits | Clock skew tolerance |\n\nRedis integration verification requires deploying multiple rate limiter instances against a shared Redis backend and orchestrating load tests that verify distributed coordination accuracy under various failure conditions.\n\n**Milestone 4: Consistent Hashing and Sharding Verification**\n\nThe fourth milestone verification confirms that consistent hashing distributes load evenly, minimizes key redistribution during topology changes, and handles hot key scenarios appropriately.\n\n| Sharding Component | Verification Test | Success Criteria | Scalability Metric |\n|-------------------|------------------|------------------|-------------------|\n| Hash Ring Distribution | 10,000 keys across 5 nodes | ±15% load variance | O(log n) lookup time |\n| Node Addition Impact | Add 6th node to running system | <20% key migration | Minimal disruption |\n| Hot Key Detection | 80/20 access pattern | Automatic identification | Response time consistency |\n| Failover Behavior | Primary node failure | Seamless redirection | <100ms recovery time |\n\nSharding verification requires comprehensive load testing with realistic key distribution patterns and careful measurement of performance characteristics during topology changes.\n\n**Milestone 5: API and Dashboard Verification**\n\nThe final milestone verification ensures that the management API functions correctly, the dashboard displays accurate real-time data, and rate limit headers conform to RFC standards.\n\n| API Component | Verification Test | Expected Result | Integration Check |\n|---------------|------------------|-----------------|------------------|\n| CRUD Operations | Create, read, update, delete rules | Immediate effect | Configuration propagation |\n| Rate Limit Headers | X-RateLimit-* header presence | RFC compliance | Client compatibility |\n| Real-time Dashboard | Live usage metrics | <5 second latency | WebSocket stability |\n| Dynamic Configuration | Rule updates without restart | Zero downtime | Gradual rollout |\n\n### Chaos and Failure Testing\n\nThe robustness of a distributed rate limiter emerges most clearly under adverse conditions where components fail, networks partition, and system resources become constrained. Chaos engineering principles guide the systematic introduction of failures to validate recovery mechanisms and identify weaknesses in distributed coordination.\n\n**Redis Failure Scenario Testing**\n\nRedis failures represent the most critical failure mode for distributed rate limiting, as they eliminate the shared state coordination mechanism. Comprehensive failure testing must cover various Redis failure patterns and validate fallback behavior.\n\nThe chaos testing framework must simulate realistic Redis failure scenarios including complete node failures, memory pressure leading to evictions, network partitions isolating Redis clusters, and performance degradation under load. Each failure scenario requires careful validation of both immediate response and recovery behavior.\n\n| Failure Mode | Simulation Method | Expected Behavior | Recovery Validation |\n|--------------|------------------|-------------------|-------------------|\n| Complete Redis Outage | Network blackhole | Local fallback activation | Automatic reconnection |\n| Memory Pressure | Maxmemory + allkeys-lru | Key eviction tolerance | Performance degradation |\n| Network Partition | Selective packet dropping | Circuit breaker activation | Split-brain prevention |\n| Performance Degradation | Artificial latency injection | Timeout handling | Graceful degradation |\n| Partial Node Failure | Single node termination | Hash ring rebalancing | Key redistribution |\n\n**Network Partition and Split-Brain Prevention**\n\nNetwork partitions represent particularly challenging failure scenarios where different parts of the distributed system lose connectivity with each other while maintaining connectivity with clients. The rate limiting system must handle these partitions without creating inconsistent states or allowing unlimited request flow.\n\nNetwork partition testing requires sophisticated network manipulation tools that can selectively block communication between specific components while maintaining test orchestration channels. The testing framework must validate that partitioned components make consistent decisions and that recovery procedures restore global consistency.\n\n| Partition Scenario | Components Affected | Decision Strategy | Consistency Check |\n|-------------------|-------------------|-------------------|-------------------|\n| Redis Cluster Split | Half of Redis nodes | Majority quorum | No split decisions |\n| Application Island | Some app instances isolated | Conservative limiting | Err on restrictive side |\n| Cross-Region Partition | Geographic separation | Regional degradation | Eventual consistency |\n| Cascading Failure | Multiple component failures | Progressive degradation | Bounded blast radius |\n\n**High Load and Resource Exhaustion Testing**\n\nDistributed rate limiters must maintain accuracy and performance under extreme load conditions that stress both computational resources and network bandwidth. High load testing reveals performance bottlenecks, memory leaks, and coordination inefficiencies that only manifest under sustained pressure.\n\nResource exhaustion testing requires careful load generation that can saturate different system resources independently - CPU usage through computational overhead, memory usage through state accumulation, network bandwidth through request volume, and disk I/O through logging and persistence operations.\n\n| Resource Constraint | Load Pattern | System Response | Performance Metric |\n|--------------------|--------------|-----------------|-------------------|\n| CPU Saturation | High-frequency requests | Graceful degradation | Latency percentiles |\n| Memory Pressure | Many unique keys | Memory management | GC frequency |\n| Network Bandwidth | Large request payloads | Request prioritization | Throughput maintenance |\n| Redis Connection Pool | Concurrent operations | Connection queuing | Pool utilization |\n\n**Clock Skew and Time Synchronization Testing**\n\nDistributed rate limiting algorithms depend critically on synchronized time across all participating nodes. Clock skew between application instances and Redis nodes can lead to inaccurate rate limit calculations, particularly for time-window-based algorithms.\n\nClock skew testing requires artificially manipulating system clocks on different nodes and measuring the impact on rate limiting accuracy. The testing framework must validate that clock skew detection mechanisms function correctly and that synchronization procedures restore accuracy.\n\n| Clock Scenario | Skew Magnitude | Algorithm Impact | Mitigation Effectiveness |\n|-----------------|---------------|------------------|-------------------------|\n| Gradual Drift | ±1 second over 1 hour | Sliding window accuracy | NTP synchronization |\n| Sudden Jump | +30 seconds instantly | Token bucket disruption | Skew detection |\n| Backwards Clock | -10 seconds | Timestamp confusion | Monotonic time usage |\n| Different Zones | Cross-timezone deployment | UTC coordination | Timezone normalization |\n\n**Cascading Failure Prevention Testing**\n\nThe most dangerous failure scenarios in distributed systems involve cascading failures where the failure of one component triggers failures in dependent components, potentially leading to total system collapse. Rate limiting systems must include circuit breakers and bulkheads to prevent failure propagation.\n\nCascading failure testing requires orchestrated failure injection across multiple system layers while monitoring the blast radius and recovery characteristics. The testing framework must validate that circuit breakers activate appropriately and that fallback mechanisms prevent total system failure.\n\n| Cascade Trigger | Initial Failure | Propagation Path | Circuit Breaker Response |\n|-----------------|----------------|------------------|-------------------------|\n| Redis Overload | High latency | Client timeout cascade | Redis circuit opening |\n| App Instance Death | Process crash | Load redistribution | Health check removal |\n| Network Congestion | Packet loss | Retry amplification | Backpressure activation |\n| Dashboard Query Load | Metrics overload | Rate limiter impact | Self-rate-limiting |\n\n> **Critical Testing Insight**: Chaos testing must be continuous rather than periodic. Production systems face constant low-level failures and stress conditions that only become apparent through sustained chaos injection. A rate limiting system that passes a one-hour chaos test might fail catastrophically after running under minor stress for weeks.\n\n### Implementation Guidance\n\nThis implementation guidance provides concrete tools and templates for building a comprehensive testing strategy that validates distributed rate limiting functionality across all development milestones.\n\n**A. Technology Recommendations Table:**\n\n| Testing Component | Simple Option | Advanced Option |\n|------------------|---------------|-----------------|\n| Unit Testing | Go testing package + testify | Ginkgo BDD framework |\n| Load Testing | Custom goroutines | Apache JMeter / k6 |\n| Redis Simulation | Embedded Redis (miniredis) | Docker Compose Redis cluster |\n| Network Partition | Manual connection dropping | Chaos Mesh / Litmus |\n| Metrics Collection | Simple counters | Prometheus + Grafana |\n| Test Orchestration | Shell scripts | Kubernetes Jobs |\n| Clock Manipulation | time.Now() mocking | libfaketime system-wide |\n\n**B. Recommended File/Module Structure**\n\n```\nproject-root/\n  cmd/\n    chaos-test/main.go              ← Chaos engineering test runner\n    integration-test/main.go        ← Multi-instance integration tests\n    benchmark/main.go               ← Performance benchmark suite\n  \n  test/\n    unit/                           ← Algorithm unit tests\n      token_bucket_test.go\n      sliding_window_test.go\n      algorithms_benchmark_test.go\n    \n    integration/                    ← Distributed coordination tests\n      multi_instance_test.go\n      redis_atomic_test.go\n      failover_test.go\n    \n    chaos/                          ← Failure injection tests\n      redis_failure_test.go\n      network_partition_test.go\n      load_stress_test.go\n    \n    fixtures/                       ← Test data and configurations\n      rate_limit_rules.yaml\n      redis_cluster_config.yaml\n    \n    helpers/                        ← Testing utilities\n      redis_helper.go              ← Redis test setup/teardown\n      load_generator.go            ← Traffic generation utilities\n      time_helper.go               ← Clock manipulation helpers\n      metrics_collector.go         ← Test metrics collection\n  \n  pkg/\n    ratelimit/\n      *_test.go                    ← Unit tests alongside implementation\n  \n  scripts/\n    run_milestone_checks.sh        ← Automated milestone verification\n    setup_test_environment.sh     ← Test infrastructure setup\n    chaos_test_suite.sh           ← Comprehensive chaos testing\n```\n\n**C. Infrastructure Starter Code (COMPLETE, ready to use):**\n\n**Redis Test Helper - Complete Implementation:**\n\n```go\n// test/helpers/redis_helper.go\npackage helpers\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n    \n    \"github.com/alicebob/miniredis/v2\"\n    \"github.com/redis/go-redis/v9\"\n    \"github.com/testcontainers/testcontainers-go\"\n    \"github.com/testcontainers/testcontainers-go/wait\"\n)\n\n// RedisTestHelper manages Redis instances for testing\ntype RedisTestHelper struct {\n    // For unit tests - embedded Redis\n    miniRedis *miniredis.Miniredis\n    \n    // For integration tests - real Redis containers\n    containers map[string]testcontainers.Container\n    clients    map[string]*redis.Client\n}\n\n// NewRedisTestHelper creates a new Redis test helper\nfunc NewRedisTestHelper() *RedisTestHelper {\n    return &RedisTestHelper{\n        containers: make(map[string]testcontainers.Container),\n        clients:    make(map[string]*redis.Client),\n    }\n}\n\n// StartEmbeddedRedis starts a lightweight Redis for unit tests\nfunc (h *RedisTestHelper) StartEmbeddedRedis() (*redis.Client, func(), error) {\n    mini, err := miniredis.Run()\n    if err != nil {\n        return nil, nil, fmt.Errorf(\"failed to start miniredis: %w\", err)\n    }\n    \n    h.miniRedis = mini\n    \n    client := redis.NewClient(&redis.Options{\n        Addr: mini.Addr(),\n        DB:   0,\n    })\n    \n    cleanup := func() {\n        client.Close()\n        mini.Close()\n    }\n    \n    return client, cleanup, nil\n}\n\n// StartRedisCluster starts a Redis cluster for integration tests\nfunc (h *RedisTestHelper) StartRedisCluster(ctx context.Context, nodeCount int) ([]*redis.Client, func(), error) {\n    var clients []*redis.Client\n    var cleanupFuncs []func()\n    \n    for i := 0; i < nodeCount; i++ {\n        container, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{\n            ContainerRequest: testcontainers.ContainerRequest{\n                Image:        \"redis:7-alpine\",\n                ExposedPorts: []string{\"6379/tcp\"},\n                WaitingFor:   wait.ForLog(\"Ready to accept connections\"),\n                Cmd:          []string{\"redis-server\", \"--appendonly\", \"yes\"},\n            },\n            Started: true,\n        })\n        if err != nil {\n            return nil, func() { h.cleanup(cleanupFuncs) }, err\n        }\n        \n        endpoint, err := container.Endpoint(ctx, \"\")\n        if err != nil {\n            return nil, func() { h.cleanup(cleanupFuncs) }, err\n        }\n        \n        client := redis.NewClient(&redis.Options{\n            Addr: endpoint,\n        })\n        \n        clients = append(clients, client)\n        h.containers[fmt.Sprintf(\"node%d\", i)] = container\n        h.clients[fmt.Sprintf(\"node%d\", i)] = client\n        \n        cleanupFuncs = append(cleanupFuncs, func() {\n            client.Close()\n            container.Terminate(ctx)\n        })\n    }\n    \n    return clients, func() { h.cleanup(cleanupFuncs) }, nil\n}\n\n// SimulateNetworkPartition blocks network access to specific Redis nodes\nfunc (h *RedisTestHelper) SimulateNetworkPartition(ctx context.Context, nodeIDs []string) error {\n    for _, nodeID := range nodeIDs {\n        if container, exists := h.containers[nodeID]; exists {\n            // Simulate network partition by pausing the container\n            err := container.Terminate(ctx)\n            if err != nil {\n                return fmt.Errorf(\"failed to partition node %s: %w\", nodeID, err)\n            }\n        }\n    }\n    return nil\n}\n\nfunc (h *RedisTestHelper) cleanup(cleanupFuncs []func()) {\n    for _, cleanup := range cleanupFuncs {\n        cleanup()\n    }\n}\n```\n\n**Load Generator - Complete Implementation:**\n\n```go\n// test/helpers/load_generator.go\npackage helpers\n\nimport (\n    \"context\"\n    \"math/rand\"\n    \"sync\"\n    \"sync/atomic\"\n    \"time\"\n    \n    \"your-project/pkg/ratelimit\"\n)\n\n// LoadPattern defines different traffic generation patterns\ntype LoadPattern int\n\nconst (\n    ConstantLoad LoadPattern = iota\n    BurstLoad\n    GradualRamp\n    SpikeLoad\n    RandomLoad\n)\n\n// LoadGeneratorConfig configures load generation parameters\ntype LoadGeneratorConfig struct {\n    Pattern          LoadPattern\n    RequestsPerSecond int\n    Duration         time.Duration\n    ClientCount      int\n    KeyPattern       string\n    BurstSize        int\n    BurstInterval    time.Duration\n}\n\n// LoadGenerator generates synthetic traffic patterns for testing\ntype LoadGenerator struct {\n    config    LoadGeneratorConfig\n    limiter   ratelimit.DistributedLimiter\n    results   *LoadTestResults\n    ctx       context.Context\n    cancelFn  context.CancelFunc\n}\n\n// LoadTestResults captures comprehensive load test metrics\ntype LoadTestResults struct {\n    TotalRequests    int64\n    AllowedRequests  int64\n    DeniedRequests   int64\n    ErrorRequests    int64\n    AverageLatency   time.Duration\n    P95Latency       time.Duration\n    P99Latency       time.Duration\n    StartTime        time.Time\n    EndTime          time.Time\n    LatencyHistogram []time.Duration\n    mutex            sync.RWMutex\n}\n\n// NewLoadGenerator creates a load generator with specified configuration\nfunc NewLoadGenerator(config LoadGeneratorConfig, limiter ratelimit.DistributedLimiter) *LoadGenerator {\n    ctx, cancel := context.WithTimeout(context.Background(), config.Duration)\n    \n    return &LoadGenerator{\n        config:  config,\n        limiter: limiter,\n        results: &LoadTestResults{\n            StartTime:        time.Now(),\n            LatencyHistogram: make([]time.Duration, 0, config.RequestsPerSecond*int(config.Duration.Seconds())),\n        },\n        ctx:      ctx,\n        cancelFn: cancel,\n    }\n}\n\n// Start begins load generation according to configured pattern\nfunc (lg *LoadGenerator) Start() *LoadTestResults {\n    defer lg.cancelFn()\n    lg.results.StartTime = time.Now()\n    \n    switch lg.config.Pattern {\n    case ConstantLoad:\n        lg.generateConstantLoad()\n    case BurstLoad:\n        lg.generateBurstLoad()\n    case GradualRamp:\n        lg.generateGradualRamp()\n    case SpikeLoad:\n        lg.generateSpikeLoad()\n    case RandomLoad:\n        lg.generateRandomLoad()\n    }\n    \n    lg.results.EndTime = time.Now()\n    lg.calculateStatistics()\n    return lg.results\n}\n\n// generateConstantLoad produces steady request rate\nfunc (lg *LoadGenerator) generateConstantLoad() {\n    interval := time.Second / time.Duration(lg.config.RequestsPerSecond)\n    ticker := time.NewTicker(interval)\n    defer ticker.Stop()\n    \n    var wg sync.WaitGroup\n    \n    for {\n        select {\n        case <-lg.ctx.Done():\n            wg.Wait()\n            return\n        case <-ticker.C:\n            for i := 0; i < lg.config.ClientCount; i++ {\n                wg.Add(1)\n                go func() {\n                    defer wg.Done()\n                    lg.executeRequest()\n                }()\n            }\n        }\n    }\n}\n\n// generateBurstLoad produces periodic bursts of requests\nfunc (lg *LoadGenerator) generateBurstLoad() {\n    burstTicker := time.NewTicker(lg.config.BurstInterval)\n    defer burstTicker.Stop()\n    \n    for {\n        select {\n        case <-lg.ctx.Done():\n            return\n        case <-burstTicker.C:\n            var wg sync.WaitGroup\n            for i := 0; i < lg.config.BurstSize; i++ {\n                wg.Add(1)\n                go func() {\n                    defer wg.Done()\n                    lg.executeRequest()\n                }()\n            }\n            wg.Wait()\n        }\n    }\n}\n\n// executeRequest performs a single rate limit check with timing\nfunc (lg *LoadGenerator) executeRequest() {\n    start := time.Now()\n    \n    key := lg.generateKey()\n    req := ratelimit.RateLimitRequest{\n        UserID:      fmt.Sprintf(\"user_%d\", rand.Intn(1000)),\n        IPAddress:   fmt.Sprintf(\"192.168.1.%d\", rand.Intn(255)+1),\n        APIEndpoint: key,\n        Tokens:      1,\n    }\n    \n    result, err := lg.limiter.Check(lg.ctx, req)\n    latency := time.Since(start)\n    \n    lg.recordResult(result, err, latency)\n}\n\n// recordResult safely updates load test results\nfunc (lg *LoadGenerator) recordResult(result *ratelimit.RateLimitResult, err error, latency time.Duration) {\n    lg.results.mutex.Lock()\n    defer lg.results.mutex.Unlock()\n    \n    atomic.AddInt64(&lg.results.TotalRequests, 1)\n    lg.results.LatencyHistogram = append(lg.results.LatencyHistogram, latency)\n    \n    if err != nil {\n        atomic.AddInt64(&lg.results.ErrorRequests, 1)\n    } else if result.Allowed {\n        atomic.AddInt64(&lg.results.AllowedRequests, 1)\n    } else {\n        atomic.AddInt64(&lg.results.DeniedRequests, 1)\n    }\n}\n\n// generateKey creates test keys according to pattern\nfunc (lg *LoadGenerator) generateKey() string {\n    if lg.config.KeyPattern == \"\" {\n        return fmt.Sprintf(\"api_endpoint_%d\", rand.Intn(10))\n    }\n    return fmt.Sprintf(lg.config.KeyPattern, rand.Intn(100))\n}\n\n// calculateStatistics computes latency percentiles and averages\nfunc (lg *LoadGenerator) calculateStatistics() {\n    if len(lg.results.LatencyHistogram) == 0 {\n        return\n    }\n    \n    // Sort latencies for percentile calculation\n    latencies := make([]time.Duration, len(lg.results.LatencyHistogram))\n    copy(latencies, lg.results.LatencyHistogram)\n    \n    sort.Slice(latencies, func(i, j int) bool {\n        return latencies[i] < latencies[j]\n    })\n    \n    // Calculate average\n    var total time.Duration\n    for _, latency := range latencies {\n        total += latency\n    }\n    lg.results.AverageLatency = total / time.Duration(len(latencies))\n    \n    // Calculate percentiles\n    p95Index := int(0.95 * float64(len(latencies)))\n    p99Index := int(0.99 * float64(len(latencies)))\n    \n    lg.results.P95Latency = latencies[p95Index]\n    lg.results.P99Latency = latencies[p99Index]\n}\n```\n\n**Time Helper for Clock Manipulation - Complete Implementation:**\n\n```go\n// test/helpers/time_helper.go\npackage helpers\n\nimport (\n    \"sync\"\n    \"time\"\n)\n\n// MockTimeProvider implements controllable time for testing\ntype MockTimeProvider struct {\n    mu          sync.RWMutex\n    currentTime time.Time\n    clockSkew   time.Duration\n}\n\n// NewMockTimeProvider creates a controllable time provider\nfunc NewMockTimeProvider(startTime time.Time) *MockTimeProvider {\n    return &MockTimeProvider{\n        currentTime: startTime,\n    }\n}\n\n// Now returns the current mock time\nfunc (m *MockTimeProvider) Now() time.Time {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n    return m.currentTime.Add(m.clockSkew)\n}\n\n// AdvanceTime moves the mock clock forward\nfunc (m *MockTimeProvider) AdvanceTime(duration time.Duration) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.currentTime = m.currentTime.Add(duration)\n}\n\n// SetClockSkew simulates time difference between nodes\nfunc (m *MockTimeProvider) SetClockSkew(skew time.Duration) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.clockSkew = skew\n}\n\n// SetTime directly sets the current time\nfunc (m *MockTimeProvider) SetTime(t time.Time) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.currentTime = t\n}\n```\n\n**D. Core Logic Skeleton Code (signature + TODOs only):**\n\n**Algorithm Unit Test Template:**\n\n```go\n// test/unit/token_bucket_test.go\npackage unit\n\nimport (\n    \"context\"\n    \"testing\"\n    \"time\"\n    \n    \"github.com/stretchr/testify/assert\"\n    \"your-project/pkg/algorithms\"\n    \"your-project/test/helpers\"\n)\n\n// TestTokenBucketBurstHandling validates burst capacity behavior\nfunc TestTokenBucketBurstHandling(t *testing.T) {\n    // TODO 1: Create TokenBucket with capacity=10, refill_rate=5/sec\n    // TODO 2: Execute 10 requests instantly - all should be allowed\n    // TODO 3: Execute 11th request - should be denied\n    // TODO 4: Wait 1 second for refill\n    // TODO 5: Execute 5 more requests - should be allowed\n    // TODO 6: Verify remaining token count matches expected value\n    \n    config := algorithms.TokenBucketConfig{\n        Capacity:   10,\n        RefillRate: 5,\n        Window:     time.Second,\n    }\n    \n    // Implementation goes here - learner fills this in\n}\n\n// TestTokenBucketRefillAccuracy validates precise refill timing\nfunc TestTokenBucketRefillAccuracy(t *testing.T) {\n    // TODO 1: Create TokenBucket with capacity=100, refill_rate=10/sec\n    // TODO 2: Drain bucket completely (100 requests)\n    // TODO 3: Advance time by 0.5 seconds\n    // TODO 4: Verify exactly 5 tokens available\n    // TODO 5: Advance time by another 0.5 seconds\n    // TODO 6: Verify exactly 10 tokens available\n    // TODO 7: Test fractional refills (0.1 second = 1 token)\n    \n    timeProvider := helpers.NewMockTimeProvider(time.Now())\n    \n    // Implementation goes here - learner fills this in\n}\n\n// TestSlidingWindowBoundaryTransition validates accurate window transitions\nfunc TestSlidingWindowBoundaryTransition(t *testing.T) {\n    // TODO 1: Create SlidingWindowCounter with limit=100, window=1min, buckets=60\n    // TODO 2: Fill first 30 seconds with 50 requests\n    // TODO 3: Fill next 30 seconds with 50 requests  \n    // TODO 4: Advance to 45 seconds - verify current count = 75 (weighted)\n    // TODO 5: Advance to 60 seconds - verify count = 50 (second half only)\n    // TODO 6: Test precision at exact second boundaries\n    \n    // Implementation goes here - learner fills this in\n}\n\n// BenchmarkAlgorithmComparison compares performance characteristics\nfunc BenchmarkAlgorithmComparison(b *testing.B) {\n    // TODO 1: Set up TokenBucket, SlidingWindowCounter, SlidingWindowLog\n    // TODO 2: Benchmark each algorithm with same request pattern\n    // TODO 3: Measure operations per second for each\n    // TODO 4: Measure memory allocation per operation\n    // TODO 5: Record results for comparison table\n    \n    // Implementation goes here - learner fills this in\n}\n```\n\n**Integration Test Template:**\n\n```go\n// test/integration/multi_instance_test.go\npackage integration\n\nimport (\n    \"context\"\n    \"sync\"\n    \"testing\"\n    \"time\"\n    \n    \"your-project/pkg/ratelimit\"\n    \"your-project/test/helpers\"\n)\n\n// TestDistributedCoordination validates multi-instance rate limiting accuracy\nfunc TestDistributedCoordination(t *testing.T) {\n    // TODO 1: Start Redis cluster with 3 nodes\n    // TODO 2: Create 5 DistributedLimiter instances connected to same Redis\n    // TODO 3: Configure rate limit: 100 requests/minute per API endpoint  \n    // TODO 4: Generate 500 requests across all instances simultaneously\n    // TODO 5: Verify total allowed requests ≤ 105 (5% tolerance for race conditions)\n    // TODO 6: Wait for next window and verify reset behavior\n    // TODO 7: Test with different request timing patterns\n    \n    redisHelper := helpers.NewRedisTestHelper()\n    // Implementation goes here - learner fills this in\n}\n\n// TestAtomicOperations validates Redis Lua script atomicity\nfunc TestAtomicOperations(t *testing.T) {\n    // TODO 1: Set up Redis with custom Lua scripts loaded\n    // TODO 2: Launch 100 goroutines executing check-and-update simultaneously  \n    // TODO 3: Each goroutine attempts to consume 1 token from bucket with capacity=50\n    // TODO 4: Verify exactly 50 operations succeed, 50 fail\n    // TODO 5: Verify final bucket state is consistent (0 tokens remaining)\n    // TODO 6: Test with different algorithms and concurrency levels\n    \n    // Implementation goes here - learner fills this in\n}\n\n// TestGracefulDegradation validates fallback behavior during Redis failures\nfunc TestGracefulDegradation(t *testing.T) {\n    // TODO 1: Start rate limiter with Redis backend + local fallback\n    // TODO 2: Verify normal operation with Redis available\n    // TODO 3: Simulate Redis failure (network partition)\n    // TODO 4: Verify automatic fallback to local limiting\n    // TODO 5: Verify degraded functionality still enforces limits\n    // TODO 6: Restore Redis and verify transition back to distributed mode\n    // TODO 7: Test various failure scenarios (partial failures, timeouts)\n    \n    // Implementation goes here - learner fills this in\n}\n```\n\n**Chaos Test Template:**\n\n```go\n// test/chaos/redis_failure_test.go\npackage chaos\n\nimport (\n    \"context\"\n    \"testing\"\n    \"time\"\n    \n    \"your-project/test/helpers\"\n)\n\n// TestRedisCompleteOutage validates behavior during total Redis failure\nfunc TestRedisCompleteOutage(t *testing.T) {\n    // TODO 1: Deploy 3 rate limiter instances with Redis cluster backend\n    // TODO 2: Start load generation: 1000 req/sec across all instances\n    // TODO 3: After 30 seconds, simulate complete Redis cluster failure\n    // TODO 4: Verify circuit breaker activates within 5 seconds\n    // TODO 5: Verify local fallback maintains some rate limiting (degraded)\n    // TODO 6: After 60 seconds, restore Redis cluster\n    // TODO 7: Verify recovery to normal operation within 30 seconds\n    // TODO 8: Measure accuracy degradation during outage period\n    \n    // Implementation goes here - learner fills this in\n}\n\n// TestNetworkPartitionSplitBrain validates split-brain prevention\nfunc TestNetworkPartitionSplitBrain(t *testing.T) {\n    // TODO 1: Set up 6-node Redis cluster + 4 rate limiter instances  \n    // TODO 2: Partition network: isolate 2 Redis nodes + 2 app instances\n    // TODO 3: Generate traffic to both network partitions\n    // TODO 4: Verify minority partition fails safely (conservative limiting)\n    // TODO 5: Verify majority partition continues normal operation\n    // TODO 6: Heal network partition after 120 seconds\n    // TODO 7: Verify convergence to consistent state\n    // TODO 8: Measure request accuracy during partition\n    \n    // Implementation goes here - learner fills this in\n}\n\n// TestClockSkewImpact validates time synchronization requirements\nfunc TestClockSkewImpact(t *testing.T) {\n    // TODO 1: Deploy rate limiters with controllable time providers\n    // TODO 2: Introduce gradual clock skew: +1 second per minute drift\n    // TODO 3: Measure sliding window accuracy degradation over time  \n    // TODO 4: Test sudden clock jumps: +30 seconds, -10 seconds\n    // TODO 5: Verify skew detection mechanisms activate\n    // TODO 6: Test algorithm-specific impacts (token bucket vs sliding window)\n    // TODO 7: Validate time sync recovery procedures\n    \n    // Implementation goes here - learner fills this in\n}\n```\n\n**E. Language-Specific Hints:**\n\n- Use `testing.TB` interface for helpers that work in both tests and benchmarks\n- Leverage `testify/suite` for complex test setup/teardown with shared state\n- Use `context.WithTimeout` extensively to prevent hanging integration tests  \n- Implement test helpers that use `t.Cleanup()` for automatic resource cleanup\n- Use `sync/atomic` for thread-safe counter operations in load tests\n- Leverage `testing.Short()` to skip expensive tests during development\n- Use `go test -race` consistently to detect race conditions\n- Implement test fixtures with `embed` for configuration files\n- Use `httptest.Server` for testing HTTP API endpoints\n- Leverage `testcontainers-go` for realistic Redis cluster testing\n\n**F. Milestone Checkpoint:**\n\nAfter implementing each milestone's testing strategy:\n\n**Milestone 1 Checkpoint:**\n```bash\n# Verify algorithm implementations\ngo test ./pkg/algorithms/... -v -race -count=5\ngo test ./pkg/algorithms/... -bench=. -benchmem\ngo test ./test/unit/... -v -timeout=60s\n\n# Expected: All tests pass, benchmarks show expected performance characteristics\n# Signs of problems: Race conditions, memory leaks, inconsistent results\n```\n\n**Milestone 2 Checkpoint:**\n```bash\n# Verify multi-tier coordination\ngo test ./pkg/multitier/... -v -race\ngo test ./test/integration/multi_tier_test.go -v\n\n# Expected: Proper tier precedence, short-circuit evaluation working\n# Signs of problems: Wrong limits enforced, performance degradation\n```\n\n**Milestone 3 Checkpoint:**\n```bash\n# Verify Redis integration\ngo test ./test/integration/redis_test.go -v\ndocker-compose -f test/redis-cluster.yml up -d\ngo test ./test/integration/... -v -timeout=300s\n\n# Expected: Atomic operations, graceful degradation, connection pooling\n# Signs of problems: Race conditions, connection leaks, fallback failures\n```\n\n**G. Debugging Tips:**\n\n| Symptom | Likely Cause | Diagnosis Method | Fix |\n|---------|--------------|------------------|-----|\n| Tests pass individually but fail in parallel | Race conditions in shared state | Run with `-race` flag | Add proper synchronization |\n| Integration tests hang indefinitely | Missing context timeouts | Check for blocking operations | Add context.WithTimeout |\n| High memory usage during load tests | Memory leaks in test harness | Use `go test -memprofile` | Fix cleanup in test helpers |\n| Inconsistent rate limiting accuracy | Clock skew or timing issues | Log timestamps and compare | Implement time synchronization |\n| Redis connection errors in tests | Connection pool exhaustion | Monitor Redis connection count | Increase pool size or add cleanup |\n| Chaos tests produce false positives | Insufficient failure simulation time | Extend chaos duration | Allow more time for failure detection |\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones - debugging techniques span rate limiting algorithms (Milestone 1), multi-tier evaluation (Milestone 2), Redis backend integration (Milestone 3), consistent hashing and sharding (Milestone 4), and API design (Milestone 5).\n\n### Mental Model: The Detective Investigation Framework\n\nThink of debugging a distributed rate limiter like conducting a complex criminal investigation across multiple crime scenes. Just as a detective must gather evidence from multiple locations, interview witnesses with different perspectives, and piece together a timeline of events, debugging distributed systems requires collecting logs from multiple nodes, understanding different component viewpoints, and reconstructing the sequence of operations that led to a problem.\n\nThe detective starts with observable symptoms (the \"crime scene\") - perhaps requests are being incorrectly allowed or denied, response times are slow, or counters seem inaccurate. Like fingerprints and DNA evidence, distributed systems leave traces in logs, metrics, and Redis state that tell the story of what happened. The challenge lies in correlating evidence across time zones (clock skew), dealing with unreliable witnesses (network partitions), and understanding that the timeline might be non-linear (asynchronous operations).\n\nA good detective follows a systematic approach: secure the scene (gather current state), collect evidence (logs and metrics), interview witnesses (check all nodes), analyze the timeline (sequence of operations), and test theories (reproduce the issue). Similarly, effective distributed system debugging requires structured approaches that account for the inherent complexity of multiple moving parts operating across network boundaries.\n\n### Symptom-Based Diagnosis Table\n\nThe foundation of effective distributed rate limiting debugging lies in mapping observable symptoms to their likely root causes through systematic analysis. This diagnostic approach transforms the overwhelming complexity of distributed system failures into manageable investigation paths.\n\n| Symptom | Observable Behavior | Likely Root Causes | Initial Diagnostic Steps | Advanced Investigation |\n|---------|-------------------|-------------------|------------------------|----------------------|\n| **Requests incorrectly allowed** | Rate limit counters show usage below threshold but actual requests exceed configured limit | Token bucket refill race condition, sliding window boundary condition, clock skew between nodes, Lua script logic error | Check Redis TIME command vs local time, examine Lua script execution logs, verify algorithm parameters | Enable request tracing with correlation IDs, compare counter values across Redis nodes, analyze time-series data for boundary transitions |\n| **Requests incorrectly denied** | Rate limit counters exceed threshold but usage appears normal, legitimate traffic rejected | Hot key concentration, Redis memory pressure, circuit breaker false positives, multi-tier evaluation precedence error | Monitor Redis memory usage and eviction stats, check circuit breaker state, verify rule priority ordering | Analyze key distribution across hash ring, examine tier evaluation logs, test rule pattern matching logic |\n| **Inconsistent behavior across instances** | Same request allowed on one app instance but denied on another | Local fallback active on some nodes, Redis connection issues, configuration propagation delay, stale rule cache | Check Redis connectivity per node, verify configuration version across instances, examine local fallback activation logs | Trace configuration update propagation, validate hash ring consistency, monitor network partitions |\n| **High latency in rate limit checks** | Response times exceed acceptable thresholds consistently | Redis cluster rebalancing, network latency to Redis, connection pool exhaustion, Lua script complexity | Monitor Redis connection pool metrics, measure network round-trip time, profile Lua script execution duration | Analyze request routing patterns, examine hot key impact on performance, trace connection lifecycle |\n| **Rate limit counters reset unexpectedly** | Usage statistics drop to zero without administrative action | Redis key expiration misconfiguration, manual counter reset, Redis failover with data loss, time provider synchronization issue | Check Redis TTL settings, review administrative action logs, verify Redis persistence configuration | Analyze Redis cluster topology changes, examine backup and recovery procedures, validate time synchronization |\n| **Memory usage growing unbounded** | Redis memory consumption increases without corresponding traffic | Sliding window log accumulation, connection leak, metrics collection retention, key namespace pollution | Monitor Redis key count and memory per key, check connection pool statistics, analyze key patterns | Profile memory allocation patterns, examine garbage collection behavior, trace key lifecycle management |\n| **Circuit breaker stuck open** | All Redis requests rejected despite Redis being healthy | Failure threshold too sensitive, health check misconfiguration, recovery timeout too long, cascading failure detection | Verify Redis health via direct connection, check circuit breaker configuration parameters, review failure classification logic | Analyze failure pattern leading to circuit opening, validate health check implementation, examine recovery criteria |\n| **Configuration changes not applied** | Rule updates don't affect rate limiting behavior | Redis pub/sub channel issues, configuration watcher stopped, rule validation failure, cache invalidation problem | Check Redis pub/sub subscription status, verify configuration watcher process, examine rule validation logs | Trace configuration update flow end-to-end, validate cache coherency mechanisms, analyze version control |\n| **Clock skew affecting time windows** | Rate limiting behavior varies with server time differences | NTP synchronization issues, Redis server time drift, timezone configuration mismatch, manual time adjustment | Compare local time with Redis TIME command, check NTP status on all nodes, verify timezone settings | Implement distributed time synchronization monitoring, analyze time drift patterns, validate time provider logic |\n| **Hot key performance degradation** | Specific keys experience severe latency while others perform normally | Uneven key distribution, single Redis instance overload, hash ring imbalance, algorithmic complexity | Identify hot keys through Redis monitoring, analyze request distribution patterns, check hash ring virtual node allocation | Implement hot key replication, analyze access pattern evolution, optimize key distribution strategy |\n\n### Redis-Specific Debugging Techniques\n\nRedis serves as the critical shared state store for distributed rate limiting, making Redis-specific debugging techniques essential for diagnosing and resolving issues. The challenge lies in Redis's single-threaded nature combined with distributed access patterns creating complex interactions between atomicity, performance, and consistency.\n\n**Redis CLI Diagnostic Commands**\n\nThe Redis command-line interface provides powerful introspection capabilities for understanding rate limiting state and system health. These commands form the foundation of Redis debugging workflows.\n\n| Command Category | Command | Purpose | Example Usage | Interpretation |\n|------------------|---------|---------|---------------|----------------|\n| **Time Synchronization** | `TIME` | Get Redis server timestamp | `redis-cli TIME` | Compare with local time to detect clock skew exceeding 100ms |\n| **Memory Analysis** | `MEMORY USAGE key` | Memory consumption per rate limit key | `MEMORY USAGE ratelimit:user:12345:api:/orders` | Identify memory-intensive keys causing Redis pressure |\n| **Key Inspection** | `TTL key` | Remaining time-to-live for rate limit counters | `TTL ratelimit:user:12345:token_bucket` | Verify expiration timing matches configured window duration |\n| **Script Debugging** | `EVAL script numkeys key [arg ...]` | Execute Lua script with debugging output | `EVAL \"return {KEYS[1], ARGV[1], redis.call('TIME')}\" 1 test_key 100` | Test Lua script logic with controlled inputs |\n| **Connection Monitoring** | `CLIENT LIST` | Active connection details | `CLIENT LIST TYPE normal` | Identify connection leaks and pool exhaustion |\n| **Performance Profiling** | `SLOWLOG GET count` | Recent slow operations | `SLOWLOG GET 10` | Detect rate limiting operations exceeding performance thresholds |\n| **Key Pattern Analysis** | `SCAN cursor MATCH pattern COUNT count` | Iterate through rate limit keys | `SCAN 0 MATCH ratelimit:user:* COUNT 1000` | Analyze key distribution and naming patterns |\n| **Memory Pressure** | `INFO memory` | Comprehensive memory statistics | `INFO memory` | Monitor memory usage, fragmentation, and eviction policies |\n| **Persistence Status** | `LASTSAVE` | Last successful save timestamp | `LASTSAVE` | Verify data persistence for rate limit state recovery |\n| **Cluster Health** | `CLUSTER NODES` | Cluster topology and node status | `CLUSTER NODES` | Diagnose Redis cluster partitions and failover status |\n\n**Lua Script Debugging Techniques**\n\nRate limiting algorithms rely heavily on atomic Lua scripts for maintaining consistency. Debugging these scripts requires specialized approaches that account for Redis's execution environment constraints.\n\nThe primary challenge in Lua script debugging lies in Redis's atomic execution model - scripts run to completion without interruption, making traditional debugging techniques ineffective. Instead, debugging relies on strategic logging, return value analysis, and controlled test environments.\n\n```lua\n-- Example debugging-enabled token bucket script with comprehensive logging\nlocal key = KEYS[1]\nlocal capacity = tonumber(ARGV[1])\nlocal refill_rate = tonumber(ARGV[2])\nlocal requested_tokens = tonumber(ARGV[3])\nlocal window_seconds = tonumber(ARGV[4])\n\n-- Debug: Create execution trace for diagnostics\nlocal debug_info = {}\ndebug_info.timestamp = redis.call('TIME')\ndebug_info.key = key\ndebug_info.inputs = {capacity, refill_rate, requested_tokens, window_seconds}\n\n-- Get current state with error handling\nlocal current_state = redis.call('HMGET', key, 'tokens', 'last_refill')\nlocal tokens = tonumber(current_state[1]) or capacity\nlocal last_refill = tonumber(current_state[2]) or tonumber(debug_info.timestamp[1])\n\ndebug_info.initial_state = {tokens, last_refill}\n\n-- Calculate refill amount with precision handling\nlocal current_time = tonumber(debug_info.timestamp[1])\nlocal time_delta = math.max(0, current_time - last_refill)\nlocal refill_amount = math.min(capacity - tokens, time_delta * refill_rate / window_seconds)\nlocal new_tokens = math.min(capacity, tokens + refill_amount)\n\ndebug_info.refill_calculation = {time_delta, refill_amount, new_tokens}\n\n-- Make rate limiting decision\nlocal allowed = new_tokens >= requested_tokens\nlocal final_tokens = allowed and (new_tokens - requested_tokens) or new_tokens\n\ndebug_info.decision = {allowed, final_tokens}\n\n-- Update state atomically\nredis.call('HMSET', key, 'tokens', final_tokens, 'last_refill', current_time)\nredis.call('EXPIRE', key, window_seconds * 2)\n\n-- Return decision with comprehensive debugging information\nreturn {\n    allowed and 1 or 0,\n    final_tokens,\n    capacity - final_tokens, -- remaining capacity\n    debug_info.timestamp[1],\n    cjson.encode(debug_info) -- serialized debug information\n}\n```\n\n**Redis Monitoring and Alerting Setup**\n\nEffective Redis monitoring for distributed rate limiting requires tracking both standard Redis metrics and rate limiting specific indicators. The monitoring system must detect performance degradation before it impacts user experience while providing sufficient detail for root cause analysis.\n\n| Metric Category | Key Metrics | Alert Thresholds | Diagnostic Value |\n|------------------|-------------|------------------|------------------|\n| **Memory Management** | `used_memory_rss`, `mem_fragmentation_ratio` | Memory usage > 80%, fragmentation > 2.0 | Indicates approaching capacity limits requiring scaling |\n| **Rate Limiting Performance** | Lua script execution time, key access patterns | Script duration > 5ms, hot key concentration > 10x average | Identifies algorithmic inefficiencies and traffic patterns |\n| **Connection Health** | `connected_clients`, `blocked_clients` | Connections > 80% of maxclients, blocked clients > 0 | Reveals connection pool exhaustion and blocking operations |\n| **Persistence Status** | `last_save_time`, `changes_since_save` | No save in 10 minutes, unsaved changes > 10000 | Ensures rate limit state durability for recovery scenarios |\n| **Cluster Coordination** | Node availability, failover events | Node down > 30 seconds, frequent failovers | Detects cluster instability affecting distributed consensus |\n\n### Distributed System Debugging\n\nDebugging distributed rate limiting systems requires sophisticated approaches that account for the fundamental challenges of multiple independent processes coordinating across network boundaries. Unlike single-node debugging where state is directly observable, distributed debugging demands correlation techniques that piece together a coherent picture from fragments scattered across multiple machines.\n\n**Correlation IDs and Request Tracing**\n\nThe foundation of distributed debugging lies in correlation IDs - unique identifiers that follow requests through their entire journey across multiple system components. In distributed rate limiting, a single user request might trigger checks against multiple tiers, require Redis operations across several nodes, and involve fallback logic during failures.\n\nThe `RequestContext` structure provides the framework for comprehensive request tracing throughout the rate limiting pipeline:\n\n| Field | Type | Tracing Purpose | Example Value |\n|-------|------|----------------|---------------|\n| `UserID` | `string` | Links requests to specific users across tiers | `user_12345` |\n| `IPAddress` | `string` | Enables IP-based correlation and geo-analysis | `192.168.1.100` |\n| `APIEndpoint` | `string` | Groups requests by functionality for pattern analysis | `/api/v1/orders` |\n| `Headers` | `map[string]string` | Contains correlation IDs and request metadata | `{\"X-Trace-ID\": \"trace_abc123\", \"X-Request-ID\": \"req_xyz789\"}` |\n| `Timestamp` | `time.Time` | Enables temporal correlation across nodes with different clocks | `2023-10-15T14:30:45.123Z` |\n\n**Distributed Tracing Implementation Strategy**\n\nEffective distributed tracing for rate limiting requires capturing decision points, performance bottlenecks, and error conditions at each system boundary. The tracing strategy must balance observability needs with performance overhead, particularly given rate limiting's position in the critical request path.\n\n> **Decision: Structured Logging with Correlation IDs**\n> - **Context**: Rate limiting checks occur on every API request, making tracing overhead critical to system performance\n> - **Options Considered**: Full distributed tracing (Jaeger/Zipkin), structured logging with correlation IDs, custom event streaming\n> - **Decision**: Structured logging with correlation IDs and sampling for detailed traces\n> - **Rationale**: Provides necessary correlation capabilities without the performance overhead and complexity of full distributed tracing systems\n> - **Consequences**: Enables request flow reconstruction while maintaining sub-millisecond overhead, but requires manual correlation for complex failure scenarios\n\nThe `FlowCoordinator` serves as the central orchestration point where distributed tracing context is established and propagated:\n\n```go\ntype FlowCoordinator struct {\n    storage         Storage\n    ruleManager     *RuleManager\n    keyComposer     *KeyComposer\n    algorithms      map[string]Algorithm\n    localFallback   Limiter\n    metricsCollector *MetricsCollector\n    circuitBreaker  *CircuitBreaker\n}\n```\n\n**Multi-Node Log Aggregation Patterns**\n\nDistributed rate limiting debugging requires aggregating logs from multiple application instances, Redis nodes, and monitoring systems into a coherent timeline. The challenge lies in correlating events across systems with different clock synchronization, log formats, and retention policies.\n\n| Log Source | Information Provided | Correlation Keys | Retention Requirements |\n|------------|---------------------|------------------|----------------------|\n| **Application Instances** | Rate limit decisions, tier evaluations, algorithm execution | Correlation ID, user ID, timestamp, instance ID | 7 days for debugging, 30 days for pattern analysis |\n| **Redis Cluster** | Lua script execution, key access patterns, cluster topology changes | Redis key, operation timestamp, node ID | 3 days for performance analysis, longer for capacity planning |\n| **Load Balancer** | Request routing, instance health, traffic distribution | Request ID, backend instance, response codes | 24 hours for routing analysis |\n| **Monitoring Systems** | Performance metrics, alert triggers, system health indicators | Metric timestamps, alert correlation IDs | 90 days for trend analysis and capacity planning |\n\n**Clock Skew Detection and Compensation**\n\nOne of the most insidious debugging challenges in distributed rate limiting stems from clock skew between system components. When different nodes have slightly different time references, time-based rate limiting algorithms can produce inconsistent and confusing behavior that's difficult to diagnose without systematic time correlation.\n\nThe `TimeProvider` component implements clock skew detection and compensation to ensure consistent time references across the distributed system:\n\n| Method | Purpose | Implementation Approach | Error Handling |\n|--------|---------|------------------------|----------------|\n| `MeasureClockSkew(ctx context.Context) (time.Duration, error)` | Compare local time with Redis server time | Execute Redis TIME command and calculate difference from local clock | Retry on network errors, alert on skew > 100ms |\n| `Now() time.Time` | Provide skew-compensated current time | Apply measured compensation to local time | Fall back to local time on Redis unavailability |\n| `SyncWithRedis(ctx context.Context) error` | Periodic synchronization with authoritative time source | Background goroutine measuring skew every 30 seconds | Circuit breaker on persistent sync failures |\n\n**Debugging Workflow for Common Scenarios**\n\nEffective distributed debugging follows systematic workflows that guide investigation from initial symptom observation through root cause identification and resolution verification. These workflows account for the complexity of distributed system interactions while providing actionable steps for developers at different experience levels.\n\n**Scenario 1: Inconsistent Rate Limiting Across Instances**\n\nWhen the same user experiences different rate limiting behavior depending on which application instance handles their request, the root cause typically lies in state synchronization, configuration propagation, or local fallback activation.\n\nInvestigation Workflow:\n1. **Gather Request Context**: Collect correlation IDs, timestamps, and instance identifiers for both the allowed and denied requests\n2. **Verify Configuration Consistency**: Check that all instances have the same rate limiting rules and Redis configuration\n3. **Examine Redis Connectivity**: Confirm all instances maintain healthy connections to the same Redis cluster\n4. **Analyze Local Fallback Status**: Determine if any instances have activated local fallback due to Redis issues\n5. **Check Clock Synchronization**: Measure time differences between instances and Redis to identify skew issues\n6. **Trace Key Composition**: Verify that identical requests generate the same Redis keys across all instances\n\n**Scenario 2: Performance Degradation Under Load**\n\nWhen rate limiting performance degrades as traffic increases, the issue typically involves connection pool exhaustion, hot key concentration, or algorithmic complexity scaling problems.\n\nInvestigation Workflow:\n1. **Baseline Performance Measurement**: Establish normal response time distribution for rate limiting operations\n2. **Connection Pool Analysis**: Monitor Redis connection pool utilization and blocking statistics\n3. **Hot Key Identification**: Use Redis monitoring to identify keys receiving disproportionate traffic\n4. **Algorithm Performance Profiling**: Measure Lua script execution times under different load conditions\n5. **Network Latency Assessment**: Evaluate round-trip times to Redis cluster under load\n6. **Scaling Factor Analysis**: Determine how performance degradation correlates with traffic volume\n\n**Distributed Debugging Tools and Techniques**\n\n| Tool Category | Specific Tools | Use Case | Implementation Approach |\n|---------------|----------------|----------|------------------------|\n| **Log Aggregation** | ELK Stack, Fluentd, Loki | Centralized log collection and search | Ship logs with correlation IDs, use structured JSON format |\n| **Metrics Collection** | Prometheus, InfluxDB | Performance monitoring and alerting | Custom metrics for rate limiting operations, Redis health |\n| **Distributed Tracing** | Jaeger, Zipkin (sampled) | Complex request flow analysis | 1% sampling for detailed traces, correlation ID propagation |\n| **Redis Monitoring** | RedisInsight, Redis monitoring commands | Redis-specific debugging | Real-time key inspection, memory analysis, performance profiling |\n| **Network Analysis** | tcpdump, Wireshark, network monitoring | Network-level debugging | Packet capture for Redis communication, latency measurement |\n\n### Implementation Guidance\n\n**Technology Recommendations for Debugging Infrastructure**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Log Aggregation | Local log files with correlation IDs | ELK Stack with Fluentd collection |\n| Metrics Collection | Built-in Prometheus metrics | Custom metrics with Grafana dashboards |\n| Distributed Tracing | Structured logging with trace IDs | Jaeger with sampling |\n| Redis Monitoring | Redis CLI commands + scripts | RedisInsight + custom monitoring |\n| Debugging Tools | Go pprof + manual log analysis | Comprehensive observability platform |\n\n**Recommended File Structure for Debugging Infrastructure**\n\n```\nproject-root/\n  internal/\n    debugging/\n      correlation/\n        trace_context.go      ← correlation ID management\n        request_context.go    ← request tracing context\n      metrics/\n        collector.go          ← metrics collection\n        redis_metrics.go      ← Redis-specific metrics\n      logging/\n        structured_logger.go  ← structured logging utilities\n        correlation_logger.go ← correlation-aware logging\n      diagnostics/\n        redis_diagnostics.go  ← Redis debugging utilities\n        cluster_diagnostics.go ← cluster health checking\n        symptom_analyzer.go   ← automated symptom analysis\n  cmd/debug/\n    redis-cli.go             ← Redis debugging CLI tool\n    cluster-health.go        ← cluster health checker\n    trace-analyzer.go        ← correlation ID trace analysis\n  scripts/\n    redis-debug.sh           ← Redis debugging scripts\n    log-correlation.py       ← log correlation utilities\n```\n\n**Correlation ID Management Infrastructure**\n\n```go\n// CorrelationContext manages request correlation across distributed components\ntype CorrelationContext struct {\n    TraceID     string            `json:\"trace_id\"`\n    RequestID   string            `json:\"request_id\"`\n    UserID      string            `json:\"user_id,omitempty\"`\n    SessionID   string            `json:\"session_id,omitempty\"`\n    Metadata    map[string]string `json:\"metadata,omitempty\"`\n    StartTime   time.Time         `json:\"start_time\"`\n}\n\n// NewCorrelationContext creates a new correlation context with unique identifiers\nfunc NewCorrelationContext(userID string) *CorrelationContext {\n    // TODO: Generate unique trace ID using UUID or similar\n    // TODO: Create request ID for this specific request\n    // TODO: Capture start timestamp for duration calculation\n    // TODO: Initialize metadata map for additional context\n    // Hint: Use crypto/rand for secure random ID generation\n}\n\n// WithMetadata adds metadata to correlation context\nfunc (c *CorrelationContext) WithMetadata(key, value string) *CorrelationContext {\n    // TODO: Add key-value pair to metadata map\n    // TODO: Return context for method chaining\n    // Hint: Consider creating a copy to avoid mutation\n}\n\n// PropagateToHeaders converts correlation context to HTTP headers\nfunc (c *CorrelationContext) PropagateToHeaders() map[string]string {\n    // TODO: Convert correlation context to HTTP headers\n    // TODO: Use standard header names (X-Trace-ID, X-Request-ID)\n    // TODO: Serialize metadata as JSON if needed\n    // Hint: Follow OpenTracing header conventions\n}\n```\n\n**Structured Logging with Correlation**\n\n```go\n// CorrelationLogger provides correlation-aware structured logging\ntype CorrelationLogger struct {\n    logger    *logrus.Logger\n    component string\n}\n\n// NewCorrelationLogger creates a correlation-aware logger for a component\nfunc NewCorrelationLogger(component string) *CorrelationLogger {\n    // TODO: Initialize logrus logger with JSON formatter\n    // TODO: Set appropriate log level from environment\n    // TODO: Configure output destination (stdout/file)\n    // TODO: Store component name for automatic field injection\n    // Hint: Use logrus.JSONFormatter for structured output\n}\n\n// LogRateLimitDecision logs rate limiting decisions with full context\nfunc (l *CorrelationLogger) LogRateLimitDecision(\n    ctx *CorrelationContext,\n    result *RateLimitResult,\n    duration time.Duration,\n    redisKey string,\n) {\n    // TODO: Create log entry with correlation fields\n    // TODO: Add rate limiting specific fields (allowed, remaining, algorithm)\n    // TODO: Include performance metrics (duration, Redis key)\n    // TODO: Use appropriate log level based on result\n    // Hint: Include all fields needed for debugging and analysis\n}\n\n// LogRedisOperation logs Redis operations for debugging\nfunc (l *CorrelationLogger) LogRedisOperation(\n    ctx *CorrelationContext,\n    operation string,\n    key string,\n    duration time.Duration,\n    err error,\n) {\n    // TODO: Log Redis operation with timing and error information\n    // TODO: Include Redis key and operation type\n    // TODO: Use different log levels for success vs error\n    // TODO: Add Redis node information if available\n    // Hint: This helps correlate application logs with Redis logs\n}\n```\n\n**Redis Debugging Utilities**\n\n```go\n// RedisDebugger provides comprehensive Redis debugging capabilities\ntype RedisDebugger struct {\n    client redis.UniversalClient\n    logger *CorrelationLogger\n}\n\n// NewRedisDebugger creates Redis debugging utilities\nfunc NewRedisDebugger(client redis.UniversalClient) *RedisDebugger {\n    // TODO: Initialize debugger with Redis client\n    // TODO: Create correlation logger for Redis operations\n    // TODO: Set up health checking capabilities\n    // Hint: This will be used for manual debugging and automated diagnostics\n}\n\n// DiagnoseKeyState provides comprehensive analysis of a rate limiting key\nfunc (d *RedisDebugger) DiagnoseKeyState(ctx context.Context, key string) (*KeyDiagnostics, error) {\n    // TODO: Get current value and TTL for the key\n    // TODO: Calculate memory usage for the key\n    // TODO: Check if key exists in all expected hash ring nodes\n    // TODO: Measure access latency for the key\n    // TODO: Return comprehensive diagnostic information\n    // Hint: Use Redis MEMORY USAGE, TTL, and timing commands\n}\n\n// MeasureClusterHealth assesses overall Redis cluster health\nfunc (d *RedisDebugger) MeasureClusterHealth(ctx context.Context) (*ClusterHealth, error) {\n    // TODO: Check connectivity to all cluster nodes\n    // TODO: Measure response times for each node\n    // TODO: Check memory usage and key distribution\n    // TODO: Identify any failing or slow nodes\n    // TODO: Return overall cluster health assessment\n    // Hint: Use Redis CLUSTER commands and timing measurements\n}\n\n// AnalyzePerformanceIssues identifies common Redis performance problems\nfunc (d *RedisDebugger) AnalyzePerformanceIssues(ctx context.Context) (*PerformanceAnalysis, error) {\n    // TODO: Check for slow operations using SLOWLOG\n    // TODO: Analyze memory fragmentation and usage patterns\n    // TODO: Identify hot keys causing performance issues\n    // TODO: Check connection pool utilization\n    // TODO: Return analysis with recommended actions\n    // Hint: Combine multiple Redis diagnostic commands\n}\n```\n\n**Debugging Data Structures**\n\n| Structure | Fields | Purpose |\n|-----------|--------|---------|\n| `KeyDiagnostics` | `Key string, Exists bool, TTL time.Duration, MemoryUsage int64, AccessLatency time.Duration, NodeLocations []string` | Complete diagnostic information for a rate limiting key |\n| `ClusterHealth` | `TotalNodes int, HealthyNodes int, NodeStatuses map[string]*NodeStatus, AverageLatency time.Duration, KeyDistribution map[string]int` | Overall Redis cluster health assessment |\n| `PerformanceAnalysis` | `SlowOperations []SlowOperation, MemoryIssues []string, HotKeys []string, RecommendedActions []string` | Performance problem analysis and recommendations |\n| `NodeStatus` | `Address string, Reachable bool, Latency time.Duration, MemoryUsage float64, Role string, LastError error` | Individual Redis node status information |\n\n**Milestone Checkpoints**\n\n**Checkpoint 1: Basic Debugging Infrastructure (After Milestone 1)**\n- Command: `go test ./internal/debugging/... -v`\n- Expected: All debugging utilities compile and basic tests pass\n- Manual verification: Generate correlation IDs, verify structured logging output includes correlation fields\n- Signs of issues: Missing correlation fields in logs, UUID generation errors\n\n**Checkpoint 2: Redis Debugging Tools (After Milestone 3)**\n- Command: `./cmd/debug/redis-cli -operation=diagnose -key=test_key`\n- Expected: Comprehensive key diagnostics including memory usage, TTL, and access latency\n- Manual verification: Connect to Redis manually and compare diagnostic output with manual commands\n- Signs of issues: Timeout errors, missing Redis connectivity, incorrect key analysis\n\n**Checkpoint 3: Distributed Correlation (After Milestone 4)**\n- Command: Start multiple application instances and trace a request across them using correlation IDs\n- Expected: Same correlation ID appears in logs from all instances handling the request\n- Manual verification: Search logs for correlation ID, verify complete request flow is captured\n- Signs of issues: Missing correlation in some instances, broken correlation chain\n\n**Checkpoint 4: Performance Debugging (After Milestone 5)**\n- Command: `go test -bench=. -memprofile=mem.prof -cpuprofile=cpu.prof`\n- Expected: Performance profiles show debugging overhead < 1% of total execution time\n- Manual verification: Run load test with debugging enabled, verify minimal performance impact\n- Signs of issues: High debugging overhead, memory leaks in correlation tracking\n\n**Common Debugging Pitfalls and Solutions**\n\n⚠️ **Pitfall: Correlation ID Not Propagated**\nMany developers forget to propagate correlation IDs through all system boundaries, leading to broken trace chains. This happens when middleware doesn't extract IDs from headers or when background processes don't inherit correlation context. Solution: Create explicit correlation context types and ensure every system boundary explicitly handles correlation propagation.\n\n⚠️ **Pitfall: Clock Skew Ignored in Debugging**\nTime-based debugging assumes synchronized clocks, but distributed systems often have clock skew that makes event ordering confusing. Log timestamps from different nodes can't be directly compared without accounting for skew. Solution: Always include a time synchronization check in debugging workflows and normalize timestamps to a common reference.\n\n⚠️ **Pitfall: Debugging Overhead in Production**\nVerbose debugging significantly impacts rate limiting performance when left enabled in production. Detailed logging and tracing can double response times. Solution: Implement sampling for detailed traces (1-5% of requests) and use structured logging with configurable verbosity levels.\n\n⚠️ **Pitfall: Redis Debug Commands in Production**\nRunning Redis debugging commands like SLOWLOG or MEMORY USAGE during high load can impact Redis performance. These commands can block Redis briefly while gathering information. Solution: Use Redis replicas for debugging commands when possible, and limit debug command frequency during peak traffic.\n\n⚠️ **Pitfall: Incomplete Error Context**\nWhen rate limiting errors occur, developers often log the immediate error without sufficient context about the request, Redis state, or system conditions. This makes root cause analysis nearly impossible. Solution: Always include correlation context, request details, Redis key information, and system state in error logs.\n\n\n## Future Extensions\n\n> **Milestone(s):** Beyond current implementation scope - these extensions represent advanced capabilities that could be added after completing all five core milestones\n\nThe distributed rate limiter we've designed provides a solid foundation for production rate limiting needs. However, the landscape of distributed systems continues to evolve, presenting new challenges and opportunities. This section explores three major extensions that would significantly enhance the system's capabilities: adaptive rate limiting that responds dynamically to system conditions, geographic distribution for global scale applications, and seamless integration with modern service mesh architectures.\n\nThese extensions represent natural evolution paths as organizations scale their infrastructure and face increasingly complex operational requirements. Each extension builds upon the core architecture while introducing sophisticated new capabilities that address real-world operational challenges.\n\n### Adaptive Rate Limiting\n\n**Mental Model: Smart Traffic Light System**\n\nThink of adaptive rate limiting like a smart traffic management system in a busy city. Traditional traffic lights operate on fixed timers regardless of actual traffic conditions - they might keep cars waiting at a red light even when no cross traffic exists. Smart traffic lights, however, use sensors to detect real traffic patterns and adjust timing dynamically. Similarly, traditional rate limiting uses fixed thresholds regardless of system health, while adaptive rate limiting monitors system performance and adjusts limits based on actual capacity and response times.\n\nAdaptive rate limiting represents a fundamental shift from static resource protection to dynamic capacity management. Instead of enforcing predetermined limits regardless of system state, an adaptive system continuously monitors performance indicators and automatically adjusts rate limits to maintain optimal system health while maximizing throughput.\n\nThe core principle behind adaptive rate limiting involves establishing a feedback loop between system performance metrics and rate limit thresholds. When the system operates well below capacity with fast response times, rate limits can be relaxed to allow more traffic. Conversely, when response times increase or error rates climb, the system tightens limits to prevent cascade failures.\n\n> **Decision: Feedback Loop Architecture**\n> - **Context**: Static rate limits either waste capacity during low load or fail to protect during unexpected traffic spikes\n> - **Options Considered**: \n>   1. External monitoring system that updates rate limit rules via API\n>   2. Built-in adaptive controller that adjusts limits based on local metrics\n>   3. Machine learning-based predictor that forecasts optimal limits\n> - **Decision**: Built-in adaptive controller with configurable feedback algorithms\n> - **Rationale**: External systems introduce latency and complexity, while ML approaches require significant data and expertise. Built-in controllers provide real-time adaptation with predictable behavior.\n> - **Consequences**: Enables automatic scaling of rate limits but requires careful tuning to prevent oscillations\n\n#### Adaptive Algorithm Design\n\nThe adaptive rate limiting system operates through several interconnected components that monitor system health, calculate optimal limits, and gradually adjust thresholds to prevent shock changes that could destabilize the system.\n\n**Performance Metrics Collection**\n\nThe foundation of adaptive rate limiting lies in comprehensive performance monitoring. The system must collect metrics across multiple dimensions to understand system health holistically.\n\n| Metric Category | Specific Metrics | Collection Method | Update Frequency |\n|----------------|------------------|-------------------|------------------|\n| Response Time | P50, P95, P99 latency | Request timing instrumentation | Every 10 seconds |\n| Error Rate | 5xx errors, timeout rate | Response status tracking | Every 10 seconds |\n| System Resources | CPU utilization, memory usage | OS metrics collection | Every 30 seconds |\n| Queue Depth | Pending requests, worker utilization | Application metrics | Every 5 seconds |\n| Downstream Health | Dependency response times | Service mesh metrics | Every 15 seconds |\n\nThe metrics collection system must balance accuracy with overhead. High-frequency collection provides better responsiveness but consumes system resources. The adaptive controller uses exponential smoothing to reduce noise while maintaining sensitivity to genuine performance changes.\n\n**Control Algorithm Implementation**\n\nThe adaptive control algorithm implements a PID (Proportional-Integral-Derivative) controller that adjusts rate limits based on the difference between target and actual performance metrics.\n\nThe proportional component responds to current performance deviation from targets. If response times exceed the target by 20%, the proportional controller immediately reduces rate limits proportionally. The integral component addresses sustained deviations by accumulating error over time, ensuring persistent performance issues result in continued limit adjustments. The derivative component anticipates future performance by observing the rate of change, helping prevent overshoot when conditions improve rapidly.\n\n1. The controller samples current performance metrics every adjustment interval (typically 30-60 seconds)\n2. It calculates the error between actual performance and target thresholds for each monitored metric\n3. The proportional component computes immediate adjustment based on current error magnitude\n4. The integral component accumulates historical error to address persistent issues\n5. The derivative component considers error rate of change to anticipate trends\n6. All components combine to produce a rate limit adjustment factor\n7. The system applies adjustment gradually over multiple intervals to prevent shock changes\n8. Safety bounds prevent adjustments beyond configured minimum and maximum limits\n\n**Oscillation Prevention**\n\nOne of the primary challenges in adaptive systems involves preventing oscillations where limits bounce between high and low values. This occurs when the system overreacts to performance changes, creating instability rather than improvement.\n\nThe system implements several oscillation prevention mechanisms:\n\n**Hysteresis**: Different thresholds trigger increases versus decreases in rate limits. Response times must drop below 80% of the target to increase limits but must exceed 120% to decrease them. This prevents constant adjustments around the threshold.\n\n**Rate of Change Limits**: Adjustments cannot exceed 10% per interval regardless of error magnitude. Large performance changes trigger multiple small adjustments rather than dramatic swings.\n\n**Adjustment History**: The controller tracks recent adjustments and reduces sensitivity when frequent changes occur. If limits have been adjusted more than three times in the past hour, subsequent changes require larger error magnitudes.\n\n**Warmup Periods**: After any adjustment, the system waits for performance metrics to stabilize before considering additional changes. This prevents cascading adjustments based on transient effects.\n\n#### Integration with Existing Architecture\n\nAdaptive rate limiting integrates into the existing distributed rate limiter architecture through several key extension points that maintain backward compatibility while adding dynamic capabilities.\n\nThe `AdaptiveController` component sits between the metrics collection system and the rate limit rule management, automatically updating rule thresholds based on performance feedback.\n\n| Component | Interface | Responsibility |\n|-----------|-----------|----------------|\n| MetricsCollector | `CollectPerformanceMetrics() *PerformanceSnapshot` | Gather system performance indicators |\n| AdaptiveController | `UpdateLimits(ctx context.Context, metrics *PerformanceSnapshot) error` | Calculate optimal rate limits |\n| ControlAlgorithm | `CalculateAdjustment(current, target PerformanceMetrics) float64` | PID control algorithm implementation |\n| SafetyBounds | `ValidateAdjustment(currentLimit, proposedLimit int64) int64` | Prevent dangerous limit changes |\n\nThe adaptive system extends the existing `RateLimitRule` structure with adaptive configuration fields while maintaining compatibility with static rules.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| adaptive_enabled | bool | Whether this rule participates in adaptive adjustment |\n| target_p99_latency | time.Duration | Maximum acceptable P99 response time |\n| target_error_rate | float64 | Maximum acceptable error rate percentage |\n| min_limit | int64 | Absolute minimum requests allowed per window |\n| max_limit | int64 | Absolute maximum requests allowed per window |\n| adjustment_sensitivity | float64 | Multiplier for control algorithm responsiveness |\n\n#### Common Pitfalls in Adaptive Systems\n\n⚠️ **Pitfall: Rapid Oscillation**\nMany adaptive implementations suffer from oscillation where limits bounce rapidly between high and low values. This occurs when the system overreacts to temporary performance changes without considering adjustment history. The fix involves implementing hysteresis with different thresholds for increasing versus decreasing limits, and rate-limiting the frequency of adjustments.\n\n⚠️ **Pitfall: Cascade Failures During Traffic Spikes**\nWhen traffic suddenly increases, response times may spike temporarily even if the system can handle the load. Poorly tuned adaptive systems may aggressively reduce limits, creating artificial scarcity that prevents the system from recovering. The solution requires distinguishing between temporary load spikes and genuine capacity problems by considering multiple metrics simultaneously.\n\n⚠️ **Pitfall: Ignoring Downstream Dependencies**\nAdaptive systems that only monitor local performance may miss downstream bottlenecks. When a database becomes slow, the adaptive system might increase local rate limits, worsening the downstream problem. Comprehensive adaptive systems must monitor the health of all critical dependencies and adjust limits based on the weakest link in the chain.\n\n### Geographic Distribution\n\n**Mental Model: Global Banking Network**\n\nConsider how major banks operate across different countries and time zones. Each branch maintains local cash reserves for immediate customer needs, but they're all connected to regional centers that can transfer funds when needed. Customer account balances must be consistent globally - you can't withdraw the same money from ATMs in New York and London simultaneously. However, the system tolerates brief inconsistencies during transfers as long as the final state is correct. Geographic rate limiting works similarly, with local enforcement for immediate decisions and eventual consistency for global accuracy.\n\nGeographic distribution extends the distributed rate limiter to operate across multiple geographic regions, each potentially containing multiple data centers. This architecture addresses the needs of global applications that serve users worldwide while maintaining rate limiting accuracy across regions.\n\nThe fundamental challenge in geographic distribution lies in balancing consistency with performance. Users expect sub-100ms response times regardless of their location, but maintaining perfect global consistency for rate limits would require cross-region network calls that add hundreds of milliseconds of latency.\n\nThe solution involves a hybrid approach combining local enforcement with asynchronous global reconciliation. Each region maintains local rate limit state that enables immediate decisions, while background processes synchronize state across regions to maintain global accuracy over time.\n\n#### Multi-Region Architecture Design\n\nThe geographic distribution architecture consists of three primary layers: regional clusters, global coordination, and conflict resolution mechanisms.\n\n**Regional Cluster Organization**\n\nEach geographic region operates as an independent rate limiting cluster with complete functionality for serving local traffic. Regional clusters contain their own Redis shards, application instances, and management APIs.\n\n| Component | Regional Instance | Global Coordination |\n|-----------|------------------|-------------------|\n| Redis Cluster | 3-5 nodes per region | Cross-region replication streams |\n| Application Instances | Serve local traffic only | Participate in global state sync |\n| Management API | Regional configuration | Global rule propagation |\n| Monitoring Dashboard | Regional metrics | Aggregated global view |\n\nRegional clusters implement the complete distributed rate limiting architecture described in previous sections. They can operate independently even when network connectivity to other regions fails, ensuring local users experience no service degradation during network partitions.\n\n**Global State Synchronization**\n\nGlobal state synchronization operates through asynchronous replication streams that propagate rate limit usage information between regions. Unlike traditional database replication, rate limiting synchronization focuses on aggregate consumption rather than individual request tracking.\n\nThe synchronization protocol operates on a periodic basis (typically every 30-60 seconds) where each region reports its usage statistics to other regions:\n\n1. Each regional cluster aggregates rate limit usage across all local keys and time windows\n2. Usage reports include consumed tokens, timestamp ranges, and region identifiers\n3. Reports transmit to other regions through reliable message queues or direct Redis streams\n4. Receiving regions incorporate remote usage into their local enforcement decisions\n5. Conflict resolution algorithms handle cases where global usage exceeds configured limits\n6. Corrective actions redistribute quota or temporarily tighten local enforcement\n\n**Eventual Consistency Model**\n\nThe geographic distribution system operates under an eventual consistency model where rate limits may temporarily exceed global thresholds during network partitions or high cross-region latency, but converge to correct enforcement as synchronization catches up.\n\n| Consistency Guarantee | Local Enforcement | Global Convergence |\n|---------------------|-------------------|-------------------|\n| Strong Consistency | Not achievable without unacceptable latency | Not suitable for user-facing responses |\n| Eventual Consistency | Immediate decisions based on local + cached remote state | Global limits enforced within sync interval |\n| Bounded Staleness | Local decisions with maximum staleness bounds | Guarantees global limit violations stay within bounds |\n\nThe bounded staleness approach provides the best balance for most applications. Local enforcement uses recently synchronized global state (maximum 2-3 minutes stale) combined with current local usage to make decisions. This ensures global violations remain bounded even during extended network partitions.\n\n#### Cross-Region Synchronization Protocol\n\nThe synchronization protocol ensures efficient propagation of rate limiting state while handling network failures, region outages, and conflicting updates from multiple sources.\n\n**Incremental State Transfer**\n\nRather than synchronizing complete rate limiting state, the protocol transmits incremental updates focusing on consumed quota within specific time windows.\n\n| Message Type | Contents | Frequency | Size |\n|--------------|----------|-----------|------|\n| Usage Update | Key, consumed tokens, window, timestamp | Every 60 seconds | <1KB per key |\n| Heartbeat | Region health, sync lag, active keys | Every 30 seconds | <100 bytes |\n| Quota Adjustment | Key, new global limit, reason | On configuration change | <500 bytes |\n| Conflict Resolution | Key, authoritative state, reconciliation | When conflicts detected | <2KB |\n\nUsage updates contain aggregated consumption data rather than individual request logs. For a key with 1000 requests in the last minute, the system sends a single update indicating \"1000 tokens consumed\" rather than 1000 individual records.\n\n**Conflict Detection and Resolution**\n\nConflicts arise when the sum of regional usage reports exceeds the configured global limit for a key. This can occur during network partitions where regions operate independently or during traffic spikes that exceed synchronization frequency.\n\nThe conflict resolution protocol implements a priority-based approach:\n\n1. **Detection Phase**: Each region compares local + remote usage against global limits during sync processing\n2. **Reporting Phase**: Regions experiencing conflicts broadcast conflict notifications to all other regions\n3. **Priority Resolution**: Regions use deterministic priority rules (timestamp, region ID) to determine authoritative state\n4. **Reconciliation Phase**: Non-authoritative regions adjust their local enforcement to compensate for over-consumption\n5. **Recovery Phase**: Gradual restoration of normal enforcement as usage patterns stabilize\n\n**Network Partition Handling**\n\nNetwork partitions between regions represent one of the most challenging scenarios for geographic distribution. The system must continue serving users in each region while preventing excessive global limit violations.\n\nDuring partition detection (missed heartbeats, failed sync operations), each region transitions to conservative enforcement mode:\n\n- Local rate limits reduce by a safety margin (typically 20-30%) to compensate for unknown remote usage\n- Cached remote state continues informing decisions but with increasing skepticism over time\n- Critical keys (those approaching limits) receive more aggressive local enforcement\n- Non-critical keys maintain normal enforcement to avoid unnecessary user impact\n\nWhen partitions heal, regions exchange complete state snapshots to reconcile any conflicts that occurred during isolation. The reconciliation process prioritizes preserving user experience over perfect quota enforcement, typically allowing temporary over-consumption rather than retroactively blocking users.\n\n#### Common Pitfalls in Geographic Distribution\n\n⚠️ **Pitfall: Ignoring Network Latency Variations**\nMany geographic distribution implementations assume consistent network latency between regions. In reality, trans-Pacific links may have 300ms latency while trans-Atlantic links have 150ms. This variation affects synchronization timing and can cause some regions to operate with staler data than others. The solution involves adaptive sync intervals based on measured network latency and prioritizing synchronization between high-latency region pairs.\n\n⚠️ **Pitfall: Global Limits That Are Too Restrictive**\nSetting global limits equal to the sum of regional capacity ignores cross-regional traffic distribution. If Europe has 40% of global users but only 25% of quota, European users will experience unnecessary blocking while other regions have unused capacity. Effective geographic distribution requires quota allocation that matches actual traffic distribution with automatic rebalancing capabilities.\n\n⚠️ **Pitfall: Insufficient Partition Tolerance Testing**\nGeographic distribution systems often work perfectly during testing with reliable networks but fail catastrophically during real network partitions. Comprehensive testing must simulate various partition scenarios: complete isolation, asymmetric partitions where region A can reach B but not vice versa, and flapping connections that repeatedly partition and heal.\n\n### Service Mesh Integration\n\n**Mental Model: Airport Security Integration**\n\nConsider how security checkpoints integrate into airport operations. Rather than requiring every airline to implement their own security protocols, airports provide centralized security infrastructure that all flights use transparently. Passengers don't need to understand different security systems - they simply pass through standardized checkpoints. Similarly, service mesh integration allows applications to benefit from rate limiting without implementing rate limiting logic themselves. The mesh intercepts traffic transparently and applies policies consistently across all services.\n\nService mesh integration represents a paradigm shift from application-embedded rate limiting to infrastructure-level policy enforcement. Instead of each service implementing its own rate limiting logic, the service mesh (Istio, Linkerd, or Consul Connect) intercepts network traffic and applies rate limiting policies transparently.\n\nThis approach offers significant operational advantages: consistent policy enforcement across all services, centralized configuration management, and the ability to apply rate limiting to legacy applications without code changes. However, it also introduces new complexity in policy management and debugging distributed policy enforcement.\n\n#### Envoy Proxy Integration Design\n\nMost service meshes use Envoy proxy as their data plane component, making Envoy integration the primary path for service mesh rate limiting support. Envoy provides several extension points for implementing custom rate limiting logic.\n\n**Rate Limiting Filter Architecture**\n\nEnvoy implements rate limiting through HTTP filters that intercept requests before they reach upstream services. The distributed rate limiter integrates as an external authorization service that Envoy queries for each request.\n\n| Envoy Component | Integration Point | Responsibility |\n|----------------|-------------------|----------------|\n| HTTP Connection Manager | Rate Limit Filter | Intercepts requests and queries rate limiting service |\n| Cluster Manager | Rate Limit Service Cluster | Manages connections to rate limiting service instances |\n| Admin Interface | Statistics and Health | Exposes rate limiting metrics and health status |\n| Configuration API | Dynamic Updates | Receives rate limiting policy updates |\n\nThe integration follows Envoy's external authorization pattern where the proxy sends request context to the rate limiting service and waits for an allow/deny decision. This design maintains clean separation between traffic proxying and policy enforcement.\n\n**Request Context Extraction**\n\nThe service mesh integration must extract relevant rate limiting context from HTTP requests and connection metadata. Unlike application-level integration where context is readily available, proxy-level integration must infer context from network-level information.\n\n| Context Source | Available Information | Rate Limiting Application |\n|---------------|----------------------|---------------------------|\n| HTTP Headers | User-Agent, Authorization, Custom headers | User identification, API versioning |\n| Connection Metadata | Source IP, Destination service | IP-based limiting, service quotas |\n| TLS Certificate | Client certificate CN, SAN | Certificate-based user identification |\n| Request Path | URL path, query parameters | API endpoint classification |\n| Service Labels | Kubernetes labels, service mesh metadata | Service-to-service rate limiting |\n\nThe context extraction process operates through configurable rules that map network-level information to rate limiting dimensions. For example, a rule might extract user ID from the \"X-User-ID\" header and map it to per-user rate limiting, while another rule uses the destination service name for global service rate limiting.\n\n**Policy Configuration Integration**\n\nService mesh policy configuration differs significantly from application-level configuration. Instead of managing rate limiting rules through dedicated APIs, policies integrate with service mesh configuration systems like Istio's VirtualService and DestinationRule resources.\n\n```yaml\n# Example Istio integration (for illustration - actual implementation in code section)\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: api-rate-limiting\nspec:\n  hosts:\n  - api.example.com\n  http:\n  - match:\n    - uri:\n        prefix: /api/v1\n    route:\n    - destination:\n        host: api-service\n    rateLimiting:\n      algorithm: token_bucket\n      limit: 1000\n      window: 60s\n      dimensions:\n      - header: \"x-user-id\"\n        tier: user\n      - source_ip: {}\n        tier: ip\n```\n\nThis approach provides several advantages over standalone rate limiting services: configuration lives alongside routing rules, policies automatically apply to matching traffic, and service mesh tools provide policy validation and deployment workflows.\n\n#### Transparent Policy Enforcement\n\nTransparent policy enforcement ensures applications receive rate limiting protection without requiring code changes or awareness of rate limiting infrastructure. This transparency extends to multiple aspects of the system: request handling, error responses, and monitoring integration.\n\n**Request Flow Transparency**\n\nFrom the application's perspective, rate limiting enforcement appears as natural network behavior rather than explicit policy enforcement. Applications see either successful request delivery or standard HTTP error responses - they don't need special handling for rate limiting scenarios.\n\nThe transparency requirement affects several design decisions:\n\n1. **Response Headers**: Rate limiting information (remaining quota, retry timing) appears in standard HTTP headers that applications can optionally consume\n2. **Error Codes**: Rate limiting uses standard HTTP 429 (Too Many Requests) responses rather than custom error formats\n3. **Latency Impact**: Rate limiting decisions complete within microseconds to avoid noticeable request latency\n4. **Circuit Breaker Integration**: When rate limiting services fail, traffic passes through normally rather than blocking\n\n**Policy Inheritance and Precedence**\n\nService mesh environments often have complex service hierarchies with multiple applicable policies. The rate limiting system must resolve policy conflicts and inheritance in predictable ways.\n\n| Policy Scope | Precedence Level | Example Application |\n|-------------|------------------|-------------------|\n| Global Mesh | Lowest | Default rate limiting for all services |\n| Namespace | Medium | Rate limiting for all services in a namespace |\n| Service | High | Rate limiting specific to one service |\n| Route | Highest | Rate limiting for specific API endpoints |\n\nHigher precedence policies override lower precedence ones, but the system also supports policy composition where multiple policies can apply to the same request. For example, a global policy might set overall limits while a service-specific policy adds burst handling.\n\n**Error Response Customization**\n\nDifferent services may require different error response formats when rate limiting blocks requests. API services might expect JSON error responses, while web applications might prefer HTML error pages with user-friendly messages.\n\nThe service mesh integration provides configurable error response templates that can adapt to service requirements:\n\n| Service Type | Response Format | Headers | Body Content |\n|-------------|----------------|---------|--------------|\n| REST API | JSON | application/json | {\"error\": \"rate_limit_exceeded\", \"retry_after\": 60} |\n| GraphQL | JSON | application/json | {\"errors\": [{\"message\": \"Rate limit exceeded\"}]} |\n| Web Service | HTML | text/html | User-friendly error page with retry guidance |\n| gRPC | Status | application/grpc | RESOURCE_EXHAUSTED with retry metadata |\n\n#### Observability and Debugging\n\nService mesh integration introduces additional complexity in observability since rate limiting decisions occur in infrastructure rather than application code. Comprehensive observability becomes critical for debugging policy enforcement and understanding traffic patterns.\n\n**Distributed Tracing Integration**\n\nRate limiting decisions must integrate with distributed tracing systems (Jaeger, Zipkin) to provide visibility into policy enforcement within request traces. Each rate limiting decision appears as a span within the overall request trace.\n\n| Trace Information | Content | Purpose |\n|------------------|---------|---------|\n| Span Name | \"rate_limiting_check\" | Identifies rate limiting operations in traces |\n| Span Tags | rule_id, algorithm, tier, decision | Provides context for rate limiting decisions |\n| Span Logs | quota_remaining, rule_evaluation_time | Detailed information for debugging |\n| Span Status | OK (allowed) or ERROR (denied) | Quick visual indication of decision |\n\nThe tracing integration helps operators understand why specific requests were rate limited and identify patterns in rate limiting decisions across different services.\n\n**Metrics Aggregation Across Services**\n\nUnlike application-embedded rate limiting where each service has its own metrics, service mesh rate limiting must aggregate metrics across all services while maintaining the ability to drill down into specific service behavior.\n\nThe metrics system provides multiple aggregation levels:\n\n- **Mesh-wide**: Total requests, rate limiting decisions, error rates across all services\n- **Service-level**: Rate limiting metrics per service, including top rate limited endpoints\n- **Policy-level**: Effectiveness metrics for each rate limiting policy\n- **Client-level**: Rate limiting behavior per client across all services they access\n\nThese aggregated metrics enable both high-level mesh monitoring and detailed debugging of specific rate limiting issues.\n\n#### Common Pitfalls in Service Mesh Integration\n\n⚠️ **Pitfall: Configuration Complexity**\nService mesh rate limiting can quickly become complex with multiple policy layers, inheritance rules, and service-specific configurations. Teams often create contradictory policies or policies that interact in unexpected ways. The solution involves policy validation tools that check for conflicts and comprehensive testing of policy interactions before deployment.\n\n⚠️ **Pitfall: Performance Impact on Critical Path**\nEvery request must wait for rate limiting decisions, making performance critical. Poorly optimized integrations can add tens of milliseconds to request latency, violating SLA requirements. High-performance integration requires connection pooling, efficient serialization, and aggressive caching of rate limiting decisions.\n\n⚠️ **Pitfall: Debugging Distributed Policies**\nWhen rate limiting occurs in the service mesh, applications may not have visibility into why requests fail. This makes debugging customer issues extremely difficult. Comprehensive logging, tracing integration, and clear error messages become essential for operational success.\n\n### Implementation Guidance\n\nThe future extensions represent advanced capabilities that build upon the core distributed rate limiter foundation. Each extension requires careful architectural decisions and implementation strategies to maintain system reliability while adding sophisticated new features.\n\n#### Technology Recommendations\n\n| Extension Area | Simple Option | Advanced Option |\n|---------------|---------------|-----------------|\n| Adaptive Control | Basic PID controller with manual tuning | Machine learning-based limit prediction with auto-tuning |\n| Geographic Sync | Direct Redis replication streams | Apache Kafka-based event streaming with exactly-once semantics |\n| Service Mesh | Envoy HTTP filter with REST API calls | Native Envoy gRPC integration with connection multiplexing |\n| Metrics Collection | Prometheus metrics with periodic scraping | OpenTelemetry with real-time streaming to observability platforms |\n| Configuration | YAML files with manual deployment | GitOps workflow with policy validation and gradual rollouts |\n\n#### Recommended Module Structure\n\nFuture extensions integrate into the existing codebase through new packages that maintain clear separation of concerns:\n\n```\ndistributed-rate-limiter/\n  internal/\n    adaptive/              ← adaptive rate limiting\n      controller.go        ← PID control algorithm\n      metrics.go          ← performance metrics collection\n      safety.go           ← oscillation prevention\n    geographic/           ← multi-region distribution\n      sync.go             ← cross-region synchronization\n      partition.go        ← network partition handling\n      conflict.go         ← conflict resolution\n    servicemesh/          ← service mesh integration\n      envoy/              ← Envoy proxy integration\n        filter.go         ← HTTP filter implementation\n        config.go         ← policy configuration\n      istio/              ← Istio-specific integrations\n        policies.go       ← VirtualService integration\n      observability.go    ← tracing and metrics\n    extensions/           ← shared extension utilities\n      timeutil.go         ← time synchronization utilities\n      configutil.go      ← configuration validation\n```\n\n#### Adaptive Rate Limiting Starter Code\n\nThe adaptive controller provides the foundation for dynamic rate limit adjustment based on system performance metrics.\n\n```go\n// Package adaptive provides dynamic rate limit adjustment based on system performance\npackage adaptive\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/distributed-rate-limiter/internal/config\"\n    \"github.com/distributed-rate-limiter/internal/metrics\"\n)\n\n// PerformanceMetrics represents current system performance indicators\ntype PerformanceMetrics struct {\n    P50Latency      time.Duration `json:\"p50_latency\"`\n    P95Latency      time.Duration `json:\"p95_latency\"`\n    P99Latency      time.Duration `json:\"p99_latency\"`\n    ErrorRate       float64       `json:\"error_rate\"`\n    CPUUtilization  float64       `json:\"cpu_utilization\"`\n    MemoryUsage     float64       `json:\"memory_usage\"`\n    QueueDepth      int64         `json:\"queue_depth\"`\n    Timestamp       time.Time     `json:\"timestamp\"`\n}\n\n// AdaptiveConfig defines configuration for adaptive rate limiting behavior\ntype AdaptiveConfig struct {\n    Enabled             bool          `yaml:\"enabled\"`\n    AdjustmentInterval  time.Duration `yaml:\"adjustment_interval\"`\n    TargetP99Latency   time.Duration `yaml:\"target_p99_latency\"`\n    TargetErrorRate    float64       `yaml:\"target_error_rate\"`\n    MaxAdjustmentRate  float64       `yaml:\"max_adjustment_rate\"`\n    ProportionalGain   float64       `yaml:\"proportional_gain\"`\n    IntegralGain       float64       `yaml:\"integral_gain\"`\n    DerivativeGain     float64       `yaml:\"derivative_gain\"`\n}\n\n// PIDController implements proportional-integral-derivative control for rate limit adjustment\ntype PIDController struct {\n    mu              sync.RWMutex\n    config          AdaptiveConfig\n    previousError   float64\n    integralError   float64\n    lastUpdate      time.Time\n    adjustmentHistory []AdjustmentRecord\n}\n\n// AdjustmentRecord tracks historical rate limit adjustments for oscillation prevention\ntype AdjustmentRecord struct {\n    Timestamp       time.Time `json:\"timestamp\"`\n    PreviousLimit   int64     `json:\"previous_limit\"`\n    NewLimit        int64     `json:\"new_limit\"`\n    AdjustmentRatio float64   `json:\"adjustment_ratio\"`\n    TriggerReason   string    `json:\"trigger_reason\"`\n}\n\n// AdaptiveController manages dynamic rate limit adjustments based on system performance\ntype AdaptiveController struct {\n    mu                sync.RWMutex\n    config            AdaptiveConfig\n    pidController     *PIDController\n    ruleManager       *config.RuleManager\n    metricsCollector  *metrics.Collector\n    adjustmentTimer   *time.Timer\n    ctx               context.Context\n    cancelFn          context.CancelFunc\n}\n\n// NewAdaptiveController creates a new adaptive rate limiting controller\nfunc NewAdaptiveController(config AdaptiveConfig, ruleManager *config.RuleManager, \n                          metricsCollector *metrics.Collector) *AdaptiveController {\n    // TODO: Initialize PID controller with configured gains\n    // TODO: Set up adjustment timer based on configured interval\n    // TODO: Create cancellable context for graceful shutdown\n    // TODO: Initialize adjustment history tracking\n    return nil\n}\n\n// Start begins adaptive rate limit adjustment processing\nfunc (ac *AdaptiveController) Start(ctx context.Context) error {\n    // TODO: Start background goroutine for periodic adjustment evaluation\n    // TODO: Set up metrics collection integration\n    // TODO: Initialize baseline performance measurements\n    // TODO: Begin adjustment timer\n    return nil\n}\n\n// Stop gracefully shuts down adaptive processing\nfunc (ac *AdaptiveController) Stop() error {\n    // TODO: Cancel background processing context\n    // TODO: Stop adjustment timer\n    // TODO: Flush any pending adjustments\n    return nil\n}\n\n// EvaluateAdjustments analyzes current performance and determines necessary rate limit changes\nfunc (ac *AdaptiveController) EvaluateAdjustments(ctx context.Context) error {\n    // TODO: Collect current performance metrics\n    // TODO: Compare against target thresholds\n    // TODO: Calculate PID controller output\n    // TODO: Apply safety bounds and oscillation prevention\n    // TODO: Update rate limit rules if adjustment needed\n    // TODO: Record adjustment in history\n    return nil\n}\n\n// CalculatePIDOutput computes rate limit adjustment using PID control algorithm\nfunc (pc *PIDController) CalculatePIDOutput(current, target PerformanceMetrics) float64 {\n    // TODO: Calculate error between current and target performance\n    // TODO: Compute proportional term based on current error\n    // TODO: Update and compute integral term based on accumulated error\n    // TODO: Compute derivative term based on error rate of change\n    // TODO: Combine P, I, and D terms with configured gains\n    // TODO: Apply output limits to prevent excessive adjustments\n    return 0.0\n}\n```\n\n#### Geographic Distribution Starter Code\n\nThe geographic synchronization system enables rate limit state sharing across multiple regions with eventual consistency guarantees.\n\n```go\n// Package geographic provides multi-region rate limiting with eventual consistency\npackage geographic\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/go-redis/redis/v8\"\n    \"github.com/distributed-rate-limiter/internal/storage\"\n)\n\n// RegionConfig defines configuration for a geographic region\ntype RegionConfig struct {\n    RegionID        string   `yaml:\"region_id\"`\n    RedisAddresses  []string `yaml:\"redis_addresses\"`\n    SyncInterval    time.Duration `yaml:\"sync_interval\"`\n    PeerRegions     []PeerRegion `yaml:\"peer_regions\"`\n    SyncTimeout     time.Duration `yaml:\"sync_timeout\"`\n    ConflictResolution string `yaml:\"conflict_resolution\"`\n}\n\n// PeerRegion defines connection information for remote regions\ntype PeerRegion struct {\n    RegionID    string `yaml:\"region_id\"`\n    SyncAddress string `yaml:\"sync_address\"`\n    Priority    int    `yaml:\"priority\"`\n}\n\n// UsageReport represents rate limit consumption data for cross-region synchronization\ntype UsageReport struct {\n    RegionID      string                    `json:\"region_id\"`\n    Timestamp     time.Time                 `json:\"timestamp\"`\n    WindowStart   time.Time                 `json:\"window_start\"`\n    WindowEnd     time.Time                 `json:\"window_end\"`\n    KeyUsage      map[string]int64          `json:\"key_usage\"`\n    Sequence      int64                     `json:\"sequence\"`\n}\n\n// ConflictReport indicates detected inconsistencies in global rate limit state\ntype ConflictReport struct {\n    Key           string                    `json:\"key\"`\n    GlobalLimit   int64                     `json:\"global_limit\"`\n    ReportedUsage map[string]int64          `json:\"reported_usage\"`\n    TotalUsage    int64                     `json:\"total_usage\"`\n    Excess        int64                     `json:\"excess\"`\n    Resolution    string                    `json:\"resolution\"`\n    Timestamp     time.Time                 `json:\"timestamp\"`\n}\n\n// GeographicSyncManager coordinates rate limiting state across multiple regions\ntype GeographicSyncManager struct {\n    mu              sync.RWMutex\n    config          RegionConfig\n    localStorage    storage.Storage\n    peerConnections map[string]*redis.Client\n    usageBuffer     []UsageReport\n    conflictResolver *ConflictResolver\n    syncTicker      *time.Ticker\n    ctx             context.Context\n    cancelFn        context.CancelFunc\n}\n\n// ConflictResolver handles inconsistencies in global rate limit state\ntype ConflictResolver struct {\n    mu              sync.RWMutex\n    resolutionStrategy string\n    priorityMap     map[string]int\n    conflictHistory []ConflictReport\n}\n\n// NewGeographicSyncManager creates a new geographic synchronization manager\nfunc NewGeographicSyncManager(config RegionConfig, localStorage storage.Storage) *GeographicSyncManager {\n    // TODO: Initialize peer region connections\n    // TODO: Create conflict resolver with configured strategy\n    // TODO: Set up usage report buffering\n    // TODO: Initialize sync ticker with configured interval\n    return nil\n}\n\n// Start begins cross-region synchronization processing\nfunc (gsm *GeographicSyncManager) Start(ctx context.Context) error {\n    // TODO: Establish connections to all peer regions\n    // TODO: Start background sync processing goroutine\n    // TODO: Begin periodic usage report generation\n    // TODO: Start conflict detection and resolution\n    return nil\n}\n\n// SynchronizeUsage sends local usage reports to peer regions and processes incoming reports\nfunc (gsm *GeographicSyncManager) SynchronizeUsage(ctx context.Context) error {\n    // TODO: Generate usage report from local Redis state\n    // TODO: Send usage report to all peer regions\n    // TODO: Receive and process usage reports from peers\n    // TODO: Detect conflicts between reported usage and global limits\n    // TODO: Trigger conflict resolution if needed\n    return nil\n}\n\n// ProcessPeerReport incorporates usage information from a remote region\nfunc (gsm *GeographicSyncManager) ProcessPeerReport(report UsageReport) error {\n    // TODO: Validate report authenticity and sequence\n    // TODO: Merge remote usage into local global state view\n    // TODO: Check for conflicts with global rate limits\n    // TODO: Update local enforcement based on global usage\n    return nil\n}\n\n// DetectConflicts identifies cases where global usage exceeds configured limits\nfunc (cr *ConflictResolver) DetectConflicts(reports map[string]UsageReport, \n                                          globalLimits map[string]int64) []ConflictReport {\n    // TODO: Sum reported usage across all regions for each key\n    // TODO: Compare total usage against configured global limits\n    // TODO: Identify keys where usage exceeds limits\n    // TODO: Calculate excess amount and affected regions\n    return nil\n}\n\n// ResolveConflict applies conflict resolution strategy to restore global consistency\nfunc (cr *ConflictResolver) ResolveConflict(ctx context.Context, conflict ConflictReport) error {\n    // TODO: Apply configured resolution strategy (priority-based, proportional reduction, etc.)\n    // TODO: Calculate corrective actions for each region\n    // TODO: Send adjustment commands to affected regions\n    // TODO: Record conflict resolution in history\n    return nil\n}\n```\n\n#### Service Mesh Integration Starter Code\n\nThe service mesh integration provides transparent rate limiting through Envoy proxy filters and Istio policy integration.\n\n```go\n// Package servicemesh provides transparent rate limiting through service mesh integration\npackage servicemesh\n\nimport (\n    \"context\"\n    \"net/http\"\n    \"time\"\n    \n    envoy_service_ratelimit_v3 \"github.com/envoyproxy/go-control-plane/envoy/service/ratelimit/v3\"\n    \"google.golang.org/grpc\"\n    \n    \"github.com/distributed-rate-limiter/internal/ratelimit\"\n)\n\n// EnvoyRateLimitService implements Envoy's RateLimitService interface\ntype EnvoyRateLimitService struct {\n    limiter           ratelimit.DistributedLimiter\n    contextExtractor  *RequestContextExtractor\n    policyManager     *PolicyManager\n    metricsCollector  *ServiceMeshMetrics\n}\n\n// RequestContextExtractor extracts rate limiting context from Envoy request metadata\ntype RequestContextExtractor struct {\n    extractionRules   map[string]ExtractionRule\n    defaultContext    DefaultContextConfig\n}\n\n// ExtractionRule defines how to extract rate limiting dimensions from requests\ntype ExtractionRule struct {\n    Dimension     string            `yaml:\"dimension\"`\n    Source        string            `yaml:\"source\"`        // header, path, query, metadata\n    Key           string            `yaml:\"key\"`           // header name, path pattern, etc.\n    Transformer   string            `yaml:\"transformer\"`   // regex, hash, lookup\n    DefaultValue  string            `yaml:\"default_value\"`\n}\n\n// PolicyManager handles service mesh rate limiting policy configuration\ntype PolicyManager struct {\n    mu              sync.RWMutex\n    policies        map[string]*ServiceMeshPolicy\n    istioClient     *istio.Client\n    configWatcher   *ConfigWatcher\n}\n\n// ServiceMeshPolicy represents rate limiting policy in service mesh context\ntype ServiceMeshPolicy struct {\n    Name            string                    `yaml:\"name\"`\n    Namespace       string                    `yaml:\"namespace\"`\n    Services        []string                  `yaml:\"services\"`\n    Routes          []RouteMatch              `yaml:\"routes\"`\n    RateLimiting    RateLimitingSpec          `yaml:\"rate_limiting\"`\n    Priority        int                       `yaml:\"priority\"`\n    Enabled         bool                      `yaml:\"enabled\"`\n}\n\n// RouteMatch defines traffic matching criteria for policy application\ntype RouteMatch struct {\n    PathRegex       string            `yaml:\"path_regex\"`\n    Methods         []string          `yaml:\"methods\"`\n    Headers         map[string]string `yaml:\"headers\"`\n    QueryParams     map[string]string `yaml:\"query_params\"`\n}\n\n// RateLimitingSpec defines rate limiting configuration within service mesh policy\ntype RateLimitingSpec struct {\n    Algorithm       string                    `yaml:\"algorithm\"`\n    Limit           int64                     `yaml:\"limit\"`\n    Window          time.Duration             `yaml:\"window\"`\n    Dimensions      []RateLimitDimension      `yaml:\"dimensions\"`\n    ErrorResponse   ErrorResponseConfig       `yaml:\"error_response\"`\n}\n\n// RateLimitDimension defines a dimension for rate limiting (user, IP, service, etc.)\ntype RateLimitDimension struct {\n    Name            string            `yaml:\"name\"`\n    Extractor       ExtractionRule    `yaml:\"extractor\"`\n    Tier            string            `yaml:\"tier\"`\n}\n\n// ErrorResponseConfig customizes error responses when rate limits are exceeded\ntype ErrorResponseConfig struct {\n    StatusCode      int               `yaml:\"status_code\"`\n    ContentType     string            `yaml:\"content_type\"`\n    Body            string            `yaml:\"body\"`\n    Headers         map[string]string `yaml:\"headers\"`\n}\n\n// NewEnvoyRateLimitService creates a new Envoy rate limiting service\nfunc NewEnvoyRateLimitService(limiter ratelimit.DistributedLimiter) *EnvoyRateLimitService {\n    // TODO: Initialize request context extractor with default rules\n    // TODO: Create policy manager with Istio client\n    // TODO: Set up service mesh metrics collection\n    return nil\n}\n\n// ShouldRateLimit implements Envoy's rate limiting service interface\nfunc (erls *EnvoyRateLimitService) ShouldRateLimit(ctx context.Context, \n                                                  req *envoy_service_ratelimit_v3.RateLimitRequest) (\n                                                  *envoy_service_ratelimit_v3.RateLimitResponse, error) {\n    // TODO: Extract request context from Envoy metadata\n    // TODO: Find matching service mesh policies\n    // TODO: Evaluate rate limiting rules for matched policies\n    // TODO: Convert rate limiting result to Envoy response format\n    // TODO: Include appropriate response headers\n    return nil, nil\n}\n\n// ExtractRequestContext converts Envoy request metadata to rate limiting context\nfunc (rce *RequestContextExtractor) ExtractRequestContext(\n    req *envoy_service_ratelimit_v3.RateLimitRequest) (*ratelimit.RequestContext, error) {\n    // TODO: Iterate through extraction rules\n    // TODO: Extract values from request headers, path, query parameters\n    // TODO: Apply transformers (regex, hashing, lookups)\n    // TODO: Build RequestContext with extracted dimensions\n    return nil, nil\n}\n\n// FindMatchingPolicies identifies service mesh policies that apply to the current request\nfunc (pm *PolicyManager) FindMatchingPolicies(ctx context.Context, \n                                             reqCtx *ratelimit.RequestContext) ([]*ServiceMeshPolicy, error) {\n    // TODO: Match request against service selectors\n    // TODO: Match request against route patterns\n    // TODO: Apply policy precedence rules\n    // TODO: Return ordered list of applicable policies\n    return nil, nil\n}\n\n// ConvertToEnvoyResponse transforms rate limiting result into Envoy response format\nfunc ConvertToEnvoyResponse(result *ratelimit.FlowResult, \n                          errorConfig ErrorResponseConfig) *envoy_service_ratelimit_v3.RateLimitResponse {\n    // TODO: Set response status based on rate limiting decision\n    // TODO: Add rate limiting headers (X-RateLimit-Limit, X-RateLimit-Remaining)\n    // TODO: Configure error response if rate limit exceeded\n    // TODO: Include retry-after header with appropriate timing\n    return nil\n}\n```\n\n#### Milestone Checkpoints\n\n**Adaptive Rate Limiting Checkpoint:**\nAfter implementing adaptive rate limiting, run load tests with varying traffic patterns. You should observe rate limits automatically adjusting based on system performance. Monitor the adjustment history to ensure the system doesn't oscillate - limits should converge to stable values that maintain target performance levels.\n\n**Geographic Distribution Checkpoint:**\nDeploy the rate limiter across two simulated regions with controlled network latency between them. Generate traffic in both regions and verify that global rate limits are eventually enforced even when regional limits are exceeded. Test network partition scenarios by blocking cross-region communication and confirm that each region continues operating with conservative local enforcement.\n\n**Service Mesh Integration Checkpoint:**\nDeploy the rate limiter as an Envoy filter in a test service mesh. Configure rate limiting policies through Istio VirtualService resources and verify that applications receive rate limiting transparently without code changes. Check that rate limiting decisions appear correctly in distributed traces and that error responses match configured formats.\n\n\n## Glossary\n\n> **Milestone(s):** All milestones - this glossary provides definitions for technical terms used across rate limiting algorithms, multi-tier evaluation, Redis backend integration, sharding, and API design throughout the distributed rate limiting system\n\nThe distributed rate limiting system introduces numerous technical concepts spanning algorithm design, distributed systems, data storage, and operational management. This glossary provides comprehensive definitions of all terms, acronyms, data structures, and domain concepts used throughout the design document. Understanding these definitions is crucial for implementing, operating, and extending the rate limiting system effectively.\n\n### Mental Model: The Technical Reference Library\n\nThink of this glossary as the technical reference library for our distributed rate limiting \"city.\" Just as a city has specialized terminology for its infrastructure (traffic signals, utility grids, zoning laws), our distributed system has its own vocabulary. Each term represents a specific concept, component, or operation that engineers use to communicate precisely about the system's behavior. Like a reference library, this glossary organizes knowledge so that anyone working with the system can quickly understand unfamiliar concepts and use consistent terminology when discussing designs, implementing features, or troubleshooting issues.\n\n### Core System Concepts\n\n**Distributed Rate Limiting**: The coordination of request quota enforcement across multiple application instances using shared state storage. Unlike local rate limiting where each instance operates independently, distributed rate limiting ensures that the combined traffic from all instances respects global quotas. This requires atomic operations, clock synchronization, and graceful degradation strategies.\n\n**Local Fallback**: The system's ability to switch to per-instance rate limiting when shared storage becomes unavailable. During fallback mode, each application instance enforces rate limits using only local state, accepting that global quotas may be exceeded but maintaining basic protection. The system automatically returns to distributed mode when shared storage connectivity is restored.\n\n**Graceful Degradation**: The system's capacity to maintain reduced functionality during component failures rather than completely failing. For rate limiting, this means continuing to provide protection (albeit with reduced accuracy) when Redis is unavailable, configuration updates fail, or individual nodes become unreachable.\n\n**Atomic Operations**: Indivisible check-and-update operations that prevent race conditions in distributed rate limiting. These operations ensure that reading a counter, comparing it to a limit, and incrementing it (if allowed) happen as a single unit, preventing the double-spending problem where multiple instances might simultaneously approve requests that should collectively exceed the limit.\n\n**Circuit Breaker**: A failure detection and prevention mechanism that protects against cascading failures by monitoring error rates and temporarily blocking operations to failing components. In rate limiting, circuit breakers prevent the system from repeatedly attempting Redis operations that are likely to fail, reducing latency and resource consumption during outages.\n\n### Rate Limiting Algorithm Terms\n\n**Token Bucket**: A rate limiting algorithm that allows controlled bursts above sustained rates by maintaining a bucket of tokens that refill at a steady rate. Requests consume tokens from the bucket; when the bucket is empty, additional requests are denied until tokens are replenished. The bucket capacity determines the maximum burst size, while the refill rate controls the sustained throughput.\n\n**Sliding Window Counter**: A memory-efficient approximate rate limiting algorithm that divides time into fixed buckets and weights recent buckets more heavily than older ones. This approach provides good accuracy while using constant memory per rate limit key, making it suitable for high-scale deployments where memory usage must be bounded.\n\n**Sliding Window Log**: A precise rate limiting algorithm that stores individual request timestamps to make exact decisions about whether new requests should be allowed. While providing perfect accuracy, this approach requires O(n) memory per key where n is the number of requests in the time window, making it suitable for scenarios where precision is more important than memory efficiency.\n\n**Burst Handling**: The system's capability to allow short traffic spikes above sustained rate limits without rejecting requests. Burst handling recognizes that real-world traffic is often bursty rather than perfectly uniform, and well-behaved clients should be able to send requests in bursts as long as their long-term average respects the configured limits.\n\n**Refill Rate**: The steady rate at which tokens are added to a token bucket, typically expressed as tokens per second. The refill rate determines the sustained throughput allowed by the rate limiter - over long periods, the average request rate cannot exceed the refill rate regardless of the bucket capacity.\n\n**Boundary Condition**: Edge cases where rate limiting algorithms face their greatest challenges, particularly during time window transitions. For example, a sliding window counter must carefully handle the transition from one time bucket to the next to avoid losing track of requests or allowing double the configured limit during boundary periods.\n\n### Multi-Tier Rate Limiting Concepts\n\n**Multi-Tier Rate Limiting**: A hierarchical system of rate limits that enforces quotas across multiple dimensions simultaneously, such as per-user, per-IP, per-API, and global limits. Each tier represents a different scope of protection, and requests must pass all applicable tiers to be allowed.\n\n**Tier Precedence Hierarchy**: The ordered evaluation system for multiple rate limit tiers, typically proceeding from most specific (per-user) to most general (global). The hierarchy determines which limits are checked first and how conflicts between tiers are resolved.\n\n**Short-Circuit Evaluation**: An optimization strategy that stops tier evaluation immediately when any rate limit is exceeded, avoiding unnecessary computation and reducing latency. Once a request is denied by one tier, there is no need to check remaining tiers since the request will be rejected regardless.\n\n**Rule Pattern Matching**: The process of identifying which rate limit rules apply to a specific request based on the request's context (user ID, IP address, API endpoint, etc.). Pattern matching uses regular expressions or exact string matching to determine which configured rules should be evaluated.\n\n**Key Composition**: The systematic construction of Redis keys for rate limit storage by combining rule patterns with request context. Proper key composition ensures that rate limits are applied to the correct scope (e.g., \"user:12345:api:/orders\" for a per-user rate limit on the orders endpoint).\n\n### Distributed Systems and Consistency\n\n**Consistent Hashing**: A distributed data placement strategy that minimizes data movement when nodes are added or removed from a cluster. In rate limiting, consistent hashing ensures that rate limit keys are distributed evenly across Redis nodes and that most keys remain on the same node during topology changes.\n\n**Virtual Nodes**: Multiple hash positions assigned to each physical node in a consistent hash ring to improve load balancing. Virtual nodes ensure that when a physical node fails, its load is distributed across multiple remaining nodes rather than overwhelming a single neighbor.\n\n**Hash Ring**: The circular hash space used in consistent hashing where keys and nodes are placed based on their hash values. The ring topology ensures that each key is assigned to the first node found by walking clockwise from the key's position.\n\n**Hot Key Detection**: The process of identifying rate limit keys that receive disproportionately high access compared to others. Hot keys can overwhelm individual Redis nodes and require special handling such as replication or redistribution to maintain system performance.\n\n**Split-Brain Prevention**: Strategies to ensure consistent cluster state during network partitions that might divide the cluster into multiple segments. Split-brain scenarios can lead to inconsistent rate limit decisions if different cluster segments allow requests that should collectively exceed limits.\n\n**Clock Skew**: Time differences between distributed system nodes that can affect time-based rate limiting algorithms. Clock skew can cause requests to be incorrectly categorized in time windows or lead to inconsistent token bucket refill timing across different application instances.\n\n### Redis and Storage Concepts\n\n**Connection Pooling**: The practice of maintaining a reusable set of Redis connections to improve performance and resource utilization. Connection pools reduce the overhead of establishing new connections for each operation while limiting the total number of connections to prevent overwhelming Redis servers.\n\n**Lua Scripting**: Redis's capability to execute Lua scripts atomically on the server side, ensuring that complex operations involving multiple Redis commands are performed without interruption. In rate limiting, Lua scripts implement atomic check-and-update operations that prevent race conditions.\n\n**Failover**: The automatic redirection of traffic from failed Redis nodes to healthy replicas or alternative nodes. Failover mechanisms ensure that rate limiting continues to function even when individual Redis instances become unavailable due to hardware failures or network issues.\n\n**Rebalancing**: The process of redistributing rate limit state across Redis nodes to maintain optimal performance and load distribution. Rebalancing occurs when nodes are added or removed from the cluster or when hot key detection indicates uneven load distribution.\n\n**Configuration Propagation**: The distribution of rate limit rule changes from the management API to all application instances. Propagation ensures that all instances use consistent rate limiting rules without requiring service restarts or manual updates.\n\n### Testing and Quality Assurance\n\n**Algorithm Unit Testing**: Isolated testing of individual rate limiting algorithms to verify correctness and boundary condition handling. Unit tests validate that algorithms behave correctly under various scenarios including burst traffic, time window boundaries, and edge cases like exactly hitting rate limits.\n\n**Distributed Integration Testing**: Multi-instance testing scenarios that verify cluster-wide rate limiting behavior works correctly across multiple application instances and Redis nodes. These tests validate that distributed coordination functions properly and that race conditions are prevented.\n\n**Milestone Verification Checkpoints**: Quality gates that confirm implementation meets acceptance criteria before advancing to subsequent development phases. Each checkpoint includes specific tests to run and expected behaviors to verify, ensuring systematic progress toward project completion.\n\n**Chaos and Failure Testing**: Systematic failure injection to validate recovery mechanisms and identify system weaknesses. Chaos testing includes simulating Redis failures, network partitions, clock skew, and other adverse conditions to verify that the system continues operating correctly.\n\n**Load Pattern**: Synthetic traffic generation strategies for testing system behavior under different types of load. Patterns include constant load (steady rate), burst load (periodic spikes), gradual ramp (increasing rate), spike load (sudden increases), and random load (variable timing).\n\n**Network Partition Simulation**: Controlled isolation of system components to test split-brain scenarios and validate that the system maintains consistency during network failures. Partition testing ensures that conflicting rate limit decisions don't occur when cluster segments can't communicate.\n\n### Monitoring and Operations\n\n**Correlation IDs**: Unique identifiers that follow requests through distributed system components, enabling tracing of request flow across multiple services and facilitating debugging of complex interactions. Correlation IDs help operators understand how rate limiting decisions relate to specific user requests.\n\n**Structured Logging**: Logging practices that use consistent formats and fields to enable automated analysis and monitoring. Structured logs facilitate debugging by providing machine-readable information about rate limiting decisions, Redis operations, and system performance.\n\n**Distributed Tracing**: The practice of tracking request flow across multiple system components to understand performance characteristics and identify bottlenecks. Tracing helps operators understand the latency contribution of different rate limiting operations.\n\n**Symptom-Based Diagnosis**: A troubleshooting methodology that maps observable problems to likely root causes through systematic analysis. This approach helps operators quickly identify the source of rate limiting issues based on symptoms like high latency, incorrect decisions, or system errors.\n\n**Metrics Aggregation**: The collection and combination of usage statistics from multiple application instances for centralized monitoring and alerting. Aggregation provides global visibility into rate limiting effectiveness and system health.\n\n### System Architecture Components\n\nThis section defines the key data structures and interfaces that comprise the distributed rate limiting system, organized by their primary responsibilities and relationships.\n\n#### Core Rate Limiting Types\n\n| Type | Purpose | Key Characteristics |\n|------|---------|-------------------|\n| `RateLimitRule` | Defines rate limiting policies | Contains pattern matching, algorithm selection, and limit configuration |\n| `RateLimitRequest` | Represents incoming requests for rate limit evaluation | Encapsulates user context, IP address, API endpoint, and token requirements |\n| `RateLimitResult` | Provides rate limiting decision and metadata | Includes allow/deny decision, remaining quota, and retry timing information |\n| `RequestContext` | Extended request information with correlation data | Adds correlation IDs, headers, and timestamp for distributed tracing |\n\nThe `RateLimitRule` structure serves as the foundation for policy definition, containing fields for pattern matching (`key_pattern`), algorithm selection (`algorithm`), limit values (`limit`, `burst_limit`), time windows (`window`), and operational controls (`enabled`, `priority`). The priority field enables conflict resolution when multiple rules match the same request.\n\nThe `RateLimitRequest` captures the essential information needed to make rate limiting decisions, including user identification (`user_id`), network identification (`ip_address`), resource identification (`api_endpoint`), client identification (`user_agent`), and resource consumption (`tokens`). This structure provides the input for key composition and rule matching.\n\nThe `RateLimitResult` communicates rate limiting decisions back to applications, indicating whether the request is allowed (`allowed`), how much quota remains (`remaining`), when the client should retry if denied (`retry_after`), when limits reset (`reset_time`), which rule caused the decision (`rule_id`), and which algorithm was used (`algorithm`).\n\n#### Algorithm Implementation Types\n\n| Type | Purpose | Key Characteristics |\n|------|---------|-------------------|\n| `TokenBucketConfig` | Configuration for token bucket algorithm | Specifies capacity, refill rate, and time window |\n| `TokenBucketState` | Runtime state for token bucket instances | Tracks current tokens, last refill time, and configuration |\n| `SlidingWindowConfig` | Configuration for sliding window algorithms | Defines limit, window size, and bucket count for approximation |\n| `SlidingWindowCounter` | Implementation of sliding window counter algorithm | Provides memory-efficient approximate rate limiting |\n| `SlidingWindowLog` | Implementation of sliding window log algorithm | Provides precise rate limiting with timestamp tracking |\n\nThe token bucket implementation uses `TokenBucketConfig` to define the bucket capacity (maximum burst), refill rate (sustained throughput), and time window for rate calculations. The `TokenBucketState` maintains runtime information including current token count, last refill timestamp, and configuration references for atomic updates.\n\nSliding window algorithms share a common configuration structure (`SlidingWindowConfig`) but differ in their implementation approaches. The counter variant approximates sliding windows using fixed time buckets with weighted averaging, while the log variant maintains precise timestamps for exact calculations.\n\n#### Multi-Tier Evaluation Types\n\n| Type | Purpose | Key Characteristics |\n|------|---------|-------------------|\n| `MultiTierLimiter` | Coordinates evaluation across multiple rate limit tiers | Manages rule matching, tier ordering, and short-circuit evaluation |\n| `TierEvaluation` | Records results from individual tier evaluation | Captures rule ID, tier name, result, duration, and algorithm used |\n| `FlowResult` | Comprehensive result from multi-tier evaluation | Aggregates tier results, identifies blocking tier, and provides response headers |\n| `KeyComposer` | Generates Redis keys from rule patterns and request context | Maintains pattern cache and handles key generation for different tiers |\n\nThe `MultiTierLimiter` orchestrates the evaluation of multiple rate limiting rules against a single request. It maintains references to storage backends, rule management, key composition, algorithm implementations, and fallback mechanisms. The design enables efficient evaluation through rule pre-filtering and short-circuit logic.\n\n`TierEvaluation` structures capture detailed information about each tier's evaluation, enabling detailed logging, metrics collection, and debugging. The `FlowResult` aggregates these individual evaluations into a comprehensive response that applications can use for decision-making and client communication.\n\n#### Distributed Storage Types\n\n| Type | Purpose | Key Characteristics |\n|------|---------|-------------------|\n| `RedisStorage` | Redis backend implementation for distributed state | Manages connections, executes Lua scripts, handles failover |\n| `RedisConfig` | Configuration for Redis connections and behavior | Specifies addresses, authentication, timeouts, and pool settings |\n| `DistributedLimiter` | Main entry point for distributed rate limiting | Coordinates storage, rule management, and fallback strategies |\n| `FallbackLimiter` | Local-only rate limiting for degraded operation | Maintains local state when Redis is unavailable |\n\nThe `RedisStorage` type encapsulates all Redis interactions, providing a clean interface for atomic operations while handling connection management, script execution, and error recovery. The design isolates Redis-specific concerns from algorithm logic, enabling easier testing and potential backend substitution.\n\n`DistributedLimiter` serves as the primary interface for applications, coordinating between Redis storage, rule management, and local fallback. This design ensures that applications have a consistent interface regardless of whether the system is operating in distributed or fallback mode.\n\n#### Consistent Hashing and Clustering Types\n\n| Type | Purpose | Key Characteristics |\n|------|---------|-------------------|\n| `HashRing` | Implements consistent hashing for key distribution | Manages virtual nodes, handles topology changes, minimizes redistribution |\n| `CircuitBreaker` | Prevents cascading failures through intelligent failure detection | Tracks failure rates, implements state machine, provides fallback paths |\n| `HealthChecker` | Monitors Redis node health and coordinates responses | Performs health checks, manages circuit breakers, handles failover |\n| `HotKeyDetector` | Identifies disproportionately accessed rate limit keys | Analyzes access patterns, triggers replication, enables load balancing |\n\n![Redis Cluster and Consistent Hashing](./diagrams/redis-cluster-topology.svg)\n\nThe `HashRing` implementation uses virtual nodes to ensure even distribution of rate limit keys across Redis instances. The virtual node approach prevents hotspots that could occur if keys were distributed only based on physical nodes, especially in small clusters.\n\n`CircuitBreaker` provides protection against cascading failures by implementing a state machine with closed (normal operation), open (all requests rejected), and half-open (limited test requests allowed) states. This pattern prevents the system from overwhelming failing components while allowing for automatic recovery.\n\n#### Configuration and Management Types\n\n| Type | Purpose | Key Characteristics |\n|------|---------|-------------------|\n| `RuleManager` | Manages rate limit rule lifecycle | Handles rule loading, validation, caching, and change notification |\n| `ConfigurationWatcher` | Monitors for configuration changes | Subscribes to Redis updates, maintains version consistency |\n| `FlowCoordinator` | Orchestrates complete request processing | Coordinates all components for end-to-end request handling |\n| `MetricsCollector` | Aggregates usage statistics and performance data | Collects metrics, detects patterns, provides monitoring data |\n\nThe `RuleManager` centralizes all rule-related operations, providing interfaces for loading rules from configuration files, validating rule syntax and logic, maintaining rule caches for performance, and notifying other components of rule changes.\n\n`ConfigurationWatcher` implements the observer pattern for configuration changes, using Redis pub/sub mechanisms to receive notifications when rules are updated. This design ensures that all application instances receive configuration updates promptly without requiring polling.\n\n#### Time and Synchronization Types\n\n| Type | Purpose | Key Characteristics |\n|------|---------|-------------------|\n| `TimeProvider` | Abstracts time operations for testing and synchronization | Provides consistent time across distributed components |\n| `MockTimeProvider` | Controllable time provider for testing | Enables deterministic testing of time-based algorithms |\n| `CorrelationContext` | Provides request correlation for distributed tracing | Maintains correlation IDs and metadata across component boundaries |\n| `CorrelationLogger` | Structured logging with correlation information | Enables efficient debugging and request tracing |\n\nThe `TimeProvider` abstraction enables consistent time handling across the distributed system while supporting testing scenarios. The interface provides methods for measuring clock skew relative to Redis servers and adjusting local timestamps accordingly.\n\n`CorrelationContext` facilitates debugging and monitoring by providing unique identifiers that follow requests through the system. This context includes trace IDs, request IDs, user IDs, session IDs, and arbitrary metadata that helps operators understand request flow.\n\n#### Testing and Quality Assurance Types\n\n| Type | Purpose | Key Characteristics |\n|------|---------|-------------------|\n| `RedisTestHelper` | Utilities for Redis testing scenarios | Manages embedded Redis, clusters, network simulation |\n| `LoadGenerator` | Generates synthetic traffic for performance testing | Supports various load patterns and measures system response |\n| `LoadTestResults` | Aggregates load testing metrics and analysis | Captures latency, throughput, and accuracy measurements |\n| `LoadGeneratorConfig` | Configuration for load testing scenarios | Defines traffic patterns, duration, and client behavior |\n\nThe `RedisTestHelper` provides utilities for creating test environments including embedded Redis instances, Redis clusters, and network partition simulation. This tooling enables comprehensive testing without requiring external Redis infrastructure.\n\n`LoadGenerator` supports multiple traffic patterns including constant load (steady request rate), burst load (periodic spikes), gradual ramp (linearly increasing rate), spike load (sudden increases), and random load (variable timing). Each pattern tests different aspects of the rate limiting system's behavior.\n\n### Algorithm-Specific Constants\n\nThe system defines several algorithm identifiers and configuration constants that standardize behavior across implementations:\n\n| Constant | Value | Purpose |\n|----------|-------|---------|\n| `ALGORITHM_TOKEN_BUCKET` | \"token_bucket\" | Identifier for token bucket algorithm selection |\n| `ALGORITHM_SLIDING_WINDOW_LOG` | \"sliding_window_log\" | Identifier for precise sliding window algorithm |\n| `ALGORITHM_SLIDING_COUNTER` | \"sliding_window_counter\" | Identifier for approximate sliding window algorithm |\n| `DEFAULT_POOL_SIZE` | 10 | Default number of Redis connections per pool |\n| `DEFAULT_TIMEOUT` | 5ms | Default timeout for Redis operations |\n\nPriority constants enable rule precedence configuration:\n\n| Constant | Value | Purpose |\n|----------|-------|---------|\n| `PRIORITY_HIGH` | 100 | High priority for critical rate limiting rules |\n| `PRIORITY_LOW` | 1 | Low priority for general rate limiting rules |\n\nCircuit breaker states manage failure handling:\n\n| Constant | Purpose | Characteristics |\n|----------|---------|-----------------|\n| `CircuitClosed` | Normal operation state | All requests are processed normally |\n| `CircuitOpen` | All requests rejected state | Requests fail fast without attempting operations |\n| `CircuitHalfOpen` | Limited test requests allowed state | Some requests are tested to determine if service has recovered |\n\n### Interface Definitions\n\nThe system defines several key interfaces that enable modularity and testing. These interfaces abstract the essential operations while allowing for multiple implementations.\n\n#### Core Rate Limiting Interfaces\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `Check` | `ctx context.Context, req RateLimitRequest` | `*RateLimitResult, error` | Performs rate limit check and updates counters atomically |\n| `Preview` | `ctx context.Context, req RateLimitRequest` | `*RateLimitResult, error` | Checks rate limit status without updating counters |\n| `Reset` | `ctx context.Context, req RateLimitRequest` | `error` | Clears rate limit counters for the specified request context |\n\nThe `Check` method represents the primary rate limiting operation, atomically evaluating whether a request should be allowed and updating counters if so. The `Preview` method enables applications to check rate limit status without consuming quota, useful for displaying remaining limits to users.\n\n#### Storage Backend Interfaces\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `CheckAndUpdate` | `ctx context.Context, key string, limit int64, window time.Duration` | `bool, int64, time.Time, error` | Atomically checks and updates rate limit counters |\n| `ExecuteLua` | `ctx context.Context, script string, keys []string, args []interface{}` | `interface{}, error` | Executes Redis Lua script atomically |\n| `GetNode` | `key string` | `string, bool` | Returns responsible node for key in sharded setup |\n\nThe storage interface abstracts Redis operations to enable testing with mock backends and potential future support for alternative storage systems. The `ExecuteLua` method enables atomic operations by running Lua scripts on Redis servers.\n\n#### Rule Management Interfaces\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `LoadRules` | `configPath string` | `error` | Loads rate limit rules from configuration file |\n| `GetMatchingRules` | `userID, ipAddress, apiEndpoint string` | `[]*RateLimitRule` | Returns rules matching request context |\n| `ComposeKey` | `rule *RateLimitRule, req *RateLimitRequest` | `string, error` | Generates Redis key from rule pattern and request context |\n\nRule management interfaces enable dynamic rule loading and efficient rule matching. The `GetMatchingRules` method filters the complete rule set to only those applicable to a specific request, optimizing evaluation performance.\n\n### Error Handling and State Management\n\nThe system defines comprehensive error handling strategies and state management approaches to ensure reliable operation under various failure conditions.\n\n#### Error Categories\n\n| Error Type | Detection Method | Recovery Strategy |\n|------------|------------------|-------------------|\n| Redis Connection Failures | Connection timeout or network errors | Automatic failover to backup Redis nodes or local fallback |\n| Clock Skew Exceeding Tolerance | Time synchronization checks | Log warnings and adjust calculations using measured skew |\n| Configuration Parsing Errors | Schema validation during rule loading | Reject invalid configuration and continue with previous valid rules |\n| Hot Key Overload | Access pattern analysis | Replicate hot keys across multiple nodes or apply specialized handling |\n| Lua Script Execution Failures | Redis error responses | Retry with exponential backoff or fall back to local limiting |\n\n#### State Machine Transitions\n\nCircuit breaker state management follows a well-defined state machine to provide predictable failure handling:\n\n| Current State | Event | Next State | Action Taken |\n|---------------|-------|------------|--------------|\n| `CircuitClosed` | Failure threshold exceeded | `CircuitOpen` | Begin rejecting all requests immediately |\n| `CircuitOpen` | Recovery timeout elapsed | `CircuitHalfOpen` | Allow limited test requests to probe service health |\n| `CircuitHalfOpen` | Test request succeeds | `CircuitClosed` | Resume normal operation with full request processing |\n| `CircuitHalfOpen` | Test request fails | `CircuitOpen` | Return to rejecting all requests and reset recovery timer |\n\nThis state machine prevents cascading failures by quickly detecting problems and avoiding unnecessary load on failing components while providing automatic recovery when services become healthy again.\n\n### Performance and Scalability Considerations\n\nThe distributed rate limiting system incorporates several design patterns and optimizations to achieve high performance and horizontal scalability.\n\n#### Memory Usage Optimization\n\nDifferent rate limiting algorithms have distinct memory characteristics that affect their suitability for different use cases:\n\n| Algorithm | Memory Per Key | Accuracy | Best Use Case |\n|-----------|---------------|----------|---------------|\n| Token Bucket | O(1) - constant | Exact for burst, approximate for sustained | General-purpose rate limiting with burst support |\n| Sliding Window Counter | O(1) - constant | Approximate (can allow 2x limit at boundaries) | High-scale deployments requiring memory efficiency |\n| Sliding Window Log | O(n) - proportional to requests | Exact | Low-to-medium scale deployments requiring precision |\n\nThe constant memory algorithms (token bucket and sliding window counter) enable the system to support millions of rate-limited keys without running out of memory, making them suitable for large-scale deployments.\n\n#### Latency Optimization Strategies\n\nThe system employs several strategies to minimize rate limiting latency:\n\n| Optimization | Technique | Benefit |\n|--------------|-----------|---------|\n| Connection Pooling | Reuse Redis connections across requests | Eliminates connection establishment overhead |\n| Lua Script Preloading | Cache compiled Lua scripts in Redis | Reduces script compilation time for each request |\n| Short-Circuit Evaluation | Stop checking tiers after first denial | Reduces computation and network calls |\n| Local Rule Caching | Cache rule matching results locally | Eliminates repeated pattern matching computation |\n| Asynchronous Metrics Collection | Decouple metrics from request path | Prevents metrics overhead from affecting latency |\n\n### Common Implementation Patterns\n\nSeveral design patterns appear throughout the distributed rate limiting system, providing consistency and reliability across different components.\n\n#### Observer Pattern for Configuration Updates\n\nThe system uses the observer pattern to propagate configuration changes from the management API to all application instances. The `ConfigurationWatcher` subscribes to Redis pub/sub channels, receives change notifications, and triggers local rule reloading. This pattern ensures that configuration updates are applied consistently across the cluster without requiring service restarts.\n\n#### Strategy Pattern for Algorithm Selection\n\nRate limiting algorithms are implemented using the strategy pattern, enabling runtime selection based on rule configuration. Each algorithm implements a common interface while providing different trade-offs between accuracy, memory usage, and performance. This design allows the same infrastructure to support multiple algorithms simultaneously.\n\n#### Circuit Breaker Pattern for Failure Isolation\n\nCircuit breakers provide failure isolation by monitoring error rates and preventing requests to failing components. The pattern includes timeout mechanisms, retry logic, and automatic recovery detection. This approach prevents cascading failures and improves overall system resilience.\n\n#### Template Method Pattern for Request Processing\n\nThe request processing flow uses the template method pattern to standardize the steps while allowing customization for different scenarios. The flow includes request validation, rule matching, tier evaluation, storage operations, and response generation. Each step can be customized while maintaining consistent overall behavior.\n\n### Implementation Guidance\n\nThis section provides practical guidance for implementing the distributed rate limiting system, including technology recommendations, code organization, and development best practices.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Storage Backend | Redis single instance with persistence | Redis Cluster with replication |\n| Configuration | YAML files with file watching | Redis-backed configuration with pub/sub updates |\n| Monitoring | Prometheus metrics with Grafana | Distributed tracing with Jaeger plus metrics |\n| Testing | Embedded Redis with unit tests | Docker Compose with integration tests |\n| Deployment | Single process with Redis connection | Kubernetes with Redis Operator |\n\n#### Recommended Module Structure\n\nThe following directory structure organizes the codebase into logical modules that align with the system's architectural components:\n\n```\ndistributed-rate-limiter/\n├── cmd/\n│   ├── server/main.go              # HTTP API server entry point\n│   └── cli/main.go                 # Command-line management tools\n├── internal/\n│   ├── algorithms/\n│   │   ├── token_bucket.go         # Token bucket implementation\n│   │   ├── sliding_window.go       # Sliding window implementations\n│   │   └── algorithm_test.go       # Algorithm unit tests\n│   ├── storage/\n│   │   ├── redis.go                # Redis storage implementation\n│   │   ├── fallback.go             # Local fallback storage\n│   │   └── storage_test.go         # Storage integration tests\n│   ├── config/\n│   │   ├── rule_manager.go         # Rule loading and management\n│   │   ├── watcher.go              # Configuration change monitoring\n│   │   └── config_test.go          # Configuration tests\n│   ├── ratelimit/\n│   │   ├── distributed_limiter.go  # Main rate limiting logic\n│   │   ├── multi_tier.go           # Multi-tier evaluation\n│   │   └── ratelimit_test.go       # Core logic tests\n│   ├── cluster/\n│   │   ├── hash_ring.go            # Consistent hashing implementation\n│   │   ├── health_checker.go       # Node health monitoring\n│   │   └── cluster_test.go         # Clustering tests\n│   └── api/\n│       ├── handlers.go             # HTTP request handlers\n│       ├── middleware.go           # Rate limiting middleware\n│       └── api_test.go             # API integration tests\n├── pkg/\n│   ├── metrics/\n│   │   ├── collector.go            # Metrics collection interfaces\n│   │   └── prometheus.go           # Prometheus metrics implementation\n│   └── logging/\n│       ├── correlation.go          # Correlation context and logging\n│       └── structured.go           # Structured logging utilities\n├── test/\n│   ├── fixtures/\n│   │   ├── rules.yaml              # Test rate limiting rules\n│   │   └── config.yaml             # Test configuration\n│   ├── helpers/\n│   │   ├── redis_helper.go         # Redis testing utilities\n│   │   └── load_generator.go       # Load testing tools\n│   └── integration/\n│       ├── distributed_test.go     # Multi-instance integration tests\n│       └── chaos_test.go           # Failure injection tests\n├── configs/\n│   ├── rules.yaml                  # Production rate limiting rules\n│   └── server.yaml                 # Server configuration\n├── scripts/\n│   ├── lua/\n│   │   ├── token_bucket.lua        # Token bucket Lua script\n│   │   └── sliding_window.lua      # Sliding window Lua script\n│   └── deploy/\n│       ├── docker-compose.yaml     # Development environment\n│       └── kubernetes/             # Kubernetes deployment manifests\n└── docs/\n    ├── api.md                      # API documentation\n    ├── algorithms.md               # Algorithm documentation\n    └── operations.md               # Operational procedures\n```\n\nThis structure separates concerns clearly while maintaining logical grouping. The `internal/` directory contains private implementation details, while `pkg/` contains reusable packages that could be imported by other projects.\n\n#### Core Interface Implementation\n\nThe following skeleton provides the foundation for implementing the main rate limiting interface:\n\n```go\n// Package ratelimit provides distributed rate limiting capabilities\npackage ratelimit\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// DistributedLimiter coordinates rate limiting across multiple application instances\ntype DistributedLimiter struct {\n    storage       Storage\n    ruleManager   *config.RuleManager\n    localFallback Limiter\n    algorithms    map[string]Algorithm\n    keyComposer   *KeyComposer\n    metrics       *metrics.Collector\n}\n\n// NewDistributedLimiter creates a new distributed rate limiter instance\nfunc NewDistributedLimiter(storage Storage, ruleManager *config.RuleManager) *DistributedLimiter {\n    // TODO 1: Initialize algorithm map with token bucket, sliding window implementations\n    // TODO 2: Create key composer for generating Redis keys from rule patterns  \n    // TODO 3: Initialize local fallback limiter for degraded operation\n    // TODO 4: Set up metrics collector for monitoring and debugging\n    // TODO 5: Return configured DistributedLimiter instance\n}\n\n// Check performs rate limit evaluation and updates counters atomically\nfunc (d *DistributedLimiter) Check(ctx context.Context, req RateLimitRequest) (*RateLimitResult, error) {\n    // TODO 1: Extract correlation context for distributed tracing\n    // TODO 2: Get matching rules based on request context (user, IP, API endpoint)\n    // TODO 3: Evaluate all applicable tiers with short-circuit logic\n    // TODO 4: For each tier, compose Redis key and select appropriate algorithm\n    // TODO 5: Execute rate limit check using selected algorithm and storage backend\n    // TODO 6: If any tier denies request, return denial with retry-after timing\n    // TODO 7: If all tiers pass, return success with remaining quota information\n    // TODO 8: Handle Redis failures by falling back to local rate limiting\n    // TODO 9: Record metrics for monitoring and hot key detection\n    // TODO 10: Log rate limiting decision with correlation context for debugging\n}\n```\n\n#### Redis Storage Implementation Skeleton\n\nThe Redis storage backend requires careful implementation of atomic operations and connection management:\n\n```go\n// Package storage provides distributed storage backends for rate limiting\npackage storage\n\nimport (\n    \"context\"\n    \"time\"\n    \"github.com/go-redis/redis/v8\"\n)\n\n// RedisStorage implements distributed rate limiting storage using Redis\ntype RedisStorage struct {\n    client redis.UniversalClient\n    config RedisConfig\n    scripts map[string]*redis.Script\n    circuitBreaker *CircuitBreaker\n}\n\n// NewRedisStorage creates Redis storage with connection pooling and health checking\nfunc NewRedisStorage(config RedisConfig) (*RedisStorage, error) {\n    // TODO 1: Create Redis universal client with cluster or sentinel support\n    // TODO 2: Configure connection pool with appropriate size and timeouts\n    // TODO 3: Load and register Lua scripts for atomic rate limit operations\n    // TODO 4: Initialize circuit breaker for failure protection\n    // TODO 5: Perform initial connectivity test and return configured storage\n}\n\n// CheckAndUpdate atomically checks rate limit and updates counters\nfunc (r *RedisStorage) CheckAndUpdate(ctx context.Context, key string, limit int64, window time.Duration) (bool, int64, time.Time, error) {\n    // TODO 1: Check circuit breaker state before attempting Redis operation\n    // TODO 2: Select appropriate Lua script based on algorithm requirements\n    // TODO 3: Prepare script arguments including current timestamp and window\n    // TODO 4: Execute Lua script with proper error handling and retries\n    // TODO 5: Parse script result to determine allow/deny decision\n    // TODO 6: Calculate remaining quota and reset time from Redis response\n    // TODO 7: Update circuit breaker state based on operation success/failure\n    // TODO 8: Return rate limiting decision with metadata for client response\n}\n```\n\n#### Load Testing and Validation Tools\n\nComprehensive testing requires tools for validating distributed behavior under various conditions:\n\n```go\n// Package testhelpers provides utilities for testing distributed rate limiting\npackage testhelpers\n\nimport (\n    \"context\"\n    \"time\"\n    \"sync\"\n)\n\n// LoadGenerator produces synthetic traffic for testing rate limiting behavior\ntype LoadGenerator struct {\n    config    LoadGeneratorConfig\n    limiter   ratelimit.DistributedLimiter\n    results   *LoadTestResults\n    ctx       context.Context\n    cancelFn  context.CancelFunc\n}\n\n// Start begins load generation according to configured pattern\nfunc (lg *LoadGenerator) Start() *LoadTestResults {\n    // TODO 1: Initialize results tracking with thread-safe counters\n    // TODO 2: Create context with cancellation for stopping load generation\n    // TODO 3: Launch worker goroutines based on configured client count\n    // TODO 4: Coordinate traffic pattern (constant, burst, ramp, spike, random)\n    // TODO 5: Execute rate limit checks with latency measurement\n    // TODO 6: Record results including success/failure counts and timing\n    // TODO 7: Monitor for test completion or cancellation signal\n    // TODO 8: Aggregate final results and return comprehensive report\n}\n\n// generateConstantLoad produces steady request rate for baseline testing\nfunc (lg *LoadGenerator) generateConstantLoad() {\n    // TODO 1: Calculate inter-request delay for desired requests per second\n    // TODO 2: Create ticker for maintaining steady request rate\n    // TODO 3: Execute requests in loop until context cancellation\n    // TODO 4: Maintain request timing accuracy despite processing variations\n    // TODO 5: Record each request result with timestamp for analysis\n}\n```\n\n#### Debugging and Troubleshooting Utilities\n\nDebugging distributed rate limiting requires specialized tools for analyzing Redis state and request flow:\n\n```go\n// Package debugging provides utilities for troubleshooting distributed rate limiting\npackage debugging\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// RedisDebugger provides Redis-specific debugging capabilities\ntype RedisDebugger struct {\n    client redis.UniversalClient\n    logger *CorrelationLogger\n}\n\n// DiagnoseKeyState provides comprehensive analysis of rate limiting key\nfunc (rd *RedisDebugger) DiagnoseKeyState(ctx context.Context, key string) (*KeyDiagnostics, error) {\n    // TODO 1: Check if key exists in Redis and retrieve TTL\n    // TODO 2: Measure memory usage for the key using MEMORY USAGE command\n    // TODO 3: Determine which Redis node(s) contain the key in cluster setup\n    // TODO 4: Measure access latency by performing test operations\n    // TODO 5: Analyze key access patterns from Redis slow log if available\n    // TODO 6: Return comprehensive diagnostics for troubleshooting\n}\n\n// MeasureClusterHealth assesses overall Redis cluster health and performance\nfunc (rd *RedisDebugger) MeasureClusterHealth(ctx context.Context) (*ClusterHealth, error) {\n    // TODO 1: Enumerate all Redis nodes in cluster configuration\n    // TODO 2: Check connectivity and responsiveness for each node\n    // TODO 3: Measure latency and memory usage across all nodes\n    // TODO 4: Analyze key distribution balance using CLUSTER NODES\n    // TODO 5: Identify potential hotspots or performance issues\n    // TODO 6: Return cluster health report with recommendations\n}\n```\n\n#### Milestone Verification Checkpoints\n\nAfter implementing each major component, verify functionality using these checkpoints:\n\n**Milestone 1 - Algorithm Implementation:**\n- Run `go test ./internal/algorithms/...` to verify algorithm correctness\n- Execute load tests with different traffic patterns to validate burst handling\n- Test boundary conditions at window transitions and capacity limits\n- Verify token bucket allows configured burst size followed by steady rate\n- Confirm sliding window algorithms respect configured request limits\n\n**Milestone 2 - Multi-Tier Evaluation:**\n- Test per-user, per-IP, and global rate limits simultaneously\n- Verify short-circuit evaluation stops at first tier denial\n- Validate rule precedence handling when multiple rules match\n- Test burst allowance behavior across different tiers\n- Confirm API endpoint rate limiting works correctly\n\n**Milestone 3 - Redis Integration:**\n- Start Redis instance and verify Lua script execution\n- Test atomic check-and-update operations under concurrent load\n- Simulate Redis failures and verify graceful degradation to local fallback\n- Validate connection pooling reduces latency compared to per-request connections\n- Test Redis cluster configuration with multiple nodes\n\n**Milestone 4 - Consistent Hashing:**\n- Add and remove Redis nodes while monitoring key redistribution\n- Generate synthetic hot keys and verify detection and replication\n- Test load balancing across nodes with different key distributions\n- Simulate node failures and verify automatic failover behavior\n- Validate virtual nodes improve distribution compared to simple hashing\n\n**Milestone 5 - Management API:**\n- Create, update, and delete rate limit rules through REST API\n- Verify configuration changes propagate to all application instances\n- Test real-time dashboard shows current usage and remaining quotas\n- Validate rate limit headers in HTTP responses match RFC standards\n- Ensure management API itself is protected by rate limiting\n\nEach checkpoint should include specific commands to run, expected outputs, and signs that indicate proper functionality or potential issues requiring investigation.\n"}