{"html":"<h1 id=\"rate-limiter-design-document\">Rate Limiter: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A distributed rate limiting system that uses the token bucket algorithm to protect services from abuse and excessive load. The key architectural challenge is maintaining consistent per-client rate limits across multiple server instances while handling high-throughput scenarios with minimal latency overhead.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section provides foundational understanding needed throughout the project</p>\n</blockquote>\n<p>The digital world operates much like the physical one when it comes to capacity constraints and resource protection. Just as a restaurant can only serve so many customers at once before service quality degrades, web services and APIs have finite computational resources that must be carefully managed to maintain stability and fairness. This section explores the fundamental problem of rate limiting - the practice of controlling the rate at which clients can make requests to a service.</p>\n<h3 id=\"mental-model-the-nightclub-bouncer\">Mental Model: The Nightclub Bouncer</h3>\n<p>To understand rate limiting intuitively, imagine yourself as a <strong>bouncer at an exclusive nightclub</strong>. Your nightclub has a maximum capacity of 200 people, and fire safety regulations require you to maintain this limit strictly. Additionally, the club&#39;s management wants to ensure a pleasant experience for all guests by preventing overcrowding that would degrade the atmosphere.</p>\n<p>As the bouncer, you implement several strategies that directly parallel rate limiting algorithms:</p>\n<p><strong>The VIP Rope System (Token Bucket)</strong>: You maintain a velvet rope system where VIP tokens are distributed at a steady rate - say 10 tokens every minute. Each person entering the club must present a token. If someone arrives when no tokens are available, they must wait until the next token is issued. However, if fewer people arrive during a quiet period, tokens accumulate up to a maximum of 50 tokens, allowing for sudden bursts of arrivals during peak times. This system smooths out the flow while accommodating natural variations in arrival patterns.</p>\n<p><strong>The Hourly Headcount (Fixed Window)</strong>: Alternatively, you might count exactly how many people entered each hour and enforce a strict limit of 60 people per hour. At the stroke of each hour, the counter resets to zero. This approach is simple to track but can lead to problematic scenarios - imagine 60 people arriving at 11:59 PM, then another 60 at 12:01 AM, overwhelming the club&#39;s capacity despite technically following the rules.</p>\n<p><strong>The Rolling Average (Sliding Window)</strong>: A more sophisticated approach involves tracking the number of people who entered in any rolling 60-minute period. This prevents the &quot;burst at window boundaries&quot; problem but requires more complex bookkeeping - you need to remember exactly when each person entered and continuously calculate rolling totals.</p>\n<p>The nightclub analogy reveals why rate limiting is essential: <strong>without controlled entry, the venue becomes overcrowded, service quality plummets, existing customers have a poor experience, and the system (club) can become completely overwhelmed and unusable</strong>.</p>\n<p>In the digital realm, your web service is the nightclub, incoming HTTP requests are the patrons, and computational resources (CPU, memory, database connections) represent the venue&#39;s capacity. Just as the bouncer protects the club&#39;s atmosphere and safety, a rate limiter protects your service&#39;s performance and availability.</p>\n<h3 id=\"why-rate-limiting-matters\">Why Rate Limiting Matters</h3>\n<p>Rate limiting serves as a critical protective mechanism against various forms of service abuse and resource exhaustion. Understanding the concrete scenarios where uncontrolled request rates cause system failures helps illustrate why this protection is essential.</p>\n<p><strong>Denial of Service Protection</strong>: Consider an e-commerce API serving product catalog requests. Under normal conditions, the service handles 1,000 requests per second comfortably. However, a malicious actor launches an attack sending 50,000 requests per second from distributed sources. Without rate limiting, the database connection pool (typically 100-200 connections) becomes exhausted within seconds. New legitimate requests cannot obtain database connections and begin timing out. The web server&#39;s memory consumption spikes as it queues thousands of pending requests, eventually triggering out-of-memory errors that crash the entire service. Rate limiting prevents this cascade by rejecting excessive requests before they consume critical resources.</p>\n<p><strong>Resource Starvation Prevention</strong>: A social media platform&#39;s API serves multiple client types - mobile apps, web interfaces, and third-party integrations. Without rate limiting, a poorly implemented third-party bot making 10,000 profile requests per minute can monopolize the database&#39;s read capacity. This causes response times for mobile app users to degrade from 200ms to 5+ seconds, creating an unacceptable user experience. Per-client rate limiting ensures that no single actor can starve others of service access.</p>\n<p><strong>Cost Control in Cloud Environments</strong>: Modern applications often depend on external APIs that charge per request - payment processors, geocoding services, or machine learning APIs. A bug in client code that retries failed requests without exponential backoff can generate millions of API calls in hours, resulting in unexpected bills of thousands of dollars. Rate limiting acts as a financial circuit breaker, capping the maximum possible cost exposure from runaway request patterns.</p>\n<p><strong>Fair Resource Allocation</strong>: A public API serving both free and premium tiers needs to ensure that free users don&#39;t consume resources intended for paying customers. Without rate limiting, free users making unlimited requests can degrade service quality for premium subscribers who expect guaranteed performance levels. Differential rate limits (e.g., 100 requests/hour for free users, 10,000 requests/hour for premium) enforce the intended service tiers and business model.</p>\n<p><strong>Preventing Resource Cascades</strong>: In microservice architectures, excessive load on one service can trigger failures across the entire system. When Service A becomes overloaded and starts responding slowly, upstream services waiting for responses begin accumulating connection pools and memory usage. These services then become overloaded and fail, creating a cascade effect. Rate limiting at service boundaries prevents one overloaded component from bringing down the entire distributed system.</p>\n<p><strong>Data Scraping and Abuse Mitigation</strong>: Public-facing APIs are frequently targeted by automated scrapers attempting to extract large datasets. A real estate website&#39;s API might be scraped by competitors trying to copy entire property listings databases. Without rate limiting, these scrapers can generate load equivalent to thousands of normal users, degrading service for legitimate visitors while potentially violating the service&#39;s terms of use and intellectual property rights.</p>\n<blockquote>\n<p>The fundamental principle underlying all these scenarios is <strong>resource finiteness</strong> - every system has limited computational capacity, and uncontrolled consumption by some clients necessarily reduces availability for others. Rate limiting enforces fair sharing of finite resources.</p>\n</blockquote>\n<h3 id=\"existing-rate-limiting-algorithms\">Existing Rate Limiting Algorithms</h3>\n<p>Different rate limiting algorithms make different trade-offs between implementation complexity, memory usage, burst handling, and fairness guarantees. Understanding these trade-offs is crucial for selecting the appropriate algorithm for your specific requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Description</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Best Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Fixed Window</strong></td>\n<td>Count requests in fixed time intervals (e.g., per minute). Reset counter at interval boundaries.</td>\n<td>Simple implementation. Low memory usage (one counter per client). Easy to understand and debug.</td>\n<td><strong>Burst problem</strong>: 2x rate limit possible at window boundaries. Uneven traffic distribution. Poor user experience during resets.</td>\n<td>Simple systems, internal APIs, scenarios where occasional bursts are acceptable</td>\n</tr>\n<tr>\n<td><strong>Sliding Window Log</strong></td>\n<td>Maintain timestamped log of all requests. Count requests within rolling time window.</td>\n<td><strong>Perfect accuracy</strong>. No burst issues. Granular request tracking.</td>\n<td>High memory usage (stores all timestamps). Expensive cleanup operations. Poor performance under high load.</td>\n<td>Low-traffic APIs, scenarios requiring perfect accuracy, audit/compliance requirements</td>\n</tr>\n<tr>\n<td><strong>Sliding Window Counter</strong></td>\n<td>Divide time into smaller buckets, estimate current window using weighted bucket counts.</td>\n<td>Good burst prevention. <strong>Lower memory</strong> than sliding log. Reasonable accuracy approximation.</td>\n<td>Complex implementation. Approximation can be inaccurate. Edge cases around bucket boundaries.</td>\n<td>Medium-traffic APIs, balance between accuracy and performance</td>\n</tr>\n<tr>\n<td><strong>Token Bucket</strong></td>\n<td>Generate tokens at fixed rate into bucket with maximum capacity. Each request consumes tokens.</td>\n<td><strong>Excellent burst handling</strong>. Intuitive mental model. Smooth traffic shaping. Natural rate smoothing.</td>\n<td>Slightly more complex than fixed window. Requires careful time handling. Token refill calculations.</td>\n<td><strong>Recommended for most cases</strong>. High-traffic APIs, client-facing services, scenarios needing burst accommodation</td>\n</tr>\n<tr>\n<td><strong>Leaky Bucket</strong></td>\n<td>Process requests at fixed rate regardless of arrival pattern. Queue excess requests up to capacity limit.</td>\n<td><strong>Perfect rate smoothing</strong>. Predictable output rate. Good for downstream protection.</td>\n<td>Request queuing increases latency. Queue management complexity. Potential memory growth from queued requests.</td>\n<td>Background processing, protecting slow downstream services, traffic shaping scenarios</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Token Bucket Algorithm Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: We need an algorithm that handles realistic traffic patterns where clients naturally send bursts of requests (page loads, mobile app launches) while maintaining long-term rate limits</li>\n<li><strong>Options Considered</strong>: Fixed window (simple but bursty), Sliding window (accurate but expensive), Token bucket (balanced approach)</li>\n<li><strong>Decision</strong>: Implement token bucket algorithm as the primary rate limiting mechanism</li>\n<li><strong>Rationale</strong>: Token bucket provides the optimal balance of burst accommodation, implementation complexity, and performance characteristics. It naturally handles the common pattern where clients need to send several requests quickly (burst) but should be limited over longer time periods</li>\n<li><strong>Consequences</strong>: Slightly more complex implementation than fixed window, but significantly better user experience and more natural traffic shaping behavior</li>\n</ul>\n</blockquote>\n<p>The token bucket algorithm&#39;s key advantage lies in its <strong>burst accommodation philosophy</strong>. Unlike fixed windows that either allow or deny requests based on arbitrary time boundaries, token buckets recognize that legitimate traffic patterns often involve short bursts of activity. A mobile app loading a dashboard might make 5-10 API calls within seconds to populate different UI components, but then remain quiet for minutes. Token buckets naturally accommodate this pattern by allowing accumulated tokens to be consumed quickly when needed.</p>\n<p><strong>Algorithm Behavior Comparison Example</strong>: Consider a rate limit of 60 requests per minute, and a client that needs to make 10 requests at once, then waits 50 seconds before making another 10 requests.</p>\n<ul>\n<li><strong>Fixed Window</strong>: If the bursts happen to straddle a minute boundary (5 requests at 59.5 seconds, 5 requests at 0.5 seconds), the client might be blocked despite averaging well under the limit.</li>\n<li><strong>Sliding Window</strong>: Would allow both bursts but requires tracking timestamps of potentially hundreds of requests per client.</li>\n<li><strong>Token Bucket</strong>: With a capacity of 10 tokens and refill rate of 1 token per second, naturally allows the first burst (consuming accumulated tokens), refills during the quiet period, and allows the second burst. Memory usage remains constant (just current token count and last refill time).</li>\n</ul>\n<p>This comparison illustrates why token buckets have become the preferred algorithm for client-facing APIs where user experience and natural traffic accommodation are priorities, while fixed windows remain suitable for internal services where simplicity outweighs sophistication.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section establishes the scope and boundaries that guide implementation decisions throughout the project</p>\n</blockquote>\n<p>Think of building a rate limiter like designing the security system for a busy office building. You need to decide what you&#39;re protecting (which floors, which entrances), who gets access (employees, visitors, VIP guests), and what happens when limits are exceeded (polite redirection vs. security escort). Most importantly, you need to decide what you&#39;re NOT responsible for - you&#39;re not running the elevators, managing the parking garage, or handling the building&#39;s fire safety system. Clear boundaries prevent scope creep and ensure you build something focused and effective.</p>\n<p>This rate limiter project has specific learning objectives around the token bucket algorithm, distributed systems coordination, and HTTP middleware patterns. By explicitly defining what we will and won&#39;t build, we can focus on the core concepts while avoiding unnecessary complexity that would distract from the learning goals.</p>\n<h3 id=\"functional-goals\">Functional Goals</h3>\n<p>The <strong>functional goals</strong> define the core capabilities that users of our rate limiter will directly interact with. These are the features that determine whether the system successfully solves the rate limiting problem.</p>\n<p><strong>Token Bucket Rate Limiting</strong>: The system must implement a proper token bucket algorithm that allows for burst traffic up to a configured limit while maintaining a steady average rate over time. Unlike simpler fixed-window approaches that can allow double the intended rate at window boundaries, the token bucket provides smooth rate limiting with controlled burst allowances. The implementation must handle token generation based on elapsed time, token consumption for each request, and proper overflow behavior when the bucket reaches capacity.</p>\n<p><strong>Per-Client Rate Limiting</strong>: Each client must have an independent rate limit bucket, identified by either IP address or API key. This isolation ensures that one client&#39;s heavy usage cannot exhaust the rate limit quota for other clients. The system must efficiently manage potentially thousands of client buckets in memory while providing fast lookup and update operations. Client identification must be consistent and reliable, handling edge cases like clients behind NAT gateways or load balancers.</p>\n<p><strong>HTTP Integration</strong>: The rate limiter must integrate seamlessly with common web frameworks as middleware, intercepting HTTP requests before they reach application logic. When rate limits are exceeded, the system must return proper HTTP 429 &quot;Too Many Requests&quot; responses with appropriate headers. All responses must include standard rate limiting headers (<code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>, <code>X-RateLimit-Reset</code>) so clients can implement intelligent backoff strategies.</p>\n<p><strong>Distributed Consistency</strong>: When deployed across multiple server instances, the rate limiter must maintain consistent per-client limits using shared storage. A client making requests to different server instances should see the same combined rate limit, not independent limits per server. This requires atomic operations on shared state and proper handling of storage failures.</p>\n<p><strong>Configurable Rate Policies</strong>: The system must support flexible rate limit configuration, including different limits for different clients (premium vs. free tiers) and different limits for different API endpoints. Configuration should be adjustable without requiring application restarts, allowing for dynamic rate limit adjustments based on system load or business requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Functional Capability</th>\n<th>Description</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Bucket Algorithm</td>\n<td>Core rate limiting logic with burst handling</td>\n<td>Allows bursts up to capacity, maintains average rate over time</td>\n</tr>\n<tr>\n<td>Client Identification</td>\n<td>Distinguish between different API consumers</td>\n<td>Unique buckets per IP/API key, consistent identification</td>\n</tr>\n<tr>\n<td>HTTP Middleware</td>\n<td>Integration with web framework request pipeline</td>\n<td>Intercepts all requests, returns 429 with proper headers</td>\n</tr>\n<tr>\n<td>Distributed State</td>\n<td>Consistent limits across multiple server instances</td>\n<td>Client sees same limit regardless of which server handles request</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>Flexible rate limit policies</td>\n<td>Per-client and per-endpoint limits, runtime configuration updates</td>\n</tr>\n</tbody></table>\n<h3 id=\"non-functional-goals\">Non-Functional Goals</h3>\n<p>The <strong>non-functional goals</strong> define the quality attributes and operational characteristics that determine how well the rate limiter performs in production environments. These requirements often drive architectural decisions more strongly than functional requirements.</p>\n<p><strong>Low Latency Overhead</strong>: Rate limiting checks must add minimal latency to request processing. The target is less than 5 milliseconds of additional latency for in-memory operations and less than 10 milliseconds for distributed operations involving Redis. This requires efficient data structures, minimal lock contention, and optimized storage access patterns. The rate limiter should never become the bottleneck in a high-throughput API.</p>\n<p><strong>High Concurrency Support</strong>: The system must handle thousands of concurrent requests without race conditions or performance degradation. This requires careful design of thread-safe data structures, efficient locking strategies, and possibly lock-free algorithms for hot paths. All shared state must be properly synchronized to prevent data corruption under concurrent access.</p>\n<p><strong>Memory Efficiency</strong>: With potentially millions of clients, the per-client bucket storage must be memory-efficient. Inactive client buckets should be automatically cleaned up to prevent memory leaks. The target is less than 100 bytes of memory overhead per active client bucket, with automatic cleanup of buckets idle for more than 1 hour.</p>\n<p><strong>High Availability</strong>: The rate limiter should continue operating even when distributed storage (Redis) becomes unavailable. This requires graceful degradation strategies, such as falling back to local in-memory buckets or allowing requests through when rate limit state cannot be determined. The system should never fail closed unless explicitly configured to do so.</p>\n<p><strong>Operational Simplicity</strong>: The rate limiter should be easy to deploy, monitor, and debug in production. This means clear logging, helpful error messages, and observable metrics. Configuration should be straightforward with sensible defaults. The system should fail fast with clear error messages when misconfigured rather than operating in a degraded state.</p>\n<table>\n<thead>\n<tr>\n<th>Quality Attribute</th>\n<th>Target Requirement</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Latency Overhead</td>\n<td>&lt; 5ms in-memory, &lt; 10ms distributed</td>\n<td>Benchmark with/without rate limiting enabled</td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td>10,000+ concurrent requests</td>\n<td>Load testing with concurrent clients</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>&lt; 100 bytes per client bucket</td>\n<td>Memory profiling with 100k active clients</td>\n</tr>\n<tr>\n<td>Availability</td>\n<td>99.9% uptime despite Redis failures</td>\n<td>Chaos testing with storage failures</td>\n</tr>\n<tr>\n<td>Recovery Time</td>\n<td>&lt; 30 seconds from storage restoration</td>\n<td>Time to consistent state after Redis restart</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Design Insight</strong>: Non-functional requirements often conflict with each other. For example, low latency favors in-memory storage while high availability favors distributed storage. Our architecture must carefully balance these trade-offs, using in-memory storage for performance with asynchronous replication to distributed storage for consistency.</p>\n</blockquote>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>The <strong>explicit non-goals</strong> are equally important as the goals because they prevent scope creep and keep the implementation focused on the core learning objectives. These are features we will explicitly NOT implement, even though they might be valuable in a production system.</p>\n<p><strong>Advanced Rate Limiting Algorithms</strong>: We will not implement sliding window counters, leaky bucket algorithms, or adaptive rate limiting. While these algorithms have benefits in certain scenarios, the token bucket algorithm provides the best balance of simplicity, effectiveness, and educational value. Implementing multiple algorithms would increase complexity without significantly enhancing the learning experience around distributed systems and HTTP middleware.</p>\n<p><strong>Authentication and Authorization</strong>: The rate limiter will not handle user authentication or authorization decisions. It assumes that client identification (IP address or API key) is already available in the HTTP request headers. Integration with OAuth, JWT tokens, or user session management is outside the scope of this project. The rate limiter&#39;s job is purely to count and limit requests, not to determine who is making them.</p>\n<p><strong>Advanced Client Classification</strong>: We will not implement sophisticated client categorization beyond simple tier-based limits (e.g., free vs. premium). Features like geographic-based limits, time-of-day restrictions, or behavior-based dynamic classification are not included. The focus is on the fundamental distributed rate limiting problem rather than complex business logic.</p>\n<p><strong>Persistent Rate Limit History</strong>: The system will not maintain long-term historical data about client request patterns or rate limit violations. While this data could be valuable for analytics or security monitoring, persisting and analyzing historical data is a separate concern that would complicate the core implementation without adding educational value around rate limiting algorithms.</p>\n<p><strong>Advanced Storage Backends</strong>: We will only support Redis for distributed storage. Integration with other databases like PostgreSQL, MongoDB, or cloud-native solutions like DynamoDB is not included. Redis provides the atomic operations and performance characteristics needed for rate limiting, and supporting multiple backends would add complexity without educational benefit.</p>\n<p><strong>Production Monitoring and Alerting</strong>: While the system will include basic logging, we will not implement comprehensive metrics collection, dashboards, or alerting systems. Production monitoring is a important operational concern, but it&#39;s separate from the core rate limiting algorithms and distributed coordination patterns we&#39;re focusing on.</p>\n<p><strong>Advanced HTTP Features</strong>: The rate limiter will not handle HTTP/2 server push, WebSocket connections, or streaming responses. It focuses on traditional HTTP request/response patterns. Additionally, we won&#39;t implement sophisticated header parsing beyond basic API key and IP address extraction.</p>\n<table>\n<thead>\n<tr>\n<th>Feature Category</th>\n<th>Specific Non-Goals</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Algorithms</td>\n<td>Sliding window, leaky bucket, adaptive limiting</td>\n<td>Token bucket provides sufficient learning value</td>\n</tr>\n<tr>\n<td>Security</td>\n<td>Authentication, authorization, user management</td>\n<td>Rate limiting is orthogonal to identity management</td>\n</tr>\n<tr>\n<td>Analytics</td>\n<td>Historical data, request pattern analysis</td>\n<td>Focus on real-time limiting, not data analysis</td>\n</tr>\n<tr>\n<td>Storage</td>\n<td>Multiple database backends, cloud integrations</td>\n<td>Redis sufficient for distributed coordination learning</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Metrics, dashboards, alerting systems</td>\n<td>Production concerns beyond core algorithm focus</td>\n</tr>\n<tr>\n<td>Protocols</td>\n<td>HTTP/2, WebSockets, streaming</td>\n<td>Traditional request/response sufficient for learning</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Focused Scope for Maximum Learning</strong></p>\n<ul>\n<li><strong>Context</strong>: Rate limiting systems in production often include dozens of additional features like analytics, monitoring, and advanced client classification. Including all these features would create a realistic system but dilute the learning focus.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Build a minimal toy system with just basic token buckets</li>\n<li>Build a comprehensive production-ready system with all enterprise features</li>\n<li>Build a focused system that demonstrates key concepts without unnecessary complexity</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Option 3 - focused system covering token buckets, distributed coordination, and HTTP integration</li>\n<li><strong>Rationale</strong>: The goal is learning distributed systems patterns and rate limiting algorithms. Additional features like monitoring and analytics, while valuable in production, don&#39;t enhance understanding of the core concepts and would significantly increase implementation complexity.</li>\n<li><strong>Consequences</strong>: The resulting system demonstrates real-world patterns and could be extended to production use, but intentionally omits features that would distract from the learning objectives. Students get deep understanding of the essential concepts without getting lost in peripheral complexity.</li>\n</ul>\n</blockquote>\n<p>The non-goals are not permanent restrictions. They represent conscious decisions about where to focus learning effort in this project. Many of these features would be natural extensions in a production deployment, and understanding how to implement the core rate limiting system provides the foundation needed to add these features later.</p>\n<p><strong>Common Pitfalls in Scope Definition</strong>:</p>\n<p>⚠️ <strong>Pitfall: Scope Creep During Implementation</strong>\nMany developers start adding &quot;just one more small feature&quot; during implementation, such as basic request logging or simple metrics collection. While these additions seem harmless, they often lead to cascading complexity - logging requires structured output formats, metrics require aggregation, and both require error handling. This complexity distracts from the core learning objectives and makes debugging more difficult. Stick rigidly to the defined scope, and maintain a separate list of &quot;future enhancements&quot; to implement after completing the core system.</p>\n<p>⚠️ <strong>Pitfall: Under-Scoping Critical Infrastructure</strong>\nThe opposite pitfall is excluding necessary infrastructure components that are required for the core functionality to work properly. For example, some developers might exclude Redis connection management as &quot;infrastructure&quot; rather than core functionality, but without proper connection handling, the distributed rate limiting cannot be properly tested or demonstrated. The key distinction is whether the component is necessary to demonstrate the core learning objectives - Redis connection management is necessary for distributed coordination learning, while comprehensive monitoring is not.</p>\n<p>⚠️ <strong>Pitfall: Confusing Non-Goals with Poor Design</strong>\nExcluding features from scope doesn&#39;t mean implementing them poorly when they do arise naturally. For example, while &quot;advanced error handling&quot; might be a non-goal, basic error handling for Redis failures is essential for the distributed coordination learning objective. The non-goals define what we won&#39;t build, not what we&#39;ll build badly.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for translating the goals and non-goals into implementation decisions throughout the project.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Web Framework</td>\n<td>Flask with basic middleware</td>\n<td>FastAPI with dependency injection</td>\n<td>Flask - simpler setup, focus on rate limiting logic</td>\n</tr>\n<tr>\n<td>Redis Client</td>\n<td>redis-py with basic connection</td>\n<td>Redis cluster with sentinel failover</td>\n<td>redis-py - adequate for learning distributed patterns</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON file or environment variables</td>\n<td>Dynamic config with hot reload</td>\n<td>Environment variables - simple and testable</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Python logging module</td>\n<td>Structured logging with JSON output</td>\n<td>Python logging - built-in and sufficient</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>pytest with basic assertions</td>\n<td>pytest with fixtures and mocks</td>\n<td>pytest with fixtures - supports integration testing</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<p>The goals and non-goals inform how we organize the codebase to maintain clear separation of concerns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>rate-limiter/\n├── src/\n│   ├── __init__.py\n│   ├── rate_limiter/\n│   │   ├── __init__.py\n│   │   ├── token_bucket.py        # Core algorithm (Milestone 1)\n│   │   ├── client_tracker.py      # Per-client management (Milestone 2)\n│   │   ├── middleware.py          # HTTP integration (Milestone 3)\n│   │   ├── storage/\n│   │   │   ├── __init__.py\n│   │   │   ├── memory.py          # In-memory storage\n│   │   │   └── redis_storage.py   # Distributed storage (Milestone 4)\n│   │   └── config.py              # Rate limit policies\n├── tests/\n│   ├── unit/                      # Individual component tests\n│   ├── integration/               # End-to-end scenarios\n│   └── performance/               # Concurrency and latency tests\n├── examples/\n│   ├── flask_app.py              # Demonstration server\n│   └── load_test.py              # Simple load testing script\n└── requirements.txt</code></pre></div>\n\n<p>This structure reflects our goals by having clear modules for each major capability (token bucket, client tracking, HTTP integration, distributed storage) while excluding directories for non-goals like analytics, monitoring, or authentication.</p>\n<p><strong>C. Configuration Schema:</strong></p>\n<p>The functional goals require flexible configuration while the non-goals limit complexity:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># config.py - Complete configuration structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenBucketConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for a single rate limit policy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add capacity field (int) - maximum tokens in bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add refill_rate field (float) - tokens per second</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add initial_tokens field (Optional[int]) - starting token count</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Global rate limiter configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add default_limits field (TokenBucketConfig) - fallback policy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add client_overrides field (Dict[str, TokenBucketConfig]) - per-client limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add endpoint_limits field (Dict[str, TokenBucketConfig]) - per-endpoint limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add cleanup_interval field (int) - seconds between bucket cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add redis_url field (Optional[str]) - distributed storage connection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_config</span><span style=\"color:#E1E4E8\">() -> RateLimitConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load configuration from environment variables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Read DEFAULT_RATE_LIMIT and DEFAULT_BURST_SIZE env vars</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse CLIENT_OVERRIDES from JSON string if present  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse ENDPOINT_LIMITS from JSON string if present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set reasonable defaults for cleanup_interval (3600 seconds)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Read REDIS_URL for distributed storage (optional)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use os.getenv() with defaults, json.loads() for complex structures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>D. Goal Validation Checkpoints:</strong></p>\n<p>After implementing each milestone, verify that the functional goals are being met:</p>\n<p><strong>Milestone 1 Checkpoint - Token Bucket Algorithm:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run unit tests for core algorithm</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_token_bucket.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output: All tests pass, including:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - test_token_generation_over_time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - test_burst_handling_up_to_capacity  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - test_rate_limiting_over_long_period</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - test_thread_safety_under_concurrency</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint - Per-Client Tracking:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run integration test with multiple clients</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/multi_client_test.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Client A can make 100 requests/minute</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Client B has independent 100 requests/minute  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Client A exhausting quota doesn't affect Client B</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Inactive clients get cleaned up after 1 hour</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint - HTTP Integration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start the Flask example server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/flask_app.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test rate limiting with curl</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -i</span><span style=\"color:#9ECBFF\"> http://localhost:5000/api/test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see: X-RateLimit-Limit, X-RateLimit-Remaining headers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Exceed rate limit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..101}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span><span style=\"color:#B392F0\"> curl</span><span style=\"color:#9ECBFF\"> http://localhost:5000/api/test</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see: HTTP 429 response with Retry-After header</span></span></code></pre></div>\n\n<p><strong>Milestone 4 Checkpoint - Distributed Consistency:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start Redis and two server instances</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-server</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/flask_app.py</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 5000</span><span style=\"color:#E1E4E8\"> &#x26;  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/flask_app.py</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 5001</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test distributed rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/distributed_test.py</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Same client hitting both servers shares single rate limit</span></span></code></pre></div>\n\n<p><strong>E. Non-Goal Validation:</strong></p>\n<p>Ensure that non-goals are respected by checking what&#39;s NOT implemented:</p>\n<table>\n<thead>\n<tr>\n<th>Non-Goal Category</th>\n<th>Validation Check</th>\n<th>Expected Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Advanced Algorithms</td>\n<td><code>grep -r &quot;sliding.*window&quot; src/</code></td>\n<td>No matches found</td>\n</tr>\n<tr>\n<td>Authentication</td>\n<td><code>grep -r &quot;jwt|oauth|session&quot; src/</code></td>\n<td>No matches found</td>\n</tr>\n<tr>\n<td>Historical Data</td>\n<td>Check for database schemas or time-series storage</td>\n<td>None present</td>\n</tr>\n<tr>\n<td>Multiple Storage Backends</td>\n<td>Count storage implementations</td>\n<td>Only memory.py and redis_storage.py</td>\n</tr>\n<tr>\n<td>Production Monitoring</td>\n<td>Check for metrics/dashboard code</td>\n<td>Only basic logging present</td>\n</tr>\n</tbody></table>\n<p><strong>F. Debugging Goal Adherence:</strong></p>\n<p>Common issues that indicate scope creep or goal misalignment:</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Implementation taking much longer than expected</td>\n<td>Adding features beyond defined scope</td>\n<td>Review code against explicit non-goals, remove scope creep</td>\n</tr>\n<tr>\n<td>Complex configuration with many options</td>\n<td>Over-engineering flexibility requirements</td>\n<td>Simplify to support only defined functional goals</td>\n</tr>\n<tr>\n<td>Difficulty testing core functionality</td>\n<td>Too many dependencies or coupled concerns</td>\n<td>Refactor to match recommended file structure</td>\n</tr>\n<tr>\n<td>Performance problems in basic scenarios</td>\n<td>Implementing non-goal features that add overhead</td>\n<td>Profile code and remove unnecessary complexity</td>\n</tr>\n<tr>\n<td>Hard to understand the main rate limiting logic</td>\n<td>Core algorithm buried in peripheral features</td>\n<td>Extract token bucket logic to dedicated module</td>\n</tr>\n</tbody></table>\n<p>The goals and non-goals serve as a constant reference throughout implementation. When facing any design decision, the first question should be: &quot;Does this serve one of our functional goals or non-functional requirements, or is this scope creep that should be deferred?&quot;</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section establishes the foundational component structure that evolves through each milestone</p>\n</blockquote>\n<p>Think of our rate limiter as a sophisticated <strong>automated traffic control system</strong> at a busy intersection. Just as traffic lights, sensors, and control systems work together to manage vehicle flow, our rate limiter coordinates multiple components to manage request flow. The traffic light controller (HTTP middleware) makes immediate allow/deny decisions, the sensor array (client tracker) monitors each lane&#39;s activity, the timing mechanism (token bucket) determines when each lane gets its turn, and the central coordination system (distributed storage) ensures all intersections work in harmony across the city.</p>\n<p>This architectural foundation provides the structural blueprint for building a production-ready rate limiting system that can scale from a single server to a distributed cluster while maintaining consistent behavior and performance characteristics.</p>\n<h3 id=\"component-responsibilities\">Component Responsibilities</h3>\n<p>Our rate limiter architecture consists of four primary components, each with distinct responsibilities and clear boundaries. Understanding these boundaries is crucial for implementing a maintainable system that can evolve through each milestone without creating tight coupling or unclear ownership.</p>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Fsystem-components.svg\" alt=\"Rate Limiter System Components\"></p>\n<p>The <strong>HTTP Middleware</strong> serves as the primary entry point and decision enforcer for all incoming requests. This component integrates directly with web frameworks like Flask or Express, intercepting every HTTP request before it reaches application logic. Its core responsibility is making immediate allow/deny decisions based on rate limit evaluations and translating those decisions into proper HTTP responses. When a request is allowed, the middleware adds informational headers like <code>X-RateLimit-Remaining</code> and <code>X-RateLimit-Limit</code> to help clients understand their current quota status. When a request exceeds limits, the middleware returns a <code>429 Too Many Requests</code> response with a <code>Retry-After</code> header indicating when the client can retry.</p>\n<p>The middleware component maintains no state of its own - it acts purely as a coordinator that delegates rate limit evaluation to other components while handling the HTTP-specific concerns of request processing and response formatting. This stateless design ensures that the middleware remains lightweight and can be easily tested in isolation from the complex rate limiting logic.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Boundary</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Interception</td>\n<td>Capture all incoming HTTP requests before application processing</td>\n<td>Only handles HTTP layer - no rate limiting logic</td>\n</tr>\n<tr>\n<td>Client Identification</td>\n<td>Extract client identifiers from IP addresses, headers, or authentication tokens</td>\n<td>Extracts identifiers but doesn&#39;t manage client state</td>\n</tr>\n<tr>\n<td>Rate Limit Evaluation</td>\n<td>Coordinate with client tracker to check current rate limit status</td>\n<td>Calls other components but makes no rate limiting decisions itself</td>\n</tr>\n<tr>\n<td>HTTP Response Generation</td>\n<td>Generate proper status codes, headers, and error messages</td>\n<td>Only handles HTTP formatting - no business logic</td>\n</tr>\n<tr>\n<td>Framework Integration</td>\n<td>Provide clean integration points for Flask, Express, and other frameworks</td>\n<td>Framework-specific adapters only - core logic is framework-agnostic</td>\n</tr>\n</tbody></table>\n<p>The <strong>Client Tracker</strong> manages the complex task of maintaining separate rate limiting state for each unique client accessing the system. This component handles client identification strategies, efficiently stores per-client token buckets, and implements cleanup mechanisms to prevent memory leaks from inactive clients. The client tracker acts as a factory and registry for token buckets, creating new buckets on-demand when previously unseen clients make requests and maintaining a mapping between client identifiers and their corresponding bucket instances.</p>\n<p>One of the most critical responsibilities of the client tracker is implementing <strong>stale bucket cleanup</strong> to prevent unbounded memory growth. In a system serving thousands or millions of unique clients over time, keeping every client&#39;s bucket in memory indefinitely would eventually exhaust available resources. The client tracker implements configurable cleanup policies that remove buckets for clients that haven&#39;t made requests within a specified timeout period.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Boundary</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client Identification</td>\n<td>Determine unique client identity from IP addresses, API keys, or custom headers</td>\n<td>Identification logic only - no rate limit evaluation</td>\n</tr>\n<tr>\n<td>Bucket Lifecycle Management</td>\n<td>Create, store, and destroy token buckets for individual clients</td>\n<td>Manages bucket instances but not bucket algorithm logic</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Clean up stale buckets to prevent memory leaks from inactive clients</td>\n<td>Cleanup timing and policies - not token bucket internals</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Provide thread-safe access to client buckets under high load</td>\n<td>Concurrency control for bucket access - not HTTP processing</td>\n</tr>\n<tr>\n<td>Per-Client Configuration</td>\n<td>Support different rate limits for premium clients or specific use cases</td>\n<td>Configuration management - not limit enforcement</td>\n</tr>\n</tbody></table>\n<p>The <strong>Token Bucket</strong> implements the core rate limiting algorithm that determines whether individual requests should be allowed or denied. Each token bucket instance maintains its own state including current token count, last refill timestamp, and configuration parameters like capacity and refill rate. The token bucket algorithm provides natural burst handling by allowing clients to accumulate tokens during periods of low activity and spend them rapidly during bursts, up to the configured bucket capacity.</p>\n<p>Token buckets operate independently of HTTP concerns, client identification, or distributed coordination. A single token bucket instance knows only about tokens flowing in and out over time - it has no knowledge of which client it serves or how it fits into the broader system architecture. This separation of concerns makes the token bucket algorithm easy to test, reason about, and optimize independently.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Boundary</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Generation</td>\n<td>Calculate new tokens to add based on elapsed time and configured refill rate</td>\n<td>Time-based calculations only - no external dependencies</td>\n</tr>\n<tr>\n<td>Token Consumption</td>\n<td>Deduct tokens for incoming requests and determine allow/deny decisions</td>\n<td>Local state only - no knowledge of clients or HTTP</td>\n</tr>\n<tr>\n<td>Burst Handling</td>\n<td>Allow short bursts up to bucket capacity while maintaining long-term rate limits</td>\n<td>Algorithm logic only - no policy decisions</td>\n</tr>\n<tr>\n<td>State Management</td>\n<td>Track current token count, last refill time, and bucket configuration</td>\n<td>Internal state only - no persistence or distribution</td>\n</tr>\n<tr>\n<td>Thread Safety</td>\n<td>Ensure atomic token operations under concurrent access</td>\n<td>Local concurrency only - no distributed coordination</td>\n</tr>\n</tbody></table>\n<p>The <strong>Distributed Storage</strong> component handles the complex challenge of maintaining consistent rate limiting state across multiple server instances in a distributed deployment. While early milestones use in-memory storage for simplicity, production systems require shared state to prevent clients from bypassing limits by sending requests to different servers. This component abstracts the storage backend (typically Redis) and provides atomic operations for reading and updating token bucket state across the cluster.</p>\n<p>The distributed storage component implements atomic read-modify-write operations using Redis Lua scripts to ensure that token consumption decisions remain consistent even under high concurrent load from multiple servers. It also handles failure scenarios gracefully, implementing fallback strategies when the distributed storage becomes unavailable while maintaining system availability.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Boundary</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>State Persistence</td>\n<td>Store and retrieve token bucket state from distributed storage (Redis)</td>\n<td>Storage operations only - no rate limiting logic</td>\n</tr>\n<tr>\n<td>Atomic Operations</td>\n<td>Ensure consistent token updates across multiple server instances</td>\n<td>Atomicity guarantees only - no business logic</td>\n</tr>\n<tr>\n<td>Connection Management</td>\n<td>Handle Redis connections, retries, and connection pooling</td>\n<td>Infrastructure concerns only - no application logic</td>\n</tr>\n<tr>\n<td>Failure Handling</td>\n<td>Provide graceful degradation when distributed storage is unavailable</td>\n<td>Error handling and fallbacks - no normal operation logic</td>\n</tr>\n<tr>\n<td>Data Serialization</td>\n<td>Convert token bucket state to/from storage format efficiently</td>\n<td>Data format concerns only - no state interpretation</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Architectural Insight</strong>: The component boundaries are designed around the principle of <strong>single responsibility</strong> with minimal coupling. Each component can be developed, tested, and evolved independently while maintaining clear interfaces with other components. This modularity becomes especially important as the system evolves from simple in-memory rate limiting to distributed coordination across multiple servers.</p>\n</blockquote>\n<h3 id=\"request-processing-flow\">Request Processing Flow</h3>\n<p>Understanding how HTTP requests flow through the rate limiting pipeline is essential for implementing each component correctly and debugging issues that arise during development. The request processing flow represents the coordination between all components to make consistent rate limiting decisions with minimal latency overhead.</p>\n<p>The following sequence describes the complete journey of an HTTP request through our rate limiting system, from initial receipt to final allow/deny decision:</p>\n<p><strong>1. HTTP Request Arrival and Middleware Interception</strong></p>\n<p>When an HTTP request arrives at our web server, the rate limiting middleware intercepts it before any application logic executes. This early interception is crucial because rate limiting decisions must be made before consuming resources on request processing. The middleware extracts the raw request object containing headers, IP address, and any authentication information needed for client identification.</p>\n<p>The middleware performs initial request validation to ensure it has sufficient information to identify the client and determine applicable rate limits. If critical identification information is missing (such as required API key headers), the middleware can immediately reject the request without consulting other components.</p>\n<p><strong>2. Client Identification and Bucket Retrieval</strong></p>\n<p>The middleware delegates client identification to the client tracker component, passing along relevant request metadata including IP address, user agent, authentication headers, or API keys. The client tracker applies configured identification strategies to determine a unique client identifier - this might be as simple as the source IP address or as complex as a combination of API key and endpoint pattern.</p>\n<p>Once the client identifier is determined, the client tracker checks its internal registry for an existing token bucket associated with this client. If a bucket exists and hasn&#39;t exceeded the stale timeout threshold, the tracker returns the existing bucket instance. If no bucket exists or the existing bucket has become stale, the tracker creates a new bucket instance with appropriate configuration parameters (which may include per-client overrides for premium users or specific rate limits).</p>\n<p><strong>3. Rate Limit Configuration Resolution</strong></p>\n<p>Before evaluating the token bucket, the system must determine which rate limiting rules apply to this specific request. The client tracker consults the <code>RateLimitConfig</code> to resolve any per-client overrides, per-endpoint limits, or default rate limiting parameters. This resolution process considers multiple configuration sources in priority order: specific client overrides take precedence over endpoint-specific limits, which take precedence over default system-wide limits.</p>\n<p>The configuration resolution also determines how many tokens this particular request should consume. While most requests consume a single token, some systems implement weighted rate limiting where expensive operations consume multiple tokens to reflect their true resource cost.</p>\n<p><strong>4. Token Bucket Evaluation and Decision</strong></p>\n<p>With the appropriate token bucket instance and consumption requirements determined, the system calls the bucket&#39;s token consumption method to evaluate whether the request should be allowed. The token bucket first performs a <strong>token refill calculation</strong> based on elapsed time since the last access, adding new tokens according to the configured refill rate while respecting the maximum bucket capacity.</p>\n<p>After refilling tokens, the bucket attempts to consume the required number of tokens for the current request. If sufficient tokens are available, they are deducted from the bucket and the method returns an allow decision along with updated bucket state. If insufficient tokens remain, the bucket state is unchanged and the method returns a deny decision with information about when sufficient tokens will next be available.</p>\n<p><strong>5. HTTP Response Generation and Header Population</strong></p>\n<p>Based on the token bucket&#39;s decision, the middleware generates an appropriate HTTP response. For allowed requests, the middleware adds standard rate limiting headers to inform the client of their current status:</p>\n<ul>\n<li><code>X-RateLimit-Limit</code>: The maximum number of requests allowed per time window</li>\n<li><code>X-RateLimit-Remaining</code>: The number of tokens currently available in the client&#39;s bucket</li>\n<li><code>X-RateLimit-Reset</code>: Timestamp when the bucket will next be refilled to capacity</li>\n</ul>\n<p>For denied requests, the middleware generates a <code>429 Too Many Requests</code> response with additional headers:</p>\n<ul>\n<li><code>Retry-After</code>: Number of seconds until sufficient tokens will be available</li>\n<li>Detailed error message explaining the rate limit violation</li>\n</ul>\n<p><strong>6. Request Forwarding or Termination</strong></p>\n<p>If the rate limiting decision was to allow the request, the middleware passes control to the next component in the HTTP processing pipeline, typically the application&#39;s routing logic. The rate limiting process is complete, and the application handles the request normally.</p>\n<p>If the rate limiting decision was to deny the request, the middleware immediately returns the <code>429</code> response without forwarding to application logic. This early termination protects downstream systems from excessive load while providing clear feedback to the client about why their request was rejected.</p>\n<table>\n<thead>\n<tr>\n<th>Processing Stage</th>\n<th>Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>Error Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Interception</td>\n<td>HTTP Middleware</td>\n<td>Raw HTTP request</td>\n<td>Client identification data</td>\n<td>Missing required headers</td>\n</tr>\n<tr>\n<td>Client Identification</td>\n<td>Client Tracker</td>\n<td>Request metadata</td>\n<td>Client ID and bucket instance</td>\n<td>Invalid API keys, malformed headers</td>\n</tr>\n<tr>\n<td>Configuration Resolution</td>\n<td>Client Tracker</td>\n<td>Client ID, endpoint pattern</td>\n<td>Rate limit configuration</td>\n<td>Missing configuration data</td>\n</tr>\n<tr>\n<td>Token Refill</td>\n<td>Token Bucket</td>\n<td>Current time, bucket state</td>\n<td>Updated token count</td>\n<td>Clock drift, time calculation overflow</td>\n</tr>\n<tr>\n<td>Token Consumption</td>\n<td>Token Bucket</td>\n<td>Required tokens, bucket state</td>\n<td>Allow/deny decision</td>\n<td>Insufficient tokens</td>\n</tr>\n<tr>\n<td>Response Generation</td>\n<td>HTTP Middleware</td>\n<td>Rate limit decision</td>\n<td>HTTP response with headers</td>\n<td>Header formatting errors</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Implementation Note</strong>: The entire request processing flow must complete in microseconds to avoid adding significant latency to API responses. This performance requirement influences many design decisions, including the choice of in-memory bucket storage for non-distributed deployments and the use of atomic Redis operations for distributed scenarios.</p>\n</blockquote>\n<p><strong>Distributed Processing Flow Considerations</strong></p>\n<p>In distributed deployments where multiple server instances share rate limiting state through Redis, the request processing flow includes additional coordination steps. Instead of updating local token bucket state, the system must perform atomic read-modify-write operations against shared Redis storage using Lua scripts to maintain consistency.</p>\n<p>The distributed flow modifies steps 4 and 5 to include Redis operations:</p>\n<ul>\n<li><strong>4a. Redis State Retrieval</strong>: Fetch current token bucket state from Redis using the client identifier as the key</li>\n<li><strong>4b. Atomic Token Operation</strong>: Execute a Lua script that performs token refill calculation, consumption attempt, and state update in a single atomic operation  </li>\n<li><strong>4c. Result Processing</strong>: Handle the Lua script result to determine allow/deny decision and updated state</li>\n<li><strong>5a. Fallback Handling</strong>: If Redis operations fail, implement graceful degradation using local rate limiting or fail-open policies</li>\n</ul>\n<p>This distributed coordination adds latency (typically 1-5ms for Redis operations) but ensures that clients cannot bypass rate limits by distributing requests across multiple server instances.</p>\n<h3 id=\"recommended-file-structure\">Recommended File Structure</h3>\n<p>Organizing the rate limiter codebase with clear module boundaries and logical separation of concerns is essential for maintainability as the system evolves through each milestone. The following file structure supports independent development of each component while maintaining clean interfaces and testability.</p>\n<p>The recommended structure separates <strong>core algorithm logic</strong> from <strong>integration concerns</strong>, <strong>storage backends</strong> from <strong>business logic</strong>, and <strong>configuration management</strong> from <strong>runtime operation</strong>. This separation enables focused development during each milestone and facilitates testing individual components in isolation.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>rate_limiter/\n├── __init__.py                     # Package initialization and main exports\n├── config/\n│   ├── __init__.py\n│   ├── config.py                   # RateLimitConfig and TokenBucketConfig definitions\n│   └── loader.py                   # Environment variable loading and validation\n├── core/\n│   ├── __init__.py\n│   ├── token_bucket.py             # Core token bucket algorithm implementation\n│   └── algorithms.py               # Alternative algorithms (sliding window, etc.)\n├── client/\n│   ├── __init__.py\n│   ├── tracker.py                  # Client identification and bucket management\n│   ├── identifier.py               # Client identification strategies\n│   └── cleanup.py                  # Background cleanup of stale buckets\n├── middleware/\n│   ├── __init__.py\n│   ├── base.py                     # Framework-agnostic middleware logic\n│   ├── flask_integration.py        # Flask-specific middleware implementation\n│   ├── fastapi_integration.py      # FastAPI integration (future extension)\n│   └── headers.py                  # HTTP header management and formatting\n├── storage/\n│   ├── __init__.py\n│   ├── base.py                     # Abstract storage interface\n│   ├── memory.py                   # In-memory storage for single-instance deployments\n│   ├── redis_storage.py            # Redis-backed distributed storage\n│   └── lua_scripts/                # Redis Lua scripts for atomic operations\n│       ├── consume_tokens.lua\n│       └── refill_tokens.lua\n├── utils/\n│   ├── __init__.py\n│   ├── time_utils.py               # Time handling utilities and precision management\n│   └── errors.py                   # Custom exception definitions\n└── tests/\n    ├── __init__.py\n    ├── unit/                       # Unit tests for individual components\n    │   ├── test_token_bucket.py\n    │   ├── test_client_tracker.py\n    │   ├── test_middleware.py\n    │   └── test_redis_storage.py\n    ├── integration/                # Integration tests across components\n    │   ├── test_http_flow.py\n    │   └── test_distributed_consistency.py\n    └── fixtures/                   # Test data and mock configurations\n        ├── config_samples.py\n        └── mock_redis.py</code></pre></div>\n\n<p><strong>Core Module Organization Rationale</strong></p>\n<p>The <code>config/</code> module centralizes all configuration management and validation logic, making it easy to add new configuration options as the system evolves. The <code>config.py</code> file defines the primary data structures (<code>RateLimitConfig</code> and <code>TokenBucketConfig</code>) used throughout the system, while <code>loader.py</code> handles the complex task of loading configuration from environment variables, validating values, and providing sensible defaults.</p>\n<p>The <code>core/</code> module contains the heart of the rate limiting logic - the token bucket algorithm implementation that operates independently of HTTP concerns, client tracking, or storage backends. This separation allows the core algorithm to be thoroughly tested in isolation and makes it possible to implement alternative rate limiting algorithms (like sliding window counters) without affecting other system components.</p>\n<blockquote>\n<p><strong>Decision: Separate Core Algorithm from Integration Logic</strong></p>\n<ul>\n<li><strong>Context</strong>: Rate limiting systems often need to support multiple algorithms and storage backends while maintaining consistent HTTP interfaces</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Monolithic design with all logic in middleware</li>\n<li>Layered architecture with separated concerns</li>\n<li>Plugin-based architecture with dynamic loading</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Layered architecture with clear separation between core algorithm, client tracking, middleware, and storage</li>\n<li><strong>Rationale</strong>: Enables independent testing and evolution of each component, supports multiple storage backends without affecting algorithm logic, and facilitates adding new rate limiting algorithms in the future</li>\n<li><strong>Consequences</strong>: Slightly more complex initial setup but much easier maintenance and testing; enables milestone-based development where each layer can be implemented and verified independently</li>\n</ul>\n</blockquote>\n<p><strong>Client and Storage Module Design</strong></p>\n<p>The <code>client/</code> module handles all aspects of per-client rate limiting including identification strategies, bucket lifecycle management, and cleanup operations. Separating client identification logic into its own module (<code>identifier.py</code>) makes it easy to support different identification schemes (IP-based, API key-based, or custom headers) without affecting bucket management code.</p>\n<p>The <code>storage/</code> module provides a clean abstraction over different storage backends, enabling the system to start with simple in-memory storage and evolve to distributed Redis storage without changing core logic. The abstract <code>base.py</code> interface ensures that all storage implementations provide the same guarantees around atomicity and consistency.</p>\n<table>\n<thead>\n<tr>\n<th>Module</th>\n<th>Primary Purpose</th>\n<th>Key Files</th>\n<th>Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>config/</code></td>\n<td>Configuration management and validation</td>\n<td><code>config.py</code>, <code>loader.py</code></td>\n<td>None (foundation module)</td>\n</tr>\n<tr>\n<td><code>core/</code></td>\n<td>Rate limiting algorithms and token bucket logic</td>\n<td><code>token_bucket.py</code></td>\n<td><code>config/</code> only</td>\n</tr>\n<tr>\n<td><code>client/</code></td>\n<td>Per-client tracking and bucket management</td>\n<td><code>tracker.py</code>, <code>cleanup.py</code></td>\n<td><code>core/</code>, <code>storage/</code></td>\n</tr>\n<tr>\n<td><code>middleware/</code></td>\n<td>HTTP integration and response handling</td>\n<td><code>flask_integration.py</code>, <code>headers.py</code></td>\n<td><code>client/</code>, <code>config/</code></td>\n</tr>\n<tr>\n<td><code>storage/</code></td>\n<td>Storage backend abstraction and implementations</td>\n<td><code>memory.py</code>, <code>redis_storage.py</code></td>\n<td><code>config/</code>, <code>utils/</code></td>\n</tr>\n<tr>\n<td><code>utils/</code></td>\n<td>Shared utilities and error handling</td>\n<td><code>time_utils.py</code>, <code>errors.py</code></td>\n<td>None</td>\n</tr>\n</tbody></table>\n<p><strong>Testing Strategy Integration</strong></p>\n<p>The file structure directly supports comprehensive testing by separating unit tests (which test individual modules in isolation) from integration tests (which test component interactions and end-to-end behavior). Each core module has corresponding unit tests that can run without external dependencies, while integration tests verify that components work correctly together.</p>\n<p>The <code>fixtures/</code> directory provides shared test data and mock implementations that can be reused across multiple test modules. This is particularly important for testing distributed scenarios where mock Redis implementations allow testing coordination logic without requiring actual Redis instances.</p>\n<p><strong>Framework Integration Patterns</strong></p>\n<p>The middleware module structure anticipates supporting multiple web frameworks while avoiding code duplication. The <code>base.py</code> file contains framework-agnostic logic for rate limiting decisions and HTTP header generation, while framework-specific files (<code>flask_integration.py</code>) handle the unique requirements of each framework&#39;s middleware system.</p>\n<p>This pattern makes it straightforward to add support for new frameworks by implementing the framework-specific integration layer while reusing all the core rate limiting logic. It also ensures that the bulk of the middleware logic can be tested independently of any specific web framework.</p>\n<blockquote>\n<p><strong>Implementation Insight</strong>: Start development with the <code>config/</code> and <code>core/</code> modules to establish the foundational data structures and algorithm logic. These modules have no external dependencies and can be completely implemented and tested before moving on to client tracking or HTTP integration. This approach aligns perfectly with the milestone-based development plan.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The following implementation guidance provides the foundational code structure needed to build the rate limiter architecture. Focus on establishing clean interfaces between components and preparing the module structure that will support all four milestones.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Web Framework</td>\n<td>Flask with built-in middleware</td>\n<td>FastAPI with custom middleware classes</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Environment variables + JSON</td>\n<td>YAML configuration with validation</td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td>threading.Lock for bucket access</td>\n<td>asyncio with async/await patterns</td>\n</tr>\n<tr>\n<td>Time Handling</td>\n<td>time.time() with float precision</td>\n<td>time.time_ns() with nanosecond precision</td>\n</tr>\n<tr>\n<td>Redis Client</td>\n<td>redis-py with connection pooling</td>\n<td>aioredis for async operations</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Python logging module</td>\n<td>Structured logging with JSON output</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure Setup</strong></p>\n<p>Start by creating the basic module structure that will support all milestones:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">mkdir</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> rate_limiter/{config,core,client,middleware,storage/lua_scripts,utils,tests/{unit,integration,fixtures}}</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">touch</span><span style=\"color:#9ECBFF\"> rate_limiter/{__init__.py,config/__init__.py,core/__init__.py,client/__init__.py,middleware/__init__.py,storage/__init__.py,utils/__init__.py,tests/__init__.py}</span></span></code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p><strong>Configuration Management (<code>config/config.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenBucketConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for a single token bucket instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refill_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    initial_tokens: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.initial_tokens </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.initial_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.capacity </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Token bucket capacity must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.refill_rate </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Token bucket refill rate must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Global configuration for the rate limiting system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    default_limits: TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_overrides: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenBucketConfig]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    endpoint_limits: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenBucketConfig] </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanup_interval: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_environment</span><span style=\"color:#E1E4E8\">(cls) -> </span><span style=\"color:#9ECBFF\">'RateLimitConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load configuration from environment variables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parse default rate limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        default_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">'DEFAULT_RATE_LIMIT'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'100'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        default_burst </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">'DEFAULT_BURST_SIZE'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'10'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        default_limits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">default_burst,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">default_rate </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 60.0</span><span style=\"color:#6A737D\">  # Convert per-minute to per-second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parse client overrides from JSON</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client_overrides </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> os.getenv(</span><span style=\"color:#9ECBFF\">'CLIENT_OVERRIDES'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            override_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(os.getenv(</span><span style=\"color:#9ECBFF\">'CLIENT_OVERRIDES'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> client_id, limits </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> override_data.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                client_overrides[client_id] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">limits)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parse endpoint limits from JSON  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        endpoint_limits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> os.getenv(</span><span style=\"color:#9ECBFF\">'ENDPOINT_LIMITS'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            endpoint_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(os.getenv(</span><span style=\"color:#9ECBFF\">'ENDPOINT_LIMITS'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> pattern, limits </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> endpoint_data.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                endpoint_limits[pattern] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">limits)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            default_limits</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">default_limits,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            client_overrides</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client_overrides,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            endpoint_limits</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">endpoint_limits,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cleanup_interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">'CLEANUP_INTERVAL'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'300'</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            redis_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'REDIS_URL'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<p><strong>Time Utilities (<code>utils/time_utils.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> current_time_seconds</span><span style=\"color:#E1E4E8\">() -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Get current time with high precision for token calculations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_time_delta</span><span style=\"color:#E1E4E8\">(start_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, end_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate time difference with overflow protection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    delta </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Protect against clock drift or system clock changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> delta </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Protect against unreasonably large deltas (more than 1 day)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> delta </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 86400</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 86400.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> delta</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_tokens_to_add</span><span style=\"color:#E1E4E8\">(time_delta: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, refill_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate number of tokens to add based on elapsed time and refill rate.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens_to_add </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time_delta </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Ensure we don't overflow integer limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(tokens_to_add), </span><span style=\"color:#79B8FF\">10_000_000</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Custom Exceptions (<code>utils/errors.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all rate limiting errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConfigurationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RateLimitError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when rate limit configuration is invalid.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StorageError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RateLimitError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when storage backend operations fail.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClientIdentificationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RateLimitError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when client cannot be properly identified.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p><strong>Token Bucket Core (<code>core/token_bucket.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, NamedTuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..config.config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..utils.time_utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> current_time_seconds, calculate_time_delta, calculate_tokens_to_add</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenConsumptionResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NamedTuple</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result of attempting to consume tokens from a bucket.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowed: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens_remaining: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_after_seconds: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenBucket</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe token bucket implementation for rate limiting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TokenBucketConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.capacity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.capacity</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.refill_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.initial_tokens</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_refill_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_time_seconds()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Attempt to consume tokens from the bucket.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns TokenConsumptionResult indicating whether the request was allowed,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        how many tokens remain, and when to retry if denied.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current time and calculate time delta since last refill</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate how many new tokens to add based on elapsed time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add new tokens to bucket, respecting maximum capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update last_refill_time to current time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check if enough tokens available for this request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If enough tokens, deduct them and return success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: If not enough tokens, calculate retry_after_seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return TokenConsumptionResult with appropriate values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: retry_after = (tokens_requested - current_tokens) / refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_current_tokens</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current token count (for monitoring/debugging).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement thread-safe token count retrieval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This should trigger a refill calculation but not consume tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Client Tracker Foundation (<code>client/tracker.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..core.token_bucket </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenBucket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..config.config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RateLimitConfig, TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..utils.errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ClientIdentificationError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClientBucketTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages per-client token buckets with automatic cleanup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: RateLimitConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.buckets: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenBucket] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_access: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_bucket_for_client</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> TokenBucket:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Get or create a token bucket for the specified client.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            client_id: Unique identifier for the client</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            endpoint: Optional endpoint pattern for endpoint-specific limits</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            TokenBucket instance for this client</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine appropriate TokenBucketConfig for this client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check client_overrides first, then endpoint_limits, then default_limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Acquire lock for thread-safe bucket access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if bucket already exists for this client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If bucket exists, update last_access time and return it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If bucket doesn't exist, create new TokenBucket with resolved config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store new bucket and set last_access time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return the bucket instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_stale_buckets</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Remove buckets that haven't been accessed recently.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of buckets removed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate cutoff time based on cleanup_interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Acquire lock for thread-safe cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Iterate through last_access times to find stale buckets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Remove stale buckets from both buckets and last_access dicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return count of removed buckets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> identify_client</span><span style=\"color:#E1E4E8\">(self, request_data: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Extract client identifier from request data.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            request_data: Dictionary containing IP, headers, etc.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Unique client identifier string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for API key in request headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Fall back to IP address if no API key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate that identifier is not empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return normalized client identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Consider rate limiting per API key vs per IP</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Milestone Checkpoint</strong></p>\n<p>After implementing the basic architecture structure:</p>\n<ol>\n<li><strong>Verify Module Structure</strong>: Run <code>python -c &quot;import rate_limiter; print(&#39;Import successful&#39;)&quot;</code> </li>\n<li><strong>Test Configuration Loading</strong>: Set environment variables and verify <code>RateLimitConfig.from_environment()</code> works correctly</li>\n<li><strong>Validate Component Isolation</strong>: Each module should be importable independently without circular dependencies</li>\n</ol>\n<p>Expected behavior after this milestone:</p>\n<ul>\n<li>All modules can be imported without errors</li>\n<li>Configuration can be loaded from environment variables</li>\n<li>Basic data structures (TokenBucketConfig, RateLimitConfig) are properly defined</li>\n<li>Time utilities handle edge cases like clock drift</li>\n<li>File structure supports independent development of each component</li>\n</ul>\n<p><strong>F. Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Import errors on module loading</td>\n<td>Circular dependencies between modules</td>\n<td>Check import statements in <code>__init__.py</code> files</td>\n<td>Use relative imports and avoid importing from parent modules</td>\n</tr>\n<tr>\n<td>Configuration validation failures</td>\n<td>Invalid environment variable formats</td>\n<td>Print parsed values before validation</td>\n<td>Add error handling with descriptive messages in config loader</td>\n</tr>\n<tr>\n<td>Thread safety test failures</td>\n<td>Missing locks or improper locking order</td>\n<td>Add logging around lock acquisition</td>\n<td>Use context managers (<code>with lock:</code>) consistently</td>\n</tr>\n<tr>\n<td>Time calculation inconsistencies</td>\n<td>Float precision issues or clock drift</td>\n<td>Log timestamps and calculated deltas</td>\n<td>Use nanosecond precision or validate time delta bounds</td>\n</tr>\n</tbody></table>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section defines the foundational data structures used from basic token bucket implementation through distributed rate limiting</p>\n</blockquote>\n<p>Think of our data model as the <strong>blueprint for a sophisticated banking system</strong>. Just as a bank needs clear definitions for accounts, transactions, customers, and policies, our rate limiter needs precise data structures for token buckets, client identities, rate limit rules, and storage mechanisms. Each data type serves a specific purpose in the rate limiting ecosystem, and their relationships determine how efficiently we can track and enforce limits across thousands of concurrent clients.</p>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Fdata-model-relationships.svg\" alt=\"Data Model and Type Relationships\"></p>\n<p>The data model forms the foundation that supports all rate limiting operations. Whether we&#39;re implementing a simple in-memory token bucket or a distributed system coordinating across multiple servers, these core data structures remain consistent. Understanding these types deeply ensures that our implementation can evolve from basic functionality to enterprise-scale distributed rate limiting without architectural rewrites.</p>\n<h3 id=\"core-data-types\">Core Data Types</h3>\n<p>The core data types represent the fundamental entities in our rate limiting system. Each type encapsulates specific responsibilities and maintains clear boundaries between different aspects of rate limiting functionality.</p>\n<h4 id=\"token-bucket-state-and-configuration\">Token Bucket State and Configuration</h4>\n<p>The <code>TokenBucketConfig</code> type defines the behavioral parameters for any token bucket instance. Think of this as the <strong>specification sheet for a water tank</strong> - it defines the tank&#39;s maximum capacity, how fast water flows in through the inlet pipe, and how many tokens should be present when the tank is first installed.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>capacity</code></td>\n<td><code>int</code></td>\n<td>Maximum number of tokens the bucket can hold, representing burst capacity</td>\n</tr>\n<tr>\n<td><code>refill_rate</code></td>\n<td><code>float</code></td>\n<td>Tokens added per second during normal operation, supporting fractional rates</td>\n</tr>\n<tr>\n<td><code>initial_tokens</code></td>\n<td><code>Optional[int]</code></td>\n<td>Starting token count when bucket is created; defaults to full capacity if not specified</td>\n</tr>\n</tbody></table>\n<p>The <code>capacity</code> field determines how large bursts of requests can be handled before rate limiting kicks in. A bucket with capacity 100 allows a client to make 100 rapid requests even if their sustained rate limit is much lower. The <code>refill_rate</code> supports fractional values like 0.5 tokens per second, enabling fine-grained rate control for premium API tiers or resource-intensive operations.</p>\n<p>The <code>initial_tokens</code> field provides flexibility in bucket initialization. Setting it to a lower value prevents immediate bursts from new clients, while <code>None</code> defaults to full capacity for standard scenarios. This becomes crucial when implementing &quot;warm-up&quot; periods for new API keys or handling client onboarding flows.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Using <code>Optional[int]</code> for <code>initial_tokens</code> rather than a default parameter allows the configuration to explicitly distinguish between &quot;start with zero tokens&quot; and &quot;use the default behavior&quot;. This prevents ambiguity in distributed scenarios where default values might differ across server instances.</p>\n</blockquote>\n<h4 id=\"token-consumption-results\">Token Consumption Results</h4>\n<p>The <code>TokenConsumptionResult</code> type encapsulates the outcome of attempting to consume tokens from a bucket. This structure provides all information needed to make HTTP response decisions and inform clients about their rate limit status.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>allowed</code></td>\n<td><code>bool</code></td>\n<td>Whether the token consumption request was approved</td>\n</tr>\n<tr>\n<td><code>tokens_remaining</code></td>\n<td><code>int</code></td>\n<td>Current token count after the consumption attempt</td>\n</tr>\n<tr>\n<td><code>retry_after_seconds</code></td>\n<td><code>float</code></td>\n<td>Time until enough tokens refill to satisfy the original request</td>\n</tr>\n</tbody></table>\n<p>The <code>allowed</code> field drives the fundamental allow/deny decision for incoming requests. When <code>False</code>, the HTTP middleware returns a 429 status code and includes rate limiting headers in the response.</p>\n<p>The <code>tokens_remaining</code> field enables the <code>X-RateLimit-Remaining</code> header, helping clients understand their current quota status. This value reflects the bucket state after the consumption attempt, whether successful or failed.</p>\n<p>The <code>retry_after_seconds</code> field calculates the precise wait time until the requested number of tokens will be available again. For a request needing 5 tokens when only 2 remain, this field indicates how long until 5 tokens accumulate, considering the bucket&#39;s refill rate. This drives the <code>Retry-After</code> HTTP header, enabling intelligent client backoff strategies.</p>\n<h4 id=\"global-rate-limit-configuration\">Global Rate Limit Configuration</h4>\n<p>The <code>RateLimitConfig</code> type serves as the <strong>central control panel</strong> for the entire rate limiting system. Think of it as the master configuration file that defines default policies, client-specific overrides, endpoint-specific rules, and operational parameters.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>default_limits</code></td>\n<td><code>TokenBucketConfig</code></td>\n<td>Baseline rate limits applied to unrecognized clients</td>\n</tr>\n<tr>\n<td><code>client_overrides</code></td>\n<td><code>Dict[str, TokenBucketConfig]</code></td>\n<td>Per-client rate limit overrides keyed by client identifier</td>\n</tr>\n<tr>\n<td><code>endpoint_limits</code></td>\n<td><code>Dict[str, TokenBucketConfig]</code></td>\n<td>Per-endpoint rate limits keyed by URL pattern or route name</td>\n</tr>\n<tr>\n<td><code>cleanup_interval</code></td>\n<td><code>int</code></td>\n<td>Seconds between stale bucket cleanup runs</td>\n</tr>\n<tr>\n<td><code>redis_url</code></td>\n<td><code>Optional[str]</code></td>\n<td>Redis connection string for distributed rate limiting</td>\n</tr>\n</tbody></table>\n<p>The <code>default_limits</code> field establishes the baseline rate limiting policy applied to all requests unless overridden by more specific rules. This provides a safety net ensuring that even unidentified traffic faces reasonable limits.</p>\n<p>The <code>client_overrides</code> dictionary enables tiered service levels where premium API keys receive higher rate limits than free tier users. Keys in this dictionary match the client identifiers returned by the <code>identify_client()</code> function, creating a direct mapping from client identity to rate limit policy.</p>\n<p>The <code>endpoint_limits</code> dictionary allows different API endpoints to have different rate limits based on their resource requirements. A simple health check endpoint might allow 1000 requests per minute, while a complex search operation might be limited to 10 requests per minute. Keys match URL patterns or route names defined in the web framework.</p>\n<p>The <code>cleanup_interval</code> field controls how frequently the system removes stale client buckets from memory. Shorter intervals reduce memory usage but increase CPU overhead from cleanup operations. Longer intervals reduce overhead but allow memory growth from inactive clients.</p>\n<p>The <code>redis_url</code> field enables distributed rate limiting by providing connection information for shared state storage. When <code>None</code>, the system operates in single-instance mode using only local memory for bucket storage.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The configuration uses a hierarchical override system where endpoint-specific limits take precedence over client-specific limits, which take precedence over default limits. This enables fine-grained control while maintaining simple configuration for common cases.</p>\n</blockquote>\n<h3 id=\"configuration-model\">Configuration Model</h3>\n<p>The configuration model defines how rate limiting policies are specified, loaded, and applied throughout the system. This model must support both simple single-instance deployments and complex distributed scenarios with thousands of unique rate limiting rules.</p>\n<h4 id=\"environment-based-configuration\">Environment-Based Configuration</h4>\n<p>The primary configuration mechanism uses environment variables to specify rate limiting policies. This approach integrates seamlessly with containerized deployments, configuration management systems, and CI/CD pipelines. Environment-based configuration also enables different rate limits across development, staging, and production environments without code changes.</p>\n<table>\n<thead>\n<tr>\n<th>Environment Variable</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>DEFAULT_RATE_LIMIT</code></td>\n<td><code>float</code></td>\n<td>Default tokens per second for unrecognized clients</td>\n</tr>\n<tr>\n<td><code>DEFAULT_BURST_SIZE</code></td>\n<td><code>int</code></td>\n<td>Default bucket capacity for burst handling</td>\n</tr>\n<tr>\n<td><code>CLIENT_OVERRIDES</code></td>\n<td><code>JSON string</code></td>\n<td>Per-client rate limit overrides as JSON object</td>\n</tr>\n<tr>\n<td><code>ENDPOINT_LIMITS</code></td>\n<td><code>JSON string</code></td>\n<td>Per-endpoint rate limits as JSON object</td>\n</tr>\n<tr>\n<td><code>REDIS_URL</code></td>\n<td><code>string</code></td>\n<td>Redis connection URL for distributed storage</td>\n</tr>\n<tr>\n<td><code>CLEANUP_INTERVAL</code></td>\n<td><code>int</code></td>\n<td>Seconds between stale bucket cleanup cycles</td>\n</tr>\n</tbody></table>\n<p>The <code>DEFAULT_RATE_LIMIT</code> and <code>DEFAULT_BURST_SIZE</code> variables define the baseline rate limiting policy. These values should be set based on the application&#39;s typical load patterns and resource capacity. A web API serving lightweight requests might use <code>DEFAULT_RATE_LIMIT=100</code> and <code>DEFAULT_BURST_SIZE=200</code>, while a resource-intensive service might use much lower values.</p>\n<p>The <code>CLIENT_OVERRIDES</code> variable contains a JSON object mapping client identifiers to rate limit configurations. The structure enables different rate limits for different client tiers:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>CLIENT_OVERRIDES='{&quot;premium_client_123&quot;: {&quot;capacity&quot;: 1000, &quot;refill_rate&quot;: 50.0}, &quot;free_client_456&quot;: {&quot;capacity&quot;: 100, &quot;refill_rate&quot;: 5.0}}'</code></pre></div>\n\n<p>The <code>ENDPOINT_LIMITS</code> variable follows a similar JSON structure but maps endpoint patterns to rate limit configurations. This enables API endpoints with different resource requirements to have appropriately scaled rate limits:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ENDPOINT_LIMITS='{&quot;/api/search&quot;: {&quot;capacity&quot;: 20, &quot;refill_rate&quot;: 2.0}, &quot;/api/health&quot;: {&quot;capacity&quot;: 1000, &quot;refill_rate&quot;: 100.0}}'</code></pre></div>\n\n<h4 id=\"configuration-loading-and-validation\">Configuration Loading and Validation</h4>\n<p>The configuration loading process must handle missing environment variables, invalid JSON structures, and inconsistent rate limit values. The <code>from_environment()</code> method implements a robust loading strategy with appropriate defaults and validation.</p>\n<p>The loading process follows these steps:</p>\n<ol>\n<li><strong>Environment Variable Extraction</strong>: Read all rate limiting environment variables, applying sensible defaults for missing values</li>\n<li><strong>JSON Parsing and Validation</strong>: Parse <code>CLIENT_OVERRIDES</code> and <code>ENDPOINT_LIMITS</code> JSON strings, validating structure and data types  </li>\n<li><strong>Rate Limit Validation</strong>: Ensure all rate limits have positive values and reasonable relationships between capacity and refill rate</li>\n<li><strong>Redis Connection Testing</strong>: If <code>REDIS_URL</code> is specified, attempt a connection test to validate the configuration</li>\n<li><strong>Configuration Object Construction</strong>: Build the complete <code>RateLimitConfig</code> object with validated values</li>\n</ol>\n<p>Configuration validation catches common mistakes such as negative rate limits, zero capacity buckets, or malformed JSON structures. These errors are reported clearly during application startup rather than causing mysterious runtime failures.</p>\n<blockquote>\n<p><strong>Critical Design Decision</strong>: Configuration loading happens once at application startup rather than dynamically reloading from environment variables. This ensures consistent rate limiting behavior throughout the application&#39;s lifetime and avoids race conditions from configuration changes affecting active rate limiting decisions.</p>\n</blockquote>\n<h4 id=\"hierarchical-override-resolution\">Hierarchical Override Resolution</h4>\n<p>The configuration model implements a hierarchical override system where more specific rules take precedence over general rules. This hierarchy enables flexible policy definition while maintaining predictable behavior.</p>\n<p>The override resolution order follows this precedence:</p>\n<ol>\n<li><strong>Endpoint-Specific Limits</strong>: Rate limits defined for specific URL patterns or routes</li>\n<li><strong>Client-Specific Overrides</strong>: Rate limits defined for specific client identifiers  </li>\n<li><strong>Default Limits</strong>: Baseline rate limits applied when no specific rules match</li>\n</ol>\n<p>When processing a request, the system first checks if the request&#39;s endpoint matches any patterns in <code>endpoint_limits</code>. If found, those limits apply regardless of client identity. If no endpoint-specific limits exist, the system checks for client-specific overrides based on the client identifier. Finally, if no specific rules apply, the default limits take effect.</p>\n<p>This hierarchy handles complex scenarios like premium clients accessing resource-intensive endpoints. The endpoint&#39;s resource requirements take precedence over the client&#39;s general rate limit tier, ensuring appropriate protection for expensive operations.</p>\n<blockquote>\n<p><strong>Architecture Decision: Configuration Hierarchy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support both client-based and endpoint-based rate limiting with clear precedence rules</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Client overrides take precedence over endpoint limits</li>\n<li>Endpoint limits take precedence over client overrides  </li>\n<li>Combine client and endpoint limits using mathematical operations</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Endpoint limits take precedence over client overrides</li>\n<li><strong>Rationale</strong>: Endpoint limits reflect resource consumption and system protection requirements, which are more critical than client service tiers for maintaining system stability</li>\n<li><strong>Consequences</strong>: Enables resource-based rate limiting while still supporting client tiers for general API usage</li>\n</ul>\n</blockquote>\n<h3 id=\"persistence-and-storage\">Persistence and Storage</h3>\n<p>The persistence and storage model defines how token bucket state is maintained across request processing cycles and system restarts. This model must support both single-instance in-memory storage and distributed Redis-based storage while maintaining consistent behavior and performance characteristics.</p>\n<h4 id=\"in-memory-storage-architecture\">In-Memory Storage Architecture</h4>\n<p>In-memory storage provides the highest performance option for single-instance deployments. Token bucket state lives entirely within the application&#39;s memory space, enabling microsecond-latency rate limiting decisions without network round trips or serialization overhead.</p>\n<p>The in-memory storage model uses a hierarchical key structure to organize client buckets:</p>\n<table>\n<thead>\n<tr>\n<th>Storage Level</th>\n<th>Key Format</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client Buckets</td>\n<td><code>client:{client_id}</code></td>\n<td>Default bucket for a specific client</td>\n</tr>\n<tr>\n<td>Endpoint Buckets</td>\n<td><code>client:{client_id}:endpoint:{endpoint}</code></td>\n<td>Client-endpoint specific bucket</td>\n</tr>\n<tr>\n<td>Metadata</td>\n<td><code>meta:{client_id}:last_access</code></td>\n<td>Last access timestamp for cleanup</td>\n</tr>\n</tbody></table>\n<p>The client bucket key format supports both general client rate limiting and client-endpoint combinations. When a request requires endpoint-specific rate limiting, the system creates a separate bucket with the combined key rather than sharing the client&#39;s default bucket.</p>\n<p>Metadata storage tracks when each client bucket was last accessed, enabling efficient stale bucket cleanup. The cleanup process iterates through metadata entries and removes buckets that haven&#39;t been accessed within the configured timeout period.</p>\n<p>Memory management becomes critical in high-traffic scenarios with many unique clients. The storage implementation must balance fast lookups with bounded memory usage, preventing memory leaks from accumulating stale buckets.</p>\n<h4 id=\"distributed-redis-storage\">Distributed Redis Storage</h4>\n<p>Redis-based storage enables consistent rate limiting across multiple server instances by centralizing token bucket state in a shared data store. This approach supports horizontal scaling of rate limiting infrastructure while maintaining accurate rate limit enforcement.</p>\n<p>The Redis storage model uses atomic operations to ensure consistency under concurrent access from multiple server instances. Each token consumption operation must atomically read the current bucket state, calculate new token counts, and update the stored values without interference from concurrent operations.</p>\n<table>\n<thead>\n<tr>\n<th>Redis Key Pattern</th>\n<th>Data Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>rl:bucket:{client_id}</code></td>\n<td>Hash</td>\n<td>Token bucket state with current count and last refill time</td>\n</tr>\n<tr>\n<td><code>rl:config:{client_id}</code></td>\n<td>Hash</td>\n<td>Client-specific rate limit configuration</td>\n</tr>\n<tr>\n<td><code>rl:endpoint:{endpoint}</code></td>\n<td>Hash</td>\n<td>Endpoint-specific rate limit configuration</td>\n</tr>\n<tr>\n<td><code>rl:meta:{client_id}</code></td>\n<td>String</td>\n<td>Last access timestamp for cleanup operations</td>\n</tr>\n</tbody></table>\n<p>The Redis hash data type stores multiple fields for each bucket, including current token count, last refill timestamp, and configuration parameters. This enables atomic updates of all bucket state within a single Redis operation.</p>\n<p>Configuration data is cached in Redis to avoid repeated environment variable parsing and JSON deserialization. The configuration cache includes both client-specific overrides and endpoint-specific limits, reducing the amount of data that must be retrieved for each rate limiting decision.</p>\n<p>Distributed cleanup operations coordinate across multiple server instances to prevent duplicate work and ensure comprehensive stale bucket removal. The cleanup process uses Redis locks to coordinate between instances and prevent race conditions.</p>\n<h4 id=\"storage-consistency-and-atomicity\">Storage Consistency and Atomicity</h4>\n<p>Both in-memory and Redis storage must handle concurrent access from multiple request processing threads. Token consumption operations involve read-modify-write cycles that must be atomic to prevent race conditions and maintain accurate rate limit enforcement.</p>\n<p>In-memory storage uses thread-local locks to ensure atomic token bucket operations. Each bucket has an associated mutex that protects both token count updates and timestamp modifications. The lock granularity is per-bucket rather than global, enabling high concurrency for requests from different clients.</p>\n<p>Redis storage uses Lua scripts to ensure atomic operations. Lua scripts execute atomically within Redis, preventing interleaving of operations from different server instances. The token consumption Lua script performs the complete read-calculate-update cycle as a single atomic operation.</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>In-Memory Approach</th>\n<th>Redis Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Consumption</td>\n<td>Per-bucket mutex lock</td>\n<td>Atomic Lua script execution</td>\n</tr>\n<tr>\n<td>Bucket Creation</td>\n<td>Thread-safe map operations</td>\n<td>Redis SET with NX flag</td>\n</tr>\n<tr>\n<td>Bucket Cleanup</td>\n<td>Global cleanup lock</td>\n<td>Distributed Redis locks</td>\n</tr>\n<tr>\n<td>Configuration Updates</td>\n<td>Immutable config objects</td>\n<td>Atomic Redis hash updates</td>\n</tr>\n</tbody></table>\n<p>Error handling becomes more complex with distributed storage due to network failures, Redis unavailability, and clock synchronization issues between server instances. The storage layer must implement graceful degradation strategies when Redis operations fail.</p>\n<blockquote>\n<p><strong>Architecture Decision: Storage Abstraction Layer</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support both in-memory and Redis storage with consistent behavior and easy switching between modes</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Separate implementations for in-memory and Redis storage</li>\n<li>Abstract storage interface with pluggable backends</li>\n<li>Single implementation that detects storage mode at runtime</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Abstract storage interface with pluggable backends</li>\n<li><strong>Rationale</strong>: Enables testing with in-memory storage while deploying with Redis storage, and supports future storage backend additions without changing core logic</li>\n<li><strong>Consequences</strong>: Adds interface complexity but improves testability and flexibility for different deployment scenarios</li>\n</ul>\n</blockquote>\n<h4 id=\"data-serialization-and-wire-format\">Data Serialization and Wire Format</h4>\n<p>Redis storage requires serialization of token bucket state and configuration data. The serialization format must be compact, human-readable for debugging, and compatible with Redis data types.</p>\n<p>Token bucket state uses Redis hash fields to store individual components:</p>\n<table>\n<thead>\n<tr>\n<th>Hash Field</th>\n<th>Data Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>tokens</code></td>\n<td>Integer</td>\n<td>Current token count in bucket</td>\n</tr>\n<tr>\n<td><code>last_refill</code></td>\n<td>Float</td>\n<td>Unix timestamp of last refill operation</td>\n</tr>\n<tr>\n<td><code>capacity</code></td>\n<td>Integer</td>\n<td>Maximum bucket capacity</td>\n</tr>\n<tr>\n<td><code>refill_rate</code></td>\n<td>Float</td>\n<td>Tokens per second refill rate</td>\n</tr>\n</tbody></table>\n<p>Configuration data uses JSON serialization for complex nested structures like client overrides and endpoint limits. JSON provides human-readable debugging output and easy integration with configuration management tools.</p>\n<p>Timestamp handling requires careful consideration of precision and time zone issues. All timestamps use Unix epoch seconds with millisecond precision to ensure consistent time calculations across server instances with different system clocks.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Using Redis hash fields rather than JSON serialization for token bucket state enables atomic updates of individual fields. This prevents races where one server instance overwrites configuration changes made by another instance during token consumption operations.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The data model implementation provides the foundation for all rate limiting functionality. This guidance focuses on creating robust, type-safe data structures that support both simple single-instance usage and complex distributed scenarios.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Loading</td>\n<td>Environment variables with <code>os.environ</code></td>\n<td>Configuration management with <code>python-decouple</code></td>\n</tr>\n<tr>\n<td>JSON Parsing</td>\n<td>Built-in <code>json</code> module</td>\n<td>Schema validation with <code>jsonschema</code></td>\n</tr>\n<tr>\n<td>Redis Client</td>\n<td><code>redis-py</code> with basic connection</td>\n<td><code>redis-py</code> with connection pooling</td>\n</tr>\n<tr>\n<td>Type Hints</td>\n<td>Basic type annotations</td>\n<td>Full <code>mypy</code> strict mode validation</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>rate_limiter/\n  config/\n    __init__.py          ← Configuration loading and validation\n    models.py            ← Core data type definitions\n    environment.py       ← Environment variable handling\n  storage/\n    __init__.py          ← Storage abstraction interfaces  \n    memory.py            ← In-memory storage implementation\n    redis.py             ← Redis storage implementation\n  core/\n    __init__.py\n    bucket.py            ← TokenBucket class implementation\n    tracker.py           ← ClientBucketTracker implementation</code></pre></div>\n\n<h4 id=\"core-data-type-definitions\">Core Data Type Definitions</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Core data models for rate limiting system.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Defines all fundamental types used across storage, configuration, and token bucket logic.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenBucketConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration parameters for a token bucket instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refill_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    initial_tokens: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that capacity > 0 and refill_rate > 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If initial_tokens is None, set it to capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that initial_tokens &#x3C;= capacity if specified</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenConsumptionResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result of attempting to consume tokens from a bucket.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowed: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens_remaining: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_after_seconds: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Global configuration for the rate limiting system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    default_limits: TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_overrides: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenBucketConfig]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    endpoint_limits: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenBucketConfig]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanup_interval: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_environment</span><span style=\"color:#E1E4E8\">(cls) -> </span><span style=\"color:#9ECBFF\">'RateLimitConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load configuration from environment variables with validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Read DEFAULT_RATE_LIMIT and DEFAULT_BURST_SIZE environment variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse CLIENT_OVERRIDES JSON string into dictionary of TokenBucketConfig objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse ENDPOINT_LIMITS JSON string into dictionary of TokenBucketConfig objects  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Read CLEANUP_INTERVAL with default value of 300 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Read optional REDIS_URL for distributed storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate all rate limit values are positive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return constructed RateLimitConfig instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"configuration-loading-implementation\">Configuration Loading Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Environment variable handling and configuration validation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides robust loading of rate limiting policies from environment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_config</span><span style=\"color:#E1E4E8\">() -> RateLimitConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load and validate rate limiting configuration from environment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use RateLimitConfig.from_environment() to load configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Catch and re-raise configuration errors with helpful messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log successful configuration loading with summary of loaded rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parse_bucket_config_dict</span><span style=\"color:#E1E4E8\">(json_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenBucketConfig]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Parse JSON string containing bucket configurations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse JSON string using json.loads()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Iterate through parsed dictionary and convert each value to TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle JSON parsing errors and invalid configuration values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return dictionary mapping string keys to TokenBucketConfig objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_rate_limits</span><span style=\"color:#E1E4E8\">(config: RateLimitConfig) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that all rate limit configurations are reasonable.\"\"\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that default_limits has positive values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate all client_overrides have positive rate limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate all endpoint_limits have positive rate limits  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that cleanup_interval is at least 60 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If redis_url is specified, validate connection string format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Raise ValueError with specific message for any validation failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"storage-interface-definition\">Storage Interface Definition</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Abstract storage interface supporting both in-memory and Redis backends.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Enables testing with fast in-memory storage while using Redis in production.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Set</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BucketStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for token bucket persistence.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_bucket_state</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve current state for a bucket, or None if not found.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement in subclasses for memory vs Redis storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_bucket_state</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, last_refill: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically update bucket state. Returns True if successful.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement atomic read-modify-write for the storage backend</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_bucket</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: TokenBucketConfig) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new bucket if it doesn't exist. Returns True if created.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement conditional bucket creation for the storage backend</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_stale_buckets</span><span style=\"color:#E1E4E8\">(self, max_age_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove buckets not accessed within max_age_seconds. Returns count removed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement stale bucket cleanup for the storage backend</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_bucket_keys</span><span style=\"color:#E1E4E8\">(self) -> Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return set of all current bucket keys for debugging/monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement bucket key enumeration for the storage backend  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Milestone 1 - Token Bucket Implementation:</strong></p>\n<ul>\n<li>Verify <code>TokenBucketConfig</code> can be created with valid parameters</li>\n<li>Test that <code>TokenConsumptionResult</code> contains expected fields </li>\n<li>Run: <code>python -c &quot;from config.models import TokenBucketConfig; print(TokenBucketConfig(100, 10.0))&quot;</code></li>\n<li>Expected output: <code>TokenBucketConfig(capacity=100, refill_rate=10.0, initial_tokens=100)</code></li>\n</ul>\n<p><strong>After Milestone 2 - Per-Client Rate Limiting:</strong>  </p>\n<ul>\n<li>Verify <code>RateLimitConfig.from_environment()</code> loads without errors</li>\n<li>Test client override parsing from JSON environment variables</li>\n<li>Run: <code>DEFAULT_RATE_LIMIT=5.0 DEFAULT_BURST_SIZE=50 python -c &quot;from config.models import RateLimitConfig; print(RateLimitConfig.from_environment())&quot;</code></li>\n<li>Expected: Configuration object with default limits and empty override dictionaries</li>\n</ul>\n<p><strong>After Milestone 3 - HTTP Middleware Integration:</strong></p>\n<ul>\n<li>Verify configuration loading works with complex JSON overrides</li>\n<li>Test endpoint limits parsing and hierarchy resolution</li>\n<li>Set <code>CLIENT_OVERRIDES=&#39;{&quot;test&quot;: {&quot;capacity&quot;: 200, &quot;refill_rate&quot;: 20.0}}&#39;</code> and verify parsing</li>\n</ul>\n<p><strong>After Milestone 4 - Distributed Rate Limiting:</strong></p>\n<ul>\n<li>Verify Redis URL configuration is properly parsed and validated</li>\n<li>Test storage interface abstraction with both in-memory and Redis backends</li>\n<li>Run storage backend tests to ensure consistent behavior between implementations</li>\n</ul>\n<h2 id=\"token-bucket-algorithm-implementation\">Token Bucket Algorithm Implementation</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Token Bucket Implementation) - this section provides the core algorithm that all subsequent milestones build upon</p>\n</blockquote>\n<h3 id=\"mental-model-water-bucket-with-holes\">Mental Model: Water Bucket with Holes</h3>\n<p>Think of a token bucket as a water bucket with a small hole in the bottom and a steady water tap dripping into it. The water represents tokens that allow requests to pass through your rate limiter. The bucket has a fixed capacity - it can only hold so much water before it overflows. The tap drips water at a constant rate, representing your configured tokens-per-second refill rate. When a request arrives, it&#39;s like someone scooping water out of the bucket - they can only scoop what&#39;s available.</p>\n<p>The beauty of this mental model lies in understanding bursts. If no one has scooped water for a while, the bucket fills up to its maximum capacity. When a sudden burst of requests arrives, they can all scoop water immediately until the bucket is empty. This allows legitimate traffic bursts while still maintaining an average rate limit over time. Once the bucket is empty, new requests must wait for the tap to drip enough water back in.</p>\n<p>The hole in the bottom represents token decay in some variations, but in our implementation, we&#39;ll use a simpler model where tokens don&#39;t decay - they just accumulate up to the bucket&#39;s capacity. This makes the algorithm more predictable and easier to reason about.</p>\n<h3 id=\"token-generation-and-refill-logic\">Token Generation and Refill Logic</h3>\n<p>The heart of the token bucket algorithm lies in calculating how many tokens to add based on elapsed time since the last refill. This calculation must handle several complexities: floating-point precision, clock changes, and avoiding integer overflow while maintaining accuracy.</p>\n<p>The fundamental refill equation is straightforward: <code>tokens_to_add = refill_rate * elapsed_seconds</code>. However, implementing this correctly requires careful consideration of time precision and edge cases. We measure elapsed time by comparing the current timestamp with the last refill timestamp stored in the bucket state.</p>\n<p><strong>Time Precision and Clock Handling</strong></p>\n<p>Our algorithm uses high-resolution timestamps to ensure accuracy even with sub-second refill rates. Python&#39;s <code>time.time()</code> provides microsecond precision, which allows us to handle refill rates as high as thousands of tokens per second without losing accuracy. We store the last refill timestamp as a floating-point number representing seconds since the Unix epoch.</p>\n<p>Clock changes present a significant challenge. If the system clock jumps backward, our elapsed time calculation could become negative, potentially causing tokens to be removed rather than added. If the clock jumps forward significantly, we might add an enormous number of tokens, effectively disabling rate limiting. Our algorithm addresses this by capping the maximum elapsed time to prevent excessive token generation and treating negative elapsed time as zero.</p>\n<p><strong>Token Calculation Algorithm</strong></p>\n<p>The token refill process follows these steps:</p>\n<ol>\n<li>Capture the current high-resolution timestamp using <code>time.time()</code></li>\n<li>Calculate elapsed seconds by subtracting the last refill timestamp from the current timestamp</li>\n<li>Handle clock edge cases by capping elapsed time to a reasonable maximum (typically 60 seconds)</li>\n<li>If elapsed time is negative or zero, skip token addition but update the timestamp</li>\n<li>Calculate tokens to add using <code>refill_rate * elapsed_seconds</code></li>\n<li>Add the calculated tokens to the current bucket count</li>\n<li>Cap the total tokens at the bucket&#39;s maximum capacity to prevent overflow</li>\n<li>Update the last refill timestamp to the current time</li>\n</ol>\n<p>The capping of elapsed time serves as protection against clock jumps and also prevents clients who haven&#39;t made requests for extended periods from accumulating excessive burst capacity beyond the intended bucket size.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: We update the timestamp even when no tokens are added (negative elapsed time) to prevent repeated attempts to add tokens when the clock is behaving erratically. This ensures the bucket state remains consistent regardless of clock behavior.</p>\n</blockquote>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Ftoken-bucket-state.svg\" alt=\"Token Bucket State Machine\"></p>\n<h3 id=\"token-consumption-and-burst-handling\">Token Consumption and Burst Handling</h3>\n<p>Token consumption operates on a simple principle: if sufficient tokens are available, deduct the requested amount and allow the request. If insufficient tokens exist, deny the request without modifying the bucket state. This atomic decision prevents partial consumption that could leave the bucket in an inconsistent state.</p>\n<p><strong>Consumption Decision Logic</strong></p>\n<p>The consumption algorithm must decide whether to allow or deny a request before modifying any state. This prevents race conditions and ensures that bucket state remains consistent even under high concurrency. The decision process follows these steps:</p>\n<ol>\n<li>Perform token refill calculation based on elapsed time</li>\n<li>Check if the current token count (after refill) meets or exceeds the requested token amount</li>\n<li>If sufficient tokens exist, subtract the requested amount and return success</li>\n<li>If insufficient tokens exist, leave the bucket unchanged and return denial</li>\n<li>Calculate retry-after seconds based on refill rate and token deficit</li>\n</ol>\n<p>The retry-after calculation helps clients understand when they can expect their request to succeed. For a deficit of N tokens at a refill rate of R tokens per second, the retry time is <code>ceiling(N / R)</code> seconds. This gives clients actionable information rather than forcing them to guess when to retry.</p>\n<p><strong>Burst Capacity Management</strong></p>\n<p>Burst handling is where the token bucket algorithm shines compared to fixed-window rate limiting. The bucket capacity determines the maximum burst size - the number of requests that can be processed immediately when the bucket is full. A well-configured bucket balances burst tolerance with sustained rate limiting.</p>\n<p>Consider a bucket configured with 100 tokens capacity and 10 tokens per second refill rate. This configuration allows bursts of up to 100 requests instantly (if the bucket is full), but sustains only 10 requests per second over longer periods. If a client sends 100 requests immediately, they&#39;ll be accepted, but the next request must wait 0.1 seconds for a new token to be generated.</p>\n<p>The relationship between burst capacity and refill rate should align with your service&#39;s characteristics. APIs that naturally see bursty traffic patterns benefit from larger bucket capacities relative to their refill rates. Services that prefer steady, predictable load might use smaller bucket capacities closer to their refill rates.</p>\n<p><strong>Token Consumption Result</strong></p>\n<p>Every consumption attempt returns comprehensive information to enable proper HTTP response handling. The <code>TokenConsumptionResult</code> structure contains the allow/deny decision, remaining token count, and retry-after timing. This information flows up to the HTTP middleware layer to generate appropriate response headers and status codes.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>allowed</code></td>\n<td><code>bool</code></td>\n<td>Whether the request should be permitted through</td>\n</tr>\n<tr>\n<td><code>tokens_remaining</code></td>\n<td><code>int</code></td>\n<td>Current token count after this operation</td>\n</tr>\n<tr>\n<td><code>retry_after_seconds</code></td>\n<td><code>float</code></td>\n<td>Time until request would likely succeed (0 if allowed)</td>\n</tr>\n</tbody></table>\n<h3 id=\"thread-safety-and-concurrency\">Thread Safety and Concurrency</h3>\n<p>Token bucket operations must be atomic to prevent race conditions that could lead to incorrect token counts or inconsistent bucket state. The primary race condition occurs when multiple threads simultaneously read the current token count, perform refill calculations, and write back updated counts. Without proper synchronization, tokens could be double-counted or lost entirely.</p>\n<p><strong>Concurrency Challenges</strong></p>\n<p>The token bucket algorithm involves multiple steps that must appear atomic to other threads: timestamp reading, elapsed time calculation, token addition, consumption decision, and state updates. Each of these operations individually might be thread-safe, but the combination creates a critical section that requires protection.</p>\n<p>Consider two threads processing requests simultaneously for the same bucket. Thread A reads the current token count (50 tokens), calculates that 10 more tokens should be added based on elapsed time, and determines the new count should be 60. Meanwhile, Thread B reads the same initial state (50 tokens), performs its own calculation, and also determines the count should be 60 after consuming 10 tokens. If both updates proceed, the bucket ends up with 60 tokens instead of the correct 50 tokens (60 from Thread A&#39;s refill minus 10 from Thread B&#39;s consumption).</p>\n<p><strong>Lock-Based Synchronization</strong></p>\n<p>Our implementation uses a per-bucket mutex to serialize all token bucket operations. This approach trades some performance for correctness and simplicity. Each <code>TokenBucket</code> instance contains its own lock, allowing concurrent access to different buckets while serializing access to individual bucket state.</p>\n<p>The locking strategy encompasses the entire token bucket operation from refill calculation through consumption decision. This ensures that token refill and consumption appear atomic to other threads. While this creates a potential bottleneck for high-traffic scenarios hitting the same bucket, it prevents all race conditions and maintains bucket state consistency.</p>\n<p>Lock contention becomes a concern when many threads access the same client&#39;s bucket simultaneously. However, in typical API scenarios, individual clients rarely generate enough concurrent requests to create significant lock contention. The bigger performance concern is usually the overhead of maintaining many individual bucket locks for different clients.</p>\n<p><strong>Alternative Concurrency Approaches</strong></p>\n<p>Lock-free implementations using atomic compare-and-swap operations offer better performance but significantly increase implementation complexity. Such approaches require careful handling of the ABA problem and complex retry logic when concurrent updates occur. For most rate limiting scenarios, the added complexity isn&#39;t justified by the performance gains.</p>\n<p>Database-backed implementations (like our Redis-based distributed approach in Milestone 4) handle concurrency through database transaction isolation or atomic operations like Lua scripts. This shifts the concurrency control responsibility to the database layer, which often provides better performance and correctness guarantees than application-level locking.</p>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<blockquote>\n<p><strong>Decision: Time Precision for Token Calculations</strong></p>\n<ul>\n<li><strong>Context</strong>: Token refill calculations require high precision to handle fractional tokens per second and avoid accumulated rounding errors over time.</li>\n<li><strong>Options Considered</strong>: Integer milliseconds, floating-point seconds with microsecond precision, fixed-point arithmetic with custom scaling</li>\n<li><strong>Decision</strong>: Use floating-point seconds with microsecond precision via Python&#39;s <code>time.time()</code></li>\n<li><strong>Rationale</strong>: Provides sufficient precision for refill rates up to 1000+ tokens/second while maintaining simple arithmetic. Python&#39;s float implementation uses double precision, giving us adequate range and precision for timestamps and calculations.</li>\n<li><strong>Consequences</strong>: Enables accurate token calculations for high-rate buckets but introduces potential floating-point precision issues over very long time periods. Requires careful handling of clock changes and negative elapsed time.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Precision</th>\n<th>Complexity</th>\n<th>Range</th>\n<th>Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Integer milliseconds</td>\n<td>1ms</td>\n<td>Low</td>\n<td>Limited by int size</td>\n<td>Fastest</td>\n</tr>\n<tr>\n<td>Float seconds</td>\n<td>~1μs</td>\n<td>Medium</td>\n<td>~290 billion years</td>\n<td>Fast</td>\n</tr>\n<tr>\n<td>Fixed-point arithmetic</td>\n<td>Configurable</td>\n<td>High</td>\n<td>Configurable</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Token Overflow and Maximum Burst Handling</strong></p>\n<ul>\n<li><strong>Context</strong>: Token buckets must prevent unbounded token accumulation while supporting legitimate burst scenarios.</li>\n<li><strong>Options Considered</strong>: Hard cap at bucket capacity, exponential decay of excess tokens, sliding time window for maximum accumulation</li>\n<li><strong>Decision</strong>: Hard cap tokens at configured bucket capacity with no decay</li>\n<li><strong>Rationale</strong>: Provides predictable behavior that&#39;s easy to reason about and configure. Clients can understand exactly how many requests they can burst, and the sustained rate is clearly defined by the refill rate.</li>\n<li><strong>Consequences</strong>: Enables controlled bursts up to bucket capacity but prevents excessive accumulation during idle periods. May be less flexible than decay-based approaches for some use cases.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Predictability</th>\n<th>Implementation</th>\n<th>Memory Usage</th>\n<th>Burst Control</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hard cap</td>\n<td>High</td>\n<td>Simple</td>\n<td>Constant</td>\n<td>Fixed maximum</td>\n</tr>\n<tr>\n<td>Exponential decay</td>\n<td>Medium</td>\n<td>Complex</td>\n<td>Constant</td>\n<td>Variable maximum</td>\n</tr>\n<tr>\n<td>Sliding window</td>\n<td>Low</td>\n<td>Very complex</td>\n<td>Higher</td>\n<td>Dynamic maximum</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Concurrency Control Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Token bucket operations must be thread-safe to prevent race conditions in concurrent request processing scenarios.</li>\n<li><strong>Options Considered</strong>: Per-bucket mutex locks, lock-free atomic operations, single global lock for all buckets</li>\n<li><strong>Decision</strong>: Per-bucket mutex locks protecting the entire refill-and-consume operation</li>\n<li><strong>Rationale</strong>: Provides strong consistency guarantees with moderate performance characteristics. Allows concurrent access to different client buckets while ensuring each bucket&#39;s state remains consistent. Implementation complexity is reasonable compared to lock-free approaches.</li>\n<li><strong>Consequences</strong>: Serializes access to individual client buckets, potentially creating bottlenecks for high-traffic clients. Provides straightforward debugging and reasoning about bucket state consistency.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Consistency</th>\n<th>Performance</th>\n<th>Complexity</th>\n<th>Deadlock Risk</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-bucket mutex</td>\n<td>Strong</td>\n<td>Good</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Lock-free atomic</td>\n<td>Strong</td>\n<td>Excellent</td>\n<td>Very High</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Global lock</td>\n<td>Strong</td>\n<td>Poor</td>\n<td>Very Low</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Race Conditions in Token Refill and Consumption</strong></p>\n<p>Many implementations incorrectly perform token refill and consumption as separate operations, creating a race condition window where another thread can modify bucket state between refill calculation and token consumption. This leads to incorrect token counts and potential over-allocation of resources.</p>\n<p>The fix requires wrapping the entire refill-check-consume sequence in a single critical section. Don&#39;t release the lock between calculating new tokens and deciding whether to allow the request. The atomic operation should include timestamp updates, token additions, consumption decisions, and state modifications.</p>\n<p>⚠️ <strong>Pitfall: Clock Change Handling</strong></p>\n<p>Naive implementations trust system time implicitly, leading to dramatic failures when clocks change. A backward clock change can cause negative elapsed time, potentially removing tokens from buckets. Forward clock jumps can add millions of tokens instantly, effectively disabling rate limiting for extended periods.</p>\n<p>The fix involves validating elapsed time calculations and capping maximum time jumps. Treat negative elapsed time as zero and limit maximum elapsed time to prevent excessive token accumulation. Always update the last refill timestamp to the current time, even when no tokens are added.</p>\n<p>⚠️ <strong>Pitfall: Floating-Point Precision Loss</strong></p>\n<p>Long-running buckets accumulate floating-point precision errors in timestamp and token calculations, eventually leading to incorrect behavior. This is especially problematic for buckets with high refill rates or systems that run for months without restart.</p>\n<p>The fix requires periodic normalization of bucket state and careful choice of floating-point operations. Use high-precision timestamp sources and consider resetting bucket state periodically for long-idle buckets. Be aware of precision limits when working with very large timestamp differences.</p>\n<p>⚠️ <strong>Pitfall: Integer Overflow in Token Calculations</strong></p>\n<p>When converting floating-point token calculations to integer bucket counts, intermediate calculations can overflow, especially with high refill rates and large time intervals. This typically manifests as buckets suddenly having negative token counts or extremely large positive counts.</p>\n<p>The fix involves range checking at each calculation step and using appropriate integer types. Cap intermediate calculations at reasonable maximums and validate final token counts before updating bucket state. Consider using language-specific overflow detection mechanisms.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time Handling</td>\n<td><code>time.time()</code> for timestamps</td>\n<td><code>time.perf_counter()</code> for intervals</td>\n</tr>\n<tr>\n<td>Thread Synchronization</td>\n<td><code>threading.Lock()</code> per bucket</td>\n<td><code>threading.RLock()</code> for read optimization</td>\n</tr>\n<tr>\n<td>Numeric Precision</td>\n<td>Built-in <code>float</code> type</td>\n<td><code>decimal.Decimal</code> for high precision</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Hard-coded constants</td>\n<td>Environment-based configuration</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>rate_limiter/\n  core/\n    __init__.py\n    token_bucket.py          ← TokenBucket class implementation\n    config.py                ← TokenBucketConfig data structure\n    exceptions.py            ← Custom exception types\n  tests/\n    test_token_bucket.py     ← Unit tests for token bucket algorithm\n    test_concurrency.py      ← Thread safety and race condition tests</code></pre></div>\n\n<h4 id=\"core-configuration-structure-complete-implementation\">Core Configuration Structure (Complete Implementation)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Token bucket configuration and result types.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenBucketConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for a single token bucket instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">                    # Maximum tokens the bucket can hold</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refill_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">              # Tokens added per second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    initial_tokens: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Starting token count (defaults to capacity)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.capacity </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Bucket capacity must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.refill_rate </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Refill rate must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.initial_tokens </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.initial_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.initial_tokens </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.capacity:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Initial tokens cannot exceed capacity\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Global rate limiter configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    default_limits: TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_overrides: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenBucketConfig]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    endpoint_limits: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenBucketConfig] </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanup_interval: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_environment</span><span style=\"color:#E1E4E8\">(cls) -> </span><span style=\"color:#9ECBFF\">'RateLimitConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create configuration from environment variables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Default rate limiting configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        default_capacity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">'DEFAULT_BURST_SIZE'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'100'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        default_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">'DEFAULT_RATE_LIMIT'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'10.0'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        default_limits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">default_capacity,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">default_rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parse client overrides from JSON</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client_overrides </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client_json </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> os.getenv(</span><span style=\"color:#9ECBFF\">'CLIENT_OVERRIDES'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> client_json:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            client_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(client_json)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> client_id, config </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> client_data.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                client_overrides[client_id] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parse endpoint limits from JSON  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        endpoint_limits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        endpoint_json </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> os.getenv(</span><span style=\"color:#9ECBFF\">'ENDPOINT_LIMITS'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> endpoint_json:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            endpoint_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(endpoint_json)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> endpoint, config </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> endpoint_data.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                endpoint_limits[endpoint] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            default_limits</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">default_limits,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            client_overrides</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client_overrides,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            endpoint_limits</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">endpoint_limits,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cleanup_interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(os.getenv(</span><span style=\"color:#9ECBFF\">'CLEANUP_INTERVAL'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'300'</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            redis_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'REDIS_URL'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenConsumptionResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result of attempting to consume tokens from a bucket.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowed: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#6A737D\">                # Whether the request was allowed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens_remaining: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">        # Tokens left in bucket after operation  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_after_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">   # Time to wait before retrying (0 if allowed)</span></span></code></pre></div>\n\n<h4 id=\"token-bucket-core-logic-skeleton\">Token Bucket Core Logic Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Core token bucket algorithm implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenBucketConfig, TokenConsumptionResult</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenBucket</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Thread-safe token bucket implementation for rate limiting.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    The token bucket algorithm allows controlled bursts while maintaining </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    average rate limits over time. Tokens are added at a steady rate up to</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    the bucket's capacity, and consumed by incoming requests.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TokenBucketConfig):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize bucket with configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._current_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(config.initial_tokens)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._last_refill_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Attempt to consume tokens from the bucket.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This method performs the complete token bucket algorithm:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        1. Refill tokens based on elapsed time</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        2. Check if sufficient tokens are available  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        3. Consume tokens if available, or deny if insufficient</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        4. Return result with current state and retry timing</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            tokens_requested: Number of tokens to attempt to consume</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            TokenConsumptionResult with allow/deny decision and bucket state</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current timestamp and calculate elapsed time since last refill</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Use time.time() and subtract self._last_refill_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            elapsed_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle clock edge cases - cap elapsed time and handle negative values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: If elapsed &#x3C; 0 or elapsed > 60, treat as special cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Always update timestamp even if no tokens added</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate tokens to add based on refill rate and elapsed time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: tokens_to_add = self._config.refill_rate * elapsed_seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tokens_to_add </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add tokens to bucket but cap at maximum capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: self._current_tokens = min(new_total, self._config.capacity)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update last refill timestamp to current time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check if sufficient tokens available for this request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Compare tokens_requested with self._current_tokens</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: If sufficient tokens, consume them and return success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Subtract tokens_requested from self._current_tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Return TokenConsumptionResult(allowed=True, tokens_remaining=..., retry_after_seconds=0.0)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: If insufficient tokens, calculate retry-after time and return denial  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: token_deficit = tokens_requested - self._current_tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: retry_after = token_deficit / self._config.refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Return TokenConsumptionResult(allowed=False, tokens_remaining=..., retry_after_seconds=...)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span><span style=\"color:#6A737D\">  # Replace with actual implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> current_tokens</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current token count (triggers refill calculation).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement read-only token count that includes refill calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Similar to try_consume but without consuming tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> reset</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reset bucket to initial state (useful for testing).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Reset tokens to initial_tokens and update timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_refill</span><span style=\"color:#E1E4E8\">(self, current_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate tokens to add based on elapsed time.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This helper method encapsulates the refill logic with proper</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        edge case handling for clock changes and precision.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            current_time: Current timestamp</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of tokens to add (may be 0)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate elapsed time since last refill</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle negative elapsed time (clock moved backward)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Cap maximum elapsed time to prevent excessive accumulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate tokens based on refill rate and elapsed time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return token count, ensuring it's non-negative</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Python Threading Considerations:</strong></p>\n<ul>\n<li>Use <code>threading.Lock()</code> rather than <code>threading.RLock()</code> unless you need reentrant locking</li>\n<li>The <code>with self._lock:</code> pattern automatically handles lock acquisition and release</li>\n<li>Consider using <code>threading.local()</code> if you need per-thread bucket state</li>\n</ul>\n<p><strong>Time Handling Best Practices:</strong></p>\n<ul>\n<li><code>time.time()</code> provides wall clock time suitable for rate limiting calculations</li>\n<li>For testing, consider dependency injection to allow mock time sources</li>\n<li>Be aware that <code>time.time()</code> can go backward on some systems during clock adjustments</li>\n</ul>\n<p><strong>Floating Point Precision:</strong></p>\n<ul>\n<li>Python&#39;s <code>float</code> type uses double precision, providing ~15 decimal digits</li>\n<li>For token calculations, this provides sufficient precision for rates up to thousands per second</li>\n<li>Consider using <code>decimal.Decimal</code> if you need exact decimal arithmetic</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<ul>\n<li>Validate configuration parameters in <code>__post_init__</code> methods using dataclasses</li>\n<li>Raise <code>ValueError</code> for invalid configuration rather than silently correcting</li>\n<li>Consider custom exception types for rate limiting specific errors</li>\n</ul>\n<h4 id=\"milestone-1-verification-checkpoint\">Milestone 1 Verification Checkpoint</h4>\n<p>After implementing the token bucket algorithm, verify correct behavior with these tests:</p>\n<p><strong>Basic Functionality Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create a bucket with 10 token capacity, 1 token/second refill</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should allow initial requests up to capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 11th request should be denied  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> result.retry_after_seconds </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span></code></pre></div>\n\n<p><strong>Burst Handling Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Large burst should be allowed up to capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config) </span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Consume all tokens at once</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> result.tokens_remaining </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Next request should be denied</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong>Thread Safety Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> concurrent.futures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> consume_token</span><span style=\"color:#E1E4E8\">(bucket):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test concurrent access doesn't create race conditions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">with</span><span style=\"color:#E1E4E8\"> concurrent.futures.ThreadPoolExecutor(</span><span style=\"color:#FFAB70\">max_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> executor:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    futures </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [executor.submit(consume_token, bucket) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [f.result() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> f </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> futures]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">allowed_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> results </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> r.allowed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">denied_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> results </span><span style=\"color:#F97583\">if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> r.allowed)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should allow exactly 1000 requests (initial capacity)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> allowed_count </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> denied_count </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span></span></code></pre></div>\n\n<p><strong>Signs of Implementation Issues:</strong></p>\n<ul>\n<li>Token counts going negative → Check consumption logic and thread safety</li>\n<li>Buckets never refilling → Verify timestamp updates and elapsed time calculation  </li>\n<li>Race conditions under load → Ensure entire operation is protected by lock</li>\n<li>Excessive retry-after times → Check refill rate calculations and token deficit math</li>\n</ul>\n<h2 id=\"per-client-rate-limiting\">Per-Client Rate Limiting</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 (Per-Client Rate Limiting) - this section builds on the core token bucket algorithm to implement individual rate limits for each API consumer</p>\n</blockquote>\n<p>Think of per-client rate limiting as running a sophisticated hotel with different room types and guest privileges. A basic guest might be limited to using the pool twice per day, while a VIP guest gets unlimited access, and a premium suite holder gets access to exclusive facilities. Each guest carries a digital key card that tracks their specific privileges and usage throughout their stay. The hotel&#39;s computer system maintains separate accounting for every guest, automatically upgrading or restricting access based on their membership level, and periodically cleans up records for guests who have checked out.</p>\n<p>In our rate limiter, each API consumer gets their own virtual &quot;key card&quot; - a unique token bucket configured according to their specific rate limit rules. The system must efficiently track thousands or millions of individual clients simultaneously, applying different limits based on client tier, endpoint sensitivity, or custom business rules. This creates several fascinating engineering challenges: how do we identify clients reliably, store their state efficiently, prevent memory leaks from inactive clients, and maintain thread-safe access under high concurrency?</p>\n<p>The architecture extends our basic token bucket implementation with a <strong>client tracking layer</strong> that acts as an intelligent bucket factory and warehouse manager. When a request arrives, the system identifies the client, retrieves or creates their dedicated bucket, applies the rate limit check, and updates their state atomically. Behind the scenes, a background cleanup process periodically removes stale buckets from clients who haven&#39;t been seen recently, preventing unbounded memory growth.</p>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Fclient-bucket-lifecycle.svg\" alt=\"Client Bucket Lifecycle\"></p>\n<h3 id=\"client-identification-strategies\">Client Identification Strategies</h3>\n<p>The foundation of per-client rate limiting lies in reliably identifying who is making each request. Think of this as checking IDs at a nightclub entrance - you need a consistent way to recognize returning customers and apply their specific privileges or restrictions. The identification strategy directly impacts both security and functionality, as clients must not be able to easily bypass their limits by changing identifiers.</p>\n<p>The most common identification approaches each offer different trade-offs between simplicity, security, and flexibility. <strong>IP address identification</strong> provides the simplest implementation but suffers from shared NAT scenarios where multiple users appear as a single client, and dynamic IP environments where the same user appears as different clients. <strong>API key identification</strong> offers the strongest control and enables sophisticated per-client configuration, but requires API key distribution and management infrastructure. <strong>HTTP header identification</strong> provides flexibility for custom client classification but depends on client cooperation and header standardization.</p>\n<table>\n<thead>\n<tr>\n<th>Identification Method</th>\n<th>Reliability</th>\n<th>Bypass Difficulty</th>\n<th>Implementation Complexity</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Source IP Address</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Low</td>\n<td>Public APIs, basic protection</td>\n</tr>\n<tr>\n<td>API Key Header</td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n<td>Authenticated APIs, tiered access</td>\n</tr>\n<tr>\n<td>Custom Header</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Internal APIs, client classification</td>\n</tr>\n<tr>\n<td>JWT Subject Claim</td>\n<td>High</td>\n<td>High</td>\n<td>High</td>\n<td>OAuth-protected APIs, user-based limits</td>\n</tr>\n<tr>\n<td>Cookie Session ID</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Web applications, session-based limits</td>\n</tr>\n</tbody></table>\n<p>For robust client identification, the system should extract the client identifier using a <strong>configurable extraction strategy</strong> that can examine multiple request attributes. The <code>identify_client</code> function serves as the central identification point, allowing different identification strategies to be plugged in based on configuration or request context.</p>\n<blockquote>\n<p><strong>Decision: Hierarchical Client Identification</strong></p>\n<ul>\n<li><strong>Context</strong>: Different API endpoints may need different client identification strategies, and some scenarios require fallback identification when primary methods fail</li>\n<li><strong>Options Considered</strong>: Single global identification strategy, per-endpoint identification configuration, hierarchical identification with fallbacks</li>\n<li><strong>Decision</strong>: Implement hierarchical identification that tries multiple extraction methods in priority order</li>\n<li><strong>Rationale</strong>: This provides maximum flexibility while maintaining consistent behavior. For example, try API key first, fall back to IP address for unauthenticated requests, enabling gradual migration from IP-based to API key-based limits</li>\n<li><strong>Consequences</strong>: Slightly more complex configuration but enables sophisticated client classification without breaking existing clients during transitions</li>\n</ul>\n</blockquote>\n<p>The identification logic must handle several edge cases that commonly cause production issues. <strong>Header injection attacks</strong> where malicious clients attempt to spoof identification headers require validation and sanitization of extracted identifiers. <strong>Missing identifier scenarios</strong> need well-defined fallback behavior - typically falling back to IP address or applying default anonymous limits. <strong>Identifier collision</strong> between different identification methods requires careful namespace design to prevent conflicts.</p>\n<table>\n<thead>\n<tr>\n<th>Edge Case</th>\n<th>Problem</th>\n<th>Solution</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing API Key</td>\n<td>Authenticated endpoint receives unauthenticated request</td>\n<td>Apply anonymous limits or reject</td>\n<td>API key required but header empty</td>\n</tr>\n<tr>\n<td>Invalid IP Format</td>\n<td>Malformed or spoofed IP addresses</td>\n<td>Validate and sanitize, use connection IP</td>\n<td>X-Forwarded-For contains invalid data</td>\n</tr>\n<tr>\n<td>Header Injection</td>\n<td>Client attempts to spoof privileged identifier</td>\n<td>Whitelist valid characters, length limits</td>\n<td>API key contains control characters</td>\n</tr>\n<tr>\n<td>Proxy Scenarios</td>\n<td>Multiple clients behind shared IP</td>\n<td>Prefer unique identifiers over IP</td>\n<td>Corporate firewall with hundreds of users</td>\n</tr>\n</tbody></table>\n<h3 id=\"client-bucket-storage\">Client Bucket Storage</h3>\n<p>Once we can identify clients, we need an efficient storage mechanism for their individual token buckets. Think of this as managing a massive safety deposit box vault where each client has their own secure compartment containing their rate limit state. The vault must allow instant access to any client&#39;s compartment while efficiently organizing thousands or millions of compartments and automatically removing compartments for clients who haven&#39;t visited recently.</p>\n<p>The storage layer must balance several competing requirements. <strong>Memory efficiency</strong> demands minimal overhead per client bucket, as systems may track millions of clients simultaneously. <strong>Access performance</strong> requires O(1) lookup time to retrieve client buckets under high request concurrency. <strong>Thread safety</strong> ensures multiple simultaneous requests from the same client don&#39;t corrupt bucket state. <strong>Automatic cleanup</strong> prevents unbounded memory growth from clients who stop making requests.</p>\n<p>The <code>ClientBucketTracker</code> serves as the central coordinator for client bucket lifecycle management. It maintains an internal mapping from client identifiers to <code>TokenBucket</code> instances, along with metadata for cleanup decisions. The tracker handles bucket creation on first access, thread-safe bucket retrieval, and coordination with background cleanup processes.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Data Stored</th>\n<th>Concurrency Model</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ClientBucketTracker</td>\n<td>Bucket lifecycle management</td>\n<td>client_id → TokenBucket mapping</td>\n<td>Read-write lock on bucket map</td>\n</tr>\n<tr>\n<td>TokenBucket</td>\n<td>Rate limit algorithm</td>\n<td>Current tokens, last refill time</td>\n<td>Internal locking per bucket</td>\n</tr>\n<tr>\n<td>CleanupManager</td>\n<td>Stale bucket removal</td>\n<td>Last access timestamps</td>\n<td>Separate background thread</td>\n</tr>\n<tr>\n<td>ConfigResolver</td>\n<td>Per-client limit lookup</td>\n<td>Rate limit overrides</td>\n<td>Read-only after initialization</td>\n</tr>\n</tbody></table>\n<p>The internal storage structure uses a <strong>concurrent hash map</strong> to provide O(1) bucket lookup while supporting safe concurrent access. Each bucket entry includes the <code>TokenBucket</code> instance and a <code>last_accessed</code> timestamp for cleanup decisions. The hash map itself requires reader-writer synchronization to handle bucket creation and cleanup operations safely.</p>\n<blockquote>\n<p><strong>Decision: Bucket-Level Locking vs Global Locking</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple requests from the same client may arrive simultaneously, requiring coordination to prevent race conditions on token bucket state</li>\n<li><strong>Options Considered</strong>: Single global lock for all buckets, per-client bucket locking, lock-free atomic operations</li>\n<li><strong>Decision</strong>: Per-client bucket locking with fine-grained synchronization</li>\n<li><strong>Rationale</strong>: Global locking would serialize all rate limit checks regardless of client, creating a bottleneck. Per-bucket locking allows parallel processing of different clients while ensuring consistency within each client&#39;s bucket. Lock-free operations add complexity without significant benefits at this scale</li>\n<li><strong>Consequences</strong>: Higher concurrency and better performance scaling, but requires careful lock ordering to prevent deadlocks during cleanup operations</li>\n</ul>\n</blockquote>\n<p>The bucket creation process follows a <strong>lazy initialization pattern</strong> where buckets are created only when first accessed. This conserves memory for systems with large potential client populations but sparse actual usage. The creation process must handle the race condition where multiple concurrent requests for the same new client attempt bucket creation simultaneously.</p>\n<p>The bucket storage also integrates with <strong>configuration override resolution</strong> to apply client-specific rate limits. When creating a new bucket, the system consults the <code>RateLimitConfig.client_overrides</code> mapping to determine if this client has custom limits that override the default configuration.</p>\n<h3 id=\"stale-bucket-cleanup\">Stale Bucket Cleanup</h3>\n<p>Without active cleanup, client bucket storage would grow unboundedly as new clients are encountered, eventually exhausting available memory. Think of stale bucket cleanup as a diligent janitor who periodically walks through the safety deposit box vault, identifies boxes that haven&#39;t been accessed recently, and removes them to make space for new clients. The janitor must work efficiently without interfering with active client operations.</p>\n<p>The cleanup mechanism operates as a <strong>background process</strong> that runs periodically to identify and remove buckets that haven&#39;t been accessed within a configurable time window. This process balances memory conservation against the cost of recreating buckets for clients who return after cleanup. The cleanup interval and staleness threshold directly impact both memory usage and performance characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Cleanup Configuration</th>\n<th>Purpose</th>\n<th>Typical Value</th>\n<th>Impact of Too Low</th>\n<th>Impact of Too High</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>cleanup_interval</td>\n<td>Time between cleanup runs</td>\n<td>300 seconds</td>\n<td>CPU overhead from frequent scans</td>\n<td>Memory growth between cleanups</td>\n</tr>\n<tr>\n<td>staleness_threshold</td>\n<td>Age before bucket removal</td>\n<td>3600 seconds</td>\n<td>Frequent bucket recreation</td>\n<td>Higher memory usage</td>\n</tr>\n<tr>\n<td>batch_size</td>\n<td>Buckets removed per cleanup cycle</td>\n<td>1000 buckets</td>\n<td>Slow memory reclamation</td>\n<td>Long cleanup pauses</td>\n</tr>\n<tr>\n<td>max_cleanup_duration</td>\n<td>Maximum time per cleanup cycle</td>\n<td>10 seconds</td>\n<td>Incomplete cleanup cycles</td>\n<td>Interference with request processing</td>\n</tr>\n</tbody></table>\n<p>The cleanup algorithm follows a <strong>two-phase approach</strong> to minimize interference with active request processing. The first phase identifies stale buckets by scanning the bucket map and collecting candidates for removal based on their last access timestamps. The second phase acquires write locks and removes the identified buckets, handling the race condition where a bucket becomes active again between identification and removal.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Cleanup Algorithm Steps:\n1. Wait for cleanup_interval duration or explicit trigger\n2. Acquire read lock on bucket map to get snapshot of all client IDs\n3. For each bucket, check if (current_time - last_accessed) &gt; staleness_threshold\n4. Collect stale bucket IDs into removal candidate list\n5. Release read lock to minimize blocking active requests\n6. Acquire write lock on bucket map for removal phase\n7. For each candidate, verify still stale and remove from map\n8. Release write lock and log cleanup statistics\n9. Schedule next cleanup cycle</code></pre></div>\n\n<p>The cleanup process must handle several race conditions that can occur during bucket removal. <strong>Concurrent access during cleanup</strong> happens when a request tries to access a bucket while it&#39;s being removed, requiring careful lock ordering to prevent deadlocks. <strong>Bucket resurrection</strong> occurs when a client makes a new request for a bucket that was just marked for removal, requiring validation that removed buckets are truly stale.</p>\n<blockquote>\n<p><strong>Decision: Cleanup Threading Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Cleanup operations require scanning potentially millions of buckets, which could block request processing if not handled carefully</li>\n<li><strong>Options Considered</strong>: Cleanup during request processing, dedicated cleanup thread, cleanup thread pool</li>\n<li><strong>Decision</strong>: Single dedicated cleanup thread with configurable timing</li>\n<li><strong>Rationale</strong>: Request-time cleanup adds latency to user requests and creates unpredictable performance. Multiple cleanup threads add complexity without significant benefits since cleanup is I/O bound by lock acquisition. Single thread simplifies coordination and provides predictable resource usage</li>\n<li><strong>Consequences</strong>: Predictable cleanup overhead and simple coordination, but cleanup throughput limited to single thread performance</li>\n</ul>\n</blockquote>\n<h3 id=\"per-client-limit-overrides\">Per-Client Limit Overrides</h3>\n<p>Real-world applications require different rate limits for different classes of clients. Think of this as a airline&#39;s tiered service model - economy passengers get basic luggage allowances, business class passengers get increased limits, and first-class passengers enjoy even higher allowances or no limits at all. The rate limiter must support this hierarchical privilege system while maintaining performance and simplicity.</p>\n<p>The override system operates through a <strong>configuration-driven approach</strong> where client-specific limits are defined in the <code>RateLimitConfig.client_overrides</code> mapping. This allows operational teams to adjust limits without code changes while providing audit trails for limit modifications. The override resolution follows a clear precedence hierarchy to handle scenarios where multiple override rules might apply.</p>\n<table>\n<thead>\n<tr>\n<th>Override Type</th>\n<th>Precedence</th>\n<th>Configuration Location</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Explicit Client Override</td>\n<td>Highest</td>\n<td>client_overrides[client_id]</td>\n<td>VIP clients, paid tiers, troubleshooting</td>\n</tr>\n<tr>\n<td>Endpoint-Specific Default</td>\n<td>Medium</td>\n<td>endpoint_limits[endpoint]</td>\n<td>Sensitive operations, bulk endpoints</td>\n</tr>\n<tr>\n<td>Global Default</td>\n<td>Lowest</td>\n<td>default_limits</td>\n<td>Standard rate limiting for all clients</td>\n</tr>\n<tr>\n<td>Emergency Override</td>\n<td>Special</td>\n<td>Runtime configuration</td>\n<td>Incident response, temporary restrictions</td>\n</tr>\n</tbody></table>\n<p>The override resolution process integrates into bucket creation within the <code>get_bucket_for_client</code> method. When creating a new bucket for a client, the system queries the configuration hierarchy to determine the appropriate <code>TokenBucketConfig</code> parameters. This resolution happens only during bucket creation, ensuring consistent limits for the lifetime of each bucket.</p>\n<p>Client override configuration supports <strong>multiple limit dimensions</strong> to handle complex business requirements. Clients may have different limits for different types of operations, time-based limits that change during peak hours, or geographic limits based on their location. The configuration structure accommodates these scenarios through nested override mappings.</p>\n<table>\n<thead>\n<tr>\n<th>Override Dimension</th>\n<th>Configuration Pattern</th>\n<th>Example Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client-Endpoint</td>\n<td>client_overrides[client_id].endpoint_limits[endpoint]</td>\n<td>Premium client gets higher limits for expensive operations</td>\n</tr>\n<tr>\n<td>Time-Based</td>\n<td>client_overrides[client_id].time_windows[hour_range]</td>\n<td>Reduced limits during peak hours</td>\n</tr>\n<tr>\n<td>Geographic</td>\n<td>client_overrides[client_id].regions[region]</td>\n<td>Compliance with regional rate limiting requirements</td>\n</tr>\n<tr>\n<td>Operation Type</td>\n<td>client_overrides[client_id].operation_types[type]</td>\n<td>Different limits for read vs write operations</td>\n</tr>\n</tbody></table>\n<p>The override system must handle several operational scenarios that arise in production environments. <strong>Dynamic override updates</strong> allow operators to modify client limits without restarting the service, requiring thread-safe configuration reloading. <strong>Override inheritance</strong> enables hierarchical client groups where premium clients automatically inherit elevated base limits. <strong>Temporary overrides</strong> support incident response scenarios where specific clients need immediate limit adjustments.</p>\n<blockquote>\n<p><strong>Decision: Override Application Timing</strong></p>\n<ul>\n<li><strong>Context</strong>: Client overrides could be applied at bucket creation time or checked on every request, with different implications for consistency and performance</li>\n<li><strong>Options Considered</strong>: Apply at bucket creation, check on every request, hybrid approach with periodic refresh</li>\n<li><strong>Decision</strong>: Apply overrides at bucket creation with optional runtime refresh</li>\n<li><strong>Rationale</strong>: Applying at creation provides consistent behavior and eliminates per-request override lookup overhead. Runtime refresh capability enables dynamic limit updates for operational needs without forcing bucket recreation</li>\n<li><strong>Consequences</strong>: Excellent performance with consistent client experience, but limit changes require explicit bucket refresh or waiting for natural bucket expiration</li>\n</ul>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>The per-client rate limiting implementation requires several architectural decisions that significantly impact system behavior, performance, and operational characteristics. These decisions form the foundation for the entire client tracking subsystem.</p>\n<blockquote>\n<p><strong>Decision: Client Identifier Namespace Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Different identification methods (IP, API key, custom headers) might produce overlapping values, potentially causing incorrect rate limit sharing between unrelated clients</li>\n<li><strong>Options Considered</strong>: Global identifier space with collision risk, prefixed identifiers by type, separate storage per identifier type</li>\n<li><strong>Decision</strong>: Implement prefixed identifier namespaces (e.g., &quot;ip:192.168.1.1&quot;, &quot;apikey:abc123&quot;, &quot;custom:user456&quot;)</li>\n<li><strong>Rationale</strong>: Prefixed namespaces eliminate collision risk while maintaining single storage and lookup mechanisms. Prefixes provide clear audit trails and debugging information while supporting multiple identification strategies simultaneously</li>\n<li><strong>Consequences</strong>: Slightly longer identifier strings but guaranteed collision avoidance and clear identifier provenance</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Memory vs Accuracy Trade-offs</strong></p>\n<ul>\n<li><strong>Context</strong>: Storing individual buckets for millions of clients consumes significant memory, but aggressive cleanup may remove buckets for temporarily inactive but legitimate clients</li>\n<li><strong>Options Considered</strong>: Aggressive cleanup with short staleness periods, conservative cleanup with long staleness periods, adaptive cleanup based on memory pressure</li>\n<li><strong>Decision</strong>: Configurable staleness thresholds with memory pressure monitoring</li>\n<li><strong>Rationale</strong>: Different deployment scenarios have different memory constraints and client behavior patterns. Configurable thresholds enable optimization for specific environments while memory monitoring provides automatic protection against unbounded growth</li>\n<li><strong>Consequences</strong>: More complex configuration surface but flexibility to optimize for different operational requirements</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Bucket State Persistence</strong></p>\n<ul>\n<li><strong>Context</strong>: In-memory bucket storage loses all client state during application restarts, potentially allowing clients to exceed their intended limits immediately after restart</li>\n<li><strong>Options Considered</strong>: Pure in-memory storage, persistent storage for all buckets, selective persistence for high-value clients</li>\n<li><strong>Decision</strong>: In-memory storage with optional persistent state for identified clients through configuration</li>\n<li><strong>Rationale</strong>: Pure in-memory provides simplest implementation and best performance. Persistent storage adds significant complexity and I/O overhead. Selective persistence enables protection for critical clients while maintaining performance for the general population</li>\n<li><strong>Consequences</strong>: Fast performance and simple implementation, but temporary limit bypass possible after restart</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Per-client rate limiting introduces several subtle bugs and design issues that commonly affect implementations. Understanding these pitfalls helps avoid production incidents and performance problems.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks from Missing Cleanup</strong>\nNew clients create buckets that remain in memory indefinitely without proper cleanup. Over time, this leads to unbounded memory growth and eventual application crashes. The problem often goes unnoticed during development with limited client populations but manifests in production with diverse client traffic. Fix this by implementing robust background cleanup with configurable staleness thresholds and monitoring cleanup effectiveness through metrics.</p>\n<p>⚠️ <strong>Pitfall: Race Conditions in Bucket Creation</strong>\nMultiple concurrent requests from a new client can create duplicate buckets for the same client identifier. This results in inconsistent rate limiting where the client effectively gets multiple token buckets and higher effective limits. The race window occurs between checking for bucket existence and creating the bucket. Fix this using atomic compare-and-swap operations or double-checked locking patterns with proper synchronization.</p>\n<p>⚠️ <strong>Pitfall: Client Identifier Spoofing</strong>\nMalicious clients can manipulate identification headers to bypass rate limits by appearing as different clients on each request. This completely undermines rate limiting effectiveness and enables abuse. Common attack vectors include randomizing API keys, cycling through IP ranges, or injecting malformed headers. Fix this by validating identifier formats, using server-controlled identification when possible, and implementing identifier allow-lists for sensitive scenarios.</p>\n<p>⚠️ <strong>Pitfall: Configuration Override Conflicts</strong>\nComplex override hierarchies can create unexpected interactions where client limits don&#39;t match operator expectations. This often manifests as premium clients receiving default limits or test clients getting production limits. The problem increases with configuration complexity and multiple override dimensions. Fix this by implementing explicit override precedence rules, comprehensive override resolution testing, and operational tools to visualize effective limits for any client.</p>\n<p>⚠️ <strong>Pitfall: Lock Contention Under High Load</strong>\nPoorly designed locking can serialize all rate limit checks, creating a performance bottleneck that defeats the purpose of rate limiting. This often appears as increased latency and reduced throughput under load, particularly for popular clients with many concurrent requests. The problem typically stems from overly coarse locking granularity or lock-holding during expensive operations. Fix this using fine-grained per-bucket locking, minimizing lock hold times, and avoiding I/O operations while holding locks.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The per-client rate limiting implementation extends the basic token bucket with sophisticated client tracking, configuration management, and lifecycle coordination. The implementation requires careful attention to concurrency, memory management, and operational concerns.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client Storage</td>\n<td>Python dict with threading.RWLock</td>\n<td>Redis with key expiration</td>\n</tr>\n<tr>\n<td>Cleanup Mechanism</td>\n<td>threading.Timer with periodic cleanup</td>\n<td>APScheduler with cron-like scheduling</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON file with live reloading</td>\n<td>Consul/etcd with change notifications</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Basic logging with counters</td>\n<td>Prometheus metrics with Grafana dashboards</td>\n</tr>\n<tr>\n<td>Identifier Validation</td>\n<td>Basic string sanitization</td>\n<td>Regex patterns with whitelisting</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ratelimiter/\n  core/\n    token_bucket.py          ← Basic TokenBucket implementation\n    client_tracker.py        ← ClientBucketTracker (this section)\n    config.py                ← RateLimitConfig and overrides\n  storage/\n    memory_storage.py        ← In-memory bucket storage\n    redis_storage.py         ← Distributed storage (Milestone 4)\n  middleware/\n    flask_middleware.py      ← HTTP integration (Milestone 3)\n  utils/\n    cleanup.py               ← Background cleanup manager\n    identifiers.py           ← Client identification strategies\n  tests/\n    test_client_tracker.py   ← Unit tests for client tracking\n    test_overrides.py        ← Configuration override testing</code></pre></div>\n\n<h4 id=\"complete-starter-code-client-identifier-management\">Complete Starter Code: Client Identifier Management</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Client identification and validation utilities.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Handles extraction of client identifiers from HTTP requests with proper</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">validation and namespace management.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ipaddress</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IdentifierType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IP_ADDRESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"ip\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    API_KEY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"apikey\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CUSTOM_HEADER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"custom\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JWT_SUBJECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"jwt_sub\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClientIdentifier</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a validated client identifier with namespace information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_value: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    identifier_type: IdentifierType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    namespace: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> namespaced_id</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns the full namespaced identifier for storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.namespace</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.raw_value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IdentifierValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates and sanitizes client identifiers to prevent injection attacks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    API_KEY_PATTERN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^[a-zA-Z0-9</span><span style=\"color:#85E89D;font-weight:bold\">\\-</span><span style=\"color:#79B8FF\">_]</span><span style=\"color:#F97583\">{8,128}</span><span style=\"color:#79B8FF\">$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CUSTOM_HEADER_PATTERN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^[a-zA-Z0-9</span><span style=\"color:#85E89D;font-weight:bold\">\\-</span><span style=\"color:#79B8FF\">_</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">]</span><span style=\"color:#F97583\">{1,64}</span><span style=\"color:#79B8FF\">$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_ip_address</span><span style=\"color:#E1E4E8\">(cls, ip_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate and normalize IP address.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ip_obj </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ipaddress.ip_address(ip_str.strip())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(ip_obj)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_api_key</span><span style=\"color:#E1E4E8\">(cls, api_key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate API key format and length.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">API_KEY_PATTERN</span><span style=\"color:#E1E4E8\">.match(api_key):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> api_key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_custom_header</span><span style=\"color:#E1E4E8\">(cls, header_value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate custom header value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">CUSTOM_HEADER_PATTERN</span><span style=\"color:#E1E4E8\">.match(header_value):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> header_value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> identify_client</span><span style=\"color:#E1E4E8\">(request_data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Extract and validate client identifier from request data.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Tries multiple identification strategies in priority order:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    1. API key from Authorization header</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    2. Custom client ID from X-Client-ID header  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    3. Source IP address from connection or X-Forwarded-For</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns namespaced identifier string for storage and tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> IdentifierValidator()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Strategy 1: API Key from Authorization header</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    auth_header </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request_data.get(</span><span style=\"color:#9ECBFF\">'authorization'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> auth_header.startswith(</span><span style=\"color:#9ECBFF\">'Bearer '</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        api_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> auth_header[</span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">:]  </span><span style=\"color:#6A737D\"># Remove 'Bearer ' prefix</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        validated_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validator.validate_api_key(api_key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> validated_key:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ClientIdentifier(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                raw_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">validated_key,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                identifier_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">IdentifierType.</span><span style=\"color:#79B8FF\">API_KEY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"apikey\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ).namespaced_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Strategy 2: Custom Client ID header</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request_data.get(</span><span style=\"color:#9ECBFF\">'x-client-id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> client_id:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        validated_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validator.validate_custom_header(client_id)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> validated_id:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ClientIdentifier(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                raw_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">validated_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                identifier_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">IdentifierType.</span><span style=\"color:#79B8FF\">CUSTOM_HEADER</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"custom\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ).namespaced_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Strategy 3: IP Address (fallback)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check X-Forwarded-For first, then remote_addr</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    forwarded_ips </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request_data.get(</span><span style=\"color:#9ECBFF\">'x-forwarded-for'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> forwarded_ips:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Take the first IP in the chain (original client)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        first_ip </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> forwarded_ips.split(</span><span style=\"color:#9ECBFF\">','</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].strip()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        validated_ip </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validator.validate_ip_address(first_ip)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> validated_ip:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ClientIdentifier(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                raw_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">validated_ip,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                identifier_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">IdentifierType.</span><span style=\"color:#79B8FF\">IP_ADDRESS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"ip\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ).namespaced_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Fallback to connection IP</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    remote_addr </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request_data.get(</span><span style=\"color:#9ECBFF\">'remote_addr'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> remote_addr:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        validated_ip </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validator.validate_ip_address(remote_addr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> validated_ip:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ClientIdentifier(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                raw_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">validated_ip,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                identifier_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">IdentifierType.</span><span style=\"color:#79B8FF\">IP_ADDRESS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"ip\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ).namespaced_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Ultimate fallback - anonymous client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"anonymous:unknown\"</span></span></code></pre></div>\n\n<h4 id=\"complete-starter-code-background-cleanup-manager\">Complete Starter Code: Background Cleanup Manager</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Background cleanup manager for removing stale client buckets.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Runs as a separate thread with configurable intervals and batch processing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CleanupStats</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Statistics from a cleanup cycle.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buckets_scanned: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buckets_removed: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cleanup_duration_seconds: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_freed_bytes: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CleanupManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages background cleanup of stale client buckets.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, client_tracker, cleanup_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 staleness_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#E1E4E8\">, max_cleanup_duration: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client_tracker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client_tracker</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cleanup_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cleanup_interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.staleness_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> staleness_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_cleanup_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_cleanup_duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cleanup_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._stop_event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Event()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the background cleanup thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cleanup_thread </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cleanup_thread.is_alive():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._logger.warning(</span><span style=\"color:#9ECBFF\">\"Cleanup thread already running\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._stop_event.clear()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cleanup_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._cleanup_loop,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"bucket-cleanup\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cleanup_thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._logger.info(</span><span style=\"color:#9ECBFF\">\"Bucket cleanup manager started\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop the background cleanup thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cleanup_thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._stop_event.set()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._cleanup_thread.join(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._logger.info(</span><span style=\"color:#9ECBFF\">\"Bucket cleanup manager stopped\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _cleanup_loop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main cleanup loop running in background thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._stop_event.is_set():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cleanup_stale_buckets()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._logger.info(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"Cleanup cycle completed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">stats.buckets_removed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> removed \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"of </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">stats.buckets_scanned</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> scanned in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">stats.cleanup_duration_seconds</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Error during bucket cleanup: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait for next cycle or stop signal</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._stop_event.wait(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cleanup_interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_stale_buckets</span><span style=\"color:#E1E4E8\">(self) -> CleanupStats:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Perform one cleanup cycle to remove stale buckets.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns statistics about the cleanup operation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stale_threshold_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.staleness_threshold</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Phase 1: Identify stale buckets (with read lock)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stale_client_ids </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_scanned </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client_tracker._bucket_map_lock.read_lock():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> client_id, bucket_info </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client_tracker._bucket_map.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_scanned </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> bucket_info.last_accessed </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> stale_threshold_time:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    stale_client_ids.add(client_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Respect maximum cleanup duration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_cleanup_duration:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">._logger.warning(</span><span style=\"color:#9ECBFF\">\"Cleanup cycle exceeded maximum duration\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Phase 2: Remove stale buckets (with write lock)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buckets_removed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> stale_client_ids:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client_tracker._bucket_map_lock.write_lock():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> client_id </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> stale_client_ids:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Double-check staleness in case bucket was accessed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    bucket_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client_tracker._bucket_map.get(client_id)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> bucket_info </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> bucket_info.last_accessed </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> stale_threshold_time:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        del</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client_tracker._bucket_map[client_id]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        buckets_removed </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cleanup_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CleanupStats(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            buckets_scanned</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">total_scanned,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            buckets_removed</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">buckets_removed,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cleanup_duration_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">cleanup_duration,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_freed_bytes</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">buckets_removed </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#6A737D\">  # Rough estimate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-clientbuckettracker\">Core Logic Skeleton: ClientBucketTracker</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Client bucket tracker manages individual rate limit buckets for each client.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Handles bucket creation, retrieval, configuration overrides, and cleanup coordination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BucketInfo</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Container for bucket instance and metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket: </span><span style=\"color:#9ECBFF\">'TokenBucket'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_accessed: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_access_time</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update the last accessed timestamp.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_accessed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClientBucketTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Manages per-client token buckets with automatic cleanup and configuration overrides.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Thread-safe storage and retrieval of client-specific rate limit buckets.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Integrates with cleanup manager for memory management.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: </span><span style=\"color:#9ECBFF\">'RateLimitConfig'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._bucket_map: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, BucketInfo] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._bucket_map_lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RWLock()  </span><span style=\"color:#6A737D\"># Reader-writer lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_bucket_for_client</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'TokenBucket'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Get or create token bucket for the specified client.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Applies configuration overrides based on client_id and endpoint.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Updates last accessed time for cleanup tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            client_id: Namespaced client identifier from identify_client()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            endpoint: Optional endpoint name for endpoint-specific limits</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            TokenBucket instance configured for this client</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Try to get existing bucket with read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Acquire read lock on bucket map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Check if client_id exists in bucket map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - If exists, update access time and return bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Release read lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create new bucket if not found (with write lock to prevent races)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Acquire write lock on bucket map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Double-check client doesn't exist (race condition prevention)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - If still doesn't exist, create new bucket with resolve_bucket_config()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Store BucketInfo with bucket and current timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Release write lock and return new bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle any locking exceptions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Ensure locks are properly released in finally blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Log bucket creation for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> resolve_bucket_config</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'TokenBucketConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Resolve effective configuration for a client, applying overrides.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Priority order:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        1. Client-specific override for endpoint: client_overrides[client_id].endpoint_limits[endpoint]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        2. Client-specific default: client_overrides[client_id] </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        3. Endpoint default: endpoint_limits[endpoint]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        4. Global default: default_limits</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            client_id: Namespaced client identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            endpoint: Optional endpoint name</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            TokenBucketConfig with resolved limits</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start with global default configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Get self.config.default_limits as base configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply endpoint-specific defaults if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Check if endpoint exists in self.config.endpoint_limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Override base config with endpoint-specific limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply client-specific overrides if available  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Check if client_id exists in self.config.client_overrides</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Override with client-specific configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply client-endpoint specific overrides if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Check if client has endpoint-specific overrides</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Apply most specific override last</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return final resolved configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Log resolved configuration for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_stale_buckets</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Remove buckets that haven't been accessed within staleness threshold.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Called by CleanupManager background thread.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of buckets removed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate staleness cutoff time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - current_time - self.config.cleanup_interval</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify stale buckets with read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Scan all buckets in map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Collect client_ids where last_accessed &#x3C; cutoff</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Remove stale buckets with write lock  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Double-check staleness before removal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Delete from bucket map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Count removed buckets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return count of removed buckets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Log cleanup statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_client_count</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current number of tracked clients.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return length of bucket map with appropriate locking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_client_stats</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get statistics for a specific client bucket.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Find bucket for client_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract current tokens, last access time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return stats dictionary or None if not found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing per-client rate limiting, verify the following behavior:</p>\n<p><strong>Unit Test Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_client_tracker.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p>Expected test coverage:</p>\n<ul>\n<li>Client identification with various header combinations</li>\n<li>Bucket creation and retrieval for new clients  </li>\n<li>Configuration override resolution with multiple precedence levels</li>\n<li>Stale bucket cleanup with timing verification</li>\n<li>Concurrent access patterns with multiple threads</li>\n</ul>\n<p><strong>Manual Testing Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test client identification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tracker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ClientBucketTracker(config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">client_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> identify_client({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'authorization'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'Bearer abc123def456'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'x-forwarded-for'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'192.168.1.100'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'remote_addr'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'10.0.0.1'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Identified client: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">client_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should be \"apikey:abc123def456\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test bucket creation and retrieval</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">bucket1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tracker.get_bucket_for_client(client_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">bucket2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tracker.get_bucket_for_client(client_id)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> bucket1 </span><span style=\"color:#F97583\">is</span><span style=\"color:#E1E4E8\"> bucket2  </span><span style=\"color:#6A737D\"># Should be same instance</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test configuration overrides</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">premium_client_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"apikey:premium123\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">premium_bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tracker.get_bucket_for_client(premium_client_id)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Premium bucket capacity: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">premium_bucket.capacity</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should show override</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test cleanup timing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">time.sleep(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">removed_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tracker.cleanup_stale_buckets()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Buckets removed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">removed_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should be 0 (not stale yet)</span></span></code></pre></div>\n\n<p><strong>Signs of Correct Implementation:</strong></p>\n<ul>\n<li>Different clients get separate bucket instances</li>\n<li>Same client always gets the same bucket instance  </li>\n<li>Premium clients receive configured override limits</li>\n<li>Stale bucket cleanup runs without errors</li>\n<li>No memory leaks under sustained load testing</li>\n<li>Thread-safe operation under concurrent access</li>\n</ul>\n<p><strong>Common Issues to Check:</strong></p>\n<ul>\n<li>Race conditions during bucket creation (test with high concurrency)</li>\n<li>Memory leaks from missing cleanup (run long-term load test)</li>\n<li>Configuration override precedence errors (test multiple override levels)</li>\n<li>Lock contention causing performance degradation (profile under load)</li>\n</ul>\n<h2 id=\"http-middleware-integration\">HTTP Middleware Integration</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (HTTP Middleware Integration) - this section transforms the per-client rate limiter into production-ready HTTP middleware with proper status codes, headers, and framework integration</p>\n</blockquote>\n<h3 id=\"mental-model-the-nightclub-bouncer\">Mental Model: The Nightclub Bouncer</h3>\n<p>Think of HTTP middleware as a bouncer at a nightclub entrance. Every person (HTTP request) must pass by the bouncer before entering the club (reaching your application logic). The bouncer checks each person&#39;s ID (client identification), consults their clipboard with the guest list and capacity limits (rate limiting rules), and either waves them through with a stamp on their hand (rate limit headers) or politely turns them away with information about when they can return (HTTP 429 response with Retry-After header).</p>\n<p>The bouncer operates at the entrance - not inside the club where the real party happens. This separation means the club&#39;s bartenders and DJ (your application code) never have to worry about checking IDs or managing capacity. They can focus on their core job while the bouncer handles all access control decisions consistently across every entrance to the venue.</p>\n<h3 id=\"middleware-design-pattern\">Middleware Design Pattern</h3>\n<p>HTTP middleware follows an <strong>interceptor pattern</strong> where each middleware component wraps the next component in the processing chain. The rate limiting middleware sits early in this chain - typically after request parsing and authentication but before any business logic processing. This positioning ensures that rate limiting decisions happen before expensive operations like database queries or external API calls.</p>\n<p>The middleware design separates concerns cleanly: the rate limiter focuses solely on request throttling while delegating client identification, token bucket management, and storage to specialized components. This separation allows the middleware to remain framework-agnostic - the same core logic works with Flask, Express, FastAPI, or any framework that supports the middleware pattern.</p>\n<blockquote>\n<p><strong>Design Principle</strong>: Middleware should be <strong>stateless and composable</strong>. Each middleware instance should not maintain internal state beyond configuration, allowing multiple instances to handle requests interchangeably. The middleware should also play nicely with other middleware - logging, authentication, CORS - without side effects or ordering dependencies.</p>\n</blockquote>\n<p>The middleware operates through a <strong>request lifecycle hook</strong>: it intercepts each incoming HTTP request, performs its rate limiting logic, and either allows the request to continue to the next middleware or returns an error response directly. This hook-based approach means the middleware integrates naturally with framework request processing pipelines.</p>\n<p><strong>Middleware Responsibilities:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client Identification</td>\n<td>Extract client identifier from request headers, IP address, or custom fields</td>\n<td>Determines which rate limit bucket to consult</td>\n</tr>\n<tr>\n<td>Rate Limit Checking</td>\n<td>Consult the appropriate token bucket and attempt to consume tokens</td>\n<td>Core rate limiting decision logic</td>\n</tr>\n<tr>\n<td>HTTP Response Generation</td>\n<td>Generate appropriate HTTP responses with correct status codes and headers</td>\n<td>Provides standard-compliant feedback to clients</td>\n</tr>\n<tr>\n<td>Configuration Resolution</td>\n<td>Determine which rate limits apply based on client and endpoint</td>\n<td>Supports flexible per-client and per-endpoint policies</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Handle storage failures, configuration errors, and edge cases gracefully</td>\n<td>Ensures system reliability under failure conditions</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Middleware Positioning in Request Pipeline</strong></p>\n<ul>\n<li><strong>Context</strong>: HTTP frameworks process requests through a pipeline of middleware components, and positioning affects both functionality and performance</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Early position (after routing, before authentication)</li>\n<li>Middle position (after authentication, before business logic)  </li>\n<li>Late position (just before business logic execution)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Middle position after authentication but before business logic</li>\n<li><strong>Rationale</strong>: This allows rate limiting to use authenticated client information for identification while still protecting expensive business logic operations from abuse</li>\n<li><strong>Consequences</strong>: Enables per-user rate limiting and protects most system resources, but authentication costs are still incurred for rate-limited requests</li>\n</ul>\n</blockquote>\n<h3 id=\"http-response-handling\">HTTP Response Handling</h3>\n<p>HTTP rate limiting follows established standards and conventions to ensure client applications can handle rate limiting responses appropriately. The middleware must generate responses that are both human-readable and machine-parseable, allowing both developers debugging issues and automated systems to understand rate limiting behavior.</p>\n<p><strong>HTTP Status Code Standards:</strong></p>\n<p>When a client exceeds their rate limit, the middleware returns HTTP status code <strong>429 Too Many Requests</strong>. This status code, defined in RFC 6585, specifically indicates that the user has sent too many requests in a given amount of time. The 429 response should never be cached by intermediate proxies or CDNs, ensuring that rate limiting decisions remain dynamic and responsive to current request patterns.</p>\n<p>For allowed requests, the middleware adds rate limiting headers to the response but preserves the original HTTP status code from the downstream application. This approach ensures that rate limiting operates transparently - successful requests continue to return 200, 201, or other appropriate success codes while carrying additional rate limiting metadata.</p>\n<p><strong>Standard Rate Limiting Headers:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Header Name</th>\n<th>When Present</th>\n<th>Value Format</th>\n<th>Example</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>X-RateLimit-Limit</code></td>\n<td>All responses</td>\n<td>Integer tokens per window</td>\n<td><code>1000</code></td>\n<td>Maximum requests allowed per time window</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Remaining</code></td>\n<td>All responses</td>\n<td>Integer remaining tokens</td>\n<td><code>847</code></td>\n<td>Requests remaining in current window</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Reset</code></td>\n<td>All responses</td>\n<td>Unix timestamp</td>\n<td><code>1699123456</code></td>\n<td>When the rate limit window resets</td>\n</tr>\n<tr>\n<td><code>Retry-After</code></td>\n<td>429 responses only</td>\n<td>Seconds to wait</td>\n<td><code>60</code></td>\n<td>How long to wait before retrying</td>\n</tr>\n</tbody></table>\n<p>The <code>X-RateLimit-*</code> headers appear on every response, not just 429 errors. This allows client applications to implement proactive rate limiting - they can slow down their request rate when <code>X-RateLimit-Remaining</code> drops low, avoiding rate limit errors entirely. Well-designed API clients use these headers to implement <strong>adaptive backoff strategies</strong>.</p>\n<p><strong>Error Response Body Format:</strong></p>\n<p>The 429 response includes a structured error body that provides both human-readable and machine-readable information about the rate limiting decision:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"rate_limit_exceeded\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Rate limit exceeded. Maximum 1000 requests per hour allowed.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"retry_after_seconds\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"limit\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"remaining\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"reset_time\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"2023-11-04T15:30:56Z\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>This response format balances human readability with programmatic parsing. The <code>error</code> field provides a stable identifier for automated error handling, while <code>message</code> offers human-readable context. The numeric fields allow client applications to implement sophisticated retry logic without parsing strings.</p>\n<blockquote>\n<p><strong>Decision: Rate Limiting Header Standardization</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple header standards exist for rate limiting (GitHub&#39;s X-RateLimit-<em>, Twitter&#39;s X-Rate-Limit-</em>, custom approaches), and consistency improves client integration</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>GitHub-style headers (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset)</li>\n<li>Twitter-style headers (X-Rate-Limit-Limit, X-Rate-Limit-Remaining, X-Rate-Limit-Reset)</li>\n<li>Custom application-specific headers</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: GitHub-style X-RateLimit-* headers with Reset as Unix timestamp</li>\n<li><strong>Rationale</strong>: GitHub&#39;s approach is widely adopted, uses clear naming, and Unix timestamps avoid timezone parsing issues</li>\n<li><strong>Consequences</strong>: Enables compatibility with existing client libraries and monitoring tools, but requires Unix timestamp handling</li>\n</ul>\n</blockquote>\n<h3 id=\"framework-integration-points\">Framework Integration Points</h3>\n<p>HTTP middleware integration varies significantly across web frameworks, but the core principles remain consistent. The middleware needs to intercept requests early in the processing pipeline, perform rate limiting logic, and either allow the request to continue or return an error response. Each framework provides specific hooks and patterns for this integration.</p>\n<p><strong>Flask Integration Pattern:</strong></p>\n<p>Flask uses <strong>decorator-based middleware</strong> where rate limiting can be applied as a decorator to individual routes or as application-wide middleware using <code>before_request</code> hooks. The decorator approach provides fine-grained control over which endpoints have rate limiting, while the application-wide approach ensures comprehensive protection.</p>\n<p>Flask middleware accesses request information through the global <code>request</code> object and can return responses directly to short-circuit normal request processing. The middleware integrates with Flask&#39;s error handling system, allowing 429 responses to be processed by custom error handlers if needed.</p>\n<p><strong>Express.js Integration Pattern:</strong></p>\n<p>Express uses <strong>function-based middleware</strong> where each middleware is a function that receives <code>(req, res, next)</code> parameters. Rate limiting middleware examines the request, consults rate limiting logic, and either calls <code>next()</code> to continue processing or sends a response directly using <code>res.status(429).json()</code>.</p>\n<p>Express middleware can modify the request object to pass rate limiting information to downstream handlers, enabling applications to adjust behavior based on remaining rate limit capacity.</p>\n<p><strong>Framework-Agnostic Design:</strong></p>\n<p>To support multiple frameworks efficiently, the middleware separates <strong>framework-specific code</strong> from <strong>core rate limiting logic</strong>. The core logic operates on framework-neutral data structures:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Framework-Specific</th>\n<th>Framework-Neutral</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Parsing</td>\n<td>Extract headers, IP, path from framework request object</td>\n<td>Process standardized request dictionary</td>\n</tr>\n<tr>\n<td>Rate Limit Logic</td>\n<td>None</td>\n<td>All token bucket and client tracking logic</td>\n</tr>\n<tr>\n<td>Response Generation</td>\n<td>Set framework-specific status codes and headers</td>\n<td>Generate standard response data structure</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Load from framework config system</td>\n<td>Process standardized configuration objects</td>\n</tr>\n</tbody></table>\n<p>This separation allows the same rate limiting engine to support Flask, Express, FastAPI, Django, and other frameworks with only thin adapter layers handling framework-specific integration details.</p>\n<p><strong>Middleware Configuration Integration:</strong></p>\n<p>Web frameworks typically provide configuration systems for environment variables, config files, and runtime settings. The rate limiting middleware integrates with these systems to load <code>RateLimitConfig</code> objects without requiring manual configuration in application code.</p>\n<p>Flask applications can configure rate limiting through environment variables or application config dictionaries. Express applications can use config modules or environment-based configuration. The middleware provides helper functions like <code>from_environment()</code> that automatically detect and parse configuration from common sources.</p>\n<blockquote>\n<p><strong>Decision: Configuration Loading Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Web applications use diverse configuration approaches (environment variables, config files, database settings), and the middleware must integrate cleanly without imposing specific patterns</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Environment variables only with standard naming conventions</li>\n<li>Framework-specific config file integration</li>\n<li>Hybrid approach supporting both environment variables and framework configs</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Environment variables with standard names as primary method, plus framework-specific config integration</li>\n<li><strong>Rationale</strong>: Environment variables work across all frameworks and deployment environments, while framework integration provides developer convenience</li>\n<li><strong>Consequences</strong>: Enables consistent deployment configuration while maintaining developer ergonomics, but requires maintaining multiple configuration code paths</li>\n</ul>\n</blockquote>\n<h3 id=\"per-endpoint-rate-limiting\">Per-Endpoint Rate Limiting</h3>\n<p>Different API endpoints have vastly different resource requirements and abuse potential. A simple health check endpoint can handle thousands of requests per second, while a complex report generation endpoint might only support a few requests per minute. Per-endpoint rate limiting allows administrators to configure appropriate limits that reflect each endpoint&#39;s actual capacity and business requirements.</p>\n<p><strong>Endpoint Identification Strategies:</strong></p>\n<p>The middleware identifies endpoints using <strong>route patterns</strong> rather than exact URL matches. Route patterns capture the logical endpoint while ignoring dynamic path parameters like user IDs or resource identifiers. For example, <code>/api/users/123</code> and <code>/api/users/456</code> both match the route pattern <code>/api/users/:id</code> and share the same rate limit configuration.</p>\n<table>\n<thead>\n<tr>\n<th>Identification Method</th>\n<th>Example</th>\n<th>Use Case</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Route Pattern</td>\n<td><code>/api/users/:id</code></td>\n<td>REST APIs with path parameters</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>HTTP Method + Path</td>\n<td><code>GET /api/reports</code></td>\n<td>Method-specific limits</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Custom Header</td>\n<td><code>X-Endpoint-Type: heavy</code></td>\n<td>Application-defined grouping</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Regex Matching</td>\n<td><code>/api/v[0-9]+/users</code></td>\n<td>Version-agnostic limits</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<p><strong>Hierarchical Rate Limit Resolution:</strong></p>\n<p>When multiple rate limiting rules could apply to a single request, the middleware uses a <strong>hierarchical resolution strategy</strong> that prioritizes more specific configurations over general ones:</p>\n<ol>\n<li><strong>Client + Endpoint Override</strong>: Specific rate limit for this client on this endpoint (highest priority)</li>\n<li><strong>Client Override</strong>: Client-specific rate limit across all endpoints</li>\n<li><strong>Endpoint Limit</strong>: Endpoint-specific rate limit for all clients</li>\n<li><strong>Default Global Limit</strong>: Fallback rate limit when no specific configuration exists (lowest priority)</li>\n</ol>\n<p>This hierarchy allows administrators to start with broad default limits and add targeted overrides for specific clients or endpoints as needed. For example, a premium API client might have higher limits globally, while the <code>/api/search</code> endpoint might have lower limits for all clients due to its computational expense.</p>\n<p><strong>Rate Limit Composition:</strong></p>\n<p>Some scenarios require <strong>multiple independent rate limits</strong> to apply simultaneously. A client might be subject to both a per-endpoint limit (100 requests/minute to <code>/api/search</code>) and a global account limit (1000 requests/hour across all endpoints). The middleware evaluates all applicable limits and denies the request if any limit would be exceeded.</p>\n<table>\n<thead>\n<tr>\n<th>Limit Type</th>\n<th>Scope</th>\n<th>Example Configuration</th>\n<th>Enforcement Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-Endpoint</td>\n<td>Single endpoint, all clients</td>\n<td><code>/api/search</code>: 10/minute</td>\n<td>Check endpoint bucket</td>\n</tr>\n<tr>\n<td>Per-Client</td>\n<td>Single client, all endpoints</td>\n<td>Client ABC: 1000/hour</td>\n<td>Check client bucket</td>\n</tr>\n<tr>\n<td>Per-Client-Endpoint</td>\n<td>Specific client + endpoint</td>\n<td>Client ABC on <code>/api/reports</code>: 5/hour</td>\n<td>Check combined bucket</td>\n</tr>\n<tr>\n<td>Global</td>\n<td>All clients, all endpoints</td>\n<td>System-wide: 10000/minute</td>\n<td>Check global bucket</td>\n</tr>\n</tbody></table>\n<p><strong>Dynamic Endpoint Configuration:</strong></p>\n<p>Production systems often need to adjust rate limits without restarting applications. The middleware supports <strong>dynamic configuration updates</strong> by reloading rate limit rules from external sources like configuration databases, admin APIs, or file system monitors.</p>\n<p>Configuration changes take effect immediately for new requests, but existing token buckets continue operating with their current parameters until they&#39;re naturally cleaned up by the stale bucket cleanup process. This approach avoids disrupting ongoing request patterns while ensuring new patterns adopt updated limits promptly.</p>\n<blockquote>\n<p><strong>Decision: Rate Limit Composition Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Some API scenarios require multiple simultaneous rate limits (per-endpoint, per-client, global), and the middleware must determine how to evaluate and enforce multiple applicable limits</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single most-specific limit only (hierarchical override)</li>\n<li>All applicable limits must pass (intersection)</li>\n<li>Most restrictive applicable limit wins (minimum)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: All applicable limits must pass (intersection approach)</li>\n<li><strong>Rationale</strong>: This provides maximum protection by ensuring no individual limit is exceeded, and matches administrator expectations about how multiple limits should interact</li>\n<li><strong>Consequences</strong>: Enables fine-grained control and prevents any single limit from being bypassed, but increases complexity and can create confusing scenarios where requests are denied by unexpected limit combinations</li>\n</ul>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<blockquote>\n<p><strong>Decision: Middleware Error Response Format</strong></p>\n<ul>\n<li><strong>Context</strong>: When rate limits are exceeded, clients need both human-readable error messages and machine-parseable data to implement proper retry logic and error handling</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Plain text error messages with rate limit headers only</li>\n<li>JSON error body with structured fields and rate limit headers</li>\n<li>Custom binary format for minimal bandwidth usage</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: JSON error body with structured fields plus standard HTTP headers</li>\n<li><strong>Rationale</strong>: JSON provides excellent balance of human readability and programmatic parsing, while headers ensure compatibility with HTTP caching and proxy infrastructure</li>\n<li><strong>Consequences</strong>: Enables sophisticated client retry logic and debugging, but increases response size for rate-limited requests (typically small fraction of total traffic)</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Middleware Integration Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Different web frameworks (Flask, Express, FastAPI) have different middleware patterns, and the rate limiter must integrate cleanly without tight coupling to any specific framework</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Separate implementation for each framework with framework-specific optimizations</li>\n<li>Framework-agnostic core with thin adapter layers for each framework</li>\n<li>Single implementation targeting the most common framework patterns</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Framework-agnostic core with thin adapter layers</li>\n<li><strong>Rationale</strong>: This maximizes code reuse while allowing framework-specific optimizations in the adapter layer, and reduces maintenance burden across multiple framework integrations</li>\n<li><strong>Consequences</strong>: Enables broad framework support with minimal code duplication, but requires careful abstraction design and may miss some framework-specific optimization opportunities</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Rate Limit Header Timing</strong></p>\n<ul>\n<li><strong>Context</strong>: Rate limit headers can be added before processing the request (showing pre-request state) or after processing (showing post-request state), and this affects client understanding of their remaining capacity</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Pre-request headers showing capacity before current request</li>\n<li>Post-request headers showing capacity after current request</li>\n<li>Both pre and post headers for complete visibility</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Post-request headers showing remaining capacity after processing current request</li>\n<li><strong>Rationale</strong>: Post-request headers provide more actionable information for client rate limiting decisions and match the behavior of major API providers like GitHub and Stripe</li>\n<li><strong>Consequences</strong>: Clients receive accurate information about their remaining capacity for subsequent requests, but may be slightly less intuitive for debugging rate limiting issues</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Missing Rate Limit Headers on Success Responses</strong></p>\n<p>Many implementations only add rate limiting headers to 429 error responses, leaving clients without visibility into their rate limit status during normal operation. This prevents clients from implementing proactive rate limiting and leads to unexpected 429 errors.</p>\n<p><strong>Why it&#39;s wrong</strong>: Clients need rate limit information on every response to implement adaptive request pacing. Without this information, well-behaved clients cannot avoid hitting rate limits, leading to poor user experience and unnecessary server load from retry attempts.</p>\n<p><strong>How to fix</strong>: Add <code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>, and <code>X-RateLimit-Reset</code> headers to every response, regardless of HTTP status code. The middleware should calculate these values after attempting to consume tokens, providing accurate post-request state information.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Retry-After Calculation</strong></p>\n<p>Some implementations calculate <code>Retry-After</code> based on the rate limit window reset time rather than the actual time when tokens will be available for the blocked request. This leads to clients waiting longer than necessary, reducing system throughput.</p>\n<p><strong>Why it&#39;s wrong</strong>: If a client needs 5 tokens but only 3 are available, they don&#39;t need to wait for the entire bucket to refill - they only need to wait until 2 additional tokens are generated. Waiting for a full window reset wastes time and reduces effective API throughput.</p>\n<p><strong>How to fix</strong>: Calculate <code>Retry-After</code> based on when sufficient tokens will be available for the specific request: <code>tokens_needed / refill_rate</code>. This provides the minimum wait time for the request to succeed.</p>\n<p>⚠️ <strong>Pitfall: Client Identification Inconsistency</strong></p>\n<p>Different middleware instances or different request processing paths sometimes extract client identifiers differently, leading to the same client being treated as multiple distinct clients. This effectively bypasses rate limiting by spreading requests across multiple buckets.</p>\n<p><strong>Why it&#39;s wrong</strong>: Inconsistent client identification allows clients to exceed rate limits by varying request characteristics like header formatting, IP address representation, or authentication token format. This undermines the entire rate limiting system&#39;s effectiveness.</p>\n<p><strong>How to fix</strong>: Implement robust client identifier normalization that handles variations in IP address format (IPv6 vs IPv4, leading zeros), header capitalization, and authentication token formatting. Use the <code>validate_ip_address()</code> and <code>validate_api_key()</code> functions consistently across all code paths.</p>\n<p>⚠️ <strong>Pitfall: Framework Integration Ordering Issues</strong></p>\n<p>Placing rate limiting middleware after authentication or authorization middleware can lead to expensive operations being performed before rate limit checking, allowing denial-of-service attacks that consume resources even when requests are ultimately rate-limited.</p>\n<p><strong>Why it&#39;s wrong</strong>: If authentication involves database queries or external API calls, attackers can consume significant resources by sending many requests that are authenticated but then rate-limited. This reduces the effectiveness of rate limiting as a protective mechanism.</p>\n<p><strong>How to fix</strong>: Position rate limiting middleware early in the processing pipeline, ideally after basic request parsing but before expensive authentication operations. For endpoints that require authenticated rate limiting, implement a two-stage approach with basic rate limiting early and authenticated rate limiting after authentication.</p>\n<p>⚠️ <strong>Pitfall: Static Configuration Without Reload</strong></p>\n<p>Hardcoding rate limit configurations or loading them only at application startup prevents operators from adjusting limits in response to changing conditions or abuse patterns without service restarts.</p>\n<p><strong>Why it&#39;s wrong</strong>: Production systems need dynamic rate limit adjustment to handle traffic spikes, mitigate abuse, or accommodate special events. Requiring application restarts for configuration changes introduces operational risk and delays incident response.</p>\n<p><strong>How to fix</strong>: Implement configuration reload capabilities that monitor external configuration sources (environment variables, config files, database tables) and update rate limiting rules without requiring application restarts. Ensure configuration changes are applied atomically to avoid inconsistent states.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides Flask-specific implementation guidance for integrating the rate limiting middleware. The implementation assumes you have already completed the token bucket algorithm and per-client rate limiting components from previous milestones.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Web Framework</td>\n<td>Flask with built-in request handling</td>\n<td>Flask with Werkzeug middleware stack</td>\n</tr>\n<tr>\n<td>HTTP Client Identification</td>\n<td><code>request.remote_addr</code> for IP</td>\n<td>X-Forwarded-For parsing with proxy chain validation</td>\n</tr>\n<tr>\n<td>Configuration Loading</td>\n<td>Environment variables with <code>os.getenv()</code></td>\n<td>Flask config with JSON schema validation</td>\n</tr>\n<tr>\n<td>Response Serialization</td>\n<td><code>flask.jsonify()</code> for JSON responses</td>\n<td>Custom JSON encoder with datetime handling</td>\n</tr>\n<tr>\n<td>Header Management</td>\n<td>Direct <code>response.headers[]</code> assignment</td>\n<td>Response middleware with header normalization</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>rate_limiter/\n├── middleware/\n│   ├── __init__.py\n│   ├── flask_middleware.py          ← main Flask integration\n│   ├── rate_limit_middleware.py     ← framework-agnostic core\n│   └── response_builder.py          ← HTTP response generation\n├── config/\n│   ├── __init__.py\n│   └── middleware_config.py         ← configuration loading and validation\n├── examples/\n│   ├── flask_app.py                 ← complete Flask application example\n│   └── flask_config_example.py      ← configuration examples\n└── tests/\n    ├── test_flask_middleware.py     ← Flask-specific integration tests\n    └── test_response_builder.py     ← HTTP response testing</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete HTTP Response Builder</strong> (ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># response_builder.py - Complete implementation for HTTP response generation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> flask </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Response</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitResponseBuilder</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Builds standardized HTTP responses for rate limiting scenarios.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.standard_headers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'Content-Type'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/json'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'Cache-Control'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'no-store'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> build_success_response</span><span style=\"color:#E1E4E8\">(self, original_response: Response, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              consumption_result, rate_config) -> Response:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add rate limiting headers to successful response.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._add_rate_limit_headers(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            original_response, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            consumption_result, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            rate_config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> original_response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> build_rate_limit_exceeded_response</span><span style=\"color:#E1E4E8\">(self, consumption_result, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                         rate_config) -> Response:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Build 429 Too Many Requests response with proper headers and body.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_body </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'error'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'rate_limit_exceeded'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'message'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'Rate limit exceeded. Maximum </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">rate_config.capacity</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> requests per </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">._format_time_window(rate_config)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> allowed.'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'retry_after_seconds'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(consumption_result.retry_after_seconds),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'limit'</span><span style=\"color:#E1E4E8\">: rate_config.capacity,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'remaining'</span><span style=\"color:#E1E4E8\">: consumption_result.tokens_remaining,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'reset_time'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._calculate_reset_time(rate_config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Response(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            response</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">json.dumps(error_body, </span><span style=\"color:#FFAB70\">indent</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            status</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">429</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            headers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.standard_headers.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add standard rate limiting headers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._add_rate_limit_headers(response, consumption_result, rate_config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response.headers[</span><span style=\"color:#9ECBFF\">'Retry-After'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(consumption_result.retry_after_seconds))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _add_rate_limit_headers</span><span style=\"color:#E1E4E8\">(self, response: Response, consumption_result, rate_config):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add X-RateLimit-* headers to response.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response.headers[</span><span style=\"color:#9ECBFF\">'X-RateLimit-Limit'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(rate_config.capacity)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response.headers[</span><span style=\"color:#9ECBFF\">'X-RateLimit-Remaining'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(consumption_result.tokens_remaining)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response.headers[</span><span style=\"color:#9ECBFF\">'X-RateLimit-Reset'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._calculate_reset_timestamp(rate_config)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_time_window</span><span style=\"color:#E1E4E8\">(self, rate_config) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert refill rate to human-readable time window.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> rate_config.refill_rate </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{int</span><span style=\"color:#E1E4E8\">(rate_config.refill_rate)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> per second\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> rate_config.refill_rate </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{int</span><span style=\"color:#E1E4E8\">(rate_config.refill_rate </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> per minute\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{int</span><span style=\"color:#E1E4E8\">(rate_config.refill_rate </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> per hour\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_reset_time</span><span style=\"color:#E1E4E8\">(self, rate_config) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate ISO 8601 formatted reset time.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        reset_timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._calculate_reset_timestamp(rate_config)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> datetime.fromtimestamp(reset_timestamp).isoformat() </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> 'Z'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_reset_timestamp</span><span style=\"color:#E1E4E8\">(self, rate_config) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate Unix timestamp when rate limit resets.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate when bucket will be full based on current state and refill rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seconds_to_full </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rate_config.capacity </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> rate_config.refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> current_time </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> seconds_to_full</span></span></code></pre></div>\n\n<p><strong>Complete Configuration Loader</strong> (ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># middleware_config.py - Complete configuration loading with validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MiddlewareConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for rate limiting middleware integration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enabled: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_identification_strategy: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'ip_address'</span><span style=\"color:#6A737D\">  # ip_address, api_key, custom_header</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    custom_header_name: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    skip_rate_limiting_header: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'X-Skip-Rate-Limit'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_response_format: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'json'</span><span style=\"color:#6A737D\">  # json, plain_text</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    include_debug_info: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_middleware_config</span><span style=\"color:#E1E4E8\">() -> MiddlewareConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load middleware configuration from environment variables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> MiddlewareConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        enabled</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'RATE_LIMIT_ENABLED'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'true'</span><span style=\"color:#E1E4E8\">).lower() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'true'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        client_identification_strategy</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'RATE_LIMIT_CLIENT_ID_STRATEGY'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'ip_address'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        custom_header_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'RATE_LIMIT_CUSTOM_HEADER'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        skip_rate_limiting_header</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'RATE_LIMIT_SKIP_HEADER'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'X-Skip-Rate-Limit'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        error_response_format</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'RATE_LIMIT_ERROR_FORMAT'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'json'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        include_debug_info</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'RATE_LIMIT_DEBUG'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'false'</span><span style=\"color:#E1E4E8\">).lower() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'true'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClientIdentificationHelper</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper for extracting and validating client identifiers from requests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extract_client_id</span><span style=\"color:#E1E4E8\">(request, strategy: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, custom_header: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract client identifier using specified strategy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> strategy </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'ip_address'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ClientIdentificationHelper._extract_ip_address(request)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> strategy </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'api_key'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ClientIdentificationHelper._extract_api_key(request)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> strategy </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'custom_header'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> custom_header:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ClientIdentificationHelper._extract_custom_header(request, custom_header)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Fallback to IP address</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ClientIdentificationHelper._extract_ip_address(request)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _extract_ip_address</span><span style=\"color:#E1E4E8\">(request) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract and validate client IP address with proxy support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for X-Forwarded-For header (proxy/load balancer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        forwarded_for </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request.headers.get(</span><span style=\"color:#9ECBFF\">'X-Forwarded-For'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> forwarded_for:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Take the first IP in the chain (original client)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            client_ip </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> forwarded_for.split(</span><span style=\"color:#9ECBFF\">','</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].strip()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Direct connection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            client_ip </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request.remote_addr </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> '127.0.0.1'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Validate and normalize IP address</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        validated_ip </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validate_ip_address(client_ip)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> validated_ip </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> '127.0.0.1'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _extract_api_key</span><span style=\"color:#E1E4E8\">(request) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract API key from Authorization header or query parameter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check Authorization header first</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        auth_header </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request.headers.get(</span><span style=\"color:#9ECBFF\">'Authorization'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> auth_header.startswith(</span><span style=\"color:#9ECBFF\">'Bearer '</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            api_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> auth_header[</span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">:]  </span><span style=\"color:#6A737D\"># Remove 'Bearer ' prefix</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            validated_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validate_api_key(api_key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> validated_key:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> validated_key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check X-API-Key header</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        api_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request.headers.get(</span><span style=\"color:#9ECBFF\">'X-API-Key'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> api_key:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            validated_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validate_api_key(api_key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> validated_key:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> validated_key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Fallback to IP address if no valid API key found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ClientIdentificationHelper._extract_ip_address(request)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _extract_custom_header</span><span style=\"color:#E1E4E8\">(request, header_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract client ID from custom header.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        custom_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request.headers.get(header_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> custom_value </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(custom_value.strip()) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> custom_value.strip()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Fallback to IP address if custom header missing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ClientIdentificationHelper._extract_ip_address(request)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Framework-Agnostic Middleware Core</strong> (skeleton for implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># rate_limit_middleware.py - Core middleware logic (implement the TODOs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitMiddleware</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Framework-agnostic rate limiting middleware core.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, client_tracker, response_builder, middleware_config):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client_tracker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client_tracker</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.response_builder </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> response_builder</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> middleware_config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client_id_helper </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ClientIdentificationHelper()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> process_request</span><span style=\"color:#E1E4E8\">(self, request_data: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Process rate limiting for a single HTTP request.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            request_data: Framework-neutral request information</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - headers: Dict[str, str]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - remote_addr: str</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - path: str</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - method: str</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Processing result dict with:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - allowed: bool</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - response_data: Optional[dict] (if rate limited)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - consumption_result: TokenConsumptionResult</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                - rate_config: TokenBucketConfig</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if rate limiting is disabled in configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If disabled, return early with allowed=True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract client identifier using configured strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use self.client_id_helper.extract_client_id() with request_data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle the strategy from self.config.client_identification_strategy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Determine endpoint identifier from request path and method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Format as \"METHOD /path/pattern\" for endpoint-specific rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Consider normalizing path parameters (e.g., /users/123 -> /users/:id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Get appropriate token bucket for client and endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use self.client_tracker.get_bucket_for_client(client_id, endpoint)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This should handle per-client and per-endpoint configuration resolution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Attempt to consume tokens for this request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use bucket.try_consume(1) to attempt consuming 1 token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store the TokenConsumptionResult for response building</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Build response based on consumption result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If allowed, return success result with rate limit headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If denied, build 429 response with Retry-After header</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Include consumption_result and rate_config for header generation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_skip_rate_limiting</span><span style=\"color:#E1E4E8\">(self, request_data: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check if request should bypass rate limiting.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if request has skip header or matches bypass conditions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for skip rate limiting header</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Look for self.config.skip_rate_limiting_header in request headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Return True if header is present and has truthy value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add any additional bypass conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Consider health check endpoints, internal service calls, etc.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Return True for requests that should bypass rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> normalize_endpoint_path</span><span style=\"color:#E1E4E8\">(self, path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, method: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Normalize endpoint path for consistent rate limit grouping.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Converts paths like /users/123 to /users/:id for rate limiting purposes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Combine HTTP method with path for unique endpoint identification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Format as \"GET /api/users\" or \"POST /api/reports\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Replace numeric path segments with parameter placeholders</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # /users/123 -> /users/:id, /reports/2023-11-04 -> /reports/:date</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use regex or string processing to identify and replace patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle common REST patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # /api/v1/users/123/posts/456 -> /api/v1/users/:id/posts/:id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Consider UUID patterns, date patterns, and numeric IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return normalized endpoint identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This becomes the key for endpoint-specific rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span></code></pre></div>\n\n<p><strong>Flask-Specific Integration</strong> (skeleton for implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># flask_middleware.py - Flask integration wrapper (implement the TODOs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> functools </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> wraps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> flask </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> request, g</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FlaskRateLimitMiddleware</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Flask-specific wrapper for rate limiting middleware.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, app</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.core_middleware </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> app:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.init_app(app)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> init_app</span><span style=\"color:#E1E4E8\">(self, app):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize middleware with Flask application.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load configuration using load_middleware_config()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store middleware configuration in self.middleware_config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load rate limiting configuration using from_environment()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store rate limit rules in self.rate_limit_config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize ClientBucketTracker with configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Pass rate_limit_config to create client_tracker instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize RateLimitResponseBuilder</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create response_builder for generating HTTP responses</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create core middleware instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize RateLimitMiddleware with tracker, response_builder, and config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Register Flask before_request handler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use app.before_request to register self._before_request_handler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Register Flask after_request handler  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use app.after_request to register self._after_request_handler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _before_request_handler</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Flask before_request handler for rate limiting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert Flask request to framework-neutral format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract headers, remote_addr, path, method from Flask request object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create request_data dict with standardized field names</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Process request through core middleware</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Call self.core_middleware.process_request(request_data)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store result in Flask g object for access in after_request</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle rate limit exceeded case</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If result['allowed'] is False, return 429 response immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use result['response_data'] to build Flask Response object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This short-circuits normal request processing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store rate limiting info for successful requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Save consumption_result and rate_config in g for after_request handler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This allows adding headers to successful responses</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _after_request_handler</span><span style=\"color:#E1E4E8\">(self, response):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Flask after_request handler to add rate limiting headers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if rate limiting information is available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Look for rate limiting data stored in g by before_request handler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Skip header addition if no rate limiting data present</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add rate limiting headers to response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use response_builder to add X-RateLimit-* headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply headers to both successful and error responses</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return modified response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Flask after_request handlers must return the response object</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> limit</span><span style=\"color:#E1E4E8\">(self, rate_limit_override</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Decorator for applying rate limiting to specific Flask routes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        def</span><span style=\"color:#B392F0\"> decorator</span><span style=\"color:#E1E4E8\">(f):</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            @wraps</span><span style=\"color:#E1E4E8\">(f)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            def</span><span style=\"color:#B392F0\"> decorated_function</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply route-specific rate limiting logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # This allows individual routes to have custom rate limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Use rate_limit_override parameter if provided</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Perform rate limit check specific to this route</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Similar to before_request_handler but with route-specific config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Call original route function if allowed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Return 429 response if rate limited</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> decorated_function</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> decorator</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Flask Request Handling:</strong></p>\n<ul>\n<li>Use <code>request.headers.get(&#39;Header-Name&#39;)</code> to safely access headers with default values</li>\n<li><code>request.remote_addr</code> provides client IP, but check <code>X-Forwarded-For</code> for proxy setups</li>\n<li>Flask&#39;s <code>g</code> object provides request-scoped storage for sharing data between before_request and after_request handlers</li>\n<li>Use <code>request.endpoint</code> to get the route function name for endpoint-specific limiting</li>\n</ul>\n<p><strong>HTTP Response Construction:</strong></p>\n<ul>\n<li>Create Flask Response objects with <code>Response(response=json_string, status=429, headers=header_dict)</code></li>\n<li>Use <code>flask.jsonify()</code> for automatic JSON serialization with proper Content-Type headers</li>\n<li>Flask automatically handles header encoding and HTTP compliance for standard headers</li>\n</ul>\n<p><strong>Configuration Management:</strong></p>\n<ul>\n<li>Use <code>app.config.get()</code> to access Flask configuration with default values</li>\n<li>Environment variables can be loaded with <code>os.getenv(&#39;VAR_NAME&#39;, &#39;default_value&#39;)</code></li>\n<li>Consider using <code>python-dotenv</code> to load environment variables from <code>.env</code> files in development</li>\n</ul>\n<p><strong>Error Handling:</strong></p>\n<ul>\n<li>Flask&#39;s <code>@app.errorhandler(429)</code> can provide global 429 error response formatting</li>\n<li>Use try/except blocks around rate limiting logic to handle Redis connection failures gracefully</li>\n<li>Log rate limiting decisions and errors using <code>app.logger</code> for debugging and monitoring</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the HTTP middleware integration:</p>\n<p><strong>Command to Run:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> rate_limiter/</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_flask_middleware.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/flask_app.py</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_flask_middleware.py::test_successful_request_includes_headers PASSED\ntest_flask_middleware.py::test_rate_limit_exceeded_returns_429 PASSED  \ntest_flask_middleware.py::test_per_endpoint_rate_limiting PASSED\ntest_flask_middleware.py::test_client_identification_strategies PASSED\n\nFlask app running on http://127.0.0.1:5000</code></pre></div>\n\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li><strong>Test successful request with headers</strong>: <code>curl -i http://127.0.0.1:5000/api/test</code> should return 200 with <code>X-RateLimit-*</code> headers</li>\n<li><strong>Test rate limit exceeded</strong>: Send multiple rapid requests: <code>for i in {1..20}; do curl -i http://127.0.0.1:5000/api/test; done</code> - should see 429 responses</li>\n<li><strong>Test Retry-After header</strong>: 429 responses should include <code>Retry-After</code> header with reasonable wait time</li>\n<li><strong>Test per-endpoint limits</strong>: <code>/api/search</code> and <code>/api/reports</code> should have different rate limits if configured</li>\n</ol>\n<p><strong>Signs Something is Wrong:</strong></p>\n<ul>\n<li><strong>Missing headers on 200 responses</strong>: Check <code>_after_request_handler</code> implementation and ensure rate limiting data is stored in <code>g</code></li>\n<li><strong>429 responses without Retry-After</strong>: Verify <code>build_rate_limit_exceeded_response</code> includes all required headers</li>\n<li><strong>Same rate limits for all endpoints</strong>: Check endpoint normalization and configuration resolution logic</li>\n<li><strong>Client identification not working</strong>: Verify IP address extraction handles <code>X-Forwarded-For</code> and proxy scenarios correctly</li>\n</ul>\n<h2 id=\"distributed-rate-limiting\">Distributed Rate Limiting</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 (Distributed Rate Limiting) - this section scales the HTTP middleware rate limiter across multiple server instances using Redis for shared state and atomic operations</p>\n</blockquote>\n<h3 id=\"mental-model-the-multi-branch-bank-with-central-ledger\">Mental Model: The Multi-Branch Bank with Central Ledger</h3>\n<p>Think of our distributed rate limiting system like a bank with multiple branch locations serving the same customers. Each customer has a single account with a fixed spending limit, but they can visit any branch to make withdrawals. Without coordination between branches, a customer could withdraw their entire limit at Branch A, then immediately drive to Branch B and withdraw the same amount again - effectively doubling their limit.</p>\n<p>The solution banks use is a <strong>central ledger system</strong>. Every branch connects to the same central database that tracks account balances in real-time. When a customer attempts a withdrawal, the branch must check the central ledger and update the balance atomically - either the withdrawal succeeds and the balance decreases, or it fails because insufficient funds remain. Multiple branches can serve the same customer simultaneously without accidentally allowing overdrafts.</p>\n<p>In our rate limiting context, each server instance is like a bank branch, each client is like a customer, and the token bucket represents their &quot;spending limit&quot; for API requests. Redis becomes our central ledger, storing the authoritative token counts for all clients. When any server instance receives a request, it must check Redis and atomically consume tokens, ensuring that the client&#39;s total request rate across all servers never exceeds their configured limit.</p>\n<h3 id=\"distributed-system-challenges\">Distributed System Challenges</h3>\n<p>When we move from a single-server rate limiter to a distributed system, several fundamental challenges emerge that make simple in-memory token buckets insufficient for maintaining consistent rate limits across multiple server instances.</p>\n<p><strong>The Coordination Problem</strong></p>\n<p>In a single-server deployment, our <code>TokenBucket</code> class maintains accurate token counts because all requests for a given client flow through the same process. The bucket&#39;s internal state reflects the true consumption history, and thread-safe operations prevent race conditions within that single process. However, when we deploy the same rate limiter code across multiple server instances, each instance maintains its own isolated view of client token buckets.</p>\n<p>Consider a client with a limit of 100 requests per minute hitting a three-server cluster. If each server maintains independent in-memory buckets, the client could potentially make 100 requests to Server A, then 100 requests to Server B, then 100 requests to Server C - achieving 300 requests per minute despite the intended 100 request limit. Each server&#39;s bucket believes it&#39;s correctly enforcing the limit, but the <strong>distributed consistency</strong> requirement is violated because no single component has the complete picture of the client&#39;s activity across all servers.</p>\n<p><strong>State Synchronization Requirements</strong></p>\n<p>Effective distributed rate limiting requires that token bucket state remain consistent across all server instances, with updates visible immediately after they occur. This creates several technical requirements that don&#39;t exist in single-server deployments:</p>\n<p><strong>Atomic Read-Modify-Write Operations</strong>: When a server needs to consume tokens, it must read the current count, verify sufficient tokens exist, and decrement the count as a single atomic operation. If these steps are separate, race conditions occur when multiple servers attempt simultaneous token consumption for the same client.</p>\n<p><strong>Immediate Consistency</strong>: Unlike some distributed systems where eventual consistency is acceptable, rate limiting requires immediate consistency. If Server A allows a request that exhausts a client&#39;s tokens, Server B must immediately see the updated token count and deny subsequent requests. Delays in propagating state updates create windows where rate limits can be exceeded.</p>\n<p><strong>High-Frequency Updates</strong>: Token buckets require frequent updates - both for token consumption (on every request) and token refill (based on elapsed time). The coordination mechanism must handle this high update frequency without introducing significant latency to request processing.</p>\n<p><strong>Cross-Server Clock Coordination</strong>: Token refill calculations depend on elapsed time measurements. When multiple servers refill the same bucket, they must agree on timing to prevent tokens from being added multiple times or at incorrect rates due to clock skew between servers.</p>\n<p><strong>The Single Source of Truth Requirement</strong></p>\n<p>Traditional distributed systems often allow each node to maintain local state and synchronize periodically. Rate limiting systems cannot follow this pattern because they require a <strong>single source of truth</strong> for token counts that all servers consult before making allow/deny decisions.</p>\n<p>This requirement eliminates several common distributed system patterns:</p>\n<ul>\n<li><strong>Local caching with background sync</strong>: Servers cannot cache token counts locally because stale cache entries lead to rate limit violations</li>\n<li><strong>Peer-to-peer coordination</strong>: Having servers communicate directly with each other creates complex consistency protocols that are difficult to implement correctly</li>\n<li><strong>Eventually consistent storage</strong>: Token bucket updates must be immediately visible to all servers, ruling out storage systems that prioritize availability over consistency</li>\n</ul>\n<p>The single source of truth approach requires that all servers coordinate through a shared storage system that provides strong consistency guarantees and supports atomic operations on stored data.</p>\n<h3 id=\"redis-based-token-storage\">Redis-Based Token Storage</h3>\n<p>Redis serves as our centralized token storage system because it provides the atomic operations, immediate consistency, and high performance required for distributed rate limiting. The design of our Redis-based storage focuses on efficient key structures, atomic update operations, and proper data encoding for token bucket state.</p>\n<p><strong>Redis Key Design Strategy</strong></p>\n<p>Our Redis key structure must uniquely identify each client&#39;s token bucket while supporting efficient operations and avoiding key collisions. The key design follows a hierarchical pattern that embeds the client identification strategy and optional endpoint-specific limits:</p>\n<p>For basic per-client rate limiting, keys follow the pattern <code>rate_limit:client:{client_id}</code>, where <code>client_id</code> contains the full client identifier including its type. For example, an IP-based client generates keys like <code>rate_limit:client:ip:192.168.1.100</code>, while API key clients generate keys like <code>rate_limit:client:api_key:abc123def456</code>.</p>\n<p>When supporting per-endpoint rate limiting, keys extend to include the endpoint identifier: <code>rate_limit:client:{client_id}:endpoint:{endpoint_hash}</code>. The endpoint hash is a consistent representation of the HTTP method and normalized path, such as <code>GET:/api/v1/users</code> becoming <code>rate_limit:client:ip:192.168.1.100:endpoint:GET_api_v1_users</code>.</p>\n<p>This hierarchical key structure enables several operational benefits: Redis key pattern matching allows monitoring tools to query all buckets for a specific client or endpoint, key expiration can be set uniformly across related buckets, and the key namespace remains organized even with millions of active clients.</p>\n<p><strong>Token Bucket State Encoding</strong></p>\n<p>Each Redis key stores a hash data structure containing the complete token bucket state required for atomic operations. The hash contains these fields:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>tokens</code></td>\n<td>Float</td>\n<td>Current token count in the bucket, may include fractional tokens for precise calculations</td>\n</tr>\n<tr>\n<td><code>capacity</code></td>\n<td>Integer</td>\n<td>Maximum token capacity for the bucket, determines burst limit</td>\n</tr>\n<tr>\n<td><code>refill_rate</code></td>\n<td>Float</td>\n<td>Tokens added per second, supports fractional rates for fine-grained control</td>\n</tr>\n<tr>\n<td><code>last_refill</code></td>\n<td>Float</td>\n<td>Unix timestamp of last token refill operation, used for elapsed time calculations</td>\n</tr>\n<tr>\n<td><code>created_at</code></td>\n<td>Float</td>\n<td>Unix timestamp when bucket was first created, useful for debugging and monitoring</td>\n</tr>\n<tr>\n<td><code>version</code></td>\n<td>Integer</td>\n<td>Version number incremented on each update, enables optimistic locking patterns</td>\n</tr>\n</tbody></table>\n<p>Using Redis hash data structures instead of simple key-value pairs provides several advantages: all bucket fields can be read or updated atomically using <code>HMGET</code> and <code>HMSET</code> commands, individual fields can be updated without reading the entire bucket state, and the encoding is human-readable for debugging and operational monitoring.</p>\n<p><strong>Atomic Token Consumption Logic</strong></p>\n<p>The core challenge in distributed token bucket implementation is ensuring that token consumption operations are atomic across multiple steps: reading current bucket state, calculating elapsed time for refill, adding refill tokens, checking if sufficient tokens exist for the request, and updating the bucket with the new token count.</p>\n<p>Redis Lua scripts solve this atomicity requirement by executing the entire token consumption logic as a single atomic operation on the Redis server. The Lua script receives the bucket key, requested token count, current timestamp, and bucket configuration as parameters. It performs all token bucket calculations within Redis, ensuring that other concurrent operations cannot interleave with the token consumption logic.</p>\n<p>The atomic consumption algorithm implemented in Lua follows these steps within a single Redis transaction:</p>\n<ol>\n<li>Read the current bucket state using <code>redis.call(&#39;HMGET&#39;, key, &#39;tokens&#39;, &#39;last_refill&#39;, &#39;capacity&#39;, &#39;refill_rate&#39;)</code></li>\n<li>Calculate elapsed time since last refill by comparing the provided current timestamp with the stored <code>last_refill</code> value</li>\n<li>Calculate tokens to add based on elapsed time and refill rate, capping the total at bucket capacity</li>\n<li>Update the token count by adding refill tokens to the current token count</li>\n<li>Check if the updated token count is sufficient for the requested token consumption</li>\n<li>If sufficient tokens exist, subtract the requested tokens and update the bucket state with the new token count and current timestamp</li>\n<li>Return a result indicating whether the consumption succeeded and the remaining token count</li>\n</ol>\n<p>This atomic script ensures that two concurrent requests for the same client cannot both succeed if their combined token consumption would exceed the bucket capacity, regardless of which server instances process the requests.</p>\n<p><strong>Handling Redis Hash Field Initialization</strong></p>\n<p>When a client makes their first request, no bucket exists in Redis for their identifier. The atomic consumption script must handle bucket initialization while maintaining atomicity with the consumption operation. This requires careful handling of Redis hash field defaults when some or all bucket fields are missing.</p>\n<p>The Lua script uses Redis&#39;s <code>HMGET</code> command behavior where missing hash fields return <code>nil</code> values. The script provides sensible defaults for missing fields: missing <code>tokens</code> defaults to the bucket&#39;s configured capacity (full bucket for new clients), missing <code>last_refill</code> defaults to the current timestamp, and missing configuration fields default to the global rate limit settings.</p>\n<p>This initialization-on-first-use pattern ensures that new clients receive their full token allocation immediately, while avoiding the need for separate bucket creation operations that would complicate the atomic consumption logic.</p>\n<h3 id=\"atomic-operations-with-lua-scripts\">Atomic Operations with Lua Scripts</h3>\n<p>Redis Lua scripts provide the foundation for atomic token bucket operations in our distributed rate limiting system. By moving the entire token consumption and refill logic into server-side scripts, we eliminate race conditions that would occur if multiple Redis commands were executed separately from client applications.</p>\n<p><strong>The Atomicity Requirement</strong></p>\n<p>Token bucket operations require atomicity because they involve multiple interdependent steps that must appear to execute instantaneously from the perspective of concurrent requests. Consider two simultaneous requests from the same client, each requesting one token from a bucket containing exactly one token. Without atomicity, both requests might read the current token count as one, both determine that sufficient tokens exist, and both proceed to consume a token - resulting in two successful requests despite only one token being available.</p>\n<p>The atomicity requirement extends beyond simple token consumption to include time-based token refill calculations. When multiple servers simultaneously process requests for the same client, they may all calculate refill tokens based on elapsed time. Without atomicity, each server might add refill tokens independently, causing the bucket to accumulate tokens faster than the configured refill rate.</p>\n<p><strong>Lua Script Architecture</strong></p>\n<p>Our Lua script architecture implements the complete token bucket algorithm within Redis, receiving all necessary parameters from client applications and returning structured results that indicate whether token consumption succeeded. The script design minimizes the number of Redis operations while maintaining clear separation between token refill logic and consumption logic.</p>\n<p>The primary consumption script, <code>token_bucket_consume.lua</code>, accepts these parameters:</p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>KEYS[1]</code></td>\n<td>String</td>\n<td>Redis key for the client&#39;s token bucket</td>\n</tr>\n<tr>\n<td><code>ARGV[1]</code></td>\n<td>Integer</td>\n<td>Number of tokens requested for consumption</td>\n</tr>\n<tr>\n<td><code>ARGV[2]</code></td>\n<td>Float</td>\n<td>Current timestamp for elapsed time calculations</td>\n</tr>\n<tr>\n<td><code>ARGV[3]</code></td>\n<td>Integer</td>\n<td>Maximum bucket capacity</td>\n</tr>\n<tr>\n<td><code>ARGV[4]</code></td>\n<td>Float</td>\n<td>Token refill rate per second</td>\n</tr>\n<tr>\n<td><code>ARGV[5]</code></td>\n<td>Integer</td>\n<td>Initial token count for new buckets</td>\n</tr>\n</tbody></table>\n<p>The script returns a structured result as a Redis array containing the consumption result (<code>1</code> for allowed, <code>0</code> for denied), remaining token count after the operation, and seconds until the next token becomes available (useful for <code>Retry-After</code> headers).</p>\n<p><strong>Token Refill Logic in Lua</strong></p>\n<p>The token refill calculation within the Lua script must handle several edge cases while maintaining precision and preventing token bucket overflow. The refill algorithm calculates tokens to add based on elapsed time, but must account for clock corrections, very large elapsed times, and floating-point precision limits.</p>\n<p>The refill calculation follows this logic within the Lua script:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Read current bucket state, defaulting missing values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> current_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(bucket_state[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> last_refill </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(bucket_state[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> current_timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> stored_capacity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(bucket_state[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> stored_refill_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(bucket_state[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> refill_rate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Calculate elapsed time, handling clock corrections</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> elapsed_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> math.max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, current_timestamp </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> last_refill)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Calculate tokens to add, preventing overflow</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> tokens_to_add </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> elapsed_seconds </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stored_refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> updated_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> math.min</span><span style=\"color:#E1E4E8\">(stored_capacity, current_tokens </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> tokens_to_add)</span></span></code></pre></div>\n\n<p>The script handles several important edge cases: negative elapsed time (indicating clock corrections) by using <code>math.max(0, ...)</code> to prevent token removal, very large elapsed times by capping total tokens at bucket capacity using <code>math.min(...)</code>, and floating-point precision by performing all calculations in Lua&#39;s native number type.</p>\n<p><strong>Consumption Decision Logic</strong></p>\n<p>After calculating the updated token count including refill tokens, the script must decide whether to allow or deny the token consumption request. This decision logic must handle partial token consumption, ensure atomic updates, and provide sufficient information for client applications to implement proper retry behavior.</p>\n<p>The consumption decision follows this algorithm:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Check if sufficient tokens exist</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> updated_tokens </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> tokens_requested </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Consumption allowed - subtract tokens and update bucket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    local</span><span style=\"color:#E1E4E8\"> final_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> updated_tokens </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> tokens_requested</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Update bucket state atomically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'HMSET'</span><span style=\"color:#E1E4E8\">, bucket_key,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'tokens'</span><span style=\"color:#E1E4E8\">, final_tokens,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'last_refill'</span><span style=\"color:#E1E4E8\">, current_timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'version'</span><span style=\"color:#E1E4E8\">, (</span><span style=\"color:#79B8FF\">tonumber</span><span style=\"color:#E1E4E8\">(bucket_state[</span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Return success result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">math.floor</span><span style=\"color:#E1E4E8\">(final_tokens), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">else</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Consumption denied - update refill time but not token count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'HMSET'</span><span style=\"color:#E1E4E8\">, bucket_key,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'tokens'</span><span style=\"color:#E1E4E8\">, updated_tokens,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'last_refill'</span><span style=\"color:#E1E4E8\">, current_timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Calculate retry delay</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    local</span><span style=\"color:#E1E4E8\"> tokens_needed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens_requested </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> updated_tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    local</span><span style=\"color:#E1E4E8\"> retry_after </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens_needed </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> stored_refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">math.floor</span><span style=\"color:#E1E4E8\">(updated_tokens), </span><span style=\"color:#79B8FF\">math.ceil</span><span style=\"color:#E1E4E8\">(retry_after)}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">end</span></span></code></pre></div>\n\n<p>This logic ensures that bucket state is updated even for denied requests (to record the refill timestamp), provides accurate remaining token counts for rate limiting headers, and calculates precise retry delays based on the token refill rate.</p>\n<p><strong>Script Error Handling and Validation</strong></p>\n<p>The Lua script must validate input parameters and handle error conditions gracefully, since script errors cause the entire Redis operation to fail and may not provide clear error messages to client applications. Input validation within the script prevents common parameter errors while maintaining atomic operation semantics.</p>\n<p>Parameter validation includes: ensuring <code>tokens_requested</code> is a positive integer, validating that <code>current_timestamp</code> is a reasonable Unix timestamp (not negative or excessively future), checking that <code>capacity</code> and <code>refill_rate</code> are positive numbers, and verifying that the Redis key format matches expected patterns.</p>\n<p>Error handling within the script uses Lua&#39;s <code>assert()</code> function for critical errors that should abort the operation, and provides sensible defaults for missing or invalid non-critical parameters. This approach ensures that client applications receive clear error messages for invalid requests while allowing the script to handle minor parameter variations gracefully.</p>\n<h3 id=\"redis-failure-handling\">Redis Failure Handling</h3>\n<p>Redis represents a single point of failure in our distributed rate limiting architecture, requiring careful design of failure detection, graceful degradation, and recovery mechanisms to maintain service availability when Redis becomes unavailable or experiences performance issues.</p>\n<p><strong>Failure Modes and Detection</strong></p>\n<p>Redis failures manifest in several ways that require different detection and response strategies. <strong>Connection failures</strong> occur when Redis servers become unreachable due to network issues, server crashes, or configuration changes. These failures are typically detected immediately when Redis client libraries encounter connection timeouts or connection refused errors.</p>\n<p><strong>Performance degradation</strong> presents a more subtle failure mode where Redis responds to requests but with significantly increased latency. This can occur due to memory pressure, CPU saturation, or network congestion. Performance degradation requires threshold-based detection since Redis continues to function but may not meet the latency requirements for real-time rate limiting decisions.</p>\n<p><strong>Partial failures</strong> occur when Redis remains available for some operations but fails for others, such as when specific Lua scripts encounter errors or when Redis runs out of memory for new keys while serving reads for existing keys. These failures require operation-level error handling rather than connection-level failover.</p>\n<p><strong>Data inconsistency</strong> can occur during Redis failover scenarios where slave instances may not have received the latest updates before being promoted to master. This failure mode requires careful consideration of consistency versus availability trade-offs.</p>\n<p><strong>Local Fallback Strategies</strong></p>\n<p>When Redis becomes unavailable, our rate limiter must choose between failing open (allowing all requests) or failing closed (denying all requests). Neither option is ideal: failing open potentially allows abuse during outages, while failing closed creates service availability issues due to rate limiting infrastructure problems.</p>\n<p>Our recommended approach implements <strong>local fallback buckets</strong> that provide approximate rate limiting during Redis outages. Each server instance maintains in-memory token buckets for recently seen clients, but with more conservative limits to account for the lack of distributed coordination.</p>\n<p>The local fallback strategy operates according to these principles:</p>\n<p><strong>Conservative Limit Scaling</strong>: Local fallback buckets use a fraction of the configured rate limit, typically 50-70%, to account for the possibility that the same client is making requests to multiple server instances during the outage. If a client has a normal limit of 100 requests per minute, the local fallback might allow 60 requests per minute per server.</p>\n<p><strong>Limited Client Memory</strong>: To prevent memory exhaustion during extended outages, local fallback buckets are limited to a maximum number of clients per server instance (typically 10,000-50,000) with LRU eviction when the limit is exceeded.</p>\n<p><strong>Automatic Recovery</strong>: When Redis connectivity is restored, local fallback buckets are gradually phased out in favor of Redis-backed buckets. The transition includes a brief period where both systems operate in parallel to ensure smooth failover without allowing clients to double their effective limits during the transition.</p>\n<p><strong>Circuit Breaker Pattern Implementation</strong></p>\n<p>To prevent Redis failures from cascading into application performance issues, our rate limiter implements a circuit breaker pattern that detects Redis problems and automatically switches to local fallback mode.</p>\n<p>The circuit breaker maintains three states: <strong>Closed</strong> (normal operation with Redis), <strong>Open</strong> (Redis is failing, use local fallback), and <strong>Half-Open</strong> (testing whether Redis has recovered). State transitions are based on success/failure rates and response times for Redis operations.</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Behavior</th>\n<th>Transition Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>All requests use Redis for token bucket operations</td>\n<td>Transitions to Open after 5 consecutive Redis failures or 50% failure rate over 30 seconds</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>All requests use local fallback buckets</td>\n<td>Transitions to Half-Open after 60-second timeout</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Test requests use Redis, others use local fallback</td>\n<td>Transitions to Closed after 3 consecutive Redis successes, or back to Open on any failure</td>\n</tr>\n</tbody></table>\n<p>The circuit breaker tracks Redis operation metrics including response times, error rates, and timeout frequencies. Gradual degradation triggers circuit opening before complete Redis failure, allowing the system to switch to local fallback before users experience request timeouts.</p>\n<p><strong>Connection Pool Management</strong></p>\n<p>Redis connection failures often stem from connection pool exhaustion or configuration issues rather than Redis server problems. Our failure handling strategy includes intelligent connection pool management that distinguishes between Redis server issues and client-side connection problems.</p>\n<p>Connection pool management includes: <strong>Adaptive pool sizing</strong> that increases connection pool size during high load periods and decreases it during quiet periods, <strong>Connection health checks</strong> that proactively detect stale connections and replace them before they cause request failures, <strong>Exponential backoff reconnection</strong> that prevents connection storms when Redis becomes available after an outage, and <strong>Connection distribution</strong> across multiple Redis instances when using Redis Cluster or master-slave setups.</p>\n<p>The connection pool monitors these metrics to detect impending failures: connection wait times (indicating pool exhaustion), connection establishment times (indicating network issues), and connection lifetime statistics (indicating connection stability issues).</p>\n<p><strong>Recovery and State Synchronization</strong></p>\n<p>When Redis connectivity is restored after an outage, our rate limiter must carefully transition from local fallback mode back to distributed mode while maintaining rate limiting accuracy and avoiding thundering herd problems.</p>\n<p>The recovery process follows these steps:</p>\n<ol>\n<li><strong>Gradual reconnection</strong>: Server instances reconnect to Redis using exponential backoff to prevent overwhelming a recovering Redis instance</li>\n<li><strong>State validation</strong>: Compare local fallback bucket states with any available Redis state to detect inconsistencies</li>\n<li><strong>Conservative merging</strong>: When both local and Redis state exist for the same client, use the more restrictive token count to prevent accidental rate limit violations</li>\n<li><strong>Monitoring period</strong>: Operate in a hybrid mode where new clients use Redis while existing clients gradually transition from local fallback</li>\n<li><strong>Full transition</strong>: Complete the switch to Redis-only operation after confirming stable performance</li>\n</ol>\n<p>This recovery process typically takes 2-5 minutes to complete, ensuring that Redis has fully recovered and can handle the production load before local fallback systems are disabled.</p>\n<h3 id=\"clock-synchronization-considerations\">Clock Synchronization Considerations</h3>\n<p>Distributed rate limiting systems are particularly sensitive to clock synchronization issues because token refill calculations depend on accurate time measurements across multiple server instances. Clock drift, leap seconds, and system clock corrections can all impact the accuracy of rate limiting decisions in subtle but important ways.</p>\n<p><strong>The Time Dependency Problem</strong></p>\n<p>Token bucket algorithms calculate refill tokens based on elapsed time since the last refill operation. In a single-server system, this calculation uses a single system clock and remains consistent. However, in distributed systems where multiple servers may update the same Redis-stored bucket, time measurements from different servers can introduce inconsistencies.</p>\n<p>Consider a scenario where Server A has a system clock that runs 30 seconds fast compared to Server B. If Server A processes a request and updates a bucket&#39;s <code>last_refill</code> timestamp using its local clock, Server B will later calculate elapsed time using the difference between its local clock and Server A&#39;s timestamp. This 30-second clock drift means Server B will calculate 30 seconds less elapsed time than actually occurred, resulting in fewer refill tokens being added to the bucket.</p>\n<p><strong>Clock Synchronization Strategies</strong></p>\n<p>Network Time Protocol (NTP) provides the standard solution for maintaining synchronized clocks across distributed systems. However, NTP synchronization is not perfect and typically maintains accuracy within 1-50 milliseconds under normal conditions, with occasional larger corrections when significant drift is detected.</p>\n<p>For rate limiting systems, clock synchronization requirements depend on the rate limiting time scales. Systems with minute-level rate limits (100 requests per minute) can tolerate several seconds of clock drift without significant impact. However, systems with second-level rate limits (10 requests per second) require much tighter clock synchronization to maintain accuracy.</p>\n<p>Our recommended clock synchronization strategy includes:</p>\n<p><strong>Mandatory NTP Configuration</strong>: All server instances must run NTP clients configured to synchronize with reliable time sources. Multiple time sources should be configured to handle individual time server failures.</p>\n<p><strong>Clock Drift Monitoring</strong>: Application monitoring should track clock drift between server instances by comparing timestamps in shared Redis operations. Drift exceeding configurable thresholds (typically 5-10 seconds) should trigger alerts.</p>\n<p><strong>Gradual Clock Correction</strong>: When NTP makes clock corrections, the changes should be gradual rather than sudden jumps when possible. Large clock corrections can cause token bucket calculations to become temporarily inaccurate.</p>\n<p><strong>Time Source Redundancy</strong>: Critical deployments should use multiple independent time sources and detect when individual sources provide inconsistent time information.</p>\n<p><strong>Timestamp Validation in Lua Scripts</strong></p>\n<p>Our Redis Lua scripts can implement timestamp validation to detect and handle obvious clock synchronization issues. The validation logic compares incoming timestamps with Redis&#39;s internal clock and with previously stored timestamps to identify potential clock problems.</p>\n<p>The Lua script timestamp validation includes these checks:</p>\n<p><strong>Future Timestamp Detection</strong>: If an incoming timestamp is significantly ahead of Redis&#39;s internal time (using the <code>TIME</code> command), the script can limit the timestamp to prevent excessive token refill calculations.</p>\n<p><strong>Backwards Time Detection</strong>: If an incoming timestamp is earlier than the stored <code>last_refill</code> timestamp, indicating clock corrections or clock drift, the script can handle this gracefully by using the stored timestamp rather than calculating negative elapsed time.</p>\n<p><strong>Reasonable Bounds Checking</strong>: Incoming timestamps should fall within reasonable bounds (not too far in the past or future) to prevent calculation errors caused by corrupted timestamps or client clock issues.</p>\n<p>Here&#39;s how the timestamp validation logic works within the Lua script:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Get Redis server time for validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> redis_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'TIME'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> redis_timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> tonumber</span><span style=\"color:#E1E4E8\">(redis_time[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">tonumber</span><span style=\"color:#E1E4E8\">(redis_time[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1000000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Validate incoming timestamp against reasonable bounds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> max_future_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#6A737D\">  -- 5 minutes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> max_past_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#6A737D\">   -- 1 hour</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> current_timestamp </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> (redis_timestamp </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> max_future_seconds) </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Timestamp too far in future, use Redis time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">elseif</span><span style=\"color:#E1E4E8\"> current_timestamp </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> (redis_timestamp </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> max_past_seconds) </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Timestamp too far in past, use Redis time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">end</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Handle backwards time (clock correction)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> current_timestamp </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> last_refill </span><span style=\"color:#F97583\">then</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    -- Use stored timestamp to prevent negative elapsed time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> last_refill</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">end</span></span></code></pre></div>\n\n<p>This validation prevents clock issues from causing token bucket calculation errors while maintaining the atomicity of Redis operations.</p>\n<p><strong>Handling Clock Corrections and Leap Seconds</strong></p>\n<p>System clock corrections, including leap seconds and NTP adjustments, can cause temporary inconsistencies in token bucket calculations. Large forward clock corrections can cause excessive token refill, while backward corrections can temporarily prevent token refill.</p>\n<p>Our handling strategy for clock corrections includes:</p>\n<p><strong>Maximum Elapsed Time Limits</strong>: Token refill calculations use a maximum elapsed time cap (typically 5-10 minutes) to prevent excessive token accumulation when clocks jump forward significantly.</p>\n<p><strong>Minimum Refill Intervals</strong>: Buckets maintain minimum intervals between refill operations to prevent rapid-fire refill calculations during clock instability.</p>\n<p><strong>Leap Second Handling</strong>: During leap second events, token refill calculations may experience one-second discrepancies. This is typically acceptable for rate limiting systems operating at minute-level granularities.</p>\n<p><strong>Clock Correction Logging</strong>: System logs capture significant clock corrections to aid in troubleshooting rate limiting anomalies that may correlate with time synchronization events.</p>\n<p>The combination of these strategies ensures that normal clock synchronization variations do not significantly impact rate limiting accuracy, while providing graceful handling of larger clock correction events.</p>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<blockquote>\n<p><strong>Decision: Redis vs Other Distributed Storage Options</strong></p>\n<ul>\n<li><strong>Context</strong>: Our distributed rate limiting system requires a shared storage layer that supports atomic operations, high throughput, and low latency for token bucket state management across multiple server instances.</li>\n<li><strong>Options Considered</strong>: Redis with Lua scripts, Apache Cassandra with lightweight transactions, PostgreSQL with advisory locks, etcd with atomic transactions</li>\n<li><strong>Decision</strong>: Redis with Lua scripts for atomic token bucket operations</li>\n<li><strong>Rationale</strong>: Redis provides the optimal combination of atomic operations (via Lua scripts), microsecond-level latency, high throughput capacity, and simple operational requirements. Lua scripts enable complex token bucket logic to execute atomically on the server side, eliminating race conditions inherent in multi-step operations. Redis&#39;s single-threaded event loop ensures consistent performance characteristics, and its simple key-value model aligns well with token bucket storage requirements.</li>\n<li><strong>Consequences</strong>: This choice provides excellent performance and consistency but introduces Redis as a critical dependency and single point of failure. Redis&#39;s memory-only storage model requires careful capacity planning, and Lua script complexity increases compared to simple key-value operations.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis + Lua</td>\n<td>Atomic operations, &lt;1ms latency, simple ops</td>\n<td>Single point of failure, memory limits</td>\n<td>✓ Yes</td>\n</tr>\n<tr>\n<td>Cassandra + LWT</td>\n<td>Distributed, high availability</td>\n<td>Complex setup, higher latency</td>\n<td>No</td>\n</tr>\n<tr>\n<td>PostgreSQL + locks</td>\n<td>ACID guarantees, familiar SQL</td>\n<td>Much higher latency, complex locking</td>\n<td>No</td>\n</tr>\n<tr>\n<td>etcd + transactions</td>\n<td>Strong consistency, distributed</td>\n<td>Limited throughput, complex API</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Lua Script Complexity vs Multiple Redis Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Token bucket operations require reading current state, calculating refill tokens, checking availability, and updating state. This can be implemented as either a single Lua script or multiple separate Redis commands coordinated by the application.</li>\n<li><strong>Options Considered</strong>: Single comprehensive Lua script, multiple Redis commands with application-side coordination, Redis transactions (MULTI/EXEC) with application logic</li>\n<li><strong>Decision</strong>: Single comprehensive Lua script containing all token bucket logic</li>\n<li><strong>Rationale</strong>: Atomic execution is critical for rate limiting correctness - any approach that allows other operations to interleave with token consumption creates race conditions where limits can be exceeded. A comprehensive Lua script ensures that the entire token bucket operation (refill calculation, availability check, consumption, state update) executes atomically on the Redis server without possibility of interference from concurrent operations.</li>\n<li><strong>Consequences</strong>: This approach guarantees correctness and eliminates race conditions but increases complexity of the Lua script and makes debugging more challenging. Script errors affect the entire operation, and script development requires Redis-specific knowledge.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Local Fallback vs Fail-Closed During Redis Outages</strong></p>\n<ul>\n<li><strong>Context</strong>: When Redis becomes unavailable, our rate limiter must choose between allowing all requests (fail-open), denying all requests (fail-closed), or implementing local fallback buckets with reduced accuracy.</li>\n<li><strong>Options Considered</strong>: Fail-open (allow all requests), fail-closed (deny all requests), local fallback with conservative limits, hybrid approach with request prioritization</li>\n<li><strong>Decision</strong>: Local fallback with conservative limits (60% of normal rate limits per server instance)</li>\n<li><strong>Rationale</strong>: Fail-open creates unacceptable risk of abuse during Redis outages, potentially causing service degradation or cost overruns. Fail-closed creates availability issues where rate limiting infrastructure problems affect core service availability. Local fallback provides reasonable protection against abuse while maintaining service availability, using conservative limits to account for lack of cross-server coordination.</li>\n<li><strong>Consequences</strong>: This approach maintains service availability during Redis outages but provides less precise rate limiting during fallback periods. It requires additional memory for local buckets and complex logic for transitioning between Redis and local modes.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Circuit Breaker Pattern vs Simple Retry Logic</strong></p>\n<ul>\n<li><strong>Context</strong>: Redis failures can manifest as complete unavailability, performance degradation, or intermittent errors. The system needs to detect these conditions and switch to fallback mode appropriately.</li>\n<li><strong>Options Considered</strong>: Simple retry with exponential backoff, circuit breaker pattern with multiple states, health check-based switching, timeout-based failover</li>\n<li><strong>Decision</strong>: Circuit breaker pattern with Closed/Open/Half-Open states</li>\n<li><strong>Rationale</strong>: Simple retry logic doesn&#39;t prevent cascading failures when Redis is experiencing performance issues rather than complete failure. Circuit breaker pattern provides graduated response to different failure modes - quickly switching to fallback for complete failures, gradually backing off during performance degradation, and automatically testing for recovery without overwhelming a struggling Redis instance.</li>\n<li><strong>Consequences</strong>: Circuit breaker provides more sophisticated failure handling but adds complexity in state management and threshold tuning. It requires careful configuration of failure detection thresholds and recovery timing.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Non-Atomic Token Consumption Operations</strong></p>\n<p>A critical mistake in distributed rate limiting implementation is performing token bucket operations as multiple separate Redis commands rather than a single atomic Lua script. Developers often implement token consumption by first reading the current bucket state with <code>HMGET</code>, performing calculations in application code, then updating the bucket with <code>HMSET</code>. This creates a race condition window where multiple concurrent requests can read the same token count, all determine that sufficient tokens exist, and all proceed to consume tokens simultaneously.</p>\n<p>For example, if two requests arrive simultaneously for a client with exactly one token remaining, both requests might execute <code>HMGET</code> and receive a token count of 1, both calculate that they can consume one token, and both execute <code>HMSET</code> to update the bucket - resulting in two successful requests despite only one token being available.</p>\n<p><strong>Fix</strong>: Implement all token bucket logic within a single Redis Lua script that executes atomically. The script should read current state, calculate refill tokens, check availability, and update state as one indivisible operation that cannot be interrupted by other operations.</p>\n<p>⚠️ <strong>Pitfall: Clock Drift Causing Token Calculation Errors</strong></p>\n<p>When multiple server instances update the same Redis-stored token bucket, differences in system clocks can cause significant errors in token refill calculations. If Server A has a clock running 60 seconds fast and updates a bucket&#39;s <code>last_refill</code> timestamp, Server B will later calculate elapsed time based on the difference between its local clock and Server A&#39;s timestamp. This clock skew results in Server B calculating 60 seconds less elapsed time than actually occurred, causing tokens to refill more slowly than configured.</p>\n<p>The impact compounds over time as different servers with different clock skews update the same buckets, creating unpredictable and inconsistent rate limiting behavior that&#39;s difficult to debug.</p>\n<p><strong>Fix</strong>: Ensure all server instances run NTP clients for clock synchronization, implement timestamp validation in Lua scripts that compare incoming timestamps against Redis server time, and add maximum elapsed time caps to prevent excessive token accumulation when clocks jump forward.</p>\n<p>⚠️ <strong>Pitfall: Missing Redis Connection Pool Configuration</strong></p>\n<p>Default Redis client configurations often use minimal connection pools that become bottlenecks under production load. Each token consumption operation requires a Redis round-trip, so inadequate connection pool sizing creates request queuing that adds latency to every API request and can cause timeouts during traffic spikes.</p>\n<p>Additionally, many developers don&#39;t configure connection health checks, leading to scenarios where stale connections remain in the pool and cause intermittent failures that are difficult to diagnose.</p>\n<p><strong>Fix</strong>: Configure Redis connection pools with sufficient size for peak load (typically 10-50 connections per server instance), enable connection health checks with reasonable timeout values, and implement connection pool monitoring to track utilization and detect pool exhaustion before it affects requests.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Lua Script Error Handling</strong></p>\n<p>Redis Lua scripts that don&#39;t validate input parameters or handle error conditions gracefully can cause the entire rate limiting operation to fail with unclear error messages. Common issues include scripts that assume all hash fields exist (causing nil value errors), scripts that don&#39;t validate timestamp formats (causing type conversion errors), and scripts that don&#39;t handle Redis memory limits (causing out-of-memory errors during bucket creation).</p>\n<p>When Lua scripts fail, they often return generic error messages that don&#39;t clearly indicate whether the failure was due to invalid input, Redis server issues, or script logic errors, making troubleshooting difficult.</p>\n<p><strong>Fix</strong>: Implement comprehensive input validation in Lua scripts using <code>assert()</code> for critical errors and sensible defaults for missing values. Add error context to script responses that help identify the specific failure cause, and test scripts with invalid inputs to ensure they fail gracefully.</p>\n<p>⚠️ <strong>Pitfall: Local Fallback Memory Leaks During Extended Outages</strong></p>\n<p>When implementing local fallback buckets for Redis outages, developers often create unlimited in-memory bucket storage without considering memory consumption during extended outages. If a Redis outage lasts several hours and the application continues serving diverse clients, local fallback buckets can accumulate indefinitely and consume all available server memory.</p>\n<p>This problem is particularly severe in environments with many unique client identifiers (such as IP-based rate limiting) where each new client creates a new in-memory bucket that persists until the outage ends.</p>\n<p><strong>Fix</strong>: Implement strict limits on local fallback bucket storage (typically 10,000-50,000 buckets per server), use LRU eviction when limits are exceeded, and add memory usage monitoring for local fallback systems. Consider using more conservative rate limits during fallback to account for reduced accuracy when buckets are evicted.</p>\n<p>⚠️ <strong>Pitfall: Improper Redis Failover During Circuit Breaker Transitions</strong></p>\n<p>When implementing circuit breaker patterns for Redis failure handling, developers often create thundering herd problems during recovery by having all server instances simultaneously test Redis availability when transitioning from Open to Half-Open state. This can overwhelm a recovering Redis instance and cause it to fail again immediately.</p>\n<p>Additionally, improper timing of circuit breaker state transitions can cause rapid oscillation between Redis and local fallback modes, creating inconsistent rate limiting behavior and confusing operational monitoring.</p>\n<p><strong>Fix</strong>: Implement jittered timing for circuit breaker state transitions so that server instances don&#39;t all test Redis recovery simultaneously. Use gradual recovery approaches where only a small percentage of requests test Redis availability during Half-Open state, and require sustained success over multiple seconds before fully reopening the circuit.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation bridges our Redis-based distributed architecture with production-ready Python code that handles atomic operations, failure scenarios, and recovery mechanisms.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Client</td>\n<td>redis-py with connection pooling</td>\n<td>redis-py-cluster for Redis Cluster</td>\n</tr>\n<tr>\n<td>Connection Management</td>\n<td>Single Redis instance with retry logic</td>\n<td>Redis Sentinel for high availability</td>\n</tr>\n<tr>\n<td>Time Synchronization</td>\n<td>NTP client with monitoring</td>\n<td>Chrony with multiple time sources</td>\n</tr>\n<tr>\n<td>Circuit Breaker</td>\n<td>Simple failure count thresholds</td>\n<td>pybreaker library with metrics</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Basic Redis metrics logging</td>\n<td>Prometheus metrics with alerting</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Environment variables</td>\n<td>Consul/etcd for dynamic config</td>\n</tr>\n</tbody></table>\n<p><strong>Redis Client Setup and Configuration</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis.sentinel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConnectionManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages Redis connections with failover and circuit breaker logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_pool: Optional[redis.ConnectionPool] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.sentinel: Optional[redis.sentinel.Sentinel] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.circuit_breaker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreaker()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._setup_connection()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _setup_connection</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize Redis connection with appropriate configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse Redis URL from config and determine if using Sentinel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create connection pool with proper sizing (max_connections=50)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure connection timeouts (socket_connect_timeout=5.0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set up connection health check parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Initialize Sentinel if using high availability setup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_redis_client</span><span style=\"color:#E1E4E8\">(self) -> redis.Redis:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get Redis client with circuit breaker protection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check circuit breaker state before creating client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return client from connection pool</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle circuit breaker Open state by raising exception</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_with_fallback</span><span style=\"color:#E1E4E8\">(self, operation, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute Redis operation with automatic fallback on failure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Attempt operation with circuit breaker monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Record success/failure for circuit breaker state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: On failure, trigger local fallback if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return result or raise appropriate exception</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple circuit breaker for Redis operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, failure_threshold</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, recovery_timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> failure_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_timeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> recovery_timeout</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'CLOSED'</span><span style=\"color:#6A737D\">  # CLOSED, OPEN, HALF_OPEN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> call</span><span style=\"color:#E1E4E8\">(self, func, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute function with circuit breaker protection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check current state and handle OPEN state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute function and handle success/failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update failure count and state transitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return result or raise CircuitBreakerError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Atomic Lua Scripts for Token Operations</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Redis Lua script for atomic token bucket consumption</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TOKEN_BUCKET_CONSUME_SCRIPT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local bucket_key = KEYS[1]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local tokens_requested = tonumber(ARGV[1])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local current_timestamp = tonumber(ARGV[2])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local capacity = tonumber(ARGV[3])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local refill_rate = tonumber(ARGV[4])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local initial_tokens = tonumber(ARGV[5])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- Read current bucket state</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local bucket_state = redis.call('HMGET', bucket_key, 'tokens', 'last_refill', 'capacity', 'refill_rate')</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- Handle new bucket creation with defaults</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local current_tokens = tonumber(bucket_state[1]) or initial_tokens or capacity</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local last_refill = tonumber(bucket_state[2]) or current_timestamp</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local stored_capacity = tonumber(bucket_state[3]) or capacity</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local stored_refill_rate = tonumber(bucket_state[4]) or refill_rate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- Validate timestamp to prevent clock issues</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local redis_time = redis.call('TIME')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local redis_timestamp = tonumber(redis_time[1]) + (tonumber(redis_time[2]) / 1000000)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local max_future_seconds = 300</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local max_past_seconds = 3600</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if current_timestamp > (redis_timestamp + max_future_seconds) then</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    current_timestamp = redis_timestamp</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">elseif current_timestamp &#x3C; (redis_timestamp - max_past_seconds) then</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    current_timestamp = redis_timestamp</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">end</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- Handle backwards time (clock correction)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if current_timestamp &#x3C; last_refill then</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    current_timestamp = last_refill</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">end</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- Calculate elapsed time and refill tokens</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local elapsed_seconds = math.max(0, current_timestamp - last_refill)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local tokens_to_add = elapsed_seconds * stored_refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local updated_tokens = math.min(stored_capacity, current_tokens + tokens_to_add)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- Make consumption decision</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if updated_tokens >= tokens_requested then</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    -- Consumption allowed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    local final_tokens = updated_tokens - tokens_requested</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    redis.call('HMSET', bucket_key,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'tokens', final_tokens,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'last_refill', current_timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'capacity', stored_capacity,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'refill_rate', stored_refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    -- Set expiration to clean up unused buckets (24 hours)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    redis.call('EXPIRE', bucket_key, 86400)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    return {1, math.floor(final_tokens), 0}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">else</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    -- Consumption denied</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    redis.call('HMSET', bucket_key,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'tokens', updated_tokens,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'last_refill', current_timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'capacity', stored_capacity,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'refill_rate', stored_refill_rate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    redis.call('EXPIRE', bucket_key, 86400)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    local tokens_needed = tokens_requested - updated_tokens</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    local retry_after = math.ceil(tokens_needed / stored_refill_rate)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    return {0, math.floor(updated_tokens), retry_after}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">end</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DistributedTokenBucket</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Redis-backed token bucket with atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_manager: RedisConnectionManager, config: TokenBucketConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_manager</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.consume_script </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._register_scripts()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _register_scripts</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register Lua scripts with Redis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get Redis client from connection manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Register TOKEN_BUCKET_CONSUME_SCRIPT using script_load()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store script SHA for efficient execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle script registration failures gracefully</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Attempt to consume tokens using atomic Redis operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate Redis key for client bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get current timestamp for refill calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute Lua script with bucket parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse script result into TokenConsumptionResult</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle Redis errors and trigger fallback if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_bucket_status</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current bucket status without consuming tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Read bucket state using HMGET</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate current tokens including refill</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return structured status information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LocalFallbackBucket</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"In-memory token bucket for Redis fallback scenarios.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TokenBucketConfig, max_clients: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_clients </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_clients</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.buckets: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, BucketInfo] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.access_order: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []  </span><span style=\"color:#6A737D\"># For LRU eviction</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume tokens from local fallback bucket.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get or create bucket for client_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Perform LRU eviction if max_clients exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate token refill based on elapsed time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Make consumption decision and update bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update access order for LRU tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _evict_lru_buckets</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove least recently used buckets when over capacity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate number of buckets to evict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Remove oldest buckets from both buckets dict and access_order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Log eviction statistics for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DistributedRateLimiter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main distributed rate limiter with Redis and local fallback.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: RateLimitConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisConnectionManager(config.redis_config)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.distributed_buckets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># client_id -> DistributedTokenBucket</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.local_fallback </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LocalFallbackBucket(config.default_limits)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_redis_available </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> process_request</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Process rate limiting request with automatic fallback.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Resolve effective rate limit config for client/endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Attempt Redis-based consumption if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Fall back to local buckets on Redis failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update Redis availability status based on operation result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return consumption result with appropriate retry timing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_distributed_bucket</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: TokenBucketConfig) -> DistributedTokenBucket:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get or create distributed token bucket for client.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if bucket already exists in cache</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create new DistributedTokenBucket with client config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Cache bucket instance for reuse</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_redis_failure</span><span style=\"color:#E1E4E8\">(self, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle Redis operation failures and update circuit breaker.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Log Redis failure details for troubleshooting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update circuit breaker state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set redis availability flag for fallback decision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Schedule Redis recovery testing if appropriate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Configuration and Environment Integration</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Redis connection configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    url: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_connections: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    socket_timeout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    socket_connect_timeout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_on_timeout: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    health_check_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DistributedRateLimitConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RateLimitConfig</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Extended configuration for distributed rate limiting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_config: RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuit_breaker_failure_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuit_breaker_recovery_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    local_fallback_enabled: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    local_fallback_max_clients: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    local_fallback_rate_multiplier: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.6</span><span style=\"color:#6A737D\">  # 60% of normal limits</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_distributed_config</span><span style=\"color:#E1E4E8\">() -> DistributedRateLimitConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load distributed rate limiting configuration from environment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load base RateLimitConfig from environment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse REDIS_URL and connection parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure circuit breaker thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set up local fallback parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate configuration and provide sensible defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing distributed rate limiting, verify the system works correctly across multiple server instances:</p>\n<ol>\n<li><strong>Start Redis server</strong>: <code>redis-server --port 6379</code></li>\n<li><strong>Start multiple application instances</strong>: Run your rate limiter application on different ports (8000, 8001, 8002)</li>\n<li><strong>Test cross-instance coordination</strong>: Send requests for the same client to different server instances and verify that the combined rate across all servers respects the configured limit</li>\n<li><strong>Verify Redis fallback</strong>: Stop Redis server and confirm that requests continue to be processed with local fallback buckets using more conservative limits</li>\n<li><strong>Test Redis recovery</strong>: Restart Redis and verify that the system gradually transitions back to distributed mode without allowing clients to exceed their limits during the transition</li>\n</ol>\n<p>Expected behavior: A client with a 60 requests/minute limit should be denied after making 60 requests total across all server instances, regardless of which specific servers handle the requests. During Redis outages, the same client should be limited to approximately 36 requests/minute per server instance (60% of normal limit).</p>\n<p>Signs of problems and debugging steps:</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Clients exceed rate limits</td>\n<td>Non-atomic Redis operations</td>\n<td>Check Redis MONITOR output for multiple commands per request</td>\n<td>Implement Lua scripts for atomicity</td>\n</tr>\n<tr>\n<td>Inconsistent rate limiting</td>\n<td>Clock drift between servers</td>\n<td>Compare server timestamps in Redis bucket data</td>\n<td>Configure NTP synchronization</td>\n</tr>\n<tr>\n<td>High request latency</td>\n<td>Redis connection pool exhaustion</td>\n<td>Monitor Redis client connection metrics</td>\n<td>Increase connection pool size</td>\n</tr>\n<tr>\n<td>Rate limiter fails completely</td>\n<td>Redis connection failure without fallback</td>\n<td>Check Redis connectivity and fallback configuration</td>\n<td>Implement local fallback buckets</td>\n</tr>\n<tr>\n<td>Memory usage grows during outages</td>\n<td>Local fallback bucket leaks</td>\n<td>Monitor local bucket count and memory usage</td>\n<td>Add LRU eviction to local fallbacks</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Fdistributed-sequence.svg\" alt=\"Distributed Rate Limiting Sequence\"></p>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Handling and Recovery Flow\"></p>\n<h2 id=\"component-interactions-and-data-flow\">Component Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section shows how components work together, evolving from basic token bucket interactions through distributed Redis coordination</p>\n</blockquote>\n<h3 id=\"mental-model-the-orchestra-performance\">Mental Model: The Orchestra Performance</h3>\n<p>Think of our rate limiter as a symphony orchestra performing a complex musical piece. The <strong>HTTP middleware</strong> acts as the concert hall entrance, greeting each audience member (HTTP request) and checking their ticket validity. The <strong>client tracker</strong> serves as the seating coordinator, maintaining a chart of who sits where and managing the flow of patrons to their designated sections. The <strong>token bucket</strong> functions as the conductor&#39;s metronome, precisely timing when each musical phrase (request) can proceed. Finally, the <strong>Redis storage layer</strong> operates like the orchestra&#39;s sheet music stand - a centralized, authoritative source that ensures all musicians (server instances) stay perfectly synchronized, even when individual performers might miss a beat.</p>\n<p>Just as an orchestra&#39;s beautiful performance depends on precise timing, clear communication, and coordinated action between all participants, our rate limiter&#39;s effectiveness relies on seamless interactions between its components. Each component has a specific role, but the magic happens in how they communicate, pass information, and coordinate their actions to create a unified rate limiting experience.</p>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Fsystem-components.svg\" alt=\"Rate Limiter System Components\"></p>\n<p>The complexity of component interactions varies dramatically across our milestones. In Milestone 1, we have a simple conversation between the token bucket algorithm and basic HTTP handling. By Milestone 4, we orchestrate a distributed dance involving Redis Lua scripts, circuit breakers, fallback mechanisms, and cross-server coordination. Understanding this progression helps us appreciate why seemingly simple rate limiting becomes architecturally sophisticated at scale.</p>\n<h2 id=\"request-processing-sequence\">Request Processing Sequence</h2>\n<h3 id=\"single-server-processing-flow\">Single Server Processing Flow</h3>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Frequest-processing-flow.svg\" alt=\"HTTP Request Processing Flow\"></p>\n<p>The journey of an HTTP request through our rate limiting system follows a carefully choreographed sequence designed to minimize latency while ensuring accurate rate limit enforcement. This sequence represents the core interaction pattern that all components must support, regardless of whether we&#39;re operating in single-server or distributed mode.</p>\n<p>The processing begins when an HTTP request arrives at our web server. The <strong>middleware layer</strong> immediately intercepts this request before it reaches any business logic handlers. This early interception is crucial because we want to reject excessive requests as quickly as possible, preserving system resources for legitimate traffic.</p>\n<p><strong>Phase 1: Request Preprocessing and Client Identification</strong></p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Component</th>\n<th>Action Taken</th>\n<th>Data Produced</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>HTTP Middleware</td>\n<td>Extract request metadata (headers, IP, path, method)</td>\n<td>Raw request context dictionary</td>\n</tr>\n<tr>\n<td>2</td>\n<td>HTTP Middleware</td>\n<td>Check for rate limiting bypass headers or whitelisted paths</td>\n<td>Boolean skip flag</td>\n</tr>\n<tr>\n<td>3</td>\n<td>HTTP Middleware</td>\n<td>Normalize endpoint path for consistent rate limit grouping</td>\n<td>Canonical endpoint identifier</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Client Tracker</td>\n<td>Apply client identification strategy (IP, API key, custom header)</td>\n<td><code>ClientIdentifier</code> object</td>\n</tr>\n<tr>\n<td>5</td>\n<td>Client Tracker</td>\n<td>Validate and sanitize client identifier format</td>\n<td>Validated client ID string</td>\n</tr>\n</tbody></table>\n<p>During client identification, our system employs a <strong>hierarchical resolution strategy</strong> to determine the most specific rate limit configuration. The middleware first checks for endpoint-specific overrides, then client-specific overrides, and finally falls back to default global limits. This resolution happens before any token bucket operations, ensuring we apply the correct limits from the start.</p>\n<p>The client identification process deserves special attention because it directly impacts both security and functionality. Our system supports multiple identification strategies simultaneously - a premium API client might be identified by their API key for generous limits, while their IP address gets tracked separately for basic abuse prevention. This dual-tracking approach prevents a single compromised API key from overwhelming our entire system.</p>\n<p><strong>Phase 2: Rate Limit Configuration Resolution</strong></p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Component</th>\n<th>Action Taken</th>\n<th>Data Produced</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>6</td>\n<td>Client Tracker</td>\n<td>Look up endpoint-specific rate limits for the canonical path</td>\n<td>Optional <code>TokenBucketConfig</code></td>\n</tr>\n<tr>\n<td>7</td>\n<td>Client Tracker</td>\n<td>Look up client-specific rate limit overrides</td>\n<td>Optional <code>TokenBucketConfig</code></td>\n</tr>\n<tr>\n<td>8</td>\n<td>Client Tracker</td>\n<td>Apply hierarchical resolution: endpoint → client → default</td>\n<td>Resolved <code>TokenBucketConfig</code></td>\n</tr>\n<tr>\n<td>9</td>\n<td>Client Tracker</td>\n<td>Calculate effective limits considering any active promotions or penalties</td>\n<td>Final <code>TokenBucketConfig</code></td>\n</tr>\n</tbody></table>\n<p>The configuration resolution phase implements sophisticated logic to handle overlapping rate limit rules. Consider a scenario where a premium API client (identified by API key) makes requests to a rate-limited endpoint from a new IP address. Our resolution logic applies the most permissive limits when multiple configurations could apply, while still maintaining separate tracking for abuse detection.</p>\n<p>This approach prevents legitimate users from being unexpectedly blocked while ensuring that abusive patterns get detected quickly. The resolution algorithm considers the specificity hierarchy: custom client overrides take precedence over endpoint defaults, which take precedence over global defaults.</p>\n<p><strong>Phase 3: Token Bucket Operations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Component</th>\n<th>Action Taken</th>\n<th>Data Produced</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>10</td>\n<td>Client Tracker</td>\n<td>Retrieve or create token bucket for client-endpoint combination</td>\n<td><code>BucketInfo</code> containing <code>TokenBucket</code></td>\n</tr>\n<tr>\n<td>11</td>\n<td>Token Bucket</td>\n<td>Calculate tokens to add based on elapsed time since last access</td>\n<td>Integer token refill amount</td>\n</tr>\n<tr>\n<td>12</td>\n<td>Token Bucket</td>\n<td>Update bucket capacity with newly generated tokens (capped at maximum)</td>\n<td>Updated token count</td>\n</tr>\n<tr>\n<td>13</td>\n<td>Token Bucket</td>\n<td>Attempt to consume requested number of tokens (usually 1)</td>\n<td><code>TokenConsumptionResult</code></td>\n</tr>\n<tr>\n<td>14</td>\n<td>Client Tracker</td>\n<td>Update bucket&#39;s last accessed timestamp for cleanup tracking</td>\n<td>Updated <code>BucketInfo</code></td>\n</tr>\n</tbody></table>\n<p>The token bucket operations represent the heart of our rate limiting algorithm. The timing precision here is critical - we calculate token refill based on elapsed time since the bucket&#39;s last access, not since the last request. This distinction matters for burst handling, where a client might make several rapid requests after a period of inactivity.</p>\n<p>Our token calculation uses floating-point arithmetic to handle fractional tokens accurately, but we store only integer token counts. This approach prevents rounding errors from accumulating over time while maintaining precise rate limiting behavior. When a bucket hasn&#39;t been accessed for a long period, we cap the refill at the bucket&#39;s maximum capacity to prevent unbounded token accumulation.</p>\n<p><strong>Phase 4: Response Generation and Cleanup</strong></p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Component</th>\n<th>Action Taken</th>\n<th>Data Produced</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>15</td>\n<td>HTTP Middleware</td>\n<td>Generate appropriate HTTP response based on consumption result</td>\n<td>HTTP response with headers</td>\n</tr>\n<tr>\n<td>16</td>\n<td>HTTP Middleware</td>\n<td>Add rate limiting headers (X-RateLimit-Limit, X-RateLimit-Remaining)</td>\n<td>Enhanced HTTP response</td>\n</tr>\n<tr>\n<td>17</td>\n<td>HTTP Middleware</td>\n<td>For rejected requests, add Retry-After header with backoff time</td>\n<td>HTTP 429 response</td>\n</tr>\n<tr>\n<td>18</td>\n<td>HTTP Middleware</td>\n<td>Log rate limiting decision and metrics for monitoring</td>\n<td>Log entries and metrics</td>\n</tr>\n<tr>\n<td>19</td>\n<td>Client Tracker</td>\n<td>Update access patterns for adaptive rate limiting (future enhancement)</td>\n<td>Updated client statistics</td>\n</tr>\n</tbody></table>\n<p>The response generation phase ensures that clients receive clear, actionable information about their rate limiting status. We include standard rate limiting headers in every response, not just rejections, so clients can proactively manage their request rates. The <code>Retry-After</code> header calculation considers the current token deficit and refill rate to provide accurate timing guidance.</p>\n<h3 id=\"distributed-processing-flow\">Distributed Processing Flow</h3>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Fdistributed-sequence.svg\" alt=\"Distributed Rate Limiting Sequence\"></p>\n<p>When operating in distributed mode across multiple server instances, our component interactions become significantly more complex. The fundamental challenge is maintaining <strong>distributed consistency</strong> while minimizing latency and handling partial failures gracefully.</p>\n<p><strong>Redis-Coordinated Token Operations</strong></p>\n<p>In distributed mode, token bucket state lives in Redis rather than local memory. This centralization ensures that a client&#39;s rate limit applies consistently regardless of which server instance handles their requests. However, it introduces new challenges around atomic operations, network latency, and failure handling.</p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Component</th>\n<th>Action Taken</th>\n<th>Distributed Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1-9</td>\n<td>Same as above</td>\n<td>Client identification and configuration resolution</td>\n<td>Same logic, but config may come from Redis</td>\n</tr>\n<tr>\n<td>10</td>\n<td>Redis Connection Manager</td>\n<td>Establish connection with circuit breaker protection</td>\n<td>Handle connection failures gracefully</td>\n</tr>\n<tr>\n<td>11</td>\n<td>Distributed Token Bucket</td>\n<td>Execute Lua script for atomic token refill and consumption</td>\n<td>Ensure atomicity across read-modify-write</td>\n</tr>\n<tr>\n<td>12</td>\n<td>Redis Storage</td>\n<td>Update token count and last access timestamp atomically</td>\n<td>Prevent race conditions between servers</td>\n</tr>\n<tr>\n<td>13</td>\n<td>Distributed Token Bucket</td>\n<td>Handle Redis failures with local fallback bucket creation</td>\n<td>Maintain service availability during outages</td>\n</tr>\n<tr>\n<td>14</td>\n<td>Circuit Breaker</td>\n<td>Monitor Redis operation success rates and adjust behavior</td>\n<td>Prevent cascading failures</td>\n</tr>\n</tbody></table>\n<p>The Redis Lua script execution in step 11 represents our most critical distributed operation. This script performs several operations atomically: calculating elapsed time, adding refill tokens, consuming requested tokens, updating timestamps, and returning the consumption result. Without this atomicity, race conditions between multiple server instances could lead to incorrect rate limiting decisions.</p>\n<p><strong>Local Fallback Coordination</strong></p>\n<p>When Redis becomes unavailable, our system doesn&#39;t simply fail open or closed. Instead, it gracefully degrades to <strong>local fallback buckets</strong> with more conservative limits. This fallback behavior requires careful coordination to prevent both service disruption and abuse.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Scenario</th>\n<th>Detection Method</th>\n<th>Fallback Action</th>\n<th>Recovery Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis connection timeout</td>\n<td>Connection attempt exceeds configured timeout</td>\n<td>Create local bucket with 50% of normal limits</td>\n<td>Retry Redis every 30 seconds with exponential backoff</td>\n</tr>\n<tr>\n<td>Redis command execution failure</td>\n<td>Command returns error or exception</td>\n<td>Use existing local bucket or create conservative one</td>\n<td>Monitor Redis health and switch back when available</td>\n</tr>\n<tr>\n<td>Redis performance degradation</td>\n<td>Command latency exceeds threshold</td>\n<td>Gradually shift traffic to local buckets</td>\n<td>Reduce Redis load while maintaining partial consistency</td>\n</tr>\n<tr>\n<td>Network partition</td>\n<td>Multiple consecutive timeouts</td>\n<td>Full local fallback mode with reduced limits</td>\n<td>Wait for network recovery and gradual Redis re-integration</td>\n</tr>\n</tbody></table>\n<p>The fallback coordination ensures that even during complete Redis outages, our rate limiting system continues protecting backend services. The conservative limits during fallback mode err on the side of caution - it&#39;s better to occasionally over-limit legitimate users than to allow abuse during infrastructure problems.</p>\n<h2 id=\"inter-component-communication\">Inter-Component Communication</h2>\n<h3 id=\"interface-contracts-and-data-exchange\">Interface Contracts and Data Exchange</h3>\n<p>The communication between our rate limiting components follows well-defined interface contracts that enable loose coupling while ensuring reliable data flow. Each component exposes specific methods and expects particular data formats, creating a clear separation of concerns.</p>\n<p><strong>HTTP Middleware to Client Tracker Communication</strong></p>\n<p>The middleware layer communicates with the client tracker through a request-response pattern, passing rich context about each HTTP request and receiving rate limiting decisions. This communication must be extremely fast since it happens on the critical path of every API request.</p>\n<table>\n<thead>\n<tr>\n<th>Method Call</th>\n<th>Input Parameters</th>\n<th>Return Value</th>\n<th>Error Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>identify_client(request_data)</code></td>\n<td>Request headers, IP address, path</td>\n<td><code>ClientIdentifier</code> object</td>\n<td>Invalid IP format, missing API key</td>\n</tr>\n<tr>\n<td><code>get_bucket_for_client(client_id, endpoint)</code></td>\n<td>Client ID string, optional endpoint</td>\n<td><code>TokenBucket</code> instance</td>\n<td>Configuration lookup failure</td>\n</tr>\n<tr>\n<td><code>resolve_bucket_config(client_id, endpoint)</code></td>\n<td>Client ID, endpoint path</td>\n<td><code>TokenBucketConfig</code></td>\n<td>No matching configuration found</td>\n</tr>\n<tr>\n<td><code>process_request(request_data)</code></td>\n<td>Complete request context</td>\n<td>Rate limit decision dict</td>\n<td>Bucket creation failure, Redis error</td>\n</tr>\n</tbody></table>\n<p>The <code>request_data</code> dictionary contains standardized fields that abstract away web framework differences. This framework-agnostic approach allows our rate limiting core to work with Flask, Django, FastAPI, or any other HTTP framework through simple adapter layers.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>request_data structure:\n{\n    'headers': dict,           # HTTP headers as key-value pairs\n    'remote_addr': str,        # Client IP address\n    'path': str,              # Request path\n    'method': str,            # HTTP method (GET, POST, etc.)\n    'query_params': dict,     # URL query parameters\n    'timestamp': float,       # Request arrival time\n    'user_agent': str,        # Client user agent string\n    'content_length': int     # Request body size\n}</code></pre></div>\n\n<p><strong>Client Tracker to Token Bucket Communication</strong></p>\n<p>The client tracker manages the lifecycle of token bucket instances, creating them on demand, tracking their usage, and cleaning them up when they become stale. This communication pattern follows a factory model where the tracker creates and configures buckets according to resolved rate limit rules.</p>\n<table>\n<thead>\n<tr>\n<th>Method Call</th>\n<th>Purpose</th>\n<th>Data Flow</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TokenBucket.try_consume(tokens)</code></td>\n<td>Attempt to consume tokens for request</td>\n<td>Pass token count, receive consumption result</td>\n<td>Critical path - must be very fast</td>\n</tr>\n<tr>\n<td><code>TokenBucket.get_status()</code></td>\n<td>Check bucket state without consuming</td>\n<td>No parameters, receive current token count</td>\n<td>Used for monitoring and debugging</td>\n</tr>\n<tr>\n<td><code>TokenBucket.refill_tokens()</code></td>\n<td>Add tokens based on elapsed time</td>\n<td>Internal time calculation, update state</td>\n<td>Called automatically before consumption</td>\n</tr>\n<tr>\n<td><code>TokenBucket.reset_bucket()</code></td>\n<td>Reset bucket to initial state</td>\n<td>Clear all state, restore to configuration defaults</td>\n<td>Used for testing and emergency recovery</td>\n</tr>\n</tbody></table>\n<p>The token bucket communication is designed to be stateless from the client tracker&#39;s perspective. The tracker doesn&#39;t need to know about internal bucket timing calculations or token arithmetic - it simply requests consumption and receives a clear allow/deny decision along with remaining capacity information.</p>\n<p><strong>Distributed Storage Communication Patterns</strong></p>\n<p>In distributed mode, our components must coordinate through Redis while handling network failures, timeouts, and consistency challenges. This communication pattern implements the <strong>single source of truth</strong> principle while providing graceful degradation capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Redis Command Pattern</th>\n<th>Fallback Behavior</th>\n<th>Consistency Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token consumption</td>\n<td>Lua script execution with EVAL</td>\n<td>Local bucket with conservative limits</td>\n<td>Eventual consistency with bounded drift</td>\n</tr>\n<tr>\n<td>Bucket status query</td>\n<td>GET with key pattern</td>\n<td>Return cached local state</td>\n<td>Read-your-writes consistency</td>\n</tr>\n<tr>\n<td>Configuration updates</td>\n<td>SET with expiration</td>\n<td>Use last known good config</td>\n<td>Strong consistency for critical settings</td>\n</tr>\n<tr>\n<td>Cleanup operations</td>\n<td>SCAN and DELETE batch</td>\n<td>Local cleanup only</td>\n<td>Best effort with periodic reconciliation</td>\n</tr>\n</tbody></table>\n<p>The Redis communication layer implements sophisticated retry logic with exponential backoff to handle transient network issues. However, we carefully limit retry attempts to prevent request processing delays from accumulating. After a small number of failed retries, we switch to local fallback mode rather than blocking the request pipeline.</p>\n<h3 id=\"message-formats-and-serialization\">Message Formats and Serialization</h3>\n<p><strong>Internal Message Structures</strong></p>\n<p>Our components exchange data through well-defined message structures that balance human readability with serialization efficiency. These structures support both in-memory communication (single server) and network communication (distributed setup).</p>\n<table>\n<thead>\n<tr>\n<th>Message Type</th>\n<th>Purpose</th>\n<th>Serialization Format</th>\n<th>Example Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate Limit Request</td>\n<td>Client requests permission to proceed</td>\n<td>JSON for Redis, direct objects locally</td>\n<td>Middleware to bucket coordination</td>\n</tr>\n<tr>\n<td>Token Consumption Result</td>\n<td>Response to rate limit request</td>\n<td>Structured object with standard fields</td>\n<td>Bucket to middleware response</td>\n</tr>\n<tr>\n<td>Configuration Update</td>\n<td>Changes to rate limiting rules</td>\n<td>JSON with schema validation</td>\n<td>Dynamic configuration reload</td>\n</tr>\n<tr>\n<td>Health Check Status</td>\n<td>Component availability and performance</td>\n<td>Lightweight JSON with timestamps</td>\n<td>Circuit breaker and monitoring</td>\n</tr>\n</tbody></table>\n<p>The <code>TokenConsumptionResult</code> represents our most frequently exchanged message, flowing from token buckets back through client trackers to HTTP middleware. Its design prioritizes both completeness and efficiency:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Data Type</th>\n<th>Purpose</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>allowed</code></td>\n<td>Boolean</td>\n<td>Whether request should be permitted</td>\n<td><code>true</code> or <code>false</code></td>\n</tr>\n<tr>\n<td><code>tokens_remaining</code></td>\n<td>Integer</td>\n<td>Current bucket token count after operation</td>\n<td><code>47</code></td>\n</tr>\n<tr>\n<td><code>retry_after_seconds</code></td>\n<td>Float</td>\n<td>Seconds until next token available</td>\n<td><code>2.5</code></td>\n</tr>\n<tr>\n<td><code>bucket_capacity</code></td>\n<td>Integer</td>\n<td>Maximum tokens this bucket can hold</td>\n<td><code>100</code></td>\n</tr>\n<tr>\n<td><code>refill_rate</code></td>\n<td>Float</td>\n<td>Tokens added per second</td>\n<td><code>10.0</code></td>\n</tr>\n<tr>\n<td><code>client_id</code></td>\n<td>String</td>\n<td>Identifier for debugging and logging</td>\n<td><code>&quot;api_key:abc123&quot;</code></td>\n</tr>\n<tr>\n<td><code>endpoint</code></td>\n<td>String</td>\n<td>Endpoint pattern that matched</td>\n<td><code>&quot;/api/v1/users&quot;</code></td>\n</tr>\n<tr>\n<td><code>consumed_tokens</code></td>\n<td>Integer</td>\n<td>Number of tokens consumed by this request</td>\n<td><code>1</code></td>\n</tr>\n</tbody></table>\n<p><strong>Redis Storage Formats</strong></p>\n<p>When operating in distributed mode, we serialize token bucket state into Redis using carefully designed key patterns and value structures. These formats balance queryability, storage efficiency, and atomic operation requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Redis Key Pattern</th>\n<th>Value Structure</th>\n<th>Purpose</th>\n<th>Expiration Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>rate_limit:bucket:{client_id}:{endpoint}</code></td>\n<td>JSON with token count and timestamp</td>\n<td>Primary bucket state</td>\n<td>Auto-expire after inactivity</td>\n</tr>\n<tr>\n<td><code>rate_limit:config:{pattern}</code></td>\n<td>JSON configuration object</td>\n<td>Per-endpoint rate limits</td>\n<td>Manual expiration on updates</td>\n</tr>\n<tr>\n<td><code>rate_limit:stats:{client_id}:daily</code></td>\n<td>Compressed usage statistics</td>\n<td>Long-term analytics and abuse detection</td>\n<td>Daily rotation</td>\n</tr>\n<tr>\n<td><code>rate_limit:circuit_breaker</code></td>\n<td>Simple counter or timestamp</td>\n<td>Circuit breaker state coordination</td>\n<td>Short TTL for fast recovery</td>\n</tr>\n</tbody></table>\n<p>The bucket state serialization includes metadata that enables accurate distributed coordination:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Redis bucket value structure:\n{\n    &quot;tokens&quot;: 42,                    # Current token count\n    &quot;last_refill&quot;: 1634567890.123,   # Unix timestamp of last token refill\n    &quot;capacity&quot;: 100,                 # Maximum bucket capacity\n    &quot;refill_rate&quot;: 10.0,             # Tokens per second\n    &quot;created_at&quot;: 1634567800.000,    # Bucket creation timestamp\n    &quot;access_count&quot;: 156,             # Total requests processed\n    &quot;last_client_ip&quot;: &quot;192.168.1.1&quot;, # For debugging and analytics\n    &quot;endpoint&quot;: &quot;/api/v1/users&quot;,     # Associated endpoint pattern\n    &quot;config_version&quot;: 3              # Configuration version for consistency\n}</code></pre></div>\n\n<p><strong>Cross-Server Coordination Messages</strong></p>\n<p>In a distributed deployment, our rate limiter instances occasionally need to coordinate beyond simple Redis operations. This coordination handles scenarios like configuration updates, emergency rate limit adjustments, and coordinated cleanup operations.</p>\n<table>\n<thead>\n<tr>\n<th>Coordination Type</th>\n<th>Message Channel</th>\n<th>Message Format</th>\n<th>Delivery Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration reload</td>\n<td>Redis pub/sub</td>\n<td>JSON with versioning</td>\n<td>At-least-once delivery</td>\n</tr>\n<tr>\n<td>Emergency rate limit</td>\n<td>Redis shared key</td>\n<td>Simple flag with expiration</td>\n<td>Immediate consistency</td>\n</tr>\n<tr>\n<td>Cleanup coordination</td>\n<td>Redis sorted set</td>\n<td>Timestamped work items</td>\n<td>Exactly-once processing</td>\n</tr>\n<tr>\n<td>Health monitoring</td>\n<td>Redis heartbeat key</td>\n<td>Instance status with TTL</td>\n<td>Best effort delivery</td>\n</tr>\n</tbody></table>\n<p>The pub/sub coordination for configuration updates includes versioning and idempotency protection to ensure that rapid-fire configuration changes don&#39;t create inconsistent states across the server fleet.</p>\n<h2 id=\"message-and-data-formats\">Message and Data Formats</h2>\n<h3 id=\"request-processing-data-structures\">Request Processing Data Structures</h3>\n<p><strong>Standardized Request Context</strong></p>\n<p>Our rate limiting system processes requests through a <strong>framework-agnostic core</strong> that normalizes data from different web frameworks into a consistent internal format. This normalization happens at the middleware boundary and flows through all subsequent processing.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Data Type</th>\n<th>Source</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>client_ip</code></td>\n<td>String</td>\n<td>HTTP headers or connection info</td>\n<td>Valid IPv4/IPv6 format, not in reserved ranges</td>\n</tr>\n<tr>\n<td><code>api_key</code></td>\n<td>Optional String</td>\n<td>Authorization header or custom header</td>\n<td>Alphanumeric, 32-64 characters, not expired</td>\n</tr>\n<tr>\n<td><code>endpoint_path</code></td>\n<td>String</td>\n<td>Request URL path</td>\n<td>Normalized to pattern (remove IDs, etc.)</td>\n</tr>\n<tr>\n<td><code>http_method</code></td>\n<td>String</td>\n<td>HTTP verb</td>\n<td>Must be in allowed methods list</td>\n</tr>\n<tr>\n<td><code>user_agent</code></td>\n<td>String</td>\n<td>User-Agent header</td>\n<td>Length limit, basic format validation</td>\n</tr>\n<tr>\n<td><code>request_size</code></td>\n<td>Integer</td>\n<td>Content-Length header</td>\n<td>Within configured limits</td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td>Float</td>\n<td>Server processing time</td>\n<td>Unix timestamp with millisecond precision</td>\n</tr>\n<tr>\n<td><code>custom_headers</code></td>\n<td>Dict</td>\n<td>Configurable header names</td>\n<td>Key-value pairs for custom identification</td>\n</tr>\n</tbody></table>\n<p>The request context normalization performs several important transformations that improve rate limiting accuracy. IP address normalization handles both IPv4 and IPv6 addresses consistently, while endpoint path normalization groups similar requests together (e.g., <code>/users/123</code> and <code>/users/456</code> both become <code>/users/{id}</code>).</p>\n<p><strong>Token Bucket State Representation</strong></p>\n<p>The token bucket state representation must support both in-memory operations and distributed serialization while maintaining precision in token calculations and timing operations.</p>\n<table>\n<thead>\n<tr>\n<th>State Component</th>\n<th>Storage Type</th>\n<th>Precision Requirements</th>\n<th>Synchronization Needs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Current token count</td>\n<td>Integer</td>\n<td>Exact integer arithmetic</td>\n<td>Atomic updates in distributed mode</td>\n</tr>\n<tr>\n<td>Last refill timestamp</td>\n<td>Float</td>\n<td>Millisecond precision</td>\n<td>Consistent across all servers</td>\n</tr>\n<tr>\n<td>Bucket capacity</td>\n<td>Integer</td>\n<td>Configuration-driven constant</td>\n<td>Read-only after creation</td>\n</tr>\n<tr>\n<td>Refill rate</td>\n<td>Float</td>\n<td>Precise decimal arithmetic</td>\n<td>Read-only after creation</td>\n</tr>\n<tr>\n<td>Access metadata</td>\n<td>Object</td>\n<td>Various types for monitoring</td>\n<td>Eventually consistent</td>\n</tr>\n</tbody></table>\n<p>The state representation uses integer arithmetic for token counts to avoid floating-point precision issues that could accumulate over time. However, we use floating-point calculations for time-based refill operations, then truncate to integers for actual token storage.</p>\n<p><strong>Configuration Data Structures</strong></p>\n<p>Rate limiting configurations follow a hierarchical structure that supports inheritance, overrides, and dynamic updates without requiring application restarts.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Level</th>\n<th>Priority</th>\n<th>Override Scope</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global defaults</td>\n<td>Lowest</td>\n<td>All clients and endpoints</td>\n<td>Rarely - deployment changes</td>\n</tr>\n<tr>\n<td>Endpoint-specific</td>\n<td>Medium</td>\n<td>All clients for specific endpoints</td>\n<td>Occasionally - feature releases</td>\n</tr>\n<tr>\n<td>Client-specific</td>\n<td>High</td>\n<td>Specific client across all endpoints</td>\n<td>Regularly - account upgrades</td>\n</tr>\n<tr>\n<td>Emergency overrides</td>\n<td>Highest</td>\n<td>Temporary restrictions during incidents</td>\n<td>Emergency only</td>\n</tr>\n</tbody></table>\n<p>The configuration inheritance system allows for sophisticated rate limiting policies. A premium client might have generous global limits, but still be subject to endpoint-specific restrictions on expensive operations like large file uploads or complex database queries.</p>\n<h3 id=\"response-message-formats\">Response Message Formats</h3>\n<p><strong>Success Response Enhancement</strong></p>\n<p>When a request passes rate limiting, our middleware enhances the response with informational headers that help clients manage their request patterns proactively. This information enables <strong>adaptive backoff strategies</strong> in client applications.</p>\n<table>\n<thead>\n<tr>\n<th>Header Name</th>\n<th>Value Format</th>\n<th>Purpose</th>\n<th>Client Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>X-RateLimit-Limit</code></td>\n<td>Integer (requests per window)</td>\n<td>Maximum requests allowed</td>\n<td>Display to users, plan request batching</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Remaining</code></td>\n<td>Integer (remaining requests)</td>\n<td>Requests remaining in current window</td>\n<td>Decide whether to make additional requests</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Reset</code></td>\n<td>Unix timestamp</td>\n<td>When the rate limit window resets</td>\n<td>Schedule delayed requests</td>\n</tr>\n<tr>\n<td><code>X-RateLimit-Policy</code></td>\n<td>String description</td>\n<td>Human-readable rate limit description</td>\n<td>Display policy to API consumers</td>\n</tr>\n</tbody></table>\n<p>These headers follow industry standards established by GitHub, Twitter, and other major APIs, ensuring that existing client libraries can automatically handle our rate limiting without custom code.</p>\n<p><strong>Rate Limit Exceeded Response</strong></p>\n<p>When we reject a request due to rate limiting, the response format provides clear guidance on when the client can retry and why the request was rejected.</p>\n<table>\n<thead>\n<tr>\n<th>Response Component</th>\n<th>Format</th>\n<th>Purpose</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Status Code</td>\n<td>429</td>\n<td>Standard &quot;Too Many Requests&quot; indicator</td>\n<td><code>429</code></td>\n</tr>\n<tr>\n<td>Retry-After</td>\n<td>Integer seconds</td>\n<td>Minimum wait time before retry</td>\n<td><code>30</code></td>\n</tr>\n<tr>\n<td>Response Body</td>\n<td>JSON object</td>\n<td>Detailed error information</td>\n<td>See structure below</td>\n</tr>\n<tr>\n<td>Rate Limit Headers</td>\n<td>Same as success case</td>\n<td>Current status information</td>\n<td>Same headers as above</td>\n</tr>\n</tbody></table>\n<p>The response body follows a structured format that supports both human debugging and programmatic error handling:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Rate limit exceeded response body:\n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;rate_limit_exceeded&quot;,\n        &quot;message&quot;: &quot;Rate limit exceeded for this API key&quot;,\n        &quot;details&quot;: {\n            &quot;limit&quot;: 100,\n            &quot;window_seconds&quot;: 3600,\n            &quot;retry_after&quot;: 30,\n            &quot;policy_name&quot;: &quot;premium_api_key&quot;,\n            &quot;endpoint&quot;: &quot;/api/v1/users&quot;\n        }\n    },\n    &quot;meta&quot;: {\n        &quot;timestamp&quot;: &quot;2023-10-15T14:30:45.123Z&quot;,\n        &quot;request_id&quot;: &quot;req_abc123def456&quot;\n    }\n}</code></pre></div>\n\n<p>This structured response format enables client applications to implement sophisticated retry logic, display meaningful error messages to users, and collect metrics about rate limiting behavior.</p>\n<h3 id=\"storage-and-persistence-formats\">Storage and Persistence Formats</h3>\n<p><strong>Redis Key Design Patterns</strong></p>\n<p>Our Redis key design follows consistent patterns that support efficient querying, automatic expiration, and operational debugging. The key structure balances readability with storage efficiency and query performance.</p>\n<table>\n<thead>\n<tr>\n<th>Key Pattern</th>\n<th>Purpose</th>\n<th>Example</th>\n<th>Expiration Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>rl:b:{client}:{endpoint}</code></td>\n<td>Token bucket state</td>\n<td><code>rl:b:ip_192.168.1.1:/api/users</code></td>\n<td>Idle timeout (1 hour)</td>\n</tr>\n<tr>\n<td><code>rl:c:{pattern}</code></td>\n<td>Configuration rules</td>\n<td><code>rl:c:endpoint:/api/upload</code></td>\n<td>Manual management</td>\n</tr>\n<tr>\n<td><code>rl:s:{client}:{date}</code></td>\n<td>Usage statistics</td>\n<td><code>rl:s:key_abc123:2023-10-15</code></td>\n<td>Retention policy (30 days)</td>\n</tr>\n<tr>\n<td><code>rl:m:{server}:{timestamp}</code></td>\n<td>Monitoring heartbeats</td>\n<td><code>rl:m:web-01:1634567890</code></td>\n<td>Short TTL (60 seconds)</td>\n</tr>\n</tbody></table>\n<p>The abbreviated key prefixes (<code>rl:b</code>, <code>rl:c</code>, etc.) minimize Redis memory usage while maintaining human readability for debugging. The hierarchical structure supports efficient pattern matching and bulk operations during maintenance tasks.</p>\n<p><strong>Atomic Operation Scripts</strong></p>\n<p>Our most critical distributed operations use Redis Lua scripts to ensure atomicity. These scripts handle the complex logic of token refill calculations, consumption checks, and state updates in a single atomic operation.</p>\n<table>\n<thead>\n<tr>\n<th>Script Purpose</th>\n<th>Input Parameters</th>\n<th>Return Value</th>\n<th>Error Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token consumption</td>\n<td>Client ID, endpoint, tokens requested, current time</td>\n<td>Consumption result object</td>\n<td>Invalid parameters, script error</td>\n</tr>\n<tr>\n<td>Bucket creation</td>\n<td>Client ID, endpoint, configuration</td>\n<td>New bucket state</td>\n<td>Configuration lookup failure</td>\n</tr>\n<tr>\n<td>Bulk cleanup</td>\n<td>Cleanup cutoff timestamp, batch size</td>\n<td>Number of buckets removed</td>\n<td>Redis operation timeout</td>\n</tr>\n<tr>\n<td>Health check</td>\n<td>Server instance ID, status data</td>\n<td>Updated health status</td>\n<td>Network partition scenarios</td>\n</tr>\n</tbody></table>\n<p>The token consumption script represents our most performance-critical operation, executed potentially thousands of times per second across our entire distributed fleet:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Token consumption Lua script logic:\n1. Retrieve current bucket state from Redis key\n2. Calculate elapsed time since last refill\n3. Compute new tokens to add based on refill rate\n4. Apply capacity limits to prevent overflow\n5. Check if requested tokens are available\n6. If available: deduct tokens and update state\n7. If not available: calculate retry-after time\n8. Update last access timestamp\n9. Return consumption result with all metadata</code></pre></div>\n\n<p>This script encapsulates all the timing calculations and state management that would otherwise require multiple Redis round trips, significantly improving both performance and consistency.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: The progression from simple in-memory data structures in early milestones to sophisticated distributed message formats in later milestones teaches learners how system complexity grows organically. Each new requirement (client tracking, HTTP integration, distribution) introduces new data format needs while building on existing structures.</p>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Inconsistent Data Formats Across Components</strong></p>\n<p>Many learners create slightly different data structures for similar purposes across components, leading to constant conversion overhead and bugs. For example, representing client IDs as strings in one component but objects in another, or using different timestamp formats (Unix seconds vs. milliseconds) across the system.</p>\n<p><strong>Why it&#39;s wrong</strong>: Inconsistent formats require conversion logic at every component boundary, creating performance overhead and opportunities for bugs. Time format inconsistencies are particularly dangerous because they can cause rate limiting to behave incorrectly without obvious error symptoms.</p>\n<p><strong>How to fix it</strong>: Define canonical data formats once in a central location and use exactly the same structures everywhere. Create factory functions or constructors that ensure consistent formatting, especially for timestamps and client identifiers.</p>\n<p>⚠️ <strong>Pitfall: Missing Error Context in Inter-Component Communication</strong></p>\n<p>Learners often design component interfaces that return only success/failure indicators without providing sufficient context for error handling or debugging. This leads to generic error messages and difficulty troubleshooting rate limiting issues.</p>\n<p><strong>Why it&#39;s wrong</strong>: Without rich error context, the middleware layer cannot provide meaningful feedback to clients or operators. Error messages like &quot;rate limit failed&quot; don&#39;t help users understand whether they should retry immediately, wait, or check their configuration.</p>\n<p><strong>How to fix it</strong>: Every error result should include specific error codes, human-readable messages, and actionable next steps. Design error types that carry enough context to generate appropriate HTTP responses and log entries.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Clock Synchronization in Distributed Messages</strong></p>\n<p>In distributed setups, learners often assume all servers have perfectly synchronized clocks, leading to inconsistent token refill calculations and unfair rate limiting when servers have clock drift.</p>\n<p><strong>Why it&#39;s wrong</strong>: Even small clock differences (seconds) can cause dramatic rate limiting inconsistencies. A client might get different effective rate limits depending on which server handles their requests, leading to unpredictable behavior and user complaints.</p>\n<p><strong>How to fix it</strong>: Use relative timing measurements within individual operations, and include authoritative timestamps from Redis or another centralized time source in distributed messages. Design algorithms that gracefully handle small clock differences.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Our implementation of component interactions requires careful attention to both performance and maintainability. The following guidance shows how to structure the communication pathways and data flow in Python.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component Communication</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Request Context</td>\n<td>Simple dictionary with validation functions</td>\n<td>Pydantic models with automatic serialization</td>\n</tr>\n<tr>\n<td>Inter-Component Messaging</td>\n<td>Direct method calls with type hints</td>\n<td>Message queue system (Redis Streams)</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>JSON files with environment variable overrides</td>\n<td>Consul/etcd with dynamic reload</td>\n</tr>\n<tr>\n<td>Redis Communication</td>\n<td>redis-py with connection pooling</td>\n<td>redis-py-cluster with automatic failover</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception-based with custom error types</td>\n<td>Result types with explicit error handling</td>\n</tr>\n</tbody></table>\n<p><strong>File Structure for Component Integration</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>rate_limiter/\n├── core/\n│   ├── __init__.py\n│   ├── interfaces.py          ← Abstract base classes for all components\n│   ├── data_models.py         ← Canonical data structures\n│   └── exceptions.py          ← Custom exception types\n├── components/\n│   ├── __init__.py\n│   ├── middleware.py          ← HTTP middleware implementation\n│   ├── client_tracker.py      ← Client identification and bucket management\n│   ├── token_bucket.py        ← Token bucket algorithm\n│   └── storage.py            ← Storage abstraction layer\n├── distributed/\n│   ├── __init__.py\n│   ├── redis_storage.py       ← Redis-backed storage implementation\n│   ├── lua_scripts.py         ← Atomic operation scripts\n│   └── circuit_breaker.py     ← Failure handling\n└── integration/\n    ├── __init__.py\n    ├── flask_integration.py   ← Flask-specific middleware\n    └── fastapi_integration.py ← FastAPI-specific middleware</code></pre></div>\n\n<p><strong>Core Data Structures Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># core/data_models.py - Complete canonical data structures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional, Any, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClientIdentifier</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Normalized client identification across all components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_value: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    identifier_type: </span><span style=\"color:#9ECBFF\">'IdentifierType'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    namespace: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"default\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_key</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate Redis key or internal identifier.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Combine namespace, type, and value into consistent key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply URL-safe encoding for special characters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure key length stays within Redis limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenConsumptionResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Standard response format for all rate limiting operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowed: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens_remaining: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_after_seconds: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket_capacity: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refill_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    consumed_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_http_headers</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert to standard HTTP rate limiting headers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate X-RateLimit-* headers from fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Format Retry-After header correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Include policy information for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RequestContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Framework-agnostic request information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_ip: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    endpoint_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    http_method: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    api_key: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    user_agent: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    custom_headers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    request_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_flask_request</span><span style=\"color:#E1E4E8\">(cls, request) -> </span><span style=\"color:#9ECBFF\">'RequestContext'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract context from Flask request object.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract IP address, handling X-Forwarded-For</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Normalize endpoint path (remove IDs, query params)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract API key from Authorization header</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate and sanitize all extracted data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Component Communication Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># core/interfaces.py - Abstract interfaces for loose coupling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract storage interface for token bucket state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get_bucket_state</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve current bucket state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> update_bucket_state</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                state: Dict) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically update bucket state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> cleanup_stale_buckets</span><span style=\"color:#E1E4E8\">(self, max_age_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove inactive buckets.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClientIdentificationStrategy</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract strategy for identifying API clients.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extract_client_id</span><span style=\"color:#E1E4E8\">(self, request_context: RequestContext) -> ClientIdentifier:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract client identifier from request.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_client_id</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate client identifier format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># components/rate_limit_coordinator.py - Main coordination logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates all rate limiting components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, storage: RateLimitStorage, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 id_strategy: ClientIdentificationStrategy):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.id_strategy </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> id_strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize other components (client tracker, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> process_request</span><span style=\"color:#E1E4E8\">(self, request_context: RequestContext) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main request processing pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract and validate client identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Resolve rate limiting configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Get or create appropriate token bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt token consumption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update access tracking and metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return detailed consumption result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> should_bypass_rate_limiting</span><span style=\"color:#E1E4E8\">(self, request_context: RequestContext) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if request should skip rate limiting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for bypass headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify whitelist patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply emergency override flags</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Redis Communication Patterns</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># distributed/redis_storage.py - Redis-backed storage implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis.asyncio </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisRateLimitStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RateLimitStorage</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Redis implementation with atomic operations and fallback.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, circuit_breaker):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.ConnectionPool.from_url(redis_url)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.circuit_breaker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> circuit_breaker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load Lua scripts for atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> atomic_consume_tokens</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute atomic token consumption using Lua script.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Prepare script parameters (client_id, endpoint, tokens, time)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute Lua script with circuit breaker protection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse script response into TokenConsumptionResult</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle Redis errors with local fallback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update circuit breaker state based on result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> batch_cleanup_stale_buckets</span><span style=\"color:#E1E4E8\">(self, batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clean up inactive buckets in batches.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use SCAN to find candidate bucket keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check last access time for each bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Delete stale buckets in batches using pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Track cleanup statistics for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># distributed/lua_scripts.py - Atomic operation scripts</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TOKEN_BUCKET_CONSUME_SCRIPT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- Atomic token bucket consumption script</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- KEYS[1]: bucket key</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- ARGV[1]: tokens requested</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- ARGV[2]: current timestamp</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- ARGV[3]: bucket configuration JSON</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- TODO 1: Parse existing bucket state from Redis</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- TODO 2: Calculate token refill based on elapsed time</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- TODO 3: Apply capacity limits and consume requested tokens</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- TODO 4: Update bucket state atomically</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- TODO 5: Return consumption result as JSON</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local bucket_key = KEYS[1]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local tokens_requested = tonumber(ARGV[1])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local current_time = tonumber(ARGV[2])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">local config = cjson.decode(ARGV[3])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">-- Implementation will be filled in by learner</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">return cjson.encode({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    allowed = false,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    tokens_remaining = 0,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    retry_after_seconds = 60</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">})</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints</strong></p>\n<p>After implementing component interactions:</p>\n<ol>\n<li><p><strong>Basic Integration Test</strong>: Create a simple HTTP server that uses your rate limiter middleware. Send requests and verify that rate limiting headers appear in responses.</p>\n</li>\n<li><p><strong>Component Isolation Test</strong>: Test each component independently. Mock the storage layer and verify that client tracking works correctly. Mock the client tracker and test token bucket operations.</p>\n</li>\n<li><p><strong>Distributed Coordination Test</strong>: If implementing Redis storage, test with multiple Python processes hitting the same Redis instance. Verify that rate limits apply consistently across processes.</p>\n</li>\n<li><p><strong>Error Handling Test</strong>: Simulate Redis failures, network timeouts, and malformed requests. Verify that the system fails gracefully and provides meaningful error messages.</p>\n</li>\n</ol>\n<p>Commands to run:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test basic middleware integration</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_integration.py::test_middleware_headers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test component communication in isolation</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_components.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test distributed coordination (requires Redis)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_distributed.py::test_multi_process_consistency</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Load test with multiple workers</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> tests/load_test.py</span><span style=\"color:#79B8FF\"> --workers</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#79B8FF\"> --requests</span><span style=\"color:#79B8FF\"> 1000</span></span></code></pre></div>\n\n<p>Expected behavior: Requests should be rate limited consistently, HTTP headers should appear on all responses, and system should continue operating during simulated failures.</p>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - robust error handling is essential from the basic token bucket through distributed rate limiting, with complexity increasing as the system evolves</p>\n</blockquote>\n<p>Think of error handling in a rate limiter like designing a city&#39;s emergency response system. Just as a city must prepare for natural disasters, power outages, traffic accidents, and infrastructure failures, our rate limiter must gracefully handle Redis outages, clock synchronization issues, network partitions, and resource exhaustion. The key insight is that <strong>rate limiting systems are safety mechanisms themselves</strong> - when they fail, they can either fail open (allowing all traffic and potentially overwhelming backend services) or fail closed (blocking all traffic and creating a service outage). Our design philosophy prioritizes <strong>graceful degradation</strong> over complete failure, allowing the system to continue protecting services even when operating under suboptimal conditions.</p>\n<p>The challenge in rate limiting error handling is that failures often compound. A Redis outage doesn&#39;t just affect token storage - it triggers fallback mechanisms that consume more memory, increases CPU usage for local bucket management, and creates thundering herd problems when Redis recovers. Our error handling strategy must account for these cascading effects and provide multiple layers of protection.</p>\n<h3 id=\"system-failure-modes\">System Failure Modes</h3>\n<p>Understanding how each component can fail and how those failures propagate through the system is crucial for building robust error handling mechanisms. The rate limiter&#39;s distributed architecture creates multiple potential points of failure, each requiring specific detection and recovery strategies.</p>\n<p><img src=\"/api/project/rate-limiter/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Handling and Recovery Flow\"></p>\n<h4 id=\"token-bucket-component-failures\">Token Bucket Component Failures</h4>\n<p>The <code>TokenBucket</code> component, being the core algorithm implementation, faces several critical failure modes that can compromise rate limiting accuracy. These failures often stem from timing precision, numerical overflow, and concurrency issues.</p>\n<p><strong>Clock-Related Failures</strong> represent one of the most subtle but dangerous failure modes in token bucket systems. When system clocks jump backward due to NTP corrections or manual adjustments, the token refill calculation can produce negative time deltas, potentially causing integer underflows or freezing token generation entirely. This creates scenarios where legitimate clients are indefinitely blocked despite being within their rate limits.</p>\n<p>The detection strategy involves comparing the current timestamp against the bucket&#39;s last refill timestamp. If the current time is significantly earlier than the last refill time (beyond expected clock precision variance), the system should treat this as a clock jump event. The recovery mechanism involves resetting the bucket&#39;s last refill timestamp to the current time and applying a conservative token refill based on the configured rate, preventing both token starvation and excessive token accumulation.</p>\n<p><strong>Numerical Overflow Conditions</strong> occur when token calculations exceed the maximum values for integer or floating-point types. This is particularly problematic for long-running systems where accumulated time values or high refill rates can cause arithmetic operations to overflow. The failure manifests as incorrect token counts, either extremely large values that effectively disable rate limiting or negative values that block all requests.</p>\n<p>Detection requires boundary checking before arithmetic operations, particularly when multiplying elapsed time by refill rates or adding tokens to existing bucket counts. The recovery strategy involves clamping values to safe ranges - time deltas to reasonable maximums (preventing calculations over extended periods) and token counts to the bucket&#39;s configured capacity. This ensures mathematical operations remain within safe bounds while maintaining rate limiting effectiveness.</p>\n<p><strong>Thread Safety Violations</strong> in concurrent environments can lead to race conditions where multiple threads simultaneously modify bucket state, resulting in inconsistent token counts. These failures are particularly insidious because they may not cause immediate failures but gradually corrupt bucket state over time, leading to unpredictable rate limiting behavior.</p>\n<p>The following table outlines the primary token bucket failure modes and their handling strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>Prevention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Clock jump backward</td>\n<td>Current time &lt; last_refill_time - tolerance</td>\n<td>Reset last_refill_time to current time, conservative refill</td>\n<td>Use monotonic clocks where available</td>\n</tr>\n<tr>\n<td>Clock jump forward</td>\n<td>Current time &gt; last_refill_time + max_reasonable_delta</td>\n<td>Cap elapsed time to maximum reasonable value</td>\n<td>Validate time deltas before calculations</td>\n</tr>\n<tr>\n<td>Token count overflow</td>\n<td>Token calculation exceeds bucket capacity significantly</td>\n<td>Clamp tokens to bucket capacity</td>\n<td>Check bounds before arithmetic operations</td>\n</tr>\n<tr>\n<td>Negative token count</td>\n<td>Token count becomes negative after consumption</td>\n<td>Reset to zero, log incident</td>\n<td>Validate consumption amounts</td>\n</tr>\n<tr>\n<td>Concurrent modification</td>\n<td>Inconsistent state after operations</td>\n<td>Re-acquire lock and retry operation</td>\n<td>Use appropriate synchronization primitives</td>\n</tr>\n<tr>\n<td>Infinite refill rate</td>\n<td>Configuration error with extremely high rates</td>\n<td>Apply maximum rate limit from configuration</td>\n<td>Validate configuration on load</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Token bucket failures should never fail open (allowing unlimited requests) as this defeats the primary purpose of rate limiting. When in doubt, the system should fail conservatively by denying requests until it can establish a known-good state.</p>\n</blockquote>\n<h4 id=\"client-tracking-component-failures\">Client Tracking Component Failures</h4>\n<p>The <code>ClientBucketTracker</code> manages potentially thousands of per-client buckets, making it vulnerable to memory exhaustion, cleanup failures, and client enumeration attacks. These failures can cause memory leaks, service degradation, or complete system unavailability.</p>\n<p><strong>Memory Exhaustion</strong> occurs when the number of tracked clients exceeds available memory, either through legitimate traffic growth or malicious attacks creating many unique client identifiers. The system must detect memory pressure before reaching critical levels and implement defensive measures to maintain functionality.</p>\n<p>Detection involves monitoring both the number of tracked buckets and overall memory usage. When bucket counts exceed configured thresholds or memory usage crosses warning levels, the system should trigger emergency cleanup procedures. The recovery strategy includes aggressive LRU eviction of inactive buckets, temporarily reducing bucket retention times, and potentially implementing more restrictive rate limits to prevent new bucket creation during memory pressure events.</p>\n<p><strong>Cleanup Process Failures</strong> happen when the background process responsible for removing stale buckets encounters errors, stops running, or falls behind the rate of new bucket creation. This leads to gradual memory leaks and eventual system failure.</p>\n<p>The detection mechanism involves tracking cleanup process health through heartbeat timestamps and monitoring the ratio of cleanup rate to bucket creation rate. When cleanup falls behind or stops entirely, the system should alert operators and potentially trigger manual cleanup operations. Recovery includes restarting failed cleanup processes, performing emergency bulk cleanup operations, and temporarily halting new bucket creation until cleanup catches up.</p>\n<p><strong>Client Enumeration Attacks</strong> involve malicious actors rapidly generating requests with different client identifiers to force the creation of many buckets, exhausting system memory. Traditional cleanup mechanisms are insufficient because the attack continuously creates new buckets faster than cleanup can remove old ones.</p>\n<p>Detection relies on monitoring bucket creation rates and identifying patterns of rapidly changing client identifiers from similar sources. The recovery strategy includes implementing rate limits on new bucket creation, requiring client identifier validation, and potentially blacklisting source IP addresses that exhibit enumeration attack patterns.</p>\n<p>The client tracking failure modes and responses are summarized below:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Symptoms</th>\n<th>Detection Threshold</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory exhaustion</td>\n<td>High memory usage, slow responses</td>\n<td>Memory &gt; 80% or bucket count &gt; configured limit</td>\n<td>Aggressive LRU eviction, reduced retention</td>\n</tr>\n<tr>\n<td>Cleanup process failure</td>\n<td>Growing bucket count, memory leaks</td>\n<td>Cleanup heartbeat &gt; 2x interval</td>\n<td>Restart cleanup, emergency bulk removal</td>\n</tr>\n<tr>\n<td>Client enumeration attack</td>\n<td>Rapid bucket creation, memory pressure</td>\n<td>Bucket creation rate &gt; 100x normal</td>\n<td>Rate limit new buckets, source IP analysis</td>\n</tr>\n<tr>\n<td>Bucket corruption</td>\n<td>Inconsistent bucket state</td>\n<td>State validation failures</td>\n<td>Remove corrupted bucket, create new one</td>\n</tr>\n<tr>\n<td>Lock contention</td>\n<td>High CPU usage, slow bucket operations</td>\n<td>Lock wait times &gt; threshold</td>\n<td>Consider lock-free data structures</td>\n</tr>\n</tbody></table>\n<h4 id=\"http-middleware-failures\">HTTP Middleware Failures</h4>\n<p>The HTTP middleware layer faces failures related to request processing, response generation, and integration with web frameworks. These failures can cause requests to be processed without rate limiting, incorrect HTTP status codes to be returned, or complete request processing failures.</p>\n<p><strong>Request Context Extraction Failures</strong> occur when the middleware cannot properly extract client identification information, endpoint details, or other required metadata from HTTP requests. This can result from malformed headers, missing required fields, or framework compatibility issues.</p>\n<p>Detection involves validating extracted request context and identifying when required fields are missing or invalid. The recovery strategy depends on the type of failure - for missing client identifiers, the system can fall back to IP-based identification, while for missing endpoint information, it can apply default rate limits. The key is ensuring that extraction failures don&#39;t bypass rate limiting entirely.</p>\n<p><strong>Response Generation Failures</strong> happen when the middleware encounters errors while building HTTP responses, particularly 429 Too Many Requests responses or adding rate limiting headers to successful responses. These failures can result in clients receiving confusing error messages or missing critical rate limiting information.</p>\n<p>Recovery involves implementing fallback response generation that uses minimal, guaranteed-safe response formats. Even if sophisticated response formatting fails, the middleware should still return proper HTTP status codes and basic headers to communicate rate limiting status to clients.</p>\n<h4 id=\"distributed-storage-failures\">Distributed Storage Failures</h4>\n<p>Redis-based distributed rate limiting introduces additional failure modes related to network connectivity, Redis server availability, data consistency, and distributed coordination. These failures are among the most complex to handle because they affect multiple server instances simultaneously.</p>\n<p><strong>Redis Connection Failures</strong> are the most common distributed storage failure, occurring due to network issues, Redis server crashes, or configuration problems. The impact affects all server instances attempting to coordinate rate limiting state through Redis.</p>\n<p>Detection uses connection health checks, operation timeouts, and error pattern analysis. The circuit breaker pattern provides automatic failure detection and recovery coordination. When Redis connections fail, the system must immediately switch to local fallback buckets while attempting background reconnection.</p>\n<p><strong>Redis Data Corruption or Inconsistency</strong> can occur due to Redis server issues, network partitions during write operations, or clock synchronization problems between server instances updating the same buckets. This results in incorrect token counts that don&#39;t reflect actual request patterns.</p>\n<p>Detection requires implementing data validation on Redis operations, comparing expected versus actual values, and monitoring for impossible state transitions (such as token counts exceeding bucket capacities). Recovery involves invalidating corrupted data, resetting affected buckets to known-good states, and potentially forcing a brief period of local fallback operation while consistency is restored.</p>\n<p><strong>Distributed Coordination Failures</strong> happen when multiple server instances make conflicting decisions about rate limiting due to network partitions, clock skew, or race conditions in Redis operations. This can result in either overly permissive rate limiting (allowing more requests than configured limits) or overly restrictive limiting (blocking legitimate requests).</p>\n<p>The following table details distributed storage failure modes:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Method</th>\n<th>Immediate Response</th>\n<th>Long-term Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis connection timeout</td>\n<td>Connection attempts fail within timeout</td>\n<td>Switch to local fallback buckets</td>\n<td>Background reconnection attempts</td>\n</tr>\n<tr>\n<td>Redis server crash</td>\n<td>All Redis operations fail with connection errors</td>\n<td>Circuit breaker opens, local fallback</td>\n<td>Monitor Redis health, reconnect when available</td>\n</tr>\n<tr>\n<td>Network partition</td>\n<td>Intermittent Redis failures, high latency</td>\n<td>Graceful degradation, local operation</td>\n<td>Wait for network recovery, validate state</td>\n</tr>\n<tr>\n<td>Data corruption</td>\n<td>Impossible bucket states, validation failures</td>\n<td>Invalidate corrupted buckets</td>\n<td>Reset affected client buckets</td>\n</tr>\n<tr>\n<td>Clock skew</td>\n<td>Token counts inconsistent across instances</td>\n<td>Use server-local timestamps</td>\n<td>NTP synchronization, clock drift monitoring</td>\n</tr>\n<tr>\n<td>Thundering herd on recovery</td>\n<td>All instances reconnect simultaneously</td>\n<td>Randomized reconnection delays</td>\n<td>Gradual state resynchronization</td>\n</tr>\n</tbody></table>\n<h3 id=\"edge-cases-and-corner-conditions\">Edge Cases and Corner Conditions</h3>\n<p>Edge cases represent unusual but possible scenarios that can cause unexpected system behavior if not properly handled. Unlike failure modes, edge cases often involve the system operating within normal parameters but encountering unusual combinations of conditions that expose design assumptions or boundary conditions.</p>\n<h4 id=\"clock-synchronization-edge-cases\">Clock Synchronization Edge Cases</h4>\n<p><strong>Clock Drift Between Distributed Instances</strong> occurs when server instances have slightly different system times, causing inconsistent token generation rates across the distributed system. Even small clock differences (seconds or minutes) can accumulate over time to create noticeable rate limiting inconsistencies.</p>\n<p>The challenge is that each server instance calculates token refill based on its local clock, but all instances share the same Redis-stored bucket state. If Server A&#39;s clock runs fast and Server B&#39;s clock runs slow, clients may experience different effective rate limits depending on which server processes their requests. This creates an unfair and unpredictable rate limiting experience.</p>\n<p>The solution involves using a consistent time source for all token calculations in distributed scenarios. Instead of relying on local server time, the system should either use Redis server time as the authoritative source or implement clock offset correction based on periodic synchronization with a central time authority. The trade-off is additional complexity and potential slight performance impact versus rate limiting consistency.</p>\n<p><strong>Daylight Saving Time Transitions</strong> create scenarios where clocks jump forward or backward by exactly one hour, potentially causing massive token bucket refills or extended blocking periods. Spring transitions (clocks jump forward) cause time calculations to show very large elapsed periods, potentially refilling buckets to maximum capacity instantly. Fall transitions (clocks jump backward) can cause negative time deltas or extended periods without token refill.</p>\n<p>Detection requires identifying when calculated elapsed time significantly exceeds expected values or when current timestamps are earlier than stored timestamps by amounts that match DST transitions. The recovery strategy involves capping token refill amounts during forward transitions and resetting refill timestamps during backward transitions, ensuring smooth operation through time changes.</p>\n<p><strong>Leap Second Adjustments</strong> are rare but can cause similar issues to clock jumps. Most systems handle leap seconds by either jumping the clock or slowing/speeding clock advancement over a period, both of which can affect token bucket calculations.</p>\n<p>The mitigation strategy involves using monotonic clocks (which measure elapsed time regardless of system clock adjustments) for token bucket calculations while using wall clock time only for logging and external interfaces. This isolates the core rate limiting algorithm from system clock irregularities.</p>\n<h4 id=\"burst-scenario-edge-cases\">Burst Scenario Edge Cases</h4>\n<p><strong>Sustained Burst Traffic Patterns</strong> occur when clients consistently use their full burst allowance, then wait for refill, then burst again. This creates a pattern where traffic arrives in concentrated waves rather than the smooth distribution that token bucket algorithms are designed to regulate.</p>\n<p>While this behavior is technically within the rate limits, it can still overwhelm downstream services that expect more evenly distributed load. The system must decide whether to allow this behavior (following the strict token bucket algorithm) or implement additional smoothing mechanisms.</p>\n<p>One approach involves implementing a secondary rate limit based on request spacing, requiring minimum intervals between requests even when tokens are available. Another approach uses averaging windows to detect sustained burst patterns and temporarily reduce burst capacity for clients exhibiting this behavior.</p>\n<p><strong>Cross-Client Burst Coordination</strong> happens when multiple clients simultaneously execute burst requests, creating aggregate load spikes that exceed system capacity even though each client individually stays within their rate limits. This is particularly problematic when client request patterns are synchronized (such as cron jobs running at the same time).</p>\n<p>Detection requires monitoring aggregate request rates across all clients and identifying when total system load exceeds safe thresholds despite individual clients being within limits. The response strategy might include implementing system-wide admission control that temporarily reduces individual client limits when aggregate load is high, or implementing request queuing to smooth out synchronized burst patterns.</p>\n<p><strong>Bucket Capacity Overflow</strong> occurs in edge cases where configuration changes or system bugs cause token refill calculations to exceed the bucket&#39;s maximum capacity. While the algorithm should cap tokens at the configured capacity, numerical precision issues or race conditions can sometimes cause temporary overflows.</p>\n<p>The following table outlines burst-related edge cases:</p>\n<table>\n<thead>\n<tr>\n<th>Edge Case</th>\n<th>Trigger Condition</th>\n<th>Potential Impact</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sustained burst cycling</td>\n<td>Client repeatedly uses full burst, waits, repeats</td>\n<td>Uneven load on downstream services</td>\n<td>Secondary spacing limits or burst capacity reduction</td>\n</tr>\n<tr>\n<td>Synchronized client bursts</td>\n<td>Multiple clients burst simultaneously</td>\n<td>Aggregate load exceeds system capacity</td>\n<td>System-wide admission control, request queuing</td>\n</tr>\n<tr>\n<td>Bucket capacity overflow</td>\n<td>Token calculation exceeds configured capacity</td>\n<td>Temporarily excessive request allowances</td>\n<td>Strict capacity enforcement, overflow detection</td>\n</tr>\n<tr>\n<td>Zero-capacity bucket configuration</td>\n<td>Configuration error sets capacity to 0</td>\n<td>All requests blocked indefinitely</td>\n<td>Configuration validation, minimum capacity enforcement</td>\n</tr>\n<tr>\n<td>Infinite burst allowance</td>\n<td>Configuration sets unreasonably high capacity</td>\n<td>Rate limiting becomes ineffective</td>\n<td>Maximum capacity limits in configuration validation</td>\n</tr>\n</tbody></table>\n<h4 id=\"client-identification-edge-cases\">Client Identification Edge Cases</h4>\n<p><strong>IP Address Spoofing</strong> involves clients manipulating their apparent IP address to circumvent IP-based rate limiting. While IP spoofing is difficult for TCP connections, it&#39;s possible for UDP-based protocols or when using proxy services that don&#39;t preserve original client IPs.</p>\n<p>The mitigation strategy involves implementing multiple identification layers, such as combining IP addresses with API keys or user agent fingerprints. The system should also validate IP addresses for reasonableness (private IP ranges reaching public services might indicate proxy issues) and implement behavioral analysis to detect unusual patterns from individual IP addresses.</p>\n<p><strong>API Key Sharing or Compromise</strong> occurs when legitimate API keys are shared among multiple clients or stolen by malicious actors. This creates scenarios where rate limits designed for individual clients are effectively bypassed through distributed usage of the same credentials.</p>\n<p>Detection involves monitoring API key usage patterns for signs of unusual distribution across IP addresses, geographic locations, or usage patterns that don&#39;t match expected behavior for individual clients. The response includes temporarily reducing rate limits for suspicious API keys, requiring additional authentication factors, or implementing sub-limits based on IP address even when API keys are provided.</p>\n<p><strong>Client Identifier Collision</strong> happens when the client identification algorithm produces the same identifier for different actual clients. This is particularly problematic when using hash-based identification or when truncating long identifiers for storage efficiency.</p>\n<p>Prevention involves using high-quality hash functions with sufficient output length to minimize collision probability, implementing collision detection by storing additional client metadata, and providing fallback identification methods when collisions are detected.</p>\n<p><strong>Proxy and CDN Complications</strong> arise when clients access services through proxy servers, content delivery networks, or other intermediaries that present the same IP address for multiple distinct clients. This causes all clients behind the proxy to share the same rate limit bucket.</p>\n<p>The solution requires implementing proxy-aware client identification that examines headers like <code>X-Forwarded-For</code>, <code>X-Real-IP</code>, or custom headers provided by trusted proxies. The system must validate that proxy headers come from trusted sources and implement fallback strategies when proxy identification is unavailable or suspicious.</p>\n<p>The client identification edge cases are summarized in this table:</p>\n<table>\n<thead>\n<tr>\n<th>Edge Case</th>\n<th>Detection Method</th>\n<th>Risk Level</th>\n<th>Recommended Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>IP address spoofing</td>\n<td>Behavioral analysis, TCP connection validation</td>\n<td>Medium</td>\n<td>Multi-factor client identification</td>\n</tr>\n<tr>\n<td>API key sharing</td>\n<td>Usage pattern analysis across IPs</td>\n<td>High</td>\n<td>Rate limit reduction, additional auth factors</td>\n</tr>\n<tr>\n<td>Client identifier collision</td>\n<td>Hash collision detection, metadata comparison</td>\n<td>Low</td>\n<td>Higher entropy identifiers, collision resolution</td>\n</tr>\n<tr>\n<td>Proxy/CDN masking</td>\n<td>Multiple clients from same IP, high request rates</td>\n<td>High</td>\n<td>Proxy-aware headers, trusted proxy validation</td>\n</tr>\n<tr>\n<td>Client enumeration attack</td>\n<td>Rapid creation of new client identifiers</td>\n<td>High</td>\n<td>Rate limit on new client creation</td>\n</tr>\n</tbody></table>\n<h3 id=\"recovery-and-degradation\">Recovery and Degradation</h3>\n<p>The rate limiter&#39;s recovery and degradation strategies ensure that the system continues protecting services even when operating under adverse conditions. The key principle is <strong>graceful degradation</strong> - reducing functionality or performance rather than complete failure, while maintaining the core protection that rate limiting provides.</p>\n<h4 id=\"failure-recovery-strategies\">Failure Recovery Strategies</h4>\n<p><strong>Circuit Breaker Pattern Implementation</strong> provides automatic failure detection and recovery for external dependencies, particularly Redis connections in distributed configurations. The circuit breaker monitors failure rates and response times, automatically switching to fallback modes when thresholds are exceeded and periodically testing for recovery conditions.</p>\n<p>The circuit breaker operates in three states: <strong>Closed</strong> (normal operation), <strong>Open</strong> (failures detected, using fallback), and <strong>Half-Open</strong> (testing for recovery). State transitions are based on configurable failure thresholds and recovery timeouts. When Redis operations fail consistently, the circuit breaker opens and directs all rate limiting operations to local fallback buckets. After a recovery timeout period, it enters the half-open state and allows a limited number of test operations to determine if Redis has recovered.</p>\n<p>The implementation must handle edge cases such as partial Redis recovery (some operations succeed while others fail) and thundering herd scenarios where multiple server instances simultaneously attempt recovery operations. The solution involves randomized recovery testing delays and gradual traffic ramp-up when Redis becomes available again.</p>\n<p><strong>Local Fallback Bucket Management</strong> provides continued rate limiting functionality when distributed storage is unavailable. Local fallback buckets use more conservative rate limits to account for the lack of coordination between server instances, ensuring that the aggregate rate limiting remains effective even though individual instance limits might be lower.</p>\n<p>The fallback strategy involves maintaining a separate set of in-memory token buckets with reduced capacities and refill rates calculated to provide reasonable protection when multiplied across all expected server instances. For example, if the normal distributed rate limit allows 1000 requests per minute and there are typically 5 server instances, each fallback bucket might allow 150 requests per minute (providing a safety margin below the distributed total).</p>\n<p>Memory management for fallback buckets requires aggressive cleanup policies since they cannot rely on distributed coordination for state management. The system implements strict limits on the number of concurrent fallback buckets and uses LRU eviction to prevent memory exhaustion during extended Redis outages.</p>\n<p><strong>Progressive Degradation Levels</strong> provide multiple fallback layers as system conditions worsen. Instead of a binary switch between full functionality and emergency mode, the system implements graduated responses that maintain as much functionality as possible under various failure conditions.</p>\n<p>Level 1 degradation occurs when Redis response times increase but operations still succeed. The system maintains full functionality but implements request timeouts and reduces the frequency of Redis operations where possible. Level 2 degradation activates when Redis operations begin failing intermittently, triggering local caching of recent rate limiting decisions and reduced precision in rate limiting calculations. Level 3 degradation engages full local fallback mode with conservative rate limits and no distributed coordination.</p>\n<p>The following table outlines the progressive degradation strategy:</p>\n<table>\n<thead>\n<tr>\n<th>Degradation Level</th>\n<th>Trigger Condition</th>\n<th>Active Features</th>\n<th>Disabled Features</th>\n<th>Recovery Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal Operation</td>\n<td>All systems healthy</td>\n<td>Full distributed rate limiting</td>\n<td>None</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Level 1: Performance</td>\n<td>Redis latency &gt; threshold</td>\n<td>Full functionality, cached decisions</td>\n<td>Real-time Redis queries for all requests</td>\n<td>Redis latency &lt; threshold for sustained period</td>\n</tr>\n<tr>\n<td>Level 2: Intermittent</td>\n<td>Redis error rate &gt; 10%</td>\n<td>Local caching, reduced precision</td>\n<td>Exact distributed synchronization</td>\n<td>Redis error rate &lt; 5% for recovery period</td>\n</tr>\n<tr>\n<td>Level 3: Full Fallback</td>\n<td>Redis error rate &gt; 50%</td>\n<td>Local conservative buckets</td>\n<td>All distributed features</td>\n<td>Redis error rate &lt; 1% and connection stability</td>\n</tr>\n<tr>\n<td>Emergency Mode</td>\n<td>Memory/CPU exhaustion</td>\n<td>Basic IP-based rate limiting</td>\n<td>Per-client tracking, burst handling</td>\n<td>Resource usage below emergency thresholds</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-consistency-recovery\">Data Consistency Recovery</h4>\n<p><strong>State Resynchronization After Network Partitions</strong> addresses scenarios where server instances operate independently during network failures and must reconcile their rate limiting decisions when connectivity is restored. The challenge is determining which instance has the most accurate view of client rate limiting state and how to merge conflicting information.</p>\n<p>The resynchronization strategy uses timestamps and sequence numbers to determine the authoritative state for each client bucket. When instances reconnect to Redis, they compare their local fallback bucket states with the distributed state, choosing the most restrictive interpretation to ensure rate limiting effectiveness wasn&#39;t compromised during the partition.</p>\n<p>The process involves uploading local bucket states to Redis with metadata indicating the partition period and fallback limits used. A reconciliation algorithm examines all uploaded states and constructs a merged view that accounts for requests processed by all instances during the partition. This ensures that clients don&#39;t receive unfairly restrictive treatment due to double-counting their requests across instances.</p>\n<p><strong>Corrupted Bucket State Recovery</strong> handles situations where Redis data becomes corrupted, inconsistent, or contains impossible values due to bugs, hardware issues, or operational errors. The recovery process must identify corrupted data, safely remove it, and re-initialize affected buckets without causing service disruption.</p>\n<p>Detection involves implementing data validation checks on all Redis read operations, flagging buckets with impossible states such as negative token counts, token counts exceeding configured capacities, or timestamps from the future. When corruption is detected, the system logs the incident, removes the corrupted data from Redis, and initializes a fresh bucket with default values.</p>\n<p>The recovery strategy prioritizes safety over continuity - it&#39;s better to temporarily reset a client&#39;s rate limiting state than to operate with corrupted data that might allow unlimited requests or permanently block legitimate clients. The system provides administrative tools to manually inspect and repair bucket state when automated recovery is insufficient.</p>\n<p><strong>Clock Drift Correction</strong> addresses gradual clock synchronization issues that can cause rate limiting inconsistencies across distributed instances. Unlike sudden clock jumps, clock drift accumulates slowly over time and can be corrected proactively before it causes significant problems.</p>\n<p>The correction mechanism involves periodic synchronization checks where instances compare their local time with Redis server time or an external time authority. When drift is detected beyond acceptable thresholds, the system gradually adjusts its token calculation parameters to compensate for the difference rather than making sudden corrections that could cause dramatic changes in rate limiting behavior.</p>\n<h4 id=\"memory-and-resource-recovery\">Memory and Resource Recovery</h4>\n<p><strong>Emergency Memory Management</strong> activates when the rate limiter detects memory pressure that threatens system stability. The emergency procedures prioritize maintaining basic rate limiting functionality while aggressively reducing memory usage through bucket eviction and configuration changes.</p>\n<p>The emergency response includes immediately halting creation of new client buckets, implementing aggressive LRU eviction of existing buckets, and temporarily switching to simpler rate limiting algorithms that require less memory per client. The system also reduces bucket retention times and cleanup intervals to accelerate memory reclamation.</p>\n<p>Memory recovery involves monitoring system memory usage and gradually restoring normal functionality as memory pressure subsides. The system implements hysteresis in memory management - emergency measures activate at higher memory usage thresholds than they deactivate, preventing oscillation between normal and emergency modes.</p>\n<p><strong>Resource Exhaustion Handling</strong> addresses scenarios where CPU usage, file descriptor limits, or other system resources become constrained. The rate limiter must continue operating while reducing its resource footprint and potentially degrading performance rather than functionality.</p>\n<p>CPU exhaustion handling involves reducing the frequency of background tasks like bucket cleanup, simplifying rate limiting calculations, and potentially queuing rate limiting decisions during peak CPU usage periods. File descriptor exhaustion primarily affects Redis connections, requiring connection pooling optimization and aggressive connection recycling.</p>\n<p>The recovery strategy monitors resource usage trends and implements predictive measures to prevent complete resource exhaustion. When resource usage approaches critical levels, the system proactively reduces its resource footprint and may temporarily reject new client registrations to preserve capacity for existing clients.</p>\n<h4 id=\"operational-recovery-procedures\">Operational Recovery Procedures</h4>\n<p><strong>Administrative Recovery Tools</strong> provide operators with capabilities to manually intervene when automated recovery mechanisms are insufficient or when unusual circumstances require human judgment. These tools must be safe to use during production incidents and provide clear feedback about their impact on system state.</p>\n<p>The toolset includes bucket inspection utilities that display current client rate limiting states, bucket reset commands that safely reinitialize corrupted or problematic client buckets, and configuration reload capabilities that allow rate limiting parameters to be adjusted without service restart. Emergency override commands allow operators to temporarily disable rate limiting for specific clients or endpoints during critical incidents.</p>\n<p>Safety features prevent accidental misuse of administrative tools, including confirmation prompts for destructive operations, audit logging of all administrative actions, and automatic rollback capabilities for configuration changes. The tools integrate with existing monitoring and alerting systems to provide visibility into their usage and impact.</p>\n<p><strong>Monitoring and Alerting Integration</strong> ensures that rate limiting failures and recovery events are properly detected, escalated, and tracked. The monitoring strategy focuses on leading indicators that predict problems before they cause service impact, as well as lagging indicators that confirm the effectiveness of recovery actions.</p>\n<p>Key metrics include Redis connection health, rate limiting decision latency, local fallback bucket usage, memory consumption trends, and client bucket creation/cleanup rates. Alert thresholds are tuned to provide early warning of developing issues while minimizing false positives during normal operation variations.</p>\n<p>The alerting strategy implements escalation procedures that automatically engage additional resources or trigger more aggressive recovery measures when initial responses are insufficient. Integration with incident management systems ensures that rate limiting issues are properly tracked and that post-incident reviews can identify opportunities for system improvement.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building robust error handling for a distributed rate limiter requires careful attention to failure detection, recovery mechanisms, and operational observability. The implementation must balance performance with reliability, ensuring that error handling doesn&#39;t become a bottleneck during normal operation while providing comprehensive protection during failure scenarios.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Detection</td>\n<td>Basic exception handling with logging</td>\n<td>Structured error codes with metrics</td>\n</tr>\n<tr>\n<td>Circuit Breaker</td>\n<td>Simple failure counting with timeouts</td>\n<td>Netflix Hystrix-style adaptive thresholds</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>Periodic connectivity checks</td>\n<td>Continuous health scoring with trends</td>\n</tr>\n<tr>\n<td>Fallback Storage</td>\n<td>In-memory dictionaries with size limits</td>\n<td>Bounded LRU caches with TTL support</td>\n</tr>\n<tr>\n<td>Recovery Testing</td>\n<td>Manual Redis reconnection attempts</td>\n<td>Automated canary testing with gradual rollback</td>\n</tr>\n<tr>\n<td>Observability</td>\n<td>Standard logging with error rates</td>\n<td>Structured metrics with distributed tracing</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The error handling implementation should be organized to separate concerns while providing comprehensive coverage across all rate limiter components:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>rate_limiter/\n├── error_handling/\n│   ├── __init__.py                 ← Error handling module exports\n│   ├── circuit_breaker.py          ← Circuit breaker implementation\n│   ├── fallback_manager.py         ← Local fallback bucket management\n│   ├── recovery_coordinator.py     ← Recovery and resynchronization logic\n│   ├── health_monitor.py           ← Component health monitoring\n│   └── error_types.py              ← Custom exception classes\n├── middleware/\n│   ├── error_middleware.py         ← HTTP error response handling\n│   └── health_endpoints.py         ← Health check HTTP endpoints\n├── storage/\n│   ├── redis_client_wrapper.py     ← Redis client with error handling\n│   └── connection_pool_manager.py  ← Connection pooling with failover\n├── monitoring/\n│   ├── metrics_collector.py        ← Error and performance metrics\n│   └── alert_manager.py            ← Alert generation and escalation\n└── admin/\n    ├── recovery_tools.py           ← Administrative recovery utilities\n    └── diagnostic_tools.py         ← System state inspection tools</code></pre></div>\n\n<h4 id=\"circuit-breaker-infrastructure-code\">Circuit Breaker Infrastructure Code</h4>\n<p>Here&#39;s a complete circuit breaker implementation that can be used throughout the rate limiting system:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreakerState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CLOSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"closed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"open\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HALF_OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"half_open\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreakerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failure_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recovery_timeout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    success_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Circuit breaker implementation for Redis and other external dependencies.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Automatically opens on repeated failures and tests for recovery.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: CircuitBreakerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreakerState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> call</span><span style=\"color:#E1E4E8\">(self, func: Callable, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute function with circuit breaker protection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises CircuitBreakerOpenException when circuit is open.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitBreakerState.</span><span style=\"color:#79B8FF\">OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.recovery_timeout:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    raise</span><span style=\"color:#E1E4E8\"> CircuitBreakerOpenException(</span><span style=\"color:#9ECBFF\">\"Circuit breaker is open\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreakerState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> func(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._record_success()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._record_failure()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _record_success</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitBreakerState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.success_threshold:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreakerState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitBreakerState.</span><span style=\"color:#79B8FF\">CLOSED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _record_failure</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.failure_threshold:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreakerState.</span><span style=\"color:#79B8FF\">OPEN</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreakerOpenException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"health-monitor-infrastructure-code\">Health Monitor Infrastructure Code</h4>\n<p>This complete health monitoring system tracks component health and triggers recovery actions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Callable, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheckResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response_time_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Optional[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_check_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    consecutive_failures: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    consecutive_successes: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    average_response_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive health monitoring for rate limiter components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Tracks Redis, memory usage, bucket operations, and triggers recovery.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.components: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ComponentHealth] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.health_checks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_handlers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Callable]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring_thread: Optional[threading.Thread] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_health_check</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: Callable):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a health check function for a component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.health_checks[component_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_func</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> component_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.components:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.components[component_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ComponentHealth(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">component_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNKNOWN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    last_check_time</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    consecutive_failures</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    consecutive_successes</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    average_response_time</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_recovery_handler</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, handler: Callable):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a recovery handler to be called when component becomes unhealthy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> component_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.recovery_handlers:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.recovery_handlers[component_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_handlers[component_name].append(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_monitoring</span><span style=\"color:#E1E4E8\">(self, check_interval: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start background health monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._monitoring_loop,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            args</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(check_interval,),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring_thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop_monitoring</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop background health monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.monitoring_thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.monitoring_thread.join()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_system_health</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ComponentHealth]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current health status of all monitored components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.components.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _monitoring_loop</span><span style=\"color:#E1E4E8\">(self, check_interval: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Background monitoring loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> component_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.health_checks:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">._check_component_health(component_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Health check error for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">component_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(check_interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _check_component_health</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Perform health check for a specific component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        check_func </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.health_checks[component_name]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_func()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            response_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._update_component_health(component_name, result, response_time)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            error_result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthCheckResult(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                response_time_ms</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                error_message</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._update_component_health(component_name, error_result, error_result.response_time_ms)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _update_component_health</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, result: HealthCheckResult, response_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update component health based on check result.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            component </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.components[component_name]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            component.last_check_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Update response time average</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> component.average_response_time </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component.average_response_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> response_time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component.average_response_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (component.average_response_time </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> response_time </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Update health status and counters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            previous_status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component.status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            component.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result.status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> result.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component.consecutive_successes </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component.consecutive_failures </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component.consecutive_failures </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component.consecutive_successes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Trigger recovery handlers if component became unhealthy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> previous_status </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> result.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">._trigger_recovery_handlers(component_name, result)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _trigger_recovery_handlers</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, result: HealthCheckResult):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Trigger registered recovery handlers for unhealthy component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        handlers </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.recovery_handlers.get(component_name, [])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> handlers:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                handler(component_name, result)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Recovery handler error for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">component_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"core-error-handling-skeleton-code\">Core Error Handling Skeleton Code</h4>\n<p>The main error handling coordinator that integrates with all rate limiter components:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitErrorHandler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Central error handling coordinator for the rate limiting system.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Manages fallback strategies, recovery procedures, and degradation levels.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: DistributedRateLimitConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.circuit_breaker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreaker(CircuitBreakerConfig())</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.health_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fallback_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Initialize in setup</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_degradation_level </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_error_handling</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize error handling components and monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize fallback bucket manager with local storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Register health checks for Redis, memory, and bucket operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Register recovery handlers for each component failure type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Start background health monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set up metrics collection for error rates and recovery times</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_redis_error</span><span style=\"color:#E1E4E8\">(self, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle Redis operation errors and determine fallback strategy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Log error with context (operation type, client_id, timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update circuit breaker with failure information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if fallback buckets should be activated</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update degradation level based on error frequency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if operation should be retried, False for fallback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_memory_pressure</span><span style=\"color:#E1E4E8\">(self, current_usage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle memory pressure by implementing emergency cleanup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate severity level based on usage vs threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Trigger aggressive bucket cleanup for stale entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Temporarily halt new bucket creation if critical</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Implement LRU eviction for existing buckets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Alert monitoring systems about memory pressure event</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_clock_drift</span><span style=\"color:#E1E4E8\">(self, detected_drift: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, max_allowed: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle clock synchronization issues in distributed environment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate drift amount against acceptable thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Implement gradual correction to avoid sudden rate changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Log drift detection for operational awareness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update bucket timestamps to compensate for drift</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Consider switching to monotonic time if drift is severe</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> attempt_recovery</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Attempt to recover failed component with exponential backoff.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if component is eligible for recovery attempt</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Implement exponential backoff between recovery attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test component functionality before declaring recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update component health status based on recovery result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if recovery successful, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_fallback_bucket_config</span><span style=\"color:#E1E4E8\">(self, original_config: TokenBucketConfig) -> TokenBucketConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate conservative fallback bucket configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Reduce capacity to account for lack of distributed coordination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Adjust refill rate based on expected number of server instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply safety margin to prevent aggregate over-limiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure configuration remains reasonable for single instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return modified configuration for local fallback use</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing comprehensive error handling, verify the system&#39;s resilience with these tests:</p>\n<p><strong>Basic Error Handling Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start rate limiter with Redis dependency</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> rate_limiter.server</span><span style=\"color:#79B8FF\"> --redis-url=redis://localhost:6379</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test circuit breaker by stopping Redis</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">docker</span><span style=\"color:#9ECBFF\"> stop</span><span style=\"color:#9ECBFF\"> redis-container</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Send test requests - should switch to local fallback</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-API-Key: test-key\"</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/test</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check health endpoint</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/health</span></span></code></pre></div>\n\n<p><strong>Expected behavior:</strong> Requests continue to be rate-limited using local fallback buckets with conservative limits. Health endpoint shows Redis as unhealthy but overall system as degraded rather than failed.</p>\n<p><strong>Recovery Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Restart Redis</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">docker</span><span style=\"color:#9ECBFF\"> start</span><span style=\"color:#9ECBFF\"> redis-container</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Monitor logs for automatic recovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">tail</span><span style=\"color:#79B8FF\"> -f</span><span style=\"color:#9ECBFF\"> rate_limiter.log</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#79B8FF\"> -i</span><span style=\"color:#9ECBFF\"> recovery</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Send more test requests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-API-Key: test-key\"</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/test</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify return to distributed rate limiting</span></span></code></pre></div>\n\n<p><strong>Expected behavior:</strong> System automatically detects Redis recovery, gradually transitions back to distributed rate limiting, and health endpoint shows all components healthy.</p>\n<h4 id=\"common-implementation-pitfalls\">Common Implementation Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Failing Open During Error Conditions</strong>\nMany implementations disable rate limiting entirely when errors occur, defeating the primary purpose of protection. Instead, always fail conservatively - if you can&#39;t accurately track rate limits, apply more restrictive limits rather than none.</p>\n<p>⚠️ <strong>Pitfall: Infinite Retry Loops</strong>\nCircuit breaker implementations that don&#39;t properly track failure counts can get stuck in infinite retry loops. Ensure failure counting is atomic and that circuit state changes are properly synchronized.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks in Fallback Mode</strong>\nLocal fallback buckets created during Redis outages can accumulate indefinitely if not properly cleaned up. Implement strict memory limits and aggressive cleanup policies for fallback scenarios.</p>\n<p>⚠️ <strong>Pitfall: Thundering Herd on Recovery</strong>\nWhen Redis recovers, all server instances may simultaneously attempt to reconnect and synchronize state. Implement randomized delays and gradual state resynchronization to prevent overwhelming the recovered system.</p>\n<p>⚠️ <strong>Pitfall: Clock Drift Accumulation</strong>\nSmall clock differences between servers can accumulate over time to create significant rate limiting inconsistencies. Regularly synchronize with authoritative time sources and implement drift detection.</p>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - comprehensive testing validates functionality from basic token bucket implementation through distributed rate limiting across multiple server instances</p>\n</blockquote>\n<p>Think of testing a rate limiter like conducting a orchestra rehearsal before the big performance. Just as a conductor tests each section individually (strings, brass, woodwinds), then brings them together for ensemble pieces, and finally runs the complete symphony under performance conditions, we need to verify our rate limiter at multiple levels. Each component must perform correctly in isolation, work harmoniously with other components, and maintain accuracy under the stress of real-world load patterns.</p>\n<p>Testing a rate limiting system presents unique challenges because it involves time-based behavior, concurrency, distributed coordination, and performance requirements. Unlike testing a simple calculator where inputs and outputs are deterministic, rate limiting involves probabilistic behavior, timing precision, and system-level interactions that can only be validated through carefully designed test scenarios.</p>\n<h3 id=\"mental-model-the-quality-assurance-pyramid\">Mental Model: The Quality Assurance Pyramid</h3>\n<p>Imagine our testing strategy as a pyramid with four distinct levels, each serving a specific purpose in building confidence in our rate limiter:</p>\n<p><strong>Foundation Level (Unit Tests)</strong>: Like testing individual musical instruments to ensure they&#39;re properly tuned, we verify each component works correctly in isolation. Token buckets generate and consume tokens accurately, client trackers manage bucket lifecycles properly, and middleware components handle HTTP interactions correctly.</p>\n<p><strong>Integration Level (Component Integration)</strong>: Like testing small ensembles of instruments playing together, we verify that components communicate correctly. The middleware properly calls the client tracker, buckets are created and cleaned up as expected, and Redis operations maintain consistency.</p>\n<p><strong>System Level (End-to-End Testing)</strong>: Like running through complete musical pieces, we test entire request flows from HTTP input through rate limiting decisions to final responses. This includes distributed scenarios where multiple servers coordinate through Redis.</p>\n<p><strong>Performance Level (Load Testing)</strong>: Like testing the orchestra under the acoustic pressure of a full concert hall, we verify the rate limiter maintains accuracy and responsiveness under realistic production loads with high concurrency and distributed coordination.</p>\n<h3 id=\"unit-test-coverage\">Unit Test Coverage</h3>\n<p>Unit testing for rate limiting requires isolating each component and verifying its behavior across all possible states and edge conditions. The challenge lies in testing time-based algorithms where token generation depends on elapsed time, and concurrent scenarios where multiple threads access shared state.</p>\n<h4 id=\"token-bucket-algorithm-testing\">Token Bucket Algorithm Testing</h4>\n<p>The <code>TokenBucket</code> class requires comprehensive testing of its core time-based algorithm, including token generation, consumption, and overflow handling. Each test must control time progression to ensure deterministic behavior.</p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Purpose</th>\n<th>Key Assertions</th>\n<th>Time Control Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Initial bucket state</td>\n<td>Verify correct initialization</td>\n<td><code>tokens_remaining == initial_tokens</code>, <code>capacity</code> and <code>refill_rate</code> match config</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Token generation over time</td>\n<td>Validate refill algorithm accuracy</td>\n<td>Tokens increase at correct rate, capped at capacity</td>\n<td>Mock time progression</td>\n</tr>\n<tr>\n<td>Basic token consumption</td>\n<td>Test successful token deduction</td>\n<td><code>tokens_remaining</code> decreases, <code>allowed == true</code></td>\n<td>No</td>\n</tr>\n<tr>\n<td>Insufficient tokens</td>\n<td>Validate rejection behavior</td>\n<td><code>allowed == false</code>, tokens unchanged, correct <code>retry_after_seconds</code></td>\n<td>No</td>\n</tr>\n<tr>\n<td>Burst consumption</td>\n<td>Test consuming up to capacity</td>\n<td>Can consume all tokens at once, subsequent requests denied</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Overflow prevention</td>\n<td>Ensure tokens never exceed capacity</td>\n<td>Tokens cap at capacity despite extended refill time</td>\n<td>Mock extended time</td>\n</tr>\n<tr>\n<td>Fractional token generation</td>\n<td>Test precision with small rates</td>\n<td>Accurate token accumulation with rates like 0.1 tokens/second</td>\n<td>Mock precise time increments</td>\n</tr>\n<tr>\n<td>Clock drift handling</td>\n<td>Test behavior with time anomalies</td>\n<td>Graceful handling of negative time deltas, clock jumps</td>\n<td>Mock time anomalies</td>\n</tr>\n<tr>\n<td>Concurrent access</td>\n<td>Verify thread safety</td>\n<td>No race conditions, accurate token counts under concurrent load</td>\n<td>Multi-threading</td>\n</tr>\n<tr>\n<td>Configuration validation</td>\n<td>Test invalid parameters</td>\n<td>Appropriate exceptions for negative rates, zero capacity</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p><strong>Critical Token Bucket Test Scenarios</strong>:</p>\n<ol>\n<li><p><strong>Precision Testing</strong>: Token generation must remain accurate over extended periods. A bucket with 1 token/second rate should have exactly 3600 tokens available after one hour (if capacity allows), not 3599 or 3601 due to floating-point accumulation errors.</p>\n</li>\n<li><p><strong>Burst Validation</strong>: A bucket with 10 tokens capacity and 1 token/second rate should allow consuming all 10 tokens immediately, then deny requests for 10 seconds, then allow 1 token consumption every second thereafter.</p>\n</li>\n<li><p><strong>Time Anomaly Handling</strong>: When system clock jumps backward (daylight saving time, NTP correction), the bucket should not add negative tokens or enter invalid states. When clock jumps forward significantly, tokens should cap at bucket capacity.</p>\n</li>\n<li><p><strong>Thread Safety Verification</strong>: Multiple threads simultaneously calling <code>try_consume</code> should never result in negative token counts, tokens exceeding capacity, or inconsistent bucket state. Use stress testing with 100+ concurrent threads.</p>\n</li>\n</ol>\n<h4 id=\"client-bucket-tracker-testing\">Client Bucket Tracker Testing</h4>\n<p>The <code>ClientBucketTracker</code> manages bucket lifecycles, cleanup operations, and client identification. Testing must verify memory management, concurrent access patterns, and cleanup effectiveness.</p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Purpose</th>\n<th>Key Validation Points</th>\n<th>Concurrency Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bucket creation</td>\n<td>Verify on-demand bucket instantiation</td>\n<td>Correct config resolution, proper initialization</td>\n<td>Single-threaded</td>\n</tr>\n<tr>\n<td>Bucket reuse</td>\n<td>Test bucket caching behavior</td>\n<td>Same client gets same bucket instance</td>\n<td>Single-threaded</td>\n</tr>\n<tr>\n<td>Client identification</td>\n<td>Validate ID extraction and normalization</td>\n<td>Consistent IDs for same client, different IDs for different clients</td>\n<td>Single-threaded</td>\n</tr>\n<tr>\n<td>Configuration resolution</td>\n<td>Test hierarchical config override</td>\n<td>Client overrides beat defaults, endpoint overrides beat client</td>\n<td>Single-threaded</td>\n</tr>\n<tr>\n<td>Stale bucket detection</td>\n<td>Verify cleanup candidate identification</td>\n<td>Correctly identifies buckets older than threshold</td>\n<td>Single-threaded</td>\n</tr>\n<tr>\n<td>Cleanup execution</td>\n<td>Test bucket removal process</td>\n<td>Removes stale buckets, preserves active ones, updates statistics</td>\n<td>Single-threaded</td>\n</tr>\n<tr>\n<td>Concurrent bucket access</td>\n<td>Test thread-safe bucket creation/access</td>\n<td>No duplicate buckets for same client, thread-safe statistics</td>\n<td>High concurrency</td>\n</tr>\n<tr>\n<td>Memory pressure handling</td>\n<td>Test behavior under resource constraints</td>\n<td>LRU eviction works correctly, memory usage stays bounded</td>\n<td>Resource-constrained</td>\n</tr>\n<tr>\n<td>Client ID validation</td>\n<td>Test malformed client identifier handling</td>\n<td>Rejects invalid IPs, malformed API keys, empty identifiers</td>\n<td>Single-threaded</td>\n</tr>\n<tr>\n<td>Configuration hot-reload</td>\n<td>Test dynamic config updates</td>\n<td>New buckets use updated config, existing buckets continue with old config</td>\n<td>Single-threaded</td>\n</tr>\n</tbody></table>\n<p><strong>Critical Client Tracker Test Scenarios</strong>:</p>\n<ol>\n<li><p><strong>Memory Leak Prevention</strong>: Create buckets for 10,000 unique clients, wait for cleanup interval, verify that stale buckets are removed and memory usage returns to baseline levels. Monitor for gradual memory growth indicating cleanup failures.</p>\n</li>\n<li><p><strong>Concurrent Client Onboarding</strong>: Have 50 threads simultaneously make first requests for unique clients. Verify that exactly one bucket gets created per client (no duplicates) and all buckets have correct configuration.</p>\n</li>\n<li><p><strong>Configuration Precedence</strong>: Test complex scenarios where client has override, endpoint has override, and global defaults exist. Verify correct hierarchical resolution: endpoint-specific &gt; client-specific &gt; global defaults.</p>\n</li>\n</ol>\n<h4 id=\"http-middleware-component-testing\">HTTP Middleware Component Testing</h4>\n<p>The middleware integration requires testing HTTP-specific behavior, header handling, and framework integration without depending on actual web frameworks running.</p>\n<table>\n<thead>\n<tr>\n<th>Test Component</th>\n<th>Test Focus</th>\n<th>Mock Requirements</th>\n<th>Validation Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request parsing</td>\n<td>Client ID extraction from various sources</td>\n<td>Mock HTTP request objects</td>\n<td>Correct client ID extracted from IP, headers, API keys</td>\n</tr>\n<tr>\n<td>Rate limit checking</td>\n<td>Integration with client tracker</td>\n<td>Mock <code>ClientBucketTracker</code></td>\n<td>Correct bucket retrieval, token consumption</td>\n</tr>\n<tr>\n<td>Response building</td>\n<td>HTTP headers and status codes</td>\n<td>Mock HTTP response objects</td>\n<td>Proper 429 status, rate limit headers, Retry-After calculation</td>\n</tr>\n<tr>\n<td>Endpoint normalization</td>\n<td>Path and method processing</td>\n<td>Mock request data</td>\n<td>Consistent endpoint identification across variations</td>\n</tr>\n<tr>\n<td>Skip logic</td>\n<td>Bypass conditions</td>\n<td>Mock requests with special headers</td>\n<td>Correctly identifies requests to skip</td>\n</tr>\n<tr>\n<td>Error handling</td>\n<td>Invalid client IDs, tracker failures</td>\n<td>Mock error conditions</td>\n<td>Graceful degradation, appropriate error responses</td>\n</tr>\n<tr>\n<td>Configuration loading</td>\n<td>Environment variable processing</td>\n<td>Mock environment</td>\n<td>Correct config parsing, validation, defaults</td>\n</tr>\n</tbody></table>\n<p><strong>Critical Middleware Test Scenarios</strong>:</p>\n<ol>\n<li><p><strong>Header Completeness</strong>: Every response (both successful and rate-limited) must include complete rate limiting headers: <code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>, <code>X-RateLimit-Reset</code>. Verify header values are accurate and consistent.</p>\n</li>\n<li><p><strong>Client Identification Robustness</strong>: Test with malformed IPs (<code>999.999.999.999</code>), missing API key headers, empty custom headers. Middleware should either extract valid IDs or provide sensible fallbacks without crashing.</p>\n</li>\n<li><p><strong>Endpoint Normalization</strong>: Requests to <code>/api/users/123</code> and <code>/api/users/456</code> should be treated as the same endpoint <code>/api/users/{id}</code> for rate limiting purposes. Test path parameter normalization and query string handling.</p>\n</li>\n</ol>\n<h3 id=\"integration-and-end-to-end-testing\">Integration and End-to-End Testing</h3>\n<p>Integration testing validates that components work together correctly, while end-to-end testing verifies complete request flows behave as expected under realistic conditions. These tests catch issues that unit tests miss: component interface mismatches, timing dependencies, and emergent system behaviors.</p>\n<h4 id=\"component-integration-testing\">Component Integration Testing</h4>\n<p>Integration tests focus on the boundaries between components, ensuring data flows correctly and contracts are honored. Unlike unit tests that mock dependencies, integration tests use real component instances working together.</p>\n<table>\n<thead>\n<tr>\n<th>Integration Boundary</th>\n<th>Test Scenarios</th>\n<th>Success Criteria</th>\n<th>Failure Modes to Test</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Middleware ↔ Client Tracker</td>\n<td>Rate limiting requests through middleware</td>\n<td>Correct bucket creation, token consumption, response headers</td>\n<td>Client tracker failures, invalid client IDs</td>\n</tr>\n<tr>\n<td>Client Tracker ↔ Token Buckets</td>\n<td>Bucket lifecycle management</td>\n<td>Proper bucket instantiation, cleanup, configuration application</td>\n<td>Memory leaks, stale bucket accumulation</td>\n</tr>\n<tr>\n<td>Token Bucket ↔ Time System</td>\n<td>Time-based token generation</td>\n<td>Accurate refill rates, overflow handling</td>\n<td>Clock drift, time jumps, precision loss</td>\n</tr>\n<tr>\n<td>Middleware ↔ HTTP Framework</td>\n<td>Request/response processing</td>\n<td>Proper header extraction, response building</td>\n<td>Malformed requests, framework version differences</td>\n</tr>\n<tr>\n<td>Distributed ↔ Redis</td>\n<td>Shared state coordination</td>\n<td>Consistent token counts across servers, atomic operations</td>\n<td>Redis connection failures, Lua script errors</td>\n</tr>\n</tbody></table>\n<p><strong>Critical Integration Scenarios</strong>:</p>\n<ol>\n<li><p><strong>Request Flow Accuracy</strong>: Send 100 requests at exactly the rate limit (e.g., 10 requests/second for a 10 RPS limit) and verify that all requests are allowed with no false denials. Then send 150 requests in the same time period and verify that exactly 100 are allowed and 50 are denied.</p>\n</li>\n<li><p><strong>Bucket Lifecycle Integration</strong>: Create buckets through middleware requests, verify they appear in client tracker, wait for cleanup interval, confirm stale buckets are removed but active ones persist. Test requires coordinated timing across components.</p>\n</li>\n<li><p><strong>Configuration Consistency</strong>: Change rate limit configuration and verify that new buckets use updated settings while existing buckets continue with their original configuration until cleanup. Test configuration propagation across component boundaries.</p>\n</li>\n</ol>\n<h4 id=\"end-to-end-request-flow-testing\">End-to-End Request Flow Testing</h4>\n<p>End-to-end tests validate complete request journeys from HTTP input to final response, ensuring the rate limiter behaves correctly from a client perspective. These tests should simulate realistic client interaction patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Request Pattern</th>\n<th>Expected Behavior</th>\n<th>Measurement Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal traffic within limits</td>\n<td>Steady requests at 80% of rate limit</td>\n<td>All requests allowed, proper headers</td>\n<td>Response times, header accuracy</td>\n</tr>\n<tr>\n<td>Burst traffic</td>\n<td>5x rate limit for 2 seconds, then normal</td>\n<td>Initial burst allowed up to capacity, then throttling</td>\n<td>Burst handling, recovery time</td>\n</tr>\n<tr>\n<td>Multiple clients</td>\n<td>3 clients at different rates</td>\n<td>Independent rate limiting, no interference</td>\n<td>Per-client accuracy, resource usage</td>\n</tr>\n<tr>\n<td>Mixed endpoints</td>\n<td>Same client hitting different endpoints</td>\n<td>Endpoint-specific limits applied correctly</td>\n<td>Configuration resolution, isolation</td>\n</tr>\n<tr>\n<td>Long-running client</td>\n<td>Single client over 1 hour</td>\n<td>Consistent rate limiting, no drift</td>\n<td>Long-term accuracy, memory stability</td>\n</tr>\n<tr>\n<td>Client reconnection</td>\n<td>Client stops, resumes after cleanup interval</td>\n<td>New bucket created, fresh token allocation</td>\n<td>Cleanup effectiveness, state reset</td>\n</tr>\n<tr>\n<td>Configuration changes</td>\n<td>Update limits during active traffic</td>\n<td>Existing clients unaffected, new clients get updated limits</td>\n<td>Hot reload behavior, transition smoothness</td>\n</tr>\n</tbody></table>\n<p><strong>Critical End-to-End Scenarios</strong>:</p>\n<ol>\n<li><p><strong>Rate Limit Accuracy Under Load</strong>: Run a client that makes exactly 600 requests over 60 seconds (10 RPS) against a 10 RPS limit. Verify that 590-600 requests are allowed (accounting for timing precision) and measure the standard deviation of inter-request timing to ensure smooth rate limiting rather than bursty approval.</p>\n</li>\n<li><p><strong>Multi-Client Isolation</strong>: Run 5 clients simultaneously, each with different rate limits (1, 5, 10, 20, 50 RPS). Verify that each client achieves its expected throughput without interference from others. Measure cross-client impact on response times and accuracy.</p>\n</li>\n<li><p><strong>Recovery After Burst</strong>: Client sends 100 requests instantly to a 10 RPS limit with 20 token capacity. Verify that first 20 requests are allowed immediately, remaining 80 are denied with correct <code>Retry-After</code> headers, and subsequent requests are allowed at exactly 10 RPS starting from the appropriate time.</p>\n</li>\n</ol>\n<h4 id=\"distributed-coordination-testing\">Distributed Coordination Testing</h4>\n<p>Distributed rate limiting introduces additional complexity requiring tests that verify consistency across multiple server instances sharing state through Redis.</p>\n<table>\n<thead>\n<tr>\n<th>Distributed Scenario</th>\n<th>Test Setup</th>\n<th>Consistency Requirements</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-server consistency</td>\n<td>3 servers, shared Redis, same client</td>\n<td>Total allowed requests ≤ rate limit across all servers</td>\n<td>Request logging, aggregate counting</td>\n</tr>\n<tr>\n<td>Redis failure handling</td>\n<td>1 server, Redis disconnect/reconnect</td>\n<td>Graceful fallback, recovery to consistent state</td>\n<td>Error monitoring, state verification</td>\n</tr>\n<tr>\n<td>Concurrent server operations</td>\n<td>10 servers, high concurrent load</td>\n<td>No double-counting, accurate token deduction</td>\n<td>Lua script verification, Redis monitoring</td>\n</tr>\n<tr>\n<td>Network partition</td>\n<td>2 servers, Redis accessible to only 1</td>\n<td>Isolated server falls back appropriately</td>\n<td>Fallback behavior monitoring</td>\n</tr>\n<tr>\n<td>Clock synchronization</td>\n<td>Servers with different system times</td>\n<td>Consistent token generation rates</td>\n<td>Time drift impact measurement</td>\n</tr>\n<tr>\n<td>Redis performance</td>\n<td>High request rate, Redis latency</td>\n<td>Maintained accuracy despite Redis delays</td>\n<td>Latency monitoring, accuracy tracking</td>\n</tr>\n</tbody></table>\n<p><strong>Critical Distributed Scenarios</strong>:</p>\n<ol>\n<li><p><strong>Cross-Server Consistency</strong>: Deploy 3 server instances sharing Redis. Send 300 requests distributed across all servers for a client with 100 RPS limit over 1 second. Verify that exactly ~100 requests are allowed total, regardless of which server processed each request.</p>\n</li>\n<li><p><strong>Redis Failover Behavior</strong>: During active traffic, disconnect Redis for 30 seconds, then reconnect. Verify that servers fall back to local buckets with conservative limits, maintain basic protection, and seamlessly return to distributed coordination when Redis recovers.</p>\n</li>\n<li><p><strong>Atomic Operation Validation</strong>: Under high concurrency (1000+ requests/second across multiple servers), verify that token bucket operations remain atomic and accurate. No tokens should be double-counted or lost due to race conditions in Lua script execution.</p>\n</li>\n</ol>\n<h3 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h3>\n<p>Each project milestone requires specific verification steps to ensure the implementation meets acceptance criteria before proceeding to the next milestone. These checkpoints provide concrete validation that core functionality works correctly.</p>\n<h4 id=\"milestone-1-token-bucket-implementation-checkpoint\">Milestone 1: Token Bucket Implementation Checkpoint</h4>\n<p>After implementing the core token bucket algorithm, verify the following behaviors through automated tests and manual validation.</p>\n<p><strong>Automated Test Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Required Tests</th>\n<th>Pass Criteria</th>\n<th>Command to Run</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic functionality</td>\n<td>Token generation, consumption, overflow</td>\n<td>All assertions pass, no timing flakiness</td>\n<td><code>python -m pytest tests/test_token_bucket.py::TestBasicFunctionality -v</code></td>\n</tr>\n<tr>\n<td>Thread safety</td>\n<td>Concurrent access under load</td>\n<td>No race conditions, accurate final state</td>\n<td><code>python -m pytest tests/test_token_bucket.py::TestConcurrency -v --timeout=30</code></td>\n</tr>\n<tr>\n<td>Time precision</td>\n<td>Long-running accuracy tests</td>\n<td>&lt;1% deviation from expected token counts</td>\n<td><code>python -m pytest tests/test_token_bucket.py::TestTimePrecision -v</code></td>\n</tr>\n<tr>\n<td>Edge cases</td>\n<td>Clock drift, overflow, invalid config</td>\n<td>Graceful handling, appropriate exceptions</td>\n<td><code>python -m pytest tests/test_token_bucket.py::TestEdgeCases -v</code></td>\n</tr>\n</tbody></table>\n<p><strong>Manual Verification Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test 1: Basic token bucket behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> token_bucket </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenBucket, TokenBucketConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">initial_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should have 5 tokens initially</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Consumed 3 tokens: allowed=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.allowed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, remaining=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.tokens_remaining</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: allowed=True, remaining=2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should deny request for more tokens than available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Tried to consume 5 tokens: allowed=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.allowed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, retry_after=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.retry_after_seconds</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: allowed=False, retry_after=3.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Wait and verify token refill</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">time.sleep(</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"After 2 seconds, consumed 1 token: allowed=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.allowed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, remaining=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.tokens_remaining</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: allowed=True, remaining=~3 (2 original + 2 refilled - 1 consumed)</span></span></code></pre></div>\n\n<p><strong>Success Indicators:</strong></p>\n<ul>\n<li>Token counts are accurate within 1% over extended periods</li>\n<li>Concurrent access produces consistent results across multiple runs  </li>\n<li>Bucket capacity is never exceeded regardless of refill time</li>\n<li><code>retry_after_seconds</code> calculations are accurate within 100ms</li>\n<li>No memory leaks or resource accumulation during long-running tests</li>\n</ul>\n<p><strong>Common Failure Modes to Check:</strong></p>\n<ul>\n<li>Tokens accumulating beyond bucket capacity</li>\n<li>Negative token counts under concurrent access</li>\n<li>Significant drift in token generation rates over time</li>\n<li>Crashes or exceptions during normal operation</li>\n<li>Thread deadlocks or race conditions under load</li>\n</ul>\n<h4 id=\"milestone-2-per-client-rate-limiting-checkpoint\">Milestone 2: Per-Client Rate Limiting Checkpoint</h4>\n<p>After implementing client-specific bucket management, verify that clients are properly isolated and buckets are managed efficiently.</p>\n<p><strong>Automated Test Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Required Tests</th>\n<th>Pass Criteria</th>\n<th>Validation Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client isolation</td>\n<td>Multiple clients with different limits</td>\n<td>Each client gets correct rate limit, no cross-contamination</td>\n<td><code>python -m pytest tests/test_client_tracker.py::TestClientIsolation -v</code></td>\n</tr>\n<tr>\n<td>Bucket lifecycle</td>\n<td>Creation, access, cleanup</td>\n<td>Buckets created on-demand, cleaned up when stale</td>\n<td><code>python -m pytest tests/test_client_tracker.py::TestBucketLifecycle -v</code></td>\n</tr>\n<tr>\n<td>Memory management</td>\n<td>Long-running with many clients</td>\n<td>Memory usage remains bounded, no leaks</td>\n<td><code>python -m pytest tests/test_client_tracker.py::TestMemoryManagement -v --timeout=60</code></td>\n</tr>\n<tr>\n<td>Configuration resolution</td>\n<td>Client overrides, endpoint limits</td>\n<td>Correct config hierarchy applied</td>\n<td><code>python -m pytest tests/test_client_tracker.py::TestConfigResolution -v</code></td>\n</tr>\n</tbody></table>\n<p><strong>Manual Verification Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test 1: Multiple clients get independent rate limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> client_tracker </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ClientBucketTracker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RateLimitConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RateLimitConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    default_limits</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    client_overrides</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"premium_client\"</span><span style=\"color:#E1E4E8\">: TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tracker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ClientBucketTracker(config)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test default client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">bucket1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tracker.get_bucket_for_client(</span><span style=\"color:#9ECBFF\">\"192.168.1.100\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket1.try_consume(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Default client consumed 10 tokens: allowed=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result1.allowed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: allowed=True (uses default 10 capacity)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test premium client  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">bucket2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tracker.get_bucket_for_client(</span><span style=\"color:#9ECBFF\">\"premium_client\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket2.try_consume(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Premium client consumed 30 tokens: allowed=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result2.allowed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: allowed=True (uses 50 capacity override)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify they're different buckets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result3 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket1.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Default client (should be empty): allowed=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result3.allowed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: allowed=False (already consumed 10 from 10 capacity)</span></span></code></pre></div>\n\n<p><strong>Success Indicators:</strong></p>\n<ul>\n<li>Each unique client ID gets its own bucket instance</li>\n<li>Client configuration overrides are applied correctly  </li>\n<li>Stale buckets are removed after cleanup interval</li>\n<li>Memory usage stabilizes even with thousands of clients</li>\n<li>Bucket access is thread-safe under high concurrency</li>\n</ul>\n<p><strong>Troubleshooting Guide:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>All clients share same limits</td>\n<td>Config overrides not working</td>\n<td>Check <code>resolve_bucket_config</code> logic</td>\n<td>Fix configuration resolution hierarchy</td>\n</tr>\n<tr>\n<td>Memory usage keeps growing</td>\n<td>Stale bucket cleanup failing</td>\n<td>Check cleanup thread execution, bucket timestamps</td>\n<td>Fix cleanup logic, reduce cleanup interval</td>\n</tr>\n<tr>\n<td>Concurrent clients get errors</td>\n<td>Race condition in bucket creation</td>\n<td>Add logging around bucket creation</td>\n<td>Add proper locking around bucket map access</td>\n</tr>\n<tr>\n<td>Client IDs inconsistent</td>\n<td>ID extraction/normalization issues</td>\n<td>Log raw client IDs before normalization</td>\n<td>Fix <code>identify_client</code> method implementation</td>\n</tr>\n</tbody></table>\n<h4 id=\"milestone-3-http-middleware-integration-checkpoint\">Milestone 3: HTTP Middleware Integration Checkpoint</h4>\n<p>After implementing HTTP middleware, verify proper integration with web frameworks and correct HTTP protocol behavior.</p>\n<p><strong>Automated Test Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Required Tests</th>\n<th>Pass Criteria</th>\n<th>Integration Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP protocol compliance</td>\n<td>Status codes, headers, response format</td>\n<td>Correct 429 responses, complete rate limit headers</td>\n<td><code>python -m pytest tests/test_middleware.py::TestHTTPCompliance -v</code></td>\n</tr>\n<tr>\n<td>Framework integration</td>\n<td>Flask/Django/FastAPI compatibility</td>\n<td>Middleware integrates without framework modifications</td>\n<td><code>python -m pytest tests/test_middleware.py::TestFrameworkIntegration -v</code></td>\n</tr>\n<tr>\n<td>Request processing</td>\n<td>Various client ID sources</td>\n<td>Correct client identification from IPs, headers, API keys</td>\n<td><code>python -m pytest tests/test_middleware.py::TestRequestProcessing -v</code></td>\n</tr>\n<tr>\n<td>Endpoint handling</td>\n<td>Different routes, path parameters</td>\n<td>Proper endpoint normalization and rate limit application</td>\n<td><code>python -m pytest tests/test_middleware.py::TestEndpointHandling -v</code></td>\n</tr>\n</tbody></table>\n<p><strong>Manual Verification Commands:</strong></p>\n<p>Start a test server with the rate limiting middleware enabled:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test_server.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> flask </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Flask</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> rate_limit_middleware </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RateLimitMiddleware</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">app </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Flask(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">rate_limiter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RateLimitMiddleware()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">rate_limiter.init_app(app)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/api/test'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_endpoint</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"success\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> '__main__'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    app.run(</span><span style=\"color:#FFAB70\">port</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">debug</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p>Test the middleware behavior using curl:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test 1: Normal request should succeed with rate limit headers</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> http://localhost:5000/api/test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 200, X-RateLimit-Limit header, X-RateLimit-Remaining header</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 2: Exceed rate limit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..15}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span><span style=\"color:#B392F0\"> curl</span><span style=\"color:#79B8FF\"> -s</span><span style=\"color:#79B8FF\"> -w</span><span style=\"color:#9ECBFF\"> \"%{http_code}\\n\"</span><span style=\"color:#9ECBFF\"> http://localhost:5000/api/test</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: First ~10 requests return 200, subsequent return 429</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 3: Check 429 response format</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> http://localhost:5000/api/test</span><span style=\"color:#6A737D\">  # (after exceeding limit)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 429, Retry-After header, JSON error body</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 4: Different client should get independent rate limit</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-API-Key: different-client\"</span><span style=\"color:#9ECBFF\"> http://localhost:5000/api/test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 200 (fresh rate limit for this client)</span></span></code></pre></div>\n\n<p><strong>Success Indicators:</strong></p>\n<ul>\n<li>All HTTP responses include proper rate limiting headers</li>\n<li>429 responses include accurate <code>Retry-After</code> header</li>\n<li>Different endpoints can have different rate limits</li>\n<li>Client identification works from multiple sources (IP, headers)</li>\n<li>Middleware doesn&#39;t interfere with normal request processing</li>\n</ul>\n<h4 id=\"milestone-4-distributed-rate-limiting-checkpoint\">Milestone 4: Distributed Rate Limiting Checkpoint</h4>\n<p>After implementing Redis-based distributed coordination, verify consistency across multiple server instances.</p>\n<p><strong>Automated Test Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Required Tests</th>\n<th>Pass Criteria</th>\n<th>Distributed Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cross-server consistency</td>\n<td>Multiple instances, shared client</td>\n<td>Total requests ≤ rate limit across all servers</td>\n<td><code>python -m pytest tests/test_distributed.py::TestConsistency -v</code></td>\n</tr>\n<tr>\n<td>Redis integration</td>\n<td>Lua scripts, atomic operations</td>\n<td>No race conditions, accurate token counts</td>\n<td><code>python -m pytest tests/test_distributed.py::TestRedisIntegration -v</code></td>\n</tr>\n<tr>\n<td>Failure handling</td>\n<td>Redis disconnection, recovery</td>\n<td>Graceful fallback, smooth recovery</td>\n<td><code>python -m pytest tests/test_distributed.py::TestFailureHandling -v</code></td>\n</tr>\n<tr>\n<td>Performance impact</td>\n<td>Latency, throughput under Redis load</td>\n<td>&lt;10ms p95 latency, minimal throughput impact</td>\n<td><code>python -m pytest tests/test_distributed.py::TestPerformance -v</code></td>\n</tr>\n</tbody></table>\n<p><strong>Manual Verification Setup:</strong></p>\n<p>Start multiple server instances sharing the same Redis instance:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Terminal 1: Start Redis</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">redis-server</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 6379</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Terminal 2: Start server instance 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">REDIS_URL</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">redis://localhost:6379</span><span style=\"color:#E1E4E8\"> FLASK_PORT</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">5001</span><span style=\"color:#B392F0\"> python</span><span style=\"color:#9ECBFF\"> server.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Terminal 3: Start server instance 2  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">REDIS_URL</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">redis://localhost:6379</span><span style=\"color:#E1E4E8\"> FLASK_PORT</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">5002</span><span style=\"color:#B392F0\"> python</span><span style=\"color:#9ECBFF\"> server.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Terminal 4: Start server instance 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">REDIS_URL</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">redis://localhost:6379</span><span style=\"color:#E1E4E8\"> FLASK_PORT</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">5003</span><span style=\"color:#B392F0\"> python</span><span style=\"color:#9ECBFF\"> server.py</span></span></code></pre></div>\n\n<p>Test distributed consistency:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test 1: Send requests to different servers for same client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..5}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span><span style=\"color:#B392F0\"> curl</span><span style=\"color:#79B8FF\"> -s</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-Client-ID: test-client\"</span><span style=\"color:#9ECBFF\"> http://localhost:5001/api/test</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..5}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span><span style=\"color:#B392F0\"> curl</span><span style=\"color:#79B8FF\"> -s</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-Client-ID: test-client\"</span><span style=\"color:#9ECBFF\"> http://localhost:5002/api/test</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..5}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span><span style=\"color:#B392F0\"> curl</span><span style=\"color:#79B8FF\"> -s</span><span style=\"color:#79B8FF\"> -H</span><span style=\"color:#9ECBFF\"> \"X-Client-ID: test-client\"</span><span style=\"color:#9ECBFF\"> http://localhost:5003/api/test</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Total allowed ≤ rate limit regardless of which server processed request</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 2: Redis failover behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Stop Redis, send requests, restart Redis</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">sudo</span><span style=\"color:#9ECBFF\"> systemctl</span><span style=\"color:#9ECBFF\"> stop</span><span style=\"color:#9ECBFF\"> redis</span><span style=\"color:#6A737D\">  # or docker stop redis-container</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -s</span><span style=\"color:#79B8FF\"> -w</span><span style=\"color:#9ECBFF\"> \"%{http_code}\\n\"</span><span style=\"color:#9ECBFF\"> http://localhost:5001/api/test</span><span style=\"color:#6A737D\">  # Should still work (fallback)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">sudo</span><span style=\"color:#9ECBFF\"> systemctl</span><span style=\"color:#9ECBFF\"> start</span><span style=\"color:#9ECBFF\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -s</span><span style=\"color:#79B8FF\"> -w</span><span style=\"color:#9ECBFF\"> \"%{http_code}\\n\"</span><span style=\"color:#9ECBFF\"> http://localhost:5001/api/test</span><span style=\"color:#6A737D\">  # Should resume distributed mode</span></span></code></pre></div>\n\n<p><strong>Success Indicators:</strong></p>\n<ul>\n<li>Token consumption is consistent across all server instances</li>\n<li>Redis connection failures trigger local fallback behavior  </li>\n<li>Recovery to distributed mode happens automatically</li>\n<li>Lua scripts execute atomically without race conditions</li>\n<li>Performance impact is minimal (&lt;10ms additional latency)</li>\n</ul>\n<h3 id=\"performance-and-load-testing\">Performance and Load Testing</h3>\n<p>Performance testing validates that the rate limiter maintains accuracy and responsiveness under realistic production loads. Unlike functional testing that verifies correctness, performance testing focuses on behavior under stress, measuring throughput, latency, and accuracy degradation.</p>\n<h4 id=\"load-testing-methodology\">Load Testing Methodology</h4>\n<p>Performance testing requires systematic measurement of rate limiting accuracy, response times, and system resource usage under various load patterns that simulate real-world traffic conditions.</p>\n<p><strong>Load Pattern Categories:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Load Pattern</th>\n<th>Description</th>\n<th>Purpose</th>\n<th>Key Metrics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Steady state</td>\n<td>Constant request rate at 80% of limit</td>\n<td>Validate baseline performance</td>\n<td>P95 latency, accuracy percentage, CPU/memory usage</td>\n</tr>\n<tr>\n<td>Burst traffic</td>\n<td>10x rate limit for short periods</td>\n<td>Test burst handling and recovery</td>\n<td>Burst accommodation, recovery time, queue depth</td>\n</tr>\n<tr>\n<td>Ramp-up</td>\n<td>Gradually increasing request rate</td>\n<td>Find performance breaking points</td>\n<td>Throughput ceiling, accuracy degradation point</td>\n</tr>\n<tr>\n<td>Mixed clients</td>\n<td>Multiple clients with different patterns</td>\n<td>Test resource contention</td>\n<td>Per-client accuracy, cross-client impact</td>\n</tr>\n<tr>\n<td>Long duration</td>\n<td>Hours of continuous load</td>\n<td>Detect memory leaks, drift issues</td>\n<td>Memory stability, long-term accuracy</td>\n</tr>\n<tr>\n<td>Distributed load</td>\n<td>Traffic across multiple server instances</td>\n<td>Validate distributed coordination overhead</td>\n<td>Cross-server consistency, Redis performance impact</td>\n</tr>\n</tbody></table>\n<h4 id=\"single-instance-performance-testing\">Single-Instance Performance Testing</h4>\n<p>Before testing distributed scenarios, establish baseline performance characteristics for a single rate limiter instance to identify bottlenecks and capacity limits.</p>\n<p><strong>Critical Single-Instance Test Scenarios:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Configuration</th>\n<th>Expected Performance</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High-throughput accuracy</td>\n<td>1000 RPS limit, 1200 RPS load</td>\n<td>&gt;99% accuracy, &lt;5ms P95 latency</td>\n<td><code>wrk -t4 -c100 -d60s --rate=1200</code></td>\n</tr>\n<tr>\n<td>Memory efficiency</td>\n<td>10,000 unique clients</td>\n<td>&lt;100MB memory usage, stable over time</td>\n<td>Memory profiling over 30 minutes</td>\n</tr>\n<tr>\n<td>Concurrent client handling</td>\n<td>500 concurrent clients</td>\n<td>Linear scalability, no contention bottlenecks</td>\n<td><code>ab -n10000 -c500</code> with unique client IDs</td>\n</tr>\n<tr>\n<td>Token precision under load</td>\n<td>100 RPS limit, sustained 24 hours</td>\n<td>&lt;0.1% drift from expected throughput</td>\n<td>Long-running accuracy measurement</td>\n</tr>\n<tr>\n<td>Burst handling capacity</td>\n<td>10 RPS limit, 100 token capacity</td>\n<td>Accommodate 100-request burst, recover in 10s</td>\n<td>Burst injection, recovery timing</td>\n</tr>\n</tbody></table>\n<p><strong>Performance Test Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># performance_test.py - Example load testing script</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> aiohttp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimiterLoadTest</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, base_url, target_rps, duration_seconds):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> base_url</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.target_rps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> target_rps</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.duration_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> duration_seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> send_request</span><span style=\"color:#E1E4E8\">(self, session, client_id):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Send single request and record response\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#E1E4E8\"> session.get(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.base_url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/api/test\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                headers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"X-Client-ID\"</span><span style=\"color:#E1E4E8\">: client_id}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> response:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.results[response.status].append(end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> response.status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.results[</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">].append(end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"error\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> run_load_test</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute load test with precise timing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        request_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#F97583\"> /</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.target_rps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        connector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> aiohttp.TCPConnector(</span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#E1E4E8\"> aiohttp.ClientSession(</span><span style=\"color:#FFAB70\">connector</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">connector) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> session:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tasks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.duration_seconds:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                task </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asyncio.create_task(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.send_request(session, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"client-</span><span style=\"color:#79B8FF\">{int</span><span style=\"color:#E1E4E8\">(time.time()</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 100}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tasks.append(task)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> asyncio.sleep(request_interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Prevent task list from growing too large</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tasks) </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    await</span><span style=\"color:#E1E4E8\"> asyncio.gather(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">tasks[:</span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    tasks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tasks[</span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait for remaining tasks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> asyncio.gather(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">tasks)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_results</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze performance metrics\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_requests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(responses) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> responses </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results.values())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.results[</span><span style=\"color:#79B8FF\">200</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> total_requests </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> total_requests </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        all_latencies </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> latencies </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results.values():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            all_latencies.extend(latencies)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> all_latencies:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            all_latencies.sort()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            p95_latency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> all_latencies[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(all_latencies) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.95</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            avg_latency </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(all_latencies) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(all_latencies)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            p95_latency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> avg_latency </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Total Requests: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">total_requests</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Success Rate: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">success_rate</span><span style=\"color:#F97583\">:.2%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Average Latency: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">avg_latency</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"P95 Latency: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">p95_latency</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Status Code Distribution: </span><span style=\"color:#79B8FF\">{dict</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.results)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RateLimiterLoadTest(</span><span style=\"color:#9ECBFF\">\"http://localhost:5000\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">target_rps</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    await</span><span style=\"color:#E1E4E8\"> test.run_load_test()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test.analyze_results()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    asyncio.run(main())</span></span></code></pre></div>\n\n<p><strong>Performance Acceptance Criteria:</strong></p>\n<blockquote>\n<p><strong>Critical Performance Requirements</strong>: The rate limiter must maintain &gt;99% accuracy (allowed requests within 1% of configured limit) while adding &lt;10ms P95 latency overhead to normal request processing. Memory usage should remain stable under sustained load and scale linearly with the number of active clients.</p>\n</blockquote>\n<h4 id=\"distributed-performance-testing\">Distributed Performance Testing</h4>\n<p>Distributed rate limiting introduces coordination overhead that must be measured under realistic multi-server scenarios. These tests verify that distributed consistency doesn&#39;t come at the cost of unacceptable performance degradation.</p>\n<p><strong>Distributed Load Test Architecture:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Role</th>\n<th>Configuration</th>\n<th>Monitoring Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Load generators</td>\n<td>Generate traffic across all servers</td>\n<td>1000 RPS distributed evenly</td>\n<td>Request distribution, client ID consistency</td>\n</tr>\n<tr>\n<td>Server instances</td>\n<td>Process requests with distributed rate limiting</td>\n<td>3-5 instances, shared Redis</td>\n<td>Individual server performance, coordination latency</td>\n</tr>\n<tr>\n<td>Redis cluster</td>\n<td>Shared state storage</td>\n<td>Single instance or cluster</td>\n<td>Operation latency, connection pool usage</td>\n</tr>\n<tr>\n<td>Monitoring system</td>\n<td>Collect metrics across all components</td>\n<td>Prometheus/Grafana or custom</td>\n<td>Cross-server consistency, aggregate throughput</td>\n</tr>\n</tbody></table>\n<p><strong>Critical Distributed Performance Scenarios:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Test Configuration</th>\n<th>Success Criteria</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cross-server consistency under load</td>\n<td>5 servers, 500 RPS each, same client</td>\n<td>Total throughput ≤ 1000 RPS</td>\n<td>&gt;5% over-limit requests, token double-counting</td>\n</tr>\n<tr>\n<td>Redis coordination latency</td>\n<td>Measure Redis operation times</td>\n<td>&lt;5ms P95 for Lua script execution</td>\n<td>&gt;20ms Redis latency, connection timeouts</td>\n</tr>\n<tr>\n<td>Failover performance</td>\n<td>Redis disconnect during peak load</td>\n<td>&lt;10s fallback activation, graceful degradation</td>\n<td>Extended unavailability, inconsistent fallback</td>\n</tr>\n<tr>\n<td>Recovery coordination</td>\n<td>Redis reconnect after 60s outage</td>\n<td>&lt;30s to resume distributed mode</td>\n<td>Split-brain behavior, state inconsistencies</td>\n</tr>\n<tr>\n<td>High client count distribution</td>\n<td>10,000 unique clients across servers</td>\n<td>Linear scaling, no performance cliff</td>\n<td>Memory explosion, coordination bottlenecks</td>\n</tr>\n</tbody></table>\n<p><strong>Distributed Consistency Validation:</strong></p>\n<p>The most critical aspect of distributed performance testing is verifying that rate limiting remains accurate when requests for the same client are processed by different server instances.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># distributed_consistency_test.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> aiohttp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Counter</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DistributedConsistencyTest</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, server_urls, client_id, rate_limit_rps, test_duration):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.server_urls </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> server_urls</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rate_limit_rps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rate_limit_rps</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.test_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> test_duration</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> send_requests_to_servers</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Send requests to different servers for same client\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#E1E4E8\"> aiohttp.ClientSession() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> session:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            request_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.test_duration:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Round-robin across servers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                server_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.server_urls[request_count </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.server_urls)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#E1E4E8\"> session.get(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">server_url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/api/test\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        headers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"X-Client-ID\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.client_id}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    ) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> response:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                        self</span><span style=\"color:#E1E4E8\">.results.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"timestamp\"</span><span style=\"color:#E1E4E8\">: time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"server\"</span><span style=\"color:#E1E4E8\">: server_url,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"status\"</span><span style=\"color:#E1E4E8\">: response.status,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"headers\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">(response.headers)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        })</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.results.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"timestamp\"</span><span style=\"color:#E1E4E8\">: time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"server\"</span><span style=\"color:#E1E4E8\">: server_url,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                request_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Send requests faster than rate limit to test consistency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> asyncio.sleep(</span><span style=\"color:#79B8FF\">0.8</span><span style=\"color:#F97583\"> /</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.rate_limit_rps)  </span><span style=\"color:#6A737D\"># 125% of rate limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_consistency</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate that distributed rate limiting is consistent\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">([r </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> r[</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_requests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.results)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate expected allowed requests (with some tolerance)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_allowed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.rate_limit_rps </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.test_duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected_allowed </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.05</span><span style=\"color:#6A737D\">  # 5% tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        consistency_valid </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            success_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> expected_allowed </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> tolerance </span><span style=\"color:#F97583\">and</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            success_count </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> expected_allowed </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        server_distribution </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Counter(r[</span><span style=\"color:#9ECBFF\">\"server\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> r[</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Total requests: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">total_requests</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Successful requests: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">success_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected allowed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_allowed</span><span style=\"color:#F97583\">:.0f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> ± </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tolerance</span><span style=\"color:#F97583\">:.0f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Consistency valid: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">consistency_valid</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Server distribution: </span><span style=\"color:#79B8FF\">{dict</span><span style=\"color:#E1E4E8\">(server_distribution)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> consistency_valid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage for testing 3-server distributed setup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    servers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"http://localhost:5001\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"http://localhost:5002\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"http://localhost:5003\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DistributedConsistencyTest(servers, </span><span style=\"color:#9ECBFF\">\"test-client\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rate_limit_rps</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">test_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    await</span><span style=\"color:#E1E4E8\"> test.send_requests_to_servers()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test.validate_consistency()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    asyncio.run(main())</span></span></code></pre></div>\n\n<h4 id=\"performance-regression-testing\">Performance Regression Testing</h4>\n<p>Performance regression testing ensures that changes to the rate limiter don&#39;t introduce performance degradation. This requires establishing baseline metrics and automated detection of significant performance changes.</p>\n<p><strong>Performance Baseline Metrics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Baseline Value</th>\n<th>Acceptable Degradation</th>\n<th>Alert Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request processing latency</td>\n<td>&lt;2ms P95</td>\n<td>&lt;50% increase</td>\n<td>&gt;3ms P95</td>\n</tr>\n<tr>\n<td>Rate limiting accuracy</td>\n<td>&gt;99.5%</td>\n<td>&lt;0.5% decrease</td>\n<td>&lt;99%</td>\n</tr>\n<tr>\n<td>Memory usage per client</td>\n<td>&lt;1KB</td>\n<td>&lt;100% increase</td>\n<td>&gt;2KB</td>\n</tr>\n<tr>\n<td>Redis operation latency</td>\n<td>&lt;1ms P95</td>\n<td>&lt;100% increase</td>\n<td>&gt;2ms P95</td>\n</tr>\n<tr>\n<td>Throughput capacity</td>\n<td>10,000 RPS</td>\n<td>&lt;20% decrease</td>\n<td>&lt;8,000 RPS</td>\n</tr>\n<tr>\n<td>CPU utilization</td>\n<td>&lt;10% at 1000 RPS</td>\n<td>&lt;50% increase</td>\n<td>&gt;15%</td>\n</tr>\n</tbody></table>\n<p><strong>Automated Performance Testing:</strong></p>\n<p>Performance tests should run automatically in CI/CD pipelines to catch regressions before deployment. The test suite should include representative workloads and fail builds that significantly degrade performance.</p>\n<blockquote>\n<p><strong>Performance Testing Philosophy</strong>: Performance testing isn&#39;t just about finding the breaking point—it&#39;s about ensuring consistent, predictable behavior under normal operating conditions. A rate limiter that works perfectly at low load but becomes inaccurate or slow under realistic traffic is fundamentally broken for production use.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The testing strategy requires a comprehensive test suite that grows with each milestone, providing both validation of functionality and confidence in system behavior under load.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Testing Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit test framework</td>\n<td><code>pytest</code> with basic fixtures</td>\n<td><code>pytest</code> with <code>pytest-asyncio</code>, <code>pytest-benchmark</code>, custom fixtures</td>\n</tr>\n<tr>\n<td>HTTP testing</td>\n<td><code>requests</code> library with mock servers</td>\n<td><code>aiohttp</code> for async testing, <code>httpx</code> for HTTP/2 support</td>\n</tr>\n<tr>\n<td>Load testing</td>\n<td><code>locust</code> or <code>wrk</code> command line</td>\n<td>Custom async load generators, distributed load testing</td>\n</tr>\n<tr>\n<td>Time mocking</td>\n<td><code>freezegun</code> or <code>time-machine</code></td>\n<td>Custom time providers with dependency injection</td>\n</tr>\n<tr>\n<td>Redis testing</td>\n<td><code>fakeredis</code> for unit tests</td>\n<td>Real Redis instance with Docker, Redis Cluster testing</td>\n</tr>\n<tr>\n<td>Concurrency testing</td>\n<td><code>threading</code> with <code>concurrent.futures</code></td>\n<td><code>asyncio</code> with proper event loop management</td>\n</tr>\n<tr>\n<td>Performance profiling</td>\n<td><code>cProfile</code> and <code>memory_profiler</code></td>\n<td><code>py-spy</code>, <code>pympler</code>, APM integration</td>\n</tr>\n<tr>\n<td>CI/CD integration</td>\n<td>GitHub Actions with basic test runs</td>\n<td>Performance regression detection, benchmark comparisons</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-test-file-structure\">Recommended Test File Structure</h4>\n<p>Organize tests to mirror the component structure and provide clear separation between different types of testing:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tests/\n├── unit/                           # Isolated component tests\n│   ├── test_token_bucket.py        # Core algorithm tests\n│   ├── test_client_tracker.py      # Client management tests  \n│   ├── test_middleware.py          # HTTP middleware tests\n│   └── test_redis_operations.py    # Redis integration tests\n├── integration/                    # Component interaction tests\n│   ├── test_request_flow.py        # End-to-end request processing\n│   ├── test_distributed_coordination.py  # Multi-server scenarios\n│   └── test_configuration.py       # Config loading and resolution\n├── performance/                    # Load and performance tests\n│   ├── test_single_instance_perf.py    # Single server performance\n│   ├── test_distributed_perf.py        # Multi-server performance\n│   └── test_memory_usage.py            # Memory efficiency tests\n├── fixtures/                       # Shared test data and utilities\n│   ├── conftest.py                 # Pytest configuration and fixtures\n│   ├── mock_redis.py               # Redis testing utilities\n│   └── load_generators.py          # Performance testing tools\n└── milestone_verification/         # Milestone-specific validation\n    ├── milestone_1_checks.py       # Token bucket verification\n    ├── milestone_2_checks.py       # Per-client verification\n    ├── milestone_3_checks.py       # Middleware verification\n    └── milestone_4_checks.py       # Distributed verification</code></pre></div>\n\n<h4 id=\"core-test-infrastructure\">Core Test Infrastructure</h4>\n<p>The test infrastructure provides essential utilities for time manipulation, Redis testing, and load generation that all other tests depend on.</p>\n<p><strong>Time Control Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/fixtures/time_control.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> unittest.mock </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> patch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MockTimeProvider</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Controllable time provider for deterministic testing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, start_time</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000000000.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.time_calls </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mock time.time() function\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.time_calls.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_time)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self, seconds):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Advance mock time by specified seconds\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_time </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> reset</span><span style=\"color:#E1E4E8\">(self, new_time</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000000000.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reset mock time to specified value\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_time</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.time_calls.clear()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> controlled_time</span><span style=\"color:#E1E4E8\">(start_time</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000000000.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for time-controlled testing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    time_provider </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MockTimeProvider(start_time)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> patch(</span><span style=\"color:#9ECBFF\">'time.time'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">side_effect</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time_provider.time):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span><span style=\"color:#E1E4E8\"> time_provider</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage in tests:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># with controlled_time() as time_provider:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     bucket = TokenBucket(config)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     time_provider.advance(10.0)  # Advance 10 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     result = bucket.try_consume(5)  # Test with controlled time</span></span></code></pre></div>\n\n<p><strong>Redis Testing Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/fixtures/redis_testing.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> fakeredis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> unittest.mock </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> patch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisTestManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages Redis instances for testing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fake_redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.real_redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_fake_redis</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get in-memory fake Redis for fast unit tests\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fake_redis </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.fake_redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fakeredis.FakeRedis(</span><span style=\"color:#FFAB70\">decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fake_redis.flushall()  </span><span style=\"color:#6A737D\"># Clean state for each test</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fake_redis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_real_redis</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get real Redis connection for integration tests\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.real_redis </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.real_redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis(</span><span style=\"color:#FFAB70\">host</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'localhost'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">port</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">6379</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.real_redis.ping()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.real_redis.flushdb()  </span><span style=\"color:#6A737D\"># Clean test database</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.real_redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.ConnectionError:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pytest.skip(</span><span style=\"color:#9ECBFF\">\"Redis server not available for integration tests\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> mock_redis_connection</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Mock Redis connections with fake Redis\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fake_redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisTestManager().get_fake_redis()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> patch(</span><span style=\"color:#9ECBFF\">'redis.Redis'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> mock_redis_class:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mock_redis_class.return_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fake_redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span><span style=\"color:#E1E4E8\"> fake_redis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage in tests:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># with mock_redis_connection() as redis_client:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     rate_limiter = DistributedRateLimiter(redis_client)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     # Test distributed functionality with fake Redis</span></span></code></pre></div>\n\n<h4 id=\"unit-test-implementation-examples\">Unit Test Implementation Examples</h4>\n<p><strong>Token Bucket Core Algorithm Tests:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/unit/test_token_bucket.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> concurrent.futures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ThreadPoolExecutor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tests.fixtures.time_control </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> controlled_time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> rate_limiter.token_bucket </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenBucket, TokenBucketConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestTokenBucketBasicFunctionality</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test core token bucket algorithm behavior\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_initial_token_allocation</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify bucket starts with correct token count\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">initial_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> result.tokens_remaining </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> result.bucket_capacity </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_token_refill_over_time</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test accurate token generation based on elapsed time\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">initial_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> controlled_time() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> time_provider:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Should have 0 tokens initially</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Advance 2 seconds = 4 tokens generated</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time_provider.advance(</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> result.tokens_remaining </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Advance 3 more seconds = 6 tokens, but capped at capacity 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time_provider.advance(</span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_retry_after_calculation</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify accurate retry timing calculations\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">initial_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> controlled_time():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Need 3 tokens at 1 token/second = 3 seconds wait</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(result.retry_after_seconds </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 3.0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.001</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestTokenBucketConcurrency</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test thread safety under concurrent access\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_concurrent_token_consumption</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify no race conditions during concurrent access\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">initial_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        successful_consumptions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        failed_consumptions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        def</span><span style=\"color:#B392F0\"> consume_tokens</span><span style=\"color:#E1E4E8\">(thread_id):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"Worker function for concurrent testing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> result.allowed:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    successful_consumptions.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"thread-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">thread_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    failed_consumptions.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"thread-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">thread_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                time.sleep(</span><span style=\"color:#79B8FF\">0.001</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Small delay to increase race condition chances</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Run 20 threads, each trying to consume 10 tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> ThreadPoolExecutor(</span><span style=\"color:#FFAB70\">max_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> executor:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            futures </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [executor.submit(consume_tokens, i) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> future </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> futures:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                future.result()  </span><span style=\"color:#6A737D\"># Wait for completion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Should have consumed exactly 100 tokens (initial capacity)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(successful_consumptions) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(failed_consumptions) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#6A737D\">  # Remaining attempts should fail</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Final bucket state should be consistent</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#6A737D\">  # No tokens remaining</span></span></code></pre></div>\n\n<h4 id=\"milestone-verification-implementation\">Milestone Verification Implementation</h4>\n<p><strong>Milestone 1 Token Bucket Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/milestone_verification/milestone_1_checks.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> rate_limiter.token_bucket </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenBucket, TokenBucketConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_milestone_1_acceptance_criteria</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive verification of Milestone 1 requirements\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 1: Configurable bucket parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">initial_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> bucket.capacity </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 20</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> bucket.refill_rate </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2.5</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add method to check current token count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 2: Token consumption with availability check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.consumed_tokens </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 3: Denial when insufficient tokens</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket.try_consume(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Only 5 tokens remaining</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.retry_after_seconds </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 4: Burst handling up to capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    full_bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">initial_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> full_bucket.try_consume(</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Consume entire capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.allowed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.tokens_remaining </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 5: Thread safety verification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shared_bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TokenBucket(TokenBucketConfig(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refill_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">initial_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> worker</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shared_bucket.try_consume(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            results.append(result.allowed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    threads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">worker) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> thread </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> threads:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> thread </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> threads:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread.join()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Exactly 100 successful consumptions expected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(results) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> run_milestone_1_manual_verification</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manual verification steps for Milestone 1\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=== Milestone 1 Manual Verification ===\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement manual test cases that developers can run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add timing-based tests that require manual observation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include performance benchmarks for single-threaded operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_milestone_1_acceptance_criteria()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_milestone_1_manual_verification()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Milestone 1 verification completed successfully!\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"performance-test-implementation\">Performance Test Implementation</h4>\n<p><strong>Load Testing Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/performance/load_test_framework.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> aiohttp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoadTestResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Results from load testing execution\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_requests: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successful_requests: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failed_requests: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    average_latency: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p95_latency: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p99_latency: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    requests_per_second: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status_code_distribution: Dict[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimiterLoadTester</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive load testing for rate limiter\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, target_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, rate_limit_rps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.target_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> target_url</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rate_limit_rps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rate_limit_rps</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> execute_load_test</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        request_rate: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        duration_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        concurrent_clients: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> LoadTestResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute load test with specified parameters\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        connector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> aiohttp.TCPConnector(</span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">concurrent_clients </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> aiohttp.ClientTimeout(</span><span style=\"color:#FFAB70\">total</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#E1E4E8\"> aiohttp.ClientSession(</span><span style=\"color:#FFAB70\">connector</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">connector, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timeout) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> session:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            semaphore </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asyncio.Semaphore(concurrent_clients)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tasks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            request_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> duration_seconds:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create request task with proper client ID rotation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement precise request timing to maintain target rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add request latency measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle different response codes appropriately</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate next request time to maintain steady rate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> asyncio.sleep(</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> request_rate)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Wait for all pending requests to complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Process results and calculate statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return comprehensive LoadTestResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> LoadTestResult(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                total_requests</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                successful_requests</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                failed_requests</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                average_latency</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                p95_latency</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                p99_latency</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                requests_per_second</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                error_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status_code_distribution</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_rate_limiting_accuracy</span><span style=\"color:#E1E4E8\">(self, result: LoadTestResult) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate that rate limiting was accurate within tolerance\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_successful </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.rate_limit_rps </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (result.total_requests </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> result.requests_per_second)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected_successful </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.05</span><span style=\"color:#6A737D\">  # 5% tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result.successful_requests </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> expected_successful </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> tolerance </span><span style=\"color:#F97583\">and</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result.successful_requests </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> expected_successful </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage example for comprehensive testing:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># async def main():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     tester = RateLimiterLoadTester(\"http://localhost:5000/api/test\", rate_limit_rps=100)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     # Test at rate limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     result = await tester.execute_load_test(request_rate=100, duration_seconds=60)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     assert tester.validate_rate_limiting_accuracy(result)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     # Test above rate limit  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     result = await tester.execute_load_test(request_rate=150, duration_seconds=60)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     assert tester.validate_rate_limiting_accuracy(result)</span></span></code></pre></div>\n\n\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - debugging skills are essential from basic token bucket implementation through distributed rate limiting, with complexity increasing as components are added</p>\n</blockquote>\n<p>Think of debugging a rate limiter like being a detective investigating a crime scene. You have symptoms (the observable behavior), evidence (logs, metrics, and state), and suspects (potential root causes). The key is methodically gathering evidence, forming hypotheses, and systematically ruling out suspects until you find the true culprit. Unlike a simple CRUD application where bugs are often straightforward, rate limiters involve timing, concurrency, and distributed state - making debugging more like solving a complex mystery with multiple moving parts.</p>\n<p>The challenge with rate limiting bugs is that they often manifest under load, involve timing-sensitive race conditions, and can have subtle symptoms that seem unrelated to their root causes. A token calculation bug might appear as inconsistent rate limiting under high concurrency. A Redis connection issue might manifest as requests being allowed when they should be blocked. Clock drift between servers might cause tokens to refill too quickly or slowly, leading to rate limits that seem &quot;loose&quot; or &quot;strict&quot; compared to configuration.</p>\n<p>This debugging guide provides a systematic approach to identifying, diagnosing, and fixing the most common issues encountered when implementing rate limiting systems. We&#39;ll cover the typical bugs that arise in each milestone, provide concrete diagnostic techniques, and offer proven solutions based on common implementation patterns.</p>\n<h3 id=\"common-implementation-bugs\">Common Implementation Bugs</h3>\n<p>Understanding the most frequent bugs helps you recognize patterns and debug more efficiently. Rate limiting bugs typically fall into several categories: concurrency issues, timing and calculation errors, client identification problems, and distributed consistency failures.</p>\n<h4 id=\"token-bucket-race-conditions\">Token Bucket Race Conditions</h4>\n<p>Race conditions in token bucket operations are among the most insidious bugs because they&#39;re timing-dependent and may not manifest during single-threaded testing.</p>\n<p>⚠️ <strong>Pitfall: Unprotected Token Refill and Consumption</strong></p>\n<p>The most common race condition occurs when token refill and consumption operations aren&#39;t atomic. Consider two threads accessing the same <code>TokenBucket</code> simultaneously - one performing a refill calculation while another consumes tokens. Without proper synchronization, you might see:</p>\n<ul>\n<li>Thread A reads current tokens (100) and calculates refill amount (50 new tokens)</li>\n<li>Thread B reads current tokens (100) and attempts to consume 75 tokens</li>\n<li>Thread A writes new token count (150)</li>\n<li>Thread B writes remaining tokens after consumption (25)</li>\n<li>Final state is 25 tokens instead of the correct 75 tokens</li>\n</ul>\n<p>This manifests as inconsistent rate limiting where clients sometimes get through when they should be blocked, or get blocked when they should be allowed through.</p>\n<p><strong>Diagnosis</strong>: Enable debug logging for all token bucket operations and look for token counts that don&#39;t match expected calculations. Run concurrent requests against the same client and observe if the token consumption results are consistent with the configured limits.</p>\n<p><strong>Fix</strong>: Ensure all token bucket operations use proper locking or atomic operations. In Python, use <code>threading.Lock()</code> to protect the entire <code>try_consume</code> operation including refill calculation.</p>\n<p>⚠️ <strong>Pitfall: Time-of-Check vs Time-of-Use in Token Calculations</strong></p>\n<p>Another subtle race condition occurs when the current time is read at the beginning of a function but used much later after other operations. The time value becomes stale, leading to incorrect refill calculations.</p>\n<p><strong>Diagnosis</strong>: Add timestamp logging to token refill operations and compare the time used for calculation with the actual system time when the operation completes. Large differences indicate stale time usage.</p>\n<p><strong>Fix</strong>: Read the current time as late as possible in the operation, ideally just before the actual calculation that needs it.</p>\n<h4 id=\"token-calculation-arithmetic-errors\">Token Calculation Arithmetic Errors</h4>\n<p>Mathematical precision and overflow issues in token calculations can cause subtle but significant rate limiting failures.</p>\n<p>⚠️ <strong>Pitfall: Floating Point Precision in Refill Calculations</strong></p>\n<p>When calculating tokens to add based on elapsed time and refill rate, floating point precision errors can accumulate. A refill rate of 10.3 tokens per second over 0.1 seconds should add 1.03 tokens, but floating point math might yield 1.0299999999999998 tokens. When truncated to integers, this becomes 1 token instead of the expected 1 token - seemingly correct, but the precision loss accumulates over time.</p>\n<p><strong>Diagnosis</strong>: Log the exact floating point values in token calculations, including intermediate steps. Run the rate limiter for extended periods and compare actual token generation rates with expected rates.</p>\n<p><strong>Fix</strong>: Use decimal arithmetic for precise calculations or implement token calculation using integer arithmetic with a time granularity that avoids precision issues.</p>\n<p>⚠️ <strong>Pitfall: Integer Overflow in Token Accumulation</strong></p>\n<p>When a token bucket hasn&#39;t been accessed for a very long time, the refill calculation might try to add an enormous number of tokens, potentially causing integer overflow. This can result in negative token counts or wrapping to very small positive numbers.</p>\n<p><strong>Diagnosis</strong>: Test with very large time gaps (simulate a bucket that hasn&#39;t been accessed for days) and observe token count calculations. Monitor for negative token counts or unexpectedly small values after long idle periods.</p>\n<p><strong>Fix</strong>: Cap the maximum tokens that can be added in a single refill operation to the bucket capacity minus current tokens. Never allow more tokens than the bucket can hold.</p>\n<h4 id=\"client-identification-and-bucket-management-issues\">Client Identification and Bucket Management Issues</h4>\n<p>Problems with identifying clients or managing per-client buckets can lead to rate limits being applied incorrectly or not at all.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Client ID Normalization</strong></p>\n<p>Different request processing paths might normalize client identifiers differently. An IP address might be extracted as &quot;192.168.1.100&quot; in some code paths and &quot;192.168.001.100&quot; in others, leading to separate buckets for the same client.</p>\n<p><strong>Diagnosis</strong>: Log all client ID extraction operations and look for the same logical client appearing with different identifier strings. Monitor bucket creation rates - if it&#39;s consistently higher than expected unique client rates, you likely have normalization issues.</p>\n<p><strong>Fix</strong>: Implement a centralized <code>normalize_client_id()</code> function that all code paths use. For IP addresses, use standard library functions that ensure consistent formatting.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks from Never-Cleaned Buckets</strong></p>\n<p>If the bucket cleanup process fails or isn&#39;t aggressive enough, memory usage will grow unboundedly as more unique clients access the system. This is particularly problematic in systems that see many one-time clients (like public APIs).</p>\n<p><strong>Diagnosis</strong>: Monitor memory usage over time and track the number of stored buckets. If buckets grow continuously without cleanup, or cleanup runs but doesn&#39;t reduce bucket counts, you have a leak.</p>\n<p><strong>Fix</strong>: Ensure cleanup runs regularly and aggressively removes stale buckets. Consider implementing LRU eviction as a backup mechanism when bucket count exceeds memory limits.</p>\n<h4 id=\"distributed-consistency-problems\">Distributed Consistency Problems</h4>\n<p>When scaling to multiple servers with Redis-backed storage, additional complexity introduces new failure modes.</p>\n<p>⚠️ <strong>Pitfall: Non-Atomic Redis Operations</strong></p>\n<p>Performing token bucket operations as separate Redis commands (read current tokens, calculate new count, write new count) creates race conditions between servers. Two servers might simultaneously read the same token count, both decide to allow a request, and both update the count, effectively allowing twice the intended rate.</p>\n<p><strong>Diagnosis</strong>: Enable Redis command logging and look for interleaved read/write patterns from different servers accessing the same keys. Monitor rate limiting accuracy under high concurrent load from multiple servers.</p>\n<p><strong>Fix</strong>: Use Redis Lua scripts to ensure atomic read-modify-write operations. All token consumption logic should execute as a single atomic script.</p>\n<p>⚠️ <strong>Pitfall: Clock Drift Between Servers</strong></p>\n<p>Different servers having clocks that drift apart can cause inconsistent token refill rates. A server with a fast clock will add tokens more quickly than configured, while a server with a slow clock will add tokens too slowly.</p>\n<p><strong>Diagnosis</strong>: Compare system time across all servers and monitor token refill rates from each server. If servers show different effective rates for the same bucket, clock drift is likely the cause.</p>\n<p><strong>Fix</strong>: Implement NTP synchronization across all servers and add clock drift detection to your monitoring. Consider using Redis-based timestamps for token calculations instead of local server time.</p>\n<h3 id=\"debugging-techniques-and-tools\">Debugging Techniques and Tools</h3>\n<p>Effective debugging requires the right tools and systematic approaches. Rate limiting bugs often require observing system behavior over time and under load, making traditional debugger breakpoints less effective than logging and monitoring-based approaches.</p>\n<h4 id=\"comprehensive-logging-strategy\">Comprehensive Logging Strategy</h4>\n<p>Strategic logging is your most powerful debugging tool for rate limiting systems. The key is logging the right information without overwhelming the system with too much data.</p>\n<p><strong>Token Bucket Operation Logging</strong></p>\n<p>Every token bucket operation should log its inputs, calculations, and results. This includes:</p>\n<table>\n<thead>\n<tr>\n<th>Log Field</th>\n<th>Description</th>\n<th>Example Value</th>\n<th>When to Log</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>client_id</code></td>\n<td>Client identifier</td>\n<td><code>&quot;192.168.1.100&quot;</code></td>\n<td>Every operation</td>\n</tr>\n<tr>\n<td><code>endpoint</code></td>\n<td>API endpoint being accessed</td>\n<td><code>&quot;/api/v1/users&quot;</code></td>\n<td>Every operation</td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td>Current system time</td>\n<td><code>1640995200.123</code></td>\n<td>Every operation</td>\n</tr>\n<tr>\n<td><code>tokens_requested</code></td>\n<td>Number of tokens requested</td>\n<td><code>1</code></td>\n<td>Every operation</td>\n</tr>\n<tr>\n<td><code>tokens_before</code></td>\n<td>Token count before operation</td>\n<td><code>42</code></td>\n<td>Before consumption</td>\n</tr>\n<tr>\n<td><code>tokens_after</code></td>\n<td>Token count after operation</td>\n<td><code>41</code></td>\n<td>After consumption</td>\n</tr>\n<tr>\n<td><code>time_elapsed</code></td>\n<td>Time since last refill</td>\n<td><code>0.5</code></td>\n<td>During refill</td>\n</tr>\n<tr>\n<td><code>tokens_added</code></td>\n<td>Tokens added in refill</td>\n<td><code>5</code></td>\n<td>During refill</td>\n</tr>\n<tr>\n<td><code>bucket_capacity</code></td>\n<td>Maximum bucket size</td>\n<td><code>100</code></td>\n<td>Every operation</td>\n</tr>\n<tr>\n<td><code>refill_rate</code></td>\n<td>Configured refill rate</td>\n<td><code>10.0</code></td>\n<td>Every operation</td>\n</tr>\n<tr>\n<td><code>operation_result</code></td>\n<td>Allowed or denied</td>\n<td><code>&quot;ALLOWED&quot;</code></td>\n<td>Every operation</td>\n</tr>\n</tbody></table>\n<p><strong>Client Identification Logging</strong></p>\n<p>Since client identification problems are common, log every step of the client ID extraction process:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>DEBUG: Extracting client ID from request\nDEBUG: Request headers: {'X-API-Key': 'abc123', 'X-Forwarded-For': '192.168.1.100'}\nDEBUG: Using IP_ADDRESS strategy\nDEBUG: Raw IP from X-Forwarded-For: '192.168.1.100'\nDEBUG: Normalized client ID: '192.168.1.100'\nDEBUG: Resolved to bucket key: 'bucket:192.168.1.100:/api/users'</code></pre></div>\n\n<p><strong>Distributed Operation Logging</strong></p>\n<p>For Redis-based distributed rate limiting, log all distributed operations including fallback scenarios:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>DEBUG: Attempting Redis token consumption for client_id=192.168.1.100\nDEBUG: Redis Lua script execution - tokens_requested=1, current_tokens=50\nDEBUG: Redis operation successful - tokens_remaining=49, allowed=true</code></pre></div>\n\n<p>When Redis failures occur:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ERROR: Redis operation failed: ConnectionError: Connection refused\nINFO: Falling back to local bucket for client_id=192.168.1.100\nDEBUG: Local fallback bucket - conservative_rate=5.0 (50% of configured 10.0)</code></pre></div>\n\n<h4 id=\"redis-inspection-techniques\">Redis Inspection Techniques</h4>\n<p>Redis provides powerful introspection capabilities for debugging distributed rate limiting issues.</p>\n<p><strong>Monitoring Token Bucket Keys</strong></p>\n<p>Use Redis commands to inspect the current state of token buckets:</p>\n<table>\n<thead>\n<tr>\n<th>Redis Command</th>\n<th>Purpose</th>\n<th>Example Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>KEYS bucket:*</code></td>\n<td>List all bucket keys</td>\n<td>Find all active client buckets</td>\n</tr>\n<tr>\n<td><code>HGETALL bucket:192.168.1.100</code></td>\n<td>Get bucket state</td>\n<td>See current tokens, last update time</td>\n</tr>\n<tr>\n<td><code>TTL bucket:192.168.1.100</code></td>\n<td>Check key expiration</td>\n<td>Verify cleanup is working</td>\n</tr>\n<tr>\n<td><code>MONITOR</code></td>\n<td>Watch all Redis commands</td>\n<td>See real-time bucket operations</td>\n</tr>\n<tr>\n<td><code>INFO memory</code></td>\n<td>Check Redis memory usage</td>\n<td>Monitor for memory leaks</td>\n</tr>\n<tr>\n<td><code>CLIENT LIST</code></td>\n<td>See connected clients</td>\n<td>Identify which servers are active</td>\n</tr>\n</tbody></table>\n<p><strong>Lua Script Debugging</strong></p>\n<p>Redis Lua scripts can be debugged by adding logging within the script:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">lua</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Add debugging output to Lua scripts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">redis.</span><span style=\"color:#79B8FF\">log</span><span style=\"color:#E1E4E8\">(redis.</span><span style=\"color:#B392F0\">LOG_WARNING</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Token consumption: key=\" </span><span style=\"color:#F97583\">..</span><span style=\"color:#E1E4E8\"> KEYS[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">..</span><span style=\"color:#9ECBFF\"> \", requested=\" </span><span style=\"color:#F97583\">..</span><span style=\"color:#E1E4E8\"> ARGV[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">local</span><span style=\"color:#E1E4E8\"> current_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.</span><span style=\"color:#79B8FF\">call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'HGET'</span><span style=\"color:#E1E4E8\">, KEYS[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#9ECBFF\">'tokens'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> bucket_capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">redis.</span><span style=\"color:#79B8FF\">log</span><span style=\"color:#E1E4E8\">(redis.</span><span style=\"color:#B392F0\">LOG_WARNING</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Current tokens: \" </span><span style=\"color:#F97583\">..</span><span style=\"color:#E1E4E8\"> current_tokens)</span></span></code></pre></div>\n\n<p>These logs appear in the Redis server logs and help debug atomic operations.</p>\n<h4 id=\"concurrency-debugging-approaches\">Concurrency Debugging Approaches</h4>\n<p>Rate limiting bugs often involve race conditions that require special debugging techniques.</p>\n<p><strong>Load Testing with Controlled Concurrency</strong></p>\n<p>Create test scenarios that expose race conditions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example load test that exposes race conditions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> concurrent.futures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> requests</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> concurrent_request_test</span><span style=\"color:#E1E4E8\">(client_id, endpoint, num_threads</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, requests_per_thread</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Send concurrent requests to expose race conditions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> make_request</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        headers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'X-Client-ID'</span><span style=\"color:#E1E4E8\">: client_id}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> requests.get(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'http://localhost:8000</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">endpoint</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">headers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">headers)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> response.status_code, response.headers.get(</span><span style=\"color:#9ECBFF\">'X-RateLimit-Remaining'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Send all requests simultaneously</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> concurrent.futures.ThreadPoolExecutor(</span><span style=\"color:#FFAB70\">max_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">num_threads) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> executor:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        futures </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(num_threads):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(requests_per_thread):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                futures.append(executor.submit(make_request))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [future.result() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> future </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> futures]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Analyze results for consistency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowed_requests </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [r </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> results </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> r[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    denied_requests </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [r </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> results </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> r[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 429</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'total_requests'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(results),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'allowed_count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(allowed_requests),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'denied_count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(denied_requests),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'remaining_tokens'</span><span style=\"color:#E1E4E8\">: [r[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> allowed_requests </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> r[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span></code></pre></div>\n\n<p><strong>Thread-Safe State Verification</strong></p>\n<p>Add verification code that checks for impossible states:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenBucket</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, tokens_requested):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:  </span><span style=\"color:#6A737D\"># Ensure atomic operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # ... normal token bucket logic ...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Verification: tokens should never be negative</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_tokens </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Negative tokens detected: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.current_tokens</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Verification: tokens should never exceed capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_tokens </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.capacity, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Tokens exceed capacity: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.current_tokens</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> > </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.capacity</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> result</span></span></code></pre></div>\n\n<h4 id=\"performance-profiling-for-rate-limiters\">Performance Profiling for Rate Limiters</h4>\n<p>Rate limiting performance issues can cause cascading problems throughout your system.</p>\n<p><strong>Latency Measurement</strong></p>\n<p>Track the time spent in rate limiting operations:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_operation</span><span style=\"color:#E1E4E8\">(self, operation_name, func, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> func(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.operation_times.append((operation_name, duration))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Log slow operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> duration </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># 10ms threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"SLOW OPERATION: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> took </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">duration</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_performance_stats</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_times:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_times]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(times),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'mean'</span><span style=\"color:#E1E4E8\">: statistics.mean(times),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'median'</span><span style=\"color:#E1E4E8\">: statistics.median(times),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'p95'</span><span style=\"color:#E1E4E8\">: statistics.quantiles(times, </span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">18</span><span style=\"color:#E1E4E8\">],  </span><span style=\"color:#6A737D\"># 95th percentile</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'max'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(times)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<h3 id=\"symptom-cause-fix-reference\">Symptom-Cause-Fix Reference</h3>\n<p>This reference table maps observable symptoms to their likely root causes and provides specific diagnostic steps and fixes.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate limits seem &quot;loose&quot; - too many requests allowed</td>\n<td>Clock drift making refill rate too fast</td>\n<td>Compare server clocks, monitor token refill rates across servers</td>\n<td>Implement NTP sync, use Redis timestamps for calculations</td>\n</tr>\n<tr>\n<td>Rate limits seem &quot;strict&quot; - requests blocked unexpectedly</td>\n<td>Clock drift making refill rate too slow</td>\n<td>Check if token refill rates are slower than configured</td>\n<td>Sync server clocks, verify time zone consistency</td>\n</tr>\n<tr>\n<td>Inconsistent rate limiting under high concurrency</td>\n<td>Race conditions in token bucket operations</td>\n<td>Enable debug logging, run concurrent load tests</td>\n<td>Add proper locking around all bucket operations</td>\n</tr>\n<tr>\n<td>Memory usage grows continuously</td>\n<td>Bucket cleanup not working or too conservative</td>\n<td>Monitor bucket count over time, check cleanup logs</td>\n<td>Fix cleanup logic, implement LRU eviction</td>\n</tr>\n<tr>\n<td>Rate limiter allows massive bursts occasionally</td>\n<td>Integer overflow in token calculations</td>\n<td>Test with very long idle periods, check for negative tokens</td>\n<td>Cap token refill to bucket capacity, use proper integer types</td>\n</tr>\n<tr>\n<td>Different clients get same rate limits</td>\n<td>Client ID normalization inconsistencies</td>\n<td>Log client ID extraction for same logical client</td>\n<td>Implement centralized client ID normalization</td>\n</tr>\n<tr>\n<td>Redis errors but no fallback behavior</td>\n<td>Circuit breaker not triggering or local fallback disabled</td>\n<td>Check Redis connection status and fallback logs</td>\n<td>Verify circuit breaker configuration and fallback implementation</td>\n</tr>\n<tr>\n<td>Rate limits work locally but fail distributed</td>\n<td>Non-atomic Redis operations</td>\n<td>Enable Redis command logging, look for interleaved operations</td>\n<td>Implement atomic Lua scripts for all token operations</td>\n</tr>\n<tr>\n<td>Requests hang or timeout in rate limiter</td>\n<td>Deadlock in locking or Redis connection issues</td>\n<td>Check for long-running lock acquisitions, Redis connection health</td>\n<td>Review locking strategy, implement connection timeouts</td>\n</tr>\n<tr>\n<td>Rate limiter has high latency impact</td>\n<td>Inefficient Redis operations or excessive locking</td>\n<td>Profile operation times, measure Redis round-trip times</td>\n<td>Optimize Redis operations, reduce lock contention</td>\n</tr>\n<tr>\n<td>Some clients bypass rate limits entirely</td>\n<td>Client identification returning empty or default IDs</td>\n<td>Log all client ID extractions, especially edge cases</td>\n<td>Add validation to client ID extraction, handle missing headers</td>\n</tr>\n<tr>\n<td>Token counts don&#39;t match expected values</td>\n<td>Floating point precision errors in calculations</td>\n<td>Log exact floating point values in token math</td>\n<td>Use decimal arithmetic or integer-based calculations</td>\n</tr>\n<tr>\n<td>Rate limits reset unexpectedly</td>\n<td>Bucket expiration or cleanup happening too aggressively</td>\n<td>Monitor bucket TTL values and cleanup operations</td>\n<td>Adjust bucket expiration times and cleanup thresholds</td>\n</tr>\n<tr>\n<td>429 responses missing proper headers</td>\n<td>Middleware not setting rate limit headers correctly</td>\n<td>Check HTTP response headers in 429 responses</td>\n<td>Fix header generation in rate limit response building</td>\n</tr>\n<tr>\n<td>Rate limits work in staging but not production</td>\n<td>Configuration differences or load-related race conditions</td>\n<td>Compare configs, test with production-level load</td>\n<td>Ensure config consistency, load test thoroughly</td>\n</tr>\n<tr>\n<td>Distributed rate limiting inconsistent across servers</td>\n<td>Different server configurations or Redis connection issues</td>\n<td>Check server configs and Redis connectivity from all servers</td>\n<td>Standardize configurations, verify Redis connectivity</td>\n</tr>\n</tbody></table>\n<h4 id=\"advanced-debugging-scenarios\">Advanced Debugging Scenarios</h4>\n<p>Some debugging scenarios require combining multiple techniques and investigating complex interactions.</p>\n<p><strong>Debugging Clock Drift Issues</strong></p>\n<p>Clock drift between servers is particularly tricky because it affects token refill rates gradually over time:</p>\n<ol>\n<li><strong>Detection</strong>: Set up monitoring that compares effective token refill rates across servers for the same bucket</li>\n<li><strong>Measurement</strong>: Log system timestamps from each server when they perform token refill operations</li>\n<li><strong>Analysis</strong>: Compare the timestamps to identify which servers have drifting clocks</li>\n<li><strong>Verification</strong>: After fixing clock sync, monitor refill rates to ensure consistency</li>\n</ol>\n<p><strong>Debugging Memory Leaks in Bucket Storage</strong></p>\n<p>Bucket cleanup failures can be subtle and may only manifest under specific conditions:</p>\n<ol>\n<li><strong>Monitoring</strong>: Track bucket count, memory usage, and cleanup operation success rates over time</li>\n<li><strong>Investigation</strong>: Identify which buckets are not being cleaned up by examining last access times</li>\n<li><strong>Root Cause</strong>: Determine if the cleanup process is failing, not running, or using wrong criteria</li>\n<li><strong>Testing</strong>: Create test scenarios with many short-lived clients to verify cleanup works correctly</li>\n</ol>\n<p><strong>Debugging Lua Script Atomicity Issues</strong></p>\n<p>When Redis Lua scripts don&#39;t behave atomically as expected:</p>\n<ol>\n<li><strong>Script Verification</strong>: Test Lua scripts in isolation with controlled inputs to verify logic</li>\n<li><strong>Concurrency Testing</strong>: Run high-concurrency tests against the same Redis keys to expose race conditions</li>\n<li><strong>Redis Monitoring</strong>: Use Redis MONITOR command to observe the actual sequence of operations</li>\n<li><strong>Error Handling</strong>: Ensure Lua scripts handle all error conditions correctly without leaving inconsistent state</li>\n</ol>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The debugging capabilities need to be built into your rate limiter from the beginning rather than added after problems occur. This section provides the infrastructure for comprehensive debugging support.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logging</td>\n<td>Python <code>logging</code> module with structured JSON</td>\n<td>ELK stack with distributed tracing</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Simple metrics in log files</td>\n<td>Prometheus + Grafana dashboards</td>\n</tr>\n<tr>\n<td>Redis Debugging</td>\n<td>Redis CLI with manual inspection</td>\n<td>Redis monitoring tools like RedisInsight</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Python <code>concurrent.futures</code> with custom scripts</td>\n<td>JMeter or k6 with comprehensive scenarios</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td>Python <code>cProfile</code> with timing decorators</td>\n<td>APM tools like New Relic or DataDog</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>Organize debugging and monitoring code to support both development and production troubleshooting:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  src/rate_limiter/\n    core/\n      token_bucket.py           ← Core algorithm with debug hooks\n      client_tracker.py         ← Client management with monitoring\n    middleware/\n      flask_middleware.py       ← HTTP middleware with request tracing\n    distributed/\n      redis_storage.py          ← Redis operations with error tracking\n    debugging/\n      __init__.py\n      logger.py                 ← Centralized logging configuration\n      performance_tracker.py    ← Operation timing and profiling\n      load_tester.py           ← Comprehensive load testing utilities\n      redis_inspector.py       ← Redis debugging and inspection tools\n    monitoring/\n      __init__.py\n      metrics.py               ← Metrics collection and reporting\n      health_checks.py         ← System health monitoring\n  tests/\n    debugging/\n      test_race_conditions.py  ← Concurrency-focused tests\n      test_clock_drift.py      ← Time-related issue tests\n  scripts/\n    debug_redis.py            ← Redis inspection scripts\n    load_test.py              ← Load testing orchestration</code></pre></div>\n\n<h4 id=\"debugging-infrastructure-starter-code\">Debugging Infrastructure Starter Code</h4>\n<p>Here&#39;s the complete debugging infrastructure that provides comprehensive logging and monitoring capabilities:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/rate_limiter/debugging/logger.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> functools </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> wraps</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimiterLogger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Centralized logging for rate limiter with structured output.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, log_level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">INFO</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.component_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"rate_limiter.</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">component_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.setLevel(log_level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create structured formatter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        formatter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.Formatter(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '</span><span style=\"color:#79B8FF\">%(asctime)s</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">%(name)s</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">%(levelname)s</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">%(message)s</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Console handler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        console_handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.StreamHandler()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        console_handler.setFormatter(formatter)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.addHandler(console_handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_token_operation</span><span style=\"color:#E1E4E8\">(self, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log token bucket operations with full context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'operation'</span><span style=\"color:#E1E4E8\">: operation,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'client_id'</span><span style=\"color:#E1E4E8\">: client_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'component'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.component_name,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            **</span><span style=\"color:#E1E4E8\">kwargs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"TOKEN_OP: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">json.dumps(log_data)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_client_identification</span><span style=\"color:#E1E4E8\">(self, raw_data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], result: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log client identification process.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'operation'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'client_identification'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'raw_headers'</span><span style=\"color:#E1E4E8\">: raw_data.get(</span><span style=\"color:#9ECBFF\">'headers'</span><span style=\"color:#E1E4E8\">, {}),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'client_id_result'</span><span style=\"color:#E1E4E8\">: result,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.debug(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"CLIENT_ID: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">json.dumps(log_data)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_redis_operation</span><span style=\"color:#E1E4E8\">(self, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, success: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log Redis operations for distributed debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'operation'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'redis_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'key'</span><span style=\"color:#E1E4E8\">: key,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'success'</span><span style=\"color:#E1E4E8\">: success,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            **</span><span style=\"color:#E1E4E8\">kwargs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        level </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">INFO</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> success </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.log(level, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"REDIS_OP: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">json.dumps(log_data)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> debug_timing</span><span style=\"color:#E1E4E8\">(logger: RateLimiterLogger):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Decorator to measure and log operation timing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> decorator</span><span style=\"color:#E1E4E8\">(func):</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        @wraps</span><span style=\"color:#E1E4E8\">(func)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        def</span><span style=\"color:#B392F0\"> wrapper</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> func(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.logger.debug(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"TIMING: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">func.</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\"> took </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">duration</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> wrapper</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> decorator</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/rate_limiter/debugging/performance_tracker.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict, deque</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Track operation performance and detect anomalies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_samples</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_times: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, deque] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">max_samples))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record timing for an operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.operation_times[operation_name].append(duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_stats</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get performance statistics for an operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            times </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.operation_times[operation_name])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> times:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(times),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'mean'</span><span style=\"color:#E1E4E8\">: statistics.mean(times),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'median'</span><span style=\"color:#E1E4E8\">: statistics.median(times),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'std_dev'</span><span style=\"color:#E1E4E8\">: statistics.stdev(times) </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(times) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'min'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">min</span><span style=\"color:#E1E4E8\">(times),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'max'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(times),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'p95'</span><span style=\"color:#E1E4E8\">: statistics.quantiles(times, </span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">18</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(times) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 20</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(times)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager to measure operation timing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> TimingContext(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">, operation_name)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TimingContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for measuring operation timing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tracker: PerformanceTracker, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tracker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tracker</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operation_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __enter__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __exit__</span><span style=\"color:#E1E4E8\">(self, exc_type, exc_val, exc_tb):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_time</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.tracker.record_operation(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.operation_name, duration)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-debugging-skeleton\">Core Logic Debugging Skeleton</h4>\n<p>Here&#39;s how to instrument the core token bucket implementation with comprehensive debugging:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Enhanced TokenBucket with debugging capabilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .debugging.logger </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RateLimiterLogger, debug_timing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .debugging.performance_tracker </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PerformanceTracker</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DebuggableTokenBucket</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token bucket implementation with comprehensive debugging support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TokenBucketConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.initial_tokens </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> config.capacity</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_refill_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Debugging infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RateLimiterLogger(</span><span style=\"color:#9ECBFF\">\"token_bucket\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.perf_tracker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PerformanceTracker()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # State validation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._validate_state(</span><span style=\"color:#9ECBFF\">\"initialization\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span><span style=\"color:#E1E4E8\">) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Attempt to consume tokens with comprehensive debugging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Acquire lock for thread safety</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Record operation start time and log request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Validate input parameters and current state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Perform token refill based on elapsed time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Check if sufficient tokens available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 6: If tokens available, deduct requested amount</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 7: If insufficient tokens, calculate retry time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 8: Log operation result with all relevant data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 9: Validate final state consistency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 10: Record performance metrics and release lock</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _refill_tokens</span><span style=\"color:#E1E4E8\">(self, current_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Refill tokens based on elapsed time with debugging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Calculate elapsed time since last refill</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Log time calculation details for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Calculate tokens to add based on refill rate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Log token calculation with intermediate values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Cap tokens at bucket capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 6: Update current tokens and last refill time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 7: Log final refill result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 8: Return number of tokens added</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate_state</span><span style=\"color:#E1E4E8\">(self, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate bucket state consistency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that current tokens is not negative</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check that current tokens does not exceed capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check that last refill time is reasonable (not future, not too old)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log any validation failures with full state dump</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Milestone 1 (Basic Token Bucket):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/test_token_bucket.py -v</code></li>\n<li>Expected: All token bucket tests pass, no race conditions under concurrent access</li>\n<li>Manual verification: Create a bucket with capacity 10, rate 1/sec, consume 5 tokens, wait 3 seconds, consume 5 more - should succeed</li>\n<li>Debug check: Enable debug logging and verify token calculations match expected values</li>\n</ul>\n<p><strong>After Milestone 2 (Per-Client Rate Limiting):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/test_client_tracker.py -v</code></li>\n<li>Expected: Client buckets created correctly, cleanup removes stale buckets</li>\n<li>Manual verification: Send requests from multiple client IPs, verify separate buckets created</li>\n<li>Debug check: Monitor memory usage during high client turnover, verify cleanup prevents leaks</li>\n</ul>\n<p><strong>After Milestone 3 (HTTP Middleware):</strong></p>\n<ul>\n<li>Run: <code>curl -H &quot;X-Client-ID: test123&quot; http://localhost:5000/api/test</code> (multiple times)</li>\n<li>Expected: First requests succeed (200), later requests get 429 with proper headers</li>\n<li>Manual verification: Check that <code>X-RateLimit-Remaining</code> header decreases with each request</li>\n<li>Debug check: Verify middleware logs show correct client identification and token consumption</li>\n</ul>\n<p><strong>After Milestone 4 (Distributed Rate Limiting):</strong></p>\n<ul>\n<li>Run: Start multiple server instances, send requests to different servers with same client ID</li>\n<li>Expected: Rate limiting consistent across all servers</li>\n<li>Manual verification: Redis should show token bucket keys, Lua script should execute atomically</li>\n<li>Debug check: Monitor Redis operations, verify fallback works when Redis unavailable</li>\n</ul>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Beyond Milestone 4 - this section explores potential enhancements to the distributed rate limiter, guiding future development and production sophistication</p>\n</blockquote>\n<p>Think of our current rate limiter as a reliable neighborhood traffic cop who knows all the locals and keeps traffic flowing smoothly. But as our digital neighborhood grows into a bustling metropolis, we need to evolve our traffic management system. We might need specialized express lanes for VIP users, dynamic traffic light timing that adapts to rush hour patterns, and comprehensive traffic monitoring systems that help city planners optimize the entire transportation network. These future extensions transform our basic rate limiter into a sophisticated traffic management platform capable of handling enterprise-scale API ecosystems.</p>\n<p>The extensions we&#39;ll explore fall into three categories: algorithmic sophistication (alternative rate limiting strategies), operational intelligence (dynamic adaptation and advanced client classification), and production observability (comprehensive monitoring and alerting). Each extension builds upon our solid foundation while opening new possibilities for API protection and performance optimization.</p>\n<h3 id=\"alternative-rate-limiting-algorithms\">Alternative Rate Limiting Algorithms</h3>\n<p>Our token bucket algorithm provides excellent burst handling and intuitive capacity management, but different traffic patterns and protection requirements may benefit from alternative approaches. Think of these algorithms as different traffic management strategies: token bucket is like a toll booth that accepts payment in advance, sliding window is like a highway patrol counting cars over specific time periods, and leaky bucket is like a traffic light with perfectly timed releases.</p>\n<h4 id=\"sliding-window-rate-limiting\">Sliding Window Rate Limiting</h4>\n<p>The <strong>sliding window algorithm</strong> maintains a continuous time-based view of request history, providing more precise rate limiting than fixed time windows. Unlike token bucket&#39;s burst-friendly approach, sliding window ensures that no matter when you measure any time period, the request count never exceeds the limit.</p>\n<p><strong>Mental Model: The Moving Surveillance Window</strong>: Imagine a security camera that continuously records a moving 60-second video clip. At any moment, you can examine the current 60-second window and count events. Unlike a token bucket that allows 100 requests instantly (if tokens are available), sliding window ensures that looking back any 60 seconds from any point in time, you never see more than 60 requests.</p>\n<p>The algorithm maintains request timestamps and continuously evicts old entries as time progresses. Each new request triggers a cleanup of expired entries, followed by a count check against the configured limit.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm Aspect</th>\n<th>Token Bucket</th>\n<th>Sliding Window</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Burst Behavior</td>\n<td>Allows bursts up to capacity</td>\n<td>Strict rate enforcement</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>Fixed (just token count)</td>\n<td>Variable (stores request timestamps)</td>\n</tr>\n<tr>\n<td>Precision</td>\n<td>Approximate over time</td>\n<td>Exact within any time window</td>\n</tr>\n<tr>\n<td>Computational Complexity</td>\n<td>O(1) per request</td>\n<td>O(k) where k = recent requests</td>\n</tr>\n<tr>\n<td>Use Case</td>\n<td>APIs with natural burst patterns</td>\n<td>Strict SLA enforcement</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Sliding Window Implementation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Some API consumers require strict rate enforcement without burst allowances, particularly for billing APIs or resource-intensive operations</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Replace token bucket entirely with sliding window</li>\n<li>Implement sliding window as alternative algorithm option</li>\n<li>Hybrid approach combining both algorithms</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement sliding window as configurable alternative algorithm</li>\n<li><strong>Rationale</strong>: Different endpoints have different burst tolerance requirements; configuration flexibility serves more use cases than forcing one approach</li>\n<li><strong>Consequences</strong>: Increased complexity in configuration and storage, but enables precise rate enforcement for critical endpoints</li>\n</ul>\n</blockquote>\n<p>The sliding window implementation extends our existing <code>BucketStorage</code> interface with timestamp-based operations:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>record_request</code></td>\n<td><code>client_id: str, endpoint: str, timestamp: float</code></td>\n<td><code>bool</code></td>\n<td>Record new request timestamp and return whether under limit</td>\n</tr>\n<tr>\n<td><code>get_request_count</code></td>\n<td><code>client_id: str, endpoint: str, window_seconds: int</code></td>\n<td><code>int</code></td>\n<td>Count requests within sliding time window</td>\n</tr>\n<tr>\n<td><code>cleanup_expired_requests</code></td>\n<td><code>client_id: str, endpoint: str, cutoff_time: float</code></td>\n<td><code>int</code></td>\n<td>Remove expired request records and return count removed</td>\n</tr>\n<tr>\n<td><code>get_request_history</code></td>\n<td><code>client_id: str, endpoint: str, limit: int</code></td>\n<td><code>List[float]</code></td>\n<td>Retrieve recent request timestamps for debugging</td>\n</tr>\n</tbody></table>\n<h4 id=\"leaky-bucket-rate-limiting\">Leaky Bucket Rate Limiting</h4>\n<p>The <strong>leaky bucket algorithm</strong> enforces perfectly smooth request rates by processing requests at a fixed interval regardless of arrival timing. This algorithm excels in scenarios requiring predictable resource consumption and steady-state processing.</p>\n<p><strong>Mental Model: The Dripping Faucet</strong>: Picture a bucket with a small hole that drips water at exactly one drop per second. Incoming requests are water poured into the bucket. If water arrives faster than it drips out, the bucket fills up. Once full, excess water overflows (requests are rejected). The key insight is that water always exits at the same rate, creating perfectly smooth output even from bursty input.</p>\n<p>Unlike token bucket which allows immediate processing when tokens are available, leaky bucket queues requests and processes them at the configured rate. This creates natural traffic shaping but introduces latency for queued requests.</p>\n<table>\n<thead>\n<tr>\n<th>Characteristic</th>\n<th>Token Bucket</th>\n<th>Leaky Bucket</th>\n<th>Sliding Window</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Processing Model</td>\n<td>Immediate when tokens available</td>\n<td>Queue and process at fixed rate</td>\n<td>Immediate with count checking</td>\n</tr>\n<tr>\n<td>Latency</td>\n<td>Variable (immediate or rejected)</td>\n<td>Variable (queuing delay)</td>\n<td>Fixed (immediate or rejected)</td>\n</tr>\n<tr>\n<td>Traffic Shaping</td>\n<td>Allows bursts, smooths over time</td>\n<td>Perfect rate smoothing</td>\n<td>No smoothing, strict limits</td>\n</tr>\n<tr>\n<td>Queue Management</td>\n<td>No queuing</td>\n<td>Request queue required</td>\n<td>No queuing</td>\n</tr>\n<tr>\n<td>Resource Predictability</td>\n<td>Variable resource usage</td>\n<td>Perfectly predictable usage</td>\n<td>Variable resource usage</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Leaky Bucket Implementation Approach</strong></p>\n<ul>\n<li><strong>Context</strong>: Some downstream services require perfectly smooth request rates to avoid overwhelming their processing capacity</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Full leaky bucket with request queuing and background processing</li>\n<li>Simulated leaky bucket using token bucket with very small refill rates</li>\n<li>Hybrid approach with configurable processing delays</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement simulated leaky bucket using token bucket with single-token capacity and precise refill timing</li>\n<li><strong>Rationale</strong>: Avoids complexity of request queuing and background processing while achieving smooth rate limiting; simpler to implement and debug</li>\n<li><strong>Consequences</strong>: Doesn&#39;t provide true traffic shaping (requests are still rejected immediately), but achieves steady-state rate limiting without queuing infrastructure</li>\n</ul>\n</blockquote>\n<h4 id=\"algorithm-selection-framework\">Algorithm Selection Framework</h4>\n<p>Different API endpoints and client tiers benefit from different rate limiting algorithms. A comprehensive rate limiter should support algorithm selection based on endpoint characteristics and client requirements.</p>\n<p><strong>Algorithm Selection Criteria:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Use Case</th>\n<th>Recommended Algorithm</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Public APIs with burst usage</td>\n<td>Token Bucket</td>\n<td>Natural user behavior involves bursts</td>\n</tr>\n<tr>\n<td>Billing/payment endpoints</td>\n<td>Sliding Window</td>\n<td>Strict enforcement prevents billing abuse</td>\n</tr>\n<tr>\n<td>Resource-intensive operations</td>\n<td>Leaky Bucket</td>\n<td>Smooth processing protects backend systems</td>\n</tr>\n<tr>\n<td>High-throughput data ingestion</td>\n<td>Token Bucket with large capacity</td>\n<td>Accommodates batch processing patterns</td>\n</tr>\n<tr>\n<td>Real-time APIs with SLA requirements</td>\n<td>Sliding Window</td>\n<td>Precise rate tracking for SLA compliance</td>\n</tr>\n<tr>\n<td>Legacy system integration</td>\n<td>Leaky Bucket</td>\n<td>Protects systems with limited concurrency</td>\n</tr>\n</tbody></table>\n<p>The <code>RateLimitConfig</code> structure extends to support algorithm selection:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>algorithm_type</code></td>\n<td><code>AlgorithmType</code></td>\n<td>TOKEN_BUCKET, SLIDING_WINDOW, or LEAKY_BUCKET</td>\n</tr>\n<tr>\n<td><code>algorithm_config</code></td>\n<td><code>Dict[str, Any]</code></td>\n<td>Algorithm-specific parameters</td>\n</tr>\n<tr>\n<td><code>fallback_algorithm</code></td>\n<td><code>AlgorithmType</code></td>\n<td>Algorithm to use during primary algorithm failures</td>\n</tr>\n<tr>\n<td><code>transition_strategy</code></td>\n<td><code>TransitionStrategy</code></td>\n<td>How to handle algorithm changes for existing clients</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-rate-limiting-features\">Advanced Rate Limiting Features</h3>\n<p>Beyond basic rate limiting, production systems often require sophisticated features that adapt to changing conditions, classify clients intelligently, and provide fine-grained control over API access patterns.</p>\n<h4 id=\"dynamic-rate-adjustment\">Dynamic Rate Adjustment</h4>\n<p><strong>Dynamic rate adjustment</strong> automatically modifies rate limits based on system load, client behavior, and external conditions. This transforms static rate limiting into an intelligent protection system that balances user experience with system stability.</p>\n<p><strong>Mental Model: The Smart Traffic Light System</strong>: Imagine traffic lights that monitor traffic density in real-time and adjust their timing accordingly. During rush hour, they extend green light durations for busy directions. When emergency vehicles approach, they preemptively clear paths. During low-traffic periods, they provide faster cycles to minimize waiting. Dynamic rate adjustment applies this same intelligence to API traffic management.</p>\n<p>Dynamic adjustment operates on multiple time scales and responds to various signals:</p>\n<table>\n<thead>\n<tr>\n<th>Adjustment Trigger</th>\n<th>Response Time</th>\n<th>Typical Action</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>System CPU/memory pressure</td>\n<td>10-30 seconds</td>\n<td>Reduce limits by 10-50%</td>\n<td>Prevent system overload</td>\n</tr>\n<tr>\n<td>Error rate increase</td>\n<td>30-60 seconds</td>\n<td>Tighten limits for error-prone clients</td>\n<td>Circuit breaker behavior</td>\n</tr>\n<tr>\n<td>Client behavioral changes</td>\n<td>5-15 minutes</td>\n<td>Adjust individual client limits</td>\n<td>Reward/penalize based on behavior</td>\n</tr>\n<tr>\n<td>Time-based patterns</td>\n<td>Hours/days</td>\n<td>Pre-adjust for known traffic patterns</td>\n<td>Handle daily/weekly cycles</td>\n</tr>\n<tr>\n<td>External dependencies</td>\n<td>1-5 minutes</td>\n<td>Reduce limits when dependencies slow</td>\n<td>Backpressure propagation</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Dynamic Adjustment Implementation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Static rate limits often become either too restrictive during low load or insufficient during peak load, requiring manual intervention</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Simple load-based adjustment using system metrics</li>\n<li>Machine learning-based prediction and adjustment</li>\n<li>Rule-based adjustment with configurable triggers and responses</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Rule-based adjustment system with pluggable adjustment strategies</li>\n<li><strong>Rationale</strong>: Rule-based systems are predictable, debuggable, and don&#39;t require ML expertise; pluggable design allows future ML integration</li>\n<li><strong>Consequences</strong>: Requires careful rule design to avoid adjustment oscillations; provides immediate value with clear upgrade path</li>\n</ul>\n</blockquote>\n<p>The dynamic adjustment system introduces several new components:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Key Methods</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AdjustmentEngine</code></td>\n<td>Coordinates adjustment decisions</td>\n<td><code>evaluate_adjustments()</code>, <code>apply_adjustments()</code></td>\n</tr>\n<tr>\n<td><code>SystemMonitor</code></td>\n<td>Tracks system health metrics</td>\n<td><code>get_cpu_usage()</code>, <code>get_memory_pressure()</code></td>\n</tr>\n<tr>\n<td><code>ClientBehaviorTracker</code></td>\n<td>Monitors client request patterns</td>\n<td><code>track_request()</code>, <code>detect_anomalies()</code></td>\n</tr>\n<tr>\n<td><code>AdjustmentStrategy</code></td>\n<td>Defines adjustment logic</td>\n<td><code>calculate_adjustment()</code>, <code>validate_adjustment()</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"intelligent-client-classification\">Intelligent Client Classification</h4>\n<p><strong>Intelligent client classification</strong> automatically categorizes API consumers based on behavior patterns, enabling differentiated service levels without manual configuration. This system learns from request patterns to identify legitimate high-volume users, potential abusers, and everything in between.</p>\n<p><strong>Mental Model: The Hotel Concierge System</strong>: Picture a luxury hotel concierge who recognizes guests by their behavior patterns. Regular business travelers get streamlined check-in processes. Families with children receive patient, detailed assistance. Suspicious individuals face additional verification. The concierge doesn&#39;t just follow rigid rules but adapts service based on observed patterns and context cues.</p>\n<p>The classification system analyzes multiple behavioral dimensions to build client profiles:</p>\n<table>\n<thead>\n<tr>\n<th>Classification Dimension</th>\n<th>Measurement Window</th>\n<th>Typical Patterns</th>\n<th>Resulting Classification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request frequency consistency</td>\n<td>24-48 hours</td>\n<td>Steady, predictable intervals</td>\n<td>Legitimate automation</td>\n</tr>\n<tr>\n<td>Error rate patterns</td>\n<td>1-6 hours</td>\n<td>Low error rates with proper backoff</td>\n<td>Well-behaved client</td>\n</tr>\n<tr>\n<td>Endpoint usage diversity</td>\n<td>24 hours</td>\n<td>Uses multiple endpoints appropriately</td>\n<td>Application integration</td>\n</tr>\n<tr>\n<td>Response handling behavior</td>\n<td>30 minutes</td>\n<td>Respects rate limit headers</td>\n<td>Compliant client</td>\n</tr>\n<tr>\n<td>Geographic consistency</td>\n<td>7-30 days</td>\n<td>Requests from consistent regions</td>\n<td>Stable user base</td>\n</tr>\n<tr>\n<td>Time zone alignment</td>\n<td>7-14 days</td>\n<td>Usage matches reasonable time zones</td>\n<td>Human-driven usage</td>\n</tr>\n</tbody></table>\n<p>Based on these patterns, clients receive automatic classification:</p>\n<table>\n<thead>\n<tr>\n<th>Classification Tier</th>\n<th>Characteristics</th>\n<th>Rate Limit Multiplier</th>\n<th>Additional Benefits</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Platinum</td>\n<td>Consistent, compliant, diverse usage</td>\n<td>3.0x base rate</td>\n<td>Priority support queue</td>\n</tr>\n<tr>\n<td>Gold</td>\n<td>Regular patterns, low error rates</td>\n<td>2.0x base rate</td>\n<td>Extended burst capacity</td>\n</tr>\n<tr>\n<td>Silver</td>\n<td>Normal usage, occasional spikes</td>\n<td>1.5x base rate</td>\n<td>Standard service</td>\n</tr>\n<tr>\n<td>Bronze</td>\n<td>Basic usage, learning patterns</td>\n<td>1.0x base rate</td>\n<td>Educational headers</td>\n</tr>\n<tr>\n<td>Restricted</td>\n<td>High error rates, suspicious patterns</td>\n<td>0.5x base rate</td>\n<td>Additional monitoring</td>\n</tr>\n<tr>\n<td>Quarantine</td>\n<td>Probable abuse, requires review</td>\n<td>0.1x base rate</td>\n<td>Manual review required</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Client Classification Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Manual client tier management doesn&#39;t scale; automated classification can improve user experience while reducing operational overhead</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Simple rule-based classification using request counts</li>\n<li>Machine learning classification with behavioral features</li>\n<li>Hybrid approach with rules for obvious cases and ML for edge cases</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Start with comprehensive rule-based classification with ML integration points</li>\n<li><strong>Rationale</strong>: Rule-based systems provide transparency and immediate value; ML integration points enable future enhancement without architectural changes</li>\n<li><strong>Consequences</strong>: Requires careful rule tuning and monitoring to avoid false classifications; provides foundation for future ML enhancement</li>\n</ul>\n</blockquote>\n<h4 id=\"geographic-and-network-based-rate-limiting\">Geographic and Network-Based Rate Limiting</h4>\n<p><strong>Geographic rate limiting</strong> applies different rate limits based on client location and network characteristics, enabling region-specific protection and compliance with local regulations.</p>\n<p><strong>Mental Model: The International Border Control</strong>: Different countries have different entry requirements and processing capacities. A busy international airport might have express lanes for citizens, standard processing for visa holders, and additional screening for visitors from certain regions. Geographic rate limiting applies similar logic to API access, considering the origin and network characteristics of requests.</p>\n<p>Geographic rate limiting requires integration with IP geolocation services and network intelligence:</p>\n<table>\n<thead>\n<tr>\n<th>Geographic Factor</th>\n<th>Rate Limit Impact</th>\n<th>Implementation Approach</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Geographic region</td>\n<td>Region-specific base rates</td>\n<td>IP geolocation lookup</td>\n<td>Comply with regional regulations</td>\n</tr>\n<tr>\n<td>Network type</td>\n<td>Adjust for mobile vs broadband</td>\n<td>ASN and network classification</td>\n<td>Account for network limitations</td>\n</tr>\n<tr>\n<td>Distance from servers</td>\n<td>Latency-based adjustments</td>\n<td>Geographic distance calculation</td>\n<td>Compensate for network delays</td>\n</tr>\n<tr>\n<td>Country risk profile</td>\n<td>Security-based restrictions</td>\n<td>Configurable country classifications</td>\n<td>Protect against geographic threats</td>\n</tr>\n<tr>\n<td>Time zone alignment</td>\n<td>Business hours consideration</td>\n<td>Local time calculation</td>\n<td>Support business hour preferences</td>\n</tr>\n</tbody></table>\n<p>The geographic enhancement extends <code>ClientIdentifier</code> with location context:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ip_address</code></td>\n<td><code>str</code></td>\n<td>Original client IP address</td>\n</tr>\n<tr>\n<td><code>country_code</code></td>\n<td><code>Optional[str]</code></td>\n<td>ISO country code from geolocation</td>\n</tr>\n<tr>\n<td><code>region</code></td>\n<td><code>Optional[str]</code></td>\n<td>Geographic region classification</td>\n</tr>\n<tr>\n<td><code>asn</code></td>\n<td><code>Optional[int]</code></td>\n<td>Autonomous System Number</td>\n</tr>\n<tr>\n<td><code>network_type</code></td>\n<td><code>Optional[NetworkType]</code></td>\n<td>RESIDENTIAL, BUSINESS, MOBILE, HOSTING</td>\n</tr>\n<tr>\n<td><code>distance_km</code></td>\n<td><code>Optional[float]</code></td>\n<td>Distance to nearest server</td>\n</tr>\n<tr>\n<td><code>local_time_offset</code></td>\n<td><code>Optional[int]</code></td>\n<td>Hours offset from UTC</td>\n</tr>\n</tbody></table>\n<h3 id=\"monitoring-and-observability\">Monitoring and Observability</h3>\n<p>Production rate limiting systems require comprehensive monitoring to ensure proper operation, detect abuse patterns, and optimize performance. Think of monitoring as the air traffic control system for our API traffic—it needs to track every flight, predict congestion, and coordinate responses to keep the entire system flowing safely.</p>\n<h4 id=\"comprehensive-metrics-collection\">Comprehensive Metrics Collection</h4>\n<p><strong>Rate limiting metrics</strong> provide visibility into system behavior, client patterns, and protection effectiveness. The metrics system must capture both operational health and business intelligence about API usage.</p>\n<p><strong>Mental Model: The Hospital Patient Monitoring System</strong>: A hospital monitors patients at multiple levels—individual vital signs (heart rate, blood pressure), department-level statistics (admission rates, bed utilization), and hospital-wide metrics (staff efficiency, resource usage). Rate limiting monitoring works similarly, tracking individual client behavior, endpoint-level patterns, and system-wide protection effectiveness.</p>\n<p>The metrics collection system captures data across multiple dimensions:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Key Metrics</th>\n<th>Collection Frequency</th>\n<th>Storage Duration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Volume</td>\n<td>Requests/second, requests/minute</td>\n<td>Real-time</td>\n<td>30 days detailed, 1 year aggregated</td>\n</tr>\n<tr>\n<td>Rate Limit Effectiveness</td>\n<td>Allow/deny ratios, limit utilization</td>\n<td>Real-time</td>\n<td>90 days detailed, 1 year aggregated</td>\n</tr>\n<tr>\n<td>Client Behavior</td>\n<td>Top clients, error rates, geographic distribution</td>\n<td>1-minute intervals</td>\n<td>30 days detailed</td>\n</tr>\n<tr>\n<td>System Performance</td>\n<td>Latency percentiles, memory usage, Redis performance</td>\n<td>10-second intervals</td>\n<td>7 days detailed, 30 days aggregated</td>\n</tr>\n<tr>\n<td>Algorithm Performance</td>\n<td>Token generation rates, bucket utilization</td>\n<td>1-minute intervals</td>\n<td>30 days</td>\n</tr>\n<tr>\n<td>Error Conditions</td>\n<td>Circuit breaker trips, Redis failures, clock drift</td>\n<td>Immediate</td>\n<td>1 year</td>\n</tr>\n</tbody></table>\n<p><strong>Core Rate Limiting Metrics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric Name</th>\n<th>Type</th>\n<th>Labels</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>rate_limit_requests_total</code></td>\n<td>Counter</td>\n<td><code>client_id</code>, <code>endpoint</code>, <code>result</code></td>\n<td>Total requests processed</td>\n</tr>\n<tr>\n<td><code>rate_limit_tokens_consumed</code></td>\n<td>Counter</td>\n<td><code>client_id</code>, <code>endpoint</code></td>\n<td>Total tokens consumed</td>\n</tr>\n<tr>\n<td><code>rate_limit_bucket_utilization</code></td>\n<td>Gauge</td>\n<td><code>client_id</code>, <code>endpoint</code></td>\n<td>Current bucket fill percentage</td>\n</tr>\n<tr>\n<td><code>rate_limit_processing_duration</code></td>\n<td>Histogram</td>\n<td><code>component</code>, <code>operation</code></td>\n<td>Processing latency distribution</td>\n</tr>\n<tr>\n<td><code>rate_limit_active_clients</code></td>\n<td>Gauge</td>\n<td><code>classification_tier</code></td>\n<td>Number of active clients by tier</td>\n</tr>\n<tr>\n<td><code>rate_limit_redis_operations</code></td>\n<td>Counter</td>\n<td><code>operation</code>, <code>result</code></td>\n<td>Redis operation counts and results</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Metrics Collection Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Rate limiting decisions happen in the critical request path; metrics collection must not impact request latency</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Synchronous metrics collection with request processing</li>\n<li>Asynchronous metrics with background aggregation</li>\n<li>Sampling-based metrics to reduce overhead</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Asynchronous metrics with configurable sampling for high-volume endpoints</li>\n<li><strong>Rationale</strong>: Asynchronous collection eliminates metrics impact on request latency; sampling reduces overhead while maintaining statistical accuracy</li>\n<li><strong>Consequences</strong>: Slight delay in metrics availability; requires careful sampling strategy to avoid bias</li>\n</ul>\n</blockquote>\n<h4 id=\"real-time-dashboard-and-alerting\">Real-Time Dashboard and Alerting</h4>\n<p><strong>Real-time dashboards</strong> provide immediate visibility into rate limiting behavior, while <strong>intelligent alerting</strong> ensures rapid response to anomalous conditions.</p>\n<p><strong>Mental Model: The Mission Control Center</strong>: NASA&#39;s mission control monitors spacecraft with multiple screens showing different aspects of mission health—trajectory, system status, communication quality, and crew vitals. Each screen serves different roles: flight directors see high-level mission status, engineers monitor specific subsystems, and specialists track their areas of expertise. Rate limiting dashboards serve similar roles for different operational teams.</p>\n<p>The dashboard system provides role-specific views:</p>\n<table>\n<thead>\n<tr>\n<th>Dashboard View</th>\n<th>Target Audience</th>\n<th>Key Visualizations</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Executive Summary</td>\n<td>Management, SRE leadership</td>\n<td>API health score, major incidents</td>\n<td>5-minute intervals</td>\n</tr>\n<tr>\n<td>Operations Overview</td>\n<td>SRE, DevOps teams</td>\n<td>Request rates, error rates, system load</td>\n<td>30-second intervals</td>\n</tr>\n<tr>\n<td>Security Monitoring</td>\n<td>Security teams</td>\n<td>Abuse patterns, geographic anomalies</td>\n<td>Real-time</td>\n</tr>\n<tr>\n<td>Client Experience</td>\n<td>Product teams</td>\n<td>Client-specific metrics, tier distributions</td>\n<td>1-minute intervals</td>\n</tr>\n<tr>\n<td>System Performance</td>\n<td>Platform engineers</td>\n<td>Latency, throughput, resource utilization</td>\n<td>10-second intervals</td>\n</tr>\n</tbody></table>\n<p><strong>Critical Alert Conditions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Alert Condition</th>\n<th>Severity</th>\n<th>Trigger Threshold</th>\n<th>Response Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Mass rate limiting triggered</td>\n<td>Critical</td>\n<td>&gt;50% of requests denied for 5+ minutes</td>\n<td>Immediate investigation</td>\n</tr>\n<tr>\n<td>Redis cluster failure</td>\n<td>Critical</td>\n<td>Circuit breaker open for 2+ minutes</td>\n<td>Failover activation</td>\n</tr>\n<tr>\n<td>Abnormal client behavior</td>\n<td>High</td>\n<td>Single client &gt;10x normal rate</td>\n<td>Security review</td>\n</tr>\n<tr>\n<td>System performance degradation</td>\n<td>High</td>\n<td>P95 latency &gt;500ms for 10+ minutes</td>\n<td>Performance investigation</td>\n</tr>\n<tr>\n<td>Geographic traffic anomaly</td>\n<td>Medium</td>\n<td>5x increase from unusual regions</td>\n<td>Security monitoring</td>\n</tr>\n<tr>\n<td>Token bucket calculation errors</td>\n<td>Medium</td>\n<td>&gt;1% calculation failures</td>\n<td>Algorithm review</td>\n</tr>\n</tbody></table>\n<p>The alerting system implements intelligent alert aggregation to prevent notification fatigue:</p>\n<table>\n<thead>\n<tr>\n<th>Aggregation Strategy</th>\n<th>Time Window</th>\n<th>Condition</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Alert deduplication</td>\n<td>15 minutes</td>\n<td>Same condition, same resource</td>\n<td>Single notification</td>\n</tr>\n<tr>\n<td>Storm detection</td>\n<td>5 minutes</td>\n<td>&gt;10 alerts from same category</td>\n<td>Storm summary alert</td>\n</tr>\n<tr>\n<td>Escalation</td>\n<td>30-60 minutes</td>\n<td>Alert not acknowledged</td>\n<td>Escalate to next tier</td>\n</tr>\n<tr>\n<td>Auto-resolution</td>\n<td>Variable</td>\n<td>Condition clears for 2x trigger duration</td>\n<td>Auto-close alert</td>\n</tr>\n</tbody></table>\n<h4 id=\"advanced-analytics-and-insights\">Advanced Analytics and Insights</h4>\n<p><strong>Advanced analytics</strong> transform raw rate limiting data into actionable business and operational intelligence, helping teams optimize API strategy and improve system efficiency.</p>\n<p><strong>Mental Model: The Traffic Engineering Department</strong>: City traffic engineers don&#39;t just manage traffic lights—they analyze traffic patterns to optimize road design, predict future capacity needs, and identify improvement opportunities. They study rush hour flows, accident patterns, and seasonal variations to make data-driven infrastructure decisions. Rate limiting analytics serve the same strategic purpose for API infrastructure.</p>\n<p>The analytics system provides multiple analytical capabilities:</p>\n<table>\n<thead>\n<tr>\n<th>Analytics Category</th>\n<th>Key Insights</th>\n<th>Analysis Methods</th>\n<th>Business Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Usage Pattern Analysis</td>\n<td>Peak usage times, seasonal trends</td>\n<td>Time series analysis, trend detection</td>\n<td>Capacity planning, cost optimization</td>\n</tr>\n<tr>\n<td>Client Behavior Segmentation</td>\n<td>Client archetypes, usage evolution</td>\n<td>Clustering, behavioral analysis</td>\n<td>Product strategy, tier optimization</td>\n</tr>\n<tr>\n<td>Abuse Detection and Prevention</td>\n<td>Attack patterns, bot identification</td>\n<td>Anomaly detection, pattern recognition</td>\n<td>Security improvement, cost reduction</td>\n</tr>\n<tr>\n<td>Performance Optimization</td>\n<td>Bottlenecks, efficiency opportunities</td>\n<td>Performance profiling, correlation analysis</td>\n<td>System optimization, user experience</td>\n</tr>\n<tr>\n<td>Revenue Impact Analysis</td>\n<td>Rate limiting effects on business metrics</td>\n<td>A/B testing, causal analysis</td>\n<td>Business optimization, pricing strategy</td>\n</tr>\n</tbody></table>\n<p><strong>Advanced Analytics Queries:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Analysis Type</th>\n<th>Query Pattern</th>\n<th>Insight Generated</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client Lifecycle Analysis</td>\n<td>Track client behavior evolution over 30-90 days</td>\n<td>Identify clients ready for tier upgrades</td>\n</tr>\n<tr>\n<td>Geographic Usage Patterns</td>\n<td>Analyze request patterns by region and time</td>\n<td>Optimize server placement and capacity</td>\n</tr>\n<tr>\n<td>Endpoint Popularity Trends</td>\n<td>Track endpoint usage changes over time</td>\n<td>Guide API development priorities</td>\n</tr>\n<tr>\n<td>Error Pattern Analysis</td>\n<td>Correlate error rates with rate limiting</td>\n<td>Identify configuration optimization opportunities</td>\n</tr>\n<tr>\n<td>Business Impact Correlation</td>\n<td>Connect rate limiting with revenue metrics</td>\n<td>Quantify business impact of protection policies</td>\n</tr>\n</tbody></table>\n<p>The analytics system integrates with the rate limiting infrastructure through dedicated data collection:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Data Collection</th>\n<th>Analysis Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AnalyticsCollector</code></td>\n<td>Structured data gathering</td>\n<td>Request metadata, timing, outcomes</td>\n<td>Standardized datasets for analysis</td>\n</tr>\n<tr>\n<td><code>PatternAnalyzer</code></td>\n<td>Behavior pattern detection</td>\n<td>Client request sequences, timing patterns</td>\n<td>Client classification recommendations</td>\n</tr>\n<tr>\n<td><code>AnomalyDetector</code></td>\n<td>Unusual behavior identification</td>\n<td>Statistical analysis of request patterns</td>\n<td>Security alerts and recommendations</td>\n</tr>\n<tr>\n<td><code>BusinessMetricsIntegrator</code></td>\n<td>Connect technical and business metrics</td>\n<td>Rate limiting outcomes, business KPIs</td>\n<td>ROI analysis and optimization suggestions</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The future extensions require careful architectural planning to ensure they integrate seamlessly with the existing rate limiting infrastructure while maintaining performance and reliability.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Implementation</th>\n<th>Advanced Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sliding Window Storage</td>\n<td>In-memory deque with timestamps</td>\n<td>Redis sorted sets with Lua scripts</td>\n</tr>\n<tr>\n<td>Dynamic Adjustment</td>\n<td>Simple rule engine with thresholds</td>\n<td>Machine learning pipeline with feature engineering</td>\n</tr>\n<tr>\n<td>Client Classification</td>\n<td>Rule-based scoring with configurable weights</td>\n<td>Behavioral analysis with clustering algorithms</td>\n</tr>\n<tr>\n<td>Geographic Services</td>\n<td>MaxMind GeoLite2 database</td>\n<td>Commercial IP intelligence with real-time updates</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Prometheus client with local aggregation</td>\n<td>InfluxDB with Telegraph agent and custom collectors</td>\n</tr>\n<tr>\n<td>Analytics Platform</td>\n<td>Grafana dashboards with basic queries</td>\n<td>Elasticsearch with Kibana and custom analytics</td>\n</tr>\n</tbody></table>\n<h4 id=\"extension-architecture-integration\">Extension Architecture Integration</h4>\n<p>The extensions integrate with the existing rate limiting architecture through well-defined interfaces that maintain backward compatibility while enabling sophisticated new capabilities.</p>\n<p><strong>File Structure for Extensions:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>rate_limiter/\n  core/                           # Existing core implementation\n    token_bucket.py\n    client_tracker.py\n    middleware.py\n    distributed.py\n  algorithms/                     # Alternative algorithms\n    sliding_window.py             # Sliding window implementation\n    leaky_bucket.py              # Leaky bucket implementation  \n    algorithm_factory.py         # Algorithm selection logic\n  intelligence/                   # Advanced features\n    dynamic_adjustment.py        # Dynamic rate adjustment\n    client_classifier.py         # Intelligent client classification\n    geographic_limiter.py        # Geographic rate limiting\n  monitoring/                     # Observability extensions\n    metrics_collector.py         # Comprehensive metrics\n    dashboard_server.py          # Real-time dashboard\n    analytics_engine.py          # Advanced analytics\n  extensions/                     # Integration points\n    extension_manager.py         # Manages extension lifecycle\n    hook_registry.py            # Extension hook system</code></pre></div>\n\n<h4 id=\"algorithm-extension-framework\">Algorithm Extension Framework</h4>\n<p>The algorithm extension framework allows adding new rate limiting algorithms without modifying existing code:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitDecision</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result of rate limiting decision with algorithm-specific context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowed: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens_remaining: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_after_seconds: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    algorithm_state: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debug_info: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitAlgorithm</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract base class for rate limiting algorithms.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   current_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> RateLimitDecision:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Attempt to consume tokens using this algorithm.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Retrieve or create algorithm state for this client/endpoint combination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Apply algorithm-specific logic to determine if request should be allowed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Update algorithm state based on the consumption decision</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Calculate retry_after_seconds based on algorithm characteristics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Prepare debug information for troubleshooting and monitoring</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_algorithm_info</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return algorithm metadata for monitoring and debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SlidingWindowAlgorithm</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RateLimitAlgorithm</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Sliding window rate limiting implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, window_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, max_requests: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 storage: RateLimitStorage):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store configuration parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize storage backend for request timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up cleanup scheduling for expired timestamps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   current_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> RateLimitDecision:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate window start time (current_time - window_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Clean up expired request timestamps before window start</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Count existing requests within the current window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check if adding tokens_requested would exceed max_requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If allowed, record the new request timestamp(s)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Calculate retry_after based on oldest request in window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return decision with current window state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"dynamic-adjustment-implementation-framework\">Dynamic Adjustment Implementation Framework</h4>\n<p>The dynamic adjustment system provides a pluggable architecture for implementing various adjustment strategies:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdjustmentContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context information for adjustment decisions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_cpu_percent: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_memory_percent: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_latency_p95: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_rate_percent: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    active_clients_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recent_adjustments: List[</span><span style=\"color:#9ECBFF\">'RateAdjustment'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateAdjustment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a rate limit adjustment decision.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_pattern: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # Pattern matching clients to adjust</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    endpoint_pattern: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # Pattern matching endpoints to adjust</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    adjustment_factor: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">  # Multiplier for current rate limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    reason: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # Human-readable reason for adjustment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expires_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">  # When this adjustment expires</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_score: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">  # Confidence in this adjustment (0.0-1.0)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdjustmentStrategy</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract base class for dynamic adjustment strategies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_adjustments</span><span style=\"color:#E1E4E8\">(self, context: AdjustmentContext) -> List[RateAdjustment]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Evaluate current conditions and return recommended adjustments.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Analyze context metrics for adjustment triggers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Calculate appropriate adjustment factors based on conditions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Determine scope of adjustments (which clients/endpoints)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Set expiration times for temporary adjustments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Assign confidence scores based on data quality and certainty</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoadBasedAdjustmentStrategy</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">AdjustmentStrategy</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Adjusts rate limits based on system load metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, cpu_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 80.0</span><span style=\"color:#E1E4E8\">, memory_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 85.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 max_reduction: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store threshold configurations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize metrics history tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up adjustment calculation parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_adjustments</span><span style=\"color:#E1E4E8\">(self, context: AdjustmentContext) -> List[RateAdjustment]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if CPU usage exceeds threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if memory usage exceeds threshold  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate adjustment factor based on severity of overload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Determine which client tiers to adjust (start with lowest priority)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set appropriate expiration times (longer for severe overload)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return list of adjustment recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DynamicRateAdjuster</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates dynamic rate limit adjustments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, strategies: List[AdjustmentStrategy], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 rate_limiter: DistributedRateLimiter):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store adjustment strategies and their priorities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize rate limiter reference for applying adjustments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up background monitoring and adjustment threads</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize adjustment history tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"monitoring-extension-points\">Monitoring Extension Points</h4>\n<p>The monitoring extensions provide comprehensive observability without impacting request processing performance:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetricDataPoint</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual metric measurement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metric_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    labels: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetricsCollector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"High-performance metrics collection with async processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, buffer_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#E1E4E8\">, flush_interval: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize ring buffer for metric storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up async processing queue and background tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure metric aggregation and sampling strategies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize connections to metric storage systems</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_rate_limit_decision</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 result: TokenConsumptionResult):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record rate limiting decision metrics asynchronously.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract key metrics from consumption result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create metric data points for counters and gauges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add to async processing queue (non-blocking)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update internal aggregations for dashboard queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_system_performance</span><span style=\"color:#E1E4E8\">(self, component: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                duration_ms: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, success: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record system performance metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create performance metric data points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update histogram buckets for latency distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Track error rates and success rates by component</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Queue for async processing and storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RealTimeDashboard</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"WebSocket-based real-time dashboard server.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, metrics_collector: MetricsCollector, port: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8080</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize web server and WebSocket handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up metric query and aggregation endpoints  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure dashboard update intervals and data retention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize alert condition monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> stream_metrics</span><span style=\"color:#E1E4E8\">(self, websocket_connection, dashboard_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stream real-time metrics to dashboard clients.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine metric subset based on dashboard type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up periodic metric queries and aggregations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Stream formatted data to WebSocket clients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle client disconnections and reconnections</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-extensions-integration\">Milestone Checkpoint: Extensions Integration</h4>\n<p>After implementing future extensions, verify the enhanced system behavior:</p>\n<p><strong>Extension Testing Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test alternative algorithms</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> rate_limiter.test_extensions</span><span style=\"color:#79B8FF\"> --test-sliding-window</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> rate_limiter.test_extensions</span><span style=\"color:#79B8FF\"> --test-leaky-bucket</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test dynamic adjustment</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> rate_limiter.test_extensions</span><span style=\"color:#79B8FF\"> --test-dynamic-adjustment</span><span style=\"color:#79B8FF\"> --simulate-load</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test client classification  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> rate_limiter.test_extensions</span><span style=\"color:#79B8FF\"> --test-client-classification</span><span style=\"color:#79B8FF\"> --duration</span><span style=\"color:#79B8FF\"> 300</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test monitoring and analytics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> rate_limiter.test_extensions</span><span style=\"color:#79B8FF\"> --test-monitoring</span><span style=\"color:#79B8FF\"> --generate-traffic</span></span></code></pre></div>\n\n<p><strong>Expected Extension Behaviors:</strong></p>\n<ol>\n<li><strong>Alternative Algorithms</strong>: Sliding window should reject bursts that token bucket would allow; leaky bucket should smooth request processing</li>\n<li><strong>Dynamic Adjustment</strong>: Rate limits should decrease under simulated load and recover when load returns to normal</li>\n<li><strong>Client Classification</strong>: Consistent well-behaved clients should receive higher rate limits over time</li>\n<li><strong>Monitoring</strong>: Real-time dashboard should display current metrics; alerts should trigger for anomalous conditions</li>\n</ol>\n<p><strong>Common Extension Issues:</strong></p>\n<p>⚠️ <strong>Pitfall: Extension Performance Impact</strong>\nExtensions can inadvertently impact rate limiting performance if not implemented carefully. All extension processing should be asynchronous and non-blocking relative to the critical request path.</p>\n<p>⚠️ <strong>Pitfall: Configuration Complexity</strong>\nAdvanced features introduce significant configuration complexity. Provide sensible defaults and clear documentation to prevent misconfiguration that could disable protection or create operational issues.</p>\n<p>⚠️ <strong>Pitfall: Extension Interaction Effects</strong><br>Multiple extensions can interact in unexpected ways. Dynamic adjustment might conflict with client classification, or geographic limiting might interfere with algorithm selection. Design extensions with clear precedence rules and conflict resolution strategies.</p>\n<p>The future extensions transform the basic rate limiter into a sophisticated API protection platform capable of handling enterprise-scale requirements while maintaining the reliability and performance of the core implementation. Each extension builds upon the solid foundation established in the earlier milestones, demonstrating how well-designed systems can grow to meet evolving requirements without sacrificing their essential characteristics.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building future extensions requires careful attention to maintainability, performance, and integration complexity. The extensions should enhance the rate limiter&#39;s capabilities while preserving the simplicity and reliability of the core implementation.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Type</th>\n<th>Simple Implementation</th>\n<th>Production Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sliding Window</td>\n<td>Python deque with threading.Lock</td>\n<td>Redis sorted sets with Lua atomic operations</td>\n</tr>\n<tr>\n<td>Client Classification</td>\n<td>Rule-based scoring with JSON config</td>\n<td>Feature engineering pipeline with ML models</td>\n</tr>\n<tr>\n<td>Dynamic Adjustment</td>\n<td>Threshold-based rules with simple PID controller</td>\n<td>Multi-factor adjustment with predictive modeling</td>\n</tr>\n<tr>\n<td>Geographic Intelligence</td>\n<td>MaxMind GeoLite2 with local database</td>\n<td>Real-time IP intelligence with CDN integration</td>\n</tr>\n<tr>\n<td>Monitoring Stack</td>\n<td>Prometheus + Grafana with basic dashboards</td>\n<td>Full observability with distributed tracing</td>\n</tr>\n<tr>\n<td>Analytics Platform</td>\n<td>SQLite with pandas for analysis</td>\n<td>Time-series database with streaming analytics</td>\n</tr>\n</tbody></table>\n<h4 id=\"extension-architecture-implementation\">Extension Architecture Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># extensions/extension_manager.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Type, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> importlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for rate limiter extensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enabled_extensions: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    extension_configs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    extension_priority: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hot_reload_enabled: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimiterExtension</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all rate limiter extensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">(self, rate_limiter, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize extension with rate limiter instance and config.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_extension_info</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return extension metadata and current status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> on_rate_limit_decision</span><span style=\"color:#E1E4E8\">(self, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Hook called after each rate limiting decision.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> on_client_registered</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Hook called when new client is registered.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> on_bucket_cleanup</span><span style=\"color:#E1E4E8\">(self, cleaned_buckets: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Hook called after bucket cleanup operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages lifecycle of rate limiter extensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: ExtensionConfig):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store extension configuration and initialize registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up extension loading and dependency resolution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize hook system for extension callbacks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set up hot reload monitoring if enabled</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.extensions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, RateLimiterExtension] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.extension_hooks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Callable]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_extensions</span><span style=\"color:#E1E4E8\">(self, rate_limiter) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Load and initialize all configured extensions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Iterate through enabled extensions list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Dynamically import each extension module</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Instantiate extension class with configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Call extension initialize method with rate limiter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Register extension hooks for callbacks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 6: Return success/failure status for each extension</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> ext_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.enabled_extensions:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Dynamic extension loading implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results[ext_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logging.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to load extension </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">ext_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results[ext_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span></code></pre></div>\n\n<h4 id=\"advanced-algorithm-implementation\">Advanced Algorithm Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># algorithms/sliding_window.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> deque</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SlidingWindowState</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"State for sliding window rate limiting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    request_timestamps: deque </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">deque)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_cleanup: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_requests: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    window_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_requests: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SlidingWindowRateLimiter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe sliding window rate limiter implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, default_window: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#E1E4E8\">, default_max_requests: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize default configuration parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create thread-safe storage for client window states</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up periodic cleanup of expired timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize performance monitoring and metrics</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._client_windows: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, SlidingWindowState] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RWLock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cleanup_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#6A737D\">  # 5 minutes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> try_consume</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tokens_requested: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   current_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> TokenConsumptionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Attempt to consume tokens using sliding window algorithm.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Get current time if not provided</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Acquire read lock and get client window state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Clean up expired timestamps from the window</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Count current requests in the sliding window</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Check if adding requested tokens would exceed limit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 6: If allowed, add new timestamps and update state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 7: Calculate retry_after based on oldest timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 8: Return consumption result with window information</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> current_time </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock.read_lock():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Implementation of sliding window logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _cleanup_expired_timestamps</span><span style=\"color:#E1E4E8\">(self, window_state: SlidingWindowState, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  current_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove expired timestamps from window.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate cutoff time (current_time - window_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Remove timestamps older than cutoff from deque</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update total_requests counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return count of removed timestamps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_or_create_window</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> SlidingWindowState:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Thread-safe retrieval or creation of client window state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Try to get existing window state with read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If not found, acquire write lock and create new state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle race condition where state created between locks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return window state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"dynamic-intelligence-framework\">Dynamic Intelligence Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># intelligence/client_classifier.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClientProfile</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Profile of client behavior for classification.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_requests: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    avg_interval_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    endpoints_used: </span><span style=\"color:#79B8FF\">set</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    geographic_regions: </span><span style=\"color:#79B8FF\">set</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    first_seen: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_seen: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    classification: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_score: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.endpoints_used </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.endpoints_used </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.geographic_regions </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.geographic_regions </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IntelligentClientClassifier</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Automatic client classification based on behavior analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, observation_period: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 86400</span><span style=\"color:#E1E4E8\">):  </span><span style=\"color:#6A737D\"># 24 hours</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize observation period and classification thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up client profile storage and tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure classification rules and scoring system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize background analysis and reclassification</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._profiles: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ClientProfile] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._request_intervals: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> track_request</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, success: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     timestamp: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, region: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Track client request for behavior analysis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Get or create client profile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Update request counts and success/error ratios  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Record endpoint usage and geographic information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Calculate request interval statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Update timestamps and trigger reclassification if needed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> timestamp </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Implementation of request tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> classify_client</span><span style=\"color:#E1E4E8\">(self, client_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Classify client based on observed behavior patterns.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Retrieve client profile and calculate behavior metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Apply classification rules for each tier level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Calculate confidence score based on observation completeness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Update profile with new classification and confidence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Return classification tier and confidence score</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        profile </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._profiles.get(client_id)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> profile:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"unknown\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Classification logic implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_behavior_score</span><span style=\"color:#E1E4E8\">(self, profile: ClientProfile) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate behavior scoring metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate consistency score from request intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate compliance score from error rates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate diversity score from endpoint usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate maturity score from observation period</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return comprehensive behavior scoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_classification_recommendations</span><span style=\"color:#E1E4E8\">(self) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get recommendations for client tier adjustments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Identify clients ready for tier upgrades</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify clients that should be downgraded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate potential impact of tier changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return prioritized list of recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"production-monitoring-implementation\">Production Monitoring Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># monitoring/advanced_analytics.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pandas </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pd</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> scipy </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> stats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AnalyticsQuery</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for analytics query execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    query_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    time_range: Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    aggregation: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_format: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"json\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RateLimitAnalyticsEngine</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Advanced analytics for rate limiting behavior and optimization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, metrics_collector, data_retention_days: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 90</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize connection to metrics data store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up data retention and archival policies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure analytical processing capabilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize caching for frequently-run analyses</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics_collector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metrics_collector</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.query_cache: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> analyze_client_behavior_patterns</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                             time_range_hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze client behavior patterns for optimization opportunities.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Query request data for specified time range</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Group requests by client and calculate behavior metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Identify patterns in request timing and frequency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Detect anomalies and unusual behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Generate recommendations for rate limit adjustments</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Implementation of behavior pattern analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> calculate_rate_limit_effectiveness</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate effectiveness metrics for current rate limiting policies.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Query rate limiting decision data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Calculate allow/deny ratios by client tier and endpoint</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Analyze correlation between limits and client satisfaction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Identify over-restrictive and under-restrictive configurations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Return effectiveness scores and improvement opportunities</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_capacity_planning_report</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate capacity planning recommendations based on trends.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Analyze historical growth patterns in API usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Project future capacity requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify seasonal and cyclical patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate infrastructure scaling recommendations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return comprehensive capacity planning report</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> detect_abuse_patterns</span><span style=\"color:#E1E4E8\">(self, sensitivity: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.95</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Detect potential abuse patterns using statistical analysis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Apply anomaly detection algorithms to request patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Identify clients with suspicious behavior profiles</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Correlate geographic and timing patterns for abuse detection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Score abuse likelihood and confidence levels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Return prioritized list of potential abuse cases</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># monitoring/real_time_dashboard.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> websockets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Set, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RealTimeDashboardServer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"WebSocket-based real-time dashboard for rate limiting metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, metrics_collector, analytics_engine, port: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8080</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize WebSocket server configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up metric streaming and update intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure dashboard layouts and visualizations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize alert condition monitoring</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics_collector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metrics_collector</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.analytics_engine </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> analytics_engine</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connected_clients: Set[websockets.WebSocketServerProtocol] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> start_server</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the WebSocket dashboard server.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start WebSocket server on configured port</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Begin background metric streaming tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize alert monitoring and notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set up graceful shutdown handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> handle_client_connection</span><span style=\"color:#E1E4E8\">(self, websocket, path):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handle new dashboard client connections.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Register new client connection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Send initial dashboard configuration and current metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Handle client-specific dashboard requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Stream real-time updates until disconnection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Clean up client resources on disconnect</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> stream_metrics_updates</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Background task to stream metric updates to all connected clients.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Continuously query latest metrics from collector</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Format metrics for dashboard consumption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Broadcast updates to all connected clients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle client disconnections and errors gracefully</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_dashboard_config</span><span style=\"color:#E1E4E8\">(self, dashboard_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate configuration for specific dashboard types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Define available dashboard layouts and widgets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Configure metrics queries and update frequencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up alert conditions and notification preferences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return dashboard configuration for client setup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"extension-testing-and-validation\">Extension Testing and Validation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test/test_extensions.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> unittest.mock </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Mock, patch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> rate_limiter.extensions.extension_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExtensionManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> rate_limiter.algorithms.sliding_window </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SlidingWindowRateLimiter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> rate_limiter.intelligence.client_classifier </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> IntelligentClientClassifier</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestExtensionIntegration</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Integration tests for rate limiter extensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extension_manager</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set up extension manager for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create test configuration with all extensions enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize extension manager with test config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up mock rate limiter for extension integration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return configured extension manager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_sliding_window_accuracy</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test sliding window algorithm accuracy under various load patterns.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize sliding window with known parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate predictable request patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify exact request counting within sliding windows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test edge cases around window boundaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate retry-after calculations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_client_classification_evolution</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test client classification changes based on behavior patterns.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize client classifier with test parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Simulate different client behavior patterns over time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify classification changes match expected progressions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test classification confidence scoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate tier upgrade and downgrade recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.mark.asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> test_dynamic_adjustment_response</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test dynamic rate adjustment response to system conditions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set up dynamic adjustment with test strategies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Simulate various system load conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify adjustment recommendations match expected responses</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test adjustment application and rollback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate adjustment coordination across multiple strategies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_extension_performance_impact</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify extensions don't significantly impact request processing performance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Measure baseline request processing latency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Enable extensions one by one and measure impact</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify latency increase stays within acceptable bounds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test under high concurrent load</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate async processing doesn't block request path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> run_extension_load_test</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Run comprehensive load test of extended rate limiter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set up rate limiter with all extensions enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate realistic mixed client traffic patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Monitor extension behavior under sustained load</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify extensions continue working correctly under stress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Report performance characteristics and bottlenecks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-extensions-validation\">Milestone Checkpoint: Extensions Validation</h4>\n<p>After implementing the future extensions, validate the enhanced system capabilities:</p>\n<p><strong>Verification Steps:</strong></p>\n<ol>\n<li><strong>Algorithm Switching</strong>: Configure different algorithms for different endpoints and verify they behave according to their characteristics</li>\n<li><strong>Dynamic Adjustment</strong>: Simulate system load and observe automatic rate limit adjustments</li>\n<li><strong>Client Classification</strong>: Run diverse client patterns and verify automatic tier assignments</li>\n<li><strong>Analytics Dashboard</strong>: Generate traffic and observe real-time metrics and insights</li>\n<li><strong>End-to-End Integration</strong>: Verify all extensions work together without conflicts</li>\n</ol>\n<p><strong>Performance Benchmarks:</strong></p>\n<ul>\n<li>Extension overhead should add &lt;5ms to request processing latency</li>\n<li>Memory usage should remain stable under sustained load</li>\n<li>Analytics processing should not impact rate limiting decisions</li>\n<li>Dashboard should update within 30 seconds of metric changes</li>\n</ul>\n<p>The future extensions demonstrate how a well-architected system can evolve to meet sophisticated requirements while maintaining its core reliability and performance characteristics. Each extension follows established patterns and interfaces, ensuring the enhanced system remains maintainable and debuggable as it grows in capability and complexity.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section provides definitions for all technical terms, rate limiting concepts, and domain-specific vocabulary used throughout the document</p>\n</blockquote>\n<h3 id=\"mental-model-the-technical-dictionary\">Mental Model: The Technical Dictionary</h3>\n<p>Think of this glossary as your technical dictionary for the rate limiting domain. Just as a medical dictionary defines specialized terms like &quot;tachycardia&quot; or &quot;myocardial infarction&quot; in precise, unambiguous ways, this glossary defines rate limiting terminology with exact meanings. Each term has been carefully chosen to eliminate confusion and ensure consistent communication throughout the implementation. When you see &quot;token bucket algorithm&quot; versus &quot;leaky bucket algorithm,&quot; these aren&#39;t interchangeable terms - they represent fundamentally different approaches with distinct characteristics and trade-offs.</p>\n<p>The glossary is organized into logical categories: core algorithms and concepts, system architecture terms, distributed systems terminology, error handling vocabulary, testing and debugging concepts, and future extension terminology. This structure mirrors the learning journey from basic rate limiting concepts through advanced distributed implementations.</p>\n<h3 id=\"core-rate-limiting-terminology\">Core Rate Limiting Terminology</h3>\n<p>The foundation of rate limiting rests on precise algorithmic and conceptual definitions that form the building blocks for more advanced topics.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Token Bucket Algorithm</strong></td>\n<td>A rate limiting algorithm that maintains a bucket with a fixed capacity of tokens, refilled at a constant rate, allowing controlled bursts up to the bucket capacity</td>\n<td>Core algorithm used throughout all milestones</td>\n</tr>\n<tr>\n<td><strong>Leaky Bucket Algorithm</strong></td>\n<td>A rate limiting algorithm that processes requests at a perfectly smooth, constant rate with queuing for excess requests</td>\n<td>Alternative algorithm discussed in future extensions</td>\n</tr>\n<tr>\n<td><strong>Sliding Window Algorithm</strong></td>\n<td>A rate limiting algorithm that maintains a continuous, time-based view of request history with precise rate enforcement over any time period</td>\n<td>Alternative algorithm with exact rate guarantees</td>\n</tr>\n<tr>\n<td><strong>Fixed Window Algorithm</strong></td>\n<td>A rate limiting algorithm that divides time into discrete intervals and counts requests per interval, resetting at interval boundaries</td>\n<td>Simple but imprecise algorithm with burst issues</td>\n</tr>\n<tr>\n<td><strong>Rate Limiting</strong></td>\n<td>The practice of controlling the frequency of requests or operations to protect system resources and ensure fair usage</td>\n<td>Overall system protection strategy</td>\n</tr>\n<tr>\n<td><strong>Burst Handling</strong></td>\n<td>Allowing short periods of high request rates up to the token bucket capacity, accommodating natural traffic spikes</td>\n<td>Key advantage of token bucket over fixed rate limiting</td>\n</tr>\n<tr>\n<td><strong>Token Consumption</strong></td>\n<td>The process of deducting a specified number of tokens from a bucket when processing a request</td>\n<td>Core operation in token bucket algorithm</td>\n</tr>\n<tr>\n<td><strong>Token Generation</strong></td>\n<td>The process of adding tokens to a bucket at a configured rate based on elapsed time</td>\n<td>Automatic refill mechanism in token bucket</td>\n</tr>\n<tr>\n<td><strong>Token Refill Rate</strong></td>\n<td>The number of tokens added per second to maintain the configured request rate</td>\n<td>Configuration parameter controlling sustained rate</td>\n</tr>\n<tr>\n<td><strong>Bucket Capacity</strong></td>\n<td>The maximum number of tokens that can be stored in a bucket, determining maximum burst size</td>\n<td>Configuration parameter controlling burst allowance</td>\n</tr>\n<tr>\n<td><strong>Bucket Overflow</strong></td>\n<td>The condition when generated tokens would exceed bucket capacity, resulting in discarded tokens</td>\n<td>Natural behavior preventing unbounded token accumulation</td>\n</tr>\n</tbody></table>\n<h3 id=\"system-architecture-terminology\">System Architecture Terminology</h3>\n<p>Rate limiting systems involve multiple components working together, each with specific responsibilities and interaction patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Per-Client Rate Limiting</strong></td>\n<td>Independent rate limits maintained for each API consumer, preventing one client from affecting others</td>\n<td>Milestone 2 core concept</td>\n</tr>\n<tr>\n<td><strong>Client Identification Strategy</strong></td>\n<td>The method used to extract and normalize unique identifiers for API consumers from request data</td>\n<td>Critical for per-client tracking</td>\n</tr>\n<tr>\n<td><strong>Client Bucket Tracker</strong></td>\n<td>Component responsible for managing separate token buckets for each client with efficient storage and cleanup</td>\n<td>Milestone 2 main component</td>\n</tr>\n<tr>\n<td><strong>Stale Bucket Cleanup</strong></td>\n<td>Background process that removes inactive client buckets to prevent memory leaks</td>\n<td>Essential memory management process</td>\n</tr>\n<tr>\n<td><strong>Bucket Lifecycle Management</strong></td>\n<td>The complete process of creating, accessing, aging, and cleaning up client-specific token buckets</td>\n<td>Comprehensive bucket management strategy</td>\n</tr>\n<tr>\n<td><strong>HTTP Middleware Integration</strong></td>\n<td>The pattern of intercepting HTTP requests in the web framework pipeline to apply rate limiting</td>\n<td>Milestone 3 integration approach</td>\n</tr>\n<tr>\n<td><strong>Middleware Design Pattern</strong></td>\n<td>Interceptor pattern where middleware wraps the request processing pipeline to add cross-cutting concerns</td>\n<td>Standard web framework architecture</td>\n</tr>\n<tr>\n<td><strong>Per-Endpoint Rate Limiting</strong></td>\n<td>Different rate limits configured for different API endpoints or routes based on resource consumption</td>\n<td>Advanced configuration capability</td>\n</tr>\n<tr>\n<td><strong>Rate Limit Composition</strong></td>\n<td>Applying multiple independent rate limits simultaneously, such as per-client and per-endpoint limits</td>\n<td>Complex rate limiting scenarios</td>\n</tr>\n<tr>\n<td><strong>Framework-Agnostic Core</strong></td>\n<td>Rate limiting logic separated from web framework-specific code for portability and testability</td>\n<td>Clean architecture principle</td>\n</tr>\n</tbody></table>\n<h3 id=\"distributed-systems-terminology\">Distributed Systems Terminology</h3>\n<p>Scaling rate limiting across multiple servers introduces distributed system challenges and specialized terminology.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Distributed Consistency</strong></td>\n<td>Maintaining consistent rate limits across multiple server instances sharing the same logical rate limit</td>\n<td>Core challenge in Milestone 4</td>\n</tr>\n<tr>\n<td><strong>Single Source of Truth</strong></td>\n<td>Centralized authoritative storage (Redis) for token bucket state across all servers in the cluster</td>\n<td>Distributed architecture principle</td>\n</tr>\n<tr>\n<td><strong>Atomic Operations</strong></td>\n<td>Read-modify-write operations that complete without interruption, preventing race conditions in concurrent access</td>\n<td>Essential for distributed token consumption</td>\n</tr>\n<tr>\n<td><strong>Circuit Breaker Pattern</strong></td>\n<td>Failure handling pattern that detects issues and switches to fallback mode to prevent cascade failures</td>\n<td>Distributed resilience mechanism</td>\n</tr>\n<tr>\n<td><strong>Local Fallback Buckets</strong></td>\n<td>In-memory token buckets used during Redis outages with conservative limits to maintain protection</td>\n<td>Graceful degradation strategy</td>\n</tr>\n<tr>\n<td><strong>Graceful Degradation</strong></td>\n<td>Reducing functionality rather than complete failure while maintaining core protection during partial system failures</td>\n<td>Resilience design principle</td>\n</tr>\n<tr>\n<td><strong>Progressive Degradation Levels</strong></td>\n<td>Graduated failure responses maintaining different levels of functionality under various failure conditions</td>\n<td>Sophisticated failure handling</td>\n</tr>\n<tr>\n<td><strong>Clock Synchronization</strong></td>\n<td>Ensuring accurate time measurements across distributed servers for consistent token refill calculations</td>\n<td>Distributed timing challenge</td>\n</tr>\n<tr>\n<td><strong>Clock Drift Correction</strong></td>\n<td>Gradual adjustment of timing calculations to compensate for server time differences</td>\n<td>Clock synchronization solution</td>\n</tr>\n<tr>\n<td><strong>Thundering Herd</strong></td>\n<td>Problem where multiple servers simultaneously attempt recovery operations, overwhelming the target system</td>\n<td>Distributed coordination anti-pattern</td>\n</tr>\n<tr>\n<td><strong>LRU Eviction</strong></td>\n<td>Least Recently Used eviction strategy for managing memory-limited local buckets during fallback scenarios</td>\n<td>Memory management strategy</td>\n</tr>\n</tbody></table>\n<h3 id=\"configuration-and-management-terminology\">Configuration and Management Terminology</h3>\n<p>Rate limiting systems require sophisticated configuration and management capabilities to handle diverse operational requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Hierarchical Resolution Strategy</strong></td>\n<td>Prioritizing specific configurations over general ones when multiple rate limit rules apply to a request</td>\n<td>Configuration precedence system</td>\n</tr>\n<tr>\n<td><strong>Client Override Configuration</strong></td>\n<td>Specific rate limits configured for individual clients, typically for premium tiers or special agreements</td>\n<td>Per-client customization capability</td>\n</tr>\n<tr>\n<td><strong>Endpoint-Specific Limits</strong></td>\n<td>Rate limits configured for specific API endpoints based on resource consumption or business requirements</td>\n<td>Resource-aware rate limiting</td>\n</tr>\n<tr>\n<td><strong>Dynamic Configuration Updates</strong></td>\n<td>Reloading rate limit rules without application restart to adapt to changing operational needs</td>\n<td>Runtime configuration management</td>\n</tr>\n<tr>\n<td><strong>Rate Limit Headers</strong></td>\n<td>Standard HTTP headers (<code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>, <code>Retry-After</code>) providing rate limit information to clients</td>\n<td>Client integration support</td>\n</tr>\n<tr>\n<td><strong>Adaptive Backoff Strategies</strong></td>\n<td>Client logic that adjusts request rates based on rate limit headers to optimize throughput and reduce rejections</td>\n<td>Client-side rate limiting cooperation</td>\n</tr>\n<tr>\n<td><strong>Emergency Memory Management</strong></td>\n<td>Aggressive cleanup procedures during memory pressure to maintain functionality under resource constraints</td>\n<td>Resource protection mechanism</td>\n</tr>\n<tr>\n<td><strong>Client Classification System</strong></td>\n<td>Automatic categorization of API consumers based on behavioral patterns for appropriate rate limit assignment</td>\n<td>Intelligent client management</td>\n</tr>\n</tbody></table>\n<h3 id=\"error-handling-and-recovery-terminology\">Error Handling and Recovery Terminology</h3>\n<p>Robust rate limiting requires comprehensive error handling and recovery mechanisms for various failure scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Race Conditions</strong></td>\n<td>Timing-dependent bugs where concurrent operations interfere, leading to inconsistent state or incorrect results</td>\n<td>Concurrency bug category</td>\n</tr>\n<tr>\n<td><strong>Token Calculation Errors</strong></td>\n<td>Arithmetic precision and overflow issues in token math, especially with large time gaps or high rates</td>\n<td>Numerical computation problems</td>\n</tr>\n<tr>\n<td><strong>Integer Overflow</strong></td>\n<td>Arithmetic overflow in token calculations when dealing with large time gaps or accumulated values</td>\n<td>Specific calculation error type</td>\n</tr>\n<tr>\n<td><strong>Floating Point Precision Errors</strong></td>\n<td>Accumulating precision loss in token calculations over time, leading to rate drift</td>\n<td>Numerical precision challenge</td>\n</tr>\n<tr>\n<td><strong>Client ID Normalization</strong></td>\n<td>Consistent formatting of client identifiers across different code paths to prevent duplicate bucket creation</td>\n<td>Data consistency requirement</td>\n</tr>\n<tr>\n<td><strong>Non-Atomic Operations</strong></td>\n<td>Redis operations that aren&#39;t executed as single atomic units, creating race condition opportunities</td>\n<td>Distributed consistency problem</td>\n</tr>\n<tr>\n<td><strong>Memory Leaks</strong></td>\n<td>Unbounded memory growth from client buckets that are never cleaned up, eventually exhausting system resources</td>\n<td>Resource management failure</td>\n</tr>\n<tr>\n<td><strong>Circuit Breaker States</strong></td>\n<td>The three states (CLOSED, OPEN, HALF_OPEN) that determine whether operations are allowed through the circuit breaker</td>\n<td>Circuit breaker state machine</td>\n</tr>\n<tr>\n<td><strong>Health Check Strategies</strong></td>\n<td>Systematic approaches to monitoring component health and detecting degraded performance or failures</td>\n<td>System monitoring methodology</td>\n</tr>\n<tr>\n<td><strong>Recovery Coordination</strong></td>\n<td>Preventing multiple components from simultaneously attempting recovery operations that could interfere</td>\n<td>Distributed recovery management</td>\n</tr>\n</tbody></table>\n<h3 id=\"testing-and-quality-assurance-terminology\">Testing and Quality Assurance Terminology</h3>\n<p>Comprehensive testing ensures rate limiting accuracy and reliability across various scenarios and load conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Unit Test Coverage</strong></td>\n<td>Testing individual components like token buckets, client tracking, and middleware in isolation from dependencies</td>\n<td>Component-level testing strategy</td>\n</tr>\n<tr>\n<td><strong>Integration and End-to-End Testing</strong></td>\n<td>Testing the complete request flow and distributed coordination scenarios with real dependencies</td>\n<td>System-level testing approach</td>\n</tr>\n<tr>\n<td><strong>Milestone Verification Checkpoints</strong></td>\n<td>After each milestone, specific behavior to verify and commands to run for validation of implementation progress</td>\n<td>Development milestone validation</td>\n</tr>\n<tr>\n<td><strong>Performance and Load Testing</strong></td>\n<td>Testing rate limiting accuracy under high concurrency and distributed load to validate system behavior at scale</td>\n<td>Scalability and performance validation</td>\n</tr>\n<tr>\n<td><strong>Load Test Result Validation</strong></td>\n<td>Verifying that rate limiting was accurate within acceptable tolerance during high-load scenarios</td>\n<td>Performance test analysis</td>\n</tr>\n<tr>\n<td><strong>Mock Time Provider</strong></td>\n<td>Controllable time provider for deterministic testing without waiting for real time passage</td>\n<td>Testing infrastructure component</td>\n</tr>\n<tr>\n<td><strong>Redis Test Manager</strong></td>\n<td>Component that manages Redis instances for testing, including setup, cleanup, and isolation</td>\n<td>Testing infrastructure for distributed scenarios</td>\n</tr>\n<tr>\n<td><strong>Fake Redis Implementation</strong></td>\n<td>In-memory Redis implementation for fast unit tests without external dependencies</td>\n<td>Testing performance optimization</td>\n</tr>\n<tr>\n<td><strong>Rate Limiting Accuracy Validation</strong></td>\n<td>Systematic verification that actual request rates match configured limits within acceptable error margins</td>\n<td>Quality assurance methodology</td>\n</tr>\n<tr>\n<td><strong>Concurrent Client Simulation</strong></td>\n<td>Testing technique that simulates multiple clients making simultaneous requests to validate thread safety</td>\n<td>Concurrency testing approach</td>\n</tr>\n</tbody></table>\n<h3 id=\"monitoring-and-observability-terminology\">Monitoring and Observability Terminology</h3>\n<p>Production rate limiting systems require comprehensive monitoring and debugging capabilities for operational excellence.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Rate Limit Decision Tracking</strong></td>\n<td>Recording rate limiting decisions asynchronously for analysis and monitoring without impacting request performance</td>\n<td>Operational monitoring capability</td>\n</tr>\n<tr>\n<td><strong>Client Behavior Analysis</strong></td>\n<td>Statistical examination of client request patterns for classification and optimization opportunities</td>\n<td>Intelligence gathering for optimization</td>\n</tr>\n<tr>\n<td><strong>Performance Tracking</strong></td>\n<td>Measuring operation timing and detecting anomalies in rate limiting performance characteristics</td>\n<td>System performance monitoring</td>\n</tr>\n<tr>\n<td><strong>Debugging Support</strong></td>\n<td>Comprehensive logging and state inspection capabilities for troubleshooting rate limiting issues</td>\n<td>Operational troubleshooting tools</td>\n</tr>\n<tr>\n<td><strong>Health Monitoring</strong></td>\n<td>Systematic monitoring of all rate limiting components with automatic failure detection and recovery coordination</td>\n<td>System health management</td>\n</tr>\n<tr>\n<td><strong>Metric Data Points</strong></td>\n<td>Structured data representing rate limiting metrics with timestamps, labels, and metadata for analysis</td>\n<td>Monitoring data structure</td>\n</tr>\n<tr>\n<td><strong>Real-Time Dashboard</strong></td>\n<td>WebSocket-based live monitoring interface providing immediate visibility into rate limiting metrics and system health</td>\n<td>Operational visibility tool</td>\n</tr>\n<tr>\n<td><strong>Component Health Status</strong></td>\n<td>Enumerated health states (HEALTHY, DEGRADED, UNHEALTHY, UNKNOWN) for systematic health tracking</td>\n<td>Health monitoring classification</td>\n</tr>\n<tr>\n<td><strong>Timing Context Management</strong></td>\n<td>Context manager pattern for measuring operation timing with automatic performance tracking</td>\n<td>Performance measurement infrastructure</td>\n</tr>\n<tr>\n<td><strong>Analytics Query System</strong></td>\n<td>Flexible query interface for analyzing historical rate limiting data with various filters and aggregations</td>\n<td>Data analysis capability</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-features-and-extensions-terminology\">Advanced Features and Extensions Terminology</h3>\n<p>Future enhancements and production-grade features extend basic rate limiting with sophisticated capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Dynamic Rate Adjustment</strong></td>\n<td>Automatic modification of rate limits based on system conditions and client behavior patterns</td>\n<td>Advanced adaptive capability</td>\n</tr>\n<tr>\n<td><strong>Intelligent Client Classification</strong></td>\n<td>Automatic categorization of API consumers based on behavioral patterns for appropriate treatment</td>\n<td>Machine learning enhanced classification</td>\n</tr>\n<tr>\n<td><strong>Geographic Rate Limiting</strong></td>\n<td>Location-based rate limit variations for regional compliance and security requirements</td>\n<td>Compliance and security feature</td>\n</tr>\n<tr>\n<td><strong>Behavioral Analysis</strong></td>\n<td>Statistical examination of client request patterns for classification and optimization insights</td>\n<td>Intelligence and optimization capability</td>\n</tr>\n<tr>\n<td><strong>Traffic Shaping</strong></td>\n<td>Modification of request timing patterns for downstream system protection and load management</td>\n<td>Advanced traffic management</td>\n</tr>\n<tr>\n<td><strong>Algorithm Selection Framework</strong></td>\n<td>Configurable system for choosing appropriate rate limiting algorithms per use case or client type</td>\n<td>Multi-algorithm support architecture</td>\n</tr>\n<tr>\n<td><strong>Extension Architecture</strong></td>\n<td>Pluggable system design enabling modular enhancement of core rate limiting functionality</td>\n<td>Extensibility framework</td>\n</tr>\n<tr>\n<td><strong>Hot Reload Configuration</strong></td>\n<td>Runtime configuration updates without service restart for operational flexibility</td>\n<td>Advanced configuration management</td>\n</tr>\n<tr>\n<td><strong>Client Profiling System</strong></td>\n<td>Comprehensive tracking of client characteristics including geographic regions, endpoints used, and behavior patterns</td>\n<td>Advanced client intelligence</td>\n</tr>\n<tr>\n<td><strong>Adjustment Context Analysis</strong></td>\n<td>Evaluation of current system conditions including CPU, memory, latency, and error rates for rate adjustment decisions</td>\n<td>Adaptive system optimization</td>\n</tr>\n</tbody></table>\n<h3 id=\"redis-and-storage-terminology\">Redis and Storage Terminology</h3>\n<p>Distributed rate limiting relies heavily on Redis for shared state management with specific operational patterns and challenges.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Redis Lua Scripts</strong></td>\n<td>Server-side scripts that execute atomically on Redis, ensuring consistent read-modify-write operations for token consumption</td>\n<td>Distributed atomicity mechanism</td>\n</tr>\n<tr>\n<td><strong>Connection Pool Management</strong></td>\n<td>Efficient management of Redis connections with proper sizing, timeout handling, and connection recycling</td>\n<td>Redis performance optimization</td>\n</tr>\n<tr>\n<td><strong>Redis Key Design</strong></td>\n<td>Strategic naming and organization of Redis keys for efficient storage, retrieval, and cleanup operations</td>\n<td>Data organization strategy</td>\n</tr>\n<tr>\n<td><strong>Batch Cleanup Operations</strong></td>\n<td>Processing multiple cleanup operations together to improve efficiency and reduce Redis load</td>\n<td>Performance optimization technique</td>\n</tr>\n<tr>\n<td><strong>Redis Connection Manager</strong></td>\n<td>Component managing Redis connections with failover, circuit breaker logic, and health monitoring</td>\n<td>Redis infrastructure management</td>\n</tr>\n<tr>\n<td><strong>Atomic Token Consumption</strong></td>\n<td>Single Redis operation that checks available tokens, deducts requested tokens, and updates bucket state without race conditions</td>\n<td>Core distributed operation</td>\n</tr>\n<tr>\n<td><strong>Redis Failure Recovery</strong></td>\n<td>Systematic approach to handling Redis outages with local fallback and automatic recovery when service returns</td>\n<td>Distributed resilience strategy</td>\n</tr>\n<tr>\n<td><strong>Connection Timeout Handling</strong></td>\n<td>Proper management of Redis operation timeouts with appropriate fallback behavior and retry logic</td>\n<td>Network reliability handling</td>\n</tr>\n<tr>\n<td><strong>Redis Health Monitoring</strong></td>\n<td>Continuous monitoring of Redis performance and availability with automatic circuit breaker activation</td>\n<td>Distributed system health management</td>\n</tr>\n<tr>\n<td><strong>Key Expiration Strategy</strong></td>\n<td>Using Redis TTL mechanisms for automatic cleanup of stale rate limiting data</td>\n<td>Automated data lifecycle management</td>\n</tr>\n</tbody></table>\n<h3 id=\"http-and-web-framework-terminology\">HTTP and Web Framework Terminology</h3>\n<p>Integration with web frameworks requires understanding of HTTP standards and middleware patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP 429 Too Many Requests</strong></td>\n<td>Standard HTTP status code indicating that the client has exceeded the configured rate limit</td>\n<td>Rate limiting response standard</td>\n</tr>\n<tr>\n<td><strong>Retry-After Header</strong></td>\n<td>HTTP header indicating the number of seconds until the next request will be allowed</td>\n<td>Client guidance for retry timing</td>\n</tr>\n<tr>\n<td><strong>Rate Limit Response Headers</strong></td>\n<td>Standard headers (<code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>) providing current rate limit status to clients</td>\n<td>Client integration support</td>\n</tr>\n<tr>\n<td><strong>Client Identification Headers</strong></td>\n<td>HTTP headers used to identify clients, such as API keys, authentication tokens, or custom identifiers</td>\n<td>Client tracking mechanism</td>\n</tr>\n<tr>\n<td><strong>Request Context Extraction</strong></td>\n<td>Process of extracting relevant information from HTTP requests for rate limiting decisions</td>\n<td>Request processing step</td>\n</tr>\n<tr>\n<td><strong>Endpoint Path Normalization</strong></td>\n<td>Consistent formatting of API endpoint paths for rate limiting grouping and configuration</td>\n<td>Path processing standardization</td>\n</tr>\n<tr>\n<td><strong>Skip Rate Limiting Headers</strong></td>\n<td>Special HTTP headers that allow bypassing rate limiting for internal or administrative requests</td>\n<td>Operational bypass mechanism</td>\n</tr>\n<tr>\n<td><strong>Framework Middleware Integration</strong></td>\n<td>Proper integration with web framework request processing pipelines (Flask, Express, etc.)</td>\n<td>Web framework compatibility</td>\n</tr>\n<tr>\n<td><strong>Response Enhancement</strong></td>\n<td>Adding rate limiting headers and information to successful HTTP responses for client awareness</td>\n<td>Client communication enhancement</td>\n</tr>\n<tr>\n<td><strong>Error Response Formatting</strong></td>\n<td>Standardized JSON error responses for rate limit exceeded conditions with appropriate detail levels</td>\n<td>API error handling standard</td>\n</tr>\n</tbody></table>\n<h3 id=\"time-and-precision-terminology\">Time and Precision Terminology</h3>\n<p>Accurate time handling is critical for rate limiting algorithms, especially in distributed environments.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Time Precision Handling</strong></td>\n<td>Managing floating-point time calculations with appropriate precision to prevent accumulation errors</td>\n<td>Numerical accuracy requirement</td>\n</tr>\n<tr>\n<td><strong>Elapsed Time Calculation</strong></td>\n<td>Computing time differences for token refill operations with proper handling of clock changes</td>\n<td>Core timing operation</td>\n</tr>\n<tr>\n<td><strong>Token Refill Timing</strong></td>\n<td>Calculating the exact number of tokens to add based on elapsed time and configured refill rate</td>\n<td>Algorithm timing calculation</td>\n</tr>\n<tr>\n<td><strong>Clock Change Detection</strong></td>\n<td>Identifying when system clocks are adjusted forward or backward and handling appropriately</td>\n<td>System time stability</td>\n</tr>\n<tr>\n<td><strong>Time-Based Token Generation</strong></td>\n<td>Algorithm for adding tokens to buckets based on elapsed time since last refill operation</td>\n<td>Core token bucket mechanism</td>\n</tr>\n<tr>\n<td><strong>Timestamp Normalization</strong></td>\n<td>Converting various time representations to consistent internal format for reliable calculations</td>\n<td>Time data consistency</td>\n</tr>\n<tr>\n<td><strong>Sub-Second Precision</strong></td>\n<td>Handling time calculations with millisecond or microsecond precision for accurate rate limiting</td>\n<td>High-precision timing requirements</td>\n</tr>\n<tr>\n<td><strong>Time Provider Abstraction</strong></td>\n<td>Interface allowing controllable time sources for testing and potential timezone handling</td>\n<td>Testing and flexibility infrastructure</td>\n</tr>\n<tr>\n<td><strong>Rate Calculation Accuracy</strong></td>\n<td>Ensuring that actual rates match configured rates within acceptable tolerance over various time periods</td>\n<td>Algorithm correctness validation</td>\n</tr>\n<tr>\n<td><strong>Time Window Management</strong></td>\n<td>Managing time-based windows for sliding window algorithms and request history tracking</td>\n<td>Time-based algorithm support</td>\n</tr>\n</tbody></table>\n","toc":[{"level":1,"text":"Rate Limiter: Design Document","id":"rate-limiter-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Nightclub Bouncer","id":"mental-model-the-nightclub-bouncer"},{"level":3,"text":"Why Rate Limiting Matters","id":"why-rate-limiting-matters"},{"level":3,"text":"Existing Rate Limiting Algorithms","id":"existing-rate-limiting-algorithms"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Functional Goals","id":"functional-goals"},{"level":3,"text":"Non-Functional Goals","id":"non-functional-goals"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Responsibilities","id":"component-responsibilities"},{"level":3,"text":"Request Processing Flow","id":"request-processing-flow"},{"level":3,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Core Data Types","id":"core-data-types"},{"level":4,"text":"Token Bucket State and Configuration","id":"token-bucket-state-and-configuration"},{"level":4,"text":"Token Consumption Results","id":"token-consumption-results"},{"level":4,"text":"Global Rate Limit Configuration","id":"global-rate-limit-configuration"},{"level":3,"text":"Configuration Model","id":"configuration-model"},{"level":4,"text":"Environment-Based Configuration","id":"environment-based-configuration"},{"level":4,"text":"Configuration Loading and Validation","id":"configuration-loading-and-validation"},{"level":4,"text":"Hierarchical Override Resolution","id":"hierarchical-override-resolution"},{"level":3,"text":"Persistence and Storage","id":"persistence-and-storage"},{"level":4,"text":"In-Memory Storage Architecture","id":"in-memory-storage-architecture"},{"level":4,"text":"Distributed Redis Storage","id":"distributed-redis-storage"},{"level":4,"text":"Storage Consistency and Atomicity","id":"storage-consistency-and-atomicity"},{"level":4,"text":"Data Serialization and Wire Format","id":"data-serialization-and-wire-format"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Data Type Definitions","id":"core-data-type-definitions"},{"level":4,"text":"Configuration Loading Implementation","id":"configuration-loading-implementation"},{"level":4,"text":"Storage Interface Definition","id":"storage-interface-definition"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Token Bucket Algorithm Implementation","id":"token-bucket-algorithm-implementation"},{"level":3,"text":"Mental Model: Water Bucket with Holes","id":"mental-model-water-bucket-with-holes"},{"level":3,"text":"Token Generation and Refill Logic","id":"token-generation-and-refill-logic"},{"level":3,"text":"Token Consumption and Burst Handling","id":"token-consumption-and-burst-handling"},{"level":3,"text":"Thread Safety and Concurrency","id":"thread-safety-and-concurrency"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Configuration Structure (Complete Implementation)","id":"core-configuration-structure-complete-implementation"},{"level":4,"text":"Token Bucket Core Logic Skeleton","id":"token-bucket-core-logic-skeleton"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone 1 Verification Checkpoint","id":"milestone-1-verification-checkpoint"},{"level":2,"text":"Per-Client Rate Limiting","id":"per-client-rate-limiting"},{"level":3,"text":"Client Identification Strategies","id":"client-identification-strategies"},{"level":3,"text":"Client Bucket Storage","id":"client-bucket-storage"},{"level":3,"text":"Stale Bucket Cleanup","id":"stale-bucket-cleanup"},{"level":3,"text":"Per-Client Limit Overrides","id":"per-client-limit-overrides"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Complete Starter Code: Client Identifier Management","id":"complete-starter-code-client-identifier-management"},{"level":4,"text":"Complete Starter Code: Background Cleanup Manager","id":"complete-starter-code-background-cleanup-manager"},{"level":4,"text":"Core Logic Skeleton: ClientBucketTracker","id":"core-logic-skeleton-clientbuckettracker"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"HTTP Middleware Integration","id":"http-middleware-integration"},{"level":3,"text":"Mental Model: The Nightclub Bouncer","id":"mental-model-the-nightclub-bouncer"},{"level":3,"text":"Middleware Design Pattern","id":"middleware-design-pattern"},{"level":3,"text":"HTTP Response Handling","id":"http-response-handling"},{"level":3,"text":"Framework Integration Points","id":"framework-integration-points"},{"level":3,"text":"Per-Endpoint Rate Limiting","id":"per-endpoint-rate-limiting"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Distributed Rate Limiting","id":"distributed-rate-limiting"},{"level":3,"text":"Mental Model: The Multi-Branch Bank with Central Ledger","id":"mental-model-the-multi-branch-bank-with-central-ledger"},{"level":3,"text":"Distributed System Challenges","id":"distributed-system-challenges"},{"level":3,"text":"Redis-Based Token Storage","id":"redis-based-token-storage"},{"level":3,"text":"Atomic Operations with Lua Scripts","id":"atomic-operations-with-lua-scripts"},{"level":3,"text":"Redis Failure Handling","id":"redis-failure-handling"},{"level":3,"text":"Clock Synchronization Considerations","id":"clock-synchronization-considerations"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Component Interactions and Data Flow","id":"component-interactions-and-data-flow"},{"level":3,"text":"Mental Model: The Orchestra Performance","id":"mental-model-the-orchestra-performance"},{"level":2,"text":"Request Processing Sequence","id":"request-processing-sequence"},{"level":3,"text":"Single Server Processing Flow","id":"single-server-processing-flow"},{"level":3,"text":"Distributed Processing Flow","id":"distributed-processing-flow"},{"level":2,"text":"Inter-Component Communication","id":"inter-component-communication"},{"level":3,"text":"Interface Contracts and Data Exchange","id":"interface-contracts-and-data-exchange"},{"level":3,"text":"Message Formats and Serialization","id":"message-formats-and-serialization"},{"level":2,"text":"Message and Data Formats","id":"message-and-data-formats"},{"level":3,"text":"Request Processing Data Structures","id":"request-processing-data-structures"},{"level":3,"text":"Response Message Formats","id":"response-message-formats"},{"level":3,"text":"Storage and Persistence Formats","id":"storage-and-persistence-formats"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"System Failure Modes","id":"system-failure-modes"},{"level":4,"text":"Token Bucket Component Failures","id":"token-bucket-component-failures"},{"level":4,"text":"Client Tracking Component Failures","id":"client-tracking-component-failures"},{"level":4,"text":"HTTP Middleware Failures","id":"http-middleware-failures"},{"level":4,"text":"Distributed Storage Failures","id":"distributed-storage-failures"},{"level":3,"text":"Edge Cases and Corner Conditions","id":"edge-cases-and-corner-conditions"},{"level":4,"text":"Clock Synchronization Edge Cases","id":"clock-synchronization-edge-cases"},{"level":4,"text":"Burst Scenario Edge Cases","id":"burst-scenario-edge-cases"},{"level":4,"text":"Client Identification Edge Cases","id":"client-identification-edge-cases"},{"level":3,"text":"Recovery and Degradation","id":"recovery-and-degradation"},{"level":4,"text":"Failure Recovery Strategies","id":"failure-recovery-strategies"},{"level":4,"text":"Data Consistency Recovery","id":"data-consistency-recovery"},{"level":4,"text":"Memory and Resource Recovery","id":"memory-and-resource-recovery"},{"level":4,"text":"Operational Recovery Procedures","id":"operational-recovery-procedures"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Circuit Breaker Infrastructure Code","id":"circuit-breaker-infrastructure-code"},{"level":4,"text":"Health Monitor Infrastructure Code","id":"health-monitor-infrastructure-code"},{"level":4,"text":"Core Error Handling Skeleton Code","id":"core-error-handling-skeleton-code"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Common Implementation Pitfalls","id":"common-implementation-pitfalls"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Mental Model: The Quality Assurance Pyramid","id":"mental-model-the-quality-assurance-pyramid"},{"level":3,"text":"Unit Test Coverage","id":"unit-test-coverage"},{"level":4,"text":"Token Bucket Algorithm Testing","id":"token-bucket-algorithm-testing"},{"level":4,"text":"Client Bucket Tracker Testing","id":"client-bucket-tracker-testing"},{"level":4,"text":"HTTP Middleware Component Testing","id":"http-middleware-component-testing"},{"level":3,"text":"Integration and End-to-End Testing","id":"integration-and-end-to-end-testing"},{"level":4,"text":"Component Integration Testing","id":"component-integration-testing"},{"level":4,"text":"End-to-End Request Flow Testing","id":"end-to-end-request-flow-testing"},{"level":4,"text":"Distributed Coordination Testing","id":"distributed-coordination-testing"},{"level":3,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"},{"level":4,"text":"Milestone 1: Token Bucket Implementation Checkpoint","id":"milestone-1-token-bucket-implementation-checkpoint"},{"level":4,"text":"Milestone 2: Per-Client Rate Limiting Checkpoint","id":"milestone-2-per-client-rate-limiting-checkpoint"},{"level":4,"text":"Milestone 3: HTTP Middleware Integration Checkpoint","id":"milestone-3-http-middleware-integration-checkpoint"},{"level":4,"text":"Milestone 4: Distributed Rate Limiting Checkpoint","id":"milestone-4-distributed-rate-limiting-checkpoint"},{"level":3,"text":"Performance and Load Testing","id":"performance-and-load-testing"},{"level":4,"text":"Load Testing Methodology","id":"load-testing-methodology"},{"level":4,"text":"Single-Instance Performance Testing","id":"single-instance-performance-testing"},{"level":4,"text":"Distributed Performance Testing","id":"distributed-performance-testing"},{"level":4,"text":"Performance Regression Testing","id":"performance-regression-testing"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Test File Structure","id":"recommended-test-file-structure"},{"level":4,"text":"Core Test Infrastructure","id":"core-test-infrastructure"},{"level":4,"text":"Unit Test Implementation Examples","id":"unit-test-implementation-examples"},{"level":4,"text":"Milestone Verification Implementation","id":"milestone-verification-implementation"},{"level":4,"text":"Performance Test Implementation","id":"performance-test-implementation"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Common Implementation Bugs","id":"common-implementation-bugs"},{"level":4,"text":"Token Bucket Race Conditions","id":"token-bucket-race-conditions"},{"level":4,"text":"Token Calculation Arithmetic Errors","id":"token-calculation-arithmetic-errors"},{"level":4,"text":"Client Identification and Bucket Management Issues","id":"client-identification-and-bucket-management-issues"},{"level":4,"text":"Distributed Consistency Problems","id":"distributed-consistency-problems"},{"level":3,"text":"Debugging Techniques and Tools","id":"debugging-techniques-and-tools"},{"level":4,"text":"Comprehensive Logging Strategy","id":"comprehensive-logging-strategy"},{"level":4,"text":"Redis Inspection Techniques","id":"redis-inspection-techniques"},{"level":4,"text":"Concurrency Debugging Approaches","id":"concurrency-debugging-approaches"},{"level":4,"text":"Performance Profiling for Rate Limiters","id":"performance-profiling-for-rate-limiters"},{"level":3,"text":"Symptom-Cause-Fix Reference","id":"symptom-cause-fix-reference"},{"level":4,"text":"Advanced Debugging Scenarios","id":"advanced-debugging-scenarios"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Debugging Infrastructure Starter Code","id":"debugging-infrastructure-starter-code"},{"level":4,"text":"Core Logic Debugging Skeleton","id":"core-logic-debugging-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Alternative Rate Limiting Algorithms","id":"alternative-rate-limiting-algorithms"},{"level":4,"text":"Sliding Window Rate Limiting","id":"sliding-window-rate-limiting"},{"level":4,"text":"Leaky Bucket Rate Limiting","id":"leaky-bucket-rate-limiting"},{"level":4,"text":"Algorithm Selection Framework","id":"algorithm-selection-framework"},{"level":3,"text":"Advanced Rate Limiting Features","id":"advanced-rate-limiting-features"},{"level":4,"text":"Dynamic Rate Adjustment","id":"dynamic-rate-adjustment"},{"level":4,"text":"Intelligent Client Classification","id":"intelligent-client-classification"},{"level":4,"text":"Geographic and Network-Based Rate Limiting","id":"geographic-and-network-based-rate-limiting"},{"level":3,"text":"Monitoring and Observability","id":"monitoring-and-observability"},{"level":4,"text":"Comprehensive Metrics Collection","id":"comprehensive-metrics-collection"},{"level":4,"text":"Real-Time Dashboard and Alerting","id":"real-time-dashboard-and-alerting"},{"level":4,"text":"Advanced Analytics and Insights","id":"advanced-analytics-and-insights"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Extension Architecture Integration","id":"extension-architecture-integration"},{"level":4,"text":"Algorithm Extension Framework","id":"algorithm-extension-framework"},{"level":4,"text":"Dynamic Adjustment Implementation Framework","id":"dynamic-adjustment-implementation-framework"},{"level":4,"text":"Monitoring Extension Points","id":"monitoring-extension-points"},{"level":4,"text":"Milestone Checkpoint: Extensions Integration","id":"milestone-checkpoint-extensions-integration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Extension Architecture Implementation","id":"extension-architecture-implementation"},{"level":4,"text":"Advanced Algorithm Implementation","id":"advanced-algorithm-implementation"},{"level":4,"text":"Dynamic Intelligence Framework","id":"dynamic-intelligence-framework"},{"level":4,"text":"Production Monitoring Implementation","id":"production-monitoring-implementation"},{"level":4,"text":"Extension Testing and Validation","id":"extension-testing-and-validation"},{"level":4,"text":"Milestone Checkpoint: Extensions Validation","id":"milestone-checkpoint-extensions-validation"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mental Model: The Technical Dictionary","id":"mental-model-the-technical-dictionary"},{"level":3,"text":"Core Rate Limiting Terminology","id":"core-rate-limiting-terminology"},{"level":3,"text":"System Architecture Terminology","id":"system-architecture-terminology"},{"level":3,"text":"Distributed Systems Terminology","id":"distributed-systems-terminology"},{"level":3,"text":"Configuration and Management Terminology","id":"configuration-and-management-terminology"},{"level":3,"text":"Error Handling and Recovery Terminology","id":"error-handling-and-recovery-terminology"},{"level":3,"text":"Testing and Quality Assurance Terminology","id":"testing-and-quality-assurance-terminology"},{"level":3,"text":"Monitoring and Observability Terminology","id":"monitoring-and-observability-terminology"},{"level":3,"text":"Advanced Features and Extensions Terminology","id":"advanced-features-and-extensions-terminology"},{"level":3,"text":"Redis and Storage Terminology","id":"redis-and-storage-terminology"},{"level":3,"text":"HTTP and Web Framework Terminology","id":"http-and-web-framework-terminology"},{"level":3,"text":"Time and Precision Terminology","id":"time-and-precision-terminology"}],"title":"Rate Limiter: Design Document","markdown":"# Rate Limiter: Design Document\n\n\n## Overview\n\nA distributed rate limiting system that uses the token bucket algorithm to protect services from abuse and excessive load. The key architectural challenge is maintaining consistent per-client rate limits across multiple server instances while handling high-throughput scenarios with minimal latency overhead.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones - this section provides foundational understanding needed throughout the project\n\nThe digital world operates much like the physical one when it comes to capacity constraints and resource protection. Just as a restaurant can only serve so many customers at once before service quality degrades, web services and APIs have finite computational resources that must be carefully managed to maintain stability and fairness. This section explores the fundamental problem of rate limiting - the practice of controlling the rate at which clients can make requests to a service.\n\n### Mental Model: The Nightclub Bouncer\n\nTo understand rate limiting intuitively, imagine yourself as a **bouncer at an exclusive nightclub**. Your nightclub has a maximum capacity of 200 people, and fire safety regulations require you to maintain this limit strictly. Additionally, the club's management wants to ensure a pleasant experience for all guests by preventing overcrowding that would degrade the atmosphere.\n\nAs the bouncer, you implement several strategies that directly parallel rate limiting algorithms:\n\n**The VIP Rope System (Token Bucket)**: You maintain a velvet rope system where VIP tokens are distributed at a steady rate - say 10 tokens every minute. Each person entering the club must present a token. If someone arrives when no tokens are available, they must wait until the next token is issued. However, if fewer people arrive during a quiet period, tokens accumulate up to a maximum of 50 tokens, allowing for sudden bursts of arrivals during peak times. This system smooths out the flow while accommodating natural variations in arrival patterns.\n\n**The Hourly Headcount (Fixed Window)**: Alternatively, you might count exactly how many people entered each hour and enforce a strict limit of 60 people per hour. At the stroke of each hour, the counter resets to zero. This approach is simple to track but can lead to problematic scenarios - imagine 60 people arriving at 11:59 PM, then another 60 at 12:01 AM, overwhelming the club's capacity despite technically following the rules.\n\n**The Rolling Average (Sliding Window)**: A more sophisticated approach involves tracking the number of people who entered in any rolling 60-minute period. This prevents the \"burst at window boundaries\" problem but requires more complex bookkeeping - you need to remember exactly when each person entered and continuously calculate rolling totals.\n\nThe nightclub analogy reveals why rate limiting is essential: **without controlled entry, the venue becomes overcrowded, service quality plummets, existing customers have a poor experience, and the system (club) can become completely overwhelmed and unusable**.\n\nIn the digital realm, your web service is the nightclub, incoming HTTP requests are the patrons, and computational resources (CPU, memory, database connections) represent the venue's capacity. Just as the bouncer protects the club's atmosphere and safety, a rate limiter protects your service's performance and availability.\n\n### Why Rate Limiting Matters\n\nRate limiting serves as a critical protective mechanism against various forms of service abuse and resource exhaustion. Understanding the concrete scenarios where uncontrolled request rates cause system failures helps illustrate why this protection is essential.\n\n**Denial of Service Protection**: Consider an e-commerce API serving product catalog requests. Under normal conditions, the service handles 1,000 requests per second comfortably. However, a malicious actor launches an attack sending 50,000 requests per second from distributed sources. Without rate limiting, the database connection pool (typically 100-200 connections) becomes exhausted within seconds. New legitimate requests cannot obtain database connections and begin timing out. The web server's memory consumption spikes as it queues thousands of pending requests, eventually triggering out-of-memory errors that crash the entire service. Rate limiting prevents this cascade by rejecting excessive requests before they consume critical resources.\n\n**Resource Starvation Prevention**: A social media platform's API serves multiple client types - mobile apps, web interfaces, and third-party integrations. Without rate limiting, a poorly implemented third-party bot making 10,000 profile requests per minute can monopolize the database's read capacity. This causes response times for mobile app users to degrade from 200ms to 5+ seconds, creating an unacceptable user experience. Per-client rate limiting ensures that no single actor can starve others of service access.\n\n**Cost Control in Cloud Environments**: Modern applications often depend on external APIs that charge per request - payment processors, geocoding services, or machine learning APIs. A bug in client code that retries failed requests without exponential backoff can generate millions of API calls in hours, resulting in unexpected bills of thousands of dollars. Rate limiting acts as a financial circuit breaker, capping the maximum possible cost exposure from runaway request patterns.\n\n**Fair Resource Allocation**: A public API serving both free and premium tiers needs to ensure that free users don't consume resources intended for paying customers. Without rate limiting, free users making unlimited requests can degrade service quality for premium subscribers who expect guaranteed performance levels. Differential rate limits (e.g., 100 requests/hour for free users, 10,000 requests/hour for premium) enforce the intended service tiers and business model.\n\n**Preventing Resource Cascades**: In microservice architectures, excessive load on one service can trigger failures across the entire system. When Service A becomes overloaded and starts responding slowly, upstream services waiting for responses begin accumulating connection pools and memory usage. These services then become overloaded and fail, creating a cascade effect. Rate limiting at service boundaries prevents one overloaded component from bringing down the entire distributed system.\n\n**Data Scraping and Abuse Mitigation**: Public-facing APIs are frequently targeted by automated scrapers attempting to extract large datasets. A real estate website's API might be scraped by competitors trying to copy entire property listings databases. Without rate limiting, these scrapers can generate load equivalent to thousands of normal users, degrading service for legitimate visitors while potentially violating the service's terms of use and intellectual property rights.\n\n> The fundamental principle underlying all these scenarios is **resource finiteness** - every system has limited computational capacity, and uncontrolled consumption by some clients necessarily reduces availability for others. Rate limiting enforces fair sharing of finite resources.\n\n### Existing Rate Limiting Algorithms\n\nDifferent rate limiting algorithms make different trade-offs between implementation complexity, memory usage, burst handling, and fairness guarantees. Understanding these trade-offs is crucial for selecting the appropriate algorithm for your specific requirements.\n\n| Algorithm | Description | Pros | Cons | Best Use Cases |\n|-----------|-------------|------|------|----------------|\n| **Fixed Window** | Count requests in fixed time intervals (e.g., per minute). Reset counter at interval boundaries. | Simple implementation. Low memory usage (one counter per client). Easy to understand and debug. | **Burst problem**: 2x rate limit possible at window boundaries. Uneven traffic distribution. Poor user experience during resets. | Simple systems, internal APIs, scenarios where occasional bursts are acceptable |\n| **Sliding Window Log** | Maintain timestamped log of all requests. Count requests within rolling time window. | **Perfect accuracy**. No burst issues. Granular request tracking. | High memory usage (stores all timestamps). Expensive cleanup operations. Poor performance under high load. | Low-traffic APIs, scenarios requiring perfect accuracy, audit/compliance requirements |\n| **Sliding Window Counter** | Divide time into smaller buckets, estimate current window using weighted bucket counts. | Good burst prevention. **Lower memory** than sliding log. Reasonable accuracy approximation. | Complex implementation. Approximation can be inaccurate. Edge cases around bucket boundaries. | Medium-traffic APIs, balance between accuracy and performance |\n| **Token Bucket** | Generate tokens at fixed rate into bucket with maximum capacity. Each request consumes tokens. | **Excellent burst handling**. Intuitive mental model. Smooth traffic shaping. Natural rate smoothing. | Slightly more complex than fixed window. Requires careful time handling. Token refill calculations. | **Recommended for most cases**. High-traffic APIs, client-facing services, scenarios needing burst accommodation |\n| **Leaky Bucket** | Process requests at fixed rate regardless of arrival pattern. Queue excess requests up to capacity limit. | **Perfect rate smoothing**. Predictable output rate. Good for downstream protection. | Request queuing increases latency. Queue management complexity. Potential memory growth from queued requests. | Background processing, protecting slow downstream services, traffic shaping scenarios |\n\n> **Decision: Token Bucket Algorithm Selection**\n> - **Context**: We need an algorithm that handles realistic traffic patterns where clients naturally send bursts of requests (page loads, mobile app launches) while maintaining long-term rate limits\n> - **Options Considered**: Fixed window (simple but bursty), Sliding window (accurate but expensive), Token bucket (balanced approach)\n> - **Decision**: Implement token bucket algorithm as the primary rate limiting mechanism\n> - **Rationale**: Token bucket provides the optimal balance of burst accommodation, implementation complexity, and performance characteristics. It naturally handles the common pattern where clients need to send several requests quickly (burst) but should be limited over longer time periods\n> - **Consequences**: Slightly more complex implementation than fixed window, but significantly better user experience and more natural traffic shaping behavior\n\nThe token bucket algorithm's key advantage lies in its **burst accommodation philosophy**. Unlike fixed windows that either allow or deny requests based on arbitrary time boundaries, token buckets recognize that legitimate traffic patterns often involve short bursts of activity. A mobile app loading a dashboard might make 5-10 API calls within seconds to populate different UI components, but then remain quiet for minutes. Token buckets naturally accommodate this pattern by allowing accumulated tokens to be consumed quickly when needed.\n\n**Algorithm Behavior Comparison Example**: Consider a rate limit of 60 requests per minute, and a client that needs to make 10 requests at once, then waits 50 seconds before making another 10 requests.\n\n- **Fixed Window**: If the bursts happen to straddle a minute boundary (5 requests at 59.5 seconds, 5 requests at 0.5 seconds), the client might be blocked despite averaging well under the limit.\n- **Sliding Window**: Would allow both bursts but requires tracking timestamps of potentially hundreds of requests per client.\n- **Token Bucket**: With a capacity of 10 tokens and refill rate of 1 token per second, naturally allows the first burst (consuming accumulated tokens), refills during the quiet period, and allows the second burst. Memory usage remains constant (just current token count and last refill time).\n\nThis comparison illustrates why token buckets have become the preferred algorithm for client-facing APIs where user experience and natural traffic accommodation are priorities, while fixed windows remain suitable for internal services where simplicity outweighs sophistication.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones - this section establishes the scope and boundaries that guide implementation decisions throughout the project\n\nThink of building a rate limiter like designing the security system for a busy office building. You need to decide what you're protecting (which floors, which entrances), who gets access (employees, visitors, VIP guests), and what happens when limits are exceeded (polite redirection vs. security escort). Most importantly, you need to decide what you're NOT responsible for - you're not running the elevators, managing the parking garage, or handling the building's fire safety system. Clear boundaries prevent scope creep and ensure you build something focused and effective.\n\nThis rate limiter project has specific learning objectives around the token bucket algorithm, distributed systems coordination, and HTTP middleware patterns. By explicitly defining what we will and won't build, we can focus on the core concepts while avoiding unnecessary complexity that would distract from the learning goals.\n\n### Functional Goals\n\nThe **functional goals** define the core capabilities that users of our rate limiter will directly interact with. These are the features that determine whether the system successfully solves the rate limiting problem.\n\n**Token Bucket Rate Limiting**: The system must implement a proper token bucket algorithm that allows for burst traffic up to a configured limit while maintaining a steady average rate over time. Unlike simpler fixed-window approaches that can allow double the intended rate at window boundaries, the token bucket provides smooth rate limiting with controlled burst allowances. The implementation must handle token generation based on elapsed time, token consumption for each request, and proper overflow behavior when the bucket reaches capacity.\n\n**Per-Client Rate Limiting**: Each client must have an independent rate limit bucket, identified by either IP address or API key. This isolation ensures that one client's heavy usage cannot exhaust the rate limit quota for other clients. The system must efficiently manage potentially thousands of client buckets in memory while providing fast lookup and update operations. Client identification must be consistent and reliable, handling edge cases like clients behind NAT gateways or load balancers.\n\n**HTTP Integration**: The rate limiter must integrate seamlessly with common web frameworks as middleware, intercepting HTTP requests before they reach application logic. When rate limits are exceeded, the system must return proper HTTP 429 \"Too Many Requests\" responses with appropriate headers. All responses must include standard rate limiting headers (`X-RateLimit-Limit`, `X-RateLimit-Remaining`, `X-RateLimit-Reset`) so clients can implement intelligent backoff strategies.\n\n**Distributed Consistency**: When deployed across multiple server instances, the rate limiter must maintain consistent per-client limits using shared storage. A client making requests to different server instances should see the same combined rate limit, not independent limits per server. This requires atomic operations on shared state and proper handling of storage failures.\n\n**Configurable Rate Policies**: The system must support flexible rate limit configuration, including different limits for different clients (premium vs. free tiers) and different limits for different API endpoints. Configuration should be adjustable without requiring application restarts, allowing for dynamic rate limit adjustments based on system load or business requirements.\n\n| Functional Capability | Description | Success Criteria |\n|----------------------|-------------|------------------|\n| Token Bucket Algorithm | Core rate limiting logic with burst handling | Allows bursts up to capacity, maintains average rate over time |\n| Client Identification | Distinguish between different API consumers | Unique buckets per IP/API key, consistent identification |\n| HTTP Middleware | Integration with web framework request pipeline | Intercepts all requests, returns 429 with proper headers |\n| Distributed State | Consistent limits across multiple server instances | Client sees same limit regardless of which server handles request |\n| Configuration Management | Flexible rate limit policies | Per-client and per-endpoint limits, runtime configuration updates |\n\n### Non-Functional Goals\n\nThe **non-functional goals** define the quality attributes and operational characteristics that determine how well the rate limiter performs in production environments. These requirements often drive architectural decisions more strongly than functional requirements.\n\n**Low Latency Overhead**: Rate limiting checks must add minimal latency to request processing. The target is less than 5 milliseconds of additional latency for in-memory operations and less than 10 milliseconds for distributed operations involving Redis. This requires efficient data structures, minimal lock contention, and optimized storage access patterns. The rate limiter should never become the bottleneck in a high-throughput API.\n\n**High Concurrency Support**: The system must handle thousands of concurrent requests without race conditions or performance degradation. This requires careful design of thread-safe data structures, efficient locking strategies, and possibly lock-free algorithms for hot paths. All shared state must be properly synchronized to prevent data corruption under concurrent access.\n\n**Memory Efficiency**: With potentially millions of clients, the per-client bucket storage must be memory-efficient. Inactive client buckets should be automatically cleaned up to prevent memory leaks. The target is less than 100 bytes of memory overhead per active client bucket, with automatic cleanup of buckets idle for more than 1 hour.\n\n**High Availability**: The rate limiter should continue operating even when distributed storage (Redis) becomes unavailable. This requires graceful degradation strategies, such as falling back to local in-memory buckets or allowing requests through when rate limit state cannot be determined. The system should never fail closed unless explicitly configured to do so.\n\n**Operational Simplicity**: The rate limiter should be easy to deploy, monitor, and debug in production. This means clear logging, helpful error messages, and observable metrics. Configuration should be straightforward with sensible defaults. The system should fail fast with clear error messages when misconfigured rather than operating in a degraded state.\n\n| Quality Attribute | Target Requirement | Measurement Method |\n|------------------|-------------------|-------------------|\n| Latency Overhead | < 5ms in-memory, < 10ms distributed | Benchmark with/without rate limiting enabled |\n| Concurrency | 10,000+ concurrent requests | Load testing with concurrent clients |\n| Memory Usage | < 100 bytes per client bucket | Memory profiling with 100k active clients |\n| Availability | 99.9% uptime despite Redis failures | Chaos testing with storage failures |\n| Recovery Time | < 30 seconds from storage restoration | Time to consistent state after Redis restart |\n\n> **Key Design Insight**: Non-functional requirements often conflict with each other. For example, low latency favors in-memory storage while high availability favors distributed storage. Our architecture must carefully balance these trade-offs, using in-memory storage for performance with asynchronous replication to distributed storage for consistency.\n\n### Explicit Non-Goals\n\nThe **explicit non-goals** are equally important as the goals because they prevent scope creep and keep the implementation focused on the core learning objectives. These are features we will explicitly NOT implement, even though they might be valuable in a production system.\n\n**Advanced Rate Limiting Algorithms**: We will not implement sliding window counters, leaky bucket algorithms, or adaptive rate limiting. While these algorithms have benefits in certain scenarios, the token bucket algorithm provides the best balance of simplicity, effectiveness, and educational value. Implementing multiple algorithms would increase complexity without significantly enhancing the learning experience around distributed systems and HTTP middleware.\n\n**Authentication and Authorization**: The rate limiter will not handle user authentication or authorization decisions. It assumes that client identification (IP address or API key) is already available in the HTTP request headers. Integration with OAuth, JWT tokens, or user session management is outside the scope of this project. The rate limiter's job is purely to count and limit requests, not to determine who is making them.\n\n**Advanced Client Classification**: We will not implement sophisticated client categorization beyond simple tier-based limits (e.g., free vs. premium). Features like geographic-based limits, time-of-day restrictions, or behavior-based dynamic classification are not included. The focus is on the fundamental distributed rate limiting problem rather than complex business logic.\n\n**Persistent Rate Limit History**: The system will not maintain long-term historical data about client request patterns or rate limit violations. While this data could be valuable for analytics or security monitoring, persisting and analyzing historical data is a separate concern that would complicate the core implementation without adding educational value around rate limiting algorithms.\n\n**Advanced Storage Backends**: We will only support Redis for distributed storage. Integration with other databases like PostgreSQL, MongoDB, or cloud-native solutions like DynamoDB is not included. Redis provides the atomic operations and performance characteristics needed for rate limiting, and supporting multiple backends would add complexity without educational benefit.\n\n**Production Monitoring and Alerting**: While the system will include basic logging, we will not implement comprehensive metrics collection, dashboards, or alerting systems. Production monitoring is a important operational concern, but it's separate from the core rate limiting algorithms and distributed coordination patterns we're focusing on.\n\n**Advanced HTTP Features**: The rate limiter will not handle HTTP/2 server push, WebSocket connections, or streaming responses. It focuses on traditional HTTP request/response patterns. Additionally, we won't implement sophisticated header parsing beyond basic API key and IP address extraction.\n\n| Feature Category | Specific Non-Goals | Rationale |\n|-----------------|-------------------|-----------|\n| Algorithms | Sliding window, leaky bucket, adaptive limiting | Token bucket provides sufficient learning value |\n| Security | Authentication, authorization, user management | Rate limiting is orthogonal to identity management |\n| Analytics | Historical data, request pattern analysis | Focus on real-time limiting, not data analysis |\n| Storage | Multiple database backends, cloud integrations | Redis sufficient for distributed coordination learning |\n| Monitoring | Metrics, dashboards, alerting systems | Production concerns beyond core algorithm focus |\n| Protocols | HTTP/2, WebSockets, streaming | Traditional request/response sufficient for learning |\n\n> **Decision: Focused Scope for Maximum Learning**\n> - **Context**: Rate limiting systems in production often include dozens of additional features like analytics, monitoring, and advanced client classification. Including all these features would create a realistic system but dilute the learning focus.\n> - **Options Considered**: \n>   1. Build a minimal toy system with just basic token buckets\n>   2. Build a comprehensive production-ready system with all enterprise features\n>   3. Build a focused system that demonstrates key concepts without unnecessary complexity\n> - **Decision**: Option 3 - focused system covering token buckets, distributed coordination, and HTTP integration\n> - **Rationale**: The goal is learning distributed systems patterns and rate limiting algorithms. Additional features like monitoring and analytics, while valuable in production, don't enhance understanding of the core concepts and would significantly increase implementation complexity.\n> - **Consequences**: The resulting system demonstrates real-world patterns and could be extended to production use, but intentionally omits features that would distract from the learning objectives. Students get deep understanding of the essential concepts without getting lost in peripheral complexity.\n\nThe non-goals are not permanent restrictions. They represent conscious decisions about where to focus learning effort in this project. Many of these features would be natural extensions in a production deployment, and understanding how to implement the core rate limiting system provides the foundation needed to add these features later.\n\n**Common Pitfalls in Scope Definition**:\n\n⚠️ **Pitfall: Scope Creep During Implementation**\nMany developers start adding \"just one more small feature\" during implementation, such as basic request logging or simple metrics collection. While these additions seem harmless, they often lead to cascading complexity - logging requires structured output formats, metrics require aggregation, and both require error handling. This complexity distracts from the core learning objectives and makes debugging more difficult. Stick rigidly to the defined scope, and maintain a separate list of \"future enhancements\" to implement after completing the core system.\n\n⚠️ **Pitfall: Under-Scoping Critical Infrastructure**\nThe opposite pitfall is excluding necessary infrastructure components that are required for the core functionality to work properly. For example, some developers might exclude Redis connection management as \"infrastructure\" rather than core functionality, but without proper connection handling, the distributed rate limiting cannot be properly tested or demonstrated. The key distinction is whether the component is necessary to demonstrate the core learning objectives - Redis connection management is necessary for distributed coordination learning, while comprehensive monitoring is not.\n\n⚠️ **Pitfall: Confusing Non-Goals with Poor Design**\nExcluding features from scope doesn't mean implementing them poorly when they do arise naturally. For example, while \"advanced error handling\" might be a non-goal, basic error handling for Redis failures is essential for the distributed coordination learning objective. The non-goals define what we won't build, not what we'll build badly.\n\n### Implementation Guidance\n\nThis section provides practical guidance for translating the goals and non-goals into implementation decisions throughout the project.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option | Recommendation |\n|-----------|--------------|-----------------|----------------|\n| Web Framework | Flask with basic middleware | FastAPI with dependency injection | Flask - simpler setup, focus on rate limiting logic |\n| Redis Client | redis-py with basic connection | Redis cluster with sentinel failover | redis-py - adequate for learning distributed patterns |\n| Configuration | JSON file or environment variables | Dynamic config with hot reload | Environment variables - simple and testable |\n| Logging | Python logging module | Structured logging with JSON output | Python logging - built-in and sufficient |\n| Testing Framework | pytest with basic assertions | pytest with fixtures and mocks | pytest with fixtures - supports integration testing |\n\n**B. Recommended File/Module Structure:**\n\nThe goals and non-goals inform how we organize the codebase to maintain clear separation of concerns:\n\n```\nrate-limiter/\n├── src/\n│   ├── __init__.py\n│   ├── rate_limiter/\n│   │   ├── __init__.py\n│   │   ├── token_bucket.py        # Core algorithm (Milestone 1)\n│   │   ├── client_tracker.py      # Per-client management (Milestone 2)\n│   │   ├── middleware.py          # HTTP integration (Milestone 3)\n│   │   ├── storage/\n│   │   │   ├── __init__.py\n│   │   │   ├── memory.py          # In-memory storage\n│   │   │   └── redis_storage.py   # Distributed storage (Milestone 4)\n│   │   └── config.py              # Rate limit policies\n├── tests/\n│   ├── unit/                      # Individual component tests\n│   ├── integration/               # End-to-end scenarios\n│   └── performance/               # Concurrency and latency tests\n├── examples/\n│   ├── flask_app.py              # Demonstration server\n│   └── load_test.py              # Simple load testing script\n└── requirements.txt\n```\n\nThis structure reflects our goals by having clear modules for each major capability (token bucket, client tracking, HTTP integration, distributed storage) while excluding directories for non-goals like analytics, monitoring, or authentication.\n\n**C. Configuration Schema:**\n\nThe functional goals require flexible configuration while the non-goals limit complexity:\n\n```python\n# config.py - Complete configuration structure\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional\nimport os\n\n@dataclass\nclass TokenBucketConfig:\n    \"\"\"Configuration for a single rate limit policy.\"\"\"\n    # TODO: Add capacity field (int) - maximum tokens in bucket\n    # TODO: Add refill_rate field (float) - tokens per second\n    # TODO: Add initial_tokens field (Optional[int]) - starting token count\n    pass\n\n@dataclass  \nclass RateLimitConfig:\n    \"\"\"Global rate limiter configuration.\"\"\"\n    # TODO: Add default_limits field (TokenBucketConfig) - fallback policy\n    # TODO: Add client_overrides field (Dict[str, TokenBucketConfig]) - per-client limits\n    # TODO: Add endpoint_limits field (Dict[str, TokenBucketConfig]) - per-endpoint limits\n    # TODO: Add cleanup_interval field (int) - seconds between bucket cleanup\n    # TODO: Add redis_url field (Optional[str]) - distributed storage connection\n    pass\n\ndef load_config() -> RateLimitConfig:\n    \"\"\"Load configuration from environment variables.\"\"\"\n    # TODO 1: Read DEFAULT_RATE_LIMIT and DEFAULT_BURST_SIZE env vars\n    # TODO 2: Parse CLIENT_OVERRIDES from JSON string if present  \n    # TODO 3: Parse ENDPOINT_LIMITS from JSON string if present\n    # TODO 4: Set reasonable defaults for cleanup_interval (3600 seconds)\n    # TODO 5: Read REDIS_URL for distributed storage (optional)\n    # Hint: Use os.getenv() with defaults, json.loads() for complex structures\n    pass\n```\n\n**D. Goal Validation Checkpoints:**\n\nAfter implementing each milestone, verify that the functional goals are being met:\n\n**Milestone 1 Checkpoint - Token Bucket Algorithm:**\n```bash\n# Run unit tests for core algorithm\npytest tests/unit/test_token_bucket.py -v\n\n# Expected output: All tests pass, including:\n# - test_token_generation_over_time\n# - test_burst_handling_up_to_capacity  \n# - test_rate_limiting_over_long_period\n# - test_thread_safety_under_concurrency\n```\n\n**Milestone 2 Checkpoint - Per-Client Tracking:**\n```bash\n# Run integration test with multiple clients\npython examples/multi_client_test.py\n\n# Expected behavior:\n# - Client A can make 100 requests/minute\n# - Client B has independent 100 requests/minute  \n# - Client A exhausting quota doesn't affect Client B\n# - Inactive clients get cleaned up after 1 hour\n```\n\n**Milestone 3 Checkpoint - HTTP Integration:**\n```bash\n# Start the Flask example server\npython examples/flask_app.py\n\n# Test rate limiting with curl\ncurl -i http://localhost:5000/api/test\n# Should see: X-RateLimit-Limit, X-RateLimit-Remaining headers\n\n# Exceed rate limit\nfor i in {1..101}; do curl http://localhost:5000/api/test; done\n# Should see: HTTP 429 response with Retry-After header\n```\n\n**Milestone 4 Checkpoint - Distributed Consistency:**\n```bash\n# Start Redis and two server instances\nredis-server &\npython examples/flask_app.py --port 5000 &  \npython examples/flask_app.py --port 5001 &\n\n# Test distributed rate limiting\npython examples/distributed_test.py\n# Expected: Same client hitting both servers shares single rate limit\n```\n\n**E. Non-Goal Validation:**\n\nEnsure that non-goals are respected by checking what's NOT implemented:\n\n| Non-Goal Category | Validation Check | Expected Result |\n|------------------|------------------|-----------------|\n| Advanced Algorithms | `grep -r \"sliding.*window\" src/` | No matches found |\n| Authentication | `grep -r \"jwt\\|oauth\\|session\" src/` | No matches found |  \n| Historical Data | Check for database schemas or time-series storage | None present |\n| Multiple Storage Backends | Count storage implementations | Only memory.py and redis_storage.py |\n| Production Monitoring | Check for metrics/dashboard code | Only basic logging present |\n\n**F. Debugging Goal Adherence:**\n\nCommon issues that indicate scope creep or goal misalignment:\n\n| Symptom | Likely Cause | Fix |\n|---------|--------------|-----|\n| Implementation taking much longer than expected | Adding features beyond defined scope | Review code against explicit non-goals, remove scope creep |\n| Complex configuration with many options | Over-engineering flexibility requirements | Simplify to support only defined functional goals |\n| Difficulty testing core functionality | Too many dependencies or coupled concerns | Refactor to match recommended file structure |\n| Performance problems in basic scenarios | Implementing non-goal features that add overhead | Profile code and remove unnecessary complexity |\n| Hard to understand the main rate limiting logic | Core algorithm buried in peripheral features | Extract token bucket logic to dedicated module |\n\nThe goals and non-goals serve as a constant reference throughout implementation. When facing any design decision, the first question should be: \"Does this serve one of our functional goals or non-functional requirements, or is this scope creep that should be deferred?\"\n\n\n## High-Level Architecture\n\n> **Milestone(s):** All milestones - this section establishes the foundational component structure that evolves through each milestone\n\nThink of our rate limiter as a sophisticated **automated traffic control system** at a busy intersection. Just as traffic lights, sensors, and control systems work together to manage vehicle flow, our rate limiter coordinates multiple components to manage request flow. The traffic light controller (HTTP middleware) makes immediate allow/deny decisions, the sensor array (client tracker) monitors each lane's activity, the timing mechanism (token bucket) determines when each lane gets its turn, and the central coordination system (distributed storage) ensures all intersections work in harmony across the city.\n\nThis architectural foundation provides the structural blueprint for building a production-ready rate limiting system that can scale from a single server to a distributed cluster while maintaining consistent behavior and performance characteristics.\n\n### Component Responsibilities\n\nOur rate limiter architecture consists of four primary components, each with distinct responsibilities and clear boundaries. Understanding these boundaries is crucial for implementing a maintainable system that can evolve through each milestone without creating tight coupling or unclear ownership.\n\n![Rate Limiter System Components](./diagrams/system-components.svg)\n\nThe **HTTP Middleware** serves as the primary entry point and decision enforcer for all incoming requests. This component integrates directly with web frameworks like Flask or Express, intercepting every HTTP request before it reaches application logic. Its core responsibility is making immediate allow/deny decisions based on rate limit evaluations and translating those decisions into proper HTTP responses. When a request is allowed, the middleware adds informational headers like `X-RateLimit-Remaining` and `X-RateLimit-Limit` to help clients understand their current quota status. When a request exceeds limits, the middleware returns a `429 Too Many Requests` response with a `Retry-After` header indicating when the client can retry.\n\nThe middleware component maintains no state of its own - it acts purely as a coordinator that delegates rate limit evaluation to other components while handling the HTTP-specific concerns of request processing and response formatting. This stateless design ensures that the middleware remains lightweight and can be easily tested in isolation from the complex rate limiting logic.\n\n| Responsibility | Description | Boundary |\n|---|---|---|\n| Request Interception | Capture all incoming HTTP requests before application processing | Only handles HTTP layer - no rate limiting logic |\n| Client Identification | Extract client identifiers from IP addresses, headers, or authentication tokens | Extracts identifiers but doesn't manage client state |\n| Rate Limit Evaluation | Coordinate with client tracker to check current rate limit status | Calls other components but makes no rate limiting decisions itself |\n| HTTP Response Generation | Generate proper status codes, headers, and error messages | Only handles HTTP formatting - no business logic |\n| Framework Integration | Provide clean integration points for Flask, Express, and other frameworks | Framework-specific adapters only - core logic is framework-agnostic |\n\nThe **Client Tracker** manages the complex task of maintaining separate rate limiting state for each unique client accessing the system. This component handles client identification strategies, efficiently stores per-client token buckets, and implements cleanup mechanisms to prevent memory leaks from inactive clients. The client tracker acts as a factory and registry for token buckets, creating new buckets on-demand when previously unseen clients make requests and maintaining a mapping between client identifiers and their corresponding bucket instances.\n\nOne of the most critical responsibilities of the client tracker is implementing **stale bucket cleanup** to prevent unbounded memory growth. In a system serving thousands or millions of unique clients over time, keeping every client's bucket in memory indefinitely would eventually exhaust available resources. The client tracker implements configurable cleanup policies that remove buckets for clients that haven't made requests within a specified timeout period.\n\n| Responsibility | Description | Boundary |\n|---|---|---|\n| Client Identification | Determine unique client identity from IP addresses, API keys, or custom headers | Identification logic only - no rate limit evaluation |\n| Bucket Lifecycle Management | Create, store, and destroy token buckets for individual clients | Manages bucket instances but not bucket algorithm logic |\n| Memory Management | Clean up stale buckets to prevent memory leaks from inactive clients | Cleanup timing and policies - not token bucket internals |\n| Concurrent Access | Provide thread-safe access to client buckets under high load | Concurrency control for bucket access - not HTTP processing |\n| Per-Client Configuration | Support different rate limits for premium clients or specific use cases | Configuration management - not limit enforcement |\n\nThe **Token Bucket** implements the core rate limiting algorithm that determines whether individual requests should be allowed or denied. Each token bucket instance maintains its own state including current token count, last refill timestamp, and configuration parameters like capacity and refill rate. The token bucket algorithm provides natural burst handling by allowing clients to accumulate tokens during periods of low activity and spend them rapidly during bursts, up to the configured bucket capacity.\n\nToken buckets operate independently of HTTP concerns, client identification, or distributed coordination. A single token bucket instance knows only about tokens flowing in and out over time - it has no knowledge of which client it serves or how it fits into the broader system architecture. This separation of concerns makes the token bucket algorithm easy to test, reason about, and optimize independently.\n\n| Responsibility | Description | Boundary |\n|---|---|---|\n| Token Generation | Calculate new tokens to add based on elapsed time and configured refill rate | Time-based calculations only - no external dependencies |\n| Token Consumption | Deduct tokens for incoming requests and determine allow/deny decisions | Local state only - no knowledge of clients or HTTP |\n| Burst Handling | Allow short bursts up to bucket capacity while maintaining long-term rate limits | Algorithm logic only - no policy decisions |\n| State Management | Track current token count, last refill time, and bucket configuration | Internal state only - no persistence or distribution |\n| Thread Safety | Ensure atomic token operations under concurrent access | Local concurrency only - no distributed coordination |\n\nThe **Distributed Storage** component handles the complex challenge of maintaining consistent rate limiting state across multiple server instances in a distributed deployment. While early milestones use in-memory storage for simplicity, production systems require shared state to prevent clients from bypassing limits by sending requests to different servers. This component abstracts the storage backend (typically Redis) and provides atomic operations for reading and updating token bucket state across the cluster.\n\nThe distributed storage component implements atomic read-modify-write operations using Redis Lua scripts to ensure that token consumption decisions remain consistent even under high concurrent load from multiple servers. It also handles failure scenarios gracefully, implementing fallback strategies when the distributed storage becomes unavailable while maintaining system availability.\n\n| Responsibility | Description | Boundary |\n|---|---|---|\n| State Persistence | Store and retrieve token bucket state from distributed storage (Redis) | Storage operations only - no rate limiting logic |\n| Atomic Operations | Ensure consistent token updates across multiple server instances | Atomicity guarantees only - no business logic |\n| Connection Management | Handle Redis connections, retries, and connection pooling | Infrastructure concerns only - no application logic |\n| Failure Handling | Provide graceful degradation when distributed storage is unavailable | Error handling and fallbacks - no normal operation logic |\n| Data Serialization | Convert token bucket state to/from storage format efficiently | Data format concerns only - no state interpretation |\n\n> **Key Architectural Insight**: The component boundaries are designed around the principle of **single responsibility** with minimal coupling. Each component can be developed, tested, and evolved independently while maintaining clear interfaces with other components. This modularity becomes especially important as the system evolves from simple in-memory rate limiting to distributed coordination across multiple servers.\n\n### Request Processing Flow\n\nUnderstanding how HTTP requests flow through the rate limiting pipeline is essential for implementing each component correctly and debugging issues that arise during development. The request processing flow represents the coordination between all components to make consistent rate limiting decisions with minimal latency overhead.\n\nThe following sequence describes the complete journey of an HTTP request through our rate limiting system, from initial receipt to final allow/deny decision:\n\n**1. HTTP Request Arrival and Middleware Interception**\n\nWhen an HTTP request arrives at our web server, the rate limiting middleware intercepts it before any application logic executes. This early interception is crucial because rate limiting decisions must be made before consuming resources on request processing. The middleware extracts the raw request object containing headers, IP address, and any authentication information needed for client identification.\n\nThe middleware performs initial request validation to ensure it has sufficient information to identify the client and determine applicable rate limits. If critical identification information is missing (such as required API key headers), the middleware can immediately reject the request without consulting other components.\n\n**2. Client Identification and Bucket Retrieval**\n\nThe middleware delegates client identification to the client tracker component, passing along relevant request metadata including IP address, user agent, authentication headers, or API keys. The client tracker applies configured identification strategies to determine a unique client identifier - this might be as simple as the source IP address or as complex as a combination of API key and endpoint pattern.\n\nOnce the client identifier is determined, the client tracker checks its internal registry for an existing token bucket associated with this client. If a bucket exists and hasn't exceeded the stale timeout threshold, the tracker returns the existing bucket instance. If no bucket exists or the existing bucket has become stale, the tracker creates a new bucket instance with appropriate configuration parameters (which may include per-client overrides for premium users or specific rate limits).\n\n**3. Rate Limit Configuration Resolution**\n\nBefore evaluating the token bucket, the system must determine which rate limiting rules apply to this specific request. The client tracker consults the `RateLimitConfig` to resolve any per-client overrides, per-endpoint limits, or default rate limiting parameters. This resolution process considers multiple configuration sources in priority order: specific client overrides take precedence over endpoint-specific limits, which take precedence over default system-wide limits.\n\nThe configuration resolution also determines how many tokens this particular request should consume. While most requests consume a single token, some systems implement weighted rate limiting where expensive operations consume multiple tokens to reflect their true resource cost.\n\n**4. Token Bucket Evaluation and Decision**\n\nWith the appropriate token bucket instance and consumption requirements determined, the system calls the bucket's token consumption method to evaluate whether the request should be allowed. The token bucket first performs a **token refill calculation** based on elapsed time since the last access, adding new tokens according to the configured refill rate while respecting the maximum bucket capacity.\n\nAfter refilling tokens, the bucket attempts to consume the required number of tokens for the current request. If sufficient tokens are available, they are deducted from the bucket and the method returns an allow decision along with updated bucket state. If insufficient tokens remain, the bucket state is unchanged and the method returns a deny decision with information about when sufficient tokens will next be available.\n\n**5. HTTP Response Generation and Header Population**\n\nBased on the token bucket's decision, the middleware generates an appropriate HTTP response. For allowed requests, the middleware adds standard rate limiting headers to inform the client of their current status:\n\n- `X-RateLimit-Limit`: The maximum number of requests allowed per time window\n- `X-RateLimit-Remaining`: The number of tokens currently available in the client's bucket\n- `X-RateLimit-Reset`: Timestamp when the bucket will next be refilled to capacity\n\nFor denied requests, the middleware generates a `429 Too Many Requests` response with additional headers:\n\n- `Retry-After`: Number of seconds until sufficient tokens will be available\n- Detailed error message explaining the rate limit violation\n\n**6. Request Forwarding or Termination**\n\nIf the rate limiting decision was to allow the request, the middleware passes control to the next component in the HTTP processing pipeline, typically the application's routing logic. The rate limiting process is complete, and the application handles the request normally.\n\nIf the rate limiting decision was to deny the request, the middleware immediately returns the `429` response without forwarding to application logic. This early termination protects downstream systems from excessive load while providing clear feedback to the client about why their request was rejected.\n\n| Processing Stage | Component | Input | Output | Error Conditions |\n|---|---|---|---|---|\n| Request Interception | HTTP Middleware | Raw HTTP request | Client identification data | Missing required headers |\n| Client Identification | Client Tracker | Request metadata | Client ID and bucket instance | Invalid API keys, malformed headers |\n| Configuration Resolution | Client Tracker | Client ID, endpoint pattern | Rate limit configuration | Missing configuration data |\n| Token Refill | Token Bucket | Current time, bucket state | Updated token count | Clock drift, time calculation overflow |\n| Token Consumption | Token Bucket | Required tokens, bucket state | Allow/deny decision | Insufficient tokens |\n| Response Generation | HTTP Middleware | Rate limit decision | HTTP response with headers | Header formatting errors |\n\n> **Critical Implementation Note**: The entire request processing flow must complete in microseconds to avoid adding significant latency to API responses. This performance requirement influences many design decisions, including the choice of in-memory bucket storage for non-distributed deployments and the use of atomic Redis operations for distributed scenarios.\n\n**Distributed Processing Flow Considerations**\n\nIn distributed deployments where multiple server instances share rate limiting state through Redis, the request processing flow includes additional coordination steps. Instead of updating local token bucket state, the system must perform atomic read-modify-write operations against shared Redis storage using Lua scripts to maintain consistency.\n\nThe distributed flow modifies steps 4 and 5 to include Redis operations:\n\n- **4a. Redis State Retrieval**: Fetch current token bucket state from Redis using the client identifier as the key\n- **4b. Atomic Token Operation**: Execute a Lua script that performs token refill calculation, consumption attempt, and state update in a single atomic operation  \n- **4c. Result Processing**: Handle the Lua script result to determine allow/deny decision and updated state\n- **5a. Fallback Handling**: If Redis operations fail, implement graceful degradation using local rate limiting or fail-open policies\n\nThis distributed coordination adds latency (typically 1-5ms for Redis operations) but ensures that clients cannot bypass rate limits by distributing requests across multiple server instances.\n\n### Recommended File Structure\n\nOrganizing the rate limiter codebase with clear module boundaries and logical separation of concerns is essential for maintainability as the system evolves through each milestone. The following file structure supports independent development of each component while maintaining clean interfaces and testability.\n\nThe recommended structure separates **core algorithm logic** from **integration concerns**, **storage backends** from **business logic**, and **configuration management** from **runtime operation**. This separation enables focused development during each milestone and facilitates testing individual components in isolation.\n\n```\nrate_limiter/\n├── __init__.py                     # Package initialization and main exports\n├── config/\n│   ├── __init__.py\n│   ├── config.py                   # RateLimitConfig and TokenBucketConfig definitions\n│   └── loader.py                   # Environment variable loading and validation\n├── core/\n│   ├── __init__.py\n│   ├── token_bucket.py             # Core token bucket algorithm implementation\n│   └── algorithms.py               # Alternative algorithms (sliding window, etc.)\n├── client/\n│   ├── __init__.py\n│   ├── tracker.py                  # Client identification and bucket management\n│   ├── identifier.py               # Client identification strategies\n│   └── cleanup.py                  # Background cleanup of stale buckets\n├── middleware/\n│   ├── __init__.py\n│   ├── base.py                     # Framework-agnostic middleware logic\n│   ├── flask_integration.py        # Flask-specific middleware implementation\n│   ├── fastapi_integration.py      # FastAPI integration (future extension)\n│   └── headers.py                  # HTTP header management and formatting\n├── storage/\n│   ├── __init__.py\n│   ├── base.py                     # Abstract storage interface\n│   ├── memory.py                   # In-memory storage for single-instance deployments\n│   ├── redis_storage.py            # Redis-backed distributed storage\n│   └── lua_scripts/                # Redis Lua scripts for atomic operations\n│       ├── consume_tokens.lua\n│       └── refill_tokens.lua\n├── utils/\n│   ├── __init__.py\n│   ├── time_utils.py               # Time handling utilities and precision management\n│   └── errors.py                   # Custom exception definitions\n└── tests/\n    ├── __init__.py\n    ├── unit/                       # Unit tests for individual components\n    │   ├── test_token_bucket.py\n    │   ├── test_client_tracker.py\n    │   ├── test_middleware.py\n    │   └── test_redis_storage.py\n    ├── integration/                # Integration tests across components\n    │   ├── test_http_flow.py\n    │   └── test_distributed_consistency.py\n    └── fixtures/                   # Test data and mock configurations\n        ├── config_samples.py\n        └── mock_redis.py\n```\n\n**Core Module Organization Rationale**\n\nThe `config/` module centralizes all configuration management and validation logic, making it easy to add new configuration options as the system evolves. The `config.py` file defines the primary data structures (`RateLimitConfig` and `TokenBucketConfig`) used throughout the system, while `loader.py` handles the complex task of loading configuration from environment variables, validating values, and providing sensible defaults.\n\nThe `core/` module contains the heart of the rate limiting logic - the token bucket algorithm implementation that operates independently of HTTP concerns, client tracking, or storage backends. This separation allows the core algorithm to be thoroughly tested in isolation and makes it possible to implement alternative rate limiting algorithms (like sliding window counters) without affecting other system components.\n\n> **Decision: Separate Core Algorithm from Integration Logic**\n> - **Context**: Rate limiting systems often need to support multiple algorithms and storage backends while maintaining consistent HTTP interfaces\n> - **Options Considered**: \n>   - Monolithic design with all logic in middleware\n>   - Layered architecture with separated concerns\n>   - Plugin-based architecture with dynamic loading\n> - **Decision**: Layered architecture with clear separation between core algorithm, client tracking, middleware, and storage\n> - **Rationale**: Enables independent testing and evolution of each component, supports multiple storage backends without affecting algorithm logic, and facilitates adding new rate limiting algorithms in the future\n> - **Consequences**: Slightly more complex initial setup but much easier maintenance and testing; enables milestone-based development where each layer can be implemented and verified independently\n\n**Client and Storage Module Design**\n\nThe `client/` module handles all aspects of per-client rate limiting including identification strategies, bucket lifecycle management, and cleanup operations. Separating client identification logic into its own module (`identifier.py`) makes it easy to support different identification schemes (IP-based, API key-based, or custom headers) without affecting bucket management code.\n\nThe `storage/` module provides a clean abstraction over different storage backends, enabling the system to start with simple in-memory storage and evolve to distributed Redis storage without changing core logic. The abstract `base.py` interface ensures that all storage implementations provide the same guarantees around atomicity and consistency.\n\n| Module | Primary Purpose | Key Files | Dependencies |\n|---|---|---|---|\n| `config/` | Configuration management and validation | `config.py`, `loader.py` | None (foundation module) |\n| `core/` | Rate limiting algorithms and token bucket logic | `token_bucket.py` | `config/` only |\n| `client/` | Per-client tracking and bucket management | `tracker.py`, `cleanup.py` | `core/`, `storage/` |\n| `middleware/` | HTTP integration and response handling | `flask_integration.py`, `headers.py` | `client/`, `config/` |\n| `storage/` | Storage backend abstraction and implementations | `memory.py`, `redis_storage.py` | `config/`, `utils/` |\n| `utils/` | Shared utilities and error handling | `time_utils.py`, `errors.py` | None |\n\n**Testing Strategy Integration**\n\nThe file structure directly supports comprehensive testing by separating unit tests (which test individual modules in isolation) from integration tests (which test component interactions and end-to-end behavior). Each core module has corresponding unit tests that can run without external dependencies, while integration tests verify that components work correctly together.\n\nThe `fixtures/` directory provides shared test data and mock implementations that can be reused across multiple test modules. This is particularly important for testing distributed scenarios where mock Redis implementations allow testing coordination logic without requiring actual Redis instances.\n\n**Framework Integration Patterns**\n\nThe middleware module structure anticipates supporting multiple web frameworks while avoiding code duplication. The `base.py` file contains framework-agnostic logic for rate limiting decisions and HTTP header generation, while framework-specific files (`flask_integration.py`) handle the unique requirements of each framework's middleware system.\n\nThis pattern makes it straightforward to add support for new frameworks by implementing the framework-specific integration layer while reusing all the core rate limiting logic. It also ensures that the bulk of the middleware logic can be tested independently of any specific web framework.\n\n> **Implementation Insight**: Start development with the `config/` and `core/` modules to establish the foundational data structures and algorithm logic. These modules have no external dependencies and can be completely implemented and tested before moving on to client tracking or HTTP integration. This approach aligns perfectly with the milestone-based development plan.\n\n### Implementation Guidance\n\nThe following implementation guidance provides the foundational code structure needed to build the rate limiter architecture. Focus on establishing clean interfaces between components and preparing the module structure that will support all four milestones.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Web Framework | Flask with built-in middleware | FastAPI with custom middleware classes |\n| Configuration | Environment variables + JSON | YAML configuration with validation |\n| Concurrency | threading.Lock for bucket access | asyncio with async/await patterns |\n| Time Handling | time.time() with float precision | time.time_ns() with nanosecond precision |\n| Redis Client | redis-py with connection pooling | aioredis for async operations |\n| Logging | Python logging module | Structured logging with JSON output |\n\n**B. Recommended File Structure Setup**\n\nStart by creating the basic module structure that will support all milestones:\n\n```bash\nmkdir -p rate_limiter/{config,core,client,middleware,storage/lua_scripts,utils,tests/{unit,integration,fixtures}}\ntouch rate_limiter/{__init__.py,config/__init__.py,core/__init__.py,client/__init__.py,middleware/__init__.py,storage/__init__.py,utils/__init__.py,tests/__init__.py}\n```\n\n**C. Infrastructure Starter Code**\n\n**Configuration Management (`config/config.py`)**:\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional\nimport json\nimport os\n\n\n@dataclass\nclass TokenBucketConfig:\n    \"\"\"Configuration for a single token bucket instance.\"\"\"\n    capacity: int\n    refill_rate: float\n    initial_tokens: Optional[int] = None\n    \n    def __post_init__(self):\n        if self.initial_tokens is None:\n            self.initial_tokens = self.capacity\n        \n        if self.capacity <= 0:\n            raise ValueError(\"Token bucket capacity must be positive\")\n        if self.refill_rate <= 0:\n            raise ValueError(\"Token bucket refill rate must be positive\")\n\n\n@dataclass \nclass RateLimitConfig:\n    \"\"\"Global configuration for the rate limiting system.\"\"\"\n    default_limits: TokenBucketConfig\n    client_overrides: Dict[str, TokenBucketConfig]\n    endpoint_limits: Dict[str, TokenBucketConfig] \n    cleanup_interval: int\n    redis_url: Optional[str] = None\n    \n    @classmethod\n    def from_environment(cls) -> 'RateLimitConfig':\n        \"\"\"Load configuration from environment variables.\"\"\"\n        # Parse default rate limits\n        default_rate = int(os.getenv('DEFAULT_RATE_LIMIT', '100'))\n        default_burst = int(os.getenv('DEFAULT_BURST_SIZE', '10'))\n        default_limits = TokenBucketConfig(\n            capacity=default_burst,\n            refill_rate=default_rate / 60.0  # Convert per-minute to per-second\n        )\n        \n        # Parse client overrides from JSON\n        client_overrides = {}\n        if os.getenv('CLIENT_OVERRIDES'):\n            override_data = json.loads(os.getenv('CLIENT_OVERRIDES'))\n            for client_id, limits in override_data.items():\n                client_overrides[client_id] = TokenBucketConfig(**limits)\n        \n        # Parse endpoint limits from JSON  \n        endpoint_limits = {}\n        if os.getenv('ENDPOINT_LIMITS'):\n            endpoint_data = json.loads(os.getenv('ENDPOINT_LIMITS'))\n            for pattern, limits in endpoint_data.items():\n                endpoint_limits[pattern] = TokenBucketConfig(**limits)\n                \n        return cls(\n            default_limits=default_limits,\n            client_overrides=client_overrides,\n            endpoint_limits=endpoint_limits,\n            cleanup_interval=int(os.getenv('CLEANUP_INTERVAL', '300')),\n            redis_url=os.getenv('REDIS_URL')\n        )\n```\n\n**Time Utilities (`utils/time_utils.py`)**:\n```python\nimport time\nfrom typing import Union\n\n\ndef current_time_seconds() -> float:\n    \"\"\"Get current time with high precision for token calculations.\"\"\"\n    return time.time()\n\n\ndef calculate_time_delta(start_time: float, end_time: float) -> float:\n    \"\"\"Calculate time difference with overflow protection.\"\"\"\n    delta = end_time - start_time\n    # Protect against clock drift or system clock changes\n    if delta < 0:\n        return 0.0\n    # Protect against unreasonably large deltas (more than 1 day)\n    if delta > 86400:\n        return 86400.0\n    return delta\n\n\ndef calculate_tokens_to_add(time_delta: float, refill_rate: float) -> int:\n    \"\"\"Calculate number of tokens to add based on elapsed time and refill rate.\"\"\"\n    tokens_to_add = time_delta * refill_rate\n    # Ensure we don't overflow integer limits\n    return min(int(tokens_to_add), 10_000_000)\n```\n\n**Custom Exceptions (`utils/errors.py`)**:\n```python\nclass RateLimitError(Exception):\n    \"\"\"Base class for all rate limiting errors.\"\"\"\n    pass\n\n\nclass ConfigurationError(RateLimitError):\n    \"\"\"Raised when rate limit configuration is invalid.\"\"\"\n    pass\n\n\nclass StorageError(RateLimitError):\n    \"\"\"Raised when storage backend operations fail.\"\"\"\n    pass\n\n\nclass ClientIdentificationError(RateLimitError):\n    \"\"\"Raised when client cannot be properly identified.\"\"\"\n    pass\n```\n\n**D. Core Logic Skeleton Code**\n\n**Token Bucket Core (`core/token_bucket.py`)**:\n```python\nimport threading\nfrom typing import Tuple, NamedTuple\nfrom ..config.config import TokenBucketConfig\nfrom ..utils.time_utils import current_time_seconds, calculate_time_delta, calculate_tokens_to_add\n\n\nclass TokenConsumptionResult(NamedTuple):\n    \"\"\"Result of attempting to consume tokens from a bucket.\"\"\"\n    allowed: bool\n    tokens_remaining: int\n    retry_after_seconds: float\n\n\nclass TokenBucket:\n    \"\"\"Thread-safe token bucket implementation for rate limiting.\"\"\"\n    \n    def __init__(self, config: TokenBucketConfig):\n        self.capacity = config.capacity\n        self.refill_rate = config.refill_rate\n        self.tokens = config.initial_tokens\n        self.last_refill_time = current_time_seconds()\n        self._lock = threading.Lock()\n    \n    def try_consume(self, tokens_requested: int = 1) -> TokenConsumptionResult:\n        \"\"\"\n        Attempt to consume tokens from the bucket.\n        \n        Returns TokenConsumptionResult indicating whether the request was allowed,\n        how many tokens remain, and when to retry if denied.\n        \"\"\"\n        with self._lock:\n            # TODO 1: Get current time and calculate time delta since last refill\n            # TODO 2: Calculate how many new tokens to add based on elapsed time\n            # TODO 3: Add new tokens to bucket, respecting maximum capacity\n            # TODO 4: Update last_refill_time to current time\n            # TODO 5: Check if enough tokens available for this request\n            # TODO 6: If enough tokens, deduct them and return success\n            # TODO 7: If not enough tokens, calculate retry_after_seconds\n            # TODO 8: Return TokenConsumptionResult with appropriate values\n            # Hint: retry_after = (tokens_requested - current_tokens) / refill_rate\n            pass\n    \n    def get_current_tokens(self) -> int:\n        \"\"\"Get current token count (for monitoring/debugging).\"\"\"\n        # TODO: Implement thread-safe token count retrieval\n        # This should trigger a refill calculation but not consume tokens\n        pass\n```\n\n**Client Tracker Foundation (`client/tracker.py`)**:\n```python\nimport threading\nimport time\nfrom typing import Dict, Optional\nfrom ..core.token_bucket import TokenBucket\nfrom ..config.config import RateLimitConfig, TokenBucketConfig\nfrom ..utils.errors import ClientIdentificationError\n\n\nclass ClientBucketTracker:\n    \"\"\"Manages per-client token buckets with automatic cleanup.\"\"\"\n    \n    def __init__(self, config: RateLimitConfig):\n        self.config = config\n        self.buckets: Dict[str, TokenBucket] = {}\n        self.last_access: Dict[str, float] = {}\n        self._lock = threading.Lock()\n    \n    def get_bucket_for_client(self, client_id: str, endpoint: Optional[str] = None) -> TokenBucket:\n        \"\"\"\n        Get or create a token bucket for the specified client.\n        \n        Args:\n            client_id: Unique identifier for the client\n            endpoint: Optional endpoint pattern for endpoint-specific limits\n            \n        Returns:\n            TokenBucket instance for this client\n        \"\"\"\n        # TODO 1: Determine appropriate TokenBucketConfig for this client\n        # Check client_overrides first, then endpoint_limits, then default_limits\n        # TODO 2: Acquire lock for thread-safe bucket access\n        # TODO 3: Check if bucket already exists for this client\n        # TODO 4: If bucket exists, update last_access time and return it\n        # TODO 5: If bucket doesn't exist, create new TokenBucket with resolved config\n        # TODO 6: Store new bucket and set last_access time\n        # TODO 7: Return the bucket instance\n        pass\n    \n    def cleanup_stale_buckets(self) -> int:\n        \"\"\"\n        Remove buckets that haven't been accessed recently.\n        \n        Returns:\n            Number of buckets removed\n        \"\"\"\n        # TODO 1: Calculate cutoff time based on cleanup_interval\n        # TODO 2: Acquire lock for thread-safe cleanup\n        # TODO 3: Iterate through last_access times to find stale buckets\n        # TODO 4: Remove stale buckets from both buckets and last_access dicts\n        # TODO 5: Return count of removed buckets\n        pass\n    \n    def identify_client(self, request_data: dict) -> str:\n        \"\"\"\n        Extract client identifier from request data.\n        \n        Args:\n            request_data: Dictionary containing IP, headers, etc.\n            \n        Returns:\n            Unique client identifier string\n        \"\"\"\n        # TODO 1: Check for API key in request headers\n        # TODO 2: Fall back to IP address if no API key\n        # TODO 3: Validate that identifier is not empty\n        # TODO 4: Return normalized client identifier\n        # Hint: Consider rate limiting per API key vs per IP\n        pass\n```\n\n**E. Milestone Checkpoint**\n\nAfter implementing the basic architecture structure:\n\n1. **Verify Module Structure**: Run `python -c \"import rate_limiter; print('Import successful')\"` \n2. **Test Configuration Loading**: Set environment variables and verify `RateLimitConfig.from_environment()` works correctly\n3. **Validate Component Isolation**: Each module should be importable independently without circular dependencies\n\nExpected behavior after this milestone:\n- All modules can be imported without errors\n- Configuration can be loaded from environment variables\n- Basic data structures (TokenBucketConfig, RateLimitConfig) are properly defined\n- Time utilities handle edge cases like clock drift\n- File structure supports independent development of each component\n\n**F. Debugging Tips**\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---|---|---|---|\n| Import errors on module loading | Circular dependencies between modules | Check import statements in `__init__.py` files | Use relative imports and avoid importing from parent modules |\n| Configuration validation failures | Invalid environment variable formats | Print parsed values before validation | Add error handling with descriptive messages in config loader |\n| Thread safety test failures | Missing locks or improper locking order | Add logging around lock acquisition | Use context managers (`with lock:`) consistently |\n| Time calculation inconsistencies | Float precision issues or clock drift | Log timestamps and calculated deltas | Use nanosecond precision or validate time delta bounds |\n\n\n## Data Model\n\n> **Milestone(s):** All milestones - this section defines the foundational data structures used from basic token bucket implementation through distributed rate limiting\n\nThink of our data model as the **blueprint for a sophisticated banking system**. Just as a bank needs clear definitions for accounts, transactions, customers, and policies, our rate limiter needs precise data structures for token buckets, client identities, rate limit rules, and storage mechanisms. Each data type serves a specific purpose in the rate limiting ecosystem, and their relationships determine how efficiently we can track and enforce limits across thousands of concurrent clients.\n\n![Data Model and Type Relationships](./diagrams/data-model-relationships.svg)\n\nThe data model forms the foundation that supports all rate limiting operations. Whether we're implementing a simple in-memory token bucket or a distributed system coordinating across multiple servers, these core data structures remain consistent. Understanding these types deeply ensures that our implementation can evolve from basic functionality to enterprise-scale distributed rate limiting without architectural rewrites.\n\n### Core Data Types\n\nThe core data types represent the fundamental entities in our rate limiting system. Each type encapsulates specific responsibilities and maintains clear boundaries between different aspects of rate limiting functionality.\n\n#### Token Bucket State and Configuration\n\nThe `TokenBucketConfig` type defines the behavioral parameters for any token bucket instance. Think of this as the **specification sheet for a water tank** - it defines the tank's maximum capacity, how fast water flows in through the inlet pipe, and how many tokens should be present when the tank is first installed.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `capacity` | `int` | Maximum number of tokens the bucket can hold, representing burst capacity |\n| `refill_rate` | `float` | Tokens added per second during normal operation, supporting fractional rates |\n| `initial_tokens` | `Optional[int]` | Starting token count when bucket is created; defaults to full capacity if not specified |\n\nThe `capacity` field determines how large bursts of requests can be handled before rate limiting kicks in. A bucket with capacity 100 allows a client to make 100 rapid requests even if their sustained rate limit is much lower. The `refill_rate` supports fractional values like 0.5 tokens per second, enabling fine-grained rate control for premium API tiers or resource-intensive operations.\n\nThe `initial_tokens` field provides flexibility in bucket initialization. Setting it to a lower value prevents immediate bursts from new clients, while `None` defaults to full capacity for standard scenarios. This becomes crucial when implementing \"warm-up\" periods for new API keys or handling client onboarding flows.\n\n> **Design Insight**: Using `Optional[int]` for `initial_tokens` rather than a default parameter allows the configuration to explicitly distinguish between \"start with zero tokens\" and \"use the default behavior\". This prevents ambiguity in distributed scenarios where default values might differ across server instances.\n\n#### Token Consumption Results\n\nThe `TokenConsumptionResult` type encapsulates the outcome of attempting to consume tokens from a bucket. This structure provides all information needed to make HTTP response decisions and inform clients about their rate limit status.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `allowed` | `bool` | Whether the token consumption request was approved |\n| `tokens_remaining` | `int` | Current token count after the consumption attempt |\n| `retry_after_seconds` | `float` | Time until enough tokens refill to satisfy the original request |\n\nThe `allowed` field drives the fundamental allow/deny decision for incoming requests. When `False`, the HTTP middleware returns a 429 status code and includes rate limiting headers in the response.\n\nThe `tokens_remaining` field enables the `X-RateLimit-Remaining` header, helping clients understand their current quota status. This value reflects the bucket state after the consumption attempt, whether successful or failed.\n\nThe `retry_after_seconds` field calculates the precise wait time until the requested number of tokens will be available again. For a request needing 5 tokens when only 2 remain, this field indicates how long until 5 tokens accumulate, considering the bucket's refill rate. This drives the `Retry-After` HTTP header, enabling intelligent client backoff strategies.\n\n#### Global Rate Limit Configuration\n\nThe `RateLimitConfig` type serves as the **central control panel** for the entire rate limiting system. Think of it as the master configuration file that defines default policies, client-specific overrides, endpoint-specific rules, and operational parameters.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `default_limits` | `TokenBucketConfig` | Baseline rate limits applied to unrecognized clients |\n| `client_overrides` | `Dict[str, TokenBucketConfig]` | Per-client rate limit overrides keyed by client identifier |\n| `endpoint_limits` | `Dict[str, TokenBucketConfig]` | Per-endpoint rate limits keyed by URL pattern or route name |\n| `cleanup_interval` | `int` | Seconds between stale bucket cleanup runs |\n| `redis_url` | `Optional[str]` | Redis connection string for distributed rate limiting |\n\nThe `default_limits` field establishes the baseline rate limiting policy applied to all requests unless overridden by more specific rules. This provides a safety net ensuring that even unidentified traffic faces reasonable limits.\n\nThe `client_overrides` dictionary enables tiered service levels where premium API keys receive higher rate limits than free tier users. Keys in this dictionary match the client identifiers returned by the `identify_client()` function, creating a direct mapping from client identity to rate limit policy.\n\nThe `endpoint_limits` dictionary allows different API endpoints to have different rate limits based on their resource requirements. A simple health check endpoint might allow 1000 requests per minute, while a complex search operation might be limited to 10 requests per minute. Keys match URL patterns or route names defined in the web framework.\n\nThe `cleanup_interval` field controls how frequently the system removes stale client buckets from memory. Shorter intervals reduce memory usage but increase CPU overhead from cleanup operations. Longer intervals reduce overhead but allow memory growth from inactive clients.\n\nThe `redis_url` field enables distributed rate limiting by providing connection information for shared state storage. When `None`, the system operates in single-instance mode using only local memory for bucket storage.\n\n> **Design Insight**: The configuration uses a hierarchical override system where endpoint-specific limits take precedence over client-specific limits, which take precedence over default limits. This enables fine-grained control while maintaining simple configuration for common cases.\n\n### Configuration Model\n\nThe configuration model defines how rate limiting policies are specified, loaded, and applied throughout the system. This model must support both simple single-instance deployments and complex distributed scenarios with thousands of unique rate limiting rules.\n\n#### Environment-Based Configuration\n\nThe primary configuration mechanism uses environment variables to specify rate limiting policies. This approach integrates seamlessly with containerized deployments, configuration management systems, and CI/CD pipelines. Environment-based configuration also enables different rate limits across development, staging, and production environments without code changes.\n\n| Environment Variable | Type | Description |\n|---------------------|------|-------------|\n| `DEFAULT_RATE_LIMIT` | `float` | Default tokens per second for unrecognized clients |\n| `DEFAULT_BURST_SIZE` | `int` | Default bucket capacity for burst handling |\n| `CLIENT_OVERRIDES` | `JSON string` | Per-client rate limit overrides as JSON object |\n| `ENDPOINT_LIMITS` | `JSON string` | Per-endpoint rate limits as JSON object |\n| `REDIS_URL` | `string` | Redis connection URL for distributed storage |\n| `CLEANUP_INTERVAL` | `int` | Seconds between stale bucket cleanup cycles |\n\nThe `DEFAULT_RATE_LIMIT` and `DEFAULT_BURST_SIZE` variables define the baseline rate limiting policy. These values should be set based on the application's typical load patterns and resource capacity. A web API serving lightweight requests might use `DEFAULT_RATE_LIMIT=100` and `DEFAULT_BURST_SIZE=200`, while a resource-intensive service might use much lower values.\n\nThe `CLIENT_OVERRIDES` variable contains a JSON object mapping client identifiers to rate limit configurations. The structure enables different rate limits for different client tiers:\n\n```\nCLIENT_OVERRIDES='{\"premium_client_123\": {\"capacity\": 1000, \"refill_rate\": 50.0}, \"free_client_456\": {\"capacity\": 100, \"refill_rate\": 5.0}}'\n```\n\nThe `ENDPOINT_LIMITS` variable follows a similar JSON structure but maps endpoint patterns to rate limit configurations. This enables API endpoints with different resource requirements to have appropriately scaled rate limits:\n\n```\nENDPOINT_LIMITS='{\"/api/search\": {\"capacity\": 20, \"refill_rate\": 2.0}, \"/api/health\": {\"capacity\": 1000, \"refill_rate\": 100.0}}'\n```\n\n#### Configuration Loading and Validation\n\nThe configuration loading process must handle missing environment variables, invalid JSON structures, and inconsistent rate limit values. The `from_environment()` method implements a robust loading strategy with appropriate defaults and validation.\n\nThe loading process follows these steps:\n\n1. **Environment Variable Extraction**: Read all rate limiting environment variables, applying sensible defaults for missing values\n2. **JSON Parsing and Validation**: Parse `CLIENT_OVERRIDES` and `ENDPOINT_LIMITS` JSON strings, validating structure and data types  \n3. **Rate Limit Validation**: Ensure all rate limits have positive values and reasonable relationships between capacity and refill rate\n4. **Redis Connection Testing**: If `REDIS_URL` is specified, attempt a connection test to validate the configuration\n5. **Configuration Object Construction**: Build the complete `RateLimitConfig` object with validated values\n\nConfiguration validation catches common mistakes such as negative rate limits, zero capacity buckets, or malformed JSON structures. These errors are reported clearly during application startup rather than causing mysterious runtime failures.\n\n> **Critical Design Decision**: Configuration loading happens once at application startup rather than dynamically reloading from environment variables. This ensures consistent rate limiting behavior throughout the application's lifetime and avoids race conditions from configuration changes affecting active rate limiting decisions.\n\n#### Hierarchical Override Resolution\n\nThe configuration model implements a hierarchical override system where more specific rules take precedence over general rules. This hierarchy enables flexible policy definition while maintaining predictable behavior.\n\nThe override resolution order follows this precedence:\n\n1. **Endpoint-Specific Limits**: Rate limits defined for specific URL patterns or routes\n2. **Client-Specific Overrides**: Rate limits defined for specific client identifiers  \n3. **Default Limits**: Baseline rate limits applied when no specific rules match\n\nWhen processing a request, the system first checks if the request's endpoint matches any patterns in `endpoint_limits`. If found, those limits apply regardless of client identity. If no endpoint-specific limits exist, the system checks for client-specific overrides based on the client identifier. Finally, if no specific rules apply, the default limits take effect.\n\nThis hierarchy handles complex scenarios like premium clients accessing resource-intensive endpoints. The endpoint's resource requirements take precedence over the client's general rate limit tier, ensuring appropriate protection for expensive operations.\n\n> **Architecture Decision: Configuration Hierarchy**\n> - **Context**: Need to support both client-based and endpoint-based rate limiting with clear precedence rules\n> - **Options Considered**: \n>   1. Client overrides take precedence over endpoint limits\n>   2. Endpoint limits take precedence over client overrides  \n>   3. Combine client and endpoint limits using mathematical operations\n> - **Decision**: Endpoint limits take precedence over client overrides\n> - **Rationale**: Endpoint limits reflect resource consumption and system protection requirements, which are more critical than client service tiers for maintaining system stability\n> - **Consequences**: Enables resource-based rate limiting while still supporting client tiers for general API usage\n\n### Persistence and Storage\n\nThe persistence and storage model defines how token bucket state is maintained across request processing cycles and system restarts. This model must support both single-instance in-memory storage and distributed Redis-based storage while maintaining consistent behavior and performance characteristics.\n\n#### In-Memory Storage Architecture\n\nIn-memory storage provides the highest performance option for single-instance deployments. Token bucket state lives entirely within the application's memory space, enabling microsecond-latency rate limiting decisions without network round trips or serialization overhead.\n\nThe in-memory storage model uses a hierarchical key structure to organize client buckets:\n\n| Storage Level | Key Format | Description |\n|---------------|------------|-------------|\n| Client Buckets | `client:{client_id}` | Default bucket for a specific client |\n| Endpoint Buckets | `client:{client_id}:endpoint:{endpoint}` | Client-endpoint specific bucket |\n| Metadata | `meta:{client_id}:last_access` | Last access timestamp for cleanup |\n\nThe client bucket key format supports both general client rate limiting and client-endpoint combinations. When a request requires endpoint-specific rate limiting, the system creates a separate bucket with the combined key rather than sharing the client's default bucket.\n\nMetadata storage tracks when each client bucket was last accessed, enabling efficient stale bucket cleanup. The cleanup process iterates through metadata entries and removes buckets that haven't been accessed within the configured timeout period.\n\nMemory management becomes critical in high-traffic scenarios with many unique clients. The storage implementation must balance fast lookups with bounded memory usage, preventing memory leaks from accumulating stale buckets.\n\n#### Distributed Redis Storage\n\nRedis-based storage enables consistent rate limiting across multiple server instances by centralizing token bucket state in a shared data store. This approach supports horizontal scaling of rate limiting infrastructure while maintaining accurate rate limit enforcement.\n\nThe Redis storage model uses atomic operations to ensure consistency under concurrent access from multiple server instances. Each token consumption operation must atomically read the current bucket state, calculate new token counts, and update the stored values without interference from concurrent operations.\n\n| Redis Key Pattern | Data Type | Description |\n|-------------------|-----------|-------------|\n| `rl:bucket:{client_id}` | Hash | Token bucket state with current count and last refill time |\n| `rl:config:{client_id}` | Hash | Client-specific rate limit configuration |\n| `rl:endpoint:{endpoint}` | Hash | Endpoint-specific rate limit configuration |\n| `rl:meta:{client_id}` | String | Last access timestamp for cleanup operations |\n\nThe Redis hash data type stores multiple fields for each bucket, including current token count, last refill timestamp, and configuration parameters. This enables atomic updates of all bucket state within a single Redis operation.\n\nConfiguration data is cached in Redis to avoid repeated environment variable parsing and JSON deserialization. The configuration cache includes both client-specific overrides and endpoint-specific limits, reducing the amount of data that must be retrieved for each rate limiting decision.\n\nDistributed cleanup operations coordinate across multiple server instances to prevent duplicate work and ensure comprehensive stale bucket removal. The cleanup process uses Redis locks to coordinate between instances and prevent race conditions.\n\n#### Storage Consistency and Atomicity\n\nBoth in-memory and Redis storage must handle concurrent access from multiple request processing threads. Token consumption operations involve read-modify-write cycles that must be atomic to prevent race conditions and maintain accurate rate limit enforcement.\n\nIn-memory storage uses thread-local locks to ensure atomic token bucket operations. Each bucket has an associated mutex that protects both token count updates and timestamp modifications. The lock granularity is per-bucket rather than global, enabling high concurrency for requests from different clients.\n\nRedis storage uses Lua scripts to ensure atomic operations. Lua scripts execute atomically within Redis, preventing interleaving of operations from different server instances. The token consumption Lua script performs the complete read-calculate-update cycle as a single atomic operation.\n\n| Operation | In-Memory Approach | Redis Approach |\n|-----------|-------------------|----------------|\n| Token Consumption | Per-bucket mutex lock | Atomic Lua script execution |\n| Bucket Creation | Thread-safe map operations | Redis SET with NX flag |\n| Bucket Cleanup | Global cleanup lock | Distributed Redis locks |\n| Configuration Updates | Immutable config objects | Atomic Redis hash updates |\n\nError handling becomes more complex with distributed storage due to network failures, Redis unavailability, and clock synchronization issues between server instances. The storage layer must implement graceful degradation strategies when Redis operations fail.\n\n> **Architecture Decision: Storage Abstraction Layer**\n> - **Context**: Need to support both in-memory and Redis storage with consistent behavior and easy switching between modes\n> - **Options Considered**:\n>   1. Separate implementations for in-memory and Redis storage\n>   2. Abstract storage interface with pluggable backends\n>   3. Single implementation that detects storage mode at runtime\n> - **Decision**: Abstract storage interface with pluggable backends\n> - **Rationale**: Enables testing with in-memory storage while deploying with Redis storage, and supports future storage backend additions without changing core logic\n> - **Consequences**: Adds interface complexity but improves testability and flexibility for different deployment scenarios\n\n#### Data Serialization and Wire Format\n\nRedis storage requires serialization of token bucket state and configuration data. The serialization format must be compact, human-readable for debugging, and compatible with Redis data types.\n\nToken bucket state uses Redis hash fields to store individual components:\n\n| Hash Field | Data Type | Description |\n|------------|-----------|-------------|\n| `tokens` | Integer | Current token count in bucket |\n| `last_refill` | Float | Unix timestamp of last refill operation |\n| `capacity` | Integer | Maximum bucket capacity |\n| `refill_rate` | Float | Tokens per second refill rate |\n\nConfiguration data uses JSON serialization for complex nested structures like client overrides and endpoint limits. JSON provides human-readable debugging output and easy integration with configuration management tools.\n\nTimestamp handling requires careful consideration of precision and time zone issues. All timestamps use Unix epoch seconds with millisecond precision to ensure consistent time calculations across server instances with different system clocks.\n\n> **Design Insight**: Using Redis hash fields rather than JSON serialization for token bucket state enables atomic updates of individual fields. This prevents races where one server instance overwrites configuration changes made by another instance during token consumption operations.\n\n### Implementation Guidance\n\nThe data model implementation provides the foundation for all rate limiting functionality. This guidance focuses on creating robust, type-safe data structures that support both simple single-instance usage and complex distributed scenarios.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Configuration Loading | Environment variables with `os.environ` | Configuration management with `python-decouple` |\n| JSON Parsing | Built-in `json` module | Schema validation with `jsonschema` |\n| Redis Client | `redis-py` with basic connection | `redis-py` with connection pooling |\n| Type Hints | Basic type annotations | Full `mypy` strict mode validation |\n\n#### Recommended File Structure\n\n```\nrate_limiter/\n  config/\n    __init__.py          ← Configuration loading and validation\n    models.py            ← Core data type definitions\n    environment.py       ← Environment variable handling\n  storage/\n    __init__.py          ← Storage abstraction interfaces  \n    memory.py            ← In-memory storage implementation\n    redis.py             ← Redis storage implementation\n  core/\n    __init__.py\n    bucket.py            ← TokenBucket class implementation\n    tracker.py           ← ClientBucketTracker implementation\n```\n\n#### Core Data Type Definitions\n\n```python\n\"\"\"\nCore data models for rate limiting system.\nDefines all fundamental types used across storage, configuration, and token bucket logic.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional\nimport json\nimport os\n\n@dataclass\nclass TokenBucketConfig:\n    \"\"\"Configuration parameters for a token bucket instance.\"\"\"\n    capacity: int\n    refill_rate: float\n    initial_tokens: Optional[int] = None\n    \n    def __post_init__(self):\n        # TODO: Validate that capacity > 0 and refill_rate > 0\n        # TODO: If initial_tokens is None, set it to capacity\n        # TODO: Validate that initial_tokens <= capacity if specified\n        pass\n\n@dataclass \nclass TokenConsumptionResult:\n    \"\"\"Result of attempting to consume tokens from a bucket.\"\"\"\n    allowed: bool\n    tokens_remaining: int\n    retry_after_seconds: float\n\n@dataclass\nclass RateLimitConfig:\n    \"\"\"Global configuration for the rate limiting system.\"\"\"\n    default_limits: TokenBucketConfig\n    client_overrides: Dict[str, TokenBucketConfig]\n    endpoint_limits: Dict[str, TokenBucketConfig]\n    cleanup_interval: int\n    redis_url: Optional[str] = None\n    \n    @classmethod\n    def from_environment(cls) -> 'RateLimitConfig':\n        \"\"\"Load configuration from environment variables with validation.\"\"\"\n        # TODO: Read DEFAULT_RATE_LIMIT and DEFAULT_BURST_SIZE environment variables\n        # TODO: Parse CLIENT_OVERRIDES JSON string into dictionary of TokenBucketConfig objects\n        # TODO: Parse ENDPOINT_LIMITS JSON string into dictionary of TokenBucketConfig objects  \n        # TODO: Read CLEANUP_INTERVAL with default value of 300 seconds\n        # TODO: Read optional REDIS_URL for distributed storage\n        # TODO: Validate all rate limit values are positive\n        # TODO: Return constructed RateLimitConfig instance\n        pass\n```\n\n#### Configuration Loading Implementation\n\n```python\n\"\"\"\nEnvironment variable handling and configuration validation.\nProvides robust loading of rate limiting policies from environment.\n\"\"\"\n\ndef load_config() -> RateLimitConfig:\n    \"\"\"Load and validate rate limiting configuration from environment.\"\"\"\n    # TODO: Use RateLimitConfig.from_environment() to load configuration\n    # TODO: Catch and re-raise configuration errors with helpful messages\n    # TODO: Log successful configuration loading with summary of loaded rules\n    pass\n\ndef parse_bucket_config_dict(json_str: str) -> Dict[str, TokenBucketConfig]:\n    \"\"\"Parse JSON string containing bucket configurations.\"\"\"\n    # TODO: Parse JSON string using json.loads()\n    # TODO: Iterate through parsed dictionary and convert each value to TokenBucketConfig\n    # TODO: Handle JSON parsing errors and invalid configuration values\n    # TODO: Return dictionary mapping string keys to TokenBucketConfig objects\n    pass\n\ndef validate_rate_limits(config: RateLimitConfig) -> None:\n    \"\"\"Validate that all rate limit configurations are reasonable.\"\"\" \n    # TODO: Check that default_limits has positive values\n    # TODO: Validate all client_overrides have positive rate limits\n    # TODO: Validate all endpoint_limits have positive rate limits  \n    # TODO: Check that cleanup_interval is at least 60 seconds\n    # TODO: If redis_url is specified, validate connection string format\n    # TODO: Raise ValueError with specific message for any validation failure\n    pass\n```\n\n#### Storage Interface Definition\n\n```python\n\"\"\"\nAbstract storage interface supporting both in-memory and Redis backends.\nEnables testing with fast in-memory storage while using Redis in production.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Set\n\nclass BucketStorage(ABC):\n    \"\"\"Abstract interface for token bucket persistence.\"\"\"\n    \n    @abstractmethod\n    def get_bucket_state(self, key: str) -> Optional[Dict]:\n        \"\"\"Retrieve current state for a bucket, or None if not found.\"\"\"\n        # TODO: Implement in subclasses for memory vs Redis storage\n        pass\n    \n    @abstractmethod\n    def update_bucket_state(self, key: str, tokens: int, last_refill: float) -> bool:\n        \"\"\"Atomically update bucket state. Returns True if successful.\"\"\"\n        # TODO: Implement atomic read-modify-write for the storage backend\n        pass\n    \n    @abstractmethod \n    def create_bucket(self, key: str, config: TokenBucketConfig) -> bool:\n        \"\"\"Create new bucket if it doesn't exist. Returns True if created.\"\"\"\n        # TODO: Implement conditional bucket creation for the storage backend\n        pass\n    \n    @abstractmethod\n    def cleanup_stale_buckets(self, max_age_seconds: int) -> int:\n        \"\"\"Remove buckets not accessed within max_age_seconds. Returns count removed.\"\"\"\n        # TODO: Implement stale bucket cleanup for the storage backend\n        pass\n    \n    @abstractmethod\n    def list_bucket_keys(self) -> Set[str]:\n        \"\"\"Return set of all current bucket keys for debugging/monitoring.\"\"\"\n        # TODO: Implement bucket key enumeration for the storage backend  \n        pass\n```\n\n#### Milestone Checkpoints\n\n**After Milestone 1 - Token Bucket Implementation:**\n- Verify `TokenBucketConfig` can be created with valid parameters\n- Test that `TokenConsumptionResult` contains expected fields \n- Run: `python -c \"from config.models import TokenBucketConfig; print(TokenBucketConfig(100, 10.0))\"`\n- Expected output: `TokenBucketConfig(capacity=100, refill_rate=10.0, initial_tokens=100)`\n\n**After Milestone 2 - Per-Client Rate Limiting:**  \n- Verify `RateLimitConfig.from_environment()` loads without errors\n- Test client override parsing from JSON environment variables\n- Run: `DEFAULT_RATE_LIMIT=5.0 DEFAULT_BURST_SIZE=50 python -c \"from config.models import RateLimitConfig; print(RateLimitConfig.from_environment())\"`\n- Expected: Configuration object with default limits and empty override dictionaries\n\n**After Milestone 3 - HTTP Middleware Integration:**\n- Verify configuration loading works with complex JSON overrides\n- Test endpoint limits parsing and hierarchy resolution\n- Set `CLIENT_OVERRIDES='{\"test\": {\"capacity\": 200, \"refill_rate\": 20.0}}'` and verify parsing\n\n**After Milestone 4 - Distributed Rate Limiting:**\n- Verify Redis URL configuration is properly parsed and validated\n- Test storage interface abstraction with both in-memory and Redis backends\n- Run storage backend tests to ensure consistent behavior between implementations\n\n\n## Token Bucket Algorithm Implementation\n\n> **Milestone(s):** Milestone 1 (Token Bucket Implementation) - this section provides the core algorithm that all subsequent milestones build upon\n\n### Mental Model: Water Bucket with Holes\n\nThink of a token bucket as a water bucket with a small hole in the bottom and a steady water tap dripping into it. The water represents tokens that allow requests to pass through your rate limiter. The bucket has a fixed capacity - it can only hold so much water before it overflows. The tap drips water at a constant rate, representing your configured tokens-per-second refill rate. When a request arrives, it's like someone scooping water out of the bucket - they can only scoop what's available.\n\nThe beauty of this mental model lies in understanding bursts. If no one has scooped water for a while, the bucket fills up to its maximum capacity. When a sudden burst of requests arrives, they can all scoop water immediately until the bucket is empty. This allows legitimate traffic bursts while still maintaining an average rate limit over time. Once the bucket is empty, new requests must wait for the tap to drip enough water back in.\n\nThe hole in the bottom represents token decay in some variations, but in our implementation, we'll use a simpler model where tokens don't decay - they just accumulate up to the bucket's capacity. This makes the algorithm more predictable and easier to reason about.\n\n### Token Generation and Refill Logic\n\nThe heart of the token bucket algorithm lies in calculating how many tokens to add based on elapsed time since the last refill. This calculation must handle several complexities: floating-point precision, clock changes, and avoiding integer overflow while maintaining accuracy.\n\nThe fundamental refill equation is straightforward: `tokens_to_add = refill_rate * elapsed_seconds`. However, implementing this correctly requires careful consideration of time precision and edge cases. We measure elapsed time by comparing the current timestamp with the last refill timestamp stored in the bucket state.\n\n**Time Precision and Clock Handling**\n\nOur algorithm uses high-resolution timestamps to ensure accuracy even with sub-second refill rates. Python's `time.time()` provides microsecond precision, which allows us to handle refill rates as high as thousands of tokens per second without losing accuracy. We store the last refill timestamp as a floating-point number representing seconds since the Unix epoch.\n\nClock changes present a significant challenge. If the system clock jumps backward, our elapsed time calculation could become negative, potentially causing tokens to be removed rather than added. If the clock jumps forward significantly, we might add an enormous number of tokens, effectively disabling rate limiting. Our algorithm addresses this by capping the maximum elapsed time to prevent excessive token generation and treating negative elapsed time as zero.\n\n**Token Calculation Algorithm**\n\nThe token refill process follows these steps:\n\n1. Capture the current high-resolution timestamp using `time.time()`\n2. Calculate elapsed seconds by subtracting the last refill timestamp from the current timestamp\n3. Handle clock edge cases by capping elapsed time to a reasonable maximum (typically 60 seconds)\n4. If elapsed time is negative or zero, skip token addition but update the timestamp\n5. Calculate tokens to add using `refill_rate * elapsed_seconds`\n6. Add the calculated tokens to the current bucket count\n7. Cap the total tokens at the bucket's maximum capacity to prevent overflow\n8. Update the last refill timestamp to the current time\n\nThe capping of elapsed time serves as protection against clock jumps and also prevents clients who haven't made requests for extended periods from accumulating excessive burst capacity beyond the intended bucket size.\n\n> **Design Insight**: We update the timestamp even when no tokens are added (negative elapsed time) to prevent repeated attempts to add tokens when the clock is behaving erratically. This ensures the bucket state remains consistent regardless of clock behavior.\n\n![Token Bucket State Machine](./diagrams/token-bucket-state.svg)\n\n### Token Consumption and Burst Handling\n\nToken consumption operates on a simple principle: if sufficient tokens are available, deduct the requested amount and allow the request. If insufficient tokens exist, deny the request without modifying the bucket state. This atomic decision prevents partial consumption that could leave the bucket in an inconsistent state.\n\n**Consumption Decision Logic**\n\nThe consumption algorithm must decide whether to allow or deny a request before modifying any state. This prevents race conditions and ensures that bucket state remains consistent even under high concurrency. The decision process follows these steps:\n\n1. Perform token refill calculation based on elapsed time\n2. Check if the current token count (after refill) meets or exceeds the requested token amount\n3. If sufficient tokens exist, subtract the requested amount and return success\n4. If insufficient tokens exist, leave the bucket unchanged and return denial\n5. Calculate retry-after seconds based on refill rate and token deficit\n\nThe retry-after calculation helps clients understand when they can expect their request to succeed. For a deficit of N tokens at a refill rate of R tokens per second, the retry time is `ceiling(N / R)` seconds. This gives clients actionable information rather than forcing them to guess when to retry.\n\n**Burst Capacity Management**\n\nBurst handling is where the token bucket algorithm shines compared to fixed-window rate limiting. The bucket capacity determines the maximum burst size - the number of requests that can be processed immediately when the bucket is full. A well-configured bucket balances burst tolerance with sustained rate limiting.\n\nConsider a bucket configured with 100 tokens capacity and 10 tokens per second refill rate. This configuration allows bursts of up to 100 requests instantly (if the bucket is full), but sustains only 10 requests per second over longer periods. If a client sends 100 requests immediately, they'll be accepted, but the next request must wait 0.1 seconds for a new token to be generated.\n\nThe relationship between burst capacity and refill rate should align with your service's characteristics. APIs that naturally see bursty traffic patterns benefit from larger bucket capacities relative to their refill rates. Services that prefer steady, predictable load might use smaller bucket capacities closer to their refill rates.\n\n**Token Consumption Result**\n\nEvery consumption attempt returns comprehensive information to enable proper HTTP response handling. The `TokenConsumptionResult` structure contains the allow/deny decision, remaining token count, and retry-after timing. This information flows up to the HTTP middleware layer to generate appropriate response headers and status codes.\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| `allowed` | `bool` | Whether the request should be permitted through |\n| `tokens_remaining` | `int` | Current token count after this operation |\n| `retry_after_seconds` | `float` | Time until request would likely succeed (0 if allowed) |\n\n### Thread Safety and Concurrency\n\nToken bucket operations must be atomic to prevent race conditions that could lead to incorrect token counts or inconsistent bucket state. The primary race condition occurs when multiple threads simultaneously read the current token count, perform refill calculations, and write back updated counts. Without proper synchronization, tokens could be double-counted or lost entirely.\n\n**Concurrency Challenges**\n\nThe token bucket algorithm involves multiple steps that must appear atomic to other threads: timestamp reading, elapsed time calculation, token addition, consumption decision, and state updates. Each of these operations individually might be thread-safe, but the combination creates a critical section that requires protection.\n\nConsider two threads processing requests simultaneously for the same bucket. Thread A reads the current token count (50 tokens), calculates that 10 more tokens should be added based on elapsed time, and determines the new count should be 60. Meanwhile, Thread B reads the same initial state (50 tokens), performs its own calculation, and also determines the count should be 60 after consuming 10 tokens. If both updates proceed, the bucket ends up with 60 tokens instead of the correct 50 tokens (60 from Thread A's refill minus 10 from Thread B's consumption).\n\n**Lock-Based Synchronization**\n\nOur implementation uses a per-bucket mutex to serialize all token bucket operations. This approach trades some performance for correctness and simplicity. Each `TokenBucket` instance contains its own lock, allowing concurrent access to different buckets while serializing access to individual bucket state.\n\nThe locking strategy encompasses the entire token bucket operation from refill calculation through consumption decision. This ensures that token refill and consumption appear atomic to other threads. While this creates a potential bottleneck for high-traffic scenarios hitting the same bucket, it prevents all race conditions and maintains bucket state consistency.\n\nLock contention becomes a concern when many threads access the same client's bucket simultaneously. However, in typical API scenarios, individual clients rarely generate enough concurrent requests to create significant lock contention. The bigger performance concern is usually the overhead of maintaining many individual bucket locks for different clients.\n\n**Alternative Concurrency Approaches**\n\nLock-free implementations using atomic compare-and-swap operations offer better performance but significantly increase implementation complexity. Such approaches require careful handling of the ABA problem and complex retry logic when concurrent updates occur. For most rate limiting scenarios, the added complexity isn't justified by the performance gains.\n\nDatabase-backed implementations (like our Redis-based distributed approach in Milestone 4) handle concurrency through database transaction isolation or atomic operations like Lua scripts. This shifts the concurrency control responsibility to the database layer, which often provides better performance and correctness guarantees than application-level locking.\n\n### Architecture Decision Records\n\n> **Decision: Time Precision for Token Calculations**\n> - **Context**: Token refill calculations require high precision to handle fractional tokens per second and avoid accumulated rounding errors over time.\n> - **Options Considered**: Integer milliseconds, floating-point seconds with microsecond precision, fixed-point arithmetic with custom scaling\n> - **Decision**: Use floating-point seconds with microsecond precision via Python's `time.time()`\n> - **Rationale**: Provides sufficient precision for refill rates up to 1000+ tokens/second while maintaining simple arithmetic. Python's float implementation uses double precision, giving us adequate range and precision for timestamps and calculations.\n> - **Consequences**: Enables accurate token calculations for high-rate buckets but introduces potential floating-point precision issues over very long time periods. Requires careful handling of clock changes and negative elapsed time.\n\n| Option | Precision | Complexity | Range | Performance |\n|--------|-----------|------------|-------|-------------|\n| Integer milliseconds | 1ms | Low | Limited by int size | Fastest |\n| Float seconds | ~1μs | Medium | ~290 billion years | Fast |\n| Fixed-point arithmetic | Configurable | High | Configurable | Medium |\n\n> **Decision: Token Overflow and Maximum Burst Handling**\n> - **Context**: Token buckets must prevent unbounded token accumulation while supporting legitimate burst scenarios.\n> - **Options Considered**: Hard cap at bucket capacity, exponential decay of excess tokens, sliding time window for maximum accumulation\n> - **Decision**: Hard cap tokens at configured bucket capacity with no decay\n> - **Rationale**: Provides predictable behavior that's easy to reason about and configure. Clients can understand exactly how many requests they can burst, and the sustained rate is clearly defined by the refill rate.\n> - **Consequences**: Enables controlled bursts up to bucket capacity but prevents excessive accumulation during idle periods. May be less flexible than decay-based approaches for some use cases.\n\n| Option | Predictability | Implementation | Memory Usage | Burst Control |\n|--------|---------------|----------------|--------------|---------------|\n| Hard cap | High | Simple | Constant | Fixed maximum |\n| Exponential decay | Medium | Complex | Constant | Variable maximum |\n| Sliding window | Low | Very complex | Higher | Dynamic maximum |\n\n> **Decision: Concurrency Control Strategy**\n> - **Context**: Token bucket operations must be thread-safe to prevent race conditions in concurrent request processing scenarios.\n> - **Options Considered**: Per-bucket mutex locks, lock-free atomic operations, single global lock for all buckets\n> - **Decision**: Per-bucket mutex locks protecting the entire refill-and-consume operation\n> - **Rationale**: Provides strong consistency guarantees with moderate performance characteristics. Allows concurrent access to different client buckets while ensuring each bucket's state remains consistent. Implementation complexity is reasonable compared to lock-free approaches.\n> - **Consequences**: Serializes access to individual client buckets, potentially creating bottlenecks for high-traffic clients. Provides straightforward debugging and reasoning about bucket state consistency.\n\n| Option | Consistency | Performance | Complexity | Deadlock Risk |\n|--------|-------------|-------------|------------|---------------|\n| Per-bucket mutex | Strong | Good | Low | Low |\n| Lock-free atomic | Strong | Excellent | Very High | None |\n| Global lock | Strong | Poor | Very Low | Medium |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Race Conditions in Token Refill and Consumption**\n\nMany implementations incorrectly perform token refill and consumption as separate operations, creating a race condition window where another thread can modify bucket state between refill calculation and token consumption. This leads to incorrect token counts and potential over-allocation of resources.\n\nThe fix requires wrapping the entire refill-check-consume sequence in a single critical section. Don't release the lock between calculating new tokens and deciding whether to allow the request. The atomic operation should include timestamp updates, token additions, consumption decisions, and state modifications.\n\n⚠️ **Pitfall: Clock Change Handling**\n\nNaive implementations trust system time implicitly, leading to dramatic failures when clocks change. A backward clock change can cause negative elapsed time, potentially removing tokens from buckets. Forward clock jumps can add millions of tokens instantly, effectively disabling rate limiting for extended periods.\n\nThe fix involves validating elapsed time calculations and capping maximum time jumps. Treat negative elapsed time as zero and limit maximum elapsed time to prevent excessive token accumulation. Always update the last refill timestamp to the current time, even when no tokens are added.\n\n⚠️ **Pitfall: Floating-Point Precision Loss**\n\nLong-running buckets accumulate floating-point precision errors in timestamp and token calculations, eventually leading to incorrect behavior. This is especially problematic for buckets with high refill rates or systems that run for months without restart.\n\nThe fix requires periodic normalization of bucket state and careful choice of floating-point operations. Use high-precision timestamp sources and consider resetting bucket state periodically for long-idle buckets. Be aware of precision limits when working with very large timestamp differences.\n\n⚠️ **Pitfall: Integer Overflow in Token Calculations**\n\nWhen converting floating-point token calculations to integer bucket counts, intermediate calculations can overflow, especially with high refill rates and large time intervals. This typically manifests as buckets suddenly having negative token counts or extremely large positive counts.\n\nThe fix involves range checking at each calculation step and using appropriate integer types. Cap intermediate calculations at reasonable maximums and validate final token counts before updating bucket state. Consider using language-specific overflow detection mechanisms.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Time Handling | `time.time()` for timestamps | `time.perf_counter()` for intervals |\n| Thread Synchronization | `threading.Lock()` per bucket | `threading.RLock()` for read optimization |\n| Numeric Precision | Built-in `float` type | `decimal.Decimal` for high precision |\n| Configuration | Hard-coded constants | Environment-based configuration |\n\n#### Recommended File Structure\n\n```\nrate_limiter/\n  core/\n    __init__.py\n    token_bucket.py          ← TokenBucket class implementation\n    config.py                ← TokenBucketConfig data structure\n    exceptions.py            ← Custom exception types\n  tests/\n    test_token_bucket.py     ← Unit tests for token bucket algorithm\n    test_concurrency.py      ← Thread safety and race condition tests\n```\n\n#### Core Configuration Structure (Complete Implementation)\n\n```python\n\"\"\"\nToken bucket configuration and result types.\n\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict\nimport json\nimport os\n\n@dataclass\nclass TokenBucketConfig:\n    \"\"\"Configuration for a single token bucket instance.\"\"\"\n    capacity: int                    # Maximum tokens the bucket can hold\n    refill_rate: float              # Tokens added per second\n    initial_tokens: Optional[int] = None  # Starting token count (defaults to capacity)\n    \n    def __post_init__(self):\n        if self.capacity <= 0:\n            raise ValueError(\"Bucket capacity must be positive\")\n        if self.refill_rate <= 0:\n            raise ValueError(\"Refill rate must be positive\")\n        if self.initial_tokens is None:\n            self.initial_tokens = self.capacity\n        elif self.initial_tokens > self.capacity:\n            raise ValueError(\"Initial tokens cannot exceed capacity\")\n\n@dataclass\nclass RateLimitConfig:\n    \"\"\"Global rate limiter configuration.\"\"\"\n    default_limits: TokenBucketConfig\n    client_overrides: Dict[str, TokenBucketConfig]\n    endpoint_limits: Dict[str, TokenBucketConfig] \n    cleanup_interval: int\n    redis_url: Optional[str] = None\n    \n    @classmethod\n    def from_environment(cls) -> 'RateLimitConfig':\n        \"\"\"Create configuration from environment variables.\"\"\"\n        # Default rate limiting configuration\n        default_capacity = int(os.getenv('DEFAULT_BURST_SIZE', '100'))\n        default_rate = float(os.getenv('DEFAULT_RATE_LIMIT', '10.0'))\n        default_limits = TokenBucketConfig(\n            capacity=default_capacity,\n            refill_rate=default_rate\n        )\n        \n        # Parse client overrides from JSON\n        client_overrides = {}\n        client_json = os.getenv('CLIENT_OVERRIDES', '{}')\n        if client_json:\n            client_data = json.loads(client_json)\n            for client_id, config in client_data.items():\n                client_overrides[client_id] = TokenBucketConfig(**config)\n        \n        # Parse endpoint limits from JSON  \n        endpoint_limits = {}\n        endpoint_json = os.getenv('ENDPOINT_LIMITS', '{}')\n        if endpoint_json:\n            endpoint_data = json.loads(endpoint_json)\n            for endpoint, config in endpoint_data.items():\n                endpoint_limits[endpoint] = TokenBucketConfig(**config)\n        \n        return cls(\n            default_limits=default_limits,\n            client_overrides=client_overrides,\n            endpoint_limits=endpoint_limits,\n            cleanup_interval=int(os.getenv('CLEANUP_INTERVAL', '300')),\n            redis_url=os.getenv('REDIS_URL')\n        )\n\n@dataclass\nclass TokenConsumptionResult:\n    \"\"\"Result of attempting to consume tokens from a bucket.\"\"\"\n    allowed: bool                # Whether the request was allowed\n    tokens_remaining: int        # Tokens left in bucket after operation  \n    retry_after_seconds: float   # Time to wait before retrying (0 if allowed)\n```\n\n#### Token Bucket Core Logic Skeleton\n\n```python\n\"\"\"\nCore token bucket algorithm implementation.\n\"\"\"\nimport threading\nimport time\nfrom typing import Optional\nfrom .config import TokenBucketConfig, TokenConsumptionResult\n\nclass TokenBucket:\n    \"\"\"\n    Thread-safe token bucket implementation for rate limiting.\n    \n    The token bucket algorithm allows controlled bursts while maintaining \n    average rate limits over time. Tokens are added at a steady rate up to\n    the bucket's capacity, and consumed by incoming requests.\n    \"\"\"\n    \n    def __init__(self, config: TokenBucketConfig):\n        \"\"\"Initialize bucket with configuration.\"\"\"\n        self._config = config\n        self._current_tokens = float(config.initial_tokens)\n        self._last_refill_time = time.time()\n        self._lock = threading.Lock()\n    \n    def try_consume(self, tokens_requested: int = 1) -> TokenConsumptionResult:\n        \"\"\"\n        Attempt to consume tokens from the bucket.\n        \n        This method performs the complete token bucket algorithm:\n        1. Refill tokens based on elapsed time\n        2. Check if sufficient tokens are available  \n        3. Consume tokens if available, or deny if insufficient\n        4. Return result with current state and retry timing\n        \n        Args:\n            tokens_requested: Number of tokens to attempt to consume\n            \n        Returns:\n            TokenConsumptionResult with allow/deny decision and bucket state\n        \"\"\"\n        with self._lock:\n            # TODO 1: Get current timestamp and calculate elapsed time since last refill\n            # Hint: Use time.time() and subtract self._last_refill_time\n            current_time = None\n            elapsed_seconds = None\n            \n            # TODO 2: Handle clock edge cases - cap elapsed time and handle negative values\n            # Hint: If elapsed < 0 or elapsed > 60, treat as special cases\n            # Hint: Always update timestamp even if no tokens added\n            \n            # TODO 3: Calculate tokens to add based on refill rate and elapsed time\n            # Hint: tokens_to_add = self._config.refill_rate * elapsed_seconds\n            tokens_to_add = None\n            \n            # TODO 4: Add tokens to bucket but cap at maximum capacity\n            # Hint: self._current_tokens = min(new_total, self._config.capacity)\n            \n            # TODO 5: Update last refill timestamp to current time\n            \n            # TODO 6: Check if sufficient tokens available for this request\n            # Hint: Compare tokens_requested with self._current_tokens\n            \n            # TODO 7: If sufficient tokens, consume them and return success\n            # Hint: Subtract tokens_requested from self._current_tokens\n            # Hint: Return TokenConsumptionResult(allowed=True, tokens_remaining=..., retry_after_seconds=0.0)\n            \n            # TODO 8: If insufficient tokens, calculate retry-after time and return denial  \n            # Hint: token_deficit = tokens_requested - self._current_tokens\n            # Hint: retry_after = token_deficit / self._config.refill_rate\n            # Hint: Return TokenConsumptionResult(allowed=False, tokens_remaining=..., retry_after_seconds=...)\n            \n            pass  # Replace with actual implementation\n    \n    @property\n    def current_tokens(self) -> int:\n        \"\"\"Get current token count (triggers refill calculation).\"\"\"\n        with self._lock:\n            # TODO: Implement read-only token count that includes refill calculation\n            # Hint: Similar to try_consume but without consuming tokens\n            pass\n    \n    def reset(self) -> None:\n        \"\"\"Reset bucket to initial state (useful for testing).\"\"\"\n        with self._lock:\n            # TODO: Reset tokens to initial_tokens and update timestamp\n            pass\n    \n    def _calculate_refill(self, current_time: float) -> float:\n        \"\"\"\n        Calculate tokens to add based on elapsed time.\n        \n        This helper method encapsulates the refill logic with proper\n        edge case handling for clock changes and precision.\n        \n        Args:\n            current_time: Current timestamp\n            \n        Returns:\n            Number of tokens to add (may be 0)\n        \"\"\"\n        # TODO 1: Calculate elapsed time since last refill\n        # TODO 2: Handle negative elapsed time (clock moved backward)\n        # TODO 3: Cap maximum elapsed time to prevent excessive accumulation\n        # TODO 4: Calculate tokens based on refill rate and elapsed time\n        # TODO 5: Return token count, ensuring it's non-negative\n        pass\n```\n\n#### Language-Specific Implementation Hints\n\n**Python Threading Considerations:**\n- Use `threading.Lock()` rather than `threading.RLock()` unless you need reentrant locking\n- The `with self._lock:` pattern automatically handles lock acquisition and release\n- Consider using `threading.local()` if you need per-thread bucket state\n\n**Time Handling Best Practices:**\n- `time.time()` provides wall clock time suitable for rate limiting calculations\n- For testing, consider dependency injection to allow mock time sources\n- Be aware that `time.time()` can go backward on some systems during clock adjustments\n\n**Floating Point Precision:**\n- Python's `float` type uses double precision, providing ~15 decimal digits\n- For token calculations, this provides sufficient precision for rates up to thousands per second\n- Consider using `decimal.Decimal` if you need exact decimal arithmetic\n\n**Error Handling Patterns:**\n- Validate configuration parameters in `__post_init__` methods using dataclasses\n- Raise `ValueError` for invalid configuration rather than silently correcting\n- Consider custom exception types for rate limiting specific errors\n\n#### Milestone 1 Verification Checkpoint\n\nAfter implementing the token bucket algorithm, verify correct behavior with these tests:\n\n**Basic Functionality Test:**\n```python\n# Create a bucket with 10 token capacity, 1 token/second refill\nconfig = TokenBucketConfig(capacity=10, refill_rate=1.0)  \nbucket = TokenBucket(config)\n\n# Should allow initial requests up to capacity\nfor i in range(10):\n    result = bucket.try_consume(1)\n    assert result.allowed == True\n\n# 11th request should be denied  \nresult = bucket.try_consume(1)\nassert result.allowed == False\nassert result.retry_after_seconds > 0\n```\n\n**Burst Handling Test:**\n```python  \n# Large burst should be allowed up to capacity\nconfig = TokenBucketConfig(capacity=100, refill_rate=10.0)\nbucket = TokenBucket(config) \n\n# Consume all tokens at once\nresult = bucket.try_consume(100)\nassert result.allowed == True\nassert result.tokens_remaining == 0\n\n# Next request should be denied\nresult = bucket.try_consume(1) \nassert result.allowed == False\n```\n\n**Thread Safety Test:**\n```python\nimport concurrent.futures\nimport threading\n\ndef consume_token(bucket):\n    return bucket.try_consume(1)\n\n# Test concurrent access doesn't create race conditions\nconfig = TokenBucketConfig(capacity=1000, refill_rate=100.0)\nbucket = TokenBucket(config)\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    futures = [executor.submit(consume_token, bucket) for _ in range(1000)]\n    results = [f.result() for f in futures]\n    \nallowed_count = sum(1 for r in results if r.allowed)\ndenied_count = sum(1 for r in results if not r.allowed)\n\n# Should allow exactly 1000 requests (initial capacity)\nassert allowed_count == 1000\nassert denied_count == 0\n```\n\n**Signs of Implementation Issues:**\n- Token counts going negative → Check consumption logic and thread safety\n- Buckets never refilling → Verify timestamp updates and elapsed time calculation  \n- Race conditions under load → Ensure entire operation is protected by lock\n- Excessive retry-after times → Check refill rate calculations and token deficit math\n\n\n## Per-Client Rate Limiting\n\n> **Milestone(s):** Milestone 2 (Per-Client Rate Limiting) - this section builds on the core token bucket algorithm to implement individual rate limits for each API consumer\n\nThink of per-client rate limiting as running a sophisticated hotel with different room types and guest privileges. A basic guest might be limited to using the pool twice per day, while a VIP guest gets unlimited access, and a premium suite holder gets access to exclusive facilities. Each guest carries a digital key card that tracks their specific privileges and usage throughout their stay. The hotel's computer system maintains separate accounting for every guest, automatically upgrading or restricting access based on their membership level, and periodically cleans up records for guests who have checked out.\n\nIn our rate limiter, each API consumer gets their own virtual \"key card\" - a unique token bucket configured according to their specific rate limit rules. The system must efficiently track thousands or millions of individual clients simultaneously, applying different limits based on client tier, endpoint sensitivity, or custom business rules. This creates several fascinating engineering challenges: how do we identify clients reliably, store their state efficiently, prevent memory leaks from inactive clients, and maintain thread-safe access under high concurrency?\n\nThe architecture extends our basic token bucket implementation with a **client tracking layer** that acts as an intelligent bucket factory and warehouse manager. When a request arrives, the system identifies the client, retrieves or creates their dedicated bucket, applies the rate limit check, and updates their state atomically. Behind the scenes, a background cleanup process periodically removes stale buckets from clients who haven't been seen recently, preventing unbounded memory growth.\n\n![Client Bucket Lifecycle](./diagrams/client-bucket-lifecycle.svg)\n\n### Client Identification Strategies\n\nThe foundation of per-client rate limiting lies in reliably identifying who is making each request. Think of this as checking IDs at a nightclub entrance - you need a consistent way to recognize returning customers and apply their specific privileges or restrictions. The identification strategy directly impacts both security and functionality, as clients must not be able to easily bypass their limits by changing identifiers.\n\nThe most common identification approaches each offer different trade-offs between simplicity, security, and flexibility. **IP address identification** provides the simplest implementation but suffers from shared NAT scenarios where multiple users appear as a single client, and dynamic IP environments where the same user appears as different clients. **API key identification** offers the strongest control and enables sophisticated per-client configuration, but requires API key distribution and management infrastructure. **HTTP header identification** provides flexibility for custom client classification but depends on client cooperation and header standardization.\n\n| Identification Method | Reliability | Bypass Difficulty | Implementation Complexity | Use Cases |\n|----------------------|-------------|-------------------|-------------------------|-----------|\n| Source IP Address | Medium | Low | Low | Public APIs, basic protection |\n| API Key Header | High | High | Medium | Authenticated APIs, tiered access |\n| Custom Header | Medium | Medium | Low | Internal APIs, client classification |\n| JWT Subject Claim | High | High | High | OAuth-protected APIs, user-based limits |\n| Cookie Session ID | Medium | Medium | Medium | Web applications, session-based limits |\n\nFor robust client identification, the system should extract the client identifier using a **configurable extraction strategy** that can examine multiple request attributes. The `identify_client` function serves as the central identification point, allowing different identification strategies to be plugged in based on configuration or request context.\n\n> **Decision: Hierarchical Client Identification**\n> - **Context**: Different API endpoints may need different client identification strategies, and some scenarios require fallback identification when primary methods fail\n> - **Options Considered**: Single global identification strategy, per-endpoint identification configuration, hierarchical identification with fallbacks\n> - **Decision**: Implement hierarchical identification that tries multiple extraction methods in priority order\n> - **Rationale**: This provides maximum flexibility while maintaining consistent behavior. For example, try API key first, fall back to IP address for unauthenticated requests, enabling gradual migration from IP-based to API key-based limits\n> - **Consequences**: Slightly more complex configuration but enables sophisticated client classification without breaking existing clients during transitions\n\nThe identification logic must handle several edge cases that commonly cause production issues. **Header injection attacks** where malicious clients attempt to spoof identification headers require validation and sanitization of extracted identifiers. **Missing identifier scenarios** need well-defined fallback behavior - typically falling back to IP address or applying default anonymous limits. **Identifier collision** between different identification methods requires careful namespace design to prevent conflicts.\n\n| Edge Case | Problem | Solution | Example |\n|-----------|---------|----------|---------|\n| Missing API Key | Authenticated endpoint receives unauthenticated request | Apply anonymous limits or reject | API key required but header empty |\n| Invalid IP Format | Malformed or spoofed IP addresses | Validate and sanitize, use connection IP | X-Forwarded-For contains invalid data |\n| Header Injection | Client attempts to spoof privileged identifier | Whitelist valid characters, length limits | API key contains control characters |\n| Proxy Scenarios | Multiple clients behind shared IP | Prefer unique identifiers over IP | Corporate firewall with hundreds of users |\n\n### Client Bucket Storage\n\nOnce we can identify clients, we need an efficient storage mechanism for their individual token buckets. Think of this as managing a massive safety deposit box vault where each client has their own secure compartment containing their rate limit state. The vault must allow instant access to any client's compartment while efficiently organizing thousands or millions of compartments and automatically removing compartments for clients who haven't visited recently.\n\nThe storage layer must balance several competing requirements. **Memory efficiency** demands minimal overhead per client bucket, as systems may track millions of clients simultaneously. **Access performance** requires O(1) lookup time to retrieve client buckets under high request concurrency. **Thread safety** ensures multiple simultaneous requests from the same client don't corrupt bucket state. **Automatic cleanup** prevents unbounded memory growth from clients who stop making requests.\n\nThe `ClientBucketTracker` serves as the central coordinator for client bucket lifecycle management. It maintains an internal mapping from client identifiers to `TokenBucket` instances, along with metadata for cleanup decisions. The tracker handles bucket creation on first access, thread-safe bucket retrieval, and coordination with background cleanup processes.\n\n| Component | Responsibility | Data Stored | Concurrency Model |\n|-----------|---------------|-------------|-------------------|\n| ClientBucketTracker | Bucket lifecycle management | client_id → TokenBucket mapping | Read-write lock on bucket map |\n| TokenBucket | Rate limit algorithm | Current tokens, last refill time | Internal locking per bucket |\n| CleanupManager | Stale bucket removal | Last access timestamps | Separate background thread |\n| ConfigResolver | Per-client limit lookup | Rate limit overrides | Read-only after initialization |\n\nThe internal storage structure uses a **concurrent hash map** to provide O(1) bucket lookup while supporting safe concurrent access. Each bucket entry includes the `TokenBucket` instance and a `last_accessed` timestamp for cleanup decisions. The hash map itself requires reader-writer synchronization to handle bucket creation and cleanup operations safely.\n\n> **Decision: Bucket-Level Locking vs Global Locking**\n> - **Context**: Multiple requests from the same client may arrive simultaneously, requiring coordination to prevent race conditions on token bucket state\n> - **Options Considered**: Single global lock for all buckets, per-client bucket locking, lock-free atomic operations\n> - **Decision**: Per-client bucket locking with fine-grained synchronization\n> - **Rationale**: Global locking would serialize all rate limit checks regardless of client, creating a bottleneck. Per-bucket locking allows parallel processing of different clients while ensuring consistency within each client's bucket. Lock-free operations add complexity without significant benefits at this scale\n> - **Consequences**: Higher concurrency and better performance scaling, but requires careful lock ordering to prevent deadlocks during cleanup operations\n\nThe bucket creation process follows a **lazy initialization pattern** where buckets are created only when first accessed. This conserves memory for systems with large potential client populations but sparse actual usage. The creation process must handle the race condition where multiple concurrent requests for the same new client attempt bucket creation simultaneously.\n\nThe bucket storage also integrates with **configuration override resolution** to apply client-specific rate limits. When creating a new bucket, the system consults the `RateLimitConfig.client_overrides` mapping to determine if this client has custom limits that override the default configuration.\n\n### Stale Bucket Cleanup\n\nWithout active cleanup, client bucket storage would grow unboundedly as new clients are encountered, eventually exhausting available memory. Think of stale bucket cleanup as a diligent janitor who periodically walks through the safety deposit box vault, identifies boxes that haven't been accessed recently, and removes them to make space for new clients. The janitor must work efficiently without interfering with active client operations.\n\nThe cleanup mechanism operates as a **background process** that runs periodically to identify and remove buckets that haven't been accessed within a configurable time window. This process balances memory conservation against the cost of recreating buckets for clients who return after cleanup. The cleanup interval and staleness threshold directly impact both memory usage and performance characteristics.\n\n| Cleanup Configuration | Purpose | Typical Value | Impact of Too Low | Impact of Too High |\n|-----------------------|---------|---------------|-------------------|-------------------|\n| cleanup_interval | Time between cleanup runs | 300 seconds | CPU overhead from frequent scans | Memory growth between cleanups |\n| staleness_threshold | Age before bucket removal | 3600 seconds | Frequent bucket recreation | Higher memory usage |\n| batch_size | Buckets removed per cleanup cycle | 1000 buckets | Slow memory reclamation | Long cleanup pauses |\n| max_cleanup_duration | Maximum time per cleanup cycle | 10 seconds | Incomplete cleanup cycles | Interference with request processing |\n\nThe cleanup algorithm follows a **two-phase approach** to minimize interference with active request processing. The first phase identifies stale buckets by scanning the bucket map and collecting candidates for removal based on their last access timestamps. The second phase acquires write locks and removes the identified buckets, handling the race condition where a bucket becomes active again between identification and removal.\n\n```\nCleanup Algorithm Steps:\n1. Wait for cleanup_interval duration or explicit trigger\n2. Acquire read lock on bucket map to get snapshot of all client IDs\n3. For each bucket, check if (current_time - last_accessed) > staleness_threshold\n4. Collect stale bucket IDs into removal candidate list\n5. Release read lock to minimize blocking active requests\n6. Acquire write lock on bucket map for removal phase\n7. For each candidate, verify still stale and remove from map\n8. Release write lock and log cleanup statistics\n9. Schedule next cleanup cycle\n```\n\nThe cleanup process must handle several race conditions that can occur during bucket removal. **Concurrent access during cleanup** happens when a request tries to access a bucket while it's being removed, requiring careful lock ordering to prevent deadlocks. **Bucket resurrection** occurs when a client makes a new request for a bucket that was just marked for removal, requiring validation that removed buckets are truly stale.\n\n> **Decision: Cleanup Threading Model**\n> - **Context**: Cleanup operations require scanning potentially millions of buckets, which could block request processing if not handled carefully\n> - **Options Considered**: Cleanup during request processing, dedicated cleanup thread, cleanup thread pool\n> - **Decision**: Single dedicated cleanup thread with configurable timing\n> - **Rationale**: Request-time cleanup adds latency to user requests and creates unpredictable performance. Multiple cleanup threads add complexity without significant benefits since cleanup is I/O bound by lock acquisition. Single thread simplifies coordination and provides predictable resource usage\n> - **Consequences**: Predictable cleanup overhead and simple coordination, but cleanup throughput limited to single thread performance\n\n### Per-Client Limit Overrides\n\nReal-world applications require different rate limits for different classes of clients. Think of this as a airline's tiered service model - economy passengers get basic luggage allowances, business class passengers get increased limits, and first-class passengers enjoy even higher allowances or no limits at all. The rate limiter must support this hierarchical privilege system while maintaining performance and simplicity.\n\nThe override system operates through a **configuration-driven approach** where client-specific limits are defined in the `RateLimitConfig.client_overrides` mapping. This allows operational teams to adjust limits without code changes while providing audit trails for limit modifications. The override resolution follows a clear precedence hierarchy to handle scenarios where multiple override rules might apply.\n\n| Override Type | Precedence | Configuration Location | Use Cases |\n|---------------|------------|----------------------|-----------|\n| Explicit Client Override | Highest | client_overrides[client_id] | VIP clients, paid tiers, troubleshooting |\n| Endpoint-Specific Default | Medium | endpoint_limits[endpoint] | Sensitive operations, bulk endpoints |\n| Global Default | Lowest | default_limits | Standard rate limiting for all clients |\n| Emergency Override | Special | Runtime configuration | Incident response, temporary restrictions |\n\nThe override resolution process integrates into bucket creation within the `get_bucket_for_client` method. When creating a new bucket for a client, the system queries the configuration hierarchy to determine the appropriate `TokenBucketConfig` parameters. This resolution happens only during bucket creation, ensuring consistent limits for the lifetime of each bucket.\n\nClient override configuration supports **multiple limit dimensions** to handle complex business requirements. Clients may have different limits for different types of operations, time-based limits that change during peak hours, or geographic limits based on their location. The configuration structure accommodates these scenarios through nested override mappings.\n\n| Override Dimension | Configuration Pattern | Example Use Case |\n|-------------------|----------------------|------------------|\n| Client-Endpoint | client_overrides[client_id].endpoint_limits[endpoint] | Premium client gets higher limits for expensive operations |\n| Time-Based | client_overrides[client_id].time_windows[hour_range] | Reduced limits during peak hours |\n| Geographic | client_overrides[client_id].regions[region] | Compliance with regional rate limiting requirements |\n| Operation Type | client_overrides[client_id].operation_types[type] | Different limits for read vs write operations |\n\nThe override system must handle several operational scenarios that arise in production environments. **Dynamic override updates** allow operators to modify client limits without restarting the service, requiring thread-safe configuration reloading. **Override inheritance** enables hierarchical client groups where premium clients automatically inherit elevated base limits. **Temporary overrides** support incident response scenarios where specific clients need immediate limit adjustments.\n\n> **Decision: Override Application Timing**\n> - **Context**: Client overrides could be applied at bucket creation time or checked on every request, with different implications for consistency and performance\n> - **Options Considered**: Apply at bucket creation, check on every request, hybrid approach with periodic refresh\n> - **Decision**: Apply overrides at bucket creation with optional runtime refresh\n> - **Rationale**: Applying at creation provides consistent behavior and eliminates per-request override lookup overhead. Runtime refresh capability enables dynamic limit updates for operational needs without forcing bucket recreation\n> - **Consequences**: Excellent performance with consistent client experience, but limit changes require explicit bucket refresh or waiting for natural bucket expiration\n\n### Architecture Decision Records\n\nThe per-client rate limiting implementation requires several architectural decisions that significantly impact system behavior, performance, and operational characteristics. These decisions form the foundation for the entire client tracking subsystem.\n\n> **Decision: Client Identifier Namespace Design**\n> - **Context**: Different identification methods (IP, API key, custom headers) might produce overlapping values, potentially causing incorrect rate limit sharing between unrelated clients\n> - **Options Considered**: Global identifier space with collision risk, prefixed identifiers by type, separate storage per identifier type\n> - **Decision**: Implement prefixed identifier namespaces (e.g., \"ip:192.168.1.1\", \"apikey:abc123\", \"custom:user456\")\n> - **Rationale**: Prefixed namespaces eliminate collision risk while maintaining single storage and lookup mechanisms. Prefixes provide clear audit trails and debugging information while supporting multiple identification strategies simultaneously\n> - **Consequences**: Slightly longer identifier strings but guaranteed collision avoidance and clear identifier provenance\n\n> **Decision: Memory vs Accuracy Trade-offs**\n> - **Context**: Storing individual buckets for millions of clients consumes significant memory, but aggressive cleanup may remove buckets for temporarily inactive but legitimate clients\n> - **Options Considered**: Aggressive cleanup with short staleness periods, conservative cleanup with long staleness periods, adaptive cleanup based on memory pressure\n> - **Decision**: Configurable staleness thresholds with memory pressure monitoring\n> - **Rationale**: Different deployment scenarios have different memory constraints and client behavior patterns. Configurable thresholds enable optimization for specific environments while memory monitoring provides automatic protection against unbounded growth\n> - **Consequences**: More complex configuration surface but flexibility to optimize for different operational requirements\n\n> **Decision: Bucket State Persistence**\n> - **Context**: In-memory bucket storage loses all client state during application restarts, potentially allowing clients to exceed their intended limits immediately after restart\n> - **Options Considered**: Pure in-memory storage, persistent storage for all buckets, selective persistence for high-value clients\n> - **Decision**: In-memory storage with optional persistent state for identified clients through configuration\n> - **Rationale**: Pure in-memory provides simplest implementation and best performance. Persistent storage adds significant complexity and I/O overhead. Selective persistence enables protection for critical clients while maintaining performance for the general population\n> - **Consequences**: Fast performance and simple implementation, but temporary limit bypass possible after restart\n\n### Common Pitfalls\n\nPer-client rate limiting introduces several subtle bugs and design issues that commonly affect implementations. Understanding these pitfalls helps avoid production incidents and performance problems.\n\n⚠️ **Pitfall: Memory Leaks from Missing Cleanup**\nNew clients create buckets that remain in memory indefinitely without proper cleanup. Over time, this leads to unbounded memory growth and eventual application crashes. The problem often goes unnoticed during development with limited client populations but manifests in production with diverse client traffic. Fix this by implementing robust background cleanup with configurable staleness thresholds and monitoring cleanup effectiveness through metrics.\n\n⚠️ **Pitfall: Race Conditions in Bucket Creation**\nMultiple concurrent requests from a new client can create duplicate buckets for the same client identifier. This results in inconsistent rate limiting where the client effectively gets multiple token buckets and higher effective limits. The race window occurs between checking for bucket existence and creating the bucket. Fix this using atomic compare-and-swap operations or double-checked locking patterns with proper synchronization.\n\n⚠️ **Pitfall: Client Identifier Spoofing**\nMalicious clients can manipulate identification headers to bypass rate limits by appearing as different clients on each request. This completely undermines rate limiting effectiveness and enables abuse. Common attack vectors include randomizing API keys, cycling through IP ranges, or injecting malformed headers. Fix this by validating identifier formats, using server-controlled identification when possible, and implementing identifier allow-lists for sensitive scenarios.\n\n⚠️ **Pitfall: Configuration Override Conflicts**\nComplex override hierarchies can create unexpected interactions where client limits don't match operator expectations. This often manifests as premium clients receiving default limits or test clients getting production limits. The problem increases with configuration complexity and multiple override dimensions. Fix this by implementing explicit override precedence rules, comprehensive override resolution testing, and operational tools to visualize effective limits for any client.\n\n⚠️ **Pitfall: Lock Contention Under High Load**\nPoorly designed locking can serialize all rate limit checks, creating a performance bottleneck that defeats the purpose of rate limiting. This often appears as increased latency and reduced throughput under load, particularly for popular clients with many concurrent requests. The problem typically stems from overly coarse locking granularity or lock-holding during expensive operations. Fix this using fine-grained per-bucket locking, minimizing lock hold times, and avoiding I/O operations while holding locks.\n\n### Implementation Guidance\n\nThe per-client rate limiting implementation extends the basic token bucket with sophisticated client tracking, configuration management, and lifecycle coordination. The implementation requires careful attention to concurrency, memory management, and operational concerns.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Client Storage | Python dict with threading.RWLock | Redis with key expiration |\n| Cleanup Mechanism | threading.Timer with periodic cleanup | APScheduler with cron-like scheduling |\n| Configuration | JSON file with live reloading | Consul/etcd with change notifications |\n| Monitoring | Basic logging with counters | Prometheus metrics with Grafana dashboards |\n| Identifier Validation | Basic string sanitization | Regex patterns with whitelisting |\n\n#### Recommended File Structure\n\n```\nratelimiter/\n  core/\n    token_bucket.py          ← Basic TokenBucket implementation\n    client_tracker.py        ← ClientBucketTracker (this section)\n    config.py                ← RateLimitConfig and overrides\n  storage/\n    memory_storage.py        ← In-memory bucket storage\n    redis_storage.py         ← Distributed storage (Milestone 4)\n  middleware/\n    flask_middleware.py      ← HTTP integration (Milestone 3)\n  utils/\n    cleanup.py               ← Background cleanup manager\n    identifiers.py           ← Client identification strategies\n  tests/\n    test_client_tracker.py   ← Unit tests for client tracking\n    test_overrides.py        ← Configuration override testing\n```\n\n#### Complete Starter Code: Client Identifier Management\n\n```python\n\"\"\"\nClient identification and validation utilities.\nHandles extraction of client identifiers from HTTP requests with proper\nvalidation and namespace management.\n\"\"\"\n\nimport re\nimport ipaddress\nfrom typing import Optional, Dict, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass IdentifierType(Enum):\n    IP_ADDRESS = \"ip\"\n    API_KEY = \"apikey\"\n    CUSTOM_HEADER = \"custom\"\n    JWT_SUBJECT = \"jwt_sub\"\n\n@dataclass\nclass ClientIdentifier:\n    \"\"\"Represents a validated client identifier with namespace information.\"\"\"\n    raw_value: str\n    identifier_type: IdentifierType\n    namespace: str\n    \n    @property\n    def namespaced_id(self) -> str:\n        \"\"\"Returns the full namespaced identifier for storage.\"\"\"\n        return f\"{self.namespace}:{self.raw_value}\"\n\nclass IdentifierValidator:\n    \"\"\"Validates and sanitizes client identifiers to prevent injection attacks.\"\"\"\n    \n    API_KEY_PATTERN = re.compile(r'^[a-zA-Z0-9\\-_]{8,128}$')\n    CUSTOM_HEADER_PATTERN = re.compile(r'^[a-zA-Z0-9\\-_\\.]{1,64}$')\n    \n    @classmethod\n    def validate_ip_address(cls, ip_str: str) -> Optional[str]:\n        \"\"\"Validate and normalize IP address.\"\"\"\n        try:\n            ip_obj = ipaddress.ip_address(ip_str.strip())\n            return str(ip_obj)\n        except ValueError:\n            return None\n    \n    @classmethod\n    def validate_api_key(cls, api_key: str) -> Optional[str]:\n        \"\"\"Validate API key format and length.\"\"\"\n        if cls.API_KEY_PATTERN.match(api_key):\n            return api_key\n        return None\n    \n    @classmethod\n    def validate_custom_header(cls, header_value: str) -> Optional[str]:\n        \"\"\"Validate custom header value.\"\"\"\n        if cls.CUSTOM_HEADER_PATTERN.match(header_value):\n            return header_value\n        return None\n\ndef identify_client(request_data: Dict[str, Any]) -> str:\n    \"\"\"\n    Extract and validate client identifier from request data.\n    \n    Tries multiple identification strategies in priority order:\n    1. API key from Authorization header\n    2. Custom client ID from X-Client-ID header  \n    3. Source IP address from connection or X-Forwarded-For\n    \n    Returns namespaced identifier string for storage and tracking.\n    \"\"\"\n    validator = IdentifierValidator()\n    \n    # Strategy 1: API Key from Authorization header\n    auth_header = request_data.get('authorization', '')\n    if auth_header.startswith('Bearer '):\n        api_key = auth_header[7:]  # Remove 'Bearer ' prefix\n        validated_key = validator.validate_api_key(api_key)\n        if validated_key:\n            return ClientIdentifier(\n                raw_value=validated_key,\n                identifier_type=IdentifierType.API_KEY,\n                namespace=\"apikey\"\n            ).namespaced_id\n    \n    # Strategy 2: Custom Client ID header\n    client_id = request_data.get('x-client-id', '')\n    if client_id:\n        validated_id = validator.validate_custom_header(client_id)\n        if validated_id:\n            return ClientIdentifier(\n                raw_value=validated_id,\n                identifier_type=IdentifierType.CUSTOM_HEADER,\n                namespace=\"custom\"\n            ).namespaced_id\n    \n    # Strategy 3: IP Address (fallback)\n    # Check X-Forwarded-For first, then remote_addr\n    forwarded_ips = request_data.get('x-forwarded-for', '')\n    if forwarded_ips:\n        # Take the first IP in the chain (original client)\n        first_ip = forwarded_ips.split(',')[0].strip()\n        validated_ip = validator.validate_ip_address(first_ip)\n        if validated_ip:\n            return ClientIdentifier(\n                raw_value=validated_ip,\n                identifier_type=IdentifierType.IP_ADDRESS,\n                namespace=\"ip\"\n            ).namespaced_id\n    \n    # Fallback to connection IP\n    remote_addr = request_data.get('remote_addr', '')\n    if remote_addr:\n        validated_ip = validator.validate_ip_address(remote_addr)\n        if validated_ip:\n            return ClientIdentifier(\n                raw_value=validated_ip,\n                identifier_type=IdentifierType.IP_ADDRESS,\n                namespace=\"ip\"\n            ).namespaced_id\n    \n    # Ultimate fallback - anonymous client\n    return \"anonymous:unknown\"\n```\n\n#### Complete Starter Code: Background Cleanup Manager\n\n```python\n\"\"\"\nBackground cleanup manager for removing stale client buckets.\nRuns as a separate thread with configurable intervals and batch processing.\n\"\"\"\n\nimport threading\nimport time\nimport logging\nfrom typing import Dict, Set\nfrom dataclasses import dataclass\n\n@dataclass\nclass CleanupStats:\n    \"\"\"Statistics from a cleanup cycle.\"\"\"\n    buckets_scanned: int\n    buckets_removed: int\n    cleanup_duration_seconds: float\n    memory_freed_bytes: int\n\nclass CleanupManager:\n    \"\"\"Manages background cleanup of stale client buckets.\"\"\"\n    \n    def __init__(self, client_tracker, cleanup_interval: int = 300, \n                 staleness_threshold: int = 3600, max_cleanup_duration: int = 10):\n        self.client_tracker = client_tracker\n        self.cleanup_interval = cleanup_interval\n        self.staleness_threshold = staleness_threshold\n        self.max_cleanup_duration = max_cleanup_duration\n        \n        self._cleanup_thread = None\n        self._stop_event = threading.Event()\n        self._logger = logging.getLogger(__name__)\n        \n    def start(self):\n        \"\"\"Start the background cleanup thread.\"\"\"\n        if self._cleanup_thread and self._cleanup_thread.is_alive():\n            self._logger.warning(\"Cleanup thread already running\")\n            return\n            \n        self._stop_event.clear()\n        self._cleanup_thread = threading.Thread(\n            target=self._cleanup_loop,\n            name=\"bucket-cleanup\",\n            daemon=True\n        )\n        self._cleanup_thread.start()\n        self._logger.info(\"Bucket cleanup manager started\")\n    \n    def stop(self):\n        \"\"\"Stop the background cleanup thread.\"\"\"\n        if self._cleanup_thread:\n            self._stop_event.set()\n            self._cleanup_thread.join(timeout=5.0)\n            self._logger.info(\"Bucket cleanup manager stopped\")\n    \n    def _cleanup_loop(self):\n        \"\"\"Main cleanup loop running in background thread.\"\"\"\n        while not self._stop_event.is_set():\n            try:\n                stats = self.cleanup_stale_buckets()\n                self._logger.info(\n                    f\"Cleanup cycle completed: {stats.buckets_removed} removed \"\n                    f\"of {stats.buckets_scanned} scanned in {stats.cleanup_duration_seconds:.2f}s\"\n                )\n            except Exception as e:\n                self._logger.error(f\"Error during bucket cleanup: {e}\")\n            \n            # Wait for next cycle or stop signal\n            self._stop_event.wait(self.cleanup_interval)\n    \n    def cleanup_stale_buckets(self) -> CleanupStats:\n        \"\"\"\n        Perform one cleanup cycle to remove stale buckets.\n        \n        Returns statistics about the cleanup operation.\n        \"\"\"\n        start_time = time.time()\n        current_time = start_time\n        stale_threshold_time = current_time - self.staleness_threshold\n        \n        # Phase 1: Identify stale buckets (with read lock)\n        stale_client_ids = set()\n        total_scanned = 0\n        \n        with self.client_tracker._bucket_map_lock.read_lock():\n            for client_id, bucket_info in self.client_tracker._bucket_map.items():\n                total_scanned += 1\n                if bucket_info.last_accessed < stale_threshold_time:\n                    stale_client_ids.add(client_id)\n                \n                # Respect maximum cleanup duration\n                if time.time() - start_time > self.max_cleanup_duration:\n                    self._logger.warning(\"Cleanup cycle exceeded maximum duration\")\n                    break\n        \n        # Phase 2: Remove stale buckets (with write lock)\n        buckets_removed = 0\n        if stale_client_ids:\n            with self.client_tracker._bucket_map_lock.write_lock():\n                for client_id in stale_client_ids:\n                    # Double-check staleness in case bucket was accessed\n                    bucket_info = self.client_tracker._bucket_map.get(client_id)\n                    if bucket_info and bucket_info.last_accessed < stale_threshold_time:\n                        del self.client_tracker._bucket_map[client_id]\n                        buckets_removed += 1\n        \n        cleanup_duration = time.time() - start_time\n        \n        return CleanupStats(\n            buckets_scanned=total_scanned,\n            buckets_removed=buckets_removed,\n            cleanup_duration_seconds=cleanup_duration,\n            memory_freed_bytes=buckets_removed * 200  # Rough estimate\n        )\n```\n\n#### Core Logic Skeleton: ClientBucketTracker\n\n```python\n\"\"\"\nClient bucket tracker manages individual rate limit buckets for each client.\nHandles bucket creation, retrieval, configuration overrides, and cleanup coordination.\n\"\"\"\n\nimport threading\nimport time\nfrom typing import Dict, Optional\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass BucketInfo:\n    \"\"\"Container for bucket instance and metadata.\"\"\"\n    bucket: 'TokenBucket'\n    last_accessed: float = field(default_factory=time.time)\n    \n    def update_access_time(self):\n        \"\"\"Update the last accessed timestamp.\"\"\"\n        self.last_accessed = time.time()\n\nclass ClientBucketTracker:\n    \"\"\"\n    Manages per-client token buckets with automatic cleanup and configuration overrides.\n    \n    Thread-safe storage and retrieval of client-specific rate limit buckets.\n    Integrates with cleanup manager for memory management.\n    \"\"\"\n    \n    def __init__(self, config: 'RateLimitConfig'):\n        self.config = config\n        self._bucket_map: Dict[str, BucketInfo] = {}\n        self._bucket_map_lock = threading.RWLock()  # Reader-writer lock\n        \n    def get_bucket_for_client(self, client_id: str, endpoint: Optional[str] = None) -> 'TokenBucket':\n        \"\"\"\n        Get or create token bucket for the specified client.\n        \n        Applies configuration overrides based on client_id and endpoint.\n        Updates last accessed time for cleanup tracking.\n        \n        Args:\n            client_id: Namespaced client identifier from identify_client()\n            endpoint: Optional endpoint name for endpoint-specific limits\n            \n        Returns:\n            TokenBucket instance configured for this client\n        \"\"\"\n        # TODO 1: Try to get existing bucket with read lock\n        # - Acquire read lock on bucket map\n        # - Check if client_id exists in bucket map\n        # - If exists, update access time and return bucket\n        # - Release read lock\n        \n        # TODO 2: Create new bucket if not found (with write lock to prevent races)\n        # - Acquire write lock on bucket map\n        # - Double-check client doesn't exist (race condition prevention)\n        # - If still doesn't exist, create new bucket with resolve_bucket_config()\n        # - Store BucketInfo with bucket and current timestamp\n        # - Release write lock and return new bucket\n        \n        # TODO 3: Handle any locking exceptions\n        # - Ensure locks are properly released in finally blocks\n        # - Log bucket creation for debugging\n        pass\n    \n    def resolve_bucket_config(self, client_id: str, endpoint: Optional[str] = None) -> 'TokenBucketConfig':\n        \"\"\"\n        Resolve effective configuration for a client, applying overrides.\n        \n        Priority order:\n        1. Client-specific override for endpoint: client_overrides[client_id].endpoint_limits[endpoint]\n        2. Client-specific default: client_overrides[client_id] \n        3. Endpoint default: endpoint_limits[endpoint]\n        4. Global default: default_limits\n        \n        Args:\n            client_id: Namespaced client identifier\n            endpoint: Optional endpoint name\n            \n        Returns:\n            TokenBucketConfig with resolved limits\n        \"\"\"\n        # TODO 1: Start with global default configuration\n        # - Get self.config.default_limits as base configuration\n        \n        # TODO 2: Apply endpoint-specific defaults if available\n        # - Check if endpoint exists in self.config.endpoint_limits\n        # - Override base config with endpoint-specific limits\n        \n        # TODO 3: Apply client-specific overrides if available  \n        # - Check if client_id exists in self.config.client_overrides\n        # - Override with client-specific configuration\n        \n        # TODO 4: Apply client-endpoint specific overrides if available\n        # - Check if client has endpoint-specific overrides\n        # - Apply most specific override last\n        \n        # TODO 5: Return final resolved configuration\n        # - Log resolved configuration for debugging\n        pass\n    \n    def cleanup_stale_buckets(self) -> int:\n        \"\"\"\n        Remove buckets that haven't been accessed within staleness threshold.\n        \n        Called by CleanupManager background thread.\n        \n        Returns:\n            Number of buckets removed\n        \"\"\"\n        # TODO 1: Calculate staleness cutoff time\n        # - current_time - self.config.cleanup_interval\n        \n        # TODO 2: Identify stale buckets with read lock\n        # - Scan all buckets in map\n        # - Collect client_ids where last_accessed < cutoff\n        \n        # TODO 3: Remove stale buckets with write lock  \n        # - Double-check staleness before removal\n        # - Delete from bucket map\n        # - Count removed buckets\n        \n        # TODO 4: Return count of removed buckets\n        # - Log cleanup statistics\n        pass\n    \n    def get_client_count(self) -> int:\n        \"\"\"Get current number of tracked clients.\"\"\"\n        # TODO: Return length of bucket map with appropriate locking\n        pass\n    \n    def get_client_stats(self, client_id: str) -> Optional[Dict]:\n        \"\"\"Get statistics for a specific client bucket.\"\"\"\n        # TODO 1: Find bucket for client_id\n        # TODO 2: Extract current tokens, last access time\n        # TODO 3: Return stats dictionary or None if not found\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing per-client rate limiting, verify the following behavior:\n\n**Unit Test Verification:**\n```bash\npython -m pytest tests/test_client_tracker.py -v\n```\n\nExpected test coverage:\n- Client identification with various header combinations\n- Bucket creation and retrieval for new clients  \n- Configuration override resolution with multiple precedence levels\n- Stale bucket cleanup with timing verification\n- Concurrent access patterns with multiple threads\n\n**Manual Testing Commands:**\n```python\n# Test client identification\ntracker = ClientBucketTracker(config)\nclient_id = identify_client({\n    'authorization': 'Bearer abc123def456',\n    'x-forwarded-for': '192.168.1.100',\n    'remote_addr': '10.0.0.1'\n})\nprint(f\"Identified client: {client_id}\")  # Should be \"apikey:abc123def456\"\n\n# Test bucket creation and retrieval\nbucket1 = tracker.get_bucket_for_client(client_id)\nbucket2 = tracker.get_bucket_for_client(client_id)\nassert bucket1 is bucket2  # Should be same instance\n\n# Test configuration overrides\npremium_client_id = \"apikey:premium123\"\npremium_bucket = tracker.get_bucket_for_client(premium_client_id)\nprint(f\"Premium bucket capacity: {premium_bucket.capacity}\")  # Should show override\n\n# Test cleanup timing\ntime.sleep(2)\nremoved_count = tracker.cleanup_stale_buckets()\nprint(f\"Buckets removed: {removed_count}\")  # Should be 0 (not stale yet)\n```\n\n**Signs of Correct Implementation:**\n- Different clients get separate bucket instances\n- Same client always gets the same bucket instance  \n- Premium clients receive configured override limits\n- Stale bucket cleanup runs without errors\n- No memory leaks under sustained load testing\n- Thread-safe operation under concurrent access\n\n**Common Issues to Check:**\n- Race conditions during bucket creation (test with high concurrency)\n- Memory leaks from missing cleanup (run long-term load test)\n- Configuration override precedence errors (test multiple override levels)\n- Lock contention causing performance degradation (profile under load)\n\n\n## HTTP Middleware Integration\n\n> **Milestone(s):** Milestone 3 (HTTP Middleware Integration) - this section transforms the per-client rate limiter into production-ready HTTP middleware with proper status codes, headers, and framework integration\n\n### Mental Model: The Nightclub Bouncer\n\nThink of HTTP middleware as a bouncer at a nightclub entrance. Every person (HTTP request) must pass by the bouncer before entering the club (reaching your application logic). The bouncer checks each person's ID (client identification), consults their clipboard with the guest list and capacity limits (rate limiting rules), and either waves them through with a stamp on their hand (rate limit headers) or politely turns them away with information about when they can return (HTTP 429 response with Retry-After header).\n\nThe bouncer operates at the entrance - not inside the club where the real party happens. This separation means the club's bartenders and DJ (your application code) never have to worry about checking IDs or managing capacity. They can focus on their core job while the bouncer handles all access control decisions consistently across every entrance to the venue.\n\n### Middleware Design Pattern\n\nHTTP middleware follows an **interceptor pattern** where each middleware component wraps the next component in the processing chain. The rate limiting middleware sits early in this chain - typically after request parsing and authentication but before any business logic processing. This positioning ensures that rate limiting decisions happen before expensive operations like database queries or external API calls.\n\nThe middleware design separates concerns cleanly: the rate limiter focuses solely on request throttling while delegating client identification, token bucket management, and storage to specialized components. This separation allows the middleware to remain framework-agnostic - the same core logic works with Flask, Express, FastAPI, or any framework that supports the middleware pattern.\n\n> **Design Principle**: Middleware should be **stateless and composable**. Each middleware instance should not maintain internal state beyond configuration, allowing multiple instances to handle requests interchangeably. The middleware should also play nicely with other middleware - logging, authentication, CORS - without side effects or ordering dependencies.\n\nThe middleware operates through a **request lifecycle hook**: it intercepts each incoming HTTP request, performs its rate limiting logic, and either allows the request to continue to the next middleware or returns an error response directly. This hook-based approach means the middleware integrates naturally with framework request processing pipelines.\n\n**Middleware Responsibilities:**\n\n| Responsibility | Description | Rationale |\n|---|---|---|\n| Client Identification | Extract client identifier from request headers, IP address, or custom fields | Determines which rate limit bucket to consult |\n| Rate Limit Checking | Consult the appropriate token bucket and attempt to consume tokens | Core rate limiting decision logic |\n| HTTP Response Generation | Generate appropriate HTTP responses with correct status codes and headers | Provides standard-compliant feedback to clients |\n| Configuration Resolution | Determine which rate limits apply based on client and endpoint | Supports flexible per-client and per-endpoint policies |\n| Error Handling | Handle storage failures, configuration errors, and edge cases gracefully | Ensures system reliability under failure conditions |\n\n> **Decision: Middleware Positioning in Request Pipeline**\n> - **Context**: HTTP frameworks process requests through a pipeline of middleware components, and positioning affects both functionality and performance\n> - **Options Considered**: \n>   1. Early position (after routing, before authentication)\n>   2. Middle position (after authentication, before business logic)  \n>   3. Late position (just before business logic execution)\n> - **Decision**: Middle position after authentication but before business logic\n> - **Rationale**: This allows rate limiting to use authenticated client information for identification while still protecting expensive business logic operations from abuse\n> - **Consequences**: Enables per-user rate limiting and protects most system resources, but authentication costs are still incurred for rate-limited requests\n\n### HTTP Response Handling\n\nHTTP rate limiting follows established standards and conventions to ensure client applications can handle rate limiting responses appropriately. The middleware must generate responses that are both human-readable and machine-parseable, allowing both developers debugging issues and automated systems to understand rate limiting behavior.\n\n**HTTP Status Code Standards:**\n\nWhen a client exceeds their rate limit, the middleware returns HTTP status code **429 Too Many Requests**. This status code, defined in RFC 6585, specifically indicates that the user has sent too many requests in a given amount of time. The 429 response should never be cached by intermediate proxies or CDNs, ensuring that rate limiting decisions remain dynamic and responsive to current request patterns.\n\nFor allowed requests, the middleware adds rate limiting headers to the response but preserves the original HTTP status code from the downstream application. This approach ensures that rate limiting operates transparently - successful requests continue to return 200, 201, or other appropriate success codes while carrying additional rate limiting metadata.\n\n**Standard Rate Limiting Headers:**\n\n| Header Name | When Present | Value Format | Example | Purpose |\n|---|---|---|---|---|\n| `X-RateLimit-Limit` | All responses | Integer tokens per window | `1000` | Maximum requests allowed per time window |\n| `X-RateLimit-Remaining` | All responses | Integer remaining tokens | `847` | Requests remaining in current window |\n| `X-RateLimit-Reset` | All responses | Unix timestamp | `1699123456` | When the rate limit window resets |\n| `Retry-After` | 429 responses only | Seconds to wait | `60` | How long to wait before retrying |\n\nThe `X-RateLimit-*` headers appear on every response, not just 429 errors. This allows client applications to implement proactive rate limiting - they can slow down their request rate when `X-RateLimit-Remaining` drops low, avoiding rate limit errors entirely. Well-designed API clients use these headers to implement **adaptive backoff strategies**.\n\n**Error Response Body Format:**\n\nThe 429 response includes a structured error body that provides both human-readable and machine-readable information about the rate limiting decision:\n\n```json\n{\n  \"error\": \"rate_limit_exceeded\",\n  \"message\": \"Rate limit exceeded. Maximum 1000 requests per hour allowed.\",\n  \"retry_after_seconds\": 60,\n  \"limit\": 1000,\n  \"remaining\": 0,\n  \"reset_time\": \"2023-11-04T15:30:56Z\"\n}\n```\n\nThis response format balances human readability with programmatic parsing. The `error` field provides a stable identifier for automated error handling, while `message` offers human-readable context. The numeric fields allow client applications to implement sophisticated retry logic without parsing strings.\n\n> **Decision: Rate Limiting Header Standardization**\n> - **Context**: Multiple header standards exist for rate limiting (GitHub's X-RateLimit-*, Twitter's X-Rate-Limit-*, custom approaches), and consistency improves client integration\n> - **Options Considered**:\n>   1. GitHub-style headers (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset)\n>   2. Twitter-style headers (X-Rate-Limit-Limit, X-Rate-Limit-Remaining, X-Rate-Limit-Reset)\n>   3. Custom application-specific headers\n> - **Decision**: GitHub-style X-RateLimit-* headers with Reset as Unix timestamp\n> - **Rationale**: GitHub's approach is widely adopted, uses clear naming, and Unix timestamps avoid timezone parsing issues\n> - **Consequences**: Enables compatibility with existing client libraries and monitoring tools, but requires Unix timestamp handling\n\n### Framework Integration Points\n\nHTTP middleware integration varies significantly across web frameworks, but the core principles remain consistent. The middleware needs to intercept requests early in the processing pipeline, perform rate limiting logic, and either allow the request to continue or return an error response. Each framework provides specific hooks and patterns for this integration.\n\n**Flask Integration Pattern:**\n\nFlask uses **decorator-based middleware** where rate limiting can be applied as a decorator to individual routes or as application-wide middleware using `before_request` hooks. The decorator approach provides fine-grained control over which endpoints have rate limiting, while the application-wide approach ensures comprehensive protection.\n\nFlask middleware accesses request information through the global `request` object and can return responses directly to short-circuit normal request processing. The middleware integrates with Flask's error handling system, allowing 429 responses to be processed by custom error handlers if needed.\n\n**Express.js Integration Pattern:**\n\nExpress uses **function-based middleware** where each middleware is a function that receives `(req, res, next)` parameters. Rate limiting middleware examines the request, consults rate limiting logic, and either calls `next()` to continue processing or sends a response directly using `res.status(429).json()`.\n\nExpress middleware can modify the request object to pass rate limiting information to downstream handlers, enabling applications to adjust behavior based on remaining rate limit capacity.\n\n**Framework-Agnostic Design:**\n\nTo support multiple frameworks efficiently, the middleware separates **framework-specific code** from **core rate limiting logic**. The core logic operates on framework-neutral data structures:\n\n| Component | Framework-Specific | Framework-Neutral |\n|---|---|---|\n| Request Parsing | Extract headers, IP, path from framework request object | Process standardized request dictionary |\n| Rate Limit Logic | None | All token bucket and client tracking logic |\n| Response Generation | Set framework-specific status codes and headers | Generate standard response data structure |\n| Configuration | Load from framework config system | Process standardized configuration objects |\n\nThis separation allows the same rate limiting engine to support Flask, Express, FastAPI, Django, and other frameworks with only thin adapter layers handling framework-specific integration details.\n\n**Middleware Configuration Integration:**\n\nWeb frameworks typically provide configuration systems for environment variables, config files, and runtime settings. The rate limiting middleware integrates with these systems to load `RateLimitConfig` objects without requiring manual configuration in application code.\n\nFlask applications can configure rate limiting through environment variables or application config dictionaries. Express applications can use config modules or environment-based configuration. The middleware provides helper functions like `from_environment()` that automatically detect and parse configuration from common sources.\n\n> **Decision: Configuration Loading Strategy**\n> - **Context**: Web applications use diverse configuration approaches (environment variables, config files, database settings), and the middleware must integrate cleanly without imposing specific patterns\n> - **Options Considered**:\n>   1. Environment variables only with standard naming conventions\n>   2. Framework-specific config file integration\n>   3. Hybrid approach supporting both environment variables and framework configs\n> - **Decision**: Environment variables with standard names as primary method, plus framework-specific config integration\n> - **Rationale**: Environment variables work across all frameworks and deployment environments, while framework integration provides developer convenience\n> - **Consequences**: Enables consistent deployment configuration while maintaining developer ergonomics, but requires maintaining multiple configuration code paths\n\n### Per-Endpoint Rate Limiting\n\nDifferent API endpoints have vastly different resource requirements and abuse potential. A simple health check endpoint can handle thousands of requests per second, while a complex report generation endpoint might only support a few requests per minute. Per-endpoint rate limiting allows administrators to configure appropriate limits that reflect each endpoint's actual capacity and business requirements.\n\n**Endpoint Identification Strategies:**\n\nThe middleware identifies endpoints using **route patterns** rather than exact URL matches. Route patterns capture the logical endpoint while ignoring dynamic path parameters like user IDs or resource identifiers. For example, `/api/users/123` and `/api/users/456` both match the route pattern `/api/users/:id` and share the same rate limit configuration.\n\n| Identification Method | Example | Use Case | Implementation Complexity |\n|---|---|---|---|\n| Route Pattern | `/api/users/:id` | REST APIs with path parameters | Medium |\n| HTTP Method + Path | `GET /api/reports` | Method-specific limits | Low |\n| Custom Header | `X-Endpoint-Type: heavy` | Application-defined grouping | Low |\n| Regex Matching | `/api/v[0-9]+/users` | Version-agnostic limits | High |\n\n**Hierarchical Rate Limit Resolution:**\n\nWhen multiple rate limiting rules could apply to a single request, the middleware uses a **hierarchical resolution strategy** that prioritizes more specific configurations over general ones:\n\n1. **Client + Endpoint Override**: Specific rate limit for this client on this endpoint (highest priority)\n2. **Client Override**: Client-specific rate limit across all endpoints\n3. **Endpoint Limit**: Endpoint-specific rate limit for all clients\n4. **Default Global Limit**: Fallback rate limit when no specific configuration exists (lowest priority)\n\nThis hierarchy allows administrators to start with broad default limits and add targeted overrides for specific clients or endpoints as needed. For example, a premium API client might have higher limits globally, while the `/api/search` endpoint might have lower limits for all clients due to its computational expense.\n\n**Rate Limit Composition:**\n\nSome scenarios require **multiple independent rate limits** to apply simultaneously. A client might be subject to both a per-endpoint limit (100 requests/minute to `/api/search`) and a global account limit (1000 requests/hour across all endpoints). The middleware evaluates all applicable limits and denies the request if any limit would be exceeded.\n\n| Limit Type | Scope | Example Configuration | Enforcement Strategy |\n|---|---|---|---|\n| Per-Endpoint | Single endpoint, all clients | `/api/search`: 10/minute | Check endpoint bucket |\n| Per-Client | Single client, all endpoints | Client ABC: 1000/hour | Check client bucket |\n| Per-Client-Endpoint | Specific client + endpoint | Client ABC on `/api/reports`: 5/hour | Check combined bucket |\n| Global | All clients, all endpoints | System-wide: 10000/minute | Check global bucket |\n\n**Dynamic Endpoint Configuration:**\n\nProduction systems often need to adjust rate limits without restarting applications. The middleware supports **dynamic configuration updates** by reloading rate limit rules from external sources like configuration databases, admin APIs, or file system monitors.\n\nConfiguration changes take effect immediately for new requests, but existing token buckets continue operating with their current parameters until they're naturally cleaned up by the stale bucket cleanup process. This approach avoids disrupting ongoing request patterns while ensuring new patterns adopt updated limits promptly.\n\n> **Decision: Rate Limit Composition Strategy**\n> - **Context**: Some API scenarios require multiple simultaneous rate limits (per-endpoint, per-client, global), and the middleware must determine how to evaluate and enforce multiple applicable limits\n> - **Options Considered**:\n>   1. Single most-specific limit only (hierarchical override)\n>   2. All applicable limits must pass (intersection)\n>   3. Most restrictive applicable limit wins (minimum)\n> - **Decision**: All applicable limits must pass (intersection approach)\n> - **Rationale**: This provides maximum protection by ensuring no individual limit is exceeded, and matches administrator expectations about how multiple limits should interact\n> - **Consequences**: Enables fine-grained control and prevents any single limit from being bypassed, but increases complexity and can create confusing scenarios where requests are denied by unexpected limit combinations\n\n### Architecture Decision Records\n\n> **Decision: Middleware Error Response Format**\n> - **Context**: When rate limits are exceeded, clients need both human-readable error messages and machine-parseable data to implement proper retry logic and error handling\n> - **Options Considered**:\n>   1. Plain text error messages with rate limit headers only\n>   2. JSON error body with structured fields and rate limit headers\n>   3. Custom binary format for minimal bandwidth usage\n> - **Decision**: JSON error body with structured fields plus standard HTTP headers\n> - **Rationale**: JSON provides excellent balance of human readability and programmatic parsing, while headers ensure compatibility with HTTP caching and proxy infrastructure\n> - **Consequences**: Enables sophisticated client retry logic and debugging, but increases response size for rate-limited requests (typically small fraction of total traffic)\n\n> **Decision: Middleware Integration Architecture**\n> - **Context**: Different web frameworks (Flask, Express, FastAPI) have different middleware patterns, and the rate limiter must integrate cleanly without tight coupling to any specific framework\n> - **Options Considered**:\n>   1. Separate implementation for each framework with framework-specific optimizations\n>   2. Framework-agnostic core with thin adapter layers for each framework\n>   3. Single implementation targeting the most common framework patterns\n> - **Decision**: Framework-agnostic core with thin adapter layers\n> - **Rationale**: This maximizes code reuse while allowing framework-specific optimizations in the adapter layer, and reduces maintenance burden across multiple framework integrations\n> - **Consequences**: Enables broad framework support with minimal code duplication, but requires careful abstraction design and may miss some framework-specific optimization opportunities\n\n> **Decision: Rate Limit Header Timing**\n> - **Context**: Rate limit headers can be added before processing the request (showing pre-request state) or after processing (showing post-request state), and this affects client understanding of their remaining capacity\n> - **Options Considered**:\n>   1. Pre-request headers showing capacity before current request\n>   2. Post-request headers showing capacity after current request\n>   3. Both pre and post headers for complete visibility\n> - **Decision**: Post-request headers showing remaining capacity after processing current request\n> - **Rationale**: Post-request headers provide more actionable information for client rate limiting decisions and match the behavior of major API providers like GitHub and Stripe\n> - **Consequences**: Clients receive accurate information about their remaining capacity for subsequent requests, but may be slightly less intuitive for debugging rate limiting issues\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Missing Rate Limit Headers on Success Responses**\n\nMany implementations only add rate limiting headers to 429 error responses, leaving clients without visibility into their rate limit status during normal operation. This prevents clients from implementing proactive rate limiting and leads to unexpected 429 errors.\n\n**Why it's wrong**: Clients need rate limit information on every response to implement adaptive request pacing. Without this information, well-behaved clients cannot avoid hitting rate limits, leading to poor user experience and unnecessary server load from retry attempts.\n\n**How to fix**: Add `X-RateLimit-Limit`, `X-RateLimit-Remaining`, and `X-RateLimit-Reset` headers to every response, regardless of HTTP status code. The middleware should calculate these values after attempting to consume tokens, providing accurate post-request state information.\n\n⚠️ **Pitfall: Incorrect Retry-After Calculation**\n\nSome implementations calculate `Retry-After` based on the rate limit window reset time rather than the actual time when tokens will be available for the blocked request. This leads to clients waiting longer than necessary, reducing system throughput.\n\n**Why it's wrong**: If a client needs 5 tokens but only 3 are available, they don't need to wait for the entire bucket to refill - they only need to wait until 2 additional tokens are generated. Waiting for a full window reset wastes time and reduces effective API throughput.\n\n**How to fix**: Calculate `Retry-After` based on when sufficient tokens will be available for the specific request: `tokens_needed / refill_rate`. This provides the minimum wait time for the request to succeed.\n\n⚠️ **Pitfall: Client Identification Inconsistency**\n\nDifferent middleware instances or different request processing paths sometimes extract client identifiers differently, leading to the same client being treated as multiple distinct clients. This effectively bypasses rate limiting by spreading requests across multiple buckets.\n\n**Why it's wrong**: Inconsistent client identification allows clients to exceed rate limits by varying request characteristics like header formatting, IP address representation, or authentication token format. This undermines the entire rate limiting system's effectiveness.\n\n**How to fix**: Implement robust client identifier normalization that handles variations in IP address format (IPv6 vs IPv4, leading zeros), header capitalization, and authentication token formatting. Use the `validate_ip_address()` and `validate_api_key()` functions consistently across all code paths.\n\n⚠️ **Pitfall: Framework Integration Ordering Issues**\n\nPlacing rate limiting middleware after authentication or authorization middleware can lead to expensive operations being performed before rate limit checking, allowing denial-of-service attacks that consume resources even when requests are ultimately rate-limited.\n\n**Why it's wrong**: If authentication involves database queries or external API calls, attackers can consume significant resources by sending many requests that are authenticated but then rate-limited. This reduces the effectiveness of rate limiting as a protective mechanism.\n\n**How to fix**: Position rate limiting middleware early in the processing pipeline, ideally after basic request parsing but before expensive authentication operations. For endpoints that require authenticated rate limiting, implement a two-stage approach with basic rate limiting early and authenticated rate limiting after authentication.\n\n⚠️ **Pitfall: Static Configuration Without Reload**\n\nHardcoding rate limit configurations or loading them only at application startup prevents operators from adjusting limits in response to changing conditions or abuse patterns without service restarts.\n\n**Why it's wrong**: Production systems need dynamic rate limit adjustment to handle traffic spikes, mitigate abuse, or accommodate special events. Requiring application restarts for configuration changes introduces operational risk and delays incident response.\n\n**How to fix**: Implement configuration reload capabilities that monitor external configuration sources (environment variables, config files, database tables) and update rate limiting rules without requiring application restarts. Ensure configuration changes are applied atomically to avoid inconsistent states.\n\n### Implementation Guidance\n\nThis subsection provides Flask-specific implementation guidance for integrating the rate limiting middleware. The implementation assumes you have already completed the token bucket algorithm and per-client rate limiting components from previous milestones.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Web Framework | Flask with built-in request handling | Flask with Werkzeug middleware stack |\n| HTTP Client Identification | `request.remote_addr` for IP | X-Forwarded-For parsing with proxy chain validation |\n| Configuration Loading | Environment variables with `os.getenv()` | Flask config with JSON schema validation |\n| Response Serialization | `flask.jsonify()` for JSON responses | Custom JSON encoder with datetime handling |\n| Header Management | Direct `response.headers[]` assignment | Response middleware with header normalization |\n\n#### Recommended File Structure\n\n```\nrate_limiter/\n├── middleware/\n│   ├── __init__.py\n│   ├── flask_middleware.py          ← main Flask integration\n│   ├── rate_limit_middleware.py     ← framework-agnostic core\n│   └── response_builder.py          ← HTTP response generation\n├── config/\n│   ├── __init__.py\n│   └── middleware_config.py         ← configuration loading and validation\n├── examples/\n│   ├── flask_app.py                 ← complete Flask application example\n│   └── flask_config_example.py      ← configuration examples\n└── tests/\n    ├── test_flask_middleware.py     ← Flask-specific integration tests\n    └── test_response_builder.py     ← HTTP response testing\n```\n\n#### Infrastructure Starter Code\n\n**Complete HTTP Response Builder** (ready to use):\n\n```python\n# response_builder.py - Complete implementation for HTTP response generation\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime\nimport json\nfrom flask import Response\n\nclass RateLimitResponseBuilder:\n    \"\"\"Builds standardized HTTP responses for rate limiting scenarios.\"\"\"\n    \n    def __init__(self):\n        self.standard_headers = {\n            'Content-Type': 'application/json',\n            'Cache-Control': 'no-store',\n        }\n    \n    def build_success_response(self, original_response: Response, \n                              consumption_result, rate_config) -> Response:\n        \"\"\"Add rate limiting headers to successful response.\"\"\"\n        self._add_rate_limit_headers(\n            original_response, \n            consumption_result, \n            rate_config\n        )\n        return original_response\n    \n    def build_rate_limit_exceeded_response(self, consumption_result, \n                                         rate_config) -> Response:\n        \"\"\"Build 429 Too Many Requests response with proper headers and body.\"\"\"\n        error_body = {\n            'error': 'rate_limit_exceeded',\n            'message': f'Rate limit exceeded. Maximum {rate_config.capacity} requests per {self._format_time_window(rate_config)} allowed.',\n            'retry_after_seconds': int(consumption_result.retry_after_seconds),\n            'limit': rate_config.capacity,\n            'remaining': consumption_result.tokens_remaining,\n            'reset_time': self._calculate_reset_time(rate_config)\n        }\n        \n        response = Response(\n            response=json.dumps(error_body, indent=2),\n            status=429,\n            headers=self.standard_headers.copy()\n        )\n        \n        # Add standard rate limiting headers\n        self._add_rate_limit_headers(response, consumption_result, rate_config)\n        response.headers['Retry-After'] = str(int(consumption_result.retry_after_seconds))\n        \n        return response\n    \n    def _add_rate_limit_headers(self, response: Response, consumption_result, rate_config):\n        \"\"\"Add X-RateLimit-* headers to response.\"\"\"\n        response.headers['X-RateLimit-Limit'] = str(rate_config.capacity)\n        response.headers['X-RateLimit-Remaining'] = str(consumption_result.tokens_remaining)\n        response.headers['X-RateLimit-Reset'] = str(int(self._calculate_reset_timestamp(rate_config)))\n    \n    def _format_time_window(self, rate_config) -> str:\n        \"\"\"Convert refill rate to human-readable time window.\"\"\"\n        if rate_config.refill_rate >= 1.0:\n            return f\"{int(rate_config.refill_rate)} per second\"\n        elif rate_config.refill_rate >= 1/60:\n            return f\"{int(rate_config.refill_rate * 60)} per minute\"\n        else:\n            return f\"{int(rate_config.refill_rate * 3600)} per hour\"\n    \n    def _calculate_reset_time(self, rate_config) -> str:\n        \"\"\"Calculate ISO 8601 formatted reset time.\"\"\"\n        reset_timestamp = self._calculate_reset_timestamp(rate_config)\n        return datetime.fromtimestamp(reset_timestamp).isoformat() + 'Z'\n    \n    def _calculate_reset_timestamp(self, rate_config) -> float:\n        \"\"\"Calculate Unix timestamp when rate limit resets.\"\"\"\n        import time\n        # Calculate when bucket will be full based on current state and refill rate\n        current_time = time.time()\n        seconds_to_full = rate_config.capacity / rate_config.refill_rate\n        return current_time + seconds_to_full\n```\n\n**Complete Configuration Loader** (ready to use):\n\n```python\n# middleware_config.py - Complete configuration loading with validation\nimport os\nimport json\nfrom typing import Dict, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass MiddlewareConfig:\n    \"\"\"Configuration for rate limiting middleware integration.\"\"\"\n    enabled: bool = True\n    client_identification_strategy: str = 'ip_address'  # ip_address, api_key, custom_header\n    custom_header_name: Optional[str] = None\n    skip_rate_limiting_header: Optional[str] = 'X-Skip-Rate-Limit'\n    error_response_format: str = 'json'  # json, plain_text\n    include_debug_info: bool = False\n\ndef load_middleware_config() -> MiddlewareConfig:\n    \"\"\"Load middleware configuration from environment variables.\"\"\"\n    return MiddlewareConfig(\n        enabled=os.getenv('RATE_LIMIT_ENABLED', 'true').lower() == 'true',\n        client_identification_strategy=os.getenv('RATE_LIMIT_CLIENT_ID_STRATEGY', 'ip_address'),\n        custom_header_name=os.getenv('RATE_LIMIT_CUSTOM_HEADER'),\n        skip_rate_limiting_header=os.getenv('RATE_LIMIT_SKIP_HEADER', 'X-Skip-Rate-Limit'),\n        error_response_format=os.getenv('RATE_LIMIT_ERROR_FORMAT', 'json'),\n        include_debug_info=os.getenv('RATE_LIMIT_DEBUG', 'false').lower() == 'true'\n    )\n\nclass ClientIdentificationHelper:\n    \"\"\"Helper for extracting and validating client identifiers from requests.\"\"\"\n    \n    @staticmethod\n    def extract_client_id(request, strategy: str, custom_header: Optional[str] = None) -> str:\n        \"\"\"Extract client identifier using specified strategy.\"\"\"\n        if strategy == 'ip_address':\n            return ClientIdentificationHelper._extract_ip_address(request)\n        elif strategy == 'api_key':\n            return ClientIdentificationHelper._extract_api_key(request)\n        elif strategy == 'custom_header' and custom_header:\n            return ClientIdentificationHelper._extract_custom_header(request, custom_header)\n        else:\n            # Fallback to IP address\n            return ClientIdentificationHelper._extract_ip_address(request)\n    \n    @staticmethod\n    def _extract_ip_address(request) -> str:\n        \"\"\"Extract and validate client IP address with proxy support.\"\"\"\n        # Check for X-Forwarded-For header (proxy/load balancer)\n        forwarded_for = request.headers.get('X-Forwarded-For')\n        if forwarded_for:\n            # Take the first IP in the chain (original client)\n            client_ip = forwarded_for.split(',')[0].strip()\n        else:\n            # Direct connection\n            client_ip = request.remote_addr or '127.0.0.1'\n        \n        # Validate and normalize IP address\n        validated_ip = validate_ip_address(client_ip)\n        return validated_ip or '127.0.0.1'\n    \n    @staticmethod\n    def _extract_api_key(request) -> str:\n        \"\"\"Extract API key from Authorization header or query parameter.\"\"\"\n        # Check Authorization header first\n        auth_header = request.headers.get('Authorization', '')\n        if auth_header.startswith('Bearer '):\n            api_key = auth_header[7:]  # Remove 'Bearer ' prefix\n            validated_key = validate_api_key(api_key)\n            if validated_key:\n                return validated_key\n        \n        # Check X-API-Key header\n        api_key = request.headers.get('X-API-Key')\n        if api_key:\n            validated_key = validate_api_key(api_key)\n            if validated_key:\n                return validated_key\n        \n        # Fallback to IP address if no valid API key found\n        return ClientIdentificationHelper._extract_ip_address(request)\n    \n    @staticmethod\n    def _extract_custom_header(request, header_name: str) -> str:\n        \"\"\"Extract client ID from custom header.\"\"\"\n        custom_value = request.headers.get(header_name)\n        if custom_value and len(custom_value.strip()) > 0:\n            return custom_value.strip()\n        \n        # Fallback to IP address if custom header missing\n        return ClientIdentificationHelper._extract_ip_address(request)\n```\n\n#### Core Logic Skeleton Code\n\n**Framework-Agnostic Middleware Core** (skeleton for implementation):\n\n```python\n# rate_limit_middleware.py - Core middleware logic (implement the TODOs)\nfrom typing import Optional\nimport time\n\nclass RateLimitMiddleware:\n    \"\"\"Framework-agnostic rate limiting middleware core.\"\"\"\n    \n    def __init__(self, client_tracker, response_builder, middleware_config):\n        self.client_tracker = client_tracker\n        self.response_builder = response_builder\n        self.config = middleware_config\n        self.client_id_helper = ClientIdentificationHelper()\n    \n    def process_request(self, request_data: dict) -> dict:\n        \"\"\"\n        Process rate limiting for a single HTTP request.\n        \n        Args:\n            request_data: Framework-neutral request information\n                - headers: Dict[str, str]\n                - remote_addr: str\n                - path: str\n                - method: str\n        \n        Returns:\n            Processing result dict with:\n                - allowed: bool\n                - response_data: Optional[dict] (if rate limited)\n                - consumption_result: TokenConsumptionResult\n                - rate_config: TokenBucketConfig\n        \"\"\"\n        # TODO 1: Check if rate limiting is disabled in configuration\n        # If disabled, return early with allowed=True\n        \n        # TODO 2: Extract client identifier using configured strategy\n        # Use self.client_id_helper.extract_client_id() with request_data\n        # Handle the strategy from self.config.client_identification_strategy\n        \n        # TODO 3: Determine endpoint identifier from request path and method\n        # Format as \"METHOD /path/pattern\" for endpoint-specific rate limiting\n        # Consider normalizing path parameters (e.g., /users/123 -> /users/:id)\n        \n        # TODO 4: Get appropriate token bucket for client and endpoint\n        # Use self.client_tracker.get_bucket_for_client(client_id, endpoint)\n        # This should handle per-client and per-endpoint configuration resolution\n        \n        # TODO 5: Attempt to consume tokens for this request\n        # Use bucket.try_consume(1) to attempt consuming 1 token\n        # Store the TokenConsumptionResult for response building\n        \n        # TODO 6: Build response based on consumption result\n        # If allowed, return success result with rate limit headers\n        # If denied, build 429 response with Retry-After header\n        # Include consumption_result and rate_config for header generation\n        \n        pass  # Remove this line when implementing\n    \n    def should_skip_rate_limiting(self, request_data: dict) -> bool:\n        \"\"\"\n        Check if request should bypass rate limiting.\n        \n        Returns True if request has skip header or matches bypass conditions.\n        \"\"\"\n        # TODO 1: Check for skip rate limiting header\n        # Look for self.config.skip_rate_limiting_header in request headers\n        # Return True if header is present and has truthy value\n        \n        # TODO 2: Add any additional bypass conditions\n        # Consider health check endpoints, internal service calls, etc.\n        # Return True for requests that should bypass rate limiting\n        \n        pass  # Remove this line when implementing\n    \n    def normalize_endpoint_path(self, path: str, method: str) -> str:\n        \"\"\"\n        Normalize endpoint path for consistent rate limit grouping.\n        \n        Converts paths like /users/123 to /users/:id for rate limiting purposes.\n        \"\"\"\n        # TODO 1: Combine HTTP method with path for unique endpoint identification\n        # Format as \"GET /api/users\" or \"POST /api/reports\"\n        \n        # TODO 2: Replace numeric path segments with parameter placeholders\n        # /users/123 -> /users/:id, /reports/2023-11-04 -> /reports/:date\n        # Use regex or string processing to identify and replace patterns\n        \n        # TODO 3: Handle common REST patterns\n        # /api/v1/users/123/posts/456 -> /api/v1/users/:id/posts/:id\n        # Consider UUID patterns, date patterns, and numeric IDs\n        \n        # TODO 4: Return normalized endpoint identifier\n        # This becomes the key for endpoint-specific rate limiting\n        \n        pass  # Remove this line when implementing\n```\n\n**Flask-Specific Integration** (skeleton for implementation):\n\n```python\n# flask_middleware.py - Flask integration wrapper (implement the TODOs)\nfrom functools import wraps\nfrom flask import request, g\nimport time\n\nclass FlaskRateLimitMiddleware:\n    \"\"\"Flask-specific wrapper for rate limiting middleware.\"\"\"\n    \n    def __init__(self, app=None):\n        self.core_middleware = None\n        if app:\n            self.init_app(app)\n    \n    def init_app(self, app):\n        \"\"\"Initialize middleware with Flask application.\"\"\"\n        # TODO 1: Load configuration using load_middleware_config()\n        # Store middleware configuration in self.middleware_config\n        \n        # TODO 2: Load rate limiting configuration using from_environment()\n        # Store rate limit rules in self.rate_limit_config\n        \n        # TODO 3: Initialize ClientBucketTracker with configuration\n        # Pass rate_limit_config to create client_tracker instance\n        \n        # TODO 4: Initialize RateLimitResponseBuilder\n        # Create response_builder for generating HTTP responses\n        \n        # TODO 5: Create core middleware instance\n        # Initialize RateLimitMiddleware with tracker, response_builder, and config\n        \n        # TODO 6: Register Flask before_request handler\n        # Use app.before_request to register self._before_request_handler\n        \n        # TODO 7: Register Flask after_request handler  \n        # Use app.after_request to register self._after_request_handler\n        \n        pass  # Remove this line when implementing\n    \n    def _before_request_handler(self):\n        \"\"\"Flask before_request handler for rate limiting.\"\"\"\n        # TODO 1: Convert Flask request to framework-neutral format\n        # Extract headers, remote_addr, path, method from Flask request object\n        # Create request_data dict with standardized field names\n        \n        # TODO 2: Process request through core middleware\n        # Call self.core_middleware.process_request(request_data)\n        # Store result in Flask g object for access in after_request\n        \n        # TODO 3: Handle rate limit exceeded case\n        # If result['allowed'] is False, return 429 response immediately\n        # Use result['response_data'] to build Flask Response object\n        # This short-circuits normal request processing\n        \n        # TODO 4: Store rate limiting info for successful requests\n        # Save consumption_result and rate_config in g for after_request handler\n        # This allows adding headers to successful responses\n        \n        pass  # Remove this line when implementing\n    \n    def _after_request_handler(self, response):\n        \"\"\"Flask after_request handler to add rate limiting headers.\"\"\"\n        # TODO 1: Check if rate limiting information is available\n        # Look for rate limiting data stored in g by before_request handler\n        # Skip header addition if no rate limiting data present\n        \n        # TODO 2: Add rate limiting headers to response\n        # Use response_builder to add X-RateLimit-* headers\n        # Apply headers to both successful and error responses\n        \n        # TODO 3: Return modified response\n        # Flask after_request handlers must return the response object\n        \n        pass  # Remove this line when implementing\n    \n    def limit(self, rate_limit_override=None):\n        \"\"\"Decorator for applying rate limiting to specific Flask routes.\"\"\"\n        def decorator(f):\n            @wraps(f)\n            def decorated_function(*args, **kwargs):\n                # TODO 1: Apply route-specific rate limiting logic\n                # This allows individual routes to have custom rate limits\n                # Use rate_limit_override parameter if provided\n                \n                # TODO 2: Perform rate limit check specific to this route\n                # Similar to before_request_handler but with route-specific config\n                \n                # TODO 3: Call original route function if allowed\n                # Return 429 response if rate limited\n                \n                pass  # Remove this line when implementing\n            \n            return decorated_function\n        return decorator\n```\n\n#### Language-Specific Hints\n\n**Flask Request Handling:**\n- Use `request.headers.get('Header-Name')` to safely access headers with default values\n- `request.remote_addr` provides client IP, but check `X-Forwarded-For` for proxy setups\n- Flask's `g` object provides request-scoped storage for sharing data between before_request and after_request handlers\n- Use `request.endpoint` to get the route function name for endpoint-specific limiting\n\n**HTTP Response Construction:**\n- Create Flask Response objects with `Response(response=json_string, status=429, headers=header_dict)`\n- Use `flask.jsonify()` for automatic JSON serialization with proper Content-Type headers\n- Flask automatically handles header encoding and HTTP compliance for standard headers\n\n**Configuration Management:**\n- Use `app.config.get()` to access Flask configuration with default values\n- Environment variables can be loaded with `os.getenv('VAR_NAME', 'default_value')`\n- Consider using `python-dotenv` to load environment variables from `.env` files in development\n\n**Error Handling:**\n- Flask's `@app.errorhandler(429)` can provide global 429 error response formatting\n- Use try/except blocks around rate limiting logic to handle Redis connection failures gracefully\n- Log rate limiting decisions and errors using `app.logger` for debugging and monitoring\n\n#### Milestone Checkpoint\n\nAfter implementing the HTTP middleware integration:\n\n**Command to Run:**\n```bash\ncd rate_limiter/\npython -m pytest tests/test_flask_middleware.py -v\npython examples/flask_app.py\n```\n\n**Expected Output:**\n```\ntest_flask_middleware.py::test_successful_request_includes_headers PASSED\ntest_flask_middleware.py::test_rate_limit_exceeded_returns_429 PASSED  \ntest_flask_middleware.py::test_per_endpoint_rate_limiting PASSED\ntest_flask_middleware.py::test_client_identification_strategies PASSED\n\nFlask app running on http://127.0.0.1:5000\n```\n\n**Manual Verification Steps:**\n1. **Test successful request with headers**: `curl -i http://127.0.0.1:5000/api/test` should return 200 with `X-RateLimit-*` headers\n2. **Test rate limit exceeded**: Send multiple rapid requests: `for i in {1..20}; do curl -i http://127.0.0.1:5000/api/test; done` - should see 429 responses\n3. **Test Retry-After header**: 429 responses should include `Retry-After` header with reasonable wait time\n4. **Test per-endpoint limits**: `/api/search` and `/api/reports` should have different rate limits if configured\n\n**Signs Something is Wrong:**\n- **Missing headers on 200 responses**: Check `_after_request_handler` implementation and ensure rate limiting data is stored in `g`\n- **429 responses without Retry-After**: Verify `build_rate_limit_exceeded_response` includes all required headers\n- **Same rate limits for all endpoints**: Check endpoint normalization and configuration resolution logic\n- **Client identification not working**: Verify IP address extraction handles `X-Forwarded-For` and proxy scenarios correctly\n\n\n## Distributed Rate Limiting\n\n> **Milestone(s):** Milestone 4 (Distributed Rate Limiting) - this section scales the HTTP middleware rate limiter across multiple server instances using Redis for shared state and atomic operations\n\n### Mental Model: The Multi-Branch Bank with Central Ledger\n\nThink of our distributed rate limiting system like a bank with multiple branch locations serving the same customers. Each customer has a single account with a fixed spending limit, but they can visit any branch to make withdrawals. Without coordination between branches, a customer could withdraw their entire limit at Branch A, then immediately drive to Branch B and withdraw the same amount again - effectively doubling their limit.\n\nThe solution banks use is a **central ledger system**. Every branch connects to the same central database that tracks account balances in real-time. When a customer attempts a withdrawal, the branch must check the central ledger and update the balance atomically - either the withdrawal succeeds and the balance decreases, or it fails because insufficient funds remain. Multiple branches can serve the same customer simultaneously without accidentally allowing overdrafts.\n\nIn our rate limiting context, each server instance is like a bank branch, each client is like a customer, and the token bucket represents their \"spending limit\" for API requests. Redis becomes our central ledger, storing the authoritative token counts for all clients. When any server instance receives a request, it must check Redis and atomically consume tokens, ensuring that the client's total request rate across all servers never exceeds their configured limit.\n\n### Distributed System Challenges\n\nWhen we move from a single-server rate limiter to a distributed system, several fundamental challenges emerge that make simple in-memory token buckets insufficient for maintaining consistent rate limits across multiple server instances.\n\n**The Coordination Problem**\n\nIn a single-server deployment, our `TokenBucket` class maintains accurate token counts because all requests for a given client flow through the same process. The bucket's internal state reflects the true consumption history, and thread-safe operations prevent race conditions within that single process. However, when we deploy the same rate limiter code across multiple server instances, each instance maintains its own isolated view of client token buckets.\n\nConsider a client with a limit of 100 requests per minute hitting a three-server cluster. If each server maintains independent in-memory buckets, the client could potentially make 100 requests to Server A, then 100 requests to Server B, then 100 requests to Server C - achieving 300 requests per minute despite the intended 100 request limit. Each server's bucket believes it's correctly enforcing the limit, but the **distributed consistency** requirement is violated because no single component has the complete picture of the client's activity across all servers.\n\n**State Synchronization Requirements**\n\nEffective distributed rate limiting requires that token bucket state remain consistent across all server instances, with updates visible immediately after they occur. This creates several technical requirements that don't exist in single-server deployments:\n\n**Atomic Read-Modify-Write Operations**: When a server needs to consume tokens, it must read the current count, verify sufficient tokens exist, and decrement the count as a single atomic operation. If these steps are separate, race conditions occur when multiple servers attempt simultaneous token consumption for the same client.\n\n**Immediate Consistency**: Unlike some distributed systems where eventual consistency is acceptable, rate limiting requires immediate consistency. If Server A allows a request that exhausts a client's tokens, Server B must immediately see the updated token count and deny subsequent requests. Delays in propagating state updates create windows where rate limits can be exceeded.\n\n**High-Frequency Updates**: Token buckets require frequent updates - both for token consumption (on every request) and token refill (based on elapsed time). The coordination mechanism must handle this high update frequency without introducing significant latency to request processing.\n\n**Cross-Server Clock Coordination**: Token refill calculations depend on elapsed time measurements. When multiple servers refill the same bucket, they must agree on timing to prevent tokens from being added multiple times or at incorrect rates due to clock skew between servers.\n\n**The Single Source of Truth Requirement**\n\nTraditional distributed systems often allow each node to maintain local state and synchronize periodically. Rate limiting systems cannot follow this pattern because they require a **single source of truth** for token counts that all servers consult before making allow/deny decisions.\n\nThis requirement eliminates several common distributed system patterns:\n\n- **Local caching with background sync**: Servers cannot cache token counts locally because stale cache entries lead to rate limit violations\n- **Peer-to-peer coordination**: Having servers communicate directly with each other creates complex consistency protocols that are difficult to implement correctly\n- **Eventually consistent storage**: Token bucket updates must be immediately visible to all servers, ruling out storage systems that prioritize availability over consistency\n\nThe single source of truth approach requires that all servers coordinate through a shared storage system that provides strong consistency guarantees and supports atomic operations on stored data.\n\n### Redis-Based Token Storage\n\nRedis serves as our centralized token storage system because it provides the atomic operations, immediate consistency, and high performance required for distributed rate limiting. The design of our Redis-based storage focuses on efficient key structures, atomic update operations, and proper data encoding for token bucket state.\n\n**Redis Key Design Strategy**\n\nOur Redis key structure must uniquely identify each client's token bucket while supporting efficient operations and avoiding key collisions. The key design follows a hierarchical pattern that embeds the client identification strategy and optional endpoint-specific limits:\n\nFor basic per-client rate limiting, keys follow the pattern `rate_limit:client:{client_id}`, where `client_id` contains the full client identifier including its type. For example, an IP-based client generates keys like `rate_limit:client:ip:192.168.1.100`, while API key clients generate keys like `rate_limit:client:api_key:abc123def456`.\n\nWhen supporting per-endpoint rate limiting, keys extend to include the endpoint identifier: `rate_limit:client:{client_id}:endpoint:{endpoint_hash}`. The endpoint hash is a consistent representation of the HTTP method and normalized path, such as `GET:/api/v1/users` becoming `rate_limit:client:ip:192.168.1.100:endpoint:GET_api_v1_users`.\n\nThis hierarchical key structure enables several operational benefits: Redis key pattern matching allows monitoring tools to query all buckets for a specific client or endpoint, key expiration can be set uniformly across related buckets, and the key namespace remains organized even with millions of active clients.\n\n**Token Bucket State Encoding**\n\nEach Redis key stores a hash data structure containing the complete token bucket state required for atomic operations. The hash contains these fields:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `tokens` | Float | Current token count in the bucket, may include fractional tokens for precise calculations |\n| `capacity` | Integer | Maximum token capacity for the bucket, determines burst limit |\n| `refill_rate` | Float | Tokens added per second, supports fractional rates for fine-grained control |\n| `last_refill` | Float | Unix timestamp of last token refill operation, used for elapsed time calculations |\n| `created_at` | Float | Unix timestamp when bucket was first created, useful for debugging and monitoring |\n| `version` | Integer | Version number incremented on each update, enables optimistic locking patterns |\n\nUsing Redis hash data structures instead of simple key-value pairs provides several advantages: all bucket fields can be read or updated atomically using `HMGET` and `HMSET` commands, individual fields can be updated without reading the entire bucket state, and the encoding is human-readable for debugging and operational monitoring.\n\n**Atomic Token Consumption Logic**\n\nThe core challenge in distributed token bucket implementation is ensuring that token consumption operations are atomic across multiple steps: reading current bucket state, calculating elapsed time for refill, adding refill tokens, checking if sufficient tokens exist for the request, and updating the bucket with the new token count.\n\nRedis Lua scripts solve this atomicity requirement by executing the entire token consumption logic as a single atomic operation on the Redis server. The Lua script receives the bucket key, requested token count, current timestamp, and bucket configuration as parameters. It performs all token bucket calculations within Redis, ensuring that other concurrent operations cannot interleave with the token consumption logic.\n\nThe atomic consumption algorithm implemented in Lua follows these steps within a single Redis transaction:\n\n1. Read the current bucket state using `redis.call('HMGET', key, 'tokens', 'last_refill', 'capacity', 'refill_rate')`\n2. Calculate elapsed time since last refill by comparing the provided current timestamp with the stored `last_refill` value\n3. Calculate tokens to add based on elapsed time and refill rate, capping the total at bucket capacity\n4. Update the token count by adding refill tokens to the current token count\n5. Check if the updated token count is sufficient for the requested token consumption\n6. If sufficient tokens exist, subtract the requested tokens and update the bucket state with the new token count and current timestamp\n7. Return a result indicating whether the consumption succeeded and the remaining token count\n\nThis atomic script ensures that two concurrent requests for the same client cannot both succeed if their combined token consumption would exceed the bucket capacity, regardless of which server instances process the requests.\n\n**Handling Redis Hash Field Initialization**\n\nWhen a client makes their first request, no bucket exists in Redis for their identifier. The atomic consumption script must handle bucket initialization while maintaining atomicity with the consumption operation. This requires careful handling of Redis hash field defaults when some or all bucket fields are missing.\n\nThe Lua script uses Redis's `HMGET` command behavior where missing hash fields return `nil` values. The script provides sensible defaults for missing fields: missing `tokens` defaults to the bucket's configured capacity (full bucket for new clients), missing `last_refill` defaults to the current timestamp, and missing configuration fields default to the global rate limit settings.\n\nThis initialization-on-first-use pattern ensures that new clients receive their full token allocation immediately, while avoiding the need for separate bucket creation operations that would complicate the atomic consumption logic.\n\n### Atomic Operations with Lua Scripts\n\nRedis Lua scripts provide the foundation for atomic token bucket operations in our distributed rate limiting system. By moving the entire token consumption and refill logic into server-side scripts, we eliminate race conditions that would occur if multiple Redis commands were executed separately from client applications.\n\n**The Atomicity Requirement**\n\nToken bucket operations require atomicity because they involve multiple interdependent steps that must appear to execute instantaneously from the perspective of concurrent requests. Consider two simultaneous requests from the same client, each requesting one token from a bucket containing exactly one token. Without atomicity, both requests might read the current token count as one, both determine that sufficient tokens exist, and both proceed to consume a token - resulting in two successful requests despite only one token being available.\n\nThe atomicity requirement extends beyond simple token consumption to include time-based token refill calculations. When multiple servers simultaneously process requests for the same client, they may all calculate refill tokens based on elapsed time. Without atomicity, each server might add refill tokens independently, causing the bucket to accumulate tokens faster than the configured refill rate.\n\n**Lua Script Architecture**\n\nOur Lua script architecture implements the complete token bucket algorithm within Redis, receiving all necessary parameters from client applications and returning structured results that indicate whether token consumption succeeded. The script design minimizes the number of Redis operations while maintaining clear separation between token refill logic and consumption logic.\n\nThe primary consumption script, `token_bucket_consume.lua`, accepts these parameters:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `KEYS[1]` | String | Redis key for the client's token bucket |\n| `ARGV[1]` | Integer | Number of tokens requested for consumption |\n| `ARGV[2]` | Float | Current timestamp for elapsed time calculations |\n| `ARGV[3]` | Integer | Maximum bucket capacity |\n| `ARGV[4]` | Float | Token refill rate per second |\n| `ARGV[5]` | Integer | Initial token count for new buckets |\n\nThe script returns a structured result as a Redis array containing the consumption result (`1` for allowed, `0` for denied), remaining token count after the operation, and seconds until the next token becomes available (useful for `Retry-After` headers).\n\n**Token Refill Logic in Lua**\n\nThe token refill calculation within the Lua script must handle several edge cases while maintaining precision and preventing token bucket overflow. The refill algorithm calculates tokens to add based on elapsed time, but must account for clock corrections, very large elapsed times, and floating-point precision limits.\n\nThe refill calculation follows this logic within the Lua script:\n\n```lua\n-- Read current bucket state, defaulting missing values\nlocal current_tokens = tonumber(bucket_state[1]) or capacity\nlocal last_refill = tonumber(bucket_state[2]) or current_timestamp\nlocal stored_capacity = tonumber(bucket_state[3]) or capacity\nlocal stored_refill_rate = tonumber(bucket_state[4]) or refill_rate\n\n-- Calculate elapsed time, handling clock corrections\nlocal elapsed_seconds = math.max(0, current_timestamp - last_refill)\n\n-- Calculate tokens to add, preventing overflow\nlocal tokens_to_add = elapsed_seconds * stored_refill_rate\nlocal updated_tokens = math.min(stored_capacity, current_tokens + tokens_to_add)\n```\n\nThe script handles several important edge cases: negative elapsed time (indicating clock corrections) by using `math.max(0, ...)` to prevent token removal, very large elapsed times by capping total tokens at bucket capacity using `math.min(...)`, and floating-point precision by performing all calculations in Lua's native number type.\n\n**Consumption Decision Logic**\n\nAfter calculating the updated token count including refill tokens, the script must decide whether to allow or deny the token consumption request. This decision logic must handle partial token consumption, ensure atomic updates, and provide sufficient information for client applications to implement proper retry behavior.\n\nThe consumption decision follows this algorithm:\n\n```lua\n-- Check if sufficient tokens exist\nif updated_tokens >= tokens_requested then\n    -- Consumption allowed - subtract tokens and update bucket\n    local final_tokens = updated_tokens - tokens_requested\n    \n    -- Update bucket state atomically\n    redis.call('HMSET', bucket_key,\n        'tokens', final_tokens,\n        'last_refill', current_timestamp,\n        'version', (tonumber(bucket_state[6]) or 0) + 1\n    )\n    \n    -- Return success result\n    return {1, math.floor(final_tokens), 0}\nelse\n    -- Consumption denied - update refill time but not token count\n    redis.call('HMSET', bucket_key,\n        'tokens', updated_tokens,\n        'last_refill', current_timestamp\n    )\n    \n    -- Calculate retry delay\n    local tokens_needed = tokens_requested - updated_tokens\n    local retry_after = tokens_needed / stored_refill_rate\n    \n    return {0, math.floor(updated_tokens), math.ceil(retry_after)}\nend\n```\n\nThis logic ensures that bucket state is updated even for denied requests (to record the refill timestamp), provides accurate remaining token counts for rate limiting headers, and calculates precise retry delays based on the token refill rate.\n\n**Script Error Handling and Validation**\n\nThe Lua script must validate input parameters and handle error conditions gracefully, since script errors cause the entire Redis operation to fail and may not provide clear error messages to client applications. Input validation within the script prevents common parameter errors while maintaining atomic operation semantics.\n\nParameter validation includes: ensuring `tokens_requested` is a positive integer, validating that `current_timestamp` is a reasonable Unix timestamp (not negative or excessively future), checking that `capacity` and `refill_rate` are positive numbers, and verifying that the Redis key format matches expected patterns.\n\nError handling within the script uses Lua's `assert()` function for critical errors that should abort the operation, and provides sensible defaults for missing or invalid non-critical parameters. This approach ensures that client applications receive clear error messages for invalid requests while allowing the script to handle minor parameter variations gracefully.\n\n### Redis Failure Handling\n\nRedis represents a single point of failure in our distributed rate limiting architecture, requiring careful design of failure detection, graceful degradation, and recovery mechanisms to maintain service availability when Redis becomes unavailable or experiences performance issues.\n\n**Failure Modes and Detection**\n\nRedis failures manifest in several ways that require different detection and response strategies. **Connection failures** occur when Redis servers become unreachable due to network issues, server crashes, or configuration changes. These failures are typically detected immediately when Redis client libraries encounter connection timeouts or connection refused errors.\n\n**Performance degradation** presents a more subtle failure mode where Redis responds to requests but with significantly increased latency. This can occur due to memory pressure, CPU saturation, or network congestion. Performance degradation requires threshold-based detection since Redis continues to function but may not meet the latency requirements for real-time rate limiting decisions.\n\n**Partial failures** occur when Redis remains available for some operations but fails for others, such as when specific Lua scripts encounter errors or when Redis runs out of memory for new keys while serving reads for existing keys. These failures require operation-level error handling rather than connection-level failover.\n\n**Data inconsistency** can occur during Redis failover scenarios where slave instances may not have received the latest updates before being promoted to master. This failure mode requires careful consideration of consistency versus availability trade-offs.\n\n**Local Fallback Strategies**\n\nWhen Redis becomes unavailable, our rate limiter must choose between failing open (allowing all requests) or failing closed (denying all requests). Neither option is ideal: failing open potentially allows abuse during outages, while failing closed creates service availability issues due to rate limiting infrastructure problems.\n\nOur recommended approach implements **local fallback buckets** that provide approximate rate limiting during Redis outages. Each server instance maintains in-memory token buckets for recently seen clients, but with more conservative limits to account for the lack of distributed coordination.\n\nThe local fallback strategy operates according to these principles:\n\n**Conservative Limit Scaling**: Local fallback buckets use a fraction of the configured rate limit, typically 50-70%, to account for the possibility that the same client is making requests to multiple server instances during the outage. If a client has a normal limit of 100 requests per minute, the local fallback might allow 60 requests per minute per server.\n\n**Limited Client Memory**: To prevent memory exhaustion during extended outages, local fallback buckets are limited to a maximum number of clients per server instance (typically 10,000-50,000) with LRU eviction when the limit is exceeded.\n\n**Automatic Recovery**: When Redis connectivity is restored, local fallback buckets are gradually phased out in favor of Redis-backed buckets. The transition includes a brief period where both systems operate in parallel to ensure smooth failover without allowing clients to double their effective limits during the transition.\n\n**Circuit Breaker Pattern Implementation**\n\nTo prevent Redis failures from cascading into application performance issues, our rate limiter implements a circuit breaker pattern that detects Redis problems and automatically switches to local fallback mode.\n\nThe circuit breaker maintains three states: **Closed** (normal operation with Redis), **Open** (Redis is failing, use local fallback), and **Half-Open** (testing whether Redis has recovered). State transitions are based on success/failure rates and response times for Redis operations.\n\n| Circuit State | Behavior | Transition Conditions |\n|---------------|----------|----------------------|\n| Closed | All requests use Redis for token bucket operations | Transitions to Open after 5 consecutive Redis failures or 50% failure rate over 30 seconds |\n| Open | All requests use local fallback buckets | Transitions to Half-Open after 60-second timeout |\n| Half-Open | Test requests use Redis, others use local fallback | Transitions to Closed after 3 consecutive Redis successes, or back to Open on any failure |\n\nThe circuit breaker tracks Redis operation metrics including response times, error rates, and timeout frequencies. Gradual degradation triggers circuit opening before complete Redis failure, allowing the system to switch to local fallback before users experience request timeouts.\n\n**Connection Pool Management**\n\nRedis connection failures often stem from connection pool exhaustion or configuration issues rather than Redis server problems. Our failure handling strategy includes intelligent connection pool management that distinguishes between Redis server issues and client-side connection problems.\n\nConnection pool management includes: **Adaptive pool sizing** that increases connection pool size during high load periods and decreases it during quiet periods, **Connection health checks** that proactively detect stale connections and replace them before they cause request failures, **Exponential backoff reconnection** that prevents connection storms when Redis becomes available after an outage, and **Connection distribution** across multiple Redis instances when using Redis Cluster or master-slave setups.\n\nThe connection pool monitors these metrics to detect impending failures: connection wait times (indicating pool exhaustion), connection establishment times (indicating network issues), and connection lifetime statistics (indicating connection stability issues).\n\n**Recovery and State Synchronization**\n\nWhen Redis connectivity is restored after an outage, our rate limiter must carefully transition from local fallback mode back to distributed mode while maintaining rate limiting accuracy and avoiding thundering herd problems.\n\nThe recovery process follows these steps:\n\n1. **Gradual reconnection**: Server instances reconnect to Redis using exponential backoff to prevent overwhelming a recovering Redis instance\n2. **State validation**: Compare local fallback bucket states with any available Redis state to detect inconsistencies\n3. **Conservative merging**: When both local and Redis state exist for the same client, use the more restrictive token count to prevent accidental rate limit violations\n4. **Monitoring period**: Operate in a hybrid mode where new clients use Redis while existing clients gradually transition from local fallback\n5. **Full transition**: Complete the switch to Redis-only operation after confirming stable performance\n\nThis recovery process typically takes 2-5 minutes to complete, ensuring that Redis has fully recovered and can handle the production load before local fallback systems are disabled.\n\n### Clock Synchronization Considerations\n\nDistributed rate limiting systems are particularly sensitive to clock synchronization issues because token refill calculations depend on accurate time measurements across multiple server instances. Clock drift, leap seconds, and system clock corrections can all impact the accuracy of rate limiting decisions in subtle but important ways.\n\n**The Time Dependency Problem**\n\nToken bucket algorithms calculate refill tokens based on elapsed time since the last refill operation. In a single-server system, this calculation uses a single system clock and remains consistent. However, in distributed systems where multiple servers may update the same Redis-stored bucket, time measurements from different servers can introduce inconsistencies.\n\nConsider a scenario where Server A has a system clock that runs 30 seconds fast compared to Server B. If Server A processes a request and updates a bucket's `last_refill` timestamp using its local clock, Server B will later calculate elapsed time using the difference between its local clock and Server A's timestamp. This 30-second clock drift means Server B will calculate 30 seconds less elapsed time than actually occurred, resulting in fewer refill tokens being added to the bucket.\n\n**Clock Synchronization Strategies**\n\nNetwork Time Protocol (NTP) provides the standard solution for maintaining synchronized clocks across distributed systems. However, NTP synchronization is not perfect and typically maintains accuracy within 1-50 milliseconds under normal conditions, with occasional larger corrections when significant drift is detected.\n\nFor rate limiting systems, clock synchronization requirements depend on the rate limiting time scales. Systems with minute-level rate limits (100 requests per minute) can tolerate several seconds of clock drift without significant impact. However, systems with second-level rate limits (10 requests per second) require much tighter clock synchronization to maintain accuracy.\n\nOur recommended clock synchronization strategy includes:\n\n**Mandatory NTP Configuration**: All server instances must run NTP clients configured to synchronize with reliable time sources. Multiple time sources should be configured to handle individual time server failures.\n\n**Clock Drift Monitoring**: Application monitoring should track clock drift between server instances by comparing timestamps in shared Redis operations. Drift exceeding configurable thresholds (typically 5-10 seconds) should trigger alerts.\n\n**Gradual Clock Correction**: When NTP makes clock corrections, the changes should be gradual rather than sudden jumps when possible. Large clock corrections can cause token bucket calculations to become temporarily inaccurate.\n\n**Time Source Redundancy**: Critical deployments should use multiple independent time sources and detect when individual sources provide inconsistent time information.\n\n**Timestamp Validation in Lua Scripts**\n\nOur Redis Lua scripts can implement timestamp validation to detect and handle obvious clock synchronization issues. The validation logic compares incoming timestamps with Redis's internal clock and with previously stored timestamps to identify potential clock problems.\n\nThe Lua script timestamp validation includes these checks:\n\n**Future Timestamp Detection**: If an incoming timestamp is significantly ahead of Redis's internal time (using the `TIME` command), the script can limit the timestamp to prevent excessive token refill calculations.\n\n**Backwards Time Detection**: If an incoming timestamp is earlier than the stored `last_refill` timestamp, indicating clock corrections or clock drift, the script can handle this gracefully by using the stored timestamp rather than calculating negative elapsed time.\n\n**Reasonable Bounds Checking**: Incoming timestamps should fall within reasonable bounds (not too far in the past or future) to prevent calculation errors caused by corrupted timestamps or client clock issues.\n\nHere's how the timestamp validation logic works within the Lua script:\n\n```lua\n-- Get Redis server time for validation\nlocal redis_time = redis.call('TIME')\nlocal redis_timestamp = tonumber(redis_time[1]) + (tonumber(redis_time[2]) / 1000000)\n\n-- Validate incoming timestamp against reasonable bounds\nlocal max_future_seconds = 300  -- 5 minutes\nlocal max_past_seconds = 3600   -- 1 hour\n\nif current_timestamp > (redis_timestamp + max_future_seconds) then\n    -- Timestamp too far in future, use Redis time\n    current_timestamp = redis_timestamp\nelseif current_timestamp < (redis_timestamp - max_past_seconds) then\n    -- Timestamp too far in past, use Redis time\n    current_timestamp = redis_timestamp\nend\n\n-- Handle backwards time (clock correction)\nif current_timestamp < last_refill then\n    -- Use stored timestamp to prevent negative elapsed time\n    current_timestamp = last_refill\nend\n```\n\nThis validation prevents clock issues from causing token bucket calculation errors while maintaining the atomicity of Redis operations.\n\n**Handling Clock Corrections and Leap Seconds**\n\nSystem clock corrections, including leap seconds and NTP adjustments, can cause temporary inconsistencies in token bucket calculations. Large forward clock corrections can cause excessive token refill, while backward corrections can temporarily prevent token refill.\n\nOur handling strategy for clock corrections includes:\n\n**Maximum Elapsed Time Limits**: Token refill calculations use a maximum elapsed time cap (typically 5-10 minutes) to prevent excessive token accumulation when clocks jump forward significantly.\n\n**Minimum Refill Intervals**: Buckets maintain minimum intervals between refill operations to prevent rapid-fire refill calculations during clock instability.\n\n**Leap Second Handling**: During leap second events, token refill calculations may experience one-second discrepancies. This is typically acceptable for rate limiting systems operating at minute-level granularities.\n\n**Clock Correction Logging**: System logs capture significant clock corrections to aid in troubleshooting rate limiting anomalies that may correlate with time synchronization events.\n\nThe combination of these strategies ensures that normal clock synchronization variations do not significantly impact rate limiting accuracy, while providing graceful handling of larger clock correction events.\n\n### Architecture Decision Records\n\n> **Decision: Redis vs Other Distributed Storage Options**\n> - **Context**: Our distributed rate limiting system requires a shared storage layer that supports atomic operations, high throughput, and low latency for token bucket state management across multiple server instances.\n> - **Options Considered**: Redis with Lua scripts, Apache Cassandra with lightweight transactions, PostgreSQL with advisory locks, etcd with atomic transactions\n> - **Decision**: Redis with Lua scripts for atomic token bucket operations\n> - **Rationale**: Redis provides the optimal combination of atomic operations (via Lua scripts), microsecond-level latency, high throughput capacity, and simple operational requirements. Lua scripts enable complex token bucket logic to execute atomically on the server side, eliminating race conditions inherent in multi-step operations. Redis's single-threaded event loop ensures consistent performance characteristics, and its simple key-value model aligns well with token bucket storage requirements.\n> - **Consequences**: This choice provides excellent performance and consistency but introduces Redis as a critical dependency and single point of failure. Redis's memory-only storage model requires careful capacity planning, and Lua script complexity increases compared to simple key-value operations.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Redis + Lua | Atomic operations, <1ms latency, simple ops | Single point of failure, memory limits | ✓ Yes |\n| Cassandra + LWT | Distributed, high availability | Complex setup, higher latency | No |\n| PostgreSQL + locks | ACID guarantees, familiar SQL | Much higher latency, complex locking | No |\n| etcd + transactions | Strong consistency, distributed | Limited throughput, complex API | No |\n\n> **Decision: Lua Script Complexity vs Multiple Redis Operations**\n> - **Context**: Token bucket operations require reading current state, calculating refill tokens, checking availability, and updating state. This can be implemented as either a single Lua script or multiple separate Redis commands coordinated by the application.\n> - **Options Considered**: Single comprehensive Lua script, multiple Redis commands with application-side coordination, Redis transactions (MULTI/EXEC) with application logic\n> - **Decision**: Single comprehensive Lua script containing all token bucket logic\n> - **Rationale**: Atomic execution is critical for rate limiting correctness - any approach that allows other operations to interleave with token consumption creates race conditions where limits can be exceeded. A comprehensive Lua script ensures that the entire token bucket operation (refill calculation, availability check, consumption, state update) executes atomically on the Redis server without possibility of interference from concurrent operations.\n> - **Consequences**: This approach guarantees correctness and eliminates race conditions but increases complexity of the Lua script and makes debugging more challenging. Script errors affect the entire operation, and script development requires Redis-specific knowledge.\n\n> **Decision: Local Fallback vs Fail-Closed During Redis Outages**\n> - **Context**: When Redis becomes unavailable, our rate limiter must choose between allowing all requests (fail-open), denying all requests (fail-closed), or implementing local fallback buckets with reduced accuracy.\n> - **Options Considered**: Fail-open (allow all requests), fail-closed (deny all requests), local fallback with conservative limits, hybrid approach with request prioritization\n> - **Decision**: Local fallback with conservative limits (60% of normal rate limits per server instance)\n> - **Rationale**: Fail-open creates unacceptable risk of abuse during Redis outages, potentially causing service degradation or cost overruns. Fail-closed creates availability issues where rate limiting infrastructure problems affect core service availability. Local fallback provides reasonable protection against abuse while maintaining service availability, using conservative limits to account for lack of cross-server coordination.\n> - **Consequences**: This approach maintains service availability during Redis outages but provides less precise rate limiting during fallback periods. It requires additional memory for local buckets and complex logic for transitioning between Redis and local modes.\n\n> **Decision: Circuit Breaker Pattern vs Simple Retry Logic**\n> - **Context**: Redis failures can manifest as complete unavailability, performance degradation, or intermittent errors. The system needs to detect these conditions and switch to fallback mode appropriately.\n> - **Options Considered**: Simple retry with exponential backoff, circuit breaker pattern with multiple states, health check-based switching, timeout-based failover\n> - **Decision**: Circuit breaker pattern with Closed/Open/Half-Open states\n> - **Rationale**: Simple retry logic doesn't prevent cascading failures when Redis is experiencing performance issues rather than complete failure. Circuit breaker pattern provides graduated response to different failure modes - quickly switching to fallback for complete failures, gradually backing off during performance degradation, and automatically testing for recovery without overwhelming a struggling Redis instance.\n> - **Consequences**: Circuit breaker provides more sophisticated failure handling but adds complexity in state management and threshold tuning. It requires careful configuration of failure detection thresholds and recovery timing.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Non-Atomic Token Consumption Operations**\n\nA critical mistake in distributed rate limiting implementation is performing token bucket operations as multiple separate Redis commands rather than a single atomic Lua script. Developers often implement token consumption by first reading the current bucket state with `HMGET`, performing calculations in application code, then updating the bucket with `HMSET`. This creates a race condition window where multiple concurrent requests can read the same token count, all determine that sufficient tokens exist, and all proceed to consume tokens simultaneously.\n\nFor example, if two requests arrive simultaneously for a client with exactly one token remaining, both requests might execute `HMGET` and receive a token count of 1, both calculate that they can consume one token, and both execute `HMSET` to update the bucket - resulting in two successful requests despite only one token being available.\n\n**Fix**: Implement all token bucket logic within a single Redis Lua script that executes atomically. The script should read current state, calculate refill tokens, check availability, and update state as one indivisible operation that cannot be interrupted by other operations.\n\n⚠️ **Pitfall: Clock Drift Causing Token Calculation Errors**\n\nWhen multiple server instances update the same Redis-stored token bucket, differences in system clocks can cause significant errors in token refill calculations. If Server A has a clock running 60 seconds fast and updates a bucket's `last_refill` timestamp, Server B will later calculate elapsed time based on the difference between its local clock and Server A's timestamp. This clock skew results in Server B calculating 60 seconds less elapsed time than actually occurred, causing tokens to refill more slowly than configured.\n\nThe impact compounds over time as different servers with different clock skews update the same buckets, creating unpredictable and inconsistent rate limiting behavior that's difficult to debug.\n\n**Fix**: Ensure all server instances run NTP clients for clock synchronization, implement timestamp validation in Lua scripts that compare incoming timestamps against Redis server time, and add maximum elapsed time caps to prevent excessive token accumulation when clocks jump forward.\n\n⚠️ **Pitfall: Missing Redis Connection Pool Configuration**\n\nDefault Redis client configurations often use minimal connection pools that become bottlenecks under production load. Each token consumption operation requires a Redis round-trip, so inadequate connection pool sizing creates request queuing that adds latency to every API request and can cause timeouts during traffic spikes.\n\nAdditionally, many developers don't configure connection health checks, leading to scenarios where stale connections remain in the pool and cause intermittent failures that are difficult to diagnose.\n\n**Fix**: Configure Redis connection pools with sufficient size for peak load (typically 10-50 connections per server instance), enable connection health checks with reasonable timeout values, and implement connection pool monitoring to track utilization and detect pool exhaustion before it affects requests.\n\n⚠️ **Pitfall: Inadequate Lua Script Error Handling**\n\nRedis Lua scripts that don't validate input parameters or handle error conditions gracefully can cause the entire rate limiting operation to fail with unclear error messages. Common issues include scripts that assume all hash fields exist (causing nil value errors), scripts that don't validate timestamp formats (causing type conversion errors), and scripts that don't handle Redis memory limits (causing out-of-memory errors during bucket creation).\n\nWhen Lua scripts fail, they often return generic error messages that don't clearly indicate whether the failure was due to invalid input, Redis server issues, or script logic errors, making troubleshooting difficult.\n\n**Fix**: Implement comprehensive input validation in Lua scripts using `assert()` for critical errors and sensible defaults for missing values. Add error context to script responses that help identify the specific failure cause, and test scripts with invalid inputs to ensure they fail gracefully.\n\n⚠️ **Pitfall: Local Fallback Memory Leaks During Extended Outages**\n\nWhen implementing local fallback buckets for Redis outages, developers often create unlimited in-memory bucket storage without considering memory consumption during extended outages. If a Redis outage lasts several hours and the application continues serving diverse clients, local fallback buckets can accumulate indefinitely and consume all available server memory.\n\nThis problem is particularly severe in environments with many unique client identifiers (such as IP-based rate limiting) where each new client creates a new in-memory bucket that persists until the outage ends.\n\n**Fix**: Implement strict limits on local fallback bucket storage (typically 10,000-50,000 buckets per server), use LRU eviction when limits are exceeded, and add memory usage monitoring for local fallback systems. Consider using more conservative rate limits during fallback to account for reduced accuracy when buckets are evicted.\n\n⚠️ **Pitfall: Improper Redis Failover During Circuit Breaker Transitions**\n\nWhen implementing circuit breaker patterns for Redis failure handling, developers often create thundering herd problems during recovery by having all server instances simultaneously test Redis availability when transitioning from Open to Half-Open state. This can overwhelm a recovering Redis instance and cause it to fail again immediately.\n\nAdditionally, improper timing of circuit breaker state transitions can cause rapid oscillation between Redis and local fallback modes, creating inconsistent rate limiting behavior and confusing operational monitoring.\n\n**Fix**: Implement jittered timing for circuit breaker state transitions so that server instances don't all test Redis recovery simultaneously. Use gradual recovery approaches where only a small percentage of requests test Redis availability during Half-Open state, and require sustained success over multiple seconds before fully reopening the circuit.\n\n### Implementation Guidance\n\nThis implementation bridges our Redis-based distributed architecture with production-ready Python code that handles atomic operations, failure scenarios, and recovery mechanisms.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Redis Client | redis-py with connection pooling | redis-py-cluster for Redis Cluster |\n| Connection Management | Single Redis instance with retry logic | Redis Sentinel for high availability |\n| Time Synchronization | NTP client with monitoring | Chrony with multiple time sources |\n| Circuit Breaker | Simple failure count thresholds | pybreaker library with metrics |\n| Monitoring | Basic Redis metrics logging | Prometheus metrics with alerting |\n| Configuration | Environment variables | Consul/etcd for dynamic config |\n\n**Redis Client Setup and Configuration**\n\n```python\nimport redis\nimport redis.sentinel\nimport logging\nfrom typing import Optional, Dict, Any\nimport time\nimport json\n\nclass RedisConnectionManager:\n    \"\"\"Manages Redis connections with failover and circuit breaker logic.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.connection_pool: Optional[redis.ConnectionPool] = None\n        self.sentinel: Optional[redis.sentinel.Sentinel] = None\n        self.circuit_breaker = CircuitBreaker()\n        self._setup_connection()\n    \n    def _setup_connection(self):\n        \"\"\"Initialize Redis connection with appropriate configuration.\"\"\"\n        # TODO 1: Parse Redis URL from config and determine if using Sentinel\n        # TODO 2: Create connection pool with proper sizing (max_connections=50)\n        # TODO 3: Configure connection timeouts (socket_connect_timeout=5.0)\n        # TODO 4: Set up connection health check parameters\n        # TODO 5: Initialize Sentinel if using high availability setup\n        pass\n    \n    def get_redis_client(self) -> redis.Redis:\n        \"\"\"Get Redis client with circuit breaker protection.\"\"\"\n        # TODO 1: Check circuit breaker state before creating client\n        # TODO 2: Return client from connection pool\n        # TODO 3: Handle circuit breaker Open state by raising exception\n        pass\n    \n    def execute_with_fallback(self, operation, *args, **kwargs):\n        \"\"\"Execute Redis operation with automatic fallback on failure.\"\"\"\n        # TODO 1: Attempt operation with circuit breaker monitoring\n        # TODO 2: Record success/failure for circuit breaker state\n        # TODO 3: On failure, trigger local fallback if configured\n        # TODO 4: Return result or raise appropriate exception\n        pass\n\nclass CircuitBreaker:\n    \"\"\"Simple circuit breaker for Redis operations.\"\"\"\n    \n    def __init__(self, failure_threshold=5, recovery_timeout=60):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.failure_count = 0\n        self.last_failure_time = 0\n        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN\n    \n    def call(self, func, *args, **kwargs):\n        \"\"\"Execute function with circuit breaker protection.\"\"\"\n        # TODO 1: Check current state and handle OPEN state\n        # TODO 2: Execute function and handle success/failure\n        # TODO 3: Update failure count and state transitions\n        # TODO 4: Return result or raise CircuitBreakerError\n        pass\n```\n\n**Atomic Lua Scripts for Token Operations**\n\n```python\n# Redis Lua script for atomic token bucket consumption\nTOKEN_BUCKET_CONSUME_SCRIPT = \"\"\"\nlocal bucket_key = KEYS[1]\nlocal tokens_requested = tonumber(ARGV[1])\nlocal current_timestamp = tonumber(ARGV[2])\nlocal capacity = tonumber(ARGV[3])\nlocal refill_rate = tonumber(ARGV[4])\nlocal initial_tokens = tonumber(ARGV[5])\n\n-- Read current bucket state\nlocal bucket_state = redis.call('HMGET', bucket_key, 'tokens', 'last_refill', 'capacity', 'refill_rate')\n\n-- Handle new bucket creation with defaults\nlocal current_tokens = tonumber(bucket_state[1]) or initial_tokens or capacity\nlocal last_refill = tonumber(bucket_state[2]) or current_timestamp\nlocal stored_capacity = tonumber(bucket_state[3]) or capacity\nlocal stored_refill_rate = tonumber(bucket_state[4]) or refill_rate\n\n-- Validate timestamp to prevent clock issues\nlocal redis_time = redis.call('TIME')\nlocal redis_timestamp = tonumber(redis_time[1]) + (tonumber(redis_time[2]) / 1000000)\nlocal max_future_seconds = 300\nlocal max_past_seconds = 3600\n\nif current_timestamp > (redis_timestamp + max_future_seconds) then\n    current_timestamp = redis_timestamp\nelseif current_timestamp < (redis_timestamp - max_past_seconds) then\n    current_timestamp = redis_timestamp\nend\n\n-- Handle backwards time (clock correction)\nif current_timestamp < last_refill then\n    current_timestamp = last_refill\nend\n\n-- Calculate elapsed time and refill tokens\nlocal elapsed_seconds = math.max(0, current_timestamp - last_refill)\nlocal tokens_to_add = elapsed_seconds * stored_refill_rate\nlocal updated_tokens = math.min(stored_capacity, current_tokens + tokens_to_add)\n\n-- Make consumption decision\nif updated_tokens >= tokens_requested then\n    -- Consumption allowed\n    local final_tokens = updated_tokens - tokens_requested\n    redis.call('HMSET', bucket_key,\n        'tokens', final_tokens,\n        'last_refill', current_timestamp,\n        'capacity', stored_capacity,\n        'refill_rate', stored_refill_rate\n    )\n    -- Set expiration to clean up unused buckets (24 hours)\n    redis.call('EXPIRE', bucket_key, 86400)\n    return {1, math.floor(final_tokens), 0}\nelse\n    -- Consumption denied\n    redis.call('HMSET', bucket_key,\n        'tokens', updated_tokens,\n        'last_refill', current_timestamp,\n        'capacity', stored_capacity,\n        'refill_rate', stored_refill_rate\n    )\n    redis.call('EXPIRE', bucket_key, 86400)\n    local tokens_needed = tokens_requested - updated_tokens\n    local retry_after = math.ceil(tokens_needed / stored_refill_rate)\n    return {0, math.floor(updated_tokens), retry_after}\nend\n\"\"\"\n\nclass DistributedTokenBucket:\n    \"\"\"Redis-backed token bucket with atomic operations.\"\"\"\n    \n    def __init__(self, redis_manager: RedisConnectionManager, config: TokenBucketConfig):\n        self.redis_manager = redis_manager\n        self.config = config\n        self.consume_script = None\n        self._register_scripts()\n    \n    def _register_scripts(self):\n        \"\"\"Register Lua scripts with Redis.\"\"\"\n        # TODO 1: Get Redis client from connection manager\n        # TODO 2: Register TOKEN_BUCKET_CONSUME_SCRIPT using script_load()\n        # TODO 3: Store script SHA for efficient execution\n        # TODO 4: Handle script registration failures gracefully\n        pass\n    \n    def try_consume(self, client_id: str, tokens_requested: int = 1) -> TokenConsumptionResult:\n        \"\"\"Attempt to consume tokens using atomic Redis operation.\"\"\"\n        # TODO 1: Generate Redis key for client bucket\n        # TODO 2: Get current timestamp for refill calculations\n        # TODO 3: Execute Lua script with bucket parameters\n        # TODO 4: Parse script result into TokenConsumptionResult\n        # TODO 5: Handle Redis errors and trigger fallback if needed\n        pass\n    \n    def get_bucket_status(self, client_id: str) -> Dict[str, Any]:\n        \"\"\"Get current bucket status without consuming tokens.\"\"\"\n        # TODO 1: Read bucket state using HMGET\n        # TODO 2: Calculate current tokens including refill\n        # TODO 3: Return structured status information\n        pass\n\nclass LocalFallbackBucket:\n    \"\"\"In-memory token bucket for Redis fallback scenarios.\"\"\"\n    \n    def __init__(self, config: TokenBucketConfig, max_clients: int = 10000):\n        self.config = config\n        self.max_clients = max_clients\n        self.buckets: Dict[str, BucketInfo] = {}\n        self.access_order: List[str] = []  # For LRU eviction\n        self.lock = threading.RLock()\n    \n    def try_consume(self, client_id: str, tokens_requested: int = 1) -> TokenConsumptionResult:\n        \"\"\"Consume tokens from local fallback bucket.\"\"\"\n        with self.lock:\n            # TODO 1: Get or create bucket for client_id\n            # TODO 2: Perform LRU eviction if max_clients exceeded\n            # TODO 3: Calculate token refill based on elapsed time\n            # TODO 4: Make consumption decision and update bucket\n            # TODO 5: Update access order for LRU tracking\n            pass\n    \n    def _evict_lru_buckets(self):\n        \"\"\"Remove least recently used buckets when over capacity.\"\"\"\n        # TODO 1: Calculate number of buckets to evict\n        # TODO 2: Remove oldest buckets from both buckets dict and access_order\n        # TODO 3: Log eviction statistics for monitoring\n        pass\n\nclass DistributedRateLimiter:\n    \"\"\"Main distributed rate limiter with Redis and local fallback.\"\"\"\n    \n    def __init__(self, config: RateLimitConfig):\n        self.config = config\n        self.redis_manager = RedisConnectionManager(config.redis_config)\n        self.distributed_buckets = {}  # client_id -> DistributedTokenBucket\n        self.local_fallback = LocalFallbackBucket(config.default_limits)\n        self.is_redis_available = True\n    \n    def process_request(self, client_id: str, endpoint: Optional[str] = None, \n                       tokens_requested: int = 1) -> TokenConsumptionResult:\n        \"\"\"Process rate limiting request with automatic fallback.\"\"\"\n        # TODO 1: Resolve effective rate limit config for client/endpoint\n        # TODO 2: Attempt Redis-based consumption if available\n        # TODO 3: Fall back to local buckets on Redis failure\n        # TODO 4: Update Redis availability status based on operation result\n        # TODO 5: Return consumption result with appropriate retry timing\n        pass\n    \n    def _get_distributed_bucket(self, client_id: str, config: TokenBucketConfig) -> DistributedTokenBucket:\n        \"\"\"Get or create distributed token bucket for client.\"\"\"\n        # TODO 1: Check if bucket already exists in cache\n        # TODO 2: Create new DistributedTokenBucket with client config\n        # TODO 3: Cache bucket instance for reuse\n        pass\n    \n    def _handle_redis_failure(self, error: Exception):\n        \"\"\"Handle Redis operation failures and update circuit breaker.\"\"\"\n        # TODO 1: Log Redis failure details for troubleshooting\n        # TODO 2: Update circuit breaker state\n        # TODO 3: Set redis availability flag for fallback decision\n        # TODO 4: Schedule Redis recovery testing if appropriate\n        pass\n```\n\n**Configuration and Environment Integration**\n\n```python\nimport os\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional\n\n@dataclass\nclass RedisConfig:\n    \"\"\"Redis connection configuration.\"\"\"\n    url: str\n    max_connections: int = 50\n    socket_timeout: float = 5.0\n    socket_connect_timeout: float = 5.0\n    retry_on_timeout: bool = True\n    health_check_interval: int = 30\n\n@dataclass\nclass DistributedRateLimitConfig(RateLimitConfig):\n    \"\"\"Extended configuration for distributed rate limiting.\"\"\"\n    redis_config: RedisConfig\n    circuit_breaker_failure_threshold: int = 5\n    circuit_breaker_recovery_timeout: int = 60\n    local_fallback_enabled: bool = True\n    local_fallback_max_clients: int = 10000\n    local_fallback_rate_multiplier: float = 0.6  # 60% of normal limits\n\ndef load_distributed_config() -> DistributedRateLimitConfig:\n    \"\"\"Load distributed rate limiting configuration from environment.\"\"\"\n    # TODO 1: Load base RateLimitConfig from environment\n    # TODO 2: Parse REDIS_URL and connection parameters\n    # TODO 3: Configure circuit breaker thresholds\n    # TODO 4: Set up local fallback parameters\n    # TODO 5: Validate configuration and provide sensible defaults\n    pass\n```\n\n**Milestone Checkpoint**\n\nAfter implementing distributed rate limiting, verify the system works correctly across multiple server instances:\n\n1. **Start Redis server**: `redis-server --port 6379`\n2. **Start multiple application instances**: Run your rate limiter application on different ports (8000, 8001, 8002)\n3. **Test cross-instance coordination**: Send requests for the same client to different server instances and verify that the combined rate across all servers respects the configured limit\n4. **Verify Redis fallback**: Stop Redis server and confirm that requests continue to be processed with local fallback buckets using more conservative limits\n5. **Test Redis recovery**: Restart Redis and verify that the system gradually transitions back to distributed mode without allowing clients to exceed their limits during the transition\n\nExpected behavior: A client with a 60 requests/minute limit should be denied after making 60 requests total across all server instances, regardless of which specific servers handle the requests. During Redis outages, the same client should be limited to approximately 36 requests/minute per server instance (60% of normal limit).\n\nSigns of problems and debugging steps:\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Clients exceed rate limits | Non-atomic Redis operations | Check Redis MONITOR output for multiple commands per request | Implement Lua scripts for atomicity |\n| Inconsistent rate limiting | Clock drift between servers | Compare server timestamps in Redis bucket data | Configure NTP synchronization |\n| High request latency | Redis connection pool exhaustion | Monitor Redis client connection metrics | Increase connection pool size |\n| Rate limiter fails completely | Redis connection failure without fallback | Check Redis connectivity and fallback configuration | Implement local fallback buckets |\n| Memory usage grows during outages | Local fallback bucket leaks | Monitor local bucket count and memory usage | Add LRU eviction to local fallbacks |\n\n![Distributed Rate Limiting Sequence](./diagrams/distributed-sequence.svg)\n\n![Error Handling and Recovery Flow](./diagrams/error-handling-flow.svg)\n\n\n## Component Interactions and Data Flow\n\n> **Milestone(s):** All milestones - this section shows how components work together, evolving from basic token bucket interactions through distributed Redis coordination\n\n### Mental Model: The Orchestra Performance\n\nThink of our rate limiter as a symphony orchestra performing a complex musical piece. The **HTTP middleware** acts as the concert hall entrance, greeting each audience member (HTTP request) and checking their ticket validity. The **client tracker** serves as the seating coordinator, maintaining a chart of who sits where and managing the flow of patrons to their designated sections. The **token bucket** functions as the conductor's metronome, precisely timing when each musical phrase (request) can proceed. Finally, the **Redis storage layer** operates like the orchestra's sheet music stand - a centralized, authoritative source that ensures all musicians (server instances) stay perfectly synchronized, even when individual performers might miss a beat.\n\nJust as an orchestra's beautiful performance depends on precise timing, clear communication, and coordinated action between all participants, our rate limiter's effectiveness relies on seamless interactions between its components. Each component has a specific role, but the magic happens in how they communicate, pass information, and coordinate their actions to create a unified rate limiting experience.\n\n![Rate Limiter System Components](./diagrams/system-components.svg)\n\nThe complexity of component interactions varies dramatically across our milestones. In Milestone 1, we have a simple conversation between the token bucket algorithm and basic HTTP handling. By Milestone 4, we orchestrate a distributed dance involving Redis Lua scripts, circuit breakers, fallback mechanisms, and cross-server coordination. Understanding this progression helps us appreciate why seemingly simple rate limiting becomes architecturally sophisticated at scale.\n\n## Request Processing Sequence\n\n### Single Server Processing Flow\n\n![HTTP Request Processing Flow](./diagrams/request-processing-flow.svg)\n\nThe journey of an HTTP request through our rate limiting system follows a carefully choreographed sequence designed to minimize latency while ensuring accurate rate limit enforcement. This sequence represents the core interaction pattern that all components must support, regardless of whether we're operating in single-server or distributed mode.\n\nThe processing begins when an HTTP request arrives at our web server. The **middleware layer** immediately intercepts this request before it reaches any business logic handlers. This early interception is crucial because we want to reject excessive requests as quickly as possible, preserving system resources for legitimate traffic.\n\n**Phase 1: Request Preprocessing and Client Identification**\n\n| Step | Component | Action Taken | Data Produced |\n|------|-----------|--------------|---------------|\n| 1 | HTTP Middleware | Extract request metadata (headers, IP, path, method) | Raw request context dictionary |\n| 2 | HTTP Middleware | Check for rate limiting bypass headers or whitelisted paths | Boolean skip flag |\n| 3 | HTTP Middleware | Normalize endpoint path for consistent rate limit grouping | Canonical endpoint identifier |\n| 4 | Client Tracker | Apply client identification strategy (IP, API key, custom header) | `ClientIdentifier` object |\n| 5 | Client Tracker | Validate and sanitize client identifier format | Validated client ID string |\n\nDuring client identification, our system employs a **hierarchical resolution strategy** to determine the most specific rate limit configuration. The middleware first checks for endpoint-specific overrides, then client-specific overrides, and finally falls back to default global limits. This resolution happens before any token bucket operations, ensuring we apply the correct limits from the start.\n\nThe client identification process deserves special attention because it directly impacts both security and functionality. Our system supports multiple identification strategies simultaneously - a premium API client might be identified by their API key for generous limits, while their IP address gets tracked separately for basic abuse prevention. This dual-tracking approach prevents a single compromised API key from overwhelming our entire system.\n\n**Phase 2: Rate Limit Configuration Resolution**\n\n| Step | Component | Action Taken | Data Produced |\n|------|-----------|--------------|---------------|\n| 6 | Client Tracker | Look up endpoint-specific rate limits for the canonical path | Optional `TokenBucketConfig` |\n| 7 | Client Tracker | Look up client-specific rate limit overrides | Optional `TokenBucketConfig` |\n| 8 | Client Tracker | Apply hierarchical resolution: endpoint → client → default | Resolved `TokenBucketConfig` |\n| 9 | Client Tracker | Calculate effective limits considering any active promotions or penalties | Final `TokenBucketConfig` |\n\nThe configuration resolution phase implements sophisticated logic to handle overlapping rate limit rules. Consider a scenario where a premium API client (identified by API key) makes requests to a rate-limited endpoint from a new IP address. Our resolution logic applies the most permissive limits when multiple configurations could apply, while still maintaining separate tracking for abuse detection.\n\nThis approach prevents legitimate users from being unexpectedly blocked while ensuring that abusive patterns get detected quickly. The resolution algorithm considers the specificity hierarchy: custom client overrides take precedence over endpoint defaults, which take precedence over global defaults.\n\n**Phase 3: Token Bucket Operations**\n\n| Step | Component | Action Taken | Data Produced |\n|------|-----------|--------------|---------------|\n| 10 | Client Tracker | Retrieve or create token bucket for client-endpoint combination | `BucketInfo` containing `TokenBucket` |\n| 11 | Token Bucket | Calculate tokens to add based on elapsed time since last access | Integer token refill amount |\n| 12 | Token Bucket | Update bucket capacity with newly generated tokens (capped at maximum) | Updated token count |\n| 13 | Token Bucket | Attempt to consume requested number of tokens (usually 1) | `TokenConsumptionResult` |\n| 14 | Client Tracker | Update bucket's last accessed timestamp for cleanup tracking | Updated `BucketInfo` |\n\nThe token bucket operations represent the heart of our rate limiting algorithm. The timing precision here is critical - we calculate token refill based on elapsed time since the bucket's last access, not since the last request. This distinction matters for burst handling, where a client might make several rapid requests after a period of inactivity.\n\nOur token calculation uses floating-point arithmetic to handle fractional tokens accurately, but we store only integer token counts. This approach prevents rounding errors from accumulating over time while maintaining precise rate limiting behavior. When a bucket hasn't been accessed for a long period, we cap the refill at the bucket's maximum capacity to prevent unbounded token accumulation.\n\n**Phase 4: Response Generation and Cleanup**\n\n| Step | Component | Action Taken | Data Produced |\n|------|-----------|--------------|---------------|\n| 15 | HTTP Middleware | Generate appropriate HTTP response based on consumption result | HTTP response with headers |\n| 16 | HTTP Middleware | Add rate limiting headers (X-RateLimit-Limit, X-RateLimit-Remaining) | Enhanced HTTP response |\n| 17 | HTTP Middleware | For rejected requests, add Retry-After header with backoff time | HTTP 429 response |\n| 18 | HTTP Middleware | Log rate limiting decision and metrics for monitoring | Log entries and metrics |\n| 19 | Client Tracker | Update access patterns for adaptive rate limiting (future enhancement) | Updated client statistics |\n\nThe response generation phase ensures that clients receive clear, actionable information about their rate limiting status. We include standard rate limiting headers in every response, not just rejections, so clients can proactively manage their request rates. The `Retry-After` header calculation considers the current token deficit and refill rate to provide accurate timing guidance.\n\n### Distributed Processing Flow\n\n![Distributed Rate Limiting Sequence](./diagrams/distributed-sequence.svg)\n\nWhen operating in distributed mode across multiple server instances, our component interactions become significantly more complex. The fundamental challenge is maintaining **distributed consistency** while minimizing latency and handling partial failures gracefully.\n\n**Redis-Coordinated Token Operations**\n\nIn distributed mode, token bucket state lives in Redis rather than local memory. This centralization ensures that a client's rate limit applies consistently regardless of which server instance handles their requests. However, it introduces new challenges around atomic operations, network latency, and failure handling.\n\n| Step | Component | Action Taken | Distributed Considerations |\n|------|-----------|--------------|---------------------------|\n| 1-9 | Same as above | Client identification and configuration resolution | Same logic, but config may come from Redis |\n| 10 | Redis Connection Manager | Establish connection with circuit breaker protection | Handle connection failures gracefully |\n| 11 | Distributed Token Bucket | Execute Lua script for atomic token refill and consumption | Ensure atomicity across read-modify-write |\n| 12 | Redis Storage | Update token count and last access timestamp atomically | Prevent race conditions between servers |\n| 13 | Distributed Token Bucket | Handle Redis failures with local fallback bucket creation | Maintain service availability during outages |\n| 14 | Circuit Breaker | Monitor Redis operation success rates and adjust behavior | Prevent cascading failures |\n\nThe Redis Lua script execution in step 11 represents our most critical distributed operation. This script performs several operations atomically: calculating elapsed time, adding refill tokens, consuming requested tokens, updating timestamps, and returning the consumption result. Without this atomicity, race conditions between multiple server instances could lead to incorrect rate limiting decisions.\n\n**Local Fallback Coordination**\n\nWhen Redis becomes unavailable, our system doesn't simply fail open or closed. Instead, it gracefully degrades to **local fallback buckets** with more conservative limits. This fallback behavior requires careful coordination to prevent both service disruption and abuse.\n\n| Failure Scenario | Detection Method | Fallback Action | Recovery Behavior |\n|------------------|------------------|-----------------|-------------------|\n| Redis connection timeout | Connection attempt exceeds configured timeout | Create local bucket with 50% of normal limits | Retry Redis every 30 seconds with exponential backoff |\n| Redis command execution failure | Command returns error or exception | Use existing local bucket or create conservative one | Monitor Redis health and switch back when available |\n| Redis performance degradation | Command latency exceeds threshold | Gradually shift traffic to local buckets | Reduce Redis load while maintaining partial consistency |\n| Network partition | Multiple consecutive timeouts | Full local fallback mode with reduced limits | Wait for network recovery and gradual Redis re-integration |\n\nThe fallback coordination ensures that even during complete Redis outages, our rate limiting system continues protecting backend services. The conservative limits during fallback mode err on the side of caution - it's better to occasionally over-limit legitimate users than to allow abuse during infrastructure problems.\n\n## Inter-Component Communication\n\n### Interface Contracts and Data Exchange\n\nThe communication between our rate limiting components follows well-defined interface contracts that enable loose coupling while ensuring reliable data flow. Each component exposes specific methods and expects particular data formats, creating a clear separation of concerns.\n\n**HTTP Middleware to Client Tracker Communication**\n\nThe middleware layer communicates with the client tracker through a request-response pattern, passing rich context about each HTTP request and receiving rate limiting decisions. This communication must be extremely fast since it happens on the critical path of every API request.\n\n| Method Call | Input Parameters | Return Value | Error Conditions |\n|-------------|------------------|--------------|------------------|\n| `identify_client(request_data)` | Request headers, IP address, path | `ClientIdentifier` object | Invalid IP format, missing API key |\n| `get_bucket_for_client(client_id, endpoint)` | Client ID string, optional endpoint | `TokenBucket` instance | Configuration lookup failure |\n| `resolve_bucket_config(client_id, endpoint)` | Client ID, endpoint path | `TokenBucketConfig` | No matching configuration found |\n| `process_request(request_data)` | Complete request context | Rate limit decision dict | Bucket creation failure, Redis error |\n\nThe `request_data` dictionary contains standardized fields that abstract away web framework differences. This framework-agnostic approach allows our rate limiting core to work with Flask, Django, FastAPI, or any other HTTP framework through simple adapter layers.\n\n```\nrequest_data structure:\n{\n    'headers': dict,           # HTTP headers as key-value pairs\n    'remote_addr': str,        # Client IP address\n    'path': str,              # Request path\n    'method': str,            # HTTP method (GET, POST, etc.)\n    'query_params': dict,     # URL query parameters\n    'timestamp': float,       # Request arrival time\n    'user_agent': str,        # Client user agent string\n    'content_length': int     # Request body size\n}\n```\n\n**Client Tracker to Token Bucket Communication**\n\nThe client tracker manages the lifecycle of token bucket instances, creating them on demand, tracking their usage, and cleaning them up when they become stale. This communication pattern follows a factory model where the tracker creates and configures buckets according to resolved rate limit rules.\n\n| Method Call | Purpose | Data Flow | Performance Impact |\n|-------------|---------|-----------|-------------------|\n| `TokenBucket.try_consume(tokens)` | Attempt to consume tokens for request | Pass token count, receive consumption result | Critical path - must be very fast |\n| `TokenBucket.get_status()` | Check bucket state without consuming | No parameters, receive current token count | Used for monitoring and debugging |\n| `TokenBucket.refill_tokens()` | Add tokens based on elapsed time | Internal time calculation, update state | Called automatically before consumption |\n| `TokenBucket.reset_bucket()` | Reset bucket to initial state | Clear all state, restore to configuration defaults | Used for testing and emergency recovery |\n\nThe token bucket communication is designed to be stateless from the client tracker's perspective. The tracker doesn't need to know about internal bucket timing calculations or token arithmetic - it simply requests consumption and receives a clear allow/deny decision along with remaining capacity information.\n\n**Distributed Storage Communication Patterns**\n\nIn distributed mode, our components must coordinate through Redis while handling network failures, timeouts, and consistency challenges. This communication pattern implements the **single source of truth** principle while providing graceful degradation capabilities.\n\n| Operation Type | Redis Command Pattern | Fallback Behavior | Consistency Guarantee |\n|----------------|----------------------|-------------------|----------------------|\n| Token consumption | Lua script execution with EVAL | Local bucket with conservative limits | Eventual consistency with bounded drift |\n| Bucket status query | GET with key pattern | Return cached local state | Read-your-writes consistency |\n| Configuration updates | SET with expiration | Use last known good config | Strong consistency for critical settings |\n| Cleanup operations | SCAN and DELETE batch | Local cleanup only | Best effort with periodic reconciliation |\n\nThe Redis communication layer implements sophisticated retry logic with exponential backoff to handle transient network issues. However, we carefully limit retry attempts to prevent request processing delays from accumulating. After a small number of failed retries, we switch to local fallback mode rather than blocking the request pipeline.\n\n### Message Formats and Serialization\n\n**Internal Message Structures**\n\nOur components exchange data through well-defined message structures that balance human readability with serialization efficiency. These structures support both in-memory communication (single server) and network communication (distributed setup).\n\n| Message Type | Purpose | Serialization Format | Example Usage |\n|--------------|---------|---------------------|---------------|\n| Rate Limit Request | Client requests permission to proceed | JSON for Redis, direct objects locally | Middleware to bucket coordination |\n| Token Consumption Result | Response to rate limit request | Structured object with standard fields | Bucket to middleware response |\n| Configuration Update | Changes to rate limiting rules | JSON with schema validation | Dynamic configuration reload |\n| Health Check Status | Component availability and performance | Lightweight JSON with timestamps | Circuit breaker and monitoring |\n\nThe `TokenConsumptionResult` represents our most frequently exchanged message, flowing from token buckets back through client trackers to HTTP middleware. Its design prioritizes both completeness and efficiency:\n\n| Field Name | Data Type | Purpose | Example Value |\n|------------|-----------|---------|---------------|\n| `allowed` | Boolean | Whether request should be permitted | `true` or `false` |\n| `tokens_remaining` | Integer | Current bucket token count after operation | `47` |\n| `retry_after_seconds` | Float | Seconds until next token available | `2.5` |\n| `bucket_capacity` | Integer | Maximum tokens this bucket can hold | `100` |\n| `refill_rate` | Float | Tokens added per second | `10.0` |\n| `client_id` | String | Identifier for debugging and logging | `\"api_key:abc123\"` |\n| `endpoint` | String | Endpoint pattern that matched | `\"/api/v1/users\"` |\n| `consumed_tokens` | Integer | Number of tokens consumed by this request | `1` |\n\n**Redis Storage Formats**\n\nWhen operating in distributed mode, we serialize token bucket state into Redis using carefully designed key patterns and value structures. These formats balance queryability, storage efficiency, and atomic operation requirements.\n\n| Redis Key Pattern | Value Structure | Purpose | Expiration Policy |\n|-------------------|-----------------|---------|------------------|\n| `rate_limit:bucket:{client_id}:{endpoint}` | JSON with token count and timestamp | Primary bucket state | Auto-expire after inactivity |\n| `rate_limit:config:{pattern}` | JSON configuration object | Per-endpoint rate limits | Manual expiration on updates |\n| `rate_limit:stats:{client_id}:daily` | Compressed usage statistics | Long-term analytics and abuse detection | Daily rotation |\n| `rate_limit:circuit_breaker` | Simple counter or timestamp | Circuit breaker state coordination | Short TTL for fast recovery |\n\nThe bucket state serialization includes metadata that enables accurate distributed coordination:\n\n```\nRedis bucket value structure:\n{\n    \"tokens\": 42,                    # Current token count\n    \"last_refill\": 1634567890.123,   # Unix timestamp of last token refill\n    \"capacity\": 100,                 # Maximum bucket capacity\n    \"refill_rate\": 10.0,             # Tokens per second\n    \"created_at\": 1634567800.000,    # Bucket creation timestamp\n    \"access_count\": 156,             # Total requests processed\n    \"last_client_ip\": \"192.168.1.1\", # For debugging and analytics\n    \"endpoint\": \"/api/v1/users\",     # Associated endpoint pattern\n    \"config_version\": 3              # Configuration version for consistency\n}\n```\n\n**Cross-Server Coordination Messages**\n\nIn a distributed deployment, our rate limiter instances occasionally need to coordinate beyond simple Redis operations. This coordination handles scenarios like configuration updates, emergency rate limit adjustments, and coordinated cleanup operations.\n\n| Coordination Type | Message Channel | Message Format | Delivery Guarantee |\n|-------------------|-----------------|----------------|--------------------|\n| Configuration reload | Redis pub/sub | JSON with versioning | At-least-once delivery |\n| Emergency rate limit | Redis shared key | Simple flag with expiration | Immediate consistency |\n| Cleanup coordination | Redis sorted set | Timestamped work items | Exactly-once processing |\n| Health monitoring | Redis heartbeat key | Instance status with TTL | Best effort delivery |\n\nThe pub/sub coordination for configuration updates includes versioning and idempotency protection to ensure that rapid-fire configuration changes don't create inconsistent states across the server fleet.\n\n## Message and Data Formats\n\n### Request Processing Data Structures\n\n**Standardized Request Context**\n\nOur rate limiting system processes requests through a **framework-agnostic core** that normalizes data from different web frameworks into a consistent internal format. This normalization happens at the middleware boundary and flows through all subsequent processing.\n\n| Field Name | Data Type | Source | Validation Rules |\n|------------|-----------|--------|------------------|\n| `client_ip` | String | HTTP headers or connection info | Valid IPv4/IPv6 format, not in reserved ranges |\n| `api_key` | Optional String | Authorization header or custom header | Alphanumeric, 32-64 characters, not expired |\n| `endpoint_path` | String | Request URL path | Normalized to pattern (remove IDs, etc.) |\n| `http_method` | String | HTTP verb | Must be in allowed methods list |\n| `user_agent` | String | User-Agent header | Length limit, basic format validation |\n| `request_size` | Integer | Content-Length header | Within configured limits |\n| `timestamp` | Float | Server processing time | Unix timestamp with millisecond precision |\n| `custom_headers` | Dict | Configurable header names | Key-value pairs for custom identification |\n\nThe request context normalization performs several important transformations that improve rate limiting accuracy. IP address normalization handles both IPv4 and IPv6 addresses consistently, while endpoint path normalization groups similar requests together (e.g., `/users/123` and `/users/456` both become `/users/{id}`).\n\n**Token Bucket State Representation**\n\nThe token bucket state representation must support both in-memory operations and distributed serialization while maintaining precision in token calculations and timing operations.\n\n| State Component | Storage Type | Precision Requirements | Synchronization Needs |\n|-----------------|--------------|----------------------|----------------------|\n| Current token count | Integer | Exact integer arithmetic | Atomic updates in distributed mode |\n| Last refill timestamp | Float | Millisecond precision | Consistent across all servers |\n| Bucket capacity | Integer | Configuration-driven constant | Read-only after creation |\n| Refill rate | Float | Precise decimal arithmetic | Read-only after creation |\n| Access metadata | Object | Various types for monitoring | Eventually consistent |\n\nThe state representation uses integer arithmetic for token counts to avoid floating-point precision issues that could accumulate over time. However, we use floating-point calculations for time-based refill operations, then truncate to integers for actual token storage.\n\n**Configuration Data Structures**\n\nRate limiting configurations follow a hierarchical structure that supports inheritance, overrides, and dynamic updates without requiring application restarts.\n\n| Configuration Level | Priority | Override Scope | Update Frequency |\n|-------------------|----------|----------------|------------------|\n| Global defaults | Lowest | All clients and endpoints | Rarely - deployment changes |\n| Endpoint-specific | Medium | All clients for specific endpoints | Occasionally - feature releases |\n| Client-specific | High | Specific client across all endpoints | Regularly - account upgrades |\n| Emergency overrides | Highest | Temporary restrictions during incidents | Emergency only |\n\nThe configuration inheritance system allows for sophisticated rate limiting policies. A premium client might have generous global limits, but still be subject to endpoint-specific restrictions on expensive operations like large file uploads or complex database queries.\n\n### Response Message Formats\n\n**Success Response Enhancement**\n\nWhen a request passes rate limiting, our middleware enhances the response with informational headers that help clients manage their request patterns proactively. This information enables **adaptive backoff strategies** in client applications.\n\n| Header Name | Value Format | Purpose | Client Usage |\n|-------------|--------------|---------|--------------|\n| `X-RateLimit-Limit` | Integer (requests per window) | Maximum requests allowed | Display to users, plan request batching |\n| `X-RateLimit-Remaining` | Integer (remaining requests) | Requests remaining in current window | Decide whether to make additional requests |\n| `X-RateLimit-Reset` | Unix timestamp | When the rate limit window resets | Schedule delayed requests |\n| `X-RateLimit-Policy` | String description | Human-readable rate limit description | Display policy to API consumers |\n\nThese headers follow industry standards established by GitHub, Twitter, and other major APIs, ensuring that existing client libraries can automatically handle our rate limiting without custom code.\n\n**Rate Limit Exceeded Response**\n\nWhen we reject a request due to rate limiting, the response format provides clear guidance on when the client can retry and why the request was rejected.\n\n| Response Component | Format | Purpose | Example Value |\n|-------------------|--------|---------|---------------|\n| HTTP Status Code | 429 | Standard \"Too Many Requests\" indicator | `429` |\n| Retry-After | Integer seconds | Minimum wait time before retry | `30` |\n| Response Body | JSON object | Detailed error information | See structure below |\n| Rate Limit Headers | Same as success case | Current status information | Same headers as above |\n\nThe response body follows a structured format that supports both human debugging and programmatic error handling:\n\n```\nRate limit exceeded response body:\n{\n    \"error\": {\n        \"code\": \"rate_limit_exceeded\",\n        \"message\": \"Rate limit exceeded for this API key\",\n        \"details\": {\n            \"limit\": 100,\n            \"window_seconds\": 3600,\n            \"retry_after\": 30,\n            \"policy_name\": \"premium_api_key\",\n            \"endpoint\": \"/api/v1/users\"\n        }\n    },\n    \"meta\": {\n        \"timestamp\": \"2023-10-15T14:30:45.123Z\",\n        \"request_id\": \"req_abc123def456\"\n    }\n}\n```\n\nThis structured response format enables client applications to implement sophisticated retry logic, display meaningful error messages to users, and collect metrics about rate limiting behavior.\n\n### Storage and Persistence Formats\n\n**Redis Key Design Patterns**\n\nOur Redis key design follows consistent patterns that support efficient querying, automatic expiration, and operational debugging. The key structure balances readability with storage efficiency and query performance.\n\n| Key Pattern | Purpose | Example | Expiration Policy |\n|-------------|---------|---------|------------------|\n| `rl:b:{client}:{endpoint}` | Token bucket state | `rl:b:ip_192.168.1.1:/api/users` | Idle timeout (1 hour) |\n| `rl:c:{pattern}` | Configuration rules | `rl:c:endpoint:/api/upload` | Manual management |\n| `rl:s:{client}:{date}` | Usage statistics | `rl:s:key_abc123:2023-10-15` | Retention policy (30 days) |\n| `rl:m:{server}:{timestamp}` | Monitoring heartbeats | `rl:m:web-01:1634567890` | Short TTL (60 seconds) |\n\nThe abbreviated key prefixes (`rl:b`, `rl:c`, etc.) minimize Redis memory usage while maintaining human readability for debugging. The hierarchical structure supports efficient pattern matching and bulk operations during maintenance tasks.\n\n**Atomic Operation Scripts**\n\nOur most critical distributed operations use Redis Lua scripts to ensure atomicity. These scripts handle the complex logic of token refill calculations, consumption checks, and state updates in a single atomic operation.\n\n| Script Purpose | Input Parameters | Return Value | Error Conditions |\n|-----------------|------------------|--------------|------------------|\n| Token consumption | Client ID, endpoint, tokens requested, current time | Consumption result object | Invalid parameters, script error |\n| Bucket creation | Client ID, endpoint, configuration | New bucket state | Configuration lookup failure |\n| Bulk cleanup | Cleanup cutoff timestamp, batch size | Number of buckets removed | Redis operation timeout |\n| Health check | Server instance ID, status data | Updated health status | Network partition scenarios |\n\nThe token consumption script represents our most performance-critical operation, executed potentially thousands of times per second across our entire distributed fleet:\n\n```\nToken consumption Lua script logic:\n1. Retrieve current bucket state from Redis key\n2. Calculate elapsed time since last refill\n3. Compute new tokens to add based on refill rate\n4. Apply capacity limits to prevent overflow\n5. Check if requested tokens are available\n6. If available: deduct tokens and update state\n7. If not available: calculate retry-after time\n8. Update last access timestamp\n9. Return consumption result with all metadata\n```\n\nThis script encapsulates all the timing calculations and state management that would otherwise require multiple Redis round trips, significantly improving both performance and consistency.\n\n> **Key Design Insight**: The progression from simple in-memory data structures in early milestones to sophisticated distributed message formats in later milestones teaches learners how system complexity grows organically. Each new requirement (client tracking, HTTP integration, distribution) introduces new data format needs while building on existing structures.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Inconsistent Data Formats Across Components**\n\nMany learners create slightly different data structures for similar purposes across components, leading to constant conversion overhead and bugs. For example, representing client IDs as strings in one component but objects in another, or using different timestamp formats (Unix seconds vs. milliseconds) across the system.\n\n**Why it's wrong**: Inconsistent formats require conversion logic at every component boundary, creating performance overhead and opportunities for bugs. Time format inconsistencies are particularly dangerous because they can cause rate limiting to behave incorrectly without obvious error symptoms.\n\n**How to fix it**: Define canonical data formats once in a central location and use exactly the same structures everywhere. Create factory functions or constructors that ensure consistent formatting, especially for timestamps and client identifiers.\n\n⚠️ **Pitfall: Missing Error Context in Inter-Component Communication**\n\nLearners often design component interfaces that return only success/failure indicators without providing sufficient context for error handling or debugging. This leads to generic error messages and difficulty troubleshooting rate limiting issues.\n\n**Why it's wrong**: Without rich error context, the middleware layer cannot provide meaningful feedback to clients or operators. Error messages like \"rate limit failed\" don't help users understand whether they should retry immediately, wait, or check their configuration.\n\n**How to fix it**: Every error result should include specific error codes, human-readable messages, and actionable next steps. Design error types that carry enough context to generate appropriate HTTP responses and log entries.\n\n⚠️ **Pitfall: Ignoring Clock Synchronization in Distributed Messages**\n\nIn distributed setups, learners often assume all servers have perfectly synchronized clocks, leading to inconsistent token refill calculations and unfair rate limiting when servers have clock drift.\n\n**Why it's wrong**: Even small clock differences (seconds) can cause dramatic rate limiting inconsistencies. A client might get different effective rate limits depending on which server handles their requests, leading to unpredictable behavior and user complaints.\n\n**How to fix it**: Use relative timing measurements within individual operations, and include authoritative timestamps from Redis or another centralized time source in distributed messages. Design algorithms that gracefully handle small clock differences.\n\n### Implementation Guidance\n\nOur implementation of component interactions requires careful attention to both performance and maintainability. The following guidance shows how to structure the communication pathways and data flow in Python.\n\n**Technology Recommendations**\n\n| Component Communication | Simple Option | Advanced Option |\n|------------------------|---------------|-----------------|\n| HTTP Request Context | Simple dictionary with validation functions | Pydantic models with automatic serialization |\n| Inter-Component Messaging | Direct method calls with type hints | Message queue system (Redis Streams) |\n| Configuration Management | JSON files with environment variable overrides | Consul/etcd with dynamic reload |\n| Redis Communication | redis-py with connection pooling | redis-py-cluster with automatic failover |\n| Error Handling | Exception-based with custom error types | Result types with explicit error handling |\n\n**File Structure for Component Integration**\n\n```\nrate_limiter/\n├── core/\n│   ├── __init__.py\n│   ├── interfaces.py          ← Abstract base classes for all components\n│   ├── data_models.py         ← Canonical data structures\n│   └── exceptions.py          ← Custom exception types\n├── components/\n│   ├── __init__.py\n│   ├── middleware.py          ← HTTP middleware implementation\n│   ├── client_tracker.py      ← Client identification and bucket management\n│   ├── token_bucket.py        ← Token bucket algorithm\n│   └── storage.py            ← Storage abstraction layer\n├── distributed/\n│   ├── __init__.py\n│   ├── redis_storage.py       ← Redis-backed storage implementation\n│   ├── lua_scripts.py         ← Atomic operation scripts\n│   └── circuit_breaker.py     ← Failure handling\n└── integration/\n    ├── __init__.py\n    ├── flask_integration.py   ← Flask-specific middleware\n    └── fastapi_integration.py ← FastAPI-specific middleware\n```\n\n**Core Data Structures Implementation**\n\n```python\n# core/data_models.py - Complete canonical data structures\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional, Any, Union\nfrom enum import Enum\nimport time\n\n@dataclass\nclass ClientIdentifier:\n    \"\"\"Normalized client identification across all components.\"\"\"\n    raw_value: str\n    identifier_type: 'IdentifierType'\n    namespace: str = \"default\"\n    \n    def to_key(self) -> str:\n        \"\"\"Generate Redis key or internal identifier.\"\"\"\n        # TODO 1: Combine namespace, type, and value into consistent key\n        # TODO 2: Apply URL-safe encoding for special characters\n        # TODO 3: Ensure key length stays within Redis limits\n        pass\n\n@dataclass\nclass TokenConsumptionResult:\n    \"\"\"Standard response format for all rate limiting operations.\"\"\"\n    allowed: bool\n    tokens_remaining: int\n    retry_after_seconds: float\n    bucket_capacity: int = 0\n    refill_rate: float = 0.0\n    client_id: str = \"\"\n    endpoint: str = \"\"\n    consumed_tokens: int = 1\n    \n    def to_http_headers(self) -> Dict[str, str]:\n        \"\"\"Convert to standard HTTP rate limiting headers.\"\"\"\n        # TODO 1: Generate X-RateLimit-* headers from fields\n        # TODO 2: Format Retry-After header correctly\n        # TODO 3: Include policy information for debugging\n        pass\n\n@dataclass\nclass RequestContext:\n    \"\"\"Framework-agnostic request information.\"\"\"\n    client_ip: str\n    endpoint_path: str\n    http_method: str\n    timestamp: float = field(default_factory=time.time)\n    api_key: Optional[str] = None\n    user_agent: str = \"\"\n    custom_headers: Dict[str, str] = field(default_factory=dict)\n    request_size: int = 0\n    \n    @classmethod\n    def from_flask_request(cls, request) -> 'RequestContext':\n        \"\"\"Extract context from Flask request object.\"\"\"\n        # TODO 1: Extract IP address, handling X-Forwarded-For\n        # TODO 2: Normalize endpoint path (remove IDs, query params)\n        # TODO 3: Extract API key from Authorization header\n        # TODO 4: Validate and sanitize all extracted data\n        pass\n```\n\n**Component Communication Infrastructure**\n\n```python\n# core/interfaces.py - Abstract interfaces for loose coupling\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Optional\n\nclass RateLimitStorage(ABC):\n    \"\"\"Abstract storage interface for token bucket state.\"\"\"\n    \n    @abstractmethod\n    async def get_bucket_state(self, client_id: str, endpoint: str) -> Optional[Dict]:\n        \"\"\"Retrieve current bucket state.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def update_bucket_state(self, client_id: str, endpoint: str, \n                                state: Dict) -> bool:\n        \"\"\"Atomically update bucket state.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def cleanup_stale_buckets(self, max_age_seconds: int) -> int:\n        \"\"\"Remove inactive buckets.\"\"\"\n        pass\n\nclass ClientIdentificationStrategy(ABC):\n    \"\"\"Abstract strategy for identifying API clients.\"\"\"\n    \n    @abstractmethod\n    def extract_client_id(self, request_context: RequestContext) -> ClientIdentifier:\n        \"\"\"Extract client identifier from request.\"\"\"\n        pass\n    \n    @abstractmethod\n    def validate_client_id(self, client_id: str) -> bool:\n        \"\"\"Validate client identifier format.\"\"\"\n        pass\n\n# components/rate_limit_coordinator.py - Main coordination logic\nclass RateLimitCoordinator:\n    \"\"\"Coordinates all rate limiting components.\"\"\"\n    \n    def __init__(self, storage: RateLimitStorage, \n                 id_strategy: ClientIdentificationStrategy):\n        self.storage = storage\n        self.id_strategy = id_strategy\n        # TODO: Initialize other components (client tracker, etc.)\n    \n    async def process_request(self, request_context: RequestContext) -> TokenConsumptionResult:\n        \"\"\"Main request processing pipeline.\"\"\"\n        # TODO 1: Extract and validate client identifier\n        # TODO 2: Resolve rate limiting configuration\n        # TODO 3: Get or create appropriate token bucket\n        # TODO 4: Attempt token consumption\n        # TODO 5: Update access tracking and metrics\n        # TODO 6: Return detailed consumption result\n        pass\n    \n    async def should_bypass_rate_limiting(self, request_context: RequestContext) -> bool:\n        \"\"\"Check if request should skip rate limiting.\"\"\"\n        # TODO 1: Check for bypass headers\n        # TODO 2: Verify whitelist patterns\n        # TODO 3: Apply emergency override flags\n        pass\n```\n\n**Redis Communication Patterns**\n\n```python\n# distributed/redis_storage.py - Redis-backed storage implementation\nimport redis.asyncio as redis\nimport json\nfrom typing import Dict, Optional\n\nclass RedisRateLimitStorage(RateLimitStorage):\n    \"\"\"Redis implementation with atomic operations and fallback.\"\"\"\n    \n    def __init__(self, redis_url: str, circuit_breaker):\n        self.redis_pool = redis.ConnectionPool.from_url(redis_url)\n        self.circuit_breaker = circuit_breaker\n        # TODO: Load Lua scripts for atomic operations\n    \n    async def atomic_consume_tokens(self, client_id: str, endpoint: str,\n                                  tokens_requested: int) -> TokenConsumptionResult:\n        \"\"\"Execute atomic token consumption using Lua script.\"\"\"\n        # TODO 1: Prepare script parameters (client_id, endpoint, tokens, time)\n        # TODO 2: Execute Lua script with circuit breaker protection\n        # TODO 3: Parse script response into TokenConsumptionResult\n        # TODO 4: Handle Redis errors with local fallback\n        # TODO 5: Update circuit breaker state based on result\n        pass\n    \n    async def batch_cleanup_stale_buckets(self, batch_size: int = 100) -> int:\n        \"\"\"Clean up inactive buckets in batches.\"\"\"\n        # TODO 1: Use SCAN to find candidate bucket keys\n        # TODO 2: Check last access time for each bucket\n        # TODO 3: Delete stale buckets in batches using pipeline\n        # TODO 4: Track cleanup statistics for monitoring\n        pass\n\n# distributed/lua_scripts.py - Atomic operation scripts\nTOKEN_BUCKET_CONSUME_SCRIPT = \"\"\"\n-- Atomic token bucket consumption script\n-- KEYS[1]: bucket key\n-- ARGV[1]: tokens requested\n-- ARGV[2]: current timestamp\n-- ARGV[3]: bucket configuration JSON\n\n-- TODO 1: Parse existing bucket state from Redis\n-- TODO 2: Calculate token refill based on elapsed time\n-- TODO 3: Apply capacity limits and consume requested tokens\n-- TODO 4: Update bucket state atomically\n-- TODO 5: Return consumption result as JSON\n\nlocal bucket_key = KEYS[1]\nlocal tokens_requested = tonumber(ARGV[1])\nlocal current_time = tonumber(ARGV[2])\nlocal config = cjson.decode(ARGV[3])\n\n-- Implementation will be filled in by learner\nreturn cjson.encode({\n    allowed = false,\n    tokens_remaining = 0,\n    retry_after_seconds = 60\n})\n\"\"\"\n```\n\n**Milestone Checkpoints**\n\nAfter implementing component interactions:\n\n1. **Basic Integration Test**: Create a simple HTTP server that uses your rate limiter middleware. Send requests and verify that rate limiting headers appear in responses.\n\n2. **Component Isolation Test**: Test each component independently. Mock the storage layer and verify that client tracking works correctly. Mock the client tracker and test token bucket operations.\n\n3. **Distributed Coordination Test**: If implementing Redis storage, test with multiple Python processes hitting the same Redis instance. Verify that rate limits apply consistently across processes.\n\n4. **Error Handling Test**: Simulate Redis failures, network timeouts, and malformed requests. Verify that the system fails gracefully and provides meaningful error messages.\n\nCommands to run:\n```bash\n# Test basic middleware integration\npython -m pytest tests/test_integration.py::test_middleware_headers\n\n# Test component communication in isolation\npython -m pytest tests/test_components.py -v\n\n# Test distributed coordination (requires Redis)\npython -m pytest tests/test_distributed.py::test_multi_process_consistency\n\n# Load test with multiple workers\npython tests/load_test.py --workers 4 --requests 1000\n```\n\nExpected behavior: Requests should be rate limited consistently, HTTP headers should appear on all responses, and system should continue operating during simulated failures.\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones - robust error handling is essential from the basic token bucket through distributed rate limiting, with complexity increasing as the system evolves\n\nThink of error handling in a rate limiter like designing a city's emergency response system. Just as a city must prepare for natural disasters, power outages, traffic accidents, and infrastructure failures, our rate limiter must gracefully handle Redis outages, clock synchronization issues, network partitions, and resource exhaustion. The key insight is that **rate limiting systems are safety mechanisms themselves** - when they fail, they can either fail open (allowing all traffic and potentially overwhelming backend services) or fail closed (blocking all traffic and creating a service outage). Our design philosophy prioritizes **graceful degradation** over complete failure, allowing the system to continue protecting services even when operating under suboptimal conditions.\n\nThe challenge in rate limiting error handling is that failures often compound. A Redis outage doesn't just affect token storage - it triggers fallback mechanisms that consume more memory, increases CPU usage for local bucket management, and creates thundering herd problems when Redis recovers. Our error handling strategy must account for these cascading effects and provide multiple layers of protection.\n\n### System Failure Modes\n\nUnderstanding how each component can fail and how those failures propagate through the system is crucial for building robust error handling mechanisms. The rate limiter's distributed architecture creates multiple potential points of failure, each requiring specific detection and recovery strategies.\n\n![Error Handling and Recovery Flow](./diagrams/error-handling-flow.svg)\n\n#### Token Bucket Component Failures\n\nThe `TokenBucket` component, being the core algorithm implementation, faces several critical failure modes that can compromise rate limiting accuracy. These failures often stem from timing precision, numerical overflow, and concurrency issues.\n\n**Clock-Related Failures** represent one of the most subtle but dangerous failure modes in token bucket systems. When system clocks jump backward due to NTP corrections or manual adjustments, the token refill calculation can produce negative time deltas, potentially causing integer underflows or freezing token generation entirely. This creates scenarios where legitimate clients are indefinitely blocked despite being within their rate limits.\n\nThe detection strategy involves comparing the current timestamp against the bucket's last refill timestamp. If the current time is significantly earlier than the last refill time (beyond expected clock precision variance), the system should treat this as a clock jump event. The recovery mechanism involves resetting the bucket's last refill timestamp to the current time and applying a conservative token refill based on the configured rate, preventing both token starvation and excessive token accumulation.\n\n**Numerical Overflow Conditions** occur when token calculations exceed the maximum values for integer or floating-point types. This is particularly problematic for long-running systems where accumulated time values or high refill rates can cause arithmetic operations to overflow. The failure manifests as incorrect token counts, either extremely large values that effectively disable rate limiting or negative values that block all requests.\n\nDetection requires boundary checking before arithmetic operations, particularly when multiplying elapsed time by refill rates or adding tokens to existing bucket counts. The recovery strategy involves clamping values to safe ranges - time deltas to reasonable maximums (preventing calculations over extended periods) and token counts to the bucket's configured capacity. This ensures mathematical operations remain within safe bounds while maintaining rate limiting effectiveness.\n\n**Thread Safety Violations** in concurrent environments can lead to race conditions where multiple threads simultaneously modify bucket state, resulting in inconsistent token counts. These failures are particularly insidious because they may not cause immediate failures but gradually corrupt bucket state over time, leading to unpredictable rate limiting behavior.\n\nThe following table outlines the primary token bucket failure modes and their handling strategies:\n\n| Failure Mode | Detection Method | Recovery Strategy | Prevention |\n|---|---|---|---|\n| Clock jump backward | Current time < last_refill_time - tolerance | Reset last_refill_time to current time, conservative refill | Use monotonic clocks where available |\n| Clock jump forward | Current time > last_refill_time + max_reasonable_delta | Cap elapsed time to maximum reasonable value | Validate time deltas before calculations |\n| Token count overflow | Token calculation exceeds bucket capacity significantly | Clamp tokens to bucket capacity | Check bounds before arithmetic operations |\n| Negative token count | Token count becomes negative after consumption | Reset to zero, log incident | Validate consumption amounts |\n| Concurrent modification | Inconsistent state after operations | Re-acquire lock and retry operation | Use appropriate synchronization primitives |\n| Infinite refill rate | Configuration error with extremely high rates | Apply maximum rate limit from configuration | Validate configuration on load |\n\n> **Critical Design Insight**: Token bucket failures should never fail open (allowing unlimited requests) as this defeats the primary purpose of rate limiting. When in doubt, the system should fail conservatively by denying requests until it can establish a known-good state.\n\n#### Client Tracking Component Failures\n\nThe `ClientBucketTracker` manages potentially thousands of per-client buckets, making it vulnerable to memory exhaustion, cleanup failures, and client enumeration attacks. These failures can cause memory leaks, service degradation, or complete system unavailability.\n\n**Memory Exhaustion** occurs when the number of tracked clients exceeds available memory, either through legitimate traffic growth or malicious attacks creating many unique client identifiers. The system must detect memory pressure before reaching critical levels and implement defensive measures to maintain functionality.\n\nDetection involves monitoring both the number of tracked buckets and overall memory usage. When bucket counts exceed configured thresholds or memory usage crosses warning levels, the system should trigger emergency cleanup procedures. The recovery strategy includes aggressive LRU eviction of inactive buckets, temporarily reducing bucket retention times, and potentially implementing more restrictive rate limits to prevent new bucket creation during memory pressure events.\n\n**Cleanup Process Failures** happen when the background process responsible for removing stale buckets encounters errors, stops running, or falls behind the rate of new bucket creation. This leads to gradual memory leaks and eventual system failure.\n\nThe detection mechanism involves tracking cleanup process health through heartbeat timestamps and monitoring the ratio of cleanup rate to bucket creation rate. When cleanup falls behind or stops entirely, the system should alert operators and potentially trigger manual cleanup operations. Recovery includes restarting failed cleanup processes, performing emergency bulk cleanup operations, and temporarily halting new bucket creation until cleanup catches up.\n\n**Client Enumeration Attacks** involve malicious actors rapidly generating requests with different client identifiers to force the creation of many buckets, exhausting system memory. Traditional cleanup mechanisms are insufficient because the attack continuously creates new buckets faster than cleanup can remove old ones.\n\nDetection relies on monitoring bucket creation rates and identifying patterns of rapidly changing client identifiers from similar sources. The recovery strategy includes implementing rate limits on new bucket creation, requiring client identifier validation, and potentially blacklisting source IP addresses that exhibit enumeration attack patterns.\n\nThe client tracking failure modes and responses are summarized below:\n\n| Failure Mode | Symptoms | Detection Threshold | Recovery Action |\n|---|---|---|---|\n| Memory exhaustion | High memory usage, slow responses | Memory > 80% or bucket count > configured limit | Aggressive LRU eviction, reduced retention |\n| Cleanup process failure | Growing bucket count, memory leaks | Cleanup heartbeat > 2x interval | Restart cleanup, emergency bulk removal |\n| Client enumeration attack | Rapid bucket creation, memory pressure | Bucket creation rate > 100x normal | Rate limit new buckets, source IP analysis |\n| Bucket corruption | Inconsistent bucket state | State validation failures | Remove corrupted bucket, create new one |\n| Lock contention | High CPU usage, slow bucket operations | Lock wait times > threshold | Consider lock-free data structures |\n\n#### HTTP Middleware Failures\n\nThe HTTP middleware layer faces failures related to request processing, response generation, and integration with web frameworks. These failures can cause requests to be processed without rate limiting, incorrect HTTP status codes to be returned, or complete request processing failures.\n\n**Request Context Extraction Failures** occur when the middleware cannot properly extract client identification information, endpoint details, or other required metadata from HTTP requests. This can result from malformed headers, missing required fields, or framework compatibility issues.\n\nDetection involves validating extracted request context and identifying when required fields are missing or invalid. The recovery strategy depends on the type of failure - for missing client identifiers, the system can fall back to IP-based identification, while for missing endpoint information, it can apply default rate limits. The key is ensuring that extraction failures don't bypass rate limiting entirely.\n\n**Response Generation Failures** happen when the middleware encounters errors while building HTTP responses, particularly 429 Too Many Requests responses or adding rate limiting headers to successful responses. These failures can result in clients receiving confusing error messages or missing critical rate limiting information.\n\nRecovery involves implementing fallback response generation that uses minimal, guaranteed-safe response formats. Even if sophisticated response formatting fails, the middleware should still return proper HTTP status codes and basic headers to communicate rate limiting status to clients.\n\n#### Distributed Storage Failures\n\nRedis-based distributed rate limiting introduces additional failure modes related to network connectivity, Redis server availability, data consistency, and distributed coordination. These failures are among the most complex to handle because they affect multiple server instances simultaneously.\n\n**Redis Connection Failures** are the most common distributed storage failure, occurring due to network issues, Redis server crashes, or configuration problems. The impact affects all server instances attempting to coordinate rate limiting state through Redis.\n\nDetection uses connection health checks, operation timeouts, and error pattern analysis. The circuit breaker pattern provides automatic failure detection and recovery coordination. When Redis connections fail, the system must immediately switch to local fallback buckets while attempting background reconnection.\n\n**Redis Data Corruption or Inconsistency** can occur due to Redis server issues, network partitions during write operations, or clock synchronization problems between server instances updating the same buckets. This results in incorrect token counts that don't reflect actual request patterns.\n\nDetection requires implementing data validation on Redis operations, comparing expected versus actual values, and monitoring for impossible state transitions (such as token counts exceeding bucket capacities). Recovery involves invalidating corrupted data, resetting affected buckets to known-good states, and potentially forcing a brief period of local fallback operation while consistency is restored.\n\n**Distributed Coordination Failures** happen when multiple server instances make conflicting decisions about rate limiting due to network partitions, clock skew, or race conditions in Redis operations. This can result in either overly permissive rate limiting (allowing more requests than configured limits) or overly restrictive limiting (blocking legitimate requests).\n\nThe following table details distributed storage failure modes:\n\n| Failure Mode | Detection Method | Immediate Response | Long-term Recovery |\n|---|---|---|---|\n| Redis connection timeout | Connection attempts fail within timeout | Switch to local fallback buckets | Background reconnection attempts |\n| Redis server crash | All Redis operations fail with connection errors | Circuit breaker opens, local fallback | Monitor Redis health, reconnect when available |\n| Network partition | Intermittent Redis failures, high latency | Graceful degradation, local operation | Wait for network recovery, validate state |\n| Data corruption | Impossible bucket states, validation failures | Invalidate corrupted buckets | Reset affected client buckets |\n| Clock skew | Token counts inconsistent across instances | Use server-local timestamps | NTP synchronization, clock drift monitoring |\n| Thundering herd on recovery | All instances reconnect simultaneously | Randomized reconnection delays | Gradual state resynchronization |\n\n### Edge Cases and Corner Conditions\n\nEdge cases represent unusual but possible scenarios that can cause unexpected system behavior if not properly handled. Unlike failure modes, edge cases often involve the system operating within normal parameters but encountering unusual combinations of conditions that expose design assumptions or boundary conditions.\n\n#### Clock Synchronization Edge Cases\n\n**Clock Drift Between Distributed Instances** occurs when server instances have slightly different system times, causing inconsistent token generation rates across the distributed system. Even small clock differences (seconds or minutes) can accumulate over time to create noticeable rate limiting inconsistencies.\n\nThe challenge is that each server instance calculates token refill based on its local clock, but all instances share the same Redis-stored bucket state. If Server A's clock runs fast and Server B's clock runs slow, clients may experience different effective rate limits depending on which server processes their requests. This creates an unfair and unpredictable rate limiting experience.\n\nThe solution involves using a consistent time source for all token calculations in distributed scenarios. Instead of relying on local server time, the system should either use Redis server time as the authoritative source or implement clock offset correction based on periodic synchronization with a central time authority. The trade-off is additional complexity and potential slight performance impact versus rate limiting consistency.\n\n**Daylight Saving Time Transitions** create scenarios where clocks jump forward or backward by exactly one hour, potentially causing massive token bucket refills or extended blocking periods. Spring transitions (clocks jump forward) cause time calculations to show very large elapsed periods, potentially refilling buckets to maximum capacity instantly. Fall transitions (clocks jump backward) can cause negative time deltas or extended periods without token refill.\n\nDetection requires identifying when calculated elapsed time significantly exceeds expected values or when current timestamps are earlier than stored timestamps by amounts that match DST transitions. The recovery strategy involves capping token refill amounts during forward transitions and resetting refill timestamps during backward transitions, ensuring smooth operation through time changes.\n\n**Leap Second Adjustments** are rare but can cause similar issues to clock jumps. Most systems handle leap seconds by either jumping the clock or slowing/speeding clock advancement over a period, both of which can affect token bucket calculations.\n\nThe mitigation strategy involves using monotonic clocks (which measure elapsed time regardless of system clock adjustments) for token bucket calculations while using wall clock time only for logging and external interfaces. This isolates the core rate limiting algorithm from system clock irregularities.\n\n#### Burst Scenario Edge Cases\n\n**Sustained Burst Traffic Patterns** occur when clients consistently use their full burst allowance, then wait for refill, then burst again. This creates a pattern where traffic arrives in concentrated waves rather than the smooth distribution that token bucket algorithms are designed to regulate.\n\nWhile this behavior is technically within the rate limits, it can still overwhelm downstream services that expect more evenly distributed load. The system must decide whether to allow this behavior (following the strict token bucket algorithm) or implement additional smoothing mechanisms.\n\nOne approach involves implementing a secondary rate limit based on request spacing, requiring minimum intervals between requests even when tokens are available. Another approach uses averaging windows to detect sustained burst patterns and temporarily reduce burst capacity for clients exhibiting this behavior.\n\n**Cross-Client Burst Coordination** happens when multiple clients simultaneously execute burst requests, creating aggregate load spikes that exceed system capacity even though each client individually stays within their rate limits. This is particularly problematic when client request patterns are synchronized (such as cron jobs running at the same time).\n\nDetection requires monitoring aggregate request rates across all clients and identifying when total system load exceeds safe thresholds despite individual clients being within limits. The response strategy might include implementing system-wide admission control that temporarily reduces individual client limits when aggregate load is high, or implementing request queuing to smooth out synchronized burst patterns.\n\n**Bucket Capacity Overflow** occurs in edge cases where configuration changes or system bugs cause token refill calculations to exceed the bucket's maximum capacity. While the algorithm should cap tokens at the configured capacity, numerical precision issues or race conditions can sometimes cause temporary overflows.\n\nThe following table outlines burst-related edge cases:\n\n| Edge Case | Trigger Condition | Potential Impact | Mitigation Strategy |\n|---|---|---|---|\n| Sustained burst cycling | Client repeatedly uses full burst, waits, repeats | Uneven load on downstream services | Secondary spacing limits or burst capacity reduction |\n| Synchronized client bursts | Multiple clients burst simultaneously | Aggregate load exceeds system capacity | System-wide admission control, request queuing |\n| Bucket capacity overflow | Token calculation exceeds configured capacity | Temporarily excessive request allowances | Strict capacity enforcement, overflow detection |\n| Zero-capacity bucket configuration | Configuration error sets capacity to 0 | All requests blocked indefinitely | Configuration validation, minimum capacity enforcement |\n| Infinite burst allowance | Configuration sets unreasonably high capacity | Rate limiting becomes ineffective | Maximum capacity limits in configuration validation |\n\n#### Client Identification Edge Cases\n\n**IP Address Spoofing** involves clients manipulating their apparent IP address to circumvent IP-based rate limiting. While IP spoofing is difficult for TCP connections, it's possible for UDP-based protocols or when using proxy services that don't preserve original client IPs.\n\nThe mitigation strategy involves implementing multiple identification layers, such as combining IP addresses with API keys or user agent fingerprints. The system should also validate IP addresses for reasonableness (private IP ranges reaching public services might indicate proxy issues) and implement behavioral analysis to detect unusual patterns from individual IP addresses.\n\n**API Key Sharing or Compromise** occurs when legitimate API keys are shared among multiple clients or stolen by malicious actors. This creates scenarios where rate limits designed for individual clients are effectively bypassed through distributed usage of the same credentials.\n\nDetection involves monitoring API key usage patterns for signs of unusual distribution across IP addresses, geographic locations, or usage patterns that don't match expected behavior for individual clients. The response includes temporarily reducing rate limits for suspicious API keys, requiring additional authentication factors, or implementing sub-limits based on IP address even when API keys are provided.\n\n**Client Identifier Collision** happens when the client identification algorithm produces the same identifier for different actual clients. This is particularly problematic when using hash-based identification or when truncating long identifiers for storage efficiency.\n\nPrevention involves using high-quality hash functions with sufficient output length to minimize collision probability, implementing collision detection by storing additional client metadata, and providing fallback identification methods when collisions are detected.\n\n**Proxy and CDN Complications** arise when clients access services through proxy servers, content delivery networks, or other intermediaries that present the same IP address for multiple distinct clients. This causes all clients behind the proxy to share the same rate limit bucket.\n\nThe solution requires implementing proxy-aware client identification that examines headers like `X-Forwarded-For`, `X-Real-IP`, or custom headers provided by trusted proxies. The system must validate that proxy headers come from trusted sources and implement fallback strategies when proxy identification is unavailable or suspicious.\n\nThe client identification edge cases are summarized in this table:\n\n| Edge Case | Detection Method | Risk Level | Recommended Response |\n|---|---|---|---|\n| IP address spoofing | Behavioral analysis, TCP connection validation | Medium | Multi-factor client identification |\n| API key sharing | Usage pattern analysis across IPs | High | Rate limit reduction, additional auth factors |\n| Client identifier collision | Hash collision detection, metadata comparison | Low | Higher entropy identifiers, collision resolution |\n| Proxy/CDN masking | Multiple clients from same IP, high request rates | High | Proxy-aware headers, trusted proxy validation |\n| Client enumeration attack | Rapid creation of new client identifiers | High | Rate limit on new client creation |\n\n### Recovery and Degradation\n\nThe rate limiter's recovery and degradation strategies ensure that the system continues protecting services even when operating under adverse conditions. The key principle is **graceful degradation** - reducing functionality or performance rather than complete failure, while maintaining the core protection that rate limiting provides.\n\n#### Failure Recovery Strategies\n\n**Circuit Breaker Pattern Implementation** provides automatic failure detection and recovery for external dependencies, particularly Redis connections in distributed configurations. The circuit breaker monitors failure rates and response times, automatically switching to fallback modes when thresholds are exceeded and periodically testing for recovery conditions.\n\nThe circuit breaker operates in three states: **Closed** (normal operation), **Open** (failures detected, using fallback), and **Half-Open** (testing for recovery). State transitions are based on configurable failure thresholds and recovery timeouts. When Redis operations fail consistently, the circuit breaker opens and directs all rate limiting operations to local fallback buckets. After a recovery timeout period, it enters the half-open state and allows a limited number of test operations to determine if Redis has recovered.\n\nThe implementation must handle edge cases such as partial Redis recovery (some operations succeed while others fail) and thundering herd scenarios where multiple server instances simultaneously attempt recovery operations. The solution involves randomized recovery testing delays and gradual traffic ramp-up when Redis becomes available again.\n\n**Local Fallback Bucket Management** provides continued rate limiting functionality when distributed storage is unavailable. Local fallback buckets use more conservative rate limits to account for the lack of coordination between server instances, ensuring that the aggregate rate limiting remains effective even though individual instance limits might be lower.\n\nThe fallback strategy involves maintaining a separate set of in-memory token buckets with reduced capacities and refill rates calculated to provide reasonable protection when multiplied across all expected server instances. For example, if the normal distributed rate limit allows 1000 requests per minute and there are typically 5 server instances, each fallback bucket might allow 150 requests per minute (providing a safety margin below the distributed total).\n\nMemory management for fallback buckets requires aggressive cleanup policies since they cannot rely on distributed coordination for state management. The system implements strict limits on the number of concurrent fallback buckets and uses LRU eviction to prevent memory exhaustion during extended Redis outages.\n\n**Progressive Degradation Levels** provide multiple fallback layers as system conditions worsen. Instead of a binary switch between full functionality and emergency mode, the system implements graduated responses that maintain as much functionality as possible under various failure conditions.\n\nLevel 1 degradation occurs when Redis response times increase but operations still succeed. The system maintains full functionality but implements request timeouts and reduces the frequency of Redis operations where possible. Level 2 degradation activates when Redis operations begin failing intermittently, triggering local caching of recent rate limiting decisions and reduced precision in rate limiting calculations. Level 3 degradation engages full local fallback mode with conservative rate limits and no distributed coordination.\n\nThe following table outlines the progressive degradation strategy:\n\n| Degradation Level | Trigger Condition | Active Features | Disabled Features | Recovery Condition |\n|---|---|---|---|---|\n| Normal Operation | All systems healthy | Full distributed rate limiting | None | N/A |\n| Level 1: Performance | Redis latency > threshold | Full functionality, cached decisions | Real-time Redis queries for all requests | Redis latency < threshold for sustained period |\n| Level 2: Intermittent | Redis error rate > 10% | Local caching, reduced precision | Exact distributed synchronization | Redis error rate < 5% for recovery period |\n| Level 3: Full Fallback | Redis error rate > 50% | Local conservative buckets | All distributed features | Redis error rate < 1% and connection stability |\n| Emergency Mode | Memory/CPU exhaustion | Basic IP-based rate limiting | Per-client tracking, burst handling | Resource usage below emergency thresholds |\n\n#### Data Consistency Recovery\n\n**State Resynchronization After Network Partitions** addresses scenarios where server instances operate independently during network failures and must reconcile their rate limiting decisions when connectivity is restored. The challenge is determining which instance has the most accurate view of client rate limiting state and how to merge conflicting information.\n\nThe resynchronization strategy uses timestamps and sequence numbers to determine the authoritative state for each client bucket. When instances reconnect to Redis, they compare their local fallback bucket states with the distributed state, choosing the most restrictive interpretation to ensure rate limiting effectiveness wasn't compromised during the partition.\n\nThe process involves uploading local bucket states to Redis with metadata indicating the partition period and fallback limits used. A reconciliation algorithm examines all uploaded states and constructs a merged view that accounts for requests processed by all instances during the partition. This ensures that clients don't receive unfairly restrictive treatment due to double-counting their requests across instances.\n\n**Corrupted Bucket State Recovery** handles situations where Redis data becomes corrupted, inconsistent, or contains impossible values due to bugs, hardware issues, or operational errors. The recovery process must identify corrupted data, safely remove it, and re-initialize affected buckets without causing service disruption.\n\nDetection involves implementing data validation checks on all Redis read operations, flagging buckets with impossible states such as negative token counts, token counts exceeding configured capacities, or timestamps from the future. When corruption is detected, the system logs the incident, removes the corrupted data from Redis, and initializes a fresh bucket with default values.\n\nThe recovery strategy prioritizes safety over continuity - it's better to temporarily reset a client's rate limiting state than to operate with corrupted data that might allow unlimited requests or permanently block legitimate clients. The system provides administrative tools to manually inspect and repair bucket state when automated recovery is insufficient.\n\n**Clock Drift Correction** addresses gradual clock synchronization issues that can cause rate limiting inconsistencies across distributed instances. Unlike sudden clock jumps, clock drift accumulates slowly over time and can be corrected proactively before it causes significant problems.\n\nThe correction mechanism involves periodic synchronization checks where instances compare their local time with Redis server time or an external time authority. When drift is detected beyond acceptable thresholds, the system gradually adjusts its token calculation parameters to compensate for the difference rather than making sudden corrections that could cause dramatic changes in rate limiting behavior.\n\n#### Memory and Resource Recovery\n\n**Emergency Memory Management** activates when the rate limiter detects memory pressure that threatens system stability. The emergency procedures prioritize maintaining basic rate limiting functionality while aggressively reducing memory usage through bucket eviction and configuration changes.\n\nThe emergency response includes immediately halting creation of new client buckets, implementing aggressive LRU eviction of existing buckets, and temporarily switching to simpler rate limiting algorithms that require less memory per client. The system also reduces bucket retention times and cleanup intervals to accelerate memory reclamation.\n\nMemory recovery involves monitoring system memory usage and gradually restoring normal functionality as memory pressure subsides. The system implements hysteresis in memory management - emergency measures activate at higher memory usage thresholds than they deactivate, preventing oscillation between normal and emergency modes.\n\n**Resource Exhaustion Handling** addresses scenarios where CPU usage, file descriptor limits, or other system resources become constrained. The rate limiter must continue operating while reducing its resource footprint and potentially degrading performance rather than functionality.\n\nCPU exhaustion handling involves reducing the frequency of background tasks like bucket cleanup, simplifying rate limiting calculations, and potentially queuing rate limiting decisions during peak CPU usage periods. File descriptor exhaustion primarily affects Redis connections, requiring connection pooling optimization and aggressive connection recycling.\n\nThe recovery strategy monitors resource usage trends and implements predictive measures to prevent complete resource exhaustion. When resource usage approaches critical levels, the system proactively reduces its resource footprint and may temporarily reject new client registrations to preserve capacity for existing clients.\n\n#### Operational Recovery Procedures\n\n**Administrative Recovery Tools** provide operators with capabilities to manually intervene when automated recovery mechanisms are insufficient or when unusual circumstances require human judgment. These tools must be safe to use during production incidents and provide clear feedback about their impact on system state.\n\nThe toolset includes bucket inspection utilities that display current client rate limiting states, bucket reset commands that safely reinitialize corrupted or problematic client buckets, and configuration reload capabilities that allow rate limiting parameters to be adjusted without service restart. Emergency override commands allow operators to temporarily disable rate limiting for specific clients or endpoints during critical incidents.\n\nSafety features prevent accidental misuse of administrative tools, including confirmation prompts for destructive operations, audit logging of all administrative actions, and automatic rollback capabilities for configuration changes. The tools integrate with existing monitoring and alerting systems to provide visibility into their usage and impact.\n\n**Monitoring and Alerting Integration** ensures that rate limiting failures and recovery events are properly detected, escalated, and tracked. The monitoring strategy focuses on leading indicators that predict problems before they cause service impact, as well as lagging indicators that confirm the effectiveness of recovery actions.\n\nKey metrics include Redis connection health, rate limiting decision latency, local fallback bucket usage, memory consumption trends, and client bucket creation/cleanup rates. Alert thresholds are tuned to provide early warning of developing issues while minimizing false positives during normal operation variations.\n\nThe alerting strategy implements escalation procedures that automatically engage additional resources or trigger more aggressive recovery measures when initial responses are insufficient. Integration with incident management systems ensures that rate limiting issues are properly tracked and that post-incident reviews can identify opportunities for system improvement.\n\n### Implementation Guidance\n\nBuilding robust error handling for a distributed rate limiter requires careful attention to failure detection, recovery mechanisms, and operational observability. The implementation must balance performance with reliability, ensuring that error handling doesn't become a bottleneck during normal operation while providing comprehensive protection during failure scenarios.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Error Detection | Basic exception handling with logging | Structured error codes with metrics |\n| Circuit Breaker | Simple failure counting with timeouts | Netflix Hystrix-style adaptive thresholds |\n| Health Monitoring | Periodic connectivity checks | Continuous health scoring with trends |\n| Fallback Storage | In-memory dictionaries with size limits | Bounded LRU caches with TTL support |\n| Recovery Testing | Manual Redis reconnection attempts | Automated canary testing with gradual rollback |\n| Observability | Standard logging with error rates | Structured metrics with distributed tracing |\n\n#### Recommended File Structure\n\nThe error handling implementation should be organized to separate concerns while providing comprehensive coverage across all rate limiter components:\n\n```\nrate_limiter/\n├── error_handling/\n│   ├── __init__.py                 ← Error handling module exports\n│   ├── circuit_breaker.py          ← Circuit breaker implementation\n│   ├── fallback_manager.py         ← Local fallback bucket management\n│   ├── recovery_coordinator.py     ← Recovery and resynchronization logic\n│   ├── health_monitor.py           ← Component health monitoring\n│   └── error_types.py              ← Custom exception classes\n├── middleware/\n│   ├── error_middleware.py         ← HTTP error response handling\n│   └── health_endpoints.py         ← Health check HTTP endpoints\n├── storage/\n│   ├── redis_client_wrapper.py     ← Redis client with error handling\n│   └── connection_pool_manager.py  ← Connection pooling with failover\n├── monitoring/\n│   ├── metrics_collector.py        ← Error and performance metrics\n│   └── alert_manager.py            ← Alert generation and escalation\n└── admin/\n    ├── recovery_tools.py           ← Administrative recovery utilities\n    └── diagnostic_tools.py         ← System state inspection tools\n```\n\n#### Circuit Breaker Infrastructure Code\n\nHere's a complete circuit breaker implementation that can be used throughout the rate limiting system:\n\n```python\nimport time\nimport threading\nfrom enum import Enum\nfrom typing import Callable, Any, Optional\nfrom dataclasses import dataclass\n\nclass CircuitBreakerState(Enum):\n    CLOSED = \"closed\"\n    OPEN = \"open\"\n    HALF_OPEN = \"half_open\"\n\n@dataclass\nclass CircuitBreakerConfig:\n    failure_threshold: int = 5\n    recovery_timeout: float = 60.0\n    success_threshold: int = 3\n    timeout_duration: float = 10.0\n\nclass CircuitBreaker:\n    \"\"\"\n    Circuit breaker implementation for Redis and other external dependencies.\n    Automatically opens on repeated failures and tests for recovery.\n    \"\"\"\n    \n    def __init__(self, config: CircuitBreakerConfig):\n        self.config = config\n        self.state = CircuitBreakerState.CLOSED\n        self.failure_count = 0\n        self.success_count = 0\n        self.last_failure_time = 0.0\n        self.lock = threading.Lock()\n    \n    def call(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"\n        Execute function with circuit breaker protection.\n        Raises CircuitBreakerOpenException when circuit is open.\n        \"\"\"\n        with self.lock:\n            if self.state == CircuitBreakerState.OPEN:\n                if time.time() - self.last_failure_time < self.config.recovery_timeout:\n                    raise CircuitBreakerOpenException(\"Circuit breaker is open\")\n                else:\n                    self.state = CircuitBreakerState.HALF_OPEN\n                    self.success_count = 0\n        \n        try:\n            result = func(*args, **kwargs)\n            self._record_success()\n            return result\n        except Exception as e:\n            self._record_failure()\n            raise\n    \n    def _record_success(self):\n        with self.lock:\n            if self.state == CircuitBreakerState.HALF_OPEN:\n                self.success_count += 1\n                if self.success_count >= self.config.success_threshold:\n                    self.state = CircuitBreakerState.CLOSED\n                    self.failure_count = 0\n            elif self.state == CircuitBreakerState.CLOSED:\n                self.failure_count = max(0, self.failure_count - 1)\n    \n    def _record_failure(self):\n        with self.lock:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            \n            if self.failure_count >= self.config.failure_threshold:\n                self.state = CircuitBreakerState.OPEN\n\nclass CircuitBreakerOpenException(Exception):\n    pass\n```\n\n#### Health Monitor Infrastructure Code\n\nThis complete health monitoring system tracks component health and triggers recovery actions:\n\n```python\nimport time\nimport threading\nfrom typing import Dict, List, Callable, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass HealthStatus(Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass HealthCheckResult:\n    status: HealthStatus\n    response_time_ms: float\n    error_message: Optional[str] = None\n    metadata: Optional[Dict] = None\n\n@dataclass\nclass ComponentHealth:\n    name: str\n    status: HealthStatus\n    last_check_time: float\n    consecutive_failures: int\n    consecutive_successes: int\n    average_response_time: float\n\nclass HealthMonitor:\n    \"\"\"\n    Comprehensive health monitoring for rate limiter components.\n    Tracks Redis, memory usage, bucket operations, and triggers recovery.\n    \"\"\"\n    \n    def __init__(self):\n        self.components: Dict[str, ComponentHealth] = {}\n        self.health_checks: Dict[str, Callable] = {}\n        self.recovery_handlers: Dict[str, List[Callable]] = {}\n        self.lock = threading.Lock()\n        self.monitoring_thread: Optional[threading.Thread] = None\n        self.running = False\n    \n    def register_health_check(self, component_name: str, check_func: Callable):\n        \"\"\"Register a health check function for a component.\"\"\"\n        self.health_checks[component_name] = check_func\n        with self.lock:\n            if component_name not in self.components:\n                self.components[component_name] = ComponentHealth(\n                    name=component_name,\n                    status=HealthStatus.UNKNOWN,\n                    last_check_time=0.0,\n                    consecutive_failures=0,\n                    consecutive_successes=0,\n                    average_response_time=0.0\n                )\n    \n    def register_recovery_handler(self, component_name: str, handler: Callable):\n        \"\"\"Register a recovery handler to be called when component becomes unhealthy.\"\"\"\n        if component_name not in self.recovery_handlers:\n            self.recovery_handlers[component_name] = []\n        self.recovery_handlers[component_name].append(handler)\n    \n    def start_monitoring(self, check_interval: float = 30.0):\n        \"\"\"Start background health monitoring.\"\"\"\n        if self.running:\n            return\n        \n        self.running = True\n        self.monitoring_thread = threading.Thread(\n            target=self._monitoring_loop,\n            args=(check_interval,),\n            daemon=True\n        )\n        self.monitoring_thread.start()\n    \n    def stop_monitoring(self):\n        \"\"\"Stop background health monitoring.\"\"\"\n        self.running = False\n        if self.monitoring_thread:\n            self.monitoring_thread.join()\n    \n    def get_system_health(self) -> Dict[str, ComponentHealth]:\n        \"\"\"Get current health status of all monitored components.\"\"\"\n        with self.lock:\n            return self.components.copy()\n    \n    def _monitoring_loop(self, check_interval: float):\n        \"\"\"Background monitoring loop.\"\"\"\n        while self.running:\n            for component_name in self.health_checks:\n                try:\n                    self._check_component_health(component_name)\n                except Exception as e:\n                    print(f\"Health check error for {component_name}: {e}\")\n            \n            time.sleep(check_interval)\n    \n    def _check_component_health(self, component_name: str):\n        \"\"\"Perform health check for a specific component.\"\"\"\n        check_func = self.health_checks[component_name]\n        start_time = time.time()\n        \n        try:\n            result = check_func()\n            response_time = (time.time() - start_time) * 1000\n            self._update_component_health(component_name, result, response_time)\n        except Exception as e:\n            error_result = HealthCheckResult(\n                status=HealthStatus.UNHEALTHY,\n                response_time_ms=(time.time() - start_time) * 1000,\n                error_message=str(e)\n            )\n            self._update_component_health(component_name, error_result, error_result.response_time_ms)\n    \n    def _update_component_health(self, component_name: str, result: HealthCheckResult, response_time: float):\n        \"\"\"Update component health based on check result.\"\"\"\n        with self.lock:\n            component = self.components[component_name]\n            component.last_check_time = time.time()\n            \n            # Update response time average\n            if component.average_response_time == 0:\n                component.average_response_time = response_time\n            else:\n                component.average_response_time = (component.average_response_time * 0.8 + response_time * 0.2)\n            \n            # Update health status and counters\n            previous_status = component.status\n            component.status = result.status\n            \n            if result.status == HealthStatus.HEALTHY:\n                component.consecutive_successes += 1\n                component.consecutive_failures = 0\n            else:\n                component.consecutive_failures += 1\n                component.consecutive_successes = 0\n                \n                # Trigger recovery handlers if component became unhealthy\n                if previous_status != HealthStatus.UNHEALTHY and result.status == HealthStatus.UNHEALTHY:\n                    self._trigger_recovery_handlers(component_name, result)\n    \n    def _trigger_recovery_handlers(self, component_name: str, result: HealthCheckResult):\n        \"\"\"Trigger registered recovery handlers for unhealthy component.\"\"\"\n        handlers = self.recovery_handlers.get(component_name, [])\n        for handler in handlers:\n            try:\n                handler(component_name, result)\n            except Exception as e:\n                print(f\"Recovery handler error for {component_name}: {e}\")\n```\n\n#### Core Error Handling Skeleton Code\n\nThe main error handling coordinator that integrates with all rate limiter components:\n\n```python\nclass RateLimitErrorHandler:\n    \"\"\"\n    Central error handling coordinator for the rate limiting system.\n    Manages fallback strategies, recovery procedures, and degradation levels.\n    \"\"\"\n    \n    def __init__(self, config: DistributedRateLimitConfig):\n        self.config = config\n        self.circuit_breaker = CircuitBreaker(CircuitBreakerConfig())\n        self.health_monitor = HealthMonitor()\n        self.fallback_manager = None  # Initialize in setup\n        self.current_degradation_level = 0\n        \n    def setup_error_handling(self):\n        \"\"\"Initialize error handling components and monitoring.\"\"\"\n        # TODO 1: Initialize fallback bucket manager with local storage\n        # TODO 2: Register health checks for Redis, memory, and bucket operations\n        # TODO 3: Register recovery handlers for each component failure type\n        # TODO 4: Start background health monitoring\n        # TODO 5: Set up metrics collection for error rates and recovery times\n        pass\n    \n    def handle_redis_error(self, operation: str, error: Exception) -> bool:\n        \"\"\"Handle Redis operation errors and determine fallback strategy.\"\"\"\n        # TODO 1: Log error with context (operation type, client_id, timestamp)\n        # TODO 2: Update circuit breaker with failure information\n        # TODO 3: Check if fallback buckets should be activated\n        # TODO 4: Update degradation level based on error frequency\n        # TODO 5: Return True if operation should be retried, False for fallback\n        pass\n    \n    def handle_memory_pressure(self, current_usage: float, threshold: float):\n        \"\"\"Handle memory pressure by implementing emergency cleanup.\"\"\"\n        # TODO 1: Calculate severity level based on usage vs threshold\n        # TODO 2: Trigger aggressive bucket cleanup for stale entries\n        # TODO 3: Temporarily halt new bucket creation if critical\n        # TODO 4: Implement LRU eviction for existing buckets\n        # TODO 5: Alert monitoring systems about memory pressure event\n        pass\n    \n    def handle_clock_drift(self, detected_drift: float, max_allowed: float):\n        \"\"\"Handle clock synchronization issues in distributed environment.\"\"\"\n        # TODO 1: Validate drift amount against acceptable thresholds\n        # TODO 2: Implement gradual correction to avoid sudden rate changes\n        # TODO 3: Log drift detection for operational awareness\n        # TODO 4: Update bucket timestamps to compensate for drift\n        # TODO 5: Consider switching to monotonic time if drift is severe\n        pass\n    \n    def attempt_recovery(self, component_name: str, max_attempts: int = 3) -> bool:\n        \"\"\"Attempt to recover failed component with exponential backoff.\"\"\"\n        # TODO 1: Check if component is eligible for recovery attempt\n        # TODO 2: Implement exponential backoff between recovery attempts\n        # TODO 3: Test component functionality before declaring recovery\n        # TODO 4: Update component health status based on recovery result\n        # TODO 5: Return True if recovery successful, False otherwise\n        pass\n    \n    def get_fallback_bucket_config(self, original_config: TokenBucketConfig) -> TokenBucketConfig:\n        \"\"\"Calculate conservative fallback bucket configuration.\"\"\"\n        # TODO 1: Reduce capacity to account for lack of distributed coordination\n        # TODO 2: Adjust refill rate based on expected number of server instances\n        # TODO 3: Apply safety margin to prevent aggregate over-limiting\n        # TODO 4: Ensure configuration remains reasonable for single instance\n        # TODO 5: Return modified configuration for local fallback use\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing comprehensive error handling, verify the system's resilience with these tests:\n\n**Basic Error Handling Verification:**\n```bash\n# Start rate limiter with Redis dependency\npython -m rate_limiter.server --redis-url=redis://localhost:6379\n\n# Test circuit breaker by stopping Redis\ndocker stop redis-container\n\n# Send test requests - should switch to local fallback\ncurl -H \"X-API-Key: test-key\" http://localhost:8080/api/test\n\n# Check health endpoint\ncurl http://localhost:8080/health\n```\n\n**Expected behavior:** Requests continue to be rate-limited using local fallback buckets with conservative limits. Health endpoint shows Redis as unhealthy but overall system as degraded rather than failed.\n\n**Recovery Testing:**\n```bash\n# Restart Redis\ndocker start redis-container\n\n# Monitor logs for automatic recovery\ntail -f rate_limiter.log | grep -i recovery\n\n# Send more test requests\ncurl -H \"X-API-Key: test-key\" http://localhost:8080/api/test\n\n# Verify return to distributed rate limiting\n```\n\n**Expected behavior:** System automatically detects Redis recovery, gradually transitions back to distributed rate limiting, and health endpoint shows all components healthy.\n\n#### Common Implementation Pitfalls\n\n⚠️ **Pitfall: Failing Open During Error Conditions**\nMany implementations disable rate limiting entirely when errors occur, defeating the primary purpose of protection. Instead, always fail conservatively - if you can't accurately track rate limits, apply more restrictive limits rather than none.\n\n⚠️ **Pitfall: Infinite Retry Loops**\nCircuit breaker implementations that don't properly track failure counts can get stuck in infinite retry loops. Ensure failure counting is atomic and that circuit state changes are properly synchronized.\n\n⚠️ **Pitfall: Memory Leaks in Fallback Mode**\nLocal fallback buckets created during Redis outages can accumulate indefinitely if not properly cleaned up. Implement strict memory limits and aggressive cleanup policies for fallback scenarios.\n\n⚠️ **Pitfall: Thundering Herd on Recovery**\nWhen Redis recovers, all server instances may simultaneously attempt to reconnect and synchronize state. Implement randomized delays and gradual state resynchronization to prevent overwhelming the recovered system.\n\n⚠️ **Pitfall: Clock Drift Accumulation**\nSmall clock differences between servers can accumulate over time to create significant rate limiting inconsistencies. Regularly synchronize with authoritative time sources and implement drift detection.\n\n\n## Testing Strategy\n\n> **Milestone(s):** All milestones - comprehensive testing validates functionality from basic token bucket implementation through distributed rate limiting across multiple server instances\n\nThink of testing a rate limiter like conducting a orchestra rehearsal before the big performance. Just as a conductor tests each section individually (strings, brass, woodwinds), then brings them together for ensemble pieces, and finally runs the complete symphony under performance conditions, we need to verify our rate limiter at multiple levels. Each component must perform correctly in isolation, work harmoniously with other components, and maintain accuracy under the stress of real-world load patterns.\n\nTesting a rate limiting system presents unique challenges because it involves time-based behavior, concurrency, distributed coordination, and performance requirements. Unlike testing a simple calculator where inputs and outputs are deterministic, rate limiting involves probabilistic behavior, timing precision, and system-level interactions that can only be validated through carefully designed test scenarios.\n\n### Mental Model: The Quality Assurance Pyramid\n\nImagine our testing strategy as a pyramid with four distinct levels, each serving a specific purpose in building confidence in our rate limiter:\n\n**Foundation Level (Unit Tests)**: Like testing individual musical instruments to ensure they're properly tuned, we verify each component works correctly in isolation. Token buckets generate and consume tokens accurately, client trackers manage bucket lifecycles properly, and middleware components handle HTTP interactions correctly.\n\n**Integration Level (Component Integration)**: Like testing small ensembles of instruments playing together, we verify that components communicate correctly. The middleware properly calls the client tracker, buckets are created and cleaned up as expected, and Redis operations maintain consistency.\n\n**System Level (End-to-End Testing)**: Like running through complete musical pieces, we test entire request flows from HTTP input through rate limiting decisions to final responses. This includes distributed scenarios where multiple servers coordinate through Redis.\n\n**Performance Level (Load Testing)**: Like testing the orchestra under the acoustic pressure of a full concert hall, we verify the rate limiter maintains accuracy and responsiveness under realistic production loads with high concurrency and distributed coordination.\n\n### Unit Test Coverage\n\nUnit testing for rate limiting requires isolating each component and verifying its behavior across all possible states and edge conditions. The challenge lies in testing time-based algorithms where token generation depends on elapsed time, and concurrent scenarios where multiple threads access shared state.\n\n#### Token Bucket Algorithm Testing\n\nThe `TokenBucket` class requires comprehensive testing of its core time-based algorithm, including token generation, consumption, and overflow handling. Each test must control time progression to ensure deterministic behavior.\n\n| Test Scenario | Purpose | Key Assertions | Time Control Required |\n|---------------|---------|----------------|----------------------|\n| Initial bucket state | Verify correct initialization | `tokens_remaining == initial_tokens`, `capacity` and `refill_rate` match config | No |\n| Token generation over time | Validate refill algorithm accuracy | Tokens increase at correct rate, capped at capacity | Mock time progression |\n| Basic token consumption | Test successful token deduction | `tokens_remaining` decreases, `allowed == true` | No |\n| Insufficient tokens | Validate rejection behavior | `allowed == false`, tokens unchanged, correct `retry_after_seconds` | No |\n| Burst consumption | Test consuming up to capacity | Can consume all tokens at once, subsequent requests denied | No |\n| Overflow prevention | Ensure tokens never exceed capacity | Tokens cap at capacity despite extended refill time | Mock extended time |\n| Fractional token generation | Test precision with small rates | Accurate token accumulation with rates like 0.1 tokens/second | Mock precise time increments |\n| Clock drift handling | Test behavior with time anomalies | Graceful handling of negative time deltas, clock jumps | Mock time anomalies |\n| Concurrent access | Verify thread safety | No race conditions, accurate token counts under concurrent load | Multi-threading |\n| Configuration validation | Test invalid parameters | Appropriate exceptions for negative rates, zero capacity | No |\n\n**Critical Token Bucket Test Scenarios**:\n\n1. **Precision Testing**: Token generation must remain accurate over extended periods. A bucket with 1 token/second rate should have exactly 3600 tokens available after one hour (if capacity allows), not 3599 or 3601 due to floating-point accumulation errors.\n\n2. **Burst Validation**: A bucket with 10 tokens capacity and 1 token/second rate should allow consuming all 10 tokens immediately, then deny requests for 10 seconds, then allow 1 token consumption every second thereafter.\n\n3. **Time Anomaly Handling**: When system clock jumps backward (daylight saving time, NTP correction), the bucket should not add negative tokens or enter invalid states. When clock jumps forward significantly, tokens should cap at bucket capacity.\n\n4. **Thread Safety Verification**: Multiple threads simultaneously calling `try_consume` should never result in negative token counts, tokens exceeding capacity, or inconsistent bucket state. Use stress testing with 100+ concurrent threads.\n\n#### Client Bucket Tracker Testing\n\nThe `ClientBucketTracker` manages bucket lifecycles, cleanup operations, and client identification. Testing must verify memory management, concurrent access patterns, and cleanup effectiveness.\n\n| Test Scenario | Purpose | Key Validation Points | Concurrency Level |\n|---------------|---------|----------------------|------------------|\n| Bucket creation | Verify on-demand bucket instantiation | Correct config resolution, proper initialization | Single-threaded |\n| Bucket reuse | Test bucket caching behavior | Same client gets same bucket instance | Single-threaded |\n| Client identification | Validate ID extraction and normalization | Consistent IDs for same client, different IDs for different clients | Single-threaded |\n| Configuration resolution | Test hierarchical config override | Client overrides beat defaults, endpoint overrides beat client | Single-threaded |\n| Stale bucket detection | Verify cleanup candidate identification | Correctly identifies buckets older than threshold | Single-threaded |\n| Cleanup execution | Test bucket removal process | Removes stale buckets, preserves active ones, updates statistics | Single-threaded |\n| Concurrent bucket access | Test thread-safe bucket creation/access | No duplicate buckets for same client, thread-safe statistics | High concurrency |\n| Memory pressure handling | Test behavior under resource constraints | LRU eviction works correctly, memory usage stays bounded | Resource-constrained |\n| Client ID validation | Test malformed client identifier handling | Rejects invalid IPs, malformed API keys, empty identifiers | Single-threaded |\n| Configuration hot-reload | Test dynamic config updates | New buckets use updated config, existing buckets continue with old config | Single-threaded |\n\n**Critical Client Tracker Test Scenarios**:\n\n1. **Memory Leak Prevention**: Create buckets for 10,000 unique clients, wait for cleanup interval, verify that stale buckets are removed and memory usage returns to baseline levels. Monitor for gradual memory growth indicating cleanup failures.\n\n2. **Concurrent Client Onboarding**: Have 50 threads simultaneously make first requests for unique clients. Verify that exactly one bucket gets created per client (no duplicates) and all buckets have correct configuration.\n\n3. **Configuration Precedence**: Test complex scenarios where client has override, endpoint has override, and global defaults exist. Verify correct hierarchical resolution: endpoint-specific > client-specific > global defaults.\n\n#### HTTP Middleware Component Testing\n\nThe middleware integration requires testing HTTP-specific behavior, header handling, and framework integration without depending on actual web frameworks running.\n\n| Test Component | Test Focus | Mock Requirements | Validation Points |\n|----------------|------------|-------------------|------------------|\n| Request parsing | Client ID extraction from various sources | Mock HTTP request objects | Correct client ID extracted from IP, headers, API keys |\n| Rate limit checking | Integration with client tracker | Mock `ClientBucketTracker` | Correct bucket retrieval, token consumption |\n| Response building | HTTP headers and status codes | Mock HTTP response objects | Proper 429 status, rate limit headers, Retry-After calculation |\n| Endpoint normalization | Path and method processing | Mock request data | Consistent endpoint identification across variations |\n| Skip logic | Bypass conditions | Mock requests with special headers | Correctly identifies requests to skip |\n| Error handling | Invalid client IDs, tracker failures | Mock error conditions | Graceful degradation, appropriate error responses |\n| Configuration loading | Environment variable processing | Mock environment | Correct config parsing, validation, defaults |\n\n**Critical Middleware Test Scenarios**:\n\n1. **Header Completeness**: Every response (both successful and rate-limited) must include complete rate limiting headers: `X-RateLimit-Limit`, `X-RateLimit-Remaining`, `X-RateLimit-Reset`. Verify header values are accurate and consistent.\n\n2. **Client Identification Robustness**: Test with malformed IPs (`999.999.999.999`), missing API key headers, empty custom headers. Middleware should either extract valid IDs or provide sensible fallbacks without crashing.\n\n3. **Endpoint Normalization**: Requests to `/api/users/123` and `/api/users/456` should be treated as the same endpoint `/api/users/{id}` for rate limiting purposes. Test path parameter normalization and query string handling.\n\n### Integration and End-to-End Testing\n\nIntegration testing validates that components work together correctly, while end-to-end testing verifies complete request flows behave as expected under realistic conditions. These tests catch issues that unit tests miss: component interface mismatches, timing dependencies, and emergent system behaviors.\n\n#### Component Integration Testing\n\nIntegration tests focus on the boundaries between components, ensuring data flows correctly and contracts are honored. Unlike unit tests that mock dependencies, integration tests use real component instances working together.\n\n| Integration Boundary | Test Scenarios | Success Criteria | Failure Modes to Test |\n|---------------------|----------------|------------------|----------------------|\n| Middleware ↔ Client Tracker | Rate limiting requests through middleware | Correct bucket creation, token consumption, response headers | Client tracker failures, invalid client IDs |\n| Client Tracker ↔ Token Buckets | Bucket lifecycle management | Proper bucket instantiation, cleanup, configuration application | Memory leaks, stale bucket accumulation |\n| Token Bucket ↔ Time System | Time-based token generation | Accurate refill rates, overflow handling | Clock drift, time jumps, precision loss |\n| Middleware ↔ HTTP Framework | Request/response processing | Proper header extraction, response building | Malformed requests, framework version differences |\n| Distributed ↔ Redis | Shared state coordination | Consistent token counts across servers, atomic operations | Redis connection failures, Lua script errors |\n\n**Critical Integration Scenarios**:\n\n1. **Request Flow Accuracy**: Send 100 requests at exactly the rate limit (e.g., 10 requests/second for a 10 RPS limit) and verify that all requests are allowed with no false denials. Then send 150 requests in the same time period and verify that exactly 100 are allowed and 50 are denied.\n\n2. **Bucket Lifecycle Integration**: Create buckets through middleware requests, verify they appear in client tracker, wait for cleanup interval, confirm stale buckets are removed but active ones persist. Test requires coordinated timing across components.\n\n3. **Configuration Consistency**: Change rate limit configuration and verify that new buckets use updated settings while existing buckets continue with their original configuration until cleanup. Test configuration propagation across component boundaries.\n\n#### End-to-End Request Flow Testing\n\nEnd-to-end tests validate complete request journeys from HTTP input to final response, ensuring the rate limiter behaves correctly from a client perspective. These tests should simulate realistic client interaction patterns.\n\n| Test Scenario | Request Pattern | Expected Behavior | Measurement Points |\n|---------------|----------------|-------------------|-------------------|\n| Normal traffic within limits | Steady requests at 80% of rate limit | All requests allowed, proper headers | Response times, header accuracy |\n| Burst traffic | 5x rate limit for 2 seconds, then normal | Initial burst allowed up to capacity, then throttling | Burst handling, recovery time |\n| Multiple clients | 3 clients at different rates | Independent rate limiting, no interference | Per-client accuracy, resource usage |\n| Mixed endpoints | Same client hitting different endpoints | Endpoint-specific limits applied correctly | Configuration resolution, isolation |\n| Long-running client | Single client over 1 hour | Consistent rate limiting, no drift | Long-term accuracy, memory stability |\n| Client reconnection | Client stops, resumes after cleanup interval | New bucket created, fresh token allocation | Cleanup effectiveness, state reset |\n| Configuration changes | Update limits during active traffic | Existing clients unaffected, new clients get updated limits | Hot reload behavior, transition smoothness |\n\n**Critical End-to-End Scenarios**:\n\n1. **Rate Limit Accuracy Under Load**: Run a client that makes exactly 600 requests over 60 seconds (10 RPS) against a 10 RPS limit. Verify that 590-600 requests are allowed (accounting for timing precision) and measure the standard deviation of inter-request timing to ensure smooth rate limiting rather than bursty approval.\n\n2. **Multi-Client Isolation**: Run 5 clients simultaneously, each with different rate limits (1, 5, 10, 20, 50 RPS). Verify that each client achieves its expected throughput without interference from others. Measure cross-client impact on response times and accuracy.\n\n3. **Recovery After Burst**: Client sends 100 requests instantly to a 10 RPS limit with 20 token capacity. Verify that first 20 requests are allowed immediately, remaining 80 are denied with correct `Retry-After` headers, and subsequent requests are allowed at exactly 10 RPS starting from the appropriate time.\n\n#### Distributed Coordination Testing\n\nDistributed rate limiting introduces additional complexity requiring tests that verify consistency across multiple server instances sharing state through Redis.\n\n| Distributed Scenario | Test Setup | Consistency Requirements | Validation Method |\n|---------------------|------------|-------------------------|-------------------|\n| Multi-server consistency | 3 servers, shared Redis, same client | Total allowed requests ≤ rate limit across all servers | Request logging, aggregate counting |\n| Redis failure handling | 1 server, Redis disconnect/reconnect | Graceful fallback, recovery to consistent state | Error monitoring, state verification |\n| Concurrent server operations | 10 servers, high concurrent load | No double-counting, accurate token deduction | Lua script verification, Redis monitoring |\n| Network partition | 2 servers, Redis accessible to only 1 | Isolated server falls back appropriately | Fallback behavior monitoring |\n| Clock synchronization | Servers with different system times | Consistent token generation rates | Time drift impact measurement |\n| Redis performance | High request rate, Redis latency | Maintained accuracy despite Redis delays | Latency monitoring, accuracy tracking |\n\n**Critical Distributed Scenarios**:\n\n1. **Cross-Server Consistency**: Deploy 3 server instances sharing Redis. Send 300 requests distributed across all servers for a client with 100 RPS limit over 1 second. Verify that exactly ~100 requests are allowed total, regardless of which server processed each request.\n\n2. **Redis Failover Behavior**: During active traffic, disconnect Redis for 30 seconds, then reconnect. Verify that servers fall back to local buckets with conservative limits, maintain basic protection, and seamlessly return to distributed coordination when Redis recovers.\n\n3. **Atomic Operation Validation**: Under high concurrency (1000+ requests/second across multiple servers), verify that token bucket operations remain atomic and accurate. No tokens should be double-counted or lost due to race conditions in Lua script execution.\n\n### Milestone Verification Checkpoints\n\nEach project milestone requires specific verification steps to ensure the implementation meets acceptance criteria before proceeding to the next milestone. These checkpoints provide concrete validation that core functionality works correctly.\n\n#### Milestone 1: Token Bucket Implementation Checkpoint\n\nAfter implementing the core token bucket algorithm, verify the following behaviors through automated tests and manual validation.\n\n**Automated Test Requirements:**\n\n| Test Category | Required Tests | Pass Criteria | Command to Run |\n|---------------|----------------|---------------|----------------|\n| Basic functionality | Token generation, consumption, overflow | All assertions pass, no timing flakiness | `python -m pytest tests/test_token_bucket.py::TestBasicFunctionality -v` |\n| Thread safety | Concurrent access under load | No race conditions, accurate final state | `python -m pytest tests/test_token_bucket.py::TestConcurrency -v --timeout=30` |\n| Time precision | Long-running accuracy tests | <1% deviation from expected token counts | `python -m pytest tests/test_token_bucket.py::TestTimePrecision -v` |\n| Edge cases | Clock drift, overflow, invalid config | Graceful handling, appropriate exceptions | `python -m pytest tests/test_token_bucket.py::TestEdgeCases -v` |\n\n**Manual Verification Commands:**\n\n```python\n# Test 1: Basic token bucket behavior\nfrom token_bucket import TokenBucket, TokenBucketConfig\n\nconfig = TokenBucketConfig(capacity=10, refill_rate=1.0, initial_tokens=5)\nbucket = TokenBucket(config)\n\n# Should have 5 tokens initially\nresult = bucket.try_consume(3)\nprint(f\"Consumed 3 tokens: allowed={result.allowed}, remaining={result.tokens_remaining}\")\n# Expected: allowed=True, remaining=2\n\n# Should deny request for more tokens than available\nresult = bucket.try_consume(5)\nprint(f\"Tried to consume 5 tokens: allowed={result.allowed}, retry_after={result.retry_after_seconds}\")\n# Expected: allowed=False, retry_after=3.0\n\n# Wait and verify token refill\nimport time\ntime.sleep(2.0)\nresult = bucket.try_consume(1)\nprint(f\"After 2 seconds, consumed 1 token: allowed={result.allowed}, remaining={result.tokens_remaining}\")\n# Expected: allowed=True, remaining=~3 (2 original + 2 refilled - 1 consumed)\n```\n\n**Success Indicators:**\n- Token counts are accurate within 1% over extended periods\n- Concurrent access produces consistent results across multiple runs  \n- Bucket capacity is never exceeded regardless of refill time\n- `retry_after_seconds` calculations are accurate within 100ms\n- No memory leaks or resource accumulation during long-running tests\n\n**Common Failure Modes to Check:**\n- Tokens accumulating beyond bucket capacity\n- Negative token counts under concurrent access\n- Significant drift in token generation rates over time\n- Crashes or exceptions during normal operation\n- Thread deadlocks or race conditions under load\n\n#### Milestone 2: Per-Client Rate Limiting Checkpoint\n\nAfter implementing client-specific bucket management, verify that clients are properly isolated and buckets are managed efficiently.\n\n**Automated Test Requirements:**\n\n| Test Category | Required Tests | Pass Criteria | Validation Focus |\n|---------------|----------------|---------------|------------------|\n| Client isolation | Multiple clients with different limits | Each client gets correct rate limit, no cross-contamination | `python -m pytest tests/test_client_tracker.py::TestClientIsolation -v` |\n| Bucket lifecycle | Creation, access, cleanup | Buckets created on-demand, cleaned up when stale | `python -m pytest tests/test_client_tracker.py::TestBucketLifecycle -v` |\n| Memory management | Long-running with many clients | Memory usage remains bounded, no leaks | `python -m pytest tests/test_client_tracker.py::TestMemoryManagement -v --timeout=60` |\n| Configuration resolution | Client overrides, endpoint limits | Correct config hierarchy applied | `python -m pytest tests/test_client_tracker.py::TestConfigResolution -v` |\n\n**Manual Verification Commands:**\n\n```python\n# Test 1: Multiple clients get independent rate limits\nfrom client_tracker import ClientBucketTracker\nfrom config import RateLimitConfig\n\nconfig = RateLimitConfig(\n    default_limits=TokenBucketConfig(capacity=10, refill_rate=1.0),\n    client_overrides={\"premium_client\": TokenBucketConfig(capacity=50, refill_rate=5.0)}\n)\ntracker = ClientBucketTracker(config)\n\n# Test default client\nbucket1 = tracker.get_bucket_for_client(\"192.168.1.100\")\nresult1 = bucket1.try_consume(10)\nprint(f\"Default client consumed 10 tokens: allowed={result1.allowed}\")\n# Expected: allowed=True (uses default 10 capacity)\n\n# Test premium client  \nbucket2 = tracker.get_bucket_for_client(\"premium_client\")\nresult2 = bucket2.try_consume(30)\nprint(f\"Premium client consumed 30 tokens: allowed={result2.allowed}\")\n# Expected: allowed=True (uses 50 capacity override)\n\n# Verify they're different buckets\nresult3 = bucket1.try_consume(1)\nprint(f\"Default client (should be empty): allowed={result3.allowed}\")\n# Expected: allowed=False (already consumed 10 from 10 capacity)\n```\n\n**Success Indicators:**\n- Each unique client ID gets its own bucket instance\n- Client configuration overrides are applied correctly  \n- Stale buckets are removed after cleanup interval\n- Memory usage stabilizes even with thousands of clients\n- Bucket access is thread-safe under high concurrency\n\n**Troubleshooting Guide:**\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|--------------|------------------|-----|\n| All clients share same limits | Config overrides not working | Check `resolve_bucket_config` logic | Fix configuration resolution hierarchy |\n| Memory usage keeps growing | Stale bucket cleanup failing | Check cleanup thread execution, bucket timestamps | Fix cleanup logic, reduce cleanup interval |\n| Concurrent clients get errors | Race condition in bucket creation | Add logging around bucket creation | Add proper locking around bucket map access |\n| Client IDs inconsistent | ID extraction/normalization issues | Log raw client IDs before normalization | Fix `identify_client` method implementation |\n\n#### Milestone 3: HTTP Middleware Integration Checkpoint\n\nAfter implementing HTTP middleware, verify proper integration with web frameworks and correct HTTP protocol behavior.\n\n**Automated Test Requirements:**\n\n| Test Category | Required Tests | Pass Criteria | Integration Level |\n|---------------|----------------|---------------|-------------------|\n| HTTP protocol compliance | Status codes, headers, response format | Correct 429 responses, complete rate limit headers | `python -m pytest tests/test_middleware.py::TestHTTPCompliance -v` |\n| Framework integration | Flask/Django/FastAPI compatibility | Middleware integrates without framework modifications | `python -m pytest tests/test_middleware.py::TestFrameworkIntegration -v` |\n| Request processing | Various client ID sources | Correct client identification from IPs, headers, API keys | `python -m pytest tests/test_middleware.py::TestRequestProcessing -v` |\n| Endpoint handling | Different routes, path parameters | Proper endpoint normalization and rate limit application | `python -m pytest tests/test_middleware.py::TestEndpointHandling -v` |\n\n**Manual Verification Commands:**\n\nStart a test server with the rate limiting middleware enabled:\n\n```python\n# test_server.py\nfrom flask import Flask\nfrom rate_limit_middleware import RateLimitMiddleware\n\napp = Flask(__name__)\nrate_limiter = RateLimitMiddleware()\nrate_limiter.init_app(app)\n\n@app.route('/api/test')\ndef test_endpoint():\n    return {\"message\": \"success\"}\n\nif __name__ == '__main__':\n    app.run(port=5000, debug=True)\n```\n\nTest the middleware behavior using curl:\n\n```bash\n# Test 1: Normal request should succeed with rate limit headers\ncurl -v http://localhost:5000/api/test\n# Expected: HTTP 200, X-RateLimit-Limit header, X-RateLimit-Remaining header\n\n# Test 2: Exceed rate limit\nfor i in {1..15}; do curl -s -w \"%{http_code}\\n\" http://localhost:5000/api/test; done\n# Expected: First ~10 requests return 200, subsequent return 429\n\n# Test 3: Check 429 response format\ncurl -v http://localhost:5000/api/test  # (after exceeding limit)\n# Expected: HTTP 429, Retry-After header, JSON error body\n\n# Test 4: Different client should get independent rate limit\ncurl -v -H \"X-API-Key: different-client\" http://localhost:5000/api/test\n# Expected: HTTP 200 (fresh rate limit for this client)\n```\n\n**Success Indicators:**\n- All HTTP responses include proper rate limiting headers\n- 429 responses include accurate `Retry-After` header\n- Different endpoints can have different rate limits\n- Client identification works from multiple sources (IP, headers)\n- Middleware doesn't interfere with normal request processing\n\n#### Milestone 4: Distributed Rate Limiting Checkpoint\n\nAfter implementing Redis-based distributed coordination, verify consistency across multiple server instances.\n\n**Automated Test Requirements:**\n\n| Test Category | Required Tests | Pass Criteria | Distributed Complexity |\n|---------------|----------------|---------------|------------------------|\n| Cross-server consistency | Multiple instances, shared client | Total requests ≤ rate limit across all servers | `python -m pytest tests/test_distributed.py::TestConsistency -v` |\n| Redis integration | Lua scripts, atomic operations | No race conditions, accurate token counts | `python -m pytest tests/test_distributed.py::TestRedisIntegration -v` |\n| Failure handling | Redis disconnection, recovery | Graceful fallback, smooth recovery | `python -m pytest tests/test_distributed.py::TestFailureHandling -v` |\n| Performance impact | Latency, throughput under Redis load | <10ms p95 latency, minimal throughput impact | `python -m pytest tests/test_distributed.py::TestPerformance -v` |\n\n**Manual Verification Setup:**\n\nStart multiple server instances sharing the same Redis instance:\n\n```bash\n# Terminal 1: Start Redis\nredis-server --port 6379\n\n# Terminal 2: Start server instance 1\nREDIS_URL=redis://localhost:6379 FLASK_PORT=5001 python server.py\n\n# Terminal 3: Start server instance 2  \nREDIS_URL=redis://localhost:6379 FLASK_PORT=5002 python server.py\n\n# Terminal 4: Start server instance 3\nREDIS_URL=redis://localhost:6379 FLASK_PORT=5003 python server.py\n```\n\nTest distributed consistency:\n\n```bash\n# Test 1: Send requests to different servers for same client\nfor i in {1..5}; do curl -s -H \"X-Client-ID: test-client\" http://localhost:5001/api/test; done\nfor i in {1..5}; do curl -s -H \"X-Client-ID: test-client\" http://localhost:5002/api/test; done\nfor i in {1..5}; do curl -s -H \"X-Client-ID: test-client\" http://localhost:5003/api/test; done\n# Expected: Total allowed ≤ rate limit regardless of which server processed request\n\n# Test 2: Redis failover behavior\n# Stop Redis, send requests, restart Redis\nsudo systemctl stop redis  # or docker stop redis-container\ncurl -s -w \"%{http_code}\\n\" http://localhost:5001/api/test  # Should still work (fallback)\nsudo systemctl start redis\ncurl -s -w \"%{http_code}\\n\" http://localhost:5001/api/test  # Should resume distributed mode\n```\n\n**Success Indicators:**\n- Token consumption is consistent across all server instances\n- Redis connection failures trigger local fallback behavior  \n- Recovery to distributed mode happens automatically\n- Lua scripts execute atomically without race conditions\n- Performance impact is minimal (<10ms additional latency)\n\n### Performance and Load Testing\n\nPerformance testing validates that the rate limiter maintains accuracy and responsiveness under realistic production loads. Unlike functional testing that verifies correctness, performance testing focuses on behavior under stress, measuring throughput, latency, and accuracy degradation.\n\n#### Load Testing Methodology\n\nPerformance testing requires systematic measurement of rate limiting accuracy, response times, and system resource usage under various load patterns that simulate real-world traffic conditions.\n\n**Load Pattern Categories:**\n\n| Load Pattern | Description | Purpose | Key Metrics |\n|--------------|-------------|---------|-------------|\n| Steady state | Constant request rate at 80% of limit | Validate baseline performance | P95 latency, accuracy percentage, CPU/memory usage |\n| Burst traffic | 10x rate limit for short periods | Test burst handling and recovery | Burst accommodation, recovery time, queue depth |\n| Ramp-up | Gradually increasing request rate | Find performance breaking points | Throughput ceiling, accuracy degradation point |\n| Mixed clients | Multiple clients with different patterns | Test resource contention | Per-client accuracy, cross-client impact |\n| Long duration | Hours of continuous load | Detect memory leaks, drift issues | Memory stability, long-term accuracy |\n| Distributed load | Traffic across multiple server instances | Validate distributed coordination overhead | Cross-server consistency, Redis performance impact |\n\n#### Single-Instance Performance Testing\n\nBefore testing distributed scenarios, establish baseline performance characteristics for a single rate limiter instance to identify bottlenecks and capacity limits.\n\n**Critical Single-Instance Test Scenarios:**\n\n| Test Scenario | Configuration | Expected Performance | Measurement Method |\n|---------------|---------------|---------------------|-------------------|\n| High-throughput accuracy | 1000 RPS limit, 1200 RPS load | >99% accuracy, <5ms P95 latency | `wrk -t4 -c100 -d60s --rate=1200` |\n| Memory efficiency | 10,000 unique clients | <100MB memory usage, stable over time | Memory profiling over 30 minutes |\n| Concurrent client handling | 500 concurrent clients | Linear scalability, no contention bottlenecks | `ab -n10000 -c500` with unique client IDs |\n| Token precision under load | 100 RPS limit, sustained 24 hours | <0.1% drift from expected throughput | Long-running accuracy measurement |\n| Burst handling capacity | 10 RPS limit, 100 token capacity | Accommodate 100-request burst, recover in 10s | Burst injection, recovery timing |\n\n**Performance Test Implementation:**\n\n```python\n# performance_test.py - Example load testing script\nimport asyncio\nimport aiohttp\nimport time\nfrom collections import defaultdict\n\nclass RateLimiterLoadTest:\n    def __init__(self, base_url, target_rps, duration_seconds):\n        self.base_url = base_url\n        self.target_rps = target_rps\n        self.duration_seconds = duration_seconds\n        self.results = defaultdict(list)\n    \n    async def send_request(self, session, client_id):\n        \"\"\"Send single request and record response\"\"\"\n        start_time = time.time()\n        try:\n            async with session.get(\n                f\"{self.base_url}/api/test\",\n                headers={\"X-Client-ID\": client_id}\n            ) as response:\n                end_time = time.time()\n                self.results[response.status].append(end_time - start_time)\n                return response.status\n        except Exception as e:\n            end_time = time.time()\n            self.results[\"error\"].append(end_time - start_time)\n            return \"error\"\n    \n    async def run_load_test(self):\n        \"\"\"Execute load test with precise timing\"\"\"\n        request_interval = 1.0 / self.target_rps\n        connector = aiohttp.TCPConnector(limit=1000)\n        \n        async with aiohttp.ClientSession(connector=connector) as session:\n            start_time = time.time()\n            tasks = []\n            \n            while time.time() - start_time < self.duration_seconds:\n                task = asyncio.create_task(\n                    self.send_request(session, f\"client-{int(time.time()*1000) % 100}\")\n                )\n                tasks.append(task)\n                \n                await asyncio.sleep(request_interval)\n                \n                # Prevent task list from growing too large\n                if len(tasks) % 1000 == 0:\n                    await asyncio.gather(*tasks[:500])\n                    tasks = tasks[500:]\n            \n            # Wait for remaining tasks\n            await asyncio.gather(*tasks)\n    \n    def analyze_results(self):\n        \"\"\"Analyze performance metrics\"\"\"\n        total_requests = sum(len(responses) for responses in self.results.values())\n        success_rate = len(self.results[200]) / total_requests if total_requests > 0 else 0\n        \n        all_latencies = []\n        for latencies in self.results.values():\n            all_latencies.extend(latencies)\n        \n        if all_latencies:\n            all_latencies.sort()\n            p95_latency = all_latencies[int(len(all_latencies) * 0.95)]\n            avg_latency = sum(all_latencies) / len(all_latencies)\n        else:\n            p95_latency = avg_latency = 0\n        \n        print(f\"Total Requests: {total_requests}\")\n        print(f\"Success Rate: {success_rate:.2%}\")\n        print(f\"Average Latency: {avg_latency*1000:.2f}ms\")\n        print(f\"P95 Latency: {p95_latency*1000:.2f}ms\")\n        print(f\"Status Code Distribution: {dict(self.results)}\")\n\n# Usage\nasync def main():\n    test = RateLimiterLoadTest(\"http://localhost:5000\", target_rps=1000, duration_seconds=60)\n    await test.run_load_test()\n    test.analyze_results()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n**Performance Acceptance Criteria:**\n\n> **Critical Performance Requirements**: The rate limiter must maintain >99% accuracy (allowed requests within 1% of configured limit) while adding <10ms P95 latency overhead to normal request processing. Memory usage should remain stable under sustained load and scale linearly with the number of active clients.\n\n#### Distributed Performance Testing\n\nDistributed rate limiting introduces coordination overhead that must be measured under realistic multi-server scenarios. These tests verify that distributed consistency doesn't come at the cost of unacceptable performance degradation.\n\n**Distributed Load Test Architecture:**\n\n| Component | Role | Configuration | Monitoring Points |\n|-----------|------|---------------|-------------------|\n| Load generators | Generate traffic across all servers | 1000 RPS distributed evenly | Request distribution, client ID consistency |\n| Server instances | Process requests with distributed rate limiting | 3-5 instances, shared Redis | Individual server performance, coordination latency |\n| Redis cluster | Shared state storage | Single instance or cluster | Operation latency, connection pool usage |\n| Monitoring system | Collect metrics across all components | Prometheus/Grafana or custom | Cross-server consistency, aggregate throughput |\n\n**Critical Distributed Performance Scenarios:**\n\n| Scenario | Test Configuration | Success Criteria | Failure Indicators |\n|----------|-------------------|------------------|-------------------|\n| Cross-server consistency under load | 5 servers, 500 RPS each, same client | Total throughput ≤ 1000 RPS | >5% over-limit requests, token double-counting |\n| Redis coordination latency | Measure Redis operation times | <5ms P95 for Lua script execution | >20ms Redis latency, connection timeouts |\n| Failover performance | Redis disconnect during peak load | <10s fallback activation, graceful degradation | Extended unavailability, inconsistent fallback |\n| Recovery coordination | Redis reconnect after 60s outage | <30s to resume distributed mode | Split-brain behavior, state inconsistencies |\n| High client count distribution | 10,000 unique clients across servers | Linear scaling, no performance cliff | Memory explosion, coordination bottlenecks |\n\n**Distributed Consistency Validation:**\n\nThe most critical aspect of distributed performance testing is verifying that rate limiting remains accurate when requests for the same client are processed by different server instances.\n\n```python\n# distributed_consistency_test.py\nimport asyncio\nimport aiohttp\nimport time\nfrom collections import Counter\n\nclass DistributedConsistencyTest:\n    def __init__(self, server_urls, client_id, rate_limit_rps, test_duration):\n        self.server_urls = server_urls\n        self.client_id = client_id\n        self.rate_limit_rps = rate_limit_rps\n        self.test_duration = test_duration\n        self.results = []\n    \n    async def send_requests_to_servers(self):\n        \"\"\"Send requests to different servers for same client\"\"\"\n        async with aiohttp.ClientSession() as session:\n            start_time = time.time()\n            request_count = 0\n            \n            while time.time() - start_time < self.test_duration:\n                # Round-robin across servers\n                server_url = self.server_urls[request_count % len(self.server_urls)]\n                \n                try:\n                    async with session.get(\n                        f\"{server_url}/api/test\",\n                        headers={\"X-Client-ID\": self.client_id}\n                    ) as response:\n                        self.results.append({\n                            \"timestamp\": time.time(),\n                            \"server\": server_url,\n                            \"status\": response.status,\n                            \"headers\": dict(response.headers)\n                        })\n                except Exception as e:\n                    self.results.append({\n                        \"timestamp\": time.time(),\n                        \"server\": server_url,\n                        \"status\": \"error\",\n                        \"error\": str(e)\n                    })\n                \n                request_count += 1\n                # Send requests faster than rate limit to test consistency\n                await asyncio.sleep(0.8 / self.rate_limit_rps)  # 125% of rate limit\n    \n    def validate_consistency(self):\n        \"\"\"Validate that distributed rate limiting is consistent\"\"\"\n        success_count = len([r for r in self.results if r[\"status\"] == 200])\n        total_requests = len(self.results)\n        \n        # Calculate expected allowed requests (with some tolerance)\n        expected_allowed = self.rate_limit_rps * self.test_duration\n        tolerance = expected_allowed * 0.05  # 5% tolerance\n        \n        consistency_valid = (\n            success_count >= expected_allowed - tolerance and\n            success_count <= expected_allowed + tolerance\n        )\n        \n        server_distribution = Counter(r[\"server\"] for r in self.results if r[\"status\"] == 200)\n        \n        print(f\"Total requests: {total_requests}\")\n        print(f\"Successful requests: {success_count}\")\n        print(f\"Expected allowed: {expected_allowed:.0f} ± {tolerance:.0f}\")\n        print(f\"Consistency valid: {consistency_valid}\")\n        print(f\"Server distribution: {dict(server_distribution)}\")\n        \n        return consistency_valid\n\n# Usage for testing 3-server distributed setup\nasync def main():\n    servers = [\"http://localhost:5001\", \"http://localhost:5002\", \"http://localhost:5003\"]\n    test = DistributedConsistencyTest(servers, \"test-client\", rate_limit_rps=10, test_duration=60)\n    await test.send_requests_to_servers()\n    test.validate_consistency()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n#### Performance Regression Testing\n\nPerformance regression testing ensures that changes to the rate limiter don't introduce performance degradation. This requires establishing baseline metrics and automated detection of significant performance changes.\n\n**Performance Baseline Metrics:**\n\n| Metric | Baseline Value | Acceptable Degradation | Alert Threshold |\n|--------|---------------|------------------------|-----------------|\n| Request processing latency | <2ms P95 | <50% increase | >3ms P95 |\n| Rate limiting accuracy | >99.5% | <0.5% decrease | <99% |\n| Memory usage per client | <1KB | <100% increase | >2KB |\n| Redis operation latency | <1ms P95 | <100% increase | >2ms P95 |\n| Throughput capacity | 10,000 RPS | <20% decrease | <8,000 RPS |\n| CPU utilization | <10% at 1000 RPS | <50% increase | >15% |\n\n**Automated Performance Testing:**\n\nPerformance tests should run automatically in CI/CD pipelines to catch regressions before deployment. The test suite should include representative workloads and fail builds that significantly degrade performance.\n\n> **Performance Testing Philosophy**: Performance testing isn't just about finding the breaking point—it's about ensuring consistent, predictable behavior under normal operating conditions. A rate limiter that works perfectly at low load but becomes inaccurate or slow under realistic traffic is fundamentally broken for production use.\n\n### Implementation Guidance\n\nThe testing strategy requires a comprehensive test suite that grows with each milestone, providing both validation of functionality and confidence in system behavior under load.\n\n#### Technology Recommendations\n\n| Testing Component | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| Unit test framework | `pytest` with basic fixtures | `pytest` with `pytest-asyncio`, `pytest-benchmark`, custom fixtures |\n| HTTP testing | `requests` library with mock servers | `aiohttp` for async testing, `httpx` for HTTP/2 support |\n| Load testing | `locust` or `wrk` command line | Custom async load generators, distributed load testing |\n| Time mocking | `freezegun` or `time-machine` | Custom time providers with dependency injection |\n| Redis testing | `fakeredis` for unit tests | Real Redis instance with Docker, Redis Cluster testing |\n| Concurrency testing | `threading` with `concurrent.futures` | `asyncio` with proper event loop management |\n| Performance profiling | `cProfile` and `memory_profiler` | `py-spy`, `pympler`, APM integration |\n| CI/CD integration | GitHub Actions with basic test runs | Performance regression detection, benchmark comparisons |\n\n#### Recommended Test File Structure\n\nOrganize tests to mirror the component structure and provide clear separation between different types of testing:\n\n```\ntests/\n├── unit/                           # Isolated component tests\n│   ├── test_token_bucket.py        # Core algorithm tests\n│   ├── test_client_tracker.py      # Client management tests  \n│   ├── test_middleware.py          # HTTP middleware tests\n│   └── test_redis_operations.py    # Redis integration tests\n├── integration/                    # Component interaction tests\n│   ├── test_request_flow.py        # End-to-end request processing\n│   ├── test_distributed_coordination.py  # Multi-server scenarios\n│   └── test_configuration.py       # Config loading and resolution\n├── performance/                    # Load and performance tests\n│   ├── test_single_instance_perf.py    # Single server performance\n│   ├── test_distributed_perf.py        # Multi-server performance\n│   └── test_memory_usage.py            # Memory efficiency tests\n├── fixtures/                       # Shared test data and utilities\n│   ├── conftest.py                 # Pytest configuration and fixtures\n│   ├── mock_redis.py               # Redis testing utilities\n│   └── load_generators.py          # Performance testing tools\n└── milestone_verification/         # Milestone-specific validation\n    ├── milestone_1_checks.py       # Token bucket verification\n    ├── milestone_2_checks.py       # Per-client verification\n    ├── milestone_3_checks.py       # Middleware verification\n    └── milestone_4_checks.py       # Distributed verification\n```\n\n#### Core Test Infrastructure\n\nThe test infrastructure provides essential utilities for time manipulation, Redis testing, and load generation that all other tests depend on.\n\n**Time Control Infrastructure:**\n\n```python\n# tests/fixtures/time_control.py\nimport time\nfrom unittest.mock import patch\nfrom contextlib import contextmanager\n\nclass MockTimeProvider:\n    \"\"\"Controllable time provider for deterministic testing\"\"\"\n    \n    def __init__(self, start_time=1000000000.0):\n        self.current_time = start_time\n        self.time_calls = []\n    \n    def time(self):\n        \"\"\"Mock time.time() function\"\"\"\n        self.time_calls.append(self.current_time)\n        return self.current_time\n    \n    def advance(self, seconds):\n        \"\"\"Advance mock time by specified seconds\"\"\"\n        self.current_time += seconds\n    \n    def reset(self, new_time=1000000000.0):\n        \"\"\"Reset mock time to specified value\"\"\"\n        self.current_time = new_time\n        self.time_calls.clear()\n\n@contextmanager\ndef controlled_time(start_time=1000000000.0):\n    \"\"\"Context manager for time-controlled testing\"\"\"\n    time_provider = MockTimeProvider(start_time)\n    with patch('time.time', side_effect=time_provider.time):\n        yield time_provider\n\n# Usage in tests:\n# with controlled_time() as time_provider:\n#     bucket = TokenBucket(config)\n#     time_provider.advance(10.0)  # Advance 10 seconds\n#     result = bucket.try_consume(5)  # Test with controlled time\n```\n\n**Redis Testing Infrastructure:**\n\n```python\n# tests/fixtures/redis_testing.py\nimport redis\nimport fakeredis\nfrom contextlib import contextmanager\nfrom unittest.mock import patch\n\nclass RedisTestManager:\n    \"\"\"Manages Redis instances for testing\"\"\"\n    \n    def __init__(self):\n        self.fake_redis = None\n        self.real_redis = None\n    \n    def get_fake_redis(self):\n        \"\"\"Get in-memory fake Redis for fast unit tests\"\"\"\n        if self.fake_redis is None:\n            self.fake_redis = fakeredis.FakeRedis(decode_responses=True)\n        self.fake_redis.flushall()  # Clean state for each test\n        return self.fake_redis\n    \n    def get_real_redis(self):\n        \"\"\"Get real Redis connection for integration tests\"\"\"\n        if self.real_redis is None:\n            self.real_redis = redis.Redis(host='localhost', port=6379, decode_responses=True)\n        try:\n            self.real_redis.ping()\n            self.real_redis.flushdb()  # Clean test database\n            return self.real_redis\n        except redis.ConnectionError:\n            pytest.skip(\"Redis server not available for integration tests\")\n\n@contextmanager\ndef mock_redis_connection():\n    \"\"\"Mock Redis connections with fake Redis\"\"\"\n    fake_redis = RedisTestManager().get_fake_redis()\n    with patch('redis.Redis') as mock_redis_class:\n        mock_redis_class.return_value = fake_redis\n        yield fake_redis\n\n# Usage in tests:\n# with mock_redis_connection() as redis_client:\n#     rate_limiter = DistributedRateLimiter(redis_client)\n#     # Test distributed functionality with fake Redis\n```\n\n#### Unit Test Implementation Examples\n\n**Token Bucket Core Algorithm Tests:**\n\n```python\n# tests/unit/test_token_bucket.py\nimport pytest\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tests.fixtures.time_control import controlled_time\nfrom rate_limiter.token_bucket import TokenBucket, TokenBucketConfig\n\nclass TestTokenBucketBasicFunctionality:\n    \"\"\"Test core token bucket algorithm behavior\"\"\"\n    \n    def test_initial_token_allocation(self):\n        \"\"\"Verify bucket starts with correct token count\"\"\"\n        config = TokenBucketConfig(capacity=10, refill_rate=1.0, initial_tokens=5)\n        bucket = TokenBucket(config)\n        \n        result = bucket.try_consume(3)\n        assert result.allowed == True\n        assert result.tokens_remaining == 2\n        assert result.bucket_capacity == 10\n    \n    def test_token_refill_over_time(self):\n        \"\"\"Test accurate token generation based on elapsed time\"\"\"\n        config = TokenBucketConfig(capacity=10, refill_rate=2.0, initial_tokens=0)\n        \n        with controlled_time() as time_provider:\n            bucket = TokenBucket(config)\n            \n            # Should have 0 tokens initially\n            result = bucket.try_consume(1)\n            assert result.allowed == False\n            \n            # Advance 2 seconds = 4 tokens generated\n            time_provider.advance(2.0)\n            result = bucket.try_consume(4)\n            assert result.allowed == True\n            assert result.tokens_remaining == 0\n            \n            # Advance 3 more seconds = 6 tokens, but capped at capacity 10\n            time_provider.advance(3.0) \n            result = bucket.try_consume(6)\n            assert result.allowed == True\n    \n    def test_retry_after_calculation(self):\n        \"\"\"Verify accurate retry timing calculations\"\"\"\n        config = TokenBucketConfig(capacity=5, refill_rate=1.0, initial_tokens=0)\n        \n        with controlled_time():\n            bucket = TokenBucket(config)\n            \n            result = bucket.try_consume(3)\n            assert result.allowed == False\n            # Need 3 tokens at 1 token/second = 3 seconds wait\n            assert abs(result.retry_after_seconds - 3.0) < 0.001\n\nclass TestTokenBucketConcurrency:\n    \"\"\"Test thread safety under concurrent access\"\"\"\n    \n    def test_concurrent_token_consumption(self):\n        \"\"\"Verify no race conditions during concurrent access\"\"\"\n        config = TokenBucketConfig(capacity=100, refill_rate=10.0, initial_tokens=100)\n        bucket = TokenBucket(config)\n        \n        successful_consumptions = []\n        failed_consumptions = []\n        \n        def consume_tokens(thread_id):\n            \"\"\"Worker function for concurrent testing\"\"\"\n            for i in range(10):\n                result = bucket.try_consume(1)\n                if result.allowed:\n                    successful_consumptions.append(f\"thread-{thread_id}-{i}\")\n                else:\n                    failed_consumptions.append(f\"thread-{thread_id}-{i}\")\n                time.sleep(0.001)  # Small delay to increase race condition chances\n        \n        # Run 20 threads, each trying to consume 10 tokens\n        with ThreadPoolExecutor(max_workers=20) as executor:\n            futures = [executor.submit(consume_tokens, i) for i in range(20)]\n            for future in futures:\n                future.result()  # Wait for completion\n        \n        # Should have consumed exactly 100 tokens (initial capacity)\n        assert len(successful_consumptions) == 100\n        assert len(failed_consumptions) == 100  # Remaining attempts should fail\n        \n        # Final bucket state should be consistent\n        result = bucket.try_consume(1)\n        assert result.allowed == False  # No tokens remaining\n```\n\n#### Milestone Verification Implementation\n\n**Milestone 1 Token Bucket Verification:**\n\n```python\n# tests/milestone_verification/milestone_1_checks.py\nimport pytest\nimport time\nimport threading\nfrom rate_limiter.token_bucket import TokenBucket, TokenBucketConfig\n\ndef test_milestone_1_acceptance_criteria():\n    \"\"\"Comprehensive verification of Milestone 1 requirements\"\"\"\n    \n    # Test 1: Configurable bucket parameters\n    config = TokenBucketConfig(capacity=20, refill_rate=2.5, initial_tokens=10)\n    bucket = TokenBucket(config)\n    \n    assert bucket.capacity == 20\n    assert bucket.refill_rate == 2.5\n    # TODO: Add method to check current token count\n    \n    # Test 2: Token consumption with availability check\n    result = bucket.try_consume(5)\n    assert result.allowed == True\n    assert result.consumed_tokens == 5\n    \n    # Test 3: Denial when insufficient tokens\n    result = bucket.try_consume(10)  # Only 5 tokens remaining\n    assert result.allowed == False\n    assert result.retry_after_seconds > 0\n    \n    # Test 4: Burst handling up to capacity\n    full_bucket = TokenBucket(TokenBucketConfig(capacity=50, refill_rate=1.0, initial_tokens=50))\n    result = full_bucket.try_consume(50)  # Consume entire capacity\n    assert result.allowed == True\n    assert result.tokens_remaining == 0\n    \n    # Test 5: Thread safety verification\n    shared_bucket = TokenBucket(TokenBucketConfig(capacity=100, refill_rate=10.0, initial_tokens=100))\n    results = []\n    \n    def worker():\n        for _ in range(5):\n            result = shared_bucket.try_consume(1)\n            results.append(result.allowed)\n    \n    threads = [threading.Thread(target=worker) for _ in range(20)]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n    \n    # Exactly 100 successful consumptions expected\n    assert sum(results) == 100\n\ndef run_milestone_1_manual_verification():\n    \"\"\"Manual verification steps for Milestone 1\"\"\"\n    print(\"=== Milestone 1 Manual Verification ===\")\n    \n    # TODO: Implement manual test cases that developers can run\n    # TODO: Add timing-based tests that require manual observation\n    # TODO: Include performance benchmarks for single-threaded operation\n    \n    pass\n\nif __name__ == \"__main__\":\n    test_milestone_1_acceptance_criteria()\n    run_milestone_1_manual_verification()\n    print(\"Milestone 1 verification completed successfully!\")\n```\n\n#### Performance Test Implementation\n\n**Load Testing Infrastructure:**\n\n```python\n# tests/performance/load_test_framework.py\nimport asyncio\nimport aiohttp\nimport time\nimport statistics\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\n\n@dataclass\nclass LoadTestResult:\n    \"\"\"Results from load testing execution\"\"\"\n    total_requests: int\n    successful_requests: int\n    failed_requests: int\n    average_latency: float\n    p95_latency: float\n    p99_latency: float\n    requests_per_second: float\n    error_rate: float\n    status_code_distribution: Dict[int, int]\n\nclass RateLimiterLoadTester:\n    \"\"\"Comprehensive load testing for rate limiter\"\"\"\n    \n    def __init__(self, target_url: str, rate_limit_rps: int):\n        self.target_url = target_url\n        self.rate_limit_rps = rate_limit_rps\n        self.results = []\n    \n    async def execute_load_test(\n        self, \n        request_rate: int, \n        duration_seconds: int, \n        concurrent_clients: int = 1\n    ) -> LoadTestResult:\n        \"\"\"Execute load test with specified parameters\"\"\"\n        \n        connector = aiohttp.TCPConnector(limit=concurrent_clients * 2)\n        timeout = aiohttp.ClientTimeout(total=30)\n        \n        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n            start_time = time.time()\n            semaphore = asyncio.Semaphore(concurrent_clients)\n            \n            tasks = []\n            request_count = 0\n            \n            while time.time() - start_time < duration_seconds:\n                # TODO: Create request task with proper client ID rotation\n                # TODO: Implement precise request timing to maintain target rate\n                # TODO: Add request latency measurement\n                # TODO: Handle different response codes appropriately\n                pass\n                \n                # TODO: Calculate next request time to maintain steady rate\n                await asyncio.sleep(1.0 / request_rate)\n            \n            # TODO: Wait for all pending requests to complete\n            # TODO: Process results and calculate statistics\n            # TODO: Return comprehensive LoadTestResult\n            \n            return LoadTestResult(\n                total_requests=0,\n                successful_requests=0,\n                failed_requests=0,\n                average_latency=0.0,\n                p95_latency=0.0,\n                p99_latency=0.0,\n                requests_per_second=0.0,\n                error_rate=0.0,\n                status_code_distribution={}\n            )\n    \n    def validate_rate_limiting_accuracy(self, result: LoadTestResult) -> bool:\n        \"\"\"Validate that rate limiting was accurate within tolerance\"\"\"\n        expected_successful = self.rate_limit_rps * (result.total_requests / result.requests_per_second)\n        tolerance = expected_successful * 0.05  # 5% tolerance\n        \n        return (\n            result.successful_requests >= expected_successful - tolerance and\n            result.successful_requests <= expected_successful + tolerance\n        )\n\n# Usage example for comprehensive testing:\n# async def main():\n#     tester = RateLimiterLoadTester(\"http://localhost:5000/api/test\", rate_limit_rps=100)\n#     \n#     # Test at rate limit\n#     result = await tester.execute_load_test(request_rate=100, duration_seconds=60)\n#     assert tester.validate_rate_limiting_accuracy(result)\n#     \n#     # Test above rate limit  \n#     result = await tester.execute_load_test(request_rate=150, duration_seconds=60)\n#     assert tester.validate_rate_limiting_accuracy(result)\n\n```\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones - debugging skills are essential from basic token bucket implementation through distributed rate limiting, with complexity increasing as components are added\n\nThink of debugging a rate limiter like being a detective investigating a crime scene. You have symptoms (the observable behavior), evidence (logs, metrics, and state), and suspects (potential root causes). The key is methodically gathering evidence, forming hypotheses, and systematically ruling out suspects until you find the true culprit. Unlike a simple CRUD application where bugs are often straightforward, rate limiters involve timing, concurrency, and distributed state - making debugging more like solving a complex mystery with multiple moving parts.\n\nThe challenge with rate limiting bugs is that they often manifest under load, involve timing-sensitive race conditions, and can have subtle symptoms that seem unrelated to their root causes. A token calculation bug might appear as inconsistent rate limiting under high concurrency. A Redis connection issue might manifest as requests being allowed when they should be blocked. Clock drift between servers might cause tokens to refill too quickly or slowly, leading to rate limits that seem \"loose\" or \"strict\" compared to configuration.\n\nThis debugging guide provides a systematic approach to identifying, diagnosing, and fixing the most common issues encountered when implementing rate limiting systems. We'll cover the typical bugs that arise in each milestone, provide concrete diagnostic techniques, and offer proven solutions based on common implementation patterns.\n\n### Common Implementation Bugs\n\nUnderstanding the most frequent bugs helps you recognize patterns and debug more efficiently. Rate limiting bugs typically fall into several categories: concurrency issues, timing and calculation errors, client identification problems, and distributed consistency failures.\n\n#### Token Bucket Race Conditions\n\nRace conditions in token bucket operations are among the most insidious bugs because they're timing-dependent and may not manifest during single-threaded testing.\n\n⚠️ **Pitfall: Unprotected Token Refill and Consumption**\n\nThe most common race condition occurs when token refill and consumption operations aren't atomic. Consider two threads accessing the same `TokenBucket` simultaneously - one performing a refill calculation while another consumes tokens. Without proper synchronization, you might see:\n\n- Thread A reads current tokens (100) and calculates refill amount (50 new tokens)\n- Thread B reads current tokens (100) and attempts to consume 75 tokens\n- Thread A writes new token count (150)\n- Thread B writes remaining tokens after consumption (25)\n- Final state is 25 tokens instead of the correct 75 tokens\n\nThis manifests as inconsistent rate limiting where clients sometimes get through when they should be blocked, or get blocked when they should be allowed through.\n\n**Diagnosis**: Enable debug logging for all token bucket operations and look for token counts that don't match expected calculations. Run concurrent requests against the same client and observe if the token consumption results are consistent with the configured limits.\n\n**Fix**: Ensure all token bucket operations use proper locking or atomic operations. In Python, use `threading.Lock()` to protect the entire `try_consume` operation including refill calculation.\n\n⚠️ **Pitfall: Time-of-Check vs Time-of-Use in Token Calculations**\n\nAnother subtle race condition occurs when the current time is read at the beginning of a function but used much later after other operations. The time value becomes stale, leading to incorrect refill calculations.\n\n**Diagnosis**: Add timestamp logging to token refill operations and compare the time used for calculation with the actual system time when the operation completes. Large differences indicate stale time usage.\n\n**Fix**: Read the current time as late as possible in the operation, ideally just before the actual calculation that needs it.\n\n#### Token Calculation Arithmetic Errors\n\nMathematical precision and overflow issues in token calculations can cause subtle but significant rate limiting failures.\n\n⚠️ **Pitfall: Floating Point Precision in Refill Calculations**\n\nWhen calculating tokens to add based on elapsed time and refill rate, floating point precision errors can accumulate. A refill rate of 10.3 tokens per second over 0.1 seconds should add 1.03 tokens, but floating point math might yield 1.0299999999999998 tokens. When truncated to integers, this becomes 1 token instead of the expected 1 token - seemingly correct, but the precision loss accumulates over time.\n\n**Diagnosis**: Log the exact floating point values in token calculations, including intermediate steps. Run the rate limiter for extended periods and compare actual token generation rates with expected rates.\n\n**Fix**: Use decimal arithmetic for precise calculations or implement token calculation using integer arithmetic with a time granularity that avoids precision issues.\n\n⚠️ **Pitfall: Integer Overflow in Token Accumulation**\n\nWhen a token bucket hasn't been accessed for a very long time, the refill calculation might try to add an enormous number of tokens, potentially causing integer overflow. This can result in negative token counts or wrapping to very small positive numbers.\n\n**Diagnosis**: Test with very large time gaps (simulate a bucket that hasn't been accessed for days) and observe token count calculations. Monitor for negative token counts or unexpectedly small values after long idle periods.\n\n**Fix**: Cap the maximum tokens that can be added in a single refill operation to the bucket capacity minus current tokens. Never allow more tokens than the bucket can hold.\n\n#### Client Identification and Bucket Management Issues\n\nProblems with identifying clients or managing per-client buckets can lead to rate limits being applied incorrectly or not at all.\n\n⚠️ **Pitfall: Inconsistent Client ID Normalization**\n\nDifferent request processing paths might normalize client identifiers differently. An IP address might be extracted as \"192.168.1.100\" in some code paths and \"192.168.001.100\" in others, leading to separate buckets for the same client.\n\n**Diagnosis**: Log all client ID extraction operations and look for the same logical client appearing with different identifier strings. Monitor bucket creation rates - if it's consistently higher than expected unique client rates, you likely have normalization issues.\n\n**Fix**: Implement a centralized `normalize_client_id()` function that all code paths use. For IP addresses, use standard library functions that ensure consistent formatting.\n\n⚠️ **Pitfall: Memory Leaks from Never-Cleaned Buckets**\n\nIf the bucket cleanup process fails or isn't aggressive enough, memory usage will grow unboundedly as more unique clients access the system. This is particularly problematic in systems that see many one-time clients (like public APIs).\n\n**Diagnosis**: Monitor memory usage over time and track the number of stored buckets. If buckets grow continuously without cleanup, or cleanup runs but doesn't reduce bucket counts, you have a leak.\n\n**Fix**: Ensure cleanup runs regularly and aggressively removes stale buckets. Consider implementing LRU eviction as a backup mechanism when bucket count exceeds memory limits.\n\n#### Distributed Consistency Problems\n\nWhen scaling to multiple servers with Redis-backed storage, additional complexity introduces new failure modes.\n\n⚠️ **Pitfall: Non-Atomic Redis Operations**\n\nPerforming token bucket operations as separate Redis commands (read current tokens, calculate new count, write new count) creates race conditions between servers. Two servers might simultaneously read the same token count, both decide to allow a request, and both update the count, effectively allowing twice the intended rate.\n\n**Diagnosis**: Enable Redis command logging and look for interleaved read/write patterns from different servers accessing the same keys. Monitor rate limiting accuracy under high concurrent load from multiple servers.\n\n**Fix**: Use Redis Lua scripts to ensure atomic read-modify-write operations. All token consumption logic should execute as a single atomic script.\n\n⚠️ **Pitfall: Clock Drift Between Servers**\n\nDifferent servers having clocks that drift apart can cause inconsistent token refill rates. A server with a fast clock will add tokens more quickly than configured, while a server with a slow clock will add tokens too slowly.\n\n**Diagnosis**: Compare system time across all servers and monitor token refill rates from each server. If servers show different effective rates for the same bucket, clock drift is likely the cause.\n\n**Fix**: Implement NTP synchronization across all servers and add clock drift detection to your monitoring. Consider using Redis-based timestamps for token calculations instead of local server time.\n\n### Debugging Techniques and Tools\n\nEffective debugging requires the right tools and systematic approaches. Rate limiting bugs often require observing system behavior over time and under load, making traditional debugger breakpoints less effective than logging and monitoring-based approaches.\n\n#### Comprehensive Logging Strategy\n\nStrategic logging is your most powerful debugging tool for rate limiting systems. The key is logging the right information without overwhelming the system with too much data.\n\n**Token Bucket Operation Logging**\n\nEvery token bucket operation should log its inputs, calculations, and results. This includes:\n\n| Log Field | Description | Example Value | When to Log |\n|-----------|-------------|---------------|-------------|\n| `client_id` | Client identifier | `\"192.168.1.100\"` | Every operation |\n| `endpoint` | API endpoint being accessed | `\"/api/v1/users\"` | Every operation |\n| `timestamp` | Current system time | `1640995200.123` | Every operation |\n| `tokens_requested` | Number of tokens requested | `1` | Every operation |\n| `tokens_before` | Token count before operation | `42` | Before consumption |\n| `tokens_after` | Token count after operation | `41` | After consumption |\n| `time_elapsed` | Time since last refill | `0.5` | During refill |\n| `tokens_added` | Tokens added in refill | `5` | During refill |\n| `bucket_capacity` | Maximum bucket size | `100` | Every operation |\n| `refill_rate` | Configured refill rate | `10.0` | Every operation |\n| `operation_result` | Allowed or denied | `\"ALLOWED\"` | Every operation |\n\n**Client Identification Logging**\n\nSince client identification problems are common, log every step of the client ID extraction process:\n\n```\nDEBUG: Extracting client ID from request\nDEBUG: Request headers: {'X-API-Key': 'abc123', 'X-Forwarded-For': '192.168.1.100'}\nDEBUG: Using IP_ADDRESS strategy\nDEBUG: Raw IP from X-Forwarded-For: '192.168.1.100'\nDEBUG: Normalized client ID: '192.168.1.100'\nDEBUG: Resolved to bucket key: 'bucket:192.168.1.100:/api/users'\n```\n\n**Distributed Operation Logging**\n\nFor Redis-based distributed rate limiting, log all distributed operations including fallback scenarios:\n\n```\nDEBUG: Attempting Redis token consumption for client_id=192.168.1.100\nDEBUG: Redis Lua script execution - tokens_requested=1, current_tokens=50\nDEBUG: Redis operation successful - tokens_remaining=49, allowed=true\n```\n\nWhen Redis failures occur:\n```\nERROR: Redis operation failed: ConnectionError: Connection refused\nINFO: Falling back to local bucket for client_id=192.168.1.100\nDEBUG: Local fallback bucket - conservative_rate=5.0 (50% of configured 10.0)\n```\n\n#### Redis Inspection Techniques\n\nRedis provides powerful introspection capabilities for debugging distributed rate limiting issues.\n\n**Monitoring Token Bucket Keys**\n\nUse Redis commands to inspect the current state of token buckets:\n\n| Redis Command | Purpose | Example Usage |\n|---------------|---------|---------------|\n| `KEYS bucket:*` | List all bucket keys | Find all active client buckets |\n| `HGETALL bucket:192.168.1.100` | Get bucket state | See current tokens, last update time |\n| `TTL bucket:192.168.1.100` | Check key expiration | Verify cleanup is working |\n| `MONITOR` | Watch all Redis commands | See real-time bucket operations |\n| `INFO memory` | Check Redis memory usage | Monitor for memory leaks |\n| `CLIENT LIST` | See connected clients | Identify which servers are active |\n\n**Lua Script Debugging**\n\nRedis Lua scripts can be debugged by adding logging within the script:\n\n```lua\n-- Add debugging output to Lua scripts\nredis.log(redis.LOG_WARNING, \"Token consumption: key=\" .. KEYS[1] .. \", requested=\" .. ARGV[1])\nlocal current_tokens = redis.call('HGET', KEYS[1], 'tokens') or bucket_capacity\nredis.log(redis.LOG_WARNING, \"Current tokens: \" .. current_tokens)\n```\n\nThese logs appear in the Redis server logs and help debug atomic operations.\n\n#### Concurrency Debugging Approaches\n\nRate limiting bugs often involve race conditions that require special debugging techniques.\n\n**Load Testing with Controlled Concurrency**\n\nCreate test scenarios that expose race conditions:\n\n```python\n# Example load test that exposes race conditions\nimport concurrent.futures\nimport time\nimport requests\n\ndef concurrent_request_test(client_id, endpoint, num_threads=10, requests_per_thread=5):\n    \"\"\"Send concurrent requests to expose race conditions.\"\"\"\n    \n    def make_request():\n        headers = {'X-Client-ID': client_id}\n        response = requests.get(f'http://localhost:8000{endpoint}', headers=headers)\n        return response.status_code, response.headers.get('X-RateLimit-Remaining')\n    \n    # Send all requests simultaneously\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n        futures = []\n        for _ in range(num_threads):\n            for _ in range(requests_per_thread):\n                futures.append(executor.submit(make_request))\n        \n        results = [future.result() for future in futures]\n    \n    # Analyze results for consistency\n    allowed_requests = [r for r in results if r[0] == 200]\n    denied_requests = [r for r in results if r[0] == 429]\n    \n    return {\n        'total_requests': len(results),\n        'allowed_count': len(allowed_requests),\n        'denied_count': len(denied_requests),\n        'remaining_tokens': [r[1] for r in allowed_requests if r[1] is not None]\n    }\n```\n\n**Thread-Safe State Verification**\n\nAdd verification code that checks for impossible states:\n\n```python\nclass TokenBucket:\n    def try_consume(self, tokens_requested):\n        with self._lock:  # Ensure atomic operation\n            # ... normal token bucket logic ...\n            \n            # Verification: tokens should never be negative\n            assert self.current_tokens >= 0, f\"Negative tokens detected: {self.current_tokens}\"\n            \n            # Verification: tokens should never exceed capacity\n            assert self.current_tokens <= self.capacity, f\"Tokens exceed capacity: {self.current_tokens} > {self.capacity}\"\n            \n            return result\n```\n\n#### Performance Profiling for Rate Limiters\n\nRate limiting performance issues can cause cascading problems throughout your system.\n\n**Latency Measurement**\n\nTrack the time spent in rate limiting operations:\n\n```python\nimport time\nimport statistics\n\nclass PerformanceTracker:\n    def __init__(self):\n        self.operation_times = []\n        \n    def measure_operation(self, operation_name, func, *args, **kwargs):\n        start_time = time.perf_counter()\n        try:\n            result = func(*args, **kwargs)\n            return result\n        finally:\n            end_time = time.perf_counter()\n            duration = end_time - start_time\n            self.operation_times.append((operation_name, duration))\n            \n            # Log slow operations\n            if duration > 0.01:  # 10ms threshold\n                print(f\"SLOW OPERATION: {operation_name} took {duration*1000:.2f}ms\")\n    \n    def get_performance_stats(self):\n        if not self.operation_times:\n            return {}\n            \n        times = [t[1] for t in self.operation_times]\n        return {\n            'count': len(times),\n            'mean': statistics.mean(times),\n            'median': statistics.median(times),\n            'p95': statistics.quantiles(times, n=20)[18],  # 95th percentile\n            'max': max(times)\n        }\n```\n\n### Symptom-Cause-Fix Reference\n\nThis reference table maps observable symptoms to their likely root causes and provides specific diagnostic steps and fixes.\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Rate limits seem \"loose\" - too many requests allowed | Clock drift making refill rate too fast | Compare server clocks, monitor token refill rates across servers | Implement NTP sync, use Redis timestamps for calculations |\n| Rate limits seem \"strict\" - requests blocked unexpectedly | Clock drift making refill rate too slow | Check if token refill rates are slower than configured | Sync server clocks, verify time zone consistency |\n| Inconsistent rate limiting under high concurrency | Race conditions in token bucket operations | Enable debug logging, run concurrent load tests | Add proper locking around all bucket operations |\n| Memory usage grows continuously | Bucket cleanup not working or too conservative | Monitor bucket count over time, check cleanup logs | Fix cleanup logic, implement LRU eviction |\n| Rate limiter allows massive bursts occasionally | Integer overflow in token calculations | Test with very long idle periods, check for negative tokens | Cap token refill to bucket capacity, use proper integer types |\n| Different clients get same rate limits | Client ID normalization inconsistencies | Log client ID extraction for same logical client | Implement centralized client ID normalization |\n| Redis errors but no fallback behavior | Circuit breaker not triggering or local fallback disabled | Check Redis connection status and fallback logs | Verify circuit breaker configuration and fallback implementation |\n| Rate limits work locally but fail distributed | Non-atomic Redis operations | Enable Redis command logging, look for interleaved operations | Implement atomic Lua scripts for all token operations |\n| Requests hang or timeout in rate limiter | Deadlock in locking or Redis connection issues | Check for long-running lock acquisitions, Redis connection health | Review locking strategy, implement connection timeouts |\n| Rate limiter has high latency impact | Inefficient Redis operations or excessive locking | Profile operation times, measure Redis round-trip times | Optimize Redis operations, reduce lock contention |\n| Some clients bypass rate limits entirely | Client identification returning empty or default IDs | Log all client ID extractions, especially edge cases | Add validation to client ID extraction, handle missing headers |\n| Token counts don't match expected values | Floating point precision errors in calculations | Log exact floating point values in token math | Use decimal arithmetic or integer-based calculations |\n| Rate limits reset unexpectedly | Bucket expiration or cleanup happening too aggressively | Monitor bucket TTL values and cleanup operations | Adjust bucket expiration times and cleanup thresholds |\n| 429 responses missing proper headers | Middleware not setting rate limit headers correctly | Check HTTP response headers in 429 responses | Fix header generation in rate limit response building |\n| Rate limits work in staging but not production | Configuration differences or load-related race conditions | Compare configs, test with production-level load | Ensure config consistency, load test thoroughly |\n| Distributed rate limiting inconsistent across servers | Different server configurations or Redis connection issues | Check server configs and Redis connectivity from all servers | Standardize configurations, verify Redis connectivity |\n\n#### Advanced Debugging Scenarios\n\nSome debugging scenarios require combining multiple techniques and investigating complex interactions.\n\n**Debugging Clock Drift Issues**\n\nClock drift between servers is particularly tricky because it affects token refill rates gradually over time:\n\n1. **Detection**: Set up monitoring that compares effective token refill rates across servers for the same bucket\n2. **Measurement**: Log system timestamps from each server when they perform token refill operations\n3. **Analysis**: Compare the timestamps to identify which servers have drifting clocks\n4. **Verification**: After fixing clock sync, monitor refill rates to ensure consistency\n\n**Debugging Memory Leaks in Bucket Storage**\n\nBucket cleanup failures can be subtle and may only manifest under specific conditions:\n\n1. **Monitoring**: Track bucket count, memory usage, and cleanup operation success rates over time\n2. **Investigation**: Identify which buckets are not being cleaned up by examining last access times\n3. **Root Cause**: Determine if the cleanup process is failing, not running, or using wrong criteria\n4. **Testing**: Create test scenarios with many short-lived clients to verify cleanup works correctly\n\n**Debugging Lua Script Atomicity Issues**\n\nWhen Redis Lua scripts don't behave atomically as expected:\n\n1. **Script Verification**: Test Lua scripts in isolation with controlled inputs to verify logic\n2. **Concurrency Testing**: Run high-concurrency tests against the same Redis keys to expose race conditions\n3. **Redis Monitoring**: Use Redis MONITOR command to observe the actual sequence of operations\n4. **Error Handling**: Ensure Lua scripts handle all error conditions correctly without leaving inconsistent state\n\n### Implementation Guidance\n\nThe debugging capabilities need to be built into your rate limiter from the beginning rather than added after problems occur. This section provides the infrastructure for comprehensive debugging support.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Logging | Python `logging` module with structured JSON | ELK stack with distributed tracing |\n| Monitoring | Simple metrics in log files | Prometheus + Grafana dashboards |\n| Redis Debugging | Redis CLI with manual inspection | Redis monitoring tools like RedisInsight |\n| Load Testing | Python `concurrent.futures` with custom scripts | JMeter or k6 with comprehensive scenarios |\n| Performance Profiling | Python `cProfile` with timing decorators | APM tools like New Relic or DataDog |\n\n#### Recommended File Structure\n\nOrganize debugging and monitoring code to support both development and production troubleshooting:\n\n```\nproject-root/\n  src/rate_limiter/\n    core/\n      token_bucket.py           ← Core algorithm with debug hooks\n      client_tracker.py         ← Client management with monitoring\n    middleware/\n      flask_middleware.py       ← HTTP middleware with request tracing\n    distributed/\n      redis_storage.py          ← Redis operations with error tracking\n    debugging/\n      __init__.py\n      logger.py                 ← Centralized logging configuration\n      performance_tracker.py    ← Operation timing and profiling\n      load_tester.py           ← Comprehensive load testing utilities\n      redis_inspector.py       ← Redis debugging and inspection tools\n    monitoring/\n      __init__.py\n      metrics.py               ← Metrics collection and reporting\n      health_checks.py         ← System health monitoring\n  tests/\n    debugging/\n      test_race_conditions.py  ← Concurrency-focused tests\n      test_clock_drift.py      ← Time-related issue tests\n  scripts/\n    debug_redis.py            ← Redis inspection scripts\n    load_test.py              ← Load testing orchestration\n```\n\n#### Debugging Infrastructure Starter Code\n\nHere's the complete debugging infrastructure that provides comprehensive logging and monitoring capabilities:\n\n```python\n# src/rate_limiter/debugging/logger.py\nimport logging\nimport json\nimport time\nfrom typing import Dict, Any, Optional\nfrom functools import wraps\n\nclass RateLimiterLogger:\n    \"\"\"Centralized logging for rate limiter with structured output.\"\"\"\n    \n    def __init__(self, component_name: str, log_level: int = logging.INFO):\n        self.component_name = component_name\n        self.logger = logging.getLogger(f\"rate_limiter.{component_name}\")\n        self.logger.setLevel(log_level)\n        \n        # Create structured formatter\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n        )\n        \n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setFormatter(formatter)\n        self.logger.addHandler(console_handler)\n    \n    def log_token_operation(self, operation: str, client_id: str, **kwargs):\n        \"\"\"Log token bucket operations with full context.\"\"\"\n        log_data = {\n            'operation': operation,\n            'client_id': client_id,\n            'timestamp': time.time(),\n            'component': self.component_name,\n            **kwargs\n        }\n        self.logger.info(f\"TOKEN_OP: {json.dumps(log_data)}\")\n    \n    def log_client_identification(self, raw_data: Dict[str, Any], result: str):\n        \"\"\"Log client identification process.\"\"\"\n        log_data = {\n            'operation': 'client_identification',\n            'raw_headers': raw_data.get('headers', {}),\n            'client_id_result': result,\n            'timestamp': time.time()\n        }\n        self.logger.debug(f\"CLIENT_ID: {json.dumps(log_data)}\")\n    \n    def log_redis_operation(self, operation: str, key: str, success: bool, **kwargs):\n        \"\"\"Log Redis operations for distributed debugging.\"\"\"\n        log_data = {\n            'operation': f'redis_{operation}',\n            'key': key,\n            'success': success,\n            'timestamp': time.time(),\n            **kwargs\n        }\n        level = logging.INFO if success else logging.ERROR\n        self.logger.log(level, f\"REDIS_OP: {json.dumps(log_data)}\")\n\ndef debug_timing(logger: RateLimiterLogger):\n    \"\"\"Decorator to measure and log operation timing.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            start_time = time.perf_counter()\n            try:\n                result = func(*args, **kwargs)\n                return result\n            finally:\n                duration = time.perf_counter() - start_time\n                logger.logger.debug(f\"TIMING: {func.__name__} took {duration*1000:.2f}ms\")\n        return wrapper\n    return decorator\n```\n\n```python\n# src/rate_limiter/debugging/performance_tracker.py\nimport time\nimport threading\nimport statistics\nfrom typing import Dict, List, Tuple\nfrom collections import defaultdict, deque\n\nclass PerformanceTracker:\n    \"\"\"Track operation performance and detect anomalies.\"\"\"\n    \n    def __init__(self, max_samples: int = 1000):\n        self.max_samples = max_samples\n        self.operation_times: Dict[str, deque] = defaultdict(lambda: deque(maxlen=max_samples))\n        self._lock = threading.Lock()\n    \n    def record_operation(self, operation_name: str, duration: float):\n        \"\"\"Record timing for an operation.\"\"\"\n        with self._lock:\n            self.operation_times[operation_name].append(duration)\n    \n    def get_stats(self, operation_name: str) -> Dict[str, float]:\n        \"\"\"Get performance statistics for an operation.\"\"\"\n        with self._lock:\n            times = list(self.operation_times[operation_name])\n        \n        if not times:\n            return {}\n        \n        return {\n            'count': len(times),\n            'mean': statistics.mean(times),\n            'median': statistics.median(times),\n            'std_dev': statistics.stdev(times) if len(times) > 1 else 0,\n            'min': min(times),\n            'max': max(times),\n            'p95': statistics.quantiles(times, n=20)[18] if len(times) >= 20 else max(times)\n        }\n    \n    def measure_operation(self, operation_name: str):\n        \"\"\"Context manager to measure operation timing.\"\"\"\n        return TimingContext(self, operation_name)\n\nclass TimingContext:\n    \"\"\"Context manager for measuring operation timing.\"\"\"\n    \n    def __init__(self, tracker: PerformanceTracker, operation_name: str):\n        self.tracker = tracker\n        self.operation_name = operation_name\n        self.start_time = None\n    \n    def __enter__(self):\n        self.start_time = time.perf_counter()\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.start_time is not None:\n            duration = time.perf_counter() - self.start_time\n            self.tracker.record_operation(self.operation_name, duration)\n```\n\n#### Core Logic Debugging Skeleton\n\nHere's how to instrument the core token bucket implementation with comprehensive debugging:\n\n```python\n# Enhanced TokenBucket with debugging capabilities\nfrom .debugging.logger import RateLimiterLogger, debug_timing\nfrom .debugging.performance_tracker import PerformanceTracker\n\nclass DebuggableTokenBucket:\n    \"\"\"Token bucket implementation with comprehensive debugging support.\"\"\"\n    \n    def __init__(self, config: TokenBucketConfig):\n        self.config = config\n        self.current_tokens = config.initial_tokens or config.capacity\n        self.last_refill_time = time.time()\n        self._lock = threading.Lock()\n        \n        # Debugging infrastructure\n        self.logger = RateLimiterLogger(\"token_bucket\")\n        self.perf_tracker = PerformanceTracker()\n        \n        # State validation\n        self._validate_state(\"initialization\")\n    \n    def try_consume(self, tokens_requested: int, client_id: str = \"unknown\") -> TokenConsumptionResult:\n        \"\"\"\n        Attempt to consume tokens with comprehensive debugging.\n        \n        TODO 1: Acquire lock for thread safety\n        TODO 2: Record operation start time and log request\n        TODO 3: Validate input parameters and current state\n        TODO 4: Perform token refill based on elapsed time\n        TODO 5: Check if sufficient tokens available\n        TODO 6: If tokens available, deduct requested amount\n        TODO 7: If insufficient tokens, calculate retry time\n        TODO 8: Log operation result with all relevant data\n        TODO 9: Validate final state consistency\n        TODO 10: Record performance metrics and release lock\n        \"\"\"\n        pass  # Implementation goes here\n    \n    def _refill_tokens(self, current_time: float) -> int:\n        \"\"\"\n        Refill tokens based on elapsed time with debugging.\n        \n        TODO 1: Calculate elapsed time since last refill\n        TODO 2: Log time calculation details for debugging\n        TODO 3: Calculate tokens to add based on refill rate\n        TODO 4: Log token calculation with intermediate values\n        TODO 5: Cap tokens at bucket capacity\n        TODO 6: Update current tokens and last refill time\n        TODO 7: Log final refill result\n        TODO 8: Return number of tokens added\n        \"\"\"\n        pass  # Implementation goes here\n    \n    def _validate_state(self, operation: str):\n        \"\"\"Validate bucket state consistency.\"\"\"\n        # TODO 1: Check that current tokens is not negative\n        # TODO 2: Check that current tokens does not exceed capacity\n        # TODO 3: Check that last refill time is reasonable (not future, not too old)\n        # TODO 4: Log any validation failures with full state dump\n        pass  # Implementation goes here\n```\n\n#### Milestone Checkpoints\n\n**After Milestone 1 (Basic Token Bucket):**\n- Run: `python -m pytest tests/test_token_bucket.py -v`\n- Expected: All token bucket tests pass, no race conditions under concurrent access\n- Manual verification: Create a bucket with capacity 10, rate 1/sec, consume 5 tokens, wait 3 seconds, consume 5 more - should succeed\n- Debug check: Enable debug logging and verify token calculations match expected values\n\n**After Milestone 2 (Per-Client Rate Limiting):**\n- Run: `python -m pytest tests/test_client_tracker.py -v`\n- Expected: Client buckets created correctly, cleanup removes stale buckets\n- Manual verification: Send requests from multiple client IPs, verify separate buckets created\n- Debug check: Monitor memory usage during high client turnover, verify cleanup prevents leaks\n\n**After Milestone 3 (HTTP Middleware):**\n- Run: `curl -H \"X-Client-ID: test123\" http://localhost:5000/api/test` (multiple times)\n- Expected: First requests succeed (200), later requests get 429 with proper headers\n- Manual verification: Check that `X-RateLimit-Remaining` header decreases with each request\n- Debug check: Verify middleware logs show correct client identification and token consumption\n\n**After Milestone 4 (Distributed Rate Limiting):**\n- Run: Start multiple server instances, send requests to different servers with same client ID\n- Expected: Rate limiting consistent across all servers\n- Manual verification: Redis should show token bucket keys, Lua script should execute atomically\n- Debug check: Monitor Redis operations, verify fallback works when Redis unavailable\n\n\n## Future Extensions\n\n> **Milestone(s):** Beyond Milestone 4 - this section explores potential enhancements to the distributed rate limiter, guiding future development and production sophistication\n\nThink of our current rate limiter as a reliable neighborhood traffic cop who knows all the locals and keeps traffic flowing smoothly. But as our digital neighborhood grows into a bustling metropolis, we need to evolve our traffic management system. We might need specialized express lanes for VIP users, dynamic traffic light timing that adapts to rush hour patterns, and comprehensive traffic monitoring systems that help city planners optimize the entire transportation network. These future extensions transform our basic rate limiter into a sophisticated traffic management platform capable of handling enterprise-scale API ecosystems.\n\nThe extensions we'll explore fall into three categories: algorithmic sophistication (alternative rate limiting strategies), operational intelligence (dynamic adaptation and advanced client classification), and production observability (comprehensive monitoring and alerting). Each extension builds upon our solid foundation while opening new possibilities for API protection and performance optimization.\n\n### Alternative Rate Limiting Algorithms\n\nOur token bucket algorithm provides excellent burst handling and intuitive capacity management, but different traffic patterns and protection requirements may benefit from alternative approaches. Think of these algorithms as different traffic management strategies: token bucket is like a toll booth that accepts payment in advance, sliding window is like a highway patrol counting cars over specific time periods, and leaky bucket is like a traffic light with perfectly timed releases.\n\n#### Sliding Window Rate Limiting\n\nThe **sliding window algorithm** maintains a continuous time-based view of request history, providing more precise rate limiting than fixed time windows. Unlike token bucket's burst-friendly approach, sliding window ensures that no matter when you measure any time period, the request count never exceeds the limit.\n\n**Mental Model: The Moving Surveillance Window**: Imagine a security camera that continuously records a moving 60-second video clip. At any moment, you can examine the current 60-second window and count events. Unlike a token bucket that allows 100 requests instantly (if tokens are available), sliding window ensures that looking back any 60 seconds from any point in time, you never see more than 60 requests.\n\nThe algorithm maintains request timestamps and continuously evicts old entries as time progresses. Each new request triggers a cleanup of expired entries, followed by a count check against the configured limit.\n\n| Algorithm Aspect | Token Bucket | Sliding Window |\n|------------------|--------------|----------------|\n| Burst Behavior | Allows bursts up to capacity | Strict rate enforcement |\n| Memory Usage | Fixed (just token count) | Variable (stores request timestamps) |\n| Precision | Approximate over time | Exact within any time window |\n| Computational Complexity | O(1) per request | O(k) where k = recent requests |\n| Use Case | APIs with natural burst patterns | Strict SLA enforcement |\n\n> **Decision: Sliding Window Implementation Strategy**\n> - **Context**: Some API consumers require strict rate enforcement without burst allowances, particularly for billing APIs or resource-intensive operations\n> - **Options Considered**: \n>   1. Replace token bucket entirely with sliding window\n>   2. Implement sliding window as alternative algorithm option\n>   3. Hybrid approach combining both algorithms\n> - **Decision**: Implement sliding window as configurable alternative algorithm\n> - **Rationale**: Different endpoints have different burst tolerance requirements; configuration flexibility serves more use cases than forcing one approach\n> - **Consequences**: Increased complexity in configuration and storage, but enables precise rate enforcement for critical endpoints\n\nThe sliding window implementation extends our existing `BucketStorage` interface with timestamp-based operations:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `record_request` | `client_id: str, endpoint: str, timestamp: float` | `bool` | Record new request timestamp and return whether under limit |\n| `get_request_count` | `client_id: str, endpoint: str, window_seconds: int` | `int` | Count requests within sliding time window |\n| `cleanup_expired_requests` | `client_id: str, endpoint: str, cutoff_time: float` | `int` | Remove expired request records and return count removed |\n| `get_request_history` | `client_id: str, endpoint: str, limit: int` | `List[float]` | Retrieve recent request timestamps for debugging |\n\n#### Leaky Bucket Rate Limiting\n\nThe **leaky bucket algorithm** enforces perfectly smooth request rates by processing requests at a fixed interval regardless of arrival timing. This algorithm excels in scenarios requiring predictable resource consumption and steady-state processing.\n\n**Mental Model: The Dripping Faucet**: Picture a bucket with a small hole that drips water at exactly one drop per second. Incoming requests are water poured into the bucket. If water arrives faster than it drips out, the bucket fills up. Once full, excess water overflows (requests are rejected). The key insight is that water always exits at the same rate, creating perfectly smooth output even from bursty input.\n\nUnlike token bucket which allows immediate processing when tokens are available, leaky bucket queues requests and processes them at the configured rate. This creates natural traffic shaping but introduces latency for queued requests.\n\n| Characteristic | Token Bucket | Leaky Bucket | Sliding Window |\n|----------------|--------------|---------------|----------------|\n| Processing Model | Immediate when tokens available | Queue and process at fixed rate | Immediate with count checking |\n| Latency | Variable (immediate or rejected) | Variable (queuing delay) | Fixed (immediate or rejected) |\n| Traffic Shaping | Allows bursts, smooths over time | Perfect rate smoothing | No smoothing, strict limits |\n| Queue Management | No queuing | Request queue required | No queuing |\n| Resource Predictability | Variable resource usage | Perfectly predictable usage | Variable resource usage |\n\n> **Decision: Leaky Bucket Implementation Approach**\n> - **Context**: Some downstream services require perfectly smooth request rates to avoid overwhelming their processing capacity\n> - **Options Considered**:\n>   1. Full leaky bucket with request queuing and background processing\n>   2. Simulated leaky bucket using token bucket with very small refill rates\n>   3. Hybrid approach with configurable processing delays\n> - **Decision**: Implement simulated leaky bucket using token bucket with single-token capacity and precise refill timing\n> - **Rationale**: Avoids complexity of request queuing and background processing while achieving smooth rate limiting; simpler to implement and debug\n> - **Consequences**: Doesn't provide true traffic shaping (requests are still rejected immediately), but achieves steady-state rate limiting without queuing infrastructure\n\n#### Algorithm Selection Framework\n\nDifferent API endpoints and client tiers benefit from different rate limiting algorithms. A comprehensive rate limiter should support algorithm selection based on endpoint characteristics and client requirements.\n\n**Algorithm Selection Criteria:**\n\n| Use Case | Recommended Algorithm | Rationale |\n|----------|----------------------|-----------|\n| Public APIs with burst usage | Token Bucket | Natural user behavior involves bursts |\n| Billing/payment endpoints | Sliding Window | Strict enforcement prevents billing abuse |\n| Resource-intensive operations | Leaky Bucket | Smooth processing protects backend systems |\n| High-throughput data ingestion | Token Bucket with large capacity | Accommodates batch processing patterns |\n| Real-time APIs with SLA requirements | Sliding Window | Precise rate tracking for SLA compliance |\n| Legacy system integration | Leaky Bucket | Protects systems with limited concurrency |\n\nThe `RateLimitConfig` structure extends to support algorithm selection:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `algorithm_type` | `AlgorithmType` | TOKEN_BUCKET, SLIDING_WINDOW, or LEAKY_BUCKET |\n| `algorithm_config` | `Dict[str, Any]` | Algorithm-specific parameters |\n| `fallback_algorithm` | `AlgorithmType` | Algorithm to use during primary algorithm failures |\n| `transition_strategy` | `TransitionStrategy` | How to handle algorithm changes for existing clients |\n\n### Advanced Rate Limiting Features\n\nBeyond basic rate limiting, production systems often require sophisticated features that adapt to changing conditions, classify clients intelligently, and provide fine-grained control over API access patterns.\n\n#### Dynamic Rate Adjustment\n\n**Dynamic rate adjustment** automatically modifies rate limits based on system load, client behavior, and external conditions. This transforms static rate limiting into an intelligent protection system that balances user experience with system stability.\n\n**Mental Model: The Smart Traffic Light System**: Imagine traffic lights that monitor traffic density in real-time and adjust their timing accordingly. During rush hour, they extend green light durations for busy directions. When emergency vehicles approach, they preemptively clear paths. During low-traffic periods, they provide faster cycles to minimize waiting. Dynamic rate adjustment applies this same intelligence to API traffic management.\n\nDynamic adjustment operates on multiple time scales and responds to various signals:\n\n| Adjustment Trigger | Response Time | Typical Action | Use Case |\n|-------------------|---------------|----------------|----------|\n| System CPU/memory pressure | 10-30 seconds | Reduce limits by 10-50% | Prevent system overload |\n| Error rate increase | 30-60 seconds | Tighten limits for error-prone clients | Circuit breaker behavior |\n| Client behavioral changes | 5-15 minutes | Adjust individual client limits | Reward/penalize based on behavior |\n| Time-based patterns | Hours/days | Pre-adjust for known traffic patterns | Handle daily/weekly cycles |\n| External dependencies | 1-5 minutes | Reduce limits when dependencies slow | Backpressure propagation |\n\n> **Decision: Dynamic Adjustment Implementation Strategy**\n> - **Context**: Static rate limits often become either too restrictive during low load or insufficient during peak load, requiring manual intervention\n> - **Options Considered**:\n>   1. Simple load-based adjustment using system metrics\n>   2. Machine learning-based prediction and adjustment\n>   3. Rule-based adjustment with configurable triggers and responses\n> - **Decision**: Rule-based adjustment system with pluggable adjustment strategies\n> - **Rationale**: Rule-based systems are predictable, debuggable, and don't require ML expertise; pluggable design allows future ML integration\n> - **Consequences**: Requires careful rule design to avoid adjustment oscillations; provides immediate value with clear upgrade path\n\nThe dynamic adjustment system introduces several new components:\n\n| Component | Purpose | Key Methods |\n|-----------|---------|-------------|\n| `AdjustmentEngine` | Coordinates adjustment decisions | `evaluate_adjustments()`, `apply_adjustments()` |\n| `SystemMonitor` | Tracks system health metrics | `get_cpu_usage()`, `get_memory_pressure()` |\n| `ClientBehaviorTracker` | Monitors client request patterns | `track_request()`, `detect_anomalies()` |\n| `AdjustmentStrategy` | Defines adjustment logic | `calculate_adjustment()`, `validate_adjustment()` |\n\n#### Intelligent Client Classification\n\n**Intelligent client classification** automatically categorizes API consumers based on behavior patterns, enabling differentiated service levels without manual configuration. This system learns from request patterns to identify legitimate high-volume users, potential abusers, and everything in between.\n\n**Mental Model: The Hotel Concierge System**: Picture a luxury hotel concierge who recognizes guests by their behavior patterns. Regular business travelers get streamlined check-in processes. Families with children receive patient, detailed assistance. Suspicious individuals face additional verification. The concierge doesn't just follow rigid rules but adapts service based on observed patterns and context cues.\n\nThe classification system analyzes multiple behavioral dimensions to build client profiles:\n\n| Classification Dimension | Measurement Window | Typical Patterns | Resulting Classification |\n|-------------------------|-------------------|------------------|------------------------|\n| Request frequency consistency | 24-48 hours | Steady, predictable intervals | Legitimate automation |\n| Error rate patterns | 1-6 hours | Low error rates with proper backoff | Well-behaved client |\n| Endpoint usage diversity | 24 hours | Uses multiple endpoints appropriately | Application integration |\n| Response handling behavior | 30 minutes | Respects rate limit headers | Compliant client |\n| Geographic consistency | 7-30 days | Requests from consistent regions | Stable user base |\n| Time zone alignment | 7-14 days | Usage matches reasonable time zones | Human-driven usage |\n\nBased on these patterns, clients receive automatic classification:\n\n| Classification Tier | Characteristics | Rate Limit Multiplier | Additional Benefits |\n|---------------------|----------------|----------------------|-------------------|\n| Platinum | Consistent, compliant, diverse usage | 3.0x base rate | Priority support queue |\n| Gold | Regular patterns, low error rates | 2.0x base rate | Extended burst capacity |\n| Silver | Normal usage, occasional spikes | 1.5x base rate | Standard service |\n| Bronze | Basic usage, learning patterns | 1.0x base rate | Educational headers |\n| Restricted | High error rates, suspicious patterns | 0.5x base rate | Additional monitoring |\n| Quarantine | Probable abuse, requires review | 0.1x base rate | Manual review required |\n\n> **Decision: Client Classification Architecture**\n> - **Context**: Manual client tier management doesn't scale; automated classification can improve user experience while reducing operational overhead\n> - **Options Considered**:\n>   1. Simple rule-based classification using request counts\n>   2. Machine learning classification with behavioral features\n>   3. Hybrid approach with rules for obvious cases and ML for edge cases\n> - **Decision**: Start with comprehensive rule-based classification with ML integration points\n> - **Rationale**: Rule-based systems provide transparency and immediate value; ML integration points enable future enhancement without architectural changes\n> - **Consequences**: Requires careful rule tuning and monitoring to avoid false classifications; provides foundation for future ML enhancement\n\n#### Geographic and Network-Based Rate Limiting\n\n**Geographic rate limiting** applies different rate limits based on client location and network characteristics, enabling region-specific protection and compliance with local regulations.\n\n**Mental Model: The International Border Control**: Different countries have different entry requirements and processing capacities. A busy international airport might have express lanes for citizens, standard processing for visa holders, and additional screening for visitors from certain regions. Geographic rate limiting applies similar logic to API access, considering the origin and network characteristics of requests.\n\nGeographic rate limiting requires integration with IP geolocation services and network intelligence:\n\n| Geographic Factor | Rate Limit Impact | Implementation Approach | Use Case |\n|------------------|-------------------|------------------------|----------|\n| Geographic region | Region-specific base rates | IP geolocation lookup | Comply with regional regulations |\n| Network type | Adjust for mobile vs broadband | ASN and network classification | Account for network limitations |\n| Distance from servers | Latency-based adjustments | Geographic distance calculation | Compensate for network delays |\n| Country risk profile | Security-based restrictions | Configurable country classifications | Protect against geographic threats |\n| Time zone alignment | Business hours consideration | Local time calculation | Support business hour preferences |\n\nThe geographic enhancement extends `ClientIdentifier` with location context:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `ip_address` | `str` | Original client IP address |\n| `country_code` | `Optional[str]` | ISO country code from geolocation |\n| `region` | `Optional[str]` | Geographic region classification |\n| `asn` | `Optional[int]` | Autonomous System Number |\n| `network_type` | `Optional[NetworkType]` | RESIDENTIAL, BUSINESS, MOBILE, HOSTING |\n| `distance_km` | `Optional[float]` | Distance to nearest server |\n| `local_time_offset` | `Optional[int]` | Hours offset from UTC |\n\n### Monitoring and Observability\n\nProduction rate limiting systems require comprehensive monitoring to ensure proper operation, detect abuse patterns, and optimize performance. Think of monitoring as the air traffic control system for our API traffic—it needs to track every flight, predict congestion, and coordinate responses to keep the entire system flowing safely.\n\n#### Comprehensive Metrics Collection\n\n**Rate limiting metrics** provide visibility into system behavior, client patterns, and protection effectiveness. The metrics system must capture both operational health and business intelligence about API usage.\n\n**Mental Model: The Hospital Patient Monitoring System**: A hospital monitors patients at multiple levels—individual vital signs (heart rate, blood pressure), department-level statistics (admission rates, bed utilization), and hospital-wide metrics (staff efficiency, resource usage). Rate limiting monitoring works similarly, tracking individual client behavior, endpoint-level patterns, and system-wide protection effectiveness.\n\nThe metrics collection system captures data across multiple dimensions:\n\n| Metric Category | Key Metrics | Collection Frequency | Storage Duration |\n|----------------|-------------|---------------------|-----------------|\n| Request Volume | Requests/second, requests/minute | Real-time | 30 days detailed, 1 year aggregated |\n| Rate Limit Effectiveness | Allow/deny ratios, limit utilization | Real-time | 90 days detailed, 1 year aggregated |\n| Client Behavior | Top clients, error rates, geographic distribution | 1-minute intervals | 30 days detailed |\n| System Performance | Latency percentiles, memory usage, Redis performance | 10-second intervals | 7 days detailed, 30 days aggregated |\n| Algorithm Performance | Token generation rates, bucket utilization | 1-minute intervals | 30 days |\n| Error Conditions | Circuit breaker trips, Redis failures, clock drift | Immediate | 1 year |\n\n**Core Rate Limiting Metrics:**\n\n| Metric Name | Type | Labels | Description |\n|------------|------|---------|-------------|\n| `rate_limit_requests_total` | Counter | `client_id`, `endpoint`, `result` | Total requests processed |\n| `rate_limit_tokens_consumed` | Counter | `client_id`, `endpoint` | Total tokens consumed |\n| `rate_limit_bucket_utilization` | Gauge | `client_id`, `endpoint` | Current bucket fill percentage |\n| `rate_limit_processing_duration` | Histogram | `component`, `operation` | Processing latency distribution |\n| `rate_limit_active_clients` | Gauge | `classification_tier` | Number of active clients by tier |\n| `rate_limit_redis_operations` | Counter | `operation`, `result` | Redis operation counts and results |\n\n> **Decision: Metrics Collection Architecture**\n> - **Context**: Rate limiting decisions happen in the critical request path; metrics collection must not impact request latency\n> - **Options Considered**:\n>   1. Synchronous metrics collection with request processing\n>   2. Asynchronous metrics with background aggregation\n>   3. Sampling-based metrics to reduce overhead\n> - **Decision**: Asynchronous metrics with configurable sampling for high-volume endpoints\n> - **Rationale**: Asynchronous collection eliminates metrics impact on request latency; sampling reduces overhead while maintaining statistical accuracy\n> - **Consequences**: Slight delay in metrics availability; requires careful sampling strategy to avoid bias\n\n#### Real-Time Dashboard and Alerting\n\n**Real-time dashboards** provide immediate visibility into rate limiting behavior, while **intelligent alerting** ensures rapid response to anomalous conditions.\n\n**Mental Model: The Mission Control Center**: NASA's mission control monitors spacecraft with multiple screens showing different aspects of mission health—trajectory, system status, communication quality, and crew vitals. Each screen serves different roles: flight directors see high-level mission status, engineers monitor specific subsystems, and specialists track their areas of expertise. Rate limiting dashboards serve similar roles for different operational teams.\n\nThe dashboard system provides role-specific views:\n\n| Dashboard View | Target Audience | Key Visualizations | Update Frequency |\n|----------------|----------------|-------------------|-----------------|\n| Executive Summary | Management, SRE leadership | API health score, major incidents | 5-minute intervals |\n| Operations Overview | SRE, DevOps teams | Request rates, error rates, system load | 30-second intervals |\n| Security Monitoring | Security teams | Abuse patterns, geographic anomalies | Real-time |\n| Client Experience | Product teams | Client-specific metrics, tier distributions | 1-minute intervals |\n| System Performance | Platform engineers | Latency, throughput, resource utilization | 10-second intervals |\n\n**Critical Alert Conditions:**\n\n| Alert Condition | Severity | Trigger Threshold | Response Required |\n|----------------|----------|------------------|-------------------|\n| Mass rate limiting triggered | Critical | >50% of requests denied for 5+ minutes | Immediate investigation |\n| Redis cluster failure | Critical | Circuit breaker open for 2+ minutes | Failover activation |\n| Abnormal client behavior | High | Single client >10x normal rate | Security review |\n| System performance degradation | High | P95 latency >500ms for 10+ minutes | Performance investigation |\n| Geographic traffic anomaly | Medium | 5x increase from unusual regions | Security monitoring |\n| Token bucket calculation errors | Medium | >1% calculation failures | Algorithm review |\n\nThe alerting system implements intelligent alert aggregation to prevent notification fatigue:\n\n| Aggregation Strategy | Time Window | Condition | Result |\n|---------------------|-------------|-----------|--------|\n| Alert deduplication | 15 minutes | Same condition, same resource | Single notification |\n| Storm detection | 5 minutes | >10 alerts from same category | Storm summary alert |\n| Escalation | 30-60 minutes | Alert not acknowledged | Escalate to next tier |\n| Auto-resolution | Variable | Condition clears for 2x trigger duration | Auto-close alert |\n\n#### Advanced Analytics and Insights\n\n**Advanced analytics** transform raw rate limiting data into actionable business and operational intelligence, helping teams optimize API strategy and improve system efficiency.\n\n**Mental Model: The Traffic Engineering Department**: City traffic engineers don't just manage traffic lights—they analyze traffic patterns to optimize road design, predict future capacity needs, and identify improvement opportunities. They study rush hour flows, accident patterns, and seasonal variations to make data-driven infrastructure decisions. Rate limiting analytics serve the same strategic purpose for API infrastructure.\n\nThe analytics system provides multiple analytical capabilities:\n\n| Analytics Category | Key Insights | Analysis Methods | Business Value |\n|-------------------|--------------|------------------|----------------|\n| Usage Pattern Analysis | Peak usage times, seasonal trends | Time series analysis, trend detection | Capacity planning, cost optimization |\n| Client Behavior Segmentation | Client archetypes, usage evolution | Clustering, behavioral analysis | Product strategy, tier optimization |\n| Abuse Detection and Prevention | Attack patterns, bot identification | Anomaly detection, pattern recognition | Security improvement, cost reduction |\n| Performance Optimization | Bottlenecks, efficiency opportunities | Performance profiling, correlation analysis | System optimization, user experience |\n| Revenue Impact Analysis | Rate limiting effects on business metrics | A/B testing, causal analysis | Business optimization, pricing strategy |\n\n**Advanced Analytics Queries:**\n\n| Analysis Type | Query Pattern | Insight Generated |\n|--------------|---------------|-------------------|\n| Client Lifecycle Analysis | Track client behavior evolution over 30-90 days | Identify clients ready for tier upgrades |\n| Geographic Usage Patterns | Analyze request patterns by region and time | Optimize server placement and capacity |\n| Endpoint Popularity Trends | Track endpoint usage changes over time | Guide API development priorities |\n| Error Pattern Analysis | Correlate error rates with rate limiting | Identify configuration optimization opportunities |\n| Business Impact Correlation | Connect rate limiting with revenue metrics | Quantify business impact of protection policies |\n\nThe analytics system integrates with the rate limiting infrastructure through dedicated data collection:\n\n| Component | Purpose | Data Collection | Analysis Output |\n|-----------|---------|----------------|-----------------|\n| `AnalyticsCollector` | Structured data gathering | Request metadata, timing, outcomes | Standardized datasets for analysis |\n| `PatternAnalyzer` | Behavior pattern detection | Client request sequences, timing patterns | Client classification recommendations |\n| `AnomalyDetector` | Unusual behavior identification | Statistical analysis of request patterns | Security alerts and recommendations |\n| `BusinessMetricsIntegrator` | Connect technical and business metrics | Rate limiting outcomes, business KPIs | ROI analysis and optimization suggestions |\n\n### Implementation Guidance\n\nThe future extensions require careful architectural planning to ensure they integrate seamlessly with the existing rate limiting infrastructure while maintaining performance and reliability.\n\n#### Technology Recommendations\n\n| Extension Category | Simple Implementation | Advanced Implementation |\n|-------------------|----------------------|------------------------|\n| Sliding Window Storage | In-memory deque with timestamps | Redis sorted sets with Lua scripts |\n| Dynamic Adjustment | Simple rule engine with thresholds | Machine learning pipeline with feature engineering |\n| Client Classification | Rule-based scoring with configurable weights | Behavioral analysis with clustering algorithms |\n| Geographic Services | MaxMind GeoLite2 database | Commercial IP intelligence with real-time updates |\n| Metrics Collection | Prometheus client with local aggregation | InfluxDB with Telegraph agent and custom collectors |\n| Analytics Platform | Grafana dashboards with basic queries | Elasticsearch with Kibana and custom analytics |\n\n#### Extension Architecture Integration\n\nThe extensions integrate with the existing rate limiting architecture through well-defined interfaces that maintain backward compatibility while enabling sophisticated new capabilities.\n\n**File Structure for Extensions:**\n\n```\nrate_limiter/\n  core/                           # Existing core implementation\n    token_bucket.py\n    client_tracker.py\n    middleware.py\n    distributed.py\n  algorithms/                     # Alternative algorithms\n    sliding_window.py             # Sliding window implementation\n    leaky_bucket.py              # Leaky bucket implementation  \n    algorithm_factory.py         # Algorithm selection logic\n  intelligence/                   # Advanced features\n    dynamic_adjustment.py        # Dynamic rate adjustment\n    client_classifier.py         # Intelligent client classification\n    geographic_limiter.py        # Geographic rate limiting\n  monitoring/                     # Observability extensions\n    metrics_collector.py         # Comprehensive metrics\n    dashboard_server.py          # Real-time dashboard\n    analytics_engine.py          # Advanced analytics\n  extensions/                     # Integration points\n    extension_manager.py         # Manages extension lifecycle\n    hook_registry.py            # Extension hook system\n```\n\n#### Algorithm Extension Framework\n\nThe algorithm extension framework allows adding new rate limiting algorithms without modifying existing code:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass RateLimitDecision:\n    \"\"\"Result of rate limiting decision with algorithm-specific context.\"\"\"\n    allowed: bool\n    tokens_remaining: Optional[int]\n    retry_after_seconds: float\n    algorithm_state: Dict[str, Any]\n    debug_info: Optional[Dict[str, Any]]\n\nclass RateLimitAlgorithm(ABC):\n    \"\"\"Abstract base class for rate limiting algorithms.\"\"\"\n    \n    @abstractmethod\n    def try_consume(self, client_id: str, endpoint: str, tokens_requested: int, \n                   current_time: float) -> RateLimitDecision:\n        \"\"\"\n        Attempt to consume tokens using this algorithm.\n        \n        TODO 1: Retrieve or create algorithm state for this client/endpoint combination\n        TODO 2: Apply algorithm-specific logic to determine if request should be allowed\n        TODO 3: Update algorithm state based on the consumption decision\n        TODO 4: Calculate retry_after_seconds based on algorithm characteristics\n        TODO 5: Prepare debug information for troubleshooting and monitoring\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def get_algorithm_info(self) -> Dict[str, Any]:\n        \"\"\"Return algorithm metadata for monitoring and debugging.\"\"\"\n        pass\n\nclass SlidingWindowAlgorithm(RateLimitAlgorithm):\n    \"\"\"Sliding window rate limiting implementation.\"\"\"\n    \n    def __init__(self, window_seconds: int, max_requests: int, \n                 storage: RateLimitStorage):\n        # TODO 1: Store configuration parameters\n        # TODO 2: Initialize storage backend for request timestamps\n        # TODO 3: Set up cleanup scheduling for expired timestamps\n        pass\n        \n    def try_consume(self, client_id: str, endpoint: str, tokens_requested: int,\n                   current_time: float) -> RateLimitDecision:\n        # TODO 1: Calculate window start time (current_time - window_seconds)\n        # TODO 2: Clean up expired request timestamps before window start\n        # TODO 3: Count existing requests within the current window\n        # TODO 4: Check if adding tokens_requested would exceed max_requests\n        # TODO 5: If allowed, record the new request timestamp(s)\n        # TODO 6: Calculate retry_after based on oldest request in window\n        # TODO 7: Return decision with current window state\n        pass\n```\n\n#### Dynamic Adjustment Implementation Framework\n\nThe dynamic adjustment system provides a pluggable architecture for implementing various adjustment strategies:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport time\n\n@dataclass\nclass AdjustmentContext:\n    \"\"\"Context information for adjustment decisions.\"\"\"\n    current_time: float\n    system_cpu_percent: float\n    system_memory_percent: float\n    redis_latency_p95: float\n    error_rate_percent: float\n    active_clients_count: int\n    recent_adjustments: List['RateAdjustment']\n\n@dataclass\nclass RateAdjustment:\n    \"\"\"Represents a rate limit adjustment decision.\"\"\"\n    client_pattern: str  # Pattern matching clients to adjust\n    endpoint_pattern: str  # Pattern matching endpoints to adjust\n    adjustment_factor: float  # Multiplier for current rate limit\n    reason: str  # Human-readable reason for adjustment\n    expires_at: float  # When this adjustment expires\n    confidence_score: float  # Confidence in this adjustment (0.0-1.0)\n\nclass AdjustmentStrategy(ABC):\n    \"\"\"Abstract base class for dynamic adjustment strategies.\"\"\"\n    \n    @abstractmethod\n    def evaluate_adjustments(self, context: AdjustmentContext) -> List[RateAdjustment]:\n        \"\"\"\n        Evaluate current conditions and return recommended adjustments.\n        \n        TODO 1: Analyze context metrics for adjustment triggers\n        TODO 2: Calculate appropriate adjustment factors based on conditions\n        TODO 3: Determine scope of adjustments (which clients/endpoints)\n        TODO 4: Set expiration times for temporary adjustments\n        TODO 5: Assign confidence scores based on data quality and certainty\n        \"\"\"\n        pass\n\nclass LoadBasedAdjustmentStrategy(AdjustmentStrategy):\n    \"\"\"Adjusts rate limits based on system load metrics.\"\"\"\n    \n    def __init__(self, cpu_threshold: float = 80.0, memory_threshold: float = 85.0,\n                 max_reduction: float = 0.5):\n        # TODO 1: Store threshold configurations\n        # TODO 2: Initialize metrics history tracking\n        # TODO 3: Set up adjustment calculation parameters\n        pass\n        \n    def evaluate_adjustments(self, context: AdjustmentContext) -> List[RateAdjustment]:\n        # TODO 1: Check if CPU usage exceeds threshold\n        # TODO 2: Check if memory usage exceeds threshold  \n        # TODO 3: Calculate adjustment factor based on severity of overload\n        # TODO 4: Determine which client tiers to adjust (start with lowest priority)\n        # TODO 5: Set appropriate expiration times (longer for severe overload)\n        # TODO 6: Return list of adjustment recommendations\n        pass\n\nclass DynamicRateAdjuster:\n    \"\"\"Coordinates dynamic rate limit adjustments.\"\"\"\n    \n    def __init__(self, strategies: List[AdjustmentStrategy], \n                 rate_limiter: DistributedRateLimiter):\n        # TODO 1: Store adjustment strategies and their priorities\n        # TODO 2: Initialize rate limiter reference for applying adjustments\n        # TODO 3: Set up background monitoring and adjustment threads\n        # TODO 4: Initialize adjustment history tracking\n        pass\n```\n\n#### Monitoring Extension Points\n\nThe monitoring extensions provide comprehensive observability without impacting request processing performance:\n\n```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Callable\nimport asyncio\nfrom collections import defaultdict\n\n@dataclass \nclass MetricDataPoint:\n    \"\"\"Individual metric measurement.\"\"\"\n    timestamp: float\n    metric_name: str\n    value: float\n    labels: Dict[str, str]\n    metadata: Optional[Dict[str, str]] = None\n\nclass MetricsCollector:\n    \"\"\"High-performance metrics collection with async processing.\"\"\"\n    \n    def __init__(self, buffer_size: int = 10000, flush_interval: float = 30.0):\n        # TODO 1: Initialize ring buffer for metric storage\n        # TODO 2: Set up async processing queue and background tasks\n        # TODO 3: Configure metric aggregation and sampling strategies\n        # TODO 4: Initialize connections to metric storage systems\n        pass\n        \n    def record_rate_limit_decision(self, client_id: str, endpoint: str, \n                                 result: TokenConsumptionResult):\n        \"\"\"Record rate limiting decision metrics asynchronously.\"\"\"\n        # TODO 1: Extract key metrics from consumption result\n        # TODO 2: Create metric data points for counters and gauges\n        # TODO 3: Add to async processing queue (non-blocking)\n        # TODO 4: Update internal aggregations for dashboard queries\n        pass\n        \n    def record_system_performance(self, component: str, operation: str, \n                                duration_ms: float, success: bool):\n        \"\"\"Record system performance metrics.\"\"\"\n        # TODO 1: Create performance metric data points\n        # TODO 2: Update histogram buckets for latency distribution\n        # TODO 3: Track error rates and success rates by component\n        # TODO 4: Queue for async processing and storage\n        pass\n\nclass RealTimeDashboard:\n    \"\"\"WebSocket-based real-time dashboard server.\"\"\"\n    \n    def __init__(self, metrics_collector: MetricsCollector, port: int = 8080):\n        # TODO 1: Initialize web server and WebSocket handling\n        # TODO 2: Set up metric query and aggregation endpoints  \n        # TODO 3: Configure dashboard update intervals and data retention\n        # TODO 4: Initialize alert condition monitoring\n        pass\n        \n    async def stream_metrics(self, websocket_connection, dashboard_type: str):\n        \"\"\"Stream real-time metrics to dashboard clients.\"\"\"\n        # TODO 1: Determine metric subset based on dashboard type\n        # TODO 2: Set up periodic metric queries and aggregations\n        # TODO 3: Stream formatted data to WebSocket clients\n        # TODO 4: Handle client disconnections and reconnections\n        pass\n```\n\n#### Milestone Checkpoint: Extensions Integration\n\nAfter implementing future extensions, verify the enhanced system behavior:\n\n**Extension Testing Commands:**\n```bash\n# Test alternative algorithms\npython -m rate_limiter.test_extensions --test-sliding-window\npython -m rate_limiter.test_extensions --test-leaky-bucket\n\n# Test dynamic adjustment\npython -m rate_limiter.test_extensions --test-dynamic-adjustment --simulate-load\n\n# Test client classification  \npython -m rate_limiter.test_extensions --test-client-classification --duration 300\n\n# Test monitoring and analytics\npython -m rate_limiter.test_extensions --test-monitoring --generate-traffic\n```\n\n**Expected Extension Behaviors:**\n1. **Alternative Algorithms**: Sliding window should reject bursts that token bucket would allow; leaky bucket should smooth request processing\n2. **Dynamic Adjustment**: Rate limits should decrease under simulated load and recover when load returns to normal\n3. **Client Classification**: Consistent well-behaved clients should receive higher rate limits over time\n4. **Monitoring**: Real-time dashboard should display current metrics; alerts should trigger for anomalous conditions\n\n**Common Extension Issues:**\n\n⚠️ **Pitfall: Extension Performance Impact**\nExtensions can inadvertently impact rate limiting performance if not implemented carefully. All extension processing should be asynchronous and non-blocking relative to the critical request path.\n\n⚠️ **Pitfall: Configuration Complexity**\nAdvanced features introduce significant configuration complexity. Provide sensible defaults and clear documentation to prevent misconfiguration that could disable protection or create operational issues.\n\n⚠️ **Pitfall: Extension Interaction Effects**  \nMultiple extensions can interact in unexpected ways. Dynamic adjustment might conflict with client classification, or geographic limiting might interfere with algorithm selection. Design extensions with clear precedence rules and conflict resolution strategies.\n\nThe future extensions transform the basic rate limiter into a sophisticated API protection platform capable of handling enterprise-scale requirements while maintaining the reliability and performance of the core implementation. Each extension builds upon the solid foundation established in the earlier milestones, demonstrating how well-designed systems can grow to meet evolving requirements without sacrificing their essential characteristics.\n\n### Implementation Guidance\n\nBuilding future extensions requires careful attention to maintainability, performance, and integration complexity. The extensions should enhance the rate limiter's capabilities while preserving the simplicity and reliability of the core implementation.\n\n#### Technology Recommendations\n\n| Extension Type | Simple Implementation | Production Implementation |\n|---------------|----------------------|---------------------------|\n| Sliding Window | Python deque with threading.Lock | Redis sorted sets with Lua atomic operations |\n| Client Classification | Rule-based scoring with JSON config | Feature engineering pipeline with ML models |\n| Dynamic Adjustment | Threshold-based rules with simple PID controller | Multi-factor adjustment with predictive modeling |\n| Geographic Intelligence | MaxMind GeoLite2 with local database | Real-time IP intelligence with CDN integration |\n| Monitoring Stack | Prometheus + Grafana with basic dashboards | Full observability with distributed tracing |\n| Analytics Platform | SQLite with pandas for analysis | Time-series database with streaming analytics |\n\n#### Extension Architecture Implementation\n\n```python\n# extensions/extension_manager.py\nfrom typing import Dict, List, Type, Any, Optional\nfrom abc import ABC, abstractmethod\nimport importlib\nimport logging\nfrom dataclasses import dataclass\n\n@dataclass\nclass ExtensionConfig:\n    \"\"\"Configuration for rate limiter extensions.\"\"\"\n    enabled_extensions: List[str]\n    extension_configs: Dict[str, Dict[str, Any]]\n    extension_priority: Dict[str, int]\n    hot_reload_enabled: bool = False\n\nclass RateLimiterExtension(ABC):\n    \"\"\"Base class for all rate limiter extensions.\"\"\"\n    \n    @abstractmethod\n    def initialize(self, rate_limiter, config: Dict[str, Any]) -> bool:\n        \"\"\"Initialize extension with rate limiter instance and config.\"\"\"\n        pass\n    \n    @abstractmethod  \n    def get_extension_info(self) -> Dict[str, Any]:\n        \"\"\"Return extension metadata and current status.\"\"\"\n        pass\n    \n    def on_rate_limit_decision(self, context: Dict[str, Any]) -> None:\n        \"\"\"Hook called after each rate limiting decision.\"\"\"\n        pass\n        \n    def on_client_registered(self, client_id: str) -> None:\n        \"\"\"Hook called when new client is registered.\"\"\"\n        pass\n        \n    def on_bucket_cleanup(self, cleaned_buckets: List[str]) -> None:\n        \"\"\"Hook called after bucket cleanup operations.\"\"\"\n        pass\n\nclass ExtensionManager:\n    \"\"\"Manages lifecycle of rate limiter extensions.\"\"\"\n    \n    def __init__(self, config: ExtensionConfig):\n        # TODO 1: Store extension configuration and initialize registry\n        # TODO 2: Set up extension loading and dependency resolution\n        # TODO 3: Initialize hook system for extension callbacks\n        # TODO 4: Set up hot reload monitoring if enabled\n        self.extensions: Dict[str, RateLimiterExtension] = {}\n        self.extension_hooks: Dict[str, List[Callable]] = defaultdict(list)\n        \n    def load_extensions(self, rate_limiter) -> Dict[str, bool]:\n        \"\"\"\n        Load and initialize all configured extensions.\n        \n        TODO 1: Iterate through enabled extensions list\n        TODO 2: Dynamically import each extension module\n        TODO 3: Instantiate extension class with configuration\n        TODO 4: Call extension initialize method with rate limiter\n        TODO 5: Register extension hooks for callbacks\n        TODO 6: Return success/failure status for each extension\n        \"\"\"\n        results = {}\n        for ext_name in self.config.enabled_extensions:\n            try:\n                # Dynamic extension loading implementation\n                results[ext_name] = True\n            except Exception as e:\n                logging.error(f\"Failed to load extension {ext_name}: {e}\")\n                results[ext_name] = False\n        return results\n```\n\n#### Advanced Algorithm Implementation\n\n```python\n# algorithms/sliding_window.py\nfrom typing import Dict, List, Optional, Tuple\nfrom collections import deque\nimport time\nimport threading\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass SlidingWindowState:\n    \"\"\"State for sliding window rate limiting.\"\"\"\n    request_timestamps: deque = field(default_factory=deque)\n    last_cleanup: float = 0.0\n    total_requests: int = 0\n    window_seconds: int = 60\n    max_requests: int = 100\n\nclass SlidingWindowRateLimiter:\n    \"\"\"Thread-safe sliding window rate limiter implementation.\"\"\"\n    \n    def __init__(self, default_window: int = 60, default_max_requests: int = 100):\n        # TODO 1: Initialize default configuration parameters\n        # TODO 2: Create thread-safe storage for client window states\n        # TODO 3: Set up periodic cleanup of expired timestamps\n        # TODO 4: Initialize performance monitoring and metrics\n        self._client_windows: Dict[str, SlidingWindowState] = {}\n        self._lock = threading.RWLock()\n        self._cleanup_interval = 300  # 5 minutes\n        \n    def try_consume(self, client_id: str, tokens_requested: int = 1, \n                   current_time: Optional[float] = None) -> TokenConsumptionResult:\n        \"\"\"\n        Attempt to consume tokens using sliding window algorithm.\n        \n        TODO 1: Get current time if not provided\n        TODO 2: Acquire read lock and get client window state\n        TODO 3: Clean up expired timestamps from the window\n        TODO 4: Count current requests in the sliding window\n        TODO 5: Check if adding requested tokens would exceed limit\n        TODO 6: If allowed, add new timestamps and update state\n        TODO 7: Calculate retry_after based on oldest timestamp\n        TODO 8: Return consumption result with window information\n        \"\"\"\n        if current_time is None:\n            current_time = time.time()\n            \n        with self._lock.read_lock():\n            # Implementation of sliding window logic\n            pass\n            \n    def _cleanup_expired_timestamps(self, window_state: SlidingWindowState, \n                                  current_time: float) -> int:\n        \"\"\"Remove expired timestamps from window.\"\"\"\n        # TODO 1: Calculate cutoff time (current_time - window_seconds)\n        # TODO 2: Remove timestamps older than cutoff from deque\n        # TODO 3: Update total_requests counter\n        # TODO 4: Return count of removed timestamps\n        pass\n        \n    def _get_or_create_window(self, client_id: str) -> SlidingWindowState:\n        \"\"\"Thread-safe retrieval or creation of client window state.\"\"\"\n        # TODO 1: Try to get existing window state with read lock\n        # TODO 2: If not found, acquire write lock and create new state\n        # TODO 3: Handle race condition where state created between locks\n        # TODO 4: Return window state\n        pass\n```\n\n#### Dynamic Intelligence Framework\n\n```python\n# intelligence/client_classifier.py\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport statistics\nimport time\n\n@dataclass\nclass ClientProfile:\n    \"\"\"Profile of client behavior for classification.\"\"\"\n    total_requests: int = 0\n    error_count: int = 0\n    avg_interval_seconds: float = 0.0\n    endpoints_used: set = None\n    geographic_regions: set = None\n    first_seen: float = 0.0\n    last_seen: float = 0.0\n    classification: str = \"unknown\"\n    confidence_score: float = 0.0\n    \n    def __post_init__(self):\n        if self.endpoints_used is None:\n            self.endpoints_used = set()\n        if self.geographic_regions is None:\n            self.geographic_regions = set()\n\nclass IntelligentClientClassifier:\n    \"\"\"Automatic client classification based on behavior analysis.\"\"\"\n    \n    def __init__(self, observation_period: int = 86400):  # 24 hours\n        # TODO 1: Initialize observation period and classification thresholds\n        # TODO 2: Set up client profile storage and tracking\n        # TODO 3: Configure classification rules and scoring system\n        # TODO 4: Initialize background analysis and reclassification\n        self._profiles: Dict[str, ClientProfile] = {}\n        self._request_intervals: Dict[str, List[float]] = defaultdict(list)\n        \n    def track_request(self, client_id: str, endpoint: str, success: bool,\n                     timestamp: Optional[float] = None, region: Optional[str] = None):\n        \"\"\"\n        Track client request for behavior analysis.\n        \n        TODO 1: Get or create client profile\n        TODO 2: Update request counts and success/error ratios  \n        TODO 3: Record endpoint usage and geographic information\n        TODO 4: Calculate request interval statistics\n        TODO 5: Update timestamps and trigger reclassification if needed\n        \"\"\"\n        if timestamp is None:\n            timestamp = time.time()\n            \n        # Implementation of request tracking\n        pass\n        \n    def classify_client(self, client_id: str) -> Tuple[str, float]:\n        \"\"\"\n        Classify client based on observed behavior patterns.\n        \n        TODO 1: Retrieve client profile and calculate behavior metrics\n        TODO 2: Apply classification rules for each tier level\n        TODO 3: Calculate confidence score based on observation completeness\n        TODO 4: Update profile with new classification and confidence\n        TODO 5: Return classification tier and confidence score\n        \"\"\"\n        profile = self._profiles.get(client_id)\n        if not profile:\n            return \"unknown\", 0.0\n            \n        # Classification logic implementation\n        pass\n        \n    def _calculate_behavior_score(self, profile: ClientProfile) -> Dict[str, float]:\n        \"\"\"Calculate behavior scoring metrics.\"\"\"\n        # TODO 1: Calculate consistency score from request intervals\n        # TODO 2: Calculate compliance score from error rates\n        # TODO 3: Calculate diversity score from endpoint usage\n        # TODO 4: Calculate maturity score from observation period\n        # TODO 5: Return comprehensive behavior scoring\n        pass\n        \n    def get_classification_recommendations(self) -> List[Dict[str, Any]]:\n        \"\"\"Get recommendations for client tier adjustments.\"\"\"\n        # TODO 1: Identify clients ready for tier upgrades\n        # TODO 2: Identify clients that should be downgraded\n        # TODO 3: Calculate potential impact of tier changes\n        # TODO 4: Return prioritized list of recommendations\n        pass\n```\n\n#### Production Monitoring Implementation\n\n```python\n# monitoring/advanced_analytics.py\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport asyncio\n\n@dataclass\nclass AnalyticsQuery:\n    \"\"\"Configuration for analytics query execution.\"\"\"\n    query_type: str\n    time_range: Tuple[float, float] \n    filters: Dict[str, Any]\n    aggregation: str\n    output_format: str = \"json\"\n\nclass RateLimitAnalyticsEngine:\n    \"\"\"Advanced analytics for rate limiting behavior and optimization.\"\"\"\n    \n    def __init__(self, metrics_collector, data_retention_days: int = 90):\n        # TODO 1: Initialize connection to metrics data store\n        # TODO 2: Set up data retention and archival policies\n        # TODO 3: Configure analytical processing capabilities\n        # TODO 4: Initialize caching for frequently-run analyses\n        self.metrics_collector = metrics_collector\n        self.query_cache: Dict[str, Any] = {}\n        \n    async def analyze_client_behavior_patterns(self, \n                                             time_range_hours: int = 24) -> Dict[str, Any]:\n        \"\"\"\n        Analyze client behavior patterns for optimization opportunities.\n        \n        TODO 1: Query request data for specified time range\n        TODO 2: Group requests by client and calculate behavior metrics\n        TODO 3: Identify patterns in request timing and frequency\n        TODO 4: Detect anomalies and unusual behavior\n        TODO 5: Generate recommendations for rate limit adjustments\n        \"\"\"\n        # Implementation of behavior pattern analysis\n        pass\n        \n    async def calculate_rate_limit_effectiveness(self) -> Dict[str, float]:\n        \"\"\"\n        Calculate effectiveness metrics for current rate limiting policies.\n        \n        TODO 1: Query rate limiting decision data\n        TODO 2: Calculate allow/deny ratios by client tier and endpoint\n        TODO 3: Analyze correlation between limits and client satisfaction\n        TODO 4: Identify over-restrictive and under-restrictive configurations\n        TODO 5: Return effectiveness scores and improvement opportunities\n        \"\"\"\n        pass\n        \n    def generate_capacity_planning_report(self) -> Dict[str, Any]:\n        \"\"\"Generate capacity planning recommendations based on trends.\"\"\"\n        # TODO 1: Analyze historical growth patterns in API usage\n        # TODO 2: Project future capacity requirements\n        # TODO 3: Identify seasonal and cyclical patterns\n        # TODO 4: Calculate infrastructure scaling recommendations\n        # TODO 5: Return comprehensive capacity planning report\n        pass\n        \n    async def detect_abuse_patterns(self, sensitivity: float = 0.95) -> List[Dict[str, Any]]:\n        \"\"\"\n        Detect potential abuse patterns using statistical analysis.\n        \n        TODO 1: Apply anomaly detection algorithms to request patterns\n        TODO 2: Identify clients with suspicious behavior profiles\n        TODO 3: Correlate geographic and timing patterns for abuse detection\n        TODO 4: Score abuse likelihood and confidence levels\n        TODO 5: Return prioritized list of potential abuse cases\n        \"\"\"\n        pass\n\n# monitoring/real_time_dashboard.py\nimport asyncio\nimport websockets\nimport json\nfrom typing import Set, Dict, Any\nfrom datetime import datetime, timedelta\n\nclass RealTimeDashboardServer:\n    \"\"\"WebSocket-based real-time dashboard for rate limiting metrics.\"\"\"\n    \n    def __init__(self, metrics_collector, analytics_engine, port: int = 8080):\n        # TODO 1: Initialize WebSocket server configuration\n        # TODO 2: Set up metric streaming and update intervals\n        # TODO 3: Configure dashboard layouts and visualizations\n        # TODO 4: Initialize alert condition monitoring\n        self.metrics_collector = metrics_collector\n        self.analytics_engine = analytics_engine\n        self.connected_clients: Set[websockets.WebSocketServerProtocol] = set()\n        \n    async def start_server(self):\n        \"\"\"Start the WebSocket dashboard server.\"\"\"\n        # TODO 1: Start WebSocket server on configured port\n        # TODO 2: Begin background metric streaming tasks\n        # TODO 3: Initialize alert monitoring and notification\n        # TODO 4: Set up graceful shutdown handling\n        pass\n        \n    async def handle_client_connection(self, websocket, path):\n        \"\"\"\n        Handle new dashboard client connections.\n        \n        TODO 1: Register new client connection\n        TODO 2: Send initial dashboard configuration and current metrics\n        TODO 3: Handle client-specific dashboard requests\n        TODO 4: Stream real-time updates until disconnection\n        TODO 5: Clean up client resources on disconnect\n        \"\"\"\n        pass\n        \n    async def stream_metrics_updates(self):\n        \"\"\"Background task to stream metric updates to all connected clients.\"\"\"\n        # TODO 1: Continuously query latest metrics from collector\n        # TODO 2: Format metrics for dashboard consumption\n        # TODO 3: Broadcast updates to all connected clients\n        # TODO 4: Handle client disconnections and errors gracefully\n        pass\n        \n    def generate_dashboard_config(self, dashboard_type: str) -> Dict[str, Any]:\n        \"\"\"Generate configuration for specific dashboard types.\"\"\"\n        # TODO 1: Define available dashboard layouts and widgets\n        # TODO 2: Configure metrics queries and update frequencies\n        # TODO 3: Set up alert conditions and notification preferences\n        # TODO 4: Return dashboard configuration for client setup\n        pass\n```\n\n#### Extension Testing and Validation\n\n```python\n# test/test_extensions.py\nimport pytest\nimport asyncio\nimport time\nfrom unittest.mock import Mock, patch\nfrom rate_limiter.extensions.extension_manager import ExtensionManager\nfrom rate_limiter.algorithms.sliding_window import SlidingWindowRateLimiter\nfrom rate_limiter.intelligence.client_classifier import IntelligentClientClassifier\n\nclass TestExtensionIntegration:\n    \"\"\"Integration tests for rate limiter extensions.\"\"\"\n    \n    @pytest.fixture\n    def extension_manager(self):\n        \"\"\"Set up extension manager for testing.\"\"\"\n        # TODO 1: Create test configuration with all extensions enabled\n        # TODO 2: Initialize extension manager with test config\n        # TODO 3: Set up mock rate limiter for extension integration\n        # TODO 4: Return configured extension manager\n        pass\n        \n    def test_sliding_window_accuracy(self):\n        \"\"\"Test sliding window algorithm accuracy under various load patterns.\"\"\"\n        # TODO 1: Initialize sliding window with known parameters\n        # TODO 2: Generate predictable request patterns\n        # TODO 3: Verify exact request counting within sliding windows\n        # TODO 4: Test edge cases around window boundaries\n        # TODO 5: Validate retry-after calculations\n        pass\n        \n    def test_client_classification_evolution(self):\n        \"\"\"Test client classification changes based on behavior patterns.\"\"\"\n        # TODO 1: Initialize client classifier with test parameters\n        # TODO 2: Simulate different client behavior patterns over time\n        # TODO 3: Verify classification changes match expected progressions\n        # TODO 4: Test classification confidence scoring\n        # TODO 5: Validate tier upgrade and downgrade recommendations\n        pass\n        \n    @pytest.mark.asyncio\n    async def test_dynamic_adjustment_response(self):\n        \"\"\"Test dynamic rate adjustment response to system conditions.\"\"\"\n        # TODO 1: Set up dynamic adjustment with test strategies\n        # TODO 2: Simulate various system load conditions\n        # TODO 3: Verify adjustment recommendations match expected responses\n        # TODO 4: Test adjustment application and rollback\n        # TODO 5: Validate adjustment coordination across multiple strategies\n        pass\n        \n    def test_extension_performance_impact(self):\n        \"\"\"Verify extensions don't significantly impact request processing performance.\"\"\"\n        # TODO 1: Measure baseline request processing latency\n        # TODO 2: Enable extensions one by one and measure impact\n        # TODO 3: Verify latency increase stays within acceptable bounds\n        # TODO 4: Test under high concurrent load\n        # TODO 5: Validate async processing doesn't block request path\n        pass\n\ndef run_extension_load_test():\n    \"\"\"Run comprehensive load test of extended rate limiter.\"\"\"\n    # TODO 1: Set up rate limiter with all extensions enabled\n    # TODO 2: Generate realistic mixed client traffic patterns\n    # TODO 3: Monitor extension behavior under sustained load\n    # TODO 4: Verify extensions continue working correctly under stress\n    # TODO 5: Report performance characteristics and bottlenecks\n    pass\n```\n\n#### Milestone Checkpoint: Extensions Validation\n\nAfter implementing the future extensions, validate the enhanced system capabilities:\n\n**Verification Steps:**\n1. **Algorithm Switching**: Configure different algorithms for different endpoints and verify they behave according to their characteristics\n2. **Dynamic Adjustment**: Simulate system load and observe automatic rate limit adjustments\n3. **Client Classification**: Run diverse client patterns and verify automatic tier assignments\n4. **Analytics Dashboard**: Generate traffic and observe real-time metrics and insights\n5. **End-to-End Integration**: Verify all extensions work together without conflicts\n\n**Performance Benchmarks:**\n- Extension overhead should add <5ms to request processing latency\n- Memory usage should remain stable under sustained load\n- Analytics processing should not impact rate limiting decisions\n- Dashboard should update within 30 seconds of metric changes\n\nThe future extensions demonstrate how a well-architected system can evolve to meet sophisticated requirements while maintaining its core reliability and performance characteristics. Each extension follows established patterns and interfaces, ensuring the enhanced system remains maintainable and debuggable as it grows in capability and complexity.\n\n\n## Glossary\n\n> **Milestone(s):** All milestones - this section provides definitions for all technical terms, rate limiting concepts, and domain-specific vocabulary used throughout the document\n\n### Mental Model: The Technical Dictionary\n\nThink of this glossary as your technical dictionary for the rate limiting domain. Just as a medical dictionary defines specialized terms like \"tachycardia\" or \"myocardial infarction\" in precise, unambiguous ways, this glossary defines rate limiting terminology with exact meanings. Each term has been carefully chosen to eliminate confusion and ensure consistent communication throughout the implementation. When you see \"token bucket algorithm\" versus \"leaky bucket algorithm,\" these aren't interchangeable terms - they represent fundamentally different approaches with distinct characteristics and trade-offs.\n\nThe glossary is organized into logical categories: core algorithms and concepts, system architecture terms, distributed systems terminology, error handling vocabulary, testing and debugging concepts, and future extension terminology. This structure mirrors the learning journey from basic rate limiting concepts through advanced distributed implementations.\n\n### Core Rate Limiting Terminology\n\nThe foundation of rate limiting rests on precise algorithmic and conceptual definitions that form the building blocks for more advanced topics.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Token Bucket Algorithm** | A rate limiting algorithm that maintains a bucket with a fixed capacity of tokens, refilled at a constant rate, allowing controlled bursts up to the bucket capacity | Core algorithm used throughout all milestones |\n| **Leaky Bucket Algorithm** | A rate limiting algorithm that processes requests at a perfectly smooth, constant rate with queuing for excess requests | Alternative algorithm discussed in future extensions |\n| **Sliding Window Algorithm** | A rate limiting algorithm that maintains a continuous, time-based view of request history with precise rate enforcement over any time period | Alternative algorithm with exact rate guarantees |\n| **Fixed Window Algorithm** | A rate limiting algorithm that divides time into discrete intervals and counts requests per interval, resetting at interval boundaries | Simple but imprecise algorithm with burst issues |\n| **Rate Limiting** | The practice of controlling the frequency of requests or operations to protect system resources and ensure fair usage | Overall system protection strategy |\n| **Burst Handling** | Allowing short periods of high request rates up to the token bucket capacity, accommodating natural traffic spikes | Key advantage of token bucket over fixed rate limiting |\n| **Token Consumption** | The process of deducting a specified number of tokens from a bucket when processing a request | Core operation in token bucket algorithm |\n| **Token Generation** | The process of adding tokens to a bucket at a configured rate based on elapsed time | Automatic refill mechanism in token bucket |\n| **Token Refill Rate** | The number of tokens added per second to maintain the configured request rate | Configuration parameter controlling sustained rate |\n| **Bucket Capacity** | The maximum number of tokens that can be stored in a bucket, determining maximum burst size | Configuration parameter controlling burst allowance |\n| **Bucket Overflow** | The condition when generated tokens would exceed bucket capacity, resulting in discarded tokens | Natural behavior preventing unbounded token accumulation |\n\n### System Architecture Terminology\n\nRate limiting systems involve multiple components working together, each with specific responsibilities and interaction patterns.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Per-Client Rate Limiting** | Independent rate limits maintained for each API consumer, preventing one client from affecting others | Milestone 2 core concept |\n| **Client Identification Strategy** | The method used to extract and normalize unique identifiers for API consumers from request data | Critical for per-client tracking |\n| **Client Bucket Tracker** | Component responsible for managing separate token buckets for each client with efficient storage and cleanup | Milestone 2 main component |\n| **Stale Bucket Cleanup** | Background process that removes inactive client buckets to prevent memory leaks | Essential memory management process |\n| **Bucket Lifecycle Management** | The complete process of creating, accessing, aging, and cleaning up client-specific token buckets | Comprehensive bucket management strategy |\n| **HTTP Middleware Integration** | The pattern of intercepting HTTP requests in the web framework pipeline to apply rate limiting | Milestone 3 integration approach |\n| **Middleware Design Pattern** | Interceptor pattern where middleware wraps the request processing pipeline to add cross-cutting concerns | Standard web framework architecture |\n| **Per-Endpoint Rate Limiting** | Different rate limits configured for different API endpoints or routes based on resource consumption | Advanced configuration capability |\n| **Rate Limit Composition** | Applying multiple independent rate limits simultaneously, such as per-client and per-endpoint limits | Complex rate limiting scenarios |\n| **Framework-Agnostic Core** | Rate limiting logic separated from web framework-specific code for portability and testability | Clean architecture principle |\n\n### Distributed Systems Terminology\n\nScaling rate limiting across multiple servers introduces distributed system challenges and specialized terminology.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Distributed Consistency** | Maintaining consistent rate limits across multiple server instances sharing the same logical rate limit | Core challenge in Milestone 4 |\n| **Single Source of Truth** | Centralized authoritative storage (Redis) for token bucket state across all servers in the cluster | Distributed architecture principle |\n| **Atomic Operations** | Read-modify-write operations that complete without interruption, preventing race conditions in concurrent access | Essential for distributed token consumption |\n| **Circuit Breaker Pattern** | Failure handling pattern that detects issues and switches to fallback mode to prevent cascade failures | Distributed resilience mechanism |\n| **Local Fallback Buckets** | In-memory token buckets used during Redis outages with conservative limits to maintain protection | Graceful degradation strategy |\n| **Graceful Degradation** | Reducing functionality rather than complete failure while maintaining core protection during partial system failures | Resilience design principle |\n| **Progressive Degradation Levels** | Graduated failure responses maintaining different levels of functionality under various failure conditions | Sophisticated failure handling |\n| **Clock Synchronization** | Ensuring accurate time measurements across distributed servers for consistent token refill calculations | Distributed timing challenge |\n| **Clock Drift Correction** | Gradual adjustment of timing calculations to compensate for server time differences | Clock synchronization solution |\n| **Thundering Herd** | Problem where multiple servers simultaneously attempt recovery operations, overwhelming the target system | Distributed coordination anti-pattern |\n| **LRU Eviction** | Least Recently Used eviction strategy for managing memory-limited local buckets during fallback scenarios | Memory management strategy |\n\n### Configuration and Management Terminology\n\nRate limiting systems require sophisticated configuration and management capabilities to handle diverse operational requirements.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Hierarchical Resolution Strategy** | Prioritizing specific configurations over general ones when multiple rate limit rules apply to a request | Configuration precedence system |\n| **Client Override Configuration** | Specific rate limits configured for individual clients, typically for premium tiers or special agreements | Per-client customization capability |\n| **Endpoint-Specific Limits** | Rate limits configured for specific API endpoints based on resource consumption or business requirements | Resource-aware rate limiting |\n| **Dynamic Configuration Updates** | Reloading rate limit rules without application restart to adapt to changing operational needs | Runtime configuration management |\n| **Rate Limit Headers** | Standard HTTP headers (`X-RateLimit-Limit`, `X-RateLimit-Remaining`, `Retry-After`) providing rate limit information to clients | Client integration support |\n| **Adaptive Backoff Strategies** | Client logic that adjusts request rates based on rate limit headers to optimize throughput and reduce rejections | Client-side rate limiting cooperation |\n| **Emergency Memory Management** | Aggressive cleanup procedures during memory pressure to maintain functionality under resource constraints | Resource protection mechanism |\n| **Client Classification System** | Automatic categorization of API consumers based on behavioral patterns for appropriate rate limit assignment | Intelligent client management |\n\n### Error Handling and Recovery Terminology\n\nRobust rate limiting requires comprehensive error handling and recovery mechanisms for various failure scenarios.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Race Conditions** | Timing-dependent bugs where concurrent operations interfere, leading to inconsistent state or incorrect results | Concurrency bug category |\n| **Token Calculation Errors** | Arithmetic precision and overflow issues in token math, especially with large time gaps or high rates | Numerical computation problems |\n| **Integer Overflow** | Arithmetic overflow in token calculations when dealing with large time gaps or accumulated values | Specific calculation error type |\n| **Floating Point Precision Errors** | Accumulating precision loss in token calculations over time, leading to rate drift | Numerical precision challenge |\n| **Client ID Normalization** | Consistent formatting of client identifiers across different code paths to prevent duplicate bucket creation | Data consistency requirement |\n| **Non-Atomic Operations** | Redis operations that aren't executed as single atomic units, creating race condition opportunities | Distributed consistency problem |\n| **Memory Leaks** | Unbounded memory growth from client buckets that are never cleaned up, eventually exhausting system resources | Resource management failure |\n| **Circuit Breaker States** | The three states (CLOSED, OPEN, HALF_OPEN) that determine whether operations are allowed through the circuit breaker | Circuit breaker state machine |\n| **Health Check Strategies** | Systematic approaches to monitoring component health and detecting degraded performance or failures | System monitoring methodology |\n| **Recovery Coordination** | Preventing multiple components from simultaneously attempting recovery operations that could interfere | Distributed recovery management |\n\n### Testing and Quality Assurance Terminology\n\nComprehensive testing ensures rate limiting accuracy and reliability across various scenarios and load conditions.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Unit Test Coverage** | Testing individual components like token buckets, client tracking, and middleware in isolation from dependencies | Component-level testing strategy |\n| **Integration and End-to-End Testing** | Testing the complete request flow and distributed coordination scenarios with real dependencies | System-level testing approach |\n| **Milestone Verification Checkpoints** | After each milestone, specific behavior to verify and commands to run for validation of implementation progress | Development milestone validation |\n| **Performance and Load Testing** | Testing rate limiting accuracy under high concurrency and distributed load to validate system behavior at scale | Scalability and performance validation |\n| **Load Test Result Validation** | Verifying that rate limiting was accurate within acceptable tolerance during high-load scenarios | Performance test analysis |\n| **Mock Time Provider** | Controllable time provider for deterministic testing without waiting for real time passage | Testing infrastructure component |\n| **Redis Test Manager** | Component that manages Redis instances for testing, including setup, cleanup, and isolation | Testing infrastructure for distributed scenarios |\n| **Fake Redis Implementation** | In-memory Redis implementation for fast unit tests without external dependencies | Testing performance optimization |\n| **Rate Limiting Accuracy Validation** | Systematic verification that actual request rates match configured limits within acceptable error margins | Quality assurance methodology |\n| **Concurrent Client Simulation** | Testing technique that simulates multiple clients making simultaneous requests to validate thread safety | Concurrency testing approach |\n\n### Monitoring and Observability Terminology\n\nProduction rate limiting systems require comprehensive monitoring and debugging capabilities for operational excellence.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Rate Limit Decision Tracking** | Recording rate limiting decisions asynchronously for analysis and monitoring without impacting request performance | Operational monitoring capability |\n| **Client Behavior Analysis** | Statistical examination of client request patterns for classification and optimization opportunities | Intelligence gathering for optimization |\n| **Performance Tracking** | Measuring operation timing and detecting anomalies in rate limiting performance characteristics | System performance monitoring |\n| **Debugging Support** | Comprehensive logging and state inspection capabilities for troubleshooting rate limiting issues | Operational troubleshooting tools |\n| **Health Monitoring** | Systematic monitoring of all rate limiting components with automatic failure detection and recovery coordination | System health management |\n| **Metric Data Points** | Structured data representing rate limiting metrics with timestamps, labels, and metadata for analysis | Monitoring data structure |\n| **Real-Time Dashboard** | WebSocket-based live monitoring interface providing immediate visibility into rate limiting metrics and system health | Operational visibility tool |\n| **Component Health Status** | Enumerated health states (HEALTHY, DEGRADED, UNHEALTHY, UNKNOWN) for systematic health tracking | Health monitoring classification |\n| **Timing Context Management** | Context manager pattern for measuring operation timing with automatic performance tracking | Performance measurement infrastructure |\n| **Analytics Query System** | Flexible query interface for analyzing historical rate limiting data with various filters and aggregations | Data analysis capability |\n\n### Advanced Features and Extensions Terminology\n\nFuture enhancements and production-grade features extend basic rate limiting with sophisticated capabilities.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Dynamic Rate Adjustment** | Automatic modification of rate limits based on system conditions and client behavior patterns | Advanced adaptive capability |\n| **Intelligent Client Classification** | Automatic categorization of API consumers based on behavioral patterns for appropriate treatment | Machine learning enhanced classification |\n| **Geographic Rate Limiting** | Location-based rate limit variations for regional compliance and security requirements | Compliance and security feature |\n| **Behavioral Analysis** | Statistical examination of client request patterns for classification and optimization insights | Intelligence and optimization capability |\n| **Traffic Shaping** | Modification of request timing patterns for downstream system protection and load management | Advanced traffic management |\n| **Algorithm Selection Framework** | Configurable system for choosing appropriate rate limiting algorithms per use case or client type | Multi-algorithm support architecture |\n| **Extension Architecture** | Pluggable system design enabling modular enhancement of core rate limiting functionality | Extensibility framework |\n| **Hot Reload Configuration** | Runtime configuration updates without service restart for operational flexibility | Advanced configuration management |\n| **Client Profiling System** | Comprehensive tracking of client characteristics including geographic regions, endpoints used, and behavior patterns | Advanced client intelligence |\n| **Adjustment Context Analysis** | Evaluation of current system conditions including CPU, memory, latency, and error rates for rate adjustment decisions | Adaptive system optimization |\n\n### Redis and Storage Terminology\n\nDistributed rate limiting relies heavily on Redis for shared state management with specific operational patterns and challenges.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Redis Lua Scripts** | Server-side scripts that execute atomically on Redis, ensuring consistent read-modify-write operations for token consumption | Distributed atomicity mechanism |\n| **Connection Pool Management** | Efficient management of Redis connections with proper sizing, timeout handling, and connection recycling | Redis performance optimization |\n| **Redis Key Design** | Strategic naming and organization of Redis keys for efficient storage, retrieval, and cleanup operations | Data organization strategy |\n| **Batch Cleanup Operations** | Processing multiple cleanup operations together to improve efficiency and reduce Redis load | Performance optimization technique |\n| **Redis Connection Manager** | Component managing Redis connections with failover, circuit breaker logic, and health monitoring | Redis infrastructure management |\n| **Atomic Token Consumption** | Single Redis operation that checks available tokens, deducts requested tokens, and updates bucket state without race conditions | Core distributed operation |\n| **Redis Failure Recovery** | Systematic approach to handling Redis outages with local fallback and automatic recovery when service returns | Distributed resilience strategy |\n| **Connection Timeout Handling** | Proper management of Redis operation timeouts with appropriate fallback behavior and retry logic | Network reliability handling |\n| **Redis Health Monitoring** | Continuous monitoring of Redis performance and availability with automatic circuit breaker activation | Distributed system health management |\n| **Key Expiration Strategy** | Using Redis TTL mechanisms for automatic cleanup of stale rate limiting data | Automated data lifecycle management |\n\n### HTTP and Web Framework Terminology\n\nIntegration with web frameworks requires understanding of HTTP standards and middleware patterns.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **HTTP 429 Too Many Requests** | Standard HTTP status code indicating that the client has exceeded the configured rate limit | Rate limiting response standard |\n| **Retry-After Header** | HTTP header indicating the number of seconds until the next request will be allowed | Client guidance for retry timing |\n| **Rate Limit Response Headers** | Standard headers (`X-RateLimit-Limit`, `X-RateLimit-Remaining`) providing current rate limit status to clients | Client integration support |\n| **Client Identification Headers** | HTTP headers used to identify clients, such as API keys, authentication tokens, or custom identifiers | Client tracking mechanism |\n| **Request Context Extraction** | Process of extracting relevant information from HTTP requests for rate limiting decisions | Request processing step |\n| **Endpoint Path Normalization** | Consistent formatting of API endpoint paths for rate limiting grouping and configuration | Path processing standardization |\n| **Skip Rate Limiting Headers** | Special HTTP headers that allow bypassing rate limiting for internal or administrative requests | Operational bypass mechanism |\n| **Framework Middleware Integration** | Proper integration with web framework request processing pipelines (Flask, Express, etc.) | Web framework compatibility |\n| **Response Enhancement** | Adding rate limiting headers and information to successful HTTP responses for client awareness | Client communication enhancement |\n| **Error Response Formatting** | Standardized JSON error responses for rate limit exceeded conditions with appropriate detail levels | API error handling standard |\n\n### Time and Precision Terminology\n\nAccurate time handling is critical for rate limiting algorithms, especially in distributed environments.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Time Precision Handling** | Managing floating-point time calculations with appropriate precision to prevent accumulation errors | Numerical accuracy requirement |\n| **Elapsed Time Calculation** | Computing time differences for token refill operations with proper handling of clock changes | Core timing operation |\n| **Token Refill Timing** | Calculating the exact number of tokens to add based on elapsed time and configured refill rate | Algorithm timing calculation |\n| **Clock Change Detection** | Identifying when system clocks are adjusted forward or backward and handling appropriately | System time stability |\n| **Time-Based Token Generation** | Algorithm for adding tokens to buckets based on elapsed time since last refill operation | Core token bucket mechanism |\n| **Timestamp Normalization** | Converting various time representations to consistent internal format for reliable calculations | Time data consistency |\n| **Sub-Second Precision** | Handling time calculations with millisecond or microsecond precision for accurate rate limiting | High-precision timing requirements |\n| **Time Provider Abstraction** | Interface allowing controllable time sources for testing and potential timezone handling | Testing and flexibility infrastructure |\n| **Rate Calculation Accuracy** | Ensuring that actual rates match configured rates within acceptable tolerance over various time periods | Algorithm correctness validation |\n| **Time Window Management** | Managing time-based windows for sliding window algorithms and request history tracking | Time-based algorithm support |\n"}