{"html":"<h1 id=\"simd-library\">simd-library</h1>\n<h1 id=\"tdd\">TDD</h1>\n<p>To provide a zero-cost abstraction layer for SIMD operations across x86 (SSE/AVX/AVX-512) and ARM (NEON) architectures. The library focuses on deterministic performance, compile-time dispatch optimization, and strict memory alignment safety to prevent segmentation faults and bus errors during high-throughput data processing.</p>\n<!-- TDD_MOD_ID: mod-hardware-dispatch -->\n<h1 id=\"module-hardware-detection-amp-dispatch-engine-hdde\">Module: Hardware Detection &amp; Dispatch Engine (HDDE)</h1>\n<h2 id=\"1-technical-specification\">1. Technical Specification</h2>\n<p>The Hardware Detection &amp; Dispatch Engine (HDDE) serves as the foundational decision-maker for the <code>simd-library</code>. Its primary responsibility is the identification of CPU Instruction Set Architecture (ISA) extensions (e.g., AVX-512, NEON) and the subsequent orchestration of the &quot;Best-Fit&quot; execution path.</p>\n<p>The engine must resolve two conflicting requirements:</p>\n<ol>\n<li><strong>Runtime Versatility</strong>: The ability to run a single binary on multiple CPU generations.</li>\n<li><strong>Zero-Overhead Execution</strong>: Ensuring that the dispatch mechanism does not introduce branch mispredictions or pipeline stalls in the &quot;Hot Path.&quot;</li>\n</ol>\n<h2 id=\"2-abstraction-layers\">2. Abstraction Layers</h2>\n<p>The HDDE is structured into three discrete layers:</p>\n<ul>\n<li><strong>Layer 0: The Prober</strong>: Architecture-specific assembly (<code>cpuid</code> for x86, <code>mrs</code> or <code>/proc/self/auxv</code> for ARM) that extracts raw feature bits.</li>\n<li><strong>Layer 1: The Registry</strong>: A thread-safe, immutable bit-field representing the global hardware state.</li>\n<li><strong>Layer 2: The Dispatcher</strong>: A functional bridge that uses either Function Multiversioning (FMV) or static function pointers to route data to the optimal SIMD implementation.</li>\n</ul>\n<h2 id=\"3-struct-amp-interface-definitions\">3. Struct &amp; Interface Definitions</h2>\n<h3 id=\"31-hardware-state-representation\">3.1. Hardware State Representation</h3>\n<p>To ensure cache-line efficiency, the hardware state is encapsulated in a bit-packed structure, aligned to avoid false sharing.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Alignment to 64 bytes to prevent cache-line contention in multi-threaded environments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">struct</span><span style=\"color:#79B8FF\"> alignas(64)</span><span style=\"color:#B392F0\"> CPUFeatureSet</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uint64_t</span><span style=\"color:#E1E4E8\"> x86_extensions;</span><span style=\"color:#6A737D\"> // Bits for SSE, AVX, AVX2, AVX512(F,CD,BW,DQ,VL)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uint64_t</span><span style=\"color:#E1E4E8\"> arm_extensions;</span><span style=\"color:#6A737D\"> // Bits for NEON, ASIMD, SVE, SVE2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uint32_t</span><span style=\"color:#E1E4E8\"> cache_line_size;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uint32_t</span><span style=\"color:#E1E4E8\"> logical_cores;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Memory Alignment: </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // [8 bytes: x86] [8 bytes: arm] [4 bytes: cache] [4 bytes: cores] </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // [40 bytes: padding/reserved]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span></code></pre></div>\n\n<h3 id=\"32-dispatcher-interface\">3.2. Dispatcher Interface</h3>\n<p>The dispatcher utilizes a function pointer table or a static global pointer resolved at load-time (via <code>__attribute__((constructor))</code>).</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">typedef</span><span style=\"color:#F97583\"> void</span><span style=\"color:#E1E4E8\"> (*SimdKernel)(</span><span style=\"color:#F97583\">const</span><span style=\"color:#F97583\"> float*</span><span style=\"color:#B392F0\"> __restrict</span><span style=\"color:#FFAB70\"> src</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">float*</span><span style=\"color:#B392F0\"> __restrict</span><span style=\"color:#FFAB70\"> dst</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#FFAB70\"> len</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">struct</span><span style=\"color:#B392F0\"> KernelRegistry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SimdKernel current_best_kernel;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uint32_t</span><span style=\"color:#E1E4E8\"> isa_level;</span><span style=\"color:#6A737D\"> // Numeric rank for comparison</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span></code></pre></div>\n\n<p>{{DIAGRAM:hdde-class-hierarchy}}</p>\n<p><img src=\"/api/project/simd-library/architecture-doc/asset?path=diagrams%2Ftdd-hdde-001.svg\" alt=\"Component Interaction\"></p>\n<h2 id=\"4-algorithm-pseudo-code\">4. Algorithm Pseudo-code</h2>\n<h3 id=\"41-the-hot-path-static-lazy-dispatch\">4.1. The Hot Path: Static-Lazy Dispatch</h3>\n<p>This logic occurs at the first invocation of a SIMD-accelerated function. We use the &quot;Double-Checked Locking&quot; pattern or atomic <code>std::call_once</code>.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Global atomic pointer to the active kernel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">static</span><span style=\"color:#B392F0\"> std</span><span style=\"color:#E1E4E8\">::atomic</span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\">SimdKernel</span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> global_dispatch_ptr{</span><span style=\"color:#79B8FF\">nullptr</span><span style=\"color:#E1E4E8\">};</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">void</span><span style=\"color:#B392F0\"> dispatch_init</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CPUFeatureSet features </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> probe_hardware</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> (features.x86_extensions </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> FEATURE_AVX512) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        global_dispatch_ptr.</span><span style=\"color:#B392F0\">store</span><span style=\"color:#E1E4E8\">(kernel_avx512_impl, </span><span style=\"color:#B392F0\">std</span><span style=\"color:#E1E4E8\">::memory_order_release);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> (features.x86_extensions </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> FEATURE_AVX2) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        global_dispatch_ptr.</span><span style=\"color:#B392F0\">store</span><span style=\"color:#E1E4E8\">(kernel_avx2_impl, </span><span style=\"color:#B392F0\">std</span><span style=\"color:#E1E4E8\">::memory_order_release);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        global_dispatch_ptr.</span><span style=\"color:#B392F0\">store</span><span style=\"color:#E1E4E8\">(kernel_scalar_fallback, </span><span style=\"color:#B392F0\">std</span><span style=\"color:#E1E4E8\">::memory_order_release);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Hot Path Wrapper</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">void</span><span style=\"color:#B392F0\"> execute_simd_op</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">const</span><span style=\"color:#F97583\"> float*</span><span style=\"color:#FFAB70\"> src</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">float*</span><span style=\"color:#FFAB70\"> dst</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#FFAB70\"> len</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Micro-optimization: Use local pointer to avoid multiple atomic loads</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SimdKernel func </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> global_dispatch_ptr.</span><span style=\"color:#B392F0\">load</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">std</span><span style=\"color:#E1E4E8\">::memory_order_acquire);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#B392F0\">__builtin_expect</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">func, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)) {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        dispatch_init</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        func </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> global_dispatch_ptr.</span><span style=\"color:#B392F0\">load</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">std</span><span style=\"color:#E1E4E8\">::memory_order_relaxed);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Tail-call optimization target</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    func</span><span style=\"color:#E1E4E8\">(src, dst, len);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h2 id=\"5-engineering-constraints-amp-hazards\">5. Engineering Constraints &amp; Hazards</h2>\n<h3 id=\"51-concurrency-amp-initialization\">5.1. Concurrency &amp; Initialization</h3>\n<ul>\n<li><strong>Race Conditions</strong>: Hardware probing must be idempotent and thread-safe. Static constructors are preferred for deterministic initialization before <code>main()</code> is entered.</li>\n<li><strong>Memory Ordering</strong>: The <code>global_dispatch_ptr</code> must use <code>memory_order_acquire/release</code> to ensure that once the function pointer is visible, the underlying ISA-specific machine code is also fully loaded and executable.</li>\n</ul>\n<h3 id=\"52-memory-amp-alignment\">5.2. Memory &amp; Alignment</h3>\n<ul>\n<li><strong>Data Alignment</strong>: All SIMD kernels assume <code>alignas(32)</code> or <code>alignas(64)</code> for input buffers. The HDDE must provide a <code>malloc</code> wrapper or an alignment validator.</li>\n<li><strong>Fault Handling</strong>: If a kernel is dispatched to an ISA not supported by the hardware (e.g., AVX-512 on an AVX2 chip), the resulting <code>SIGILL</code> must be prevented via strict runtime masking in <code>probe_hardware()</code>.</li>\n</ul>\n<h3 id=\"53-micro-optimization-corner\">5.3. Micro-Optimization Corner</h3>\n<ol>\n<li><strong>Branch Target Buffer (BTB) Warming</strong>: For critical loops, the HDDE should be initialized during the application&#39;s &quot;cold start&quot; phase to ensure the BTB is primed with the correct kernel address, reducing the cost of the indirect function call.</li>\n<li><strong>IFUNC (Indirect Functions)</strong>: On GNU/Linux systems, the HDDE will leverage <code>__attribute__((ifunc))</code> to allow the dynamic linker (ld.so) to resolve the optimal function at load-time. This replaces the <code>if(!func)</code> check with a direct PLT (Procedure Linkage Table) entry, reducing the dispatch overhead to exactly zero at runtime.</li>\n<li><strong>No-Inline Policy</strong>: SIMD kernels themselves should be marked <code>__attribute__((noinline))</code> to prevent the compiler from bloating the dispatcher with multiple ISA versions, which would trash the Instruction Cache (I-Cache).</li>\n</ol>\n<p>{{DIAGRAM:hdde-sequence-ifunc}}\n{{DIAGRAM:tdd-hdde-002}}</p>\n<!-- TDD_MOD_ID: mod-memory-management -->\n<h1 id=\"module-aligned-memory-amp-buffer-manager\">Module: Aligned Memory &amp; Buffer Manager</h1>\n<h2 id=\"1-technical-specification\">1. Technical Specification</h2>\n<p>The Aligned Memory &amp; Buffer Manager (AMBM) provides a deterministic, high-performance memory allocation subsystem specifically designed for SIMD workloads. Standard heap allocators (e.g., <code>malloc</code>) typically guarantee 8 or 16-byte alignment; however, modern SIMD instructions (AVX-256, AVX-512) require 32 or 64-byte alignment to avoid <code>GPF</code> (General Protection Faults) or performance-degrading split-load penalties.</p>\n<p>The AMBM&#39;s primary responsibilities include:</p>\n<ol>\n<li><strong>Strict Alignment Enforcement</strong>: Guarantees alignment on $2^n$ boundaries (16, 32, 64 bytes).</li>\n<li><strong>Metadata Management</strong>: Efficiently tracking &quot;original&quot; pointers vs. &quot;aligned&quot; pointers for safe deallocation.</li>\n<li><strong>Buffer Lifecycle Control</strong>: Providing RAII-compliant wrappers to prevent memory leaks in complex SIMD pipelines.</li>\n</ol>\n<h2 id=\"2-abstraction-layers\">2. Abstraction Layers</h2>\n<p>The AMBM is divided into three functional strata:</p>\n<ul>\n<li><strong>Layer 0: OS Abstraction (Low Level)</strong>: Wraps platform-specific calls like <code>posix_memalign</code> (POSIX), <code>_aligned_malloc</code> (MSVC), or <code>mmap</code> with <code>MAP_HUGETLB</code>.</li>\n<li><strong>Layer 1: Aligned Pointer Arithmetic (Manual Alignment)</strong>: Logic to transform an arbitrary pointer into an aligned one by over-allocating and storing metadata.</li>\n<li><strong>Layer 2: Buffer Templates (High Level)</strong>: C++ templates (<code>AlignedVector&lt;T&gt;</code>) that interface with the Hardware Detection Engine to determine optimal alignment at compile/link time.</li>\n</ul>\n<h2 id=\"3-struct-amp-interface-definitions\">3. Struct &amp; Interface Definitions</h2>\n<h3 id=\"31-aligned-block-metadata\">3.1. Aligned Block Metadata</h3>\n<p>To support manual alignment on platforms without intrinsic aligned allocators, each block must store the offset to the original heap-allocated address.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Packed to 16 bytes to ensure the header itself doesn't cause </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// massive fragmentation.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">struct</span><span style=\"color:#B392F0\"> AlignedBlockHeader</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    void*</span><span style=\"color:#E1E4E8\"> original_ptr;</span><span style=\"color:#6A737D\">      // 8 bytes: Address returned by malloc()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    size_t</span><span style=\"color:#E1E4E8\"> requested_size;</span><span style=\"color:#6A737D\">   // 8 bytes: Size in bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uint32_t</span><span style=\"color:#E1E4E8\"> alignment_offset;</span><span style=\"color:#6A737D\"> // 4 bytes: Distance from original to aligned</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uint32_t</span><span style=\"color:#E1E4E8\"> magic_canary;</span><span style=\"color:#6A737D\">   // 4 bytes: Debug-only validity check (0xDEADBEEF)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span></code></pre></div>\n\n<h3 id=\"32-primary-manager-interface\">3.2. Primary Manager Interface</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AlignedBufferManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@brief</span><span style=\"color:#6A737D\"> Allocates memory aligned to the specified boundary.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> size</span><span style=\"color:#6A737D\"> Total bytes to allocate.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> alignment</span><span style=\"color:#6A737D\"> Must be a power of 2 (16, 32, 64).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#F97583\"> void*</span><span style=\"color:#B392F0\"> Allocate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">size_t</span><span style=\"color:#FFAB70\"> size</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#FFAB70\"> alignment</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@brief</span><span style=\"color:#6A737D\"> Frees memory allocated via Allocate.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> Free</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">void*</span><span style=\"color:#FFAB70\"> aligned_ptr</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@brief</span><span style=\"color:#6A737D\"> Validates if a pointer is aligned to a specific boundary.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#B392F0\"> IsAligned</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">const</span><span style=\"color:#F97583\"> void*</span><span style=\"color:#FFAB70\"> ptr</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#FFAB70\"> alignment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">noexcept</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">reinterpret_cast&#x3C;uintptr_t></span><span style=\"color:#E1E4E8\">(ptr) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> (alignment </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span></code></pre></div>\n\n<p>{{DIAGRAM:ambm-allocation-flow}}\n{{DIAGRAM:tdd-ambm-001}}</p>\n<h2 id=\"4-algorithm-pseudo-code\">4. Algorithm Pseudo-code</h2>\n<h3 id=\"41-the-hot-path-manual-alignment-logic\">4.1. The Hot Path: Manual Alignment Logic</h3>\n<p>When <code>_aligned_malloc</code> is unavailable, we use the &quot;Over-allocation&quot; strategy.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">void*</span><span style=\"color:#B392F0\"> AlignedBufferManager</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">Allocate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">size_t</span><span style=\"color:#FFAB70\"> size</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#FFAB70\"> alignment</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 1. Calculate total size including potential padding and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // alignment - 1 (max padding) + sizeof(void*) (to store original ptr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    size_t</span><span style=\"color:#E1E4E8\"> total_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> size </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> alignment </span><span style=\"color:#F97583\">+</span><span style=\"color:#F97583\"> sizeof</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">void*</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 2. Standard heap allocation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    void*</span><span style=\"color:#E1E4E8\"> raw_ptr </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> std</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">malloc</span><span style=\"color:#E1E4E8\">(total_size);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">raw_ptr) </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> nullptr</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 3. Pointer Arithmetic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Move forward by size of a pointer, then align up</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uintptr_t</span><span style=\"color:#E1E4E8\"> raw_addr </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> reinterpret_cast&#x3C;uintptr_t></span><span style=\"color:#E1E4E8\">(raw_ptr);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uintptr_t</span><span style=\"color:#E1E4E8\"> start_addr </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> raw_addr </span><span style=\"color:#F97583\">+</span><span style=\"color:#F97583\"> sizeof</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">void*</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uintptr_t</span><span style=\"color:#E1E4E8\"> aligned_addr </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (start_addr </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> alignment </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#F97583\"> ~</span><span style=\"color:#E1E4E8\">(alignment </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 4. Metadata Storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Store original_ptr right before the aligned_addr</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    void**</span><span style=\"color:#E1E4E8\"> metadata_loc </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> reinterpret_cast&#x3C;void**></span><span style=\"color:#E1E4E8\">(aligned_addr </span><span style=\"color:#F97583\">-</span><span style=\"color:#F97583\"> sizeof</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">void*</span><span style=\"color:#E1E4E8\">));</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    *</span><span style=\"color:#E1E4E8\">metadata_loc </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> raw_ptr;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> reinterpret_cast&#x3C;void*></span><span style=\"color:#E1E4E8\">(aligned_addr);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">void</span><span style=\"color:#B392F0\"> AlignedBufferManager</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">Free</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">void*</span><span style=\"color:#FFAB70\"> aligned_ptr</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">aligned_ptr) </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Retrieve original pointer from the slot immediately preceding the aligned address</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    void**</span><span style=\"color:#E1E4E8\"> metadata_loc </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> reinterpret_cast&#x3C;void**></span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">reinterpret_cast&#x3C;uintptr_t></span><span style=\"color:#E1E4E8\">(aligned_ptr) </span><span style=\"color:#F97583\">-</span><span style=\"color:#F97583\"> sizeof</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">void*</span><span style=\"color:#E1E4E8\">));</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    void*</span><span style=\"color:#E1E4E8\"> original_ptr </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">metadata_loc;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    std</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">free</span><span style=\"color:#E1E4E8\">(original_ptr);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h2 id=\"5-engineering-constraints-amp-hazards\">5. Engineering Constraints &amp; Hazards</h2>\n<h3 id=\"51-performance-amp-cache-locality\">5.1. Performance &amp; Cache Locality</h3>\n<ul>\n<li><strong>False Sharing</strong>: If multiple <code>AlignedBlockHeaders</code> are allocated on the same cache line as frequently written data in a multi-threaded context, performance will degrade.</li>\n<li><strong>TLB Misses</strong>: For very large buffers (e.g., &gt; 2MB), the manager should optionally use Huge Pages (<code>mmap</code> with <code>MAP_HUGETLB</code>) to reduce Translation Lookaside Buffer (TLB) pressure.</li>\n</ul>\n<h3 id=\"52-safety-hazards\">5.2. Safety Hazards</h3>\n<ul>\n<li><strong>Pointer Mismatch</strong>: Passing a pointer allocated with <code>std::malloc</code> to <code>AlignedBufferManager::Free</code> will result in a segmentation fault or heap corruption, as it will attempt to read metadata from invalid memory.</li>\n<li><strong>Alignment Granularity</strong>: Requesting an alignment that is not a power of 2 will result in undefined behavior in the bitwise logic (<code>&amp; ~(alignment - 1)</code>).</li>\n</ul>\n<h3 id=\"53-micro-optimization-corner\">5.3. Micro-Optimization Corner</h3>\n<ol>\n<li><strong>Prefetching Hint</strong>: The <code>Allocate</code> function should ideally call <code>_mm_prefetch</code> (or equivalent) on the first few cache lines of the returned buffer to warm the L1 cache before the SIMD kernel begins execution.</li>\n<li><strong>Size Rounding</strong>: To prevent &quot;tail-end&quot; complexity in SIMD loops (handling the remaining <code>N % vector_width</code> elements), the manager should round up the <code>requested_size</code> to the nearest multiple of the vector width (e.g., 64 bytes for AVX-512). This allows for &quot;over-reading&quot; into safe padding without triggering page faults.</li>\n<li><strong>Zero-Initialization (Lazy)</strong>: Use <code>mmap</code> with <code>MAP_ANONYMOUS</code> which utilizes the OS &quot;zero-page&quot; mechanism. This avoids the cost of <code>memset(0)</code> until the memory is actually touched.</li>\n</ol>\n<p>{{DIAGRAM:ambm-memory-hierarchy}}\n{{DIAGRAM:tdd-ambm-002}}</p>\n<!-- TDD_MOD_ID: mod-vector-abstraction -->\n<!-- TDD_MOD_ID: mod-unified-vector-interface -->\n<h1 id=\"module-unified-vector-interface-uvi\">Module: Unified Vector Interface (UVI)</h1>\n<h2 id=\"1-technical-specification\">1. Technical Specification</h2>\n<p>The Unified Vector Interface (UVI) provides the high-level, type-safe API for the <code>simd-library</code>. While the <strong>HDDE</strong> handles dispatch and the <strong>AMBM</strong> handles memory, the UVI defines the semantic operations performed on data. It abstracts architecture-specific SIMD registers (e.g., <code>ymm</code> on x86, <code>v</code> on ARM) into a uniform <code>Vector&lt;T, N&gt;</code> template.</p>\n<p>The UVI&#39;s primary requirements are:</p>\n<ol>\n<li><strong>Semantic Uniformity</strong>: A single source-code expression (e.g., <code>a + b</code>) must compile to the optimal ISA-specific intrinsic (e.g., <code>_mm256_add_ps</code> or <code>vaddq_f32</code>).</li>\n<li><strong>Type Safety</strong>: Prevent illegal operations (e.g., adding a 256-bit float vector to a 128-bit integer vector) at compile time.</li>\n<li><strong>Zero-Overhead Wrappers</strong>: Ensure that the abstraction layer collapses into raw assembly with no temporary object overhead or unnecessary register spills.</li>\n</ol>\n<h2 id=\"2-abstraction-layers\">2. Abstraction Layers</h2>\n<p>The UVI is architected in a &quot;Trait-to-Implementation&quot; hierarchy:</p>\n<ul>\n<li><strong>Layer 0: Semantic Traits</strong>: A set of template specializations that map <code>&lt;Type, Width&gt;</code> pairs to underlying intrinsic types (e.g., <code>&lt;float, 8&gt;</code> -&gt; <code>__m256</code>).</li>\n<li><strong>Layer 1: Intrinsic Wrappers</strong>: Inline functions that wrap raw intrinsics in a unified naming convention (<code>simd_add</code>, <code>simd_load</code>).</li>\n<li><strong>Layer 2: The Vector Object</strong>: The public-facing <code>Vector&lt;T, N&gt;</code> class that provides operator overloads and functional programming primitives (map, reduce).</li>\n</ul>\n<h2 id=\"3-struct-amp-interface-definitions\">3. Struct &amp; Interface Definitions</h2>\n<h3 id=\"31-vector-storage-amp-alignment\">3.1. Vector Storage &amp; Alignment</h3>\n<p>The <code>Vector</code> struct is the core primitive. It must be explicitly aligned to ensure it can be mapped directly to SIMD registers.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">template</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">typename</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#B392F0\"> N</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#F97583\">struct</span><span style=\"color:#79B8FF\"> alignas(</span><span style=\"color:#B392F0\">sizeof</span><span style=\"color:#79B8FF\">(T)</span><span style=\"color:#E1E4E8\"> * N) Vector {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // The underlying hardware register type (resolved via traits)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    using</span><span style=\"color:#B392F0\"> InternalType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#F97583\"> typename</span><span style=\"color:#B392F0\"> SimdTraits</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">N</span><span style=\"color:#E1E4E8\">>::</span><span style=\"color:#B392F0\">NativeType</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    InternalType data;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Constraints: </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 1. N must be a power of 2.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 2. sizeof(T) * N must be supported by the current ISA (16, 32, 64 bytes).</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#B392F0\"> Vector</span><span style=\"color:#B392F0\"> Load</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">const</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#F97583\">*</span><span style=\"color:#FFAB70\"> ptr</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#B392F0\"> Vector</span><span style=\"color:#B392F0\"> LoadAligned</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">const</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#F97583\">*</span><span style=\"color:#FFAB70\"> ptr</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    void</span><span style=\"color:#B392F0\"> Store</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">T</span><span style=\"color:#F97583\">*</span><span style=\"color:#FFAB70\"> ptr</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span></code></pre></div>\n\n<h3 id=\"32-simdtraits-specialization-example\">3.2. SimdTraits Specialization (Example)</h3>\n<p>Mapping logic used by the compiler to select the register width.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Generic template</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">template</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">typename</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#B392F0\"> N</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#F97583\">struct</span><span style=\"color:#B392F0\"> SimdTraits</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// x86 AVX Specialization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">template</span><span style=\"color:#E1E4E8\"> &#x3C;> </span><span style=\"color:#F97583\">struct</span><span style=\"color:#B392F0\"> SimdTraits</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#F97583\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    using</span><span style=\"color:#B392F0\"> NativeType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#B392F0\"> __m256</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#F97583\"> constexpr</span><span style=\"color:#F97583\"> size_t</span><span style=\"color:#E1E4E8\"> Alignment </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#F97583\"> constexpr</span><span style=\"color:#F97583\"> const</span><span style=\"color:#F97583\"> char*</span><span style=\"color:#E1E4E8\"> ISA </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"AVX/AVX2\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ARM NEON Specialization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">template</span><span style=\"color:#E1E4E8\"> &#x3C;> </span><span style=\"color:#F97583\">struct</span><span style=\"color:#B392F0\"> SimdTraits</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#F97583\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    using</span><span style=\"color:#B392F0\"> NativeType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> float32x4_t</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#F97583\"> constexpr</span><span style=\"color:#F97583\"> size_t</span><span style=\"color:#E1E4E8\"> Alignment </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#F97583\"> constexpr</span><span style=\"color:#F97583\"> const</span><span style=\"color:#F97583\"> char*</span><span style=\"color:#E1E4E8\"> ISA </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"NEON\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span></code></pre></div>\n\n<p>{{DIAGRAM:uvi-class-abstraction}}\n{{DIAGRAM:tdd-uvi-001}}</p>\n<h2 id=\"4-algorithm-pseudo-code\">4. Algorithm Pseudo-code</h2>\n<h3 id=\"41-the-hot-path-generic-vectorized-accumulation\">4.1. The Hot Path: Generic Vectorized Accumulation</h3>\n<p>This pseudo-code demonstrates how the UVI interacts with the AMBM to process an array of data using the widest available vector.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">template</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">typename</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#F97583\">void</span><span style=\"color:#B392F0\"> vector_add_pipeline</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">const</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\"> __restrict</span><span style=\"color:#FFAB70\"> a</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">const</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\"> __restrict</span><span style=\"color:#FFAB70\"> b</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">T</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\"> __restrict</span><span style=\"color:#FFAB70\"> res</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#FFAB70\"> count</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 1. Determine optimal width from HDDE</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    constexpr</span><span style=\"color:#F97583\"> size_t</span><span style=\"color:#E1E4E8\"> W </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> HardwareCapabilities</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">GetOptimalWidth</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">>();</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    using</span><span style=\"color:#B392F0\"> VType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#B392F0\"> Vector</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">W</span><span style=\"color:#E1E4E8\">>;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    size_t</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 2. Main Loop: Process W elements per iteration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> (; i </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> count </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> W; i </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> W) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Load (Assume AMBM provided aligned pointers)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        VType va </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> VType</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">a[i]);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        VType vb </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> VType</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">b[i]);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Operation (Overloaded operator + calls underlying intrinsics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        VType vr </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> va </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> vb;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        vr.</span><span style=\"color:#B392F0\">Store</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">res[i]);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 3. Tail Handling: Scalar fallback for remaining (count % W) elements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> (; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> count; </span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\">i) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        res[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a[i] </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> b[i];</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h2 id=\"5-engineering-constraints-amp-hazards\">5. Engineering Constraints &amp; Hazards</h2>\n<h3 id=\"51-concurrency-register-pressure\">5.1. Concurrency: Register Pressure</h3>\n<ul>\n<li><strong>Hazard</strong>: Over-using complex UVI objects in a single function can lead to &quot;Register Spilling,&quot; where the compiler is forced to move SIMD register data to the stack (memory).</li>\n<li><strong>Mitigation</strong>: UVI functions must be marked <code>__attribute__((always_inline))</code> and the number of live <code>Vector</code> objects in a scope should be monitored.</li>\n</ul>\n<h3 id=\"52-memory-strict-aliasing-amp-restrict\">5.2. Memory: Strict Aliasing &amp; Restrict</h3>\n<ul>\n<li><strong>Constraint</strong>: The UVI assumes that input and output buffers do not overlap.</li>\n<li><strong>Logic</strong>: Use the <code>__restrict</code> keyword in all UVI interfaces to allow the compiler to reorder instructions without worrying about pointer aliasing, which is critical for pipelining SIMD loads/stores.</li>\n</ul>\n<h3 id=\"53-micro-optimization-corner\">5.3. Micro-Optimization Corner</h3>\n<ol>\n<li><strong>FMA (Fused Multiply-Add)</strong>: The UVI should provide a <code>Vector::MultiplyAdd(a, b, c)</code> method. On supported hardware (AVX2+), this maps to a single <code>vfmadd</code> instruction ($a \\times b + c$), which has the same latency as a single addition but double the throughput and higher numerical precision.</li>\n<li><strong>Loop Unrolling</strong>: For the Hot Path, the UVI recommends unrolling loops by a factor of 2 or 4 (e.g., processing $4 \\times 512$ bits per iteration). This hides the latency of the <code>Load</code> instructions by utilizing the CPU&#39;s multiple execution ports.</li>\n<li><strong>Non-Temporal Stores</strong>: For very large datasets that exceed the L3 cache, the UVI provides <code>StoreNT()</code>. This uses &quot;Streaming Stores&quot; (<code>movntps</code>) which bypass the cache hierarchy and write directly to RAM, preventing the &quot;Cache Pollution&quot; that occurs when processing multi-gigabyte buffers.</li>\n</ol>\n<p><img src=\"/api/project/simd-library/architecture-doc/asset?path=diagrams%2Ftdd-uvi-002.svg\" alt=\"SIMD Pipelining\"></p>\n<h3 id=\"54-masked-operations-predication\">5.4. Masked Operations (Predication)</h3>\n<p>For architectures like AVX-512 and ARM SVE, the UVI supports <strong>Masked Loads/Stores</strong>.</p>\n<ul>\n<li><strong>Logic</strong>: Instead of a scalar tail loop, a bitmask is used to enable/disable specific lanes in a single vector operation.</li>\n<li><strong>Efficiency</strong>: Reduces the &quot;Tail Penalty&quot; to near-zero by allowing the final partial block to be processed by the same SIMD hardware.</li>\n</ul>\n<p>{{DIAGRAM:uvi-masked-operation-flow}}\n{{DIAGRAM:tdd-uvi-003}}</p>\n<!-- TDD_MOD_ID: mod-kernel-compute -->\n<!-- TDD_MOD_ID: mod-compute-kernel-engine -->\n<h1 id=\"module-compute-kernel-engine-cke\">Module: Compute Kernel Engine (CKE)</h1>\n<h2 id=\"1-technical-specification\">1. Technical Specification</h2>\n<p>The Compute Kernel Engine (CKE) is the orchestration layer of the <code>simd-library</code>. While the <strong>UVI</strong> provides raw vector primitives, the CKE transforms these primitives into high-level, composite mathematical operations (e.g., GEMM, 1D/2D Convolutions, Signal Transducers). </p>\n<p>The CKE&#39;s primary responsibilities are:</p>\n<ol>\n<li><strong>Pipeline Orchestration</strong>: Managing the execution of multiple SIMD kernels in a single pass to maximize L1/L2 cache residency.</li>\n<li><strong>Loop Transformation</strong>: Automatically applying loop unrolling, software prefetching, and tail-handling strategies based on the ISA reported by the <strong>HDDE</strong>.</li>\n<li><strong>Numerical Stability</strong>: Ensuring that vectorized implementations maintain parity with IEEE-754 standards, specifically regarding Fused Multiply-Add (FMA) accumulations.</li>\n</ol>\n<h2 id=\"2-abstraction-layers\">2. Abstraction Layers</h2>\n<p>The CKE operates through three distinct functional tiers:</p>\n<ul>\n<li><strong>Layer 0: Kernel Primitives</strong>: Low-level &quot;Micro-Kernels&quot; that perform a single operation on a fixed number of registers (e.g., a $4 \\times 4$ matrix-vector block).</li>\n<li><strong>Layer 1: The Iteration Engine</strong>: Logic responsible for traversing memory buffers, handling alignment boundaries via the <strong>AMBM</strong>, and managing the &quot;Horizontal-to-Vertical&quot; data transformations.</li>\n<li><strong>Layer 2: Kernel DSL/Interface</strong>: The high-level API where users define complex pipelines (e.g., <code>Map().Filter().Reduce()</code>) which the CKE compiles into an optimized instruction stream.</li>\n</ul>\n<h2 id=\"3-struct-amp-interface-definitions\">3. Struct &amp; Interface Definitions</h2>\n<h3 id=\"31-kernel-execution-context\">3.1. Kernel Execution Context</h3>\n<p>To minimize stack frame overhead, the execution state is encapsulated in a cache-aligned context.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Aligned to 64 bytes to match AVX-512 cache-line and prevent false sharing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">struct</span><span style=\"color:#79B8FF\"> alignas(64)</span><span style=\"color:#B392F0\"> KernelContext</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    const</span><span style=\"color:#F97583\"> float*</span><span style=\"color:#E1E4E8\"> __restrict src_a;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    const</span><span style=\"color:#F97583\"> float*</span><span style=\"color:#E1E4E8\"> __restrict src_b;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    float*</span><span style=\"color:#E1E4E8\"> __restrict out;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    size_t</span><span style=\"color:#E1E4E8\"> length;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Stride information for multidimensional kernels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        size_t</span><span style=\"color:#E1E4E8\"> row_stride;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        size_t</span><span style=\"color:#E1E4E8\"> col_stride;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } metrics;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Prefetching configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    uint32_t</span><span style=\"color:#E1E4E8\"> prefetch_distance;</span><span style=\"color:#6A737D\"> // Measured in cache lines</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // [Padding to 64 bytes to ensure subsequent contexts in an array are line-aligned]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span></code></pre></div>\n\n<h3 id=\"32-micro-kernel-interface\">3.2. Micro-Kernel Interface</h3>\n<p>Kernels are defined as stateless functors to allow the compiler to inline them directly into the CKE&#39;s main loop.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">template</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">typename</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">size_t</span><span style=\"color:#B392F0\"> VectorWidth</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#F97583\">struct</span><span style=\"color:#B392F0\"> DotProductKernel</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    using</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#F97583\"> =</span><span style=\"color:#B392F0\"> Vector</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">VectorWidth</span><span style=\"color:#E1E4E8\">>;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Accumulator state held in registers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    V acc0, acc1, acc2, acc3;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    inline</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> Initialize</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        acc0 </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">Zero</span><span style=\"color:#E1E4E8\">(); acc1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">Zero</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        acc2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">Zero</span><span style=\"color:#E1E4E8\">(); acc3 </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">Zero</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Process 4 vectors per call (Unroll factor 4)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    inline</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> Step</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">const</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#F97583\">*</span><span style=\"color:#FFAB70\"> a</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">const</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#F97583\">*</span><span style=\"color:#FFAB70\"> b</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        acc0 </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">FusedMultiplyAdd</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(a </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">VectorWidth), </span><span style=\"color:#B392F0\">V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(b </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">VectorWidth), acc0);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        acc1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">FusedMultiplyAdd</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(a </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">VectorWidth), </span><span style=\"color:#B392F0\">V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(b </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">VectorWidth), acc1);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        acc2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">FusedMultiplyAdd</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(a </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">VectorWidth), </span><span style=\"color:#B392F0\">V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(b </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">VectorWidth), acc2);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        acc3 </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">FusedMultiplyAdd</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(a </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">VectorWidth), </span><span style=\"color:#B392F0\">V</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">LoadAligned</span><span style=\"color:#E1E4E8\">(b </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">VectorWidth), acc3);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    inline</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#B392F0\"> Finalize</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        V total </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (acc0 </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> acc1) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (acc2 </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> acc3);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> total.</span><span style=\"color:#B392F0\">HorizontalSum</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">};</span></span></code></pre></div>\n\n<p>{{DIAGRAM:cke-kernel-hierarchy}}\n{{DIAGRAM:tdd-cke-001}}</p>\n<h2 id=\"4-algorithm-pseudo-code\">4. Algorithm Pseudo-code</h2>\n<h3 id=\"41-the-hot-path-super-scalar-iteration-engine\">4.1. The Hot Path: Super-Scalar Iteration Engine</h3>\n<p>This logic handles the main loop, utilizing 4x unrolling and software prefetching to saturate the CPU&#39;s execution ports.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">cpp</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">template</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">typename</span><span style=\"color:#B392F0\"> Kernel</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">typename</span><span style=\"color:#B392F0\"> T</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">T</span><span style=\"color:#B392F0\"> execute_kernel_pipeline</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">KernelContext</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#FFAB70\"> ctx</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Kernel k;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    k.</span><span style=\"color:#B392F0\">Initialize</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    const</span><span style=\"color:#F97583\"> size_t</span><span style=\"color:#E1E4E8\"> W </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> HardwareCapabilities</span><span style=\"color:#E1E4E8\">::</span><span style=\"color:#B392F0\">GetOptimalWidth</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">>();</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    const</span><span style=\"color:#F97583\"> size_t</span><span style=\"color:#E1E4E8\"> UNROLL </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    const</span><span style=\"color:#F97583\"> size_t</span><span style=\"color:#E1E4E8\"> BLOCK_SIZE </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> W </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> UNROLL;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    size_t</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 1. Main Unrolled Loop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> (; i </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> ctx.length </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> BLOCK_SIZE; i </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> BLOCK_SIZE) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Software Prefetching: Bring data into L1/L2 before needed</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        __builtin_prefetch</span><span style=\"color:#E1E4E8\">(ctx.src_a </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> ctx.prefetch_distance, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        __builtin_prefetch</span><span style=\"color:#E1E4E8\">(ctx.src_b </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> ctx.prefetch_distance, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Core Micro-kernel step</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        k.</span><span style=\"color:#B392F0\">Step</span><span style=\"color:#E1E4E8\">(ctx.src_a </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> i, ctx.src_b </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> i);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // 2. Remainder Handling (The \"Tail\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // If the ISA supports masking (AVX-512/SVE), use masked Load/Step</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> constexpr</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#B392F0\">SupportsMasking</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">W</span><span style=\"color:#E1E4E8\">>()) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        size_t</span><span style=\"color:#E1E4E8\"> remaining </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ctx.length </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> i;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (remaining </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            auto</span><span style=\"color:#E1E4E8\"> mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> CreateMask</span><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">W</span><span style=\"color:#E1E4E8\">>(remaining);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            k.</span><span style=\"color:#B392F0\">StepMasked</span><span style=\"color:#E1E4E8\">(ctx.src_a </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> i, ctx.src_b </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> i, mask);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Fallback to scalar for architectures without masking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> (; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> ctx.length; </span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\">i) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            k.</span><span style=\"color:#B392F0\">StepScalar</span><span style=\"color:#E1E4E8\">(ctx.src_a[i], ctx.src_b[i]);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> k.</span><span style=\"color:#B392F0\">Finalize</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h2 id=\"5-engineering-constraints-amp-hazards\">5. Engineering Constraints &amp; Hazards</h2>\n<h3 id=\"51-concurrency-data-dependencies\">5.1. Concurrency: Data Dependencies</h3>\n<ul>\n<li><strong>Hazard (Read-After-Write)</strong>: In kernels that perform reductions (like Dot Product), the <code>acc = fma(a, b, acc)</code> operation creates a dependency chain.</li>\n<li><strong>Mitigation</strong>: The CKE uses multiple accumulators (<code>acc0</code> through <code>acc3</code>). This allows the CPU&#39;s Out-of-Order (OoO) engine to schedule independent FMA instructions while waiting for the latency of the previous accumulation (typically 4-6 cycles) to resolve.</li>\n</ul>\n<h3 id=\"52-memory-the-quotthree-arrayquot-problem\">5.2. Memory: The &quot;Three-Array&quot; Problem</h3>\n<ul>\n<li><strong>Constraint</strong>: Operations like $C = A + B$ are limited by memory bandwidth, not compute throughput.</li>\n<li><strong>Strategy</strong>: When the CKE detects that data exceeds the L3 cache size, it switches the <strong>UVI</strong> to use Non-Temporal Stores (<code>StoreNT</code>) to prevent evicting useful data from the cache hierarchy.</li>\n</ul>\n<h3 id=\"53-micro-optimization-corner\">5.3. Micro-Optimization Corner</h3>\n<ol>\n<li><strong>Loop Peeling</strong>: If the <code>src</code> pointers provided by the user are not aligned to 64 bytes, the CKE performs &quot;Loop Peeling&quot;running a few scalar iterations until the pointer hits an alignment boundarybefore engaging the SIMD Hot Path.</li>\n<li><strong>Instruction Balancing</strong>: The engine attempts to balance the ratio of <code>Load</code> instructions to <code>Arithmetic</code> instructions. For AVX-256, the goal is to keep 2 Loads and 1 Compute instruction in flight per cycle to match the port distribution of Zen/Skylake architectures.</li>\n<li><strong>Kernel Fusion</strong>: The CKE can fuse multiple operations (e.g., <code>Add</code> followed by <code>ReLU</code>) into a single pass. This reduces the number of times data is loaded from RAM, transforming a &quot;Memory-Bound&quot; problem into a &quot;Compute-Bound&quot; one.</li>\n</ol>\n<p>{{DIAGRAM:cke-pipeline-sequence}}\n{{DIAGRAM:tdd-cke-002}}</p>\n<h3 id=\"54-hazards-denormal-floats\">5.4. Hazards: Denormal Floats</h3>\n<ul>\n<li><strong>Hazard</strong>: Floating point numbers very close to zero (denormals) can cause SIMD units to drop in performance by 100x as they fallback to microcode.</li>\n<li><strong>Mitigation</strong>: The CKE provides a <code>FlushToZero</code> (FTZ) and <code>DenormalsAreZero</code> (DAZ) toggle in the <code>KernelContext</code> to ensure deterministic timing in real-time signal processing applications.</li>\n</ul>\n<h1 id=\"-beyond-the-atlas-further-reading\"> Beyond the Atlas: Further Reading</h1>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Concept</th>\n<th align=\"left\">The Paper / Specification</th>\n<th align=\"left\">The Implementation</th>\n<th align=\"left\">Short Summary</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Performance-Portable SIMD</strong></td>\n<td align=\"left\"><a href=\"https://github.com/google/highway/blob/master/g3doc/design.md\">Highway: Fast, portable SIMD</a></td>\n<td align=\"left\"><a href=\"https://github.com/google/highway\">google/highway</a></td>\n<td align=\"left\">The Gold Standard for creating a single SIMD source that scales from SSE4 to AVX-512 and ARM SVE without performance loss.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Runtime Dispatch (IFUNC)</strong></td>\n<td align=\"left\"><a href=\"https://sourceware.org/glibc/wiki/GNU_IFUNC\">GNU IFUNC Specification</a></td>\n<td align=\"left\"><a href=\"https://sourceware.org/git/?p=glibc.git;a=tree;f=sysdeps/x86\">glibc (sysdeps/x86)</a></td>\n<td align=\"left\">The definitive mechanism for resolving the optimal ISA-specific function at load-time with zero runtime branching overhead.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Micro-Kernel Orchestration</strong></td>\n<td align=\"left\"><a href=\"https://www.cs.utexas.edu/~flame/pubs/GotoBLAS-revisited.pdf\">Anatomy of High-Performance Matrix Multiplication</a></td>\n<td align=\"left\"><a href=\"https://github.com/xianyi/OpenBLAS\">OpenBLAS</a></td>\n<td align=\"left\">The foundational research on using small, register-blocked &quot;micro-kernels&quot; to achieve near-theoretical peak performance on modern CPUs.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>SIMD Memory Alignment</strong></td>\n<td align=\"left\"><a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html\">Intel 64 and IA-32 Architectures Optimization Reference Manual</a></td>\n<td align=\"left\"><a href=\"https://github.com/jemalloc/jemalloc\">jemalloc (Aligned Alloc)</a></td>\n<td align=\"left\">The industry-standard guide for understanding cache-line splits, 64-byte alignment, and the hardware penalties of unaligned SIMD loads.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Scalable Vector Abstraction</strong></td>\n<td align=\"left\"><a href=\"https://developer.arm.com/documentation/ddi0584/latest/\">Arm Scalable Vector Extension (SVE) Reference</a></td>\n<td align=\"left\"><a href=\"https://github.com/llvm/llvm-project/tree/main/llvm/lib/Target/AArch64\">LLVM SVE Backend</a></td>\n<td align=\"left\">The primary specification for &quot;Vector Length Agnostic&quot; programming, which moves beyond fixed 128/256-bit widths to hardware-determined widths.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Numerical Stability (IEEE-754)</strong></td>\n<td align=\"left\"><a href=\"https://ieeexplore.ieee.org/document/8766229\">IEEE 754-2019 Standard</a></td>\n<td align=\"left\"><a href=\"http://www.jhauser.us/arithmetic/SoftFloat.html\">Berkeley SoftFloat</a></td>\n<td align=\"left\">The critical specification for floating-point behavior, essential for handling the &quot;Denormal Floats&quot; and FMA precision issues mentioned in the CKE.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Cross-Platform SIMD Mapping</strong></td>\n<td align=\"left\"><a href=\"https://github.com/simd-everywhere/simde\">SIMD Everywhere (SIMDe) Design</a></td>\n<td align=\"left\"><a href=\"https://github.com/simd-everywhere/simde\">simd-everywhere/simde</a></td>\n<td align=\"left\">The most comprehensive implementation of &quot;header-only&quot; translation layers, mapping one ISA&#39;s intrinsics (e.g., SSE) to another (e.g., NEON) with zero cost.</td>\n</tr>\n</tbody></table>\n","toc":[{"level":1,"text":"simd-library","id":"simd-library"},{"level":1,"text":"TDD","id":"tdd"},{"level":1,"text":"Module: Hardware Detection &amp; Dispatch Engine (HDDE)","id":"module-hardware-detection-amp-dispatch-engine-hdde"},{"level":2,"text":"1. Technical Specification","id":"1-technical-specification"},{"level":2,"text":"2. Abstraction Layers","id":"2-abstraction-layers"},{"level":2,"text":"3. Struct &amp; Interface Definitions","id":"3-struct-amp-interface-definitions"},{"level":3,"text":"3.1. Hardware State Representation","id":"31-hardware-state-representation"},{"level":3,"text":"3.2. Dispatcher Interface","id":"32-dispatcher-interface"},{"level":2,"text":"4. Algorithm Pseudo-code","id":"4-algorithm-pseudo-code"},{"level":3,"text":"4.1. The Hot Path: Static-Lazy Dispatch","id":"41-the-hot-path-static-lazy-dispatch"},{"level":2,"text":"5. Engineering Constraints &amp; Hazards","id":"5-engineering-constraints-amp-hazards"},{"level":3,"text":"5.1. Concurrency &amp; Initialization","id":"51-concurrency-amp-initialization"},{"level":3,"text":"5.2. Memory &amp; Alignment","id":"52-memory-amp-alignment"},{"level":3,"text":"5.3. Micro-Optimization Corner","id":"53-micro-optimization-corner"},{"level":1,"text":"Module: Aligned Memory &amp; Buffer Manager","id":"module-aligned-memory-amp-buffer-manager"},{"level":2,"text":"1. Technical Specification","id":"1-technical-specification"},{"level":2,"text":"2. Abstraction Layers","id":"2-abstraction-layers"},{"level":2,"text":"3. Struct &amp; Interface Definitions","id":"3-struct-amp-interface-definitions"},{"level":3,"text":"3.1. Aligned Block Metadata","id":"31-aligned-block-metadata"},{"level":3,"text":"3.2. Primary Manager Interface","id":"32-primary-manager-interface"},{"level":2,"text":"4. Algorithm Pseudo-code","id":"4-algorithm-pseudo-code"},{"level":3,"text":"4.1. The Hot Path: Manual Alignment Logic","id":"41-the-hot-path-manual-alignment-logic"},{"level":2,"text":"5. Engineering Constraints &amp; Hazards","id":"5-engineering-constraints-amp-hazards"},{"level":3,"text":"5.1. Performance &amp; Cache Locality","id":"51-performance-amp-cache-locality"},{"level":3,"text":"5.2. Safety Hazards","id":"52-safety-hazards"},{"level":3,"text":"5.3. Micro-Optimization Corner","id":"53-micro-optimization-corner"},{"level":1,"text":"Module: Unified Vector Interface (UVI)","id":"module-unified-vector-interface-uvi"},{"level":2,"text":"1. Technical Specification","id":"1-technical-specification"},{"level":2,"text":"2. Abstraction Layers","id":"2-abstraction-layers"},{"level":2,"text":"3. Struct &amp; Interface Definitions","id":"3-struct-amp-interface-definitions"},{"level":3,"text":"3.1. Vector Storage &amp; Alignment","id":"31-vector-storage-amp-alignment"},{"level":3,"text":"3.2. SimdTraits Specialization (Example)","id":"32-simdtraits-specialization-example"},{"level":2,"text":"4. Algorithm Pseudo-code","id":"4-algorithm-pseudo-code"},{"level":3,"text":"4.1. The Hot Path: Generic Vectorized Accumulation","id":"41-the-hot-path-generic-vectorized-accumulation"},{"level":2,"text":"5. Engineering Constraints &amp; Hazards","id":"5-engineering-constraints-amp-hazards"},{"level":3,"text":"5.1. Concurrency: Register Pressure","id":"51-concurrency-register-pressure"},{"level":3,"text":"5.2. Memory: Strict Aliasing &amp; Restrict","id":"52-memory-strict-aliasing-amp-restrict"},{"level":3,"text":"5.3. Micro-Optimization Corner","id":"53-micro-optimization-corner"},{"level":3,"text":"5.4. Masked Operations (Predication)","id":"54-masked-operations-predication"},{"level":1,"text":"Module: Compute Kernel Engine (CKE)","id":"module-compute-kernel-engine-cke"},{"level":2,"text":"1. Technical Specification","id":"1-technical-specification"},{"level":2,"text":"2. Abstraction Layers","id":"2-abstraction-layers"},{"level":2,"text":"3. Struct &amp; Interface Definitions","id":"3-struct-amp-interface-definitions"},{"level":3,"text":"3.1. Kernel Execution Context","id":"31-kernel-execution-context"},{"level":3,"text":"3.2. Micro-Kernel Interface","id":"32-micro-kernel-interface"},{"level":2,"text":"4. Algorithm Pseudo-code","id":"4-algorithm-pseudo-code"},{"level":3,"text":"4.1. The Hot Path: Super-Scalar Iteration Engine","id":"41-the-hot-path-super-scalar-iteration-engine"},{"level":2,"text":"5. Engineering Constraints &amp; Hazards","id":"5-engineering-constraints-amp-hazards"},{"level":3,"text":"5.1. Concurrency: Data Dependencies","id":"51-concurrency-data-dependencies"},{"level":3,"text":"5.2. Memory: The &quot;Three-Array&quot; Problem","id":"52-memory-the-quotthree-arrayquot-problem"},{"level":3,"text":"5.3. Micro-Optimization Corner","id":"53-micro-optimization-corner"},{"level":3,"text":"5.4. Hazards: Denormal Floats","id":"54-hazards-denormal-floats"},{"level":1,"text":" Beyond the Atlas: Further Reading","id":"-beyond-the-atlas-further-reading"}],"title":"simd-library","markdown":"# simd-library\n\n\n\n\n\n\n\n# TDD\n\nTo provide a zero-cost abstraction layer for SIMD operations across x86 (SSE/AVX/AVX-512) and ARM (NEON) architectures. The library focuses on deterministic performance, compile-time dispatch optimization, and strict memory alignment safety to prevent segmentation faults and bus errors during high-throughput data processing.\n\n\n\n<!-- TDD_MOD_ID: mod-hardware-dispatch -->\n# Module: Hardware Detection & Dispatch Engine (HDDE)\n\n## 1. Technical Specification\nThe Hardware Detection & Dispatch Engine (HDDE) serves as the foundational decision-maker for the `simd-library`. Its primary responsibility is the identification of CPU Instruction Set Architecture (ISA) extensions (e.g., AVX-512, NEON) and the subsequent orchestration of the \"Best-Fit\" execution path.\n\nThe engine must resolve two conflicting requirements:\n1.  **Runtime Versatility**: The ability to run a single binary on multiple CPU generations.\n2.  **Zero-Overhead Execution**: Ensuring that the dispatch mechanism does not introduce branch mispredictions or pipeline stalls in the \"Hot Path.\"\n\n## 2. Abstraction Layers\nThe HDDE is structured into three discrete layers:\n*   **Layer 0: The Prober**: Architecture-specific assembly (`cpuid` for x86, `mrs` or `/proc/self/auxv` for ARM) that extracts raw feature bits.\n*   **Layer 1: The Registry**: A thread-safe, immutable bit-field representing the global hardware state.\n*   **Layer 2: The Dispatcher**: A functional bridge that uses either Function Multiversioning (FMV) or static function pointers to route data to the optimal SIMD implementation.\n\n## 3. Struct & Interface Definitions\n\n### 3.1. Hardware State Representation\nTo ensure cache-line efficiency, the hardware state is encapsulated in a bit-packed structure, aligned to avoid false sharing.\n\n```cpp\n// Alignment to 64 bytes to prevent cache-line contention in multi-threaded environments\nstruct alignas(64) CPUFeatureSet {\n    uint64_t x86_extensions; // Bits for SSE, AVX, AVX2, AVX512(F,CD,BW,DQ,VL)\n    uint64_t arm_extensions; // Bits for NEON, ASIMD, SVE, SVE2\n    uint32_t cache_line_size;\n    uint32_t logical_cores;\n    \n    // Memory Alignment: \n    // [8 bytes: x86] [8 bytes: arm] [4 bytes: cache] [4 bytes: cores] \n    // [40 bytes: padding/reserved]\n};\n```\n\n### 3.2. Dispatcher Interface\nThe dispatcher utilizes a function pointer table or a static global pointer resolved at load-time (via `__attribute__((constructor))`).\n\n```cpp\ntypedef void (*SimdKernel)(const float* __restrict src, float* __restrict dst, size_t len);\n\nstruct KernelRegistry {\n    SimdKernel current_best_kernel;\n    uint32_t isa_level; // Numeric rank for comparison\n};\n```\n\n{{DIAGRAM:hdde-class-hierarchy}}\n\n![Component Interaction](./diagrams/tdd-hdde-001.svg)\n\n\n## 4. Algorithm Pseudo-code\n\n### 4.1. The Hot Path: Static-Lazy Dispatch\nThis logic occurs at the first invocation of a SIMD-accelerated function. We use the \"Double-Checked Locking\" pattern or atomic `std::call_once`.\n\n```cpp\n// Global atomic pointer to the active kernel\nstatic std::atomic<SimdKernel> global_dispatch_ptr{nullptr};\n\nvoid dispatch_init() {\n    CPUFeatureSet features = probe_hardware();\n    \n    if (features.x86_extensions & FEATURE_AVX512) {\n        global_dispatch_ptr.store(kernel_avx512_impl, std::memory_order_release);\n    } else if (features.x86_extensions & FEATURE_AVX2) {\n        global_dispatch_ptr.store(kernel_avx2_impl, std::memory_order_release);\n    } else {\n        global_dispatch_ptr.store(kernel_scalar_fallback, std::memory_order_release);\n    }\n}\n\n// Hot Path Wrapper\nvoid execute_simd_op(const float* src, float* dst, size_t len) {\n    // Micro-optimization: Use local pointer to avoid multiple atomic loads\n    SimdKernel func = global_dispatch_ptr.load(std::memory_order_acquire);\n    \n    if (__builtin_expect(!func, 0)) {\n        dispatch_init();\n        func = global_dispatch_ptr.load(std::memory_order_relaxed);\n    }\n    \n    // Tail-call optimization target\n    func(src, dst, len);\n}\n```\n\n## 5. Engineering Constraints & Hazards\n\n### 5.1. Concurrency & Initialization\n*   **Race Conditions**: Hardware probing must be idempotent and thread-safe. Static constructors are preferred for deterministic initialization before `main()` is entered.\n*   **Memory Ordering**: The `global_dispatch_ptr` must use `memory_order_acquire/release` to ensure that once the function pointer is visible, the underlying ISA-specific machine code is also fully loaded and executable.\n\n### 5.2. Memory & Alignment\n*   **Data Alignment**: All SIMD kernels assume `alignas(32)` or `alignas(64)` for input buffers. The HDDE must provide a `malloc` wrapper or an alignment validator.\n*   **Fault Handling**: If a kernel is dispatched to an ISA not supported by the hardware (e.g., AVX-512 on an AVX2 chip), the resulting `SIGILL` must be prevented via strict runtime masking in `probe_hardware()`.\n\n### 5.3. Micro-Optimization Corner\n1.  **Branch Target Buffer (BTB) Warming**: For critical loops, the HDDE should be initialized during the application's \"cold start\" phase to ensure the BTB is primed with the correct kernel address, reducing the cost of the indirect function call.\n2.  **IFUNC (Indirect Functions)**: On GNU/Linux systems, the HDDE will leverage `__attribute__((ifunc))` to allow the dynamic linker (ld.so) to resolve the optimal function at load-time. This replaces the `if(!func)` check with a direct PLT (Procedure Linkage Table) entry, reducing the dispatch overhead to exactly zero at runtime.\n3.  **No-Inline Policy**: SIMD kernels themselves should be marked `__attribute__((noinline))` to prevent the compiler from bloating the dispatcher with multiple ISA versions, which would trash the Instruction Cache (I-Cache).\n\n{{DIAGRAM:hdde-sequence-ifunc}}\n{{DIAGRAM:tdd-hdde-002}}\n\n\n<!-- TDD_MOD_ID: mod-memory-management -->\n# Module: Aligned Memory & Buffer Manager\n\n## 1. Technical Specification\nThe Aligned Memory & Buffer Manager (AMBM) provides a deterministic, high-performance memory allocation subsystem specifically designed for SIMD workloads. Standard heap allocators (e.g., `malloc`) typically guarantee 8 or 16-byte alignment; however, modern SIMD instructions (AVX-256, AVX-512) require 32 or 64-byte alignment to avoid `GPF` (General Protection Faults) or performance-degrading split-load penalties.\n\nThe AMBM's primary responsibilities include:\n1.  **Strict Alignment Enforcement**: Guarantees alignment on $2^n$ boundaries (16, 32, 64 bytes).\n2.  **Metadata Management**: Efficiently tracking \"original\" pointers vs. \"aligned\" pointers for safe deallocation.\n3.  **Buffer Lifecycle Control**: Providing RAII-compliant wrappers to prevent memory leaks in complex SIMD pipelines.\n\n## 2. Abstraction Layers\nThe AMBM is divided into three functional strata:\n*   **Layer 0: OS Abstraction (Low Level)**: Wraps platform-specific calls like `posix_memalign` (POSIX), `_aligned_malloc` (MSVC), or `mmap` with `MAP_HUGETLB`.\n*   **Layer 1: Aligned Pointer Arithmetic (Manual Alignment)**: Logic to transform an arbitrary pointer into an aligned one by over-allocating and storing metadata.\n*   **Layer 2: Buffer Templates (High Level)**: C++ templates (`AlignedVector<T>`) that interface with the Hardware Detection Engine to determine optimal alignment at compile/link time.\n\n## 3. Struct & Interface Definitions\n\n### 3.1. Aligned Block Metadata\nTo support manual alignment on platforms without intrinsic aligned allocators, each block must store the offset to the original heap-allocated address.\n\n```cpp\n// Packed to 16 bytes to ensure the header itself doesn't cause \n// massive fragmentation.\nstruct AlignedBlockHeader {\n    void* original_ptr;      // 8 bytes: Address returned by malloc()\n    size_t requested_size;   // 8 bytes: Size in bytes\n    uint32_t alignment_offset; // 4 bytes: Distance from original to aligned\n    uint32_t magic_canary;   // 4 bytes: Debug-only validity check (0xDEADBEEF)\n};\n```\n\n### 3.2. Primary Manager Interface\n```cpp\nclass AlignedBufferManager {\npublic:\n    /**\n     * @brief Allocates memory aligned to the specified boundary.\n     * @param size Total bytes to allocate.\n     * @param alignment Must be a power of 2 (16, 32, 64).\n     */\n    static void* Allocate(size_t size, size_t alignment);\n\n    /**\n     * @brief Frees memory allocated via Allocate.\n     */\n    static void Free(void* aligned_ptr);\n\n    /**\n     * @brief Validates if a pointer is aligned to a specific boundary.\n     */\n    static bool IsAligned(const void* ptr, size_t alignment) noexcept {\n        return (reinterpret_cast<uintptr_t>(ptr) & (alignment - 1)) == 0;\n    }\n};\n```\n\n{{DIAGRAM:ambm-allocation-flow}}\n{{DIAGRAM:tdd-ambm-001}}\n\n## 4. Algorithm Pseudo-code\n\n### 4.1. The Hot Path: Manual Alignment Logic\nWhen `_aligned_malloc` is unavailable, we use the \"Over-allocation\" strategy.\n\n```cpp\nvoid* AlignedBufferManager::Allocate(size_t size, size_t alignment) {\n    // 1. Calculate total size including potential padding and metadata\n    // alignment - 1 (max padding) + sizeof(void*) (to store original ptr)\n    size_t total_size = size + alignment + sizeof(void*);\n\n    // 2. Standard heap allocation\n    void* raw_ptr = std::malloc(total_size);\n    if (!raw_ptr) return nullptr;\n\n    // 3. Pointer Arithmetic\n    // Move forward by size of a pointer, then align up\n    uintptr_t raw_addr = reinterpret_cast<uintptr_t>(raw_ptr);\n    uintptr_t start_addr = raw_addr + sizeof(void*);\n    uintptr_t aligned_addr = (start_addr + alignment - 1) & ~(alignment - 1);\n\n    // 4. Metadata Storage\n    // Store original_ptr right before the aligned_addr\n    void** metadata_loc = reinterpret_cast<void**>(aligned_addr - sizeof(void*));\n    *metadata_loc = raw_ptr;\n\n    return reinterpret_cast<void*>(aligned_addr);\n}\n\nvoid AlignedBufferManager::Free(void* aligned_ptr) {\n    if (!aligned_ptr) return;\n\n    // Retrieve original pointer from the slot immediately preceding the aligned address\n    void** metadata_loc = reinterpret_cast<void**>(reinterpret_cast<uintptr_t>(aligned_ptr) - sizeof(void*));\n    void* original_ptr = *metadata_loc;\n\n    std::free(original_ptr);\n}\n```\n\n## 5. Engineering Constraints & Hazards\n\n### 5.1. Performance & Cache Locality\n*   **False Sharing**: If multiple `AlignedBlockHeaders` are allocated on the same cache line as frequently written data in a multi-threaded context, performance will degrade.\n*   **TLB Misses**: For very large buffers (e.g., > 2MB), the manager should optionally use Huge Pages (`mmap` with `MAP_HUGETLB`) to reduce Translation Lookaside Buffer (TLB) pressure.\n\n### 5.2. Safety Hazards\n*   **Pointer Mismatch**: Passing a pointer allocated with `std::malloc` to `AlignedBufferManager::Free` will result in a segmentation fault or heap corruption, as it will attempt to read metadata from invalid memory.\n*   **Alignment Granularity**: Requesting an alignment that is not a power of 2 will result in undefined behavior in the bitwise logic (`& ~(alignment - 1)`).\n\n### 5.3. Micro-Optimization Corner\n1.  **Prefetching Hint**: The `Allocate` function should ideally call `_mm_prefetch` (or equivalent) on the first few cache lines of the returned buffer to warm the L1 cache before the SIMD kernel begins execution.\n2.  **Size Rounding**: To prevent \"tail-end\" complexity in SIMD loops (handling the remaining `N % vector_width` elements), the manager should round up the `requested_size` to the nearest multiple of the vector width (e.g., 64 bytes for AVX-512). This allows for \"over-reading\" into safe padding without triggering page faults.\n3.  **Zero-Initialization (Lazy)**: Use `mmap` with `MAP_ANONYMOUS` which utilizes the OS \"zero-page\" mechanism. This avoids the cost of `memset(0)` until the memory is actually touched.\n\n{{DIAGRAM:ambm-memory-hierarchy}}\n{{DIAGRAM:tdd-ambm-002}}\n\n\n<!-- TDD_MOD_ID: mod-vector-abstraction -->\n<!-- TDD_MOD_ID: mod-unified-vector-interface -->\n# Module: Unified Vector Interface (UVI)\n\n## 1. Technical Specification\nThe Unified Vector Interface (UVI) provides the high-level, type-safe API for the `simd-library`. While the **HDDE** handles dispatch and the **AMBM** handles memory, the UVI defines the semantic operations performed on data. It abstracts architecture-specific SIMD registers (e.g., `ymm` on x86, `v` on ARM) into a uniform `Vector<T, N>` template.\n\nThe UVI's primary requirements are:\n1.  **Semantic Uniformity**: A single source-code expression (e.g., `a + b`) must compile to the optimal ISA-specific intrinsic (e.g., `_mm256_add_ps` or `vaddq_f32`).\n2.  **Type Safety**: Prevent illegal operations (e.g., adding a 256-bit float vector to a 128-bit integer vector) at compile time.\n3.  **Zero-Overhead Wrappers**: Ensure that the abstraction layer collapses into raw assembly with no temporary object overhead or unnecessary register spills.\n\n## 2. Abstraction Layers\nThe UVI is architected in a \"Trait-to-Implementation\" hierarchy:\n*   **Layer 0: Semantic Traits**: A set of template specializations that map `<Type, Width>` pairs to underlying intrinsic types (e.g., `<float, 8>` -> `__m256`).\n*   **Layer 1: Intrinsic Wrappers**: Inline functions that wrap raw intrinsics in a unified naming convention (`simd_add`, `simd_load`).\n*   **Layer 2: The Vector Object**: The public-facing `Vector<T, N>` class that provides operator overloads and functional programming primitives (map, reduce).\n\n## 3. Struct & Interface Definitions\n\n### 3.1. Vector Storage & Alignment\nThe `Vector` struct is the core primitive. It must be explicitly aligned to ensure it can be mapped directly to SIMD registers.\n\n```cpp\ntemplate <typename T, size_t N>\nstruct alignas(sizeof(T) * N) Vector {\n    // The underlying hardware register type (resolved via traits)\n    using InternalType = typename SimdTraits<T, N>::NativeType;\n    \n    InternalType data;\n\n    // Constraints: \n    // 1. N must be a power of 2.\n    // 2. sizeof(T) * N must be supported by the current ISA (16, 32, 64 bytes).\n    \n    // Interface\n    static Vector Load(const T* ptr);\n    static Vector LoadAligned(const T* ptr);\n    void Store(T* ptr) const;\n};\n```\n\n### 3.2. SimdTraits Specialization (Example)\nMapping logic used by the compiler to select the register width.\n\n```cpp\n// Generic template\ntemplate <typename T, size_t N> struct SimdTraits;\n\n// x86 AVX Specialization\ntemplate <> struct SimdTraits<float, 8> {\n    using NativeType = __m256;\n    static constexpr size_t Alignment = 32;\n    static constexpr const char* ISA = \"AVX/AVX2\";\n};\n\n// ARM NEON Specialization\ntemplate <> struct SimdTraits<float, 4> {\n    using NativeType = float32x4_t;\n    static constexpr size_t Alignment = 16;\n    static constexpr const char* ISA = \"NEON\";\n};\n```\n\n{{DIAGRAM:uvi-class-abstraction}}\n{{DIAGRAM:tdd-uvi-001}}\n\n## 4. Algorithm Pseudo-code\n\n### 4.1. The Hot Path: Generic Vectorized Accumulation\nThis pseudo-code demonstrates how the UVI interacts with the AMBM to process an array of data using the widest available vector.\n\n```cpp\ntemplate <typename T>\nvoid vector_add_pipeline(const T* __restrict a, const T* __restrict b, T* __restrict res, size_t count) {\n    // 1. Determine optimal width from HDDE\n    constexpr size_t W = HardwareCapabilities::GetOptimalWidth<T>();\n    using VType = Vector<T, W>;\n\n    size_t i = 0;\n    // 2. Main Loop: Process W elements per iteration\n    for (; i <= count - W; i += W) {\n        // Load (Assume AMBM provided aligned pointers)\n        VType va = VType::LoadAligned(&a[i]);\n        VType vb = VType::LoadAligned(&b[i]);\n        \n        // Operation (Overloaded operator + calls underlying intrinsics)\n        VType vr = va + vb;\n        \n        // Store\n        vr.Store(&res[i]);\n    }\n\n    // 3. Tail Handling: Scalar fallback for remaining (count % W) elements\n    for (; i < count; ++i) {\n        res[i] = a[i] + b[i];\n    }\n}\n```\n\n## 5. Engineering Constraints & Hazards\n\n### 5.1. Concurrency: Register Pressure\n*   **Hazard**: Over-using complex UVI objects in a single function can lead to \"Register Spilling,\" where the compiler is forced to move SIMD register data to the stack (memory).\n*   **Mitigation**: UVI functions must be marked `__attribute__((always_inline))` and the number of live `Vector` objects in a scope should be monitored.\n\n### 5.2. Memory: Strict Aliasing & Restrict\n*   **Constraint**: The UVI assumes that input and output buffers do not overlap.\n*   **Logic**: Use the `__restrict` keyword in all UVI interfaces to allow the compiler to reorder instructions without worrying about pointer aliasing, which is critical for pipelining SIMD loads/stores.\n\n### 5.3. Micro-Optimization Corner\n1.  **FMA (Fused Multiply-Add)**: The UVI should provide a `Vector::MultiplyAdd(a, b, c)` method. On supported hardware (AVX2+), this maps to a single `vfmadd` instruction ($a \\times b + c$), which has the same latency as a single addition but double the throughput and higher numerical precision.\n2.  **Loop Unrolling**: For the Hot Path, the UVI recommends unrolling loops by a factor of 2 or 4 (e.g., processing $4 \\times 512$ bits per iteration). This hides the latency of the `Load` instructions by utilizing the CPU's multiple execution ports.\n3.  **Non-Temporal Stores**: For very large datasets that exceed the L3 cache, the UVI provides `StoreNT()`. This uses \"Streaming Stores\" (`movntps`) which bypass the cache hierarchy and write directly to RAM, preventing the \"Cache Pollution\" that occurs when processing multi-gigabyte buffers.\n\n\n![SIMD Pipelining](./diagrams/tdd-uvi-002.svg)\n\n\n### 5.4. Masked Operations (Predication)\nFor architectures like AVX-512 and ARM SVE, the UVI supports **Masked Loads/Stores**.\n*   **Logic**: Instead of a scalar tail loop, a bitmask is used to enable/disable specific lanes in a single vector operation.\n*   **Efficiency**: Reduces the \"Tail Penalty\" to near-zero by allowing the final partial block to be processed by the same SIMD hardware.\n\n{{DIAGRAM:uvi-masked-operation-flow}}\n{{DIAGRAM:tdd-uvi-003}}\n\n\n<!-- TDD_MOD_ID: mod-kernel-compute -->\n<!-- TDD_MOD_ID: mod-compute-kernel-engine -->\n# Module: Compute Kernel Engine (CKE)\n\n## 1. Technical Specification\nThe Compute Kernel Engine (CKE) is the orchestration layer of the `simd-library`. While the **UVI** provides raw vector primitives, the CKE transforms these primitives into high-level, composite mathematical operations (e.g., GEMM, 1D/2D Convolutions, Signal Transducers). \n\nThe CKE's primary responsibilities are:\n1.  **Pipeline Orchestration**: Managing the execution of multiple SIMD kernels in a single pass to maximize L1/L2 cache residency.\n2.  **Loop Transformation**: Automatically applying loop unrolling, software prefetching, and tail-handling strategies based on the ISA reported by the **HDDE**.\n3.  **Numerical Stability**: Ensuring that vectorized implementations maintain parity with IEEE-754 standards, specifically regarding Fused Multiply-Add (FMA) accumulations.\n\n## 2. Abstraction Layers\nThe CKE operates through three distinct functional tiers:\n*   **Layer 0: Kernel Primitives**: Low-level \"Micro-Kernels\" that perform a single operation on a fixed number of registers (e.g., a $4 \\times 4$ matrix-vector block).\n*   **Layer 1: The Iteration Engine**: Logic responsible for traversing memory buffers, handling alignment boundaries via the **AMBM**, and managing the \"Horizontal-to-Vertical\" data transformations.\n*   **Layer 2: Kernel DSL/Interface**: The high-level API where users define complex pipelines (e.g., `Map().Filter().Reduce()`) which the CKE compiles into an optimized instruction stream.\n\n## 3. Struct & Interface Definitions\n\n### 3.1. Kernel Execution Context\nTo minimize stack frame overhead, the execution state is encapsulated in a cache-aligned context.\n\n```cpp\n// Aligned to 64 bytes to match AVX-512 cache-line and prevent false sharing\nstruct alignas(64) KernelContext {\n    const float* __restrict src_a;\n    const float* __restrict src_b;\n    float* __restrict out;\n    size_t length;\n    \n    // Stride information for multidimensional kernels\n    struct {\n        size_t row_stride;\n        size_t col_stride;\n    } metrics;\n\n    // Prefetching configuration\n    uint32_t prefetch_distance; // Measured in cache lines\n    \n    // [Padding to 64 bytes to ensure subsequent contexts in an array are line-aligned]\n};\n```\n\n### 3.2. Micro-Kernel Interface\nKernels are defined as stateless functors to allow the compiler to inline them directly into the CKE's main loop.\n\n```cpp\ntemplate <typename T, size_t VectorWidth>\nstruct DotProductKernel {\n    using V = Vector<T, VectorWidth>;\n\n    // Accumulator state held in registers\n    V acc0, acc1, acc2, acc3;\n\n    inline void Initialize() {\n        acc0 = V::Zero(); acc1 = V::Zero();\n        acc2 = V::Zero(); acc3 = V::Zero();\n    }\n\n    // Process 4 vectors per call (Unroll factor 4)\n    inline void Step(const T* a, const T* b) {\n        acc0 = V::FusedMultiplyAdd(V::LoadAligned(a + 0*VectorWidth), V::LoadAligned(b + 0*VectorWidth), acc0);\n        acc1 = V::FusedMultiplyAdd(V::LoadAligned(a + 1*VectorWidth), V::LoadAligned(b + 1*VectorWidth), acc1);\n        acc2 = V::FusedMultiplyAdd(V::LoadAligned(a + 2*VectorWidth), V::LoadAligned(b + 2*VectorWidth), acc2);\n        acc3 = V::FusedMultiplyAdd(V::LoadAligned(a + 3*VectorWidth), V::LoadAligned(b + 3*VectorWidth), acc3);\n    }\n\n    inline T Finalize() {\n        V total = (acc0 + acc1) + (acc2 + acc3);\n        return total.HorizontalSum();\n    }\n};\n```\n\n{{DIAGRAM:cke-kernel-hierarchy}}\n{{DIAGRAM:tdd-cke-001}}\n\n## 4. Algorithm Pseudo-code\n\n### 4.1. The Hot Path: Super-Scalar Iteration Engine\nThis logic handles the main loop, utilizing 4x unrolling and software prefetching to saturate the CPU's execution ports.\n\n```cpp\ntemplate <typename Kernel, typename T>\nT execute_kernel_pipeline(KernelContext& ctx) {\n    Kernel k;\n    k.Initialize();\n\n    const size_t W = HardwareCapabilities::GetOptimalWidth<T>();\n    const size_t UNROLL = 4;\n    const size_t BLOCK_SIZE = W * UNROLL;\n\n    size_t i = 0;\n    \n    // 1. Main Unrolled Loop\n    for (; i <= ctx.length - BLOCK_SIZE; i += BLOCK_SIZE) {\n        // Software Prefetching: Bring data into L1/L2 before needed\n        __builtin_prefetch(ctx.src_a + i + ctx.prefetch_distance, 0, 3);\n        __builtin_prefetch(ctx.src_b + i + ctx.prefetch_distance, 0, 3);\n\n        // Core Micro-kernel step\n        k.Step(ctx.src_a + i, ctx.src_b + i);\n    }\n\n    // 2. Remainder Handling (The \"Tail\")\n    // If the ISA supports masking (AVX-512/SVE), use masked Load/Step\n    if constexpr (SupportsMasking<T, W>()) {\n        size_t remaining = ctx.length - i;\n        if (remaining > 0) {\n            auto mask = CreateMask<T, W>(remaining);\n            k.StepMasked(ctx.src_a + i, ctx.src_b + i, mask);\n        }\n    } else {\n        // Fallback to scalar for architectures without masking\n        for (; i < ctx.length; ++i) {\n            k.StepScalar(ctx.src_a[i], ctx.src_b[i]);\n        }\n    }\n\n    return k.Finalize();\n}\n```\n\n## 5. Engineering Constraints & Hazards\n\n### 5.1. Concurrency: Data Dependencies\n*   **Hazard (Read-After-Write)**: In kernels that perform reductions (like Dot Product), the `acc = fma(a, b, acc)` operation creates a dependency chain.\n*   **Mitigation**: The CKE uses multiple accumulators (`acc0` through `acc3`). This allows the CPU's Out-of-Order (OoO) engine to schedule independent FMA instructions while waiting for the latency of the previous accumulation (typically 4-6 cycles) to resolve.\n\n### 5.2. Memory: The \"Three-Array\" Problem\n*   **Constraint**: Operations like $C = A + B$ are limited by memory bandwidth, not compute throughput.\n*   **Strategy**: When the CKE detects that data exceeds the L3 cache size, it switches the **UVI** to use Non-Temporal Stores (`StoreNT`) to prevent evicting useful data from the cache hierarchy.\n\n### 5.3. Micro-Optimization Corner\n1.  **Loop Peeling**: If the `src` pointers provided by the user are not aligned to 64 bytes, the CKE performs \"Loop Peeling\"running a few scalar iterations until the pointer hits an alignment boundarybefore engaging the SIMD Hot Path.\n2.  **Instruction Balancing**: The engine attempts to balance the ratio of `Load` instructions to `Arithmetic` instructions. For AVX-256, the goal is to keep 2 Loads and 1 Compute instruction in flight per cycle to match the port distribution of Zen/Skylake architectures.\n3.  **Kernel Fusion**: The CKE can fuse multiple operations (e.g., `Add` followed by `ReLU`) into a single pass. This reduces the number of times data is loaded from RAM, transforming a \"Memory-Bound\" problem into a \"Compute-Bound\" one.\n\n{{DIAGRAM:cke-pipeline-sequence}}\n{{DIAGRAM:tdd-cke-002}}\n\n### 5.4. Hazards: Denormal Floats\n*   **Hazard**: Floating point numbers very close to zero (denormals) can cause SIMD units to drop in performance by 100x as they fallback to microcode.\n*   **Mitigation**: The CKE provides a `FlushToZero` (FTZ) and `DenormalsAreZero` (DAZ) toggle in the `KernelContext` to ensure deterministic timing in real-time signal processing applications.\n\n\n#  Beyond the Atlas: Further Reading\n\n| Concept | The Paper / Specification | The Implementation | Short Summary |\n| :--- | :--- | :--- | :--- |\n| **Performance-Portable SIMD** | [Highway: Fast, portable SIMD](https://github.com/google/highway/blob/master/g3doc/design.md) | [google/highway](https://github.com/google/highway) | The Gold Standard for creating a single SIMD source that scales from SSE4 to AVX-512 and ARM SVE without performance loss. |\n| **Runtime Dispatch (IFUNC)** | [GNU IFUNC Specification](https://sourceware.org/glibc/wiki/GNU_IFUNC) | [glibc (sysdeps/x86)](https://sourceware.org/git/?p=glibc.git;a=tree;f=sysdeps/x86) | The definitive mechanism for resolving the optimal ISA-specific function at load-time with zero runtime branching overhead. |\n| **Micro-Kernel Orchestration** | [Anatomy of High-Performance Matrix Multiplication](https://www.cs.utexas.edu/~flame/pubs/GotoBLAS-revisited.pdf) | [OpenBLAS](https://github.com/xianyi/OpenBLAS) | The foundational research on using small, register-blocked \"micro-kernels\" to achieve near-theoretical peak performance on modern CPUs. |\n| **SIMD Memory Alignment** | [Intel 64 and IA-32 Architectures Optimization Reference Manual](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html) | [jemalloc (Aligned Alloc)](https://github.com/jemalloc/jemalloc) | The industry-standard guide for understanding cache-line splits, 64-byte alignment, and the hardware penalties of unaligned SIMD loads. |\n| **Scalable Vector Abstraction** | [Arm Scalable Vector Extension (SVE) Reference](https://developer.arm.com/documentation/ddi0584/latest/) | [LLVM SVE Backend](https://github.com/llvm/llvm-project/tree/main/llvm/lib/Target/AArch64) | The primary specification for \"Vector Length Agnostic\" programming, which moves beyond fixed 128/256-bit widths to hardware-determined widths. |\n| **Numerical Stability (IEEE-754)** | [IEEE 754-2019 Standard](https://ieeexplore.ieee.org/document/8766229) | [Berkeley SoftFloat](http://www.jhauser.us/arithmetic/SoftFloat.html) | The critical specification for floating-point behavior, essential for handling the \"Denormal Floats\" and FMA precision issues mentioned in the CKE. |\n| **Cross-Platform SIMD Mapping** | [SIMD Everywhere (SIMDe) Design](https://github.com/simd-everywhere/simde) | [simd-everywhere/simde](https://github.com/simd-everywhere/simde) | The most comprehensive implementation of \"header-only\" translation layers, mapping one ISA's intrinsics (e.g., SSE) to another (e.g., NEON) with zero cost. |"}