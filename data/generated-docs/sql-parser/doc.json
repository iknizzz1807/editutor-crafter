{"html":"<h1 id=\"sql-parser-design-document\">SQL Parser: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A SQL parser that transforms SQL query strings into Abstract Syntax Trees (ASTs) for SELECT, INSERT, UPDATE, and DELETE statements. The key architectural challenge is building a robust tokenizer and recursive descent parser that handles SQL&#39;s complex grammar rules, operator precedence, and various expression types while maintaining extensibility for future SQL features.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides foundational understanding for all milestones (1-4), establishing the conceptual framework and challenges that guide our parser design decisions.</p>\n</blockquote>\n<h3 id=\"mental-model-language-translation\">Mental Model: Language Translation</h3>\n<p>Understanding SQL parsing requires us to think about how humans process and understand language. When you read a sentence in a foreign language that you&#39;re learning, your brain goes through several distinct phases. First, you break the sentence into individual words and punctuation—this is <strong>tokenization</strong>. Then, you identify what type each word is: noun, verb, adjective, preposition. Finally, you apply grammar rules to understand how these words relate to each other and what the overall sentence means.</p>\n<p>SQL parsing works exactly the same way. Consider the SQL statement <code>SELECT name, age FROM users WHERE age &gt; 18</code>. A human reading this intuitively understands that &quot;SELECT&quot; indicates we&#39;re retrieving data, &quot;name&quot; and &quot;age&quot; are the specific pieces of information we want, &quot;users&quot; is the source of that information, and &quot;age &gt; 18&quot; is a condition that filters the results. But a computer program starts with just a string of characters and must systematically work through the same process your brain does automatically.</p>\n<p>The <strong>tokenizer</strong> acts like someone learning to read who carefully identifies each word and punctuation mark. It scans through &quot;SELECT name, age FROM users WHERE age &gt; 18&quot; character by character and produces a sequence of <strong>tokens</strong>: [SELECT_KEYWORD, IDENTIFIER(&quot;name&quot;), COMMA, IDENTIFIER(&quot;age&quot;), FROM_KEYWORD, IDENTIFIER(&quot;users&quot;), WHERE_KEYWORD, IDENTIFIER(&quot;age&quot;), GREATER_THAN, NUMBER(18)]. Each token includes not just the text but also its <strong>type</strong>—just like identifying parts of speech in human language.</p>\n<p>The <strong>parser</strong> then acts like a grammar student applying sentence structure rules. It takes the token sequence and builds an <strong>Abstract Syntax Tree (AST)</strong>—a hierarchical representation that captures the logical structure and relationships in the query. The AST for our example would have a SELECT node at the root, with child nodes representing the column list (name, age), the table reference (users), and the WHERE condition (age &gt; 18). This tree structure makes the query&#39;s meaning explicit and computable.</p>\n<p>Just as human language translation can be tricky due to ambiguous phrases, multiple meanings, and context-dependent interpretation, SQL parsing faces similar challenges that make it far more complex than it initially appears.</p>\n<h3 id=\"sql-parsing-challenges\">SQL Parsing Challenges</h3>\n<p>SQL presents a unique combination of parsing challenges that make it significantly more difficult than many other programming languages. Unlike languages with rigid, unambiguous syntax, SQL was designed for readability and flexibility, which creates numerous complications for parser implementers.</p>\n<p><strong>Keyword Context Sensitivity</strong> represents one of the most pervasive challenges in SQL parsing. Many SQL &quot;keywords&quot; can also function as valid identifiers depending on their position in the statement. For example, <code>ORDER</code> is a reserved keyword in <code>ORDER BY</code>, but it&#39;s also a perfectly valid table name in <code>SELECT * FROM ORDER</code>. The parser cannot determine whether <code>ORDER</code> should be treated as a keyword or identifier until it examines the surrounding context. This forces the parser to implement <strong>lookahead</strong> mechanisms or <strong>backtracking</strong> strategies, significantly complicating the parsing logic.</p>\n<p>Consider the statement <code>SELECT order FROM order ORDER BY order</code>. Here, the first <code>order</code> is a column name, <code>order</code> in the FROM clause is a table name, and the final <code>order</code> is the ORDER BY keyword—three different roles for the same text. The parser must track its current position in the grammar to make these distinctions correctly.</p>\n<p><strong>Case Insensitivity with Mixed Conventions</strong> adds another layer of complexity. SQL keywords are case-insensitive (<code>SELECT</code>, <code>select</code>, and <code>Select</code> are equivalent), but the handling of identifiers varies between database systems. Some systems preserve identifier case, others fold to uppercase or lowercase, and some only fold unquoted identifiers while preserving quoted ones. This means the tokenizer must track whether identifiers are quoted and apply different normalization rules accordingly.</p>\n<p><strong>Quote Character Variations</strong> create tokenization ambiguity. SQL supports multiple quote types: single quotes for string literals (<code>&#39;hello&#39;</code>), double quotes for identifiers (<code>&quot;table name&quot;</code>), and backticks for identifiers in MySQL (<code>`table name`</code>). Some systems allow these to be used interchangeably, while others have strict rules. The tokenizer must distinguish between <code>&quot;table&quot;</code> as a quoted identifier and <code>&#39;table&#39;</code> as a string literal, which requires different parsing logic and token types.</p>\n<p><strong>Operator Precedence and Associativity</strong> in WHERE clauses mirror the complexity found in mathematical expressions but with SQL-specific twists. The expression <code>a = b AND c = d OR e = f</code> must be parsed as <code>((a = b) AND (c = d)) OR (e = f)</code> due to operator precedence rules (comparison operators bind tighter than AND, which binds tighter than OR). However, SQL also includes operators like <code>BETWEEN</code>, <code>IN</code>, <code>LIKE</code>, and <code>IS NULL</code> that don&#39;t exist in typical mathematical expressions and have their own precedence relationships.</p>\n<p><strong>Ambiguous Grammar Productions</strong> occur when the same input can be parsed in multiple valid ways according to the grammar rules. The classic example is the <strong>dangling ELSE problem</strong> in programming languages, but SQL has its own version with expressions like <code>SELECT a, b FROM c</code>. Without additional tokens, it&#39;s unclear whether this is a complete statement or if a WHERE clause might follow. The parser must implement strategies to resolve these ambiguities consistently.</p>\n<p><strong>Expression vs Statement Boundary Confusion</strong> happens because SQL allows complex expressions within statements, but the boundary between where expressions end and the next clause begins can be ambiguous. Consider <code>SELECT a + b * c FROM table WHERE x</code>. The parser must correctly identify that <code>a + b * c</code> is a single expression in the SELECT clause, and <code>x</code> is the beginning of a WHERE clause expression, not a continuation of the SELECT expression.</p>\n<p><strong>Whitespace and Comment Handling</strong> requires careful consideration because SQL allows comments in multiple forms (<code>-- single line</code> and <code>/* multi-line */</code>) and has flexible whitespace rules. Comments can appear almost anywhere in a statement, including within expressions, and the parser must handle them without breaking the logical flow of the query. Additionally, some SQL dialects have specific rules about line continuation and whitespace significance.</p>\n<p><strong>Error Recovery Complexity</strong> makes SQL parsing particularly challenging for practical tools. When a human writes malformed SQL, they expect helpful error messages that point to the specific problem and suggest corrections. However, SQL&#39;s flexible syntax means that by the time the parser detects an error, it may be far from the actual mistake. For example, a missing comma in a column list might not be detected until the parser tries to parse the FROM clause and encounters an unexpected keyword.</p>\n<h3 id=\"existing-parser-approaches\">Existing Parser Approaches</h3>\n<p>The SQL parsing landscape has evolved over decades, with different approaches emerging to address the challenges outlined above. Understanding these existing approaches helps us make informed decisions about our parser architecture and avoid common pitfalls.</p>\n<blockquote>\n<p><strong>Decision: Hand-Written vs Parser Generator</strong></p>\n<ul>\n<li><strong>Context</strong>: SQL parsers can be implemented using traditional parser generators (ANTLR, Yacc, Bison) that generate parsing code from grammar files, or hand-written using techniques like recursive descent. This fundamental choice affects every aspect of the implementation.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Parser Generator (ANTLR/Yacc) approach</li>\n<li>Hand-written Recursive Descent approach</li>\n<li>Hybrid approach with generated tokenizer and hand-written parser</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hand-written Recursive Descent parser</li>\n<li><strong>Rationale</strong>: Better error messages, easier debugging, more control over AST construction, and simpler integration with custom logic for SQL&#39;s context-sensitive features</li>\n<li><strong>Consequences</strong>: More initial implementation work but greater flexibility and maintainability for educational purposes</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parser Generator (ANTLR)</td>\n<td>Fast development, formally verified grammar, handles complex precedence automatically</td>\n<td>Hard to customize error messages, difficult to debug generated code, complex AST integration</td>\n</tr>\n<tr>\n<td>Hand-Written Recursive Descent</td>\n<td>Complete control over parsing logic, excellent error messages, easy to debug and extend</td>\n<td>More initial code to write, manual precedence handling, potential for grammar mistakes</td>\n</tr>\n<tr>\n<td>Hybrid (Generated + Hand-Written)</td>\n<td>Combines benefits of both approaches</td>\n<td>Complex integration, requires expertise in both techniques</td>\n</tr>\n</tbody></table>\n<p><strong>Parser Generator Approaches</strong> use tools like ANTLR, Yacc, or Bison to automatically generate parsing code from formal grammar specifications. Major database systems like PostgreSQL use variations of this approach. The PostgreSQL parser uses a Yacc-based grammar with over 500 production rules that handle the full SQL standard plus PostgreSQL extensions. These tools excel at handling complex grammar rules automatically and can generate efficient parsing code.</p>\n<p>However, parser generators present significant challenges for SQL specifically. SQL&#39;s context-sensitive keywords require grammar hacks or post-processing steps that complicate the supposedly &quot;automatic&quot; generation. Error messages from generated parsers tend to be cryptic (&quot;expected IDENTIFIER but found ORDER&quot;) rather than helpful (&quot;ORDER is a reserved keyword here; use quotes if you meant it as a table name&quot;). Additionally, integrating custom AST construction logic into generated parsers often requires learning the generator tool&#39;s specific templating language.</p>\n<p><strong>Hand-Written Recursive Descent</strong> approaches implement parsing logic directly in the target programming language. Each grammar rule becomes a function that consumes tokens and builds AST nodes. SQLite uses this approach with a hand-written C parser that provides excellent error messages and tight integration with the query execution engine. The MySQL parser also uses hand-written techniques, allowing for fine-tuned error recovery and custom syntax extensions.</p>\n<p>The recursive descent approach maps naturally to SQL&#39;s hierarchical structure. A <code>parseSelectStatement()</code> function calls <code>parseSelectList()</code>, <code>parseFromClause()</code>, and <code>parseWhereClause()</code> functions, each of which handles its specific grammar rules. This makes the parser logic transparent and debuggable—you can step through the parsing process in a debugger and see exactly which grammar rule is being applied at each step.</p>\n<p><strong>Precedence Climbing Parsers</strong> represent a specialized technique for handling expression parsing within recursive descent parsers. Rather than encoding operator precedence into the grammar rules (which creates deep recursion and poor error messages), precedence climbing algorithms use a table-driven approach. The parser maintains a <strong>precedence table</strong> that assigns numeric priorities to operators, then uses a loop to build expression trees with correct precedence and associativity.</p>\n<p>This technique is particularly valuable for SQL WHERE clauses, which can contain complex expressions with dozens of different operators. Rather than writing separate grammar rules for each precedence level (which would require functions like <code>parseOrExpression()</code>, <code>parseAndExpression()</code>, <code>parseComparisonExpression()</code>, etc.), a single precedence climbing function can handle the entire expression hierarchy.</p>\n<p><strong>Packrat Parsing</strong> techniques use memoization to handle ambiguous grammars and provide unlimited lookahead. Some research SQL parsers use packrat parsing to handle SQL&#39;s most problematic ambiguities by trying multiple parse paths and selecting the successful one. While powerful, packrat parsing requires significantly more memory and implementation complexity than simpler approaches.</p>\n<p><strong>Multi-Pass Parsing</strong> strategies separate different aspects of parsing into distinct phases. The first pass might handle only tokenization and basic statement structure, while subsequent passes refine the AST and resolve ambiguities with additional context. Some commercial SQL tools use this approach to provide features like syntax highlighting (first pass) and semantic validation (later passes) independently.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: The choice of parsing approach significantly impacts not just implementation complexity, but also the quality of error messages, debugging experience, and extensibility for future SQL features. For educational purposes, the transparency and debuggability of hand-written recursive descent outweighs the initial implementation complexity.</p>\n</blockquote>\n<p><strong>Error Handling Strategies</strong> vary dramatically between approaches. Production SQL parsers like those in PostgreSQL and MySQL implement sophisticated error recovery that attempts to continue parsing after encountering errors, allowing them to report multiple problems in a single statement. They maintain <strong>error recovery points</strong> in the grammar where parsing can restart after consuming tokens until a stable state is reached.</p>\n<p>Educational and development-focused parsers often use <strong>panic mode recovery</strong>, where parsing stops at the first error and reports it with maximum detail. This approach provides clearer error messages for learning purposes but doesn&#39;t help users who want to see all problems in their SQL at once.</p>\n<p><strong>Performance Considerations</strong> distinguish production parsers from educational ones. Production systems must parse thousands of queries per second with minimal memory allocation. They often use techniques like <strong>token pooling</strong> (reusing token objects), <strong>string interning</strong> (avoiding duplicate identifier strings), and <strong>AST node pooling</strong> to minimize garbage collection pressure.</p>\n<p>For our educational SQL parser, we prioritize clarity and debuggability over maximum performance, but understanding these optimization techniques helps explain why production parsers are structured the way they are.</p>\n<p>The landscape of existing SQL parsers shows that there&#39;s no single &quot;correct&quot; approach—each technique involves trade-offs between development time, runtime performance, error message quality, and extensibility. Our design decisions must balance these factors while keeping our educational goals in mind.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Fsystem-components.svg\" alt=\"SQL Parser System Components\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenizer</td>\n<td>Character-by-character scanning with string operations</td>\n<td>State machine with lookup tables</td>\n</tr>\n<tr>\n<td>Parser</td>\n<td>Direct recursive descent with manual precedence</td>\n<td>Precedence climbing with operator tables</td>\n</tr>\n<tr>\n<td>AST Nodes</td>\n<td>Simple classes with basic properties</td>\n<td>Visitor pattern with type-safe node traversal</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception-based with position tracking</td>\n<td>Error accumulation with recovery strategies</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql_parser/\n├── __init__.py              # Package initialization\n├── tokens.py                # Token type definitions and Token class\n├── tokenizer.py             # Lexical analysis (Milestone 1)\n├── ast_nodes.py             # AST node class hierarchy\n├── parser.py                # Main parser class and entry point\n├── select_parser.py         # SELECT statement parsing (Milestone 2)\n├── expression_parser.py     # WHERE clause expressions (Milestone 3)\n├── dml_parser.py           # INSERT/UPDATE/DELETE (Milestone 4)\n├── errors.py               # Parser exception classes\n└── tests/\n    ├── test_tokenizer.py    # Tokenizer tests\n    ├── test_select.py       # SELECT parsing tests\n    ├── test_expressions.py  # Expression parsing tests\n    ├── test_dml.py         # DML statement tests\n    └── test_integration.py  # End-to-end parser tests</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tokens.py - Complete token type definitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Keywords</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SELECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FROM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WHERE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INSERT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UPDATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DELETE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTO</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VALUES</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Identifiers and Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Logical</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AND</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEMICOLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LEFT_PAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RIGHT_PAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASTERISK</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Special</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># SQL keyword mapping for tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SQL_KEYWORDS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'SELECT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SELECT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'FROM'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">FROM</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'WHERE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">WHERE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'INSERT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">INSERT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'UPDATE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">UPDATE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'DELETE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">DELETE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'INTO'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">INTO</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'VALUES'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">VALUES</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'SET'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'AND'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">AND</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'OR'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'NOT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">NOT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># errors.py - Complete error handling infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all parsing errors\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Column </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors during tokenization phase\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors during syntax parsing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UnexpectedTokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Specific error for unexpected tokens\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, actual: Token):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, but found </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, actual.position, actual.line, actual.column)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.actual </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tokenizer.py - Tokenizer implementation skeleton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .tokens </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, TokenType, </span><span style=\"color:#79B8FF\">SQL_KEYWORDS</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenizerError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Tokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sql_text</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: List[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Main tokenization entry point. Converts SQL text into token list.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns complete list of tokens including EOF marker.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement main tokenization loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call appropriate helper methods based on current character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle whitespace and comments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add EOF token at end</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return completed token list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _current_char</span><span style=\"color:#E1E4E8\">(self) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns current character or None if at end\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if position is within text bounds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return character at current position or None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _peek_char</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look ahead at character without advancing position\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate peek position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return character if within bounds, None otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Move to next character, updating position tracking\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Increment position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update line/column tracking for newlines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle different line ending types (\\n, \\r\\n)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Skip spaces, tabs, newlines\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Loop while current char is whitespace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call _advance() for each whitespace character</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _read_string_literal</span><span style=\"color:#E1E4E8\">(self, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Read quoted string literal, handling escape sequences\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store starting position for token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Advance past opening quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Read characters until closing quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle escape sequences (\\', \\\", \\\\)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Raise error if string is not terminated</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return STRING_LITERAL token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Python Hints:</strong></p>\n<ul>\n<li>Use <code>str.isalpha()</code>, <code>str.isdigit()</code>, and <code>str.isalnum()</code> for character classification during tokenization</li>\n<li>Python&#39;s <code>enum.auto()</code> provides automatic enumeration values for token types</li>\n<li>Use <code>@dataclass</code> for Token and AST node classes to get automatic <code>__init__</code>, <code>__repr__</code>, etc.</li>\n<li><code>typing.Optional[T]</code> indicates values that can be None (important for end-of-file conditions)</li>\n<li>String slicing (<code>text[start:end]</code>) is efficient for extracting token values</li>\n<li>Use <code>str.upper()</code> for case-insensitive keyword matching in <code>SQL_KEYWORDS</code> dictionary lookup</li>\n<li>List comprehensions are useful for filtering tokens: <code>[t for t in tokens if t.type != TokenType.WHITESPACE]</code></li>\n</ul>\n<p><strong>F. Initial Milestone Checkpoint:</strong></p>\n<p>After implementing the basic tokenizer structure:</p>\n<ol>\n<li><strong>Test Command</strong>: <code>python -m pytest tests/test_tokenizer.py -v</code></li>\n<li><strong>Expected Output</strong>: Tests should pass for basic keyword recognition and identifier tokenization</li>\n<li><strong>Manual Verification</strong>: Create a simple test script:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> sql_parser.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tokenizer(</span><span style=\"color:#9ECBFF\">\"SELECT name FROM users\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer.tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   for</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<p>   Expected output:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   SELECT: 'SELECT'\n   IDENTIFIER: 'name'\n   FROM: 'FROM'\n   IDENTIFIER: 'users'\n   EOF: ''</code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Common Issues to Check</strong>:<ul>\n<li>If keywords appear as IDENTIFIER tokens, check case-insensitive lookup in <code>SQL_KEYWORDS</code></li>\n<li>If position tracking is wrong, verify <code>_advance()</code> properly updates line/column</li>\n<li>If string literals fail, ensure quote character matching and escape sequence handling</li>\n</ul>\n</li>\n</ol>\n<p><strong>G. Debugging Tips for Context and Problem Understanding:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Keywords recognized as identifiers</td>\n<td>Case sensitivity in keyword lookup</td>\n<td>Print token values and check SQL_KEYWORDS keys</td>\n<td>Use <code>.upper()</code> before dictionary lookup</td>\n</tr>\n<tr>\n<td>Position tracking incorrect</td>\n<td>Not updating line/column in _advance()</td>\n<td>Add debug prints in _advance() method</td>\n<td>Properly handle \\n and \\r\\n line endings</td>\n</tr>\n<tr>\n<td>String parsing fails</td>\n<td>Quote character handling logic</td>\n<td>Test with simple quoted strings first</td>\n<td>Separate logic for single vs double quotes</td>\n</tr>\n<tr>\n<td>Tokenizer seems to hang</td>\n<td>Infinite loop in character scanning</td>\n<td>Add position debug prints in main loop</td>\n<td>Ensure _advance() always moves position forward</td>\n</tr>\n</tbody></table>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides foundational understanding for all milestones (1-4), establishing the scope and boundaries that guide our parser implementation decisions throughout the project.</p>\n</blockquote>\n<h3 id=\"mental-model-building-a-language-translator\">Mental Model: Building a Language Translator</h3>\n<p>Think of our SQL parser as building a specialized language translator that only works with a specific dialect of SQL. Just as a human translator might specialize in translating business documents between English and Spanish (rather than translating poetry or slang), our parser will specialize in translating basic SQL statements into a structured format that programs can understand. We&#39;re not building Google Translate for SQL - we&#39;re building a focused, reliable translator for the most common SQL operations that a database application needs.</p>\n<p>This translator has three key characteristics that define its scope. First, it has <strong>vocabulary limitations</strong> - it only understands certain SQL keywords and constructs, just like a business translator might not know specialized medical terminology. Second, it has <strong>complexity boundaries</strong> - it can handle straightforward sentences but not highly complex nested structures, similar to how a translator might handle business emails but struggle with legal documents. Third, it has <strong>quality standards</strong> - it must produce accurate, well-structured output for the cases it does handle, even if it can&#39;t handle every possible input.</p>\n<p>Understanding these boundaries upfront is crucial because they guide every design decision in our parser. When we encounter a choice between supporting more SQL features versus making our core features more robust, our goals will tell us which path to take. When we decide how much error recovery to implement, our non-goals will tell us where to stop. When we design our AST structure, our functional goals will tell us which nodes we need to include.</p>\n<h3 id=\"functional-goals\">Functional Goals</h3>\n<p>Our SQL parser must successfully handle the four fundamental categories of SQL statements that form the backbone of most database applications. These represent the core operations that developers use daily when building applications with database persistence.</p>\n<p><strong>SELECT Statement Parsing</strong> forms the foundation of our query support. The parser must correctly handle basic SELECT statements with column lists, including both explicit column names and the star wildcard for selecting all columns. It must parse FROM clauses with table references, supporting both simple table names and table aliases using either the explicit AS keyword or implicit aliasing where the alias follows the table name directly. The parser must handle qualified column references using dot notation, such as <code>users.name</code> or <code>orders.total</code>, which are essential for queries involving multiple tables or for clarity in single-table queries.</p>\n<p>Within SELECT statements, the parser must support comma-separated column lists of arbitrary length, maintaining the correct order and preserving any alias information. For example, parsing <code>SELECT id, name AS user_name, email FROM users AS u</code> should produce an AST that captures the three selected columns, the alias for the name column, and the alias for the users table. This level of detail in the AST enables downstream tools to understand not just what data is being retrieved, but how it should be labeled in result sets.</p>\n<p><strong>WHERE Clause Expression Parsing</strong> represents one of the most complex functional requirements because it involves handling operator precedence, associativity, and multiple expression types. The parser must correctly parse comparison operators including equals, not equals, less than, greater than, less than or equal to, and greater than or equal to. It must handle logical operators AND, OR, and NOT with the correct precedence rules, where NOT binds most tightly, followed by AND, then OR. Parentheses must be supported to override default precedence, allowing developers to express complex logical conditions unambiguously.</p>\n<p>The WHERE clause parser must also handle NULL checking operations, including IS NULL and IS NOT NULL, which have special semantics in SQL that differ from regular equality comparisons. String literals, numeric literals, and identifiers must all be supported as operands in expressions. For example, the parser should correctly handle complex conditions like <code>WHERE (age &gt; 18 AND status = &#39;active&#39;) OR (type = &#39;premium&#39; AND balance IS NOT NULL)</code>, building an expression tree that preserves the intended logical structure.</p>\n<p><strong>INSERT Statement Parsing</strong> must handle the standard INSERT INTO syntax with explicit column lists and corresponding value lists. The parser should support inserting into a subset of table columns, with the column list and value list in the INSERT statement specifying which columns receive values. Multiple-row inserts using a single INSERT statement should be supported, where multiple value tuples are provided after the VALUES keyword. For example, <code>INSERT INTO users (name, email) VALUES (&#39;Alice&#39;, &#39;alice@example.com&#39;), (&#39;Bob&#39;, &#39;bob@example.com&#39;)</code> should parse into an AST that captures both the column specification and the multiple value rows.</p>\n<p><strong>UPDATE and DELETE Statement Parsing</strong> rounds out our data modification language support. UPDATE statements must parse SET clauses that specify column assignments using equals signs, with support for multiple column assignments separated by commas. Both UPDATE and DELETE statements must support WHERE clauses using the same expression parsing logic developed for SELECT statements. For DELETE statements, the parser must handle the simple <code>DELETE FROM table WHERE condition</code> syntax. The parser should build AST nodes that clearly distinguish between the target table, the modification operations (for UPDATE), and the filtering conditions.</p>\n<p><strong>Error Detection and Reporting</strong> is a functional goal that spans all statement types. The parser must detect common syntax errors and report them with helpful error messages that include position information. When the parser encounters unexpected tokens, missing required keywords, or malformed expressions, it should provide error messages that help developers understand what went wrong and where. For example, if a user writes <code>SELECT name FROM WHERE age &gt; 18</code> (missing the table name), the parser should report an error indicating that a table name is expected after the FROM keyword.</p>\n<p>The following table summarizes the specific syntax elements that must be successfully parsed:</p>\n<table>\n<thead>\n<tr>\n<th>Statement Type</th>\n<th>Required Syntax Elements</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SELECT</td>\n<td>Column lists, star wildcard, table references, aliases</td>\n<td><code>SELECT id, name FROM users</code>, <code>SELECT * FROM orders AS o</code></td>\n</tr>\n<tr>\n<td>WHERE</td>\n<td>Comparison operators, logical operators, parentheses, literals</td>\n<td><code>WHERE age &gt; 18 AND status = &#39;active&#39;</code></td>\n</tr>\n<tr>\n<td>INSERT</td>\n<td>Column lists, value lists, multiple rows</td>\n<td><code>INSERT INTO users (name) VALUES (&#39;Alice&#39;), (&#39;Bob&#39;)</code></td>\n</tr>\n<tr>\n<td>UPDATE</td>\n<td>SET clauses, column assignments, WHERE conditions</td>\n<td><code>UPDATE users SET status = &#39;inactive&#39; WHERE age &lt; 13</code></td>\n</tr>\n<tr>\n<td>DELETE</td>\n<td>Table references, WHERE conditions</td>\n<td><code>DELETE FROM sessions WHERE expires &lt; &#39;2024-01-01&#39;</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"non-functional-goals\">Non-Functional Goals</h3>\n<p><strong>Parse Speed and Memory Efficiency</strong> are important non-functional requirements that affect the parser&#39;s usability in real applications. The parser should be able to handle typical SQL queries (those under 1000 characters) in under 10 milliseconds on modern hardware. While this isn&#39;t blazingly fast compared to production database parsers, it&#39;s sufficient for educational use, development tools, and small to medium applications. Memory usage should be proportional to query size, with the AST requiring roughly 10-50 times the memory of the original query string depending on the complexity of expressions and the number of nodes created.</p>\n<p>The tokenizer should process characters in a single pass without backtracking, ensuring that tokenization time grows linearly with input size. The recursive descent parser should avoid excessive function call depth by limiting expression nesting to reasonable levels (around 50 levels of parentheses nesting). These constraints ensure that the parser remains responsive even when processing moderately complex queries.</p>\n<p><strong>Extensibility and Maintainability</strong> form critical design requirements because this parser serves as an educational foundation that students and developers will modify and extend. The tokenizer design must make it straightforward to add new keywords by simply updating a keyword mapping table, without requiring changes to the core tokenization logic. Adding new operators should require only updates to operator precedence tables and the addition of corresponding token types.</p>\n<p>The AST node hierarchy must be designed for extension, with a clear base class or interface that new node types can inherit from or implement. Adding support for new statement types should follow a consistent pattern where developers implement a new parsing function following the same recursive descent approach used for existing statements. The parser should be modular enough that developers can test individual components (tokenizer, expression parser, statement parsers) in isolation.</p>\n<p><strong>Educational Value and Debuggability</strong> represent special non-functional requirements because this parser is designed for learning. The AST structure should be intuitive and easy to inspect, with node types and field names that clearly correspond to SQL concepts. When students print or debug the AST, they should be able to easily understand how their SQL query was interpreted by the parser.</p>\n<p>Error messages should be educational, explaining not just what was wrong but providing hints about correct syntax. For example, instead of reporting &quot;unexpected token,&quot; the parser might say &quot;expected table name after FROM keyword, but found WHERE.&quot; Debug output should be available to show the tokenization process and parsing decisions, helping students understand how the parser works internally.</p>\n<p><strong>Testing and Validation Support</strong> ensures that implementations can be verified for correctness. The parser design should facilitate comprehensive testing by making it easy to test individual components and verify specific aspects of the AST structure. The tokenizer output should be easily inspectable to verify that queries are being tokenized correctly. AST nodes should support equality comparison to enable straightforward assertion-based testing.</p>\n<p>The following table outlines our non-functional requirements with specific metrics:</p>\n<table>\n<thead>\n<tr>\n<th>Requirement Category</th>\n<th>Specific Goal</th>\n<th>Success Metric</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Performance</td>\n<td>Parse speed for typical queries</td>\n<td>Under 10ms for queries under 1000 characters</td>\n</tr>\n<tr>\n<td>Memory</td>\n<td>Proportional memory usage</td>\n<td>AST uses 10-50x the memory of query string</td>\n</tr>\n<tr>\n<td>Extensibility</td>\n<td>Easy keyword addition</td>\n<td>New keywords added by updating single mapping table</td>\n</tr>\n<tr>\n<td>Maintainability</td>\n<td>Modular component testing</td>\n<td>Each component (tokenizer, parser) testable in isolation</td>\n</tr>\n<tr>\n<td>Educational Value</td>\n<td>Intuitive AST structure</td>\n<td>AST node names directly correspond to SQL concepts</td>\n</tr>\n<tr>\n<td>Error Quality</td>\n<td>Helpful error messages</td>\n<td>Errors include position info and syntax hints</td>\n</tr>\n<tr>\n<td>Debugging</td>\n<td>Inspection capabilities</td>\n<td>AST and tokens easily printable for debugging</td>\n</tr>\n</tbody></table>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>Understanding what we are <strong>not</strong> building is just as important as understanding what we are building. These explicit non-goals help prevent scope creep and keep our implementation focused on the core learning objectives.</p>\n<p><strong>Advanced SQL Features</strong> are explicitly excluded from our parser scope. We will not support JOIN operations of any kind, including INNER JOIN, LEFT JOIN, RIGHT JOIN, or FULL OUTER JOIN. While JOINs are crucial in production SQL, they add significant complexity to both parsing and AST representation that would distract from learning the fundamental concepts of lexical analysis and recursive descent parsing. Supporting JOINs properly requires handling table correlation, join conditions, and the interaction between WHERE clauses and JOIN conditions.</p>\n<p>Subqueries and nested SELECT statements are not supported. Subqueries introduce recursive parsing challenges where SELECT statements can appear within expressions, requiring the parser to handle statement nesting and scope resolution. Window functions, CTEs (Common Table Expressions), and advanced SQL constructs like CASE expressions are similarly excluded. These features, while powerful, represent advanced SQL concepts that would significantly complicate our parser without proportional educational benefit for the core parsing concepts we&#39;re teaching.</p>\n<p><strong>Production-Quality Features</strong> are not included in our design goals. We are not building a parser that could be used in a production database system or SQL analysis tool. This means we explicitly exclude performance optimizations like parse result caching, incremental parsing, or advanced error recovery strategies that would allow parsing to continue after encountering errors.</p>\n<p>We do not support SQL dialects or vendor-specific extensions. Our parser targets a simplified, generic SQL syntax that captures the essential elements of SELECT, INSERT, UPDATE, and DELETE statements. We will not handle the subtle syntax differences between MySQL, PostgreSQL, SQL Server, or Oracle. Features like MySQL&#39;s LIMIT clause, PostgreSQL&#39;s RETURNING clause, or SQL Server&#39;s TOP clause are not supported.</p>\n<p><strong>Advanced Error Handling</strong> capabilities are intentionally limited. We will not implement sophisticated error recovery that attempts to continue parsing after encountering syntax errors to find additional problems in the same query. While production parsers often include such features to provide comprehensive error reporting, implementing robust error recovery is a complex topic that goes beyond our educational goals. Our parser will report the first error it encounters and stop parsing.</p>\n<p>We do not support error correction or &quot;did you mean&quot; suggestions. While these features can be helpful in development tools, they require significant additional complexity including similarity algorithms and extensive knowledge of valid SQL constructs. Our error messages will be helpful and descriptive, but they will not attempt to guess what the user intended to write.</p>\n<p><strong>Schema Validation and Semantic Analysis</strong> are completely outside our scope. Our parser builds a syntactic AST that represents the structure of the SQL query, but it does not validate that referenced tables exist, that column names are valid, or that data types are compatible. These semantic validation tasks require knowledge of database schema and type systems, which would require building a substantial metadata management system alongside our parser.</p>\n<p>We will not validate that INSERT statements specify values for all required columns, that UPDATE statements don&#39;t attempt to modify primary keys, or that WHERE clause comparisons use compatible data types. These are all important validations in a production system, but they belong to the semantic analysis phase that occurs after parsing.</p>\n<p><strong>Performance Optimization</strong> beyond basic efficiency is not a goal. We will not implement advanced parsing techniques like packrat parsing, parser combinators, or generated parsers from formal grammars. While these approaches have advantages in certain scenarios, they introduce complexity that would obscure the fundamental concepts of tokenization and recursive descent parsing that our project aims to teach.</p>\n<p>Memory optimization techniques like AST node pooling, lazy parsing, or compact AST representations are not included. Our AST will prioritize clarity and ease of understanding over memory efficiency. Similarly, we will not implement streaming parsing for very large queries or sophisticated tokenizer optimizations.</p>\n<p>The following table summarizes our explicit non-goals and the rationale for excluding them:</p>\n<table>\n<thead>\n<tr>\n<th>Excluded Feature Category</th>\n<th>Specific Examples</th>\n<th>Exclusion Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Advanced SQL</td>\n<td>JOINs, subqueries, window functions, CTEs</td>\n<td>Adds parsing complexity without educational benefit for core concepts</td>\n</tr>\n<tr>\n<td>Production Features</td>\n<td>Parse caching, incremental parsing, dialect support</td>\n<td>Beyond educational scope; would obscure fundamental techniques</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Continue parsing after errors, error correction</td>\n<td>Complex topic requiring extensive additional implementation</td>\n</tr>\n<tr>\n<td>Semantic Analysis</td>\n<td>Schema validation, type checking, constraint validation</td>\n<td>Requires metadata system; not a parsing concern</td>\n</tr>\n<tr>\n<td>Performance Optimization</td>\n<td>Advanced parsing algorithms, memory optimization</td>\n<td>Would obscure educational focus on basic recursive descent</td>\n</tr>\n<tr>\n<td>SQL Dialects</td>\n<td>MySQL LIMIT, PostgreSQL RETURNING, vendor extensions</td>\n<td>Scope limitation to keep grammar manageable</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: These non-goals are not limitations of the approach, but conscious decisions to maintain educational focus. A student who masters tokenization, recursive descent parsing, operator precedence, and AST construction using our simplified SQL grammar will have learned transferable skills that apply to parsing any language, not just SQL.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The goals and non-goals defined above have direct implications for how we structure our implementation and what technologies we choose. This section provides concrete guidance on translating these requirements into a working parser.</p>\n<p><strong>Technology Recommendations</strong></p>\n<p>For implementing our SQL parser with the defined scope, we recommend straightforward approaches that prioritize clarity and educational value over performance optimization or advanced features.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenizer</td>\n<td>Character-by-character scanning with string methods</td>\n<td>Regex-based tokenization</td>\n</tr>\n<tr>\n<td>Parser</td>\n<td>Hand-written recursive descent</td>\n<td>Parser generator (ANTLR, PLY)</td>\n</tr>\n<tr>\n<td>AST Representation</td>\n<td>Simple classes with inheritance</td>\n<td>Visitor pattern with node interfaces</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception-based with position tracking</td>\n<td>Error recovery with synchronization points</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Unit tests with direct assertions</td>\n<td>Property-based testing with query generation</td>\n</tr>\n</tbody></table>\n<p>For this educational project, we strongly recommend the simple options. Character-by-character scanning helps students understand how tokenizers work at a fundamental level. Hand-written recursive descent parsing makes the relationship between grammar rules and code explicit. Simple AST classes are easy to debug and understand.</p>\n<p><strong>Recommended Project Structure</strong></p>\n<p>Organizing the parser implementation across multiple modules helps maintain clear separation of concerns and makes the codebase easier to understand and test.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql-parser/\n├── sql_parser/\n│   ├── __init__.py              ← Main parser API\n│   ├── tokens.py                ← Token types and TokenType enum\n│   ├── tokenizer.py             ← Tokenizer class (Milestone 1)\n│   ├── ast_nodes.py             ← AST node class hierarchy\n│   ├── select_parser.py         ← SELECT statement parsing (Milestone 2)\n│   ├── expression_parser.py     ← WHERE clause expressions (Milestone 3)\n│   ├── dml_parser.py            ← INSERT/UPDATE/DELETE parsing (Milestone 4)\n│   └── exceptions.py            ← Parser exception classes\n├── tests/\n│   ├── test_tokenizer.py        ← Tokenizer tests\n│   ├── test_select_parser.py    ← SELECT parser tests\n│   ├── test_expressions.py      ← Expression parser tests\n│   ├── test_dml_parser.py       ← DML parser tests\n│   └── test_integration.py      ← End-to-end parser tests\n├── examples/\n│   ├── basic_usage.py           ← Simple parser usage examples\n│   └── query_samples/           ← Sample SQL files for testing\n└── README.md                    ← Project documentation</code></pre></div>\n\n<p>This structure separates each major component into its own module, making it easy to work on individual milestones independently. The test structure mirrors the implementation structure, encouraging comprehensive testing of each component.</p>\n<p><strong>Exception Hierarchy Setup</strong></p>\n<p>Based on our error handling goals, implement a clear exception hierarchy that provides good error messages with position information.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># exceptions.py - Complete exception hierarchy for the parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for all parser errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message, position</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, line</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, column</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Parse error at line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Parse error at position </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Parse error: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised during tokenization phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised during parsing phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UnexpectedTokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised when parser encounters unexpected token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expected, actual, position</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, line</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, column</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.actual </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, but found </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, position, line, column)</span></span></code></pre></div>\n\n<p><strong>Main Parser API Design</strong></p>\n<p>Create a simple, unified API that encapsulates the entire parsing process while maintaining access to intermediate steps for debugging and testing.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># __init__.py - Main parser API that ties everything together</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .select_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SelectParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .dml_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DMLParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SQLParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Main SQL parser that coordinates tokenization and parsing phases.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This class provides the primary interface for parsing SQL statements</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    and returning AST nodes that represent the query structure.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tokenizer()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.select_parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SelectParser()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.dml_parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DMLParser()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self, sql_text):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse a SQL statement and return the corresponding AST node.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            sql_text (str): The SQL statement to parse</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            AST node representing the parsed statement</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ParseError: If the SQL statement contains syntax errors</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Tokenize the input SQL text using self.tokenizer.tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Determine statement type from first token (SELECT, INSERT, UPDATE, DELETE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Delegate to appropriate parser based on statement type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return the resulting AST node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize_only</span><span style=\"color:#E1E4E8\">(self, sql_text):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tokenize SQL text without parsing (useful for debugging).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of Token objects</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokenizer.tokenize(sql_text)</span></span></code></pre></div>\n\n<p><strong>Milestone Validation Checkpoints</strong></p>\n<p>Each milestone should have clear checkpoints that verify the implementation is working correctly before moving to the next milestone.</p>\n<p><strong>Milestone 1 Checkpoint - Tokenizer:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test that tokenizer correctly handles basic SQL tokens</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SQLParser()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.tokenize_only(</span><span style=\"color:#9ECBFF\">\"SELECT name FROM users WHERE id = 42\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: 8 tokens with correct types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># TOKEN_SELECT, TOKEN_IDENTIFIER('name'), TOKEN_FROM, TOKEN_IDENTIFIER('users'),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># TOKEN_WHERE, TOKEN_IDENTIFIER('id'), TOKEN_EQUALS, TOKEN_NUMBER(42)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">SELECT</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].value </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"name\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">].value </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"42\"</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint - SELECT Parser:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test that SELECT statements parse into correct AST structure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(</span><span style=\"color:#9ECBFF\">\"SELECT id, name FROM users AS u\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: SelectStatement node with columns list and table reference</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(ast, SelectStatement)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.columns) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.columns[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].name </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"id\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.table.alias </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"u\"</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint - WHERE Expressions:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test complex WHERE clause with operator precedence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(</span><span style=\"color:#9ECBFF\">\"SELECT * FROM users WHERE age > 18 AND status = 'active' OR type = 'admin'\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Correct precedence with AND binding tighter than OR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(ast.where_clause, BinaryOperation)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.where_clause.operator </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"OR\"</span><span style=\"color:#6A737D\">  # Top-level operator</span></span></code></pre></div>\n\n<p><strong>Milestone 4 Checkpoint - DML Statements:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test INSERT, UPDATE, and DELETE parsing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">insert_ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(</span><span style=\"color:#9ECBFF\">\"INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com')\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">update_ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(</span><span style=\"color:#9ECBFF\">\"UPDATE users SET status = 'inactive' WHERE age &#x3C; 13\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">delete_ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(</span><span style=\"color:#9ECBFF\">\"DELETE FROM sessions WHERE expires &#x3C; '2024-01-01'\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(insert_ast, InsertStatement)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(update_ast, UpdateStatement) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(delete_ast, DeleteStatement)</span></span></code></pre></div>\n\n<p><strong>Common Implementation Pitfalls</strong></p>\n<p>⚠️ <strong>Pitfall: Overly Complex Initial Design</strong>\nMany students try to implement all features at once or design overly flexible AST nodes that can handle any possible SQL construct. Start simple and extend gradually. Implement the tokenizer completely for basic tokens before adding complex operators or string escape sequences.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Position Tracking</strong>\nFailing to track line and column positions during tokenization makes debugging very difficult. Include position information in every token from the start - it&#39;s much harder to add later.</p>\n<p>⚠️ <strong>Pitfall: Mixing Parsing Phases</strong>\nDon&#39;t try to do semantic validation (checking if tables exist) during syntax parsing. Keep the parser focused on syntax and structure. Semantic analysis can be added as a separate phase later.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Error Messages</strong>\nGeneric error messages like &quot;syntax error&quot; provide little help. Include what was expected, what was found, and position information in every error message.</p>\n<p><strong>Debugging Strategy</strong></p>\n<p>When implementing the parser, use a systematic debugging approach:</p>\n<ol>\n<li><p><strong>Debug tokenization first</strong>: Always verify that your input is being tokenized correctly before debugging parser logic. Print token sequences to ensure they match expectations.</p>\n</li>\n<li><p><strong>Test individual components</strong>: Test the expression parser independently before integrating it with statement parsing. Test each statement type independently.</p>\n</li>\n<li><p><strong>Use small test cases</strong>: Start with the simplest possible examples (<code>SELECT * FROM users</code>) before attempting complex queries.</p>\n</li>\n<li><p><strong>Print AST structures</strong>: Implement <code>__str__</code> or <code>__repr__</code> methods on AST nodes to make debugging easier. You should be able to print the AST and understand the structure visually.</p>\n</li>\n</ol>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the foundational architecture for all milestones (1-4), defining the core components and their relationships that will be implemented throughout the project.</p>\n</blockquote>\n<h3 id=\"component-overview\">Component Overview</h3>\n<p>Think of the SQL parser as a <strong>language translation service</strong> operating in a document processing office. Raw SQL text arrives like a foreign document that needs to be understood and translated into a structured format that computers can work with effectively. Just as a translation service has specialists for different stages—someone who identifies individual words and their types, someone who understands grammar rules and sentence structure, and someone who creates the final structured document—our SQL parser has three specialized components that work together in sequence.</p>\n<p>The <strong>Tokenizer</strong> acts as the lexical analyst who examines the raw SQL text character by character and identifies distinct words, symbols, and punctuation marks. It&#39;s responsible for recognizing that <code>SELECT</code> is a keyword, <code>user_id</code> is an identifier, <code>&#39;John&#39;</code> is a string literal, and <code>=</code> is an operator. This component transforms the continuous stream of characters into a sequence of classified tokens, each tagged with its type and position information for error reporting.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Fsystem-components.svg\" alt=\"SQL Parser System Components\"></p>\n<p>The <strong>Parser</strong> functions as the grammar expert who understands SQL&#39;s syntactic rules and constructs a meaningful hierarchical representation of the query. It consumes the token sequence from the tokenizer and applies recursive descent parsing techniques to build an Abstract Syntax Tree that captures both the structure and semantics of the SQL statement. The parser knows that after a <code>SELECT</code> keyword, it should expect a column list, and after a <code>FROM</code> keyword, it should find table references.</p>\n<p>The <strong>AST Node</strong> hierarchy represents the structured output—a tree-like data structure where each node encapsulates a specific SQL construct with its associated data and relationships. These nodes serve as the final parsed representation that can be easily traversed, analyzed, and transformed by other systems such as query optimizers or execution engines.</p>\n<p>The three main components work with distinct responsibilities and interfaces:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Primary Responsibility</th>\n<th>Input Format</th>\n<th>Output Format</th>\n<th>Key Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Tokenizer</code></td>\n<td>Lexical analysis and token classification</td>\n<td>Raw SQL string</td>\n<td>Sequence of <code>Token</code> objects</td>\n<td>Character scanning, keyword recognition, literal parsing</td>\n</tr>\n<tr>\n<td><code>SQLParser</code></td>\n<td>Syntactic analysis and AST construction</td>\n<td>Token sequence</td>\n<td><code>ASTNode</code> tree structure</td>\n<td>Recursive descent parsing, precedence handling, error recovery</td>\n</tr>\n<tr>\n<td>AST Nodes</td>\n<td>Structured representation of parsed SQL</td>\n<td>Parser method calls</td>\n<td>Immutable tree nodes</td>\n<td>Node creation, property access, tree traversal</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Principle</strong>: Each component operates independently with clear interfaces, enabling isolated testing and future extensibility. The tokenizer never needs to understand SQL grammar rules, and the parser never manipulates raw character data.</p>\n</blockquote>\n<p>The <strong>Tokenizer</strong> component encapsulates all lexical analysis logic within a single class that maintains scanning state and provides token generation methods. It recognizes SQL keywords through case-insensitive lookup tables, handles string literals with proper escape sequence processing, and identifies numeric literals with appropriate type classification. The tokenizer maintains position tracking for detailed error reporting and supports lookahead operations for complex token disambiguation.</p>\n<p>The <strong>SQLParser</strong> component serves as the main parsing coordinator that orchestrates statement-specific parsers and manages the overall parsing process. It delegates to specialized parsers like <code>SelectParser</code> and <code>DMLParser</code> based on the statement type detected from the leading keywords. This component handles error recovery, maintains parsing context, and provides the public API that client code uses to convert SQL strings into AST representations.</p>\n<p>The <strong>AST Node hierarchy</strong> consists of abstract base classes and concrete implementations for each SQL construct type. Statement nodes like <code>SelectStatement</code> and <code>InsertStatement</code> represent complete SQL statements, while expression nodes like <code>BinaryOperation</code> and <code>Literal</code> represent components within those statements. Each node type encapsulates its specific properties and provides methods for tree traversal and property access.</p>\n<h3 id=\"data-flow-pipeline\">Data Flow Pipeline</h3>\n<p>The SQL parsing process follows a clear linear pipeline where data transforms through distinct stages, similar to an assembly line in manufacturing. Raw SQL text enters at one end and emerges as a structured AST at the other end, with each stage adding a layer of understanding and organization to the data.</p>\n<p><strong>Stage 1: Character-Level Scanning</strong>\nThe process begins when raw SQL text enters the tokenizer as a simple string. The tokenizer initializes its scanning state with a character pointer positioned at the beginning of the input and begins systematic character-by-character analysis. It maintains several pieces of state information including current position, line number, column number, and accumulation buffers for building multi-character tokens.</p>\n<p>The character scanning process follows a state-machine approach where the tokenizer&#39;s behavior depends on its current state and the character being examined. In the normal scanning state, encountering alphabetic characters triggers identifier or keyword recognition logic, while numeric characters initiate number parsing, and quote characters begin string literal processing. Whitespace characters are consumed and discarded, while operator characters are immediately classified and converted to tokens.</p>\n<p><strong>Stage 2: Token Classification and Generation</strong>\nAs the tokenizer identifies complete lexical units, it creates <code>Token</code> objects that encapsulate both the token&#39;s content and its classification. Each token contains the original text value, its classified type from the <code>TokenType</code> enumeration, and precise position information including line and column numbers. This position information becomes crucial for generating helpful error messages when parsing fails.</p>\n<p>The token generation process involves several classification decisions. When the tokenizer encounters alphabetic character sequences, it must determine whether the sequence represents a SQL keyword like <code>SELECT</code> or <code>WHERE</code>, or a user-defined identifier like a table or column name. This classification uses case-insensitive lookup in the <code>SQL_KEYWORDS</code> mapping, with unrecognized sequences defaulting to identifier tokens.</p>\n<p>String literals require more complex processing as the tokenizer must handle quote character variations, escape sequences, and embedded quotes. The tokenizer tracks the opening quote character and scans until it finds the matching closing quote, processing escape sequences like <code>\\&#39;</code> and <code>\\&quot;</code> along the way. Numeric literals are classified as either integer or floating-point based on the presence of decimal points.</p>\n<p><strong>Stage 3: Syntactic Analysis and Tree Construction</strong>\nThe parser receives the complete token sequence and begins syntactic analysis using recursive descent parsing techniques. The parsing process starts by examining the first token to determine the statement type—<code>SELECT</code> tokens trigger SELECT statement parsing, while <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> tokens activate the appropriate data modification statement parsers.</p>\n<p>Recursive descent parsing mirrors the hierarchical structure of SQL grammar rules. Each grammar rule corresponds to a parsing method that consumes tokens and constructs appropriate AST nodes. For example, the <code>parse_select_statement</code> method calls <code>parse_select_clause</code> to handle the column list, then <code>parse_from_clause</code> to handle table references, and optionally <code>parse_where_clause</code> for filtering conditions.</p>\n<p>The parsing process maintains a current token pointer and advances through the sequence as it consumes tokens. When a parsing method recognizes the expected token pattern, it creates the corresponding AST node and populates it with the parsed information. Methods use lookahead to make parsing decisions—examining upcoming tokens without consuming them to determine which parsing path to follow.</p>\n<p><strong>Stage 4: AST Construction and Validation</strong>\nAs parsing methods complete their token consumption, they construct and return AST nodes that represent the parsed SQL constructs. These nodes form a tree structure that mirrors the hierarchical nature of SQL statements. A <code>SelectStatement</code> node contains references to its constituent parts: a column list, table references, and optional WHERE conditions.</p>\n<p>The AST construction process validates syntactic correctness as it proceeds. If the parser encounters unexpected tokens or malformed syntax, it generates specific <code>SyntaxError</code> exceptions that include position information and helpful error messages. The parser can also perform basic semantic validation, such as ensuring that column and value counts match in INSERT statements.</p>\n<p><strong>Data Flow Summary</strong></p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n<th>Error Types</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Character Scanning</td>\n<td>Raw SQL string</td>\n<td>State machine character analysis</td>\n<td>Character classifications</td>\n<td><code>TokenizerError</code> for malformed literals</td>\n</tr>\n<tr>\n<td>Token Generation</td>\n<td>Character stream</td>\n<td>Lexical analysis and classification</td>\n<td><code>Token</code> sequence</td>\n<td><code>TokenizerError</code> for invalid syntax</td>\n</tr>\n<tr>\n<td>Syntactic Parsing</td>\n<td>Token sequence</td>\n<td>Recursive descent parsing</td>\n<td>Partial AST nodes</td>\n<td><code>SyntaxError</code> for grammar violations</td>\n</tr>\n<tr>\n<td>AST Construction</td>\n<td>Parsed components</td>\n<td>Node creation and linking</td>\n<td>Complete AST tree</td>\n<td><code>UnexpectedTokenError</code> for token mismatches</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The pipeline stages are loosely coupled through well-defined data structures (<code>Token</code> and <code>ASTNode</code>), enabling independent testing and future enhancements. Each stage can be developed and debugged in isolation.</p>\n</blockquote>\n<h3 id=\"recommended-file-structure\">Recommended File Structure</h3>\n<p>The SQL parser implementation benefits from a modular file organization that mirrors the logical component separation and supports both development workflow and testing strategies. The structure follows Python package conventions while grouping related functionality and separating public interfaces from internal implementation details.</p>\n<p>The recommended organization separates concerns across multiple dimensions: component responsibility (tokenizer vs parser vs AST), statement types (SELECT vs DML), and interface visibility (public API vs internal implementation). This structure supports incremental development where each milestone can be implemented and tested independently.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql_parser/                           # Main package directory\n    __init__.py                       # Public API exports\n    exceptions.py                     # Error classes and exception hierarchy\n    \n    tokenizer/                        # Tokenization component (Milestone 1)\n        __init__.py                   # Tokenizer public interface\n        tokenizer.py                  # Main Tokenizer class implementation\n        token_types.py                # TokenType enum and Token dataclass\n        keywords.py                   # SQL_KEYWORDS mapping and constants\n    \n    parser/                           # Parsing components (Milestones 2-4)\n        __init__.py                   # Parser public interfaces\n        base_parser.py                # Common parsing utilities and base classes\n        sql_parser.py                 # Main SQLParser class and entry points\n        select_parser.py              # SelectParser for SELECT statements (Milestone 2)\n        expression_parser.py          # WHERE clause and expression parsing (Milestone 3)\n        dml_parser.py                 # DMLParser for INSERT/UPDATE/DELETE (Milestone 4)\n    \n    ast/                              # AST node definitions\n        __init__.py                   # AST node exports\n        base_nodes.py                 # Abstract base classes and common interfaces\n        statement_nodes.py            # Statement AST nodes (SelectStatement, etc.)\n        expression_nodes.py           # Expression AST nodes (BinaryOperation, etc.)\n    \n    tests/                            # Test suite organization\n        test_tokenizer.py             # Tokenizer unit tests (Milestone 1)\n        test_select_parser.py         # SELECT parsing tests (Milestone 2)\n        test_expression_parser.py     # Expression parsing tests (Milestone 3)\n        test_dml_parser.py            # DML parsing tests (Milestone 4)\n        test_integration.py           # End-to-end parser tests\n        fixtures/                     # Test SQL files and expected outputs\n            valid_queries.sql         # Valid SQL test cases\n            invalid_queries.sql       # Error condition test cases\n            expected_asts.json        # Expected AST outputs for validation</code></pre></div>\n\n<p><strong>Component Isolation Benefits</strong>\nThe tokenizer directory contains all lexical analysis logic, enabling independent development and testing of character-level processing without concerning parser logic. The <code>token_types.py</code> file centralizes all token definitions, making it easy to add new token types as SQL support expands. The <code>keywords.py</code> file isolates the SQL keyword recognition logic, supporting case-insensitive matching and future keyword additions.</p>\n<p>The parser directory separates different parsing concerns into focused modules. The <code>base_parser.py</code> file contains common utilities used by all parser types, such as token consumption methods, error handling helpers, and lookahead operations. Statement-specific parsers like <code>select_parser.py</code> and <code>dml_parser.py</code> can focus on their particular grammar rules without duplicating common functionality.</p>\n<p><strong>Testing Strategy Support</strong>\nThe test organization enables both component-level unit testing and integration testing. Each parser component has its dedicated test file, allowing milestone-by-milestone validation. The fixtures directory provides a centralized location for test SQL queries and expected outputs, supporting both positive and negative test cases.</p>\n<p>The structure supports test-driven development where developers can implement and validate each component independently. For example, <code>test_tokenizer.py</code> can thoroughly exercise tokenization logic using only the tokenizer component, without requiring parser implementation.</p>\n<p><strong>API Design and Imports</strong>\nThe <code>__init__.py</code> files control the public API surface and support clean import statements. Client code can import the main functionality with simple statements like <code>from sql_parser import SQLParser</code> or <code>from sql_parser.tokenizer import Tokenizer</code>. Internal implementation details remain hidden, enabling future refactoring without breaking client code.</p>\n<p><strong>Development Workflow Support</strong></p>\n<table>\n<thead>\n<tr>\n<th>Milestone</th>\n<th>Primary Files</th>\n<th>Testing Files</th>\n<th>Validation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1: Tokenizer</td>\n<td><code>tokenizer/tokenizer.py</code>, <code>tokenizer/token_types.py</code></td>\n<td><code>test_tokenizer.py</code></td>\n<td>Token sequence validation</td>\n</tr>\n<tr>\n<td>2: SELECT</td>\n<td><code>parser/select_parser.py</code>, <code>ast/statement_nodes.py</code></td>\n<td><code>test_select_parser.py</code></td>\n<td>AST structure validation</td>\n</tr>\n<tr>\n<td>3: WHERE</td>\n<td><code>parser/expression_parser.py</code>, <code>ast/expression_nodes.py</code></td>\n<td><code>test_expression_parser.py</code></td>\n<td>Expression tree validation</td>\n</tr>\n<tr>\n<td>4: DML</td>\n<td><code>parser/dml_parser.py</code></td>\n<td><code>test_dml_parser.py</code></td>\n<td>Statement parsing validation</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Implementation Strategy</strong>: Begin implementation with the tokenizer component, then build parsers incrementally. Each milestone should achieve working functionality for its specific SQL features before proceeding to the next milestone.</p>\n</blockquote>\n<p>The file structure supports future extensibility by isolating concerns and providing clear extension points. New statement types can be added with new parser modules, new expression types can be added to the expression parser, and new token types can be added to the tokenizer without affecting existing functionality.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenizer</td>\n<td>Character-by-character scanning with string methods</td>\n<td>RegEx-based tokenization with <code>re</code> module</td>\n</tr>\n<tr>\n<td>Parser</td>\n<td>Recursive descent with manual token management</td>\n<td>Parser generator with grammar files (PLY, ANTLR)</td>\n</tr>\n<tr>\n<td>AST Nodes</td>\n<td>Simple dataclasses with basic properties</td>\n<td>Rich node classes with visitor pattern support</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception raising with basic messages</td>\n<td>Structured error reporting with position tracking</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Manual test cases with assertions</td>\n<td>Property-based testing with Hypothesis</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># sql_parser/__init__.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"SQL Parser - Main package entry point.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .parser.sql_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SQLParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .tokenizer.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseError, TokenizerError, </span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">, UnexpectedTokenError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Public API - what client code should import</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">__all__</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'SQLParser'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Tokenizer'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'ParseError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'TokenizerError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'SyntaxError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'UnexpectedTokenError'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Convenience method for most common use case</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Parse SQL text and return AST. Main entry point for client code.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SQLParser()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> parser.parse(sql_text)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> tokenize_only</span><span style=\"color:#E1E4E8\">(sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tokenize SQL text without parsing. Useful for debugging tokenization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tokenizer()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tokenizer.tokenize(sql_text)</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># sql_parser/exceptions.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Exception classes for SQL parser errors.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all SQL parsing errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._format_message())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_message</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.message</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors during tokenization (lexical analysis).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors during parsing (syntactic analysis).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UnexpectedTokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Specific error for unexpected token encounters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, actual: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.actual </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, but found </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, line, column)</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># sql_parser/tokenizer/token_types.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Token type definitions and Token dataclass.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Keywords</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SELECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FROM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WHERE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INSERT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UPDATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DELETE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTO</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VALUES</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AND</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NULL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Identifiers and literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTEGER_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FLOAT_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()          </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()      </span><span style=\"color:#6A737D\"># != or &#x3C;></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()       </span><span style=\"color:#6A737D\"># &#x3C;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()    </span><span style=\"color:#6A737D\"># ></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()      </span><span style=\"color:#6A737D\"># &#x3C;=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># >=</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()           </span><span style=\"color:#6A737D\"># ,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEMICOLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()       </span><span style=\"color:#6A737D\"># ;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LEFT_PAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()      </span><span style=\"color:#6A737D\"># (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RIGHT_PAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()     </span><span style=\"color:#6A737D\"># )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STAR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()            </span><span style=\"color:#6A737D\"># *</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Special</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()             </span><span style=\"color:#6A737D\"># End of input</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()         </span><span style=\"color:#6A737D\"># Unrecognized token</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single token from SQL lexical analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, '</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">', </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span></code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># sql_parser/tokenizer/keywords.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"SQL keyword recognition and mapping.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .token_types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Case-insensitive keyword mapping</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SQL_KEYWORDS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'SELECT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SELECT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'FROM'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">FROM</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'WHERE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">WHERE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'INSERT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">INSERT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'UPDATE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">UPDATE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'DELETE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">DELETE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'INTO'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">INTO</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'VALUES'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">VALUES</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'SET'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'AS'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">AS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'AND'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">AND</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'OR'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'NOT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">NOT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'NULL'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'IS'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">IS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> is_keyword</span><span style=\"color:#E1E4E8\">(text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check if text is a SQL keyword (case-insensitive).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> text.upper() </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> SQL_KEYWORDS</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_keyword_token_type</span><span style=\"color:#E1E4E8\">(text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> TokenType:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Get TokenType for keyword, or None if not a keyword.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> SQL_KEYWORDS</span><span style=\"color:#E1E4E8\">.get(text.upper())</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># sql_parser/tokenizer/tokenizer.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Main tokenizer implementation - STUDENT IMPLEMENTS THIS.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .token_types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, TokenType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .keywords </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> get_keyword_token_type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenizerError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Tokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Converts SQL text into sequence of tokens through lexical analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.text </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self, text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main entry point - tokenize SQL text into list of tokens.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Initialize tokenizer state (text, position, line, column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Create empty token list for results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Loop while not at end of text:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">               - Skip whitespace and update position tracking</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">               - Identify next token type based on current character</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">               - Call appropriate token parsing method</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">               - Add resulting token to list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Add EOF token to mark end of input</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Return complete token list</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _current_char</span><span style=\"color:#E1E4E8\">(self) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return current character or None if at end of input.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Check if position is within text bounds, return char or None</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _peek_char</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look ahead at character without advancing position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Calculate peek position, check bounds, return char or None</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _advance</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Move to next character and update line/column tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Check for newline character and update line/column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Increment position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Update column (increment or reset to 1 for newlines)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_whitespace</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Skip whitespace characters and update position tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Loop while current char is whitespace, call _advance()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _read_string_literal</span><span style=\"color:#E1E4E8\">(self, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse quoted string literal handling escape sequences.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            quote_char: Opening quote character (' or \")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Store starting position for token creation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Advance past opening quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Build string content, handling escape sequences</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Look for matching closing quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Create and return STRING_LITERAL token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 6: Raise TokenizerError if unterminated string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>str.isalpha()</code>, <code>str.isdigit()</code>, and <code>str.isalnum()</code> for character classification</li>\n<li>Use <code>str.upper()</code> for case-insensitive keyword matching  </li>\n<li>Use <code>enumerate()</code> when you need both index and character in loops</li>\n<li>Use <code>dataclasses</code> for Token and AST node definitions - they provide <code>__init__</code>, <code>__repr__</code> automatically</li>\n<li>Use <code>typing.Optional[T]</code> for values that might be None (like end-of-input scenarios)</li>\n<li>Use <code>typing.List[T]</code> and <code>typing.Dict[K,V]</code> for type hints on collections</li>\n<li>Python&#39;s <code>in</code> operator works efficiently with sets and dictionaries for keyword lookup</li>\n<li>Use f-strings for error message formatting: <code>f&quot;Expected {expected}, got {actual}&quot;</code></li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the tokenizer (Milestone 1), verify with these tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Quick validation script</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sql_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tokenize_only</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 1: Basic SELECT statement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenize_only(</span><span style=\"color:#9ECBFF\">\"SELECT id, name FROM users WHERE age > 25\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">expected_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [TokenType.</span><span style=\"color:#79B8FF\">SELECT</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">COMMA</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">FROM</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  TokenType.</span><span style=\"color:#79B8FF\">WHERE</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  TokenType.</span><span style=\"color:#79B8FF\">INTEGER_LITERAL</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify token count and types match</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected_types)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i, (token, expected_type) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(tokens, expected_types)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_type, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Basic tokenization working\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 2: String literals and operators</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenize_only(</span><span style=\"color:#9ECBFF\">\"INSERT INTO users (name) VALUES ('John O</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'Connor')\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">string_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING_LITERAL</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> string_token.value </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"John O'Connor\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"String parsing failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">string_token.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ String literal parsing working\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>G. Common Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Forgetting Position Tracking</strong>\nMany students focus on token recognition but forget to maintain accurate line and column numbers. Without position tracking, error messages become useless (&quot;Syntax error somewhere in your query&quot;). Always increment column for each character and reset to 1 when encountering newlines.</p>\n<p>⚠️ <strong>Pitfall: Case Sensitivity in Keywords</strong> \nSQL keywords are case-insensitive, but Python string comparison is case-sensitive. Always use <code>.upper()</code> when checking against your keyword dictionary: <code>SQL_KEYWORDS.get(text.upper())</code>.</p>\n<p>⚠️ <strong>Pitfall: String Literal Escape Sequences</strong>\nDon&#39;t forget to handle escape sequences in string literals. SQL supports <code>\\&#39;</code>, <code>\\&quot;</code>, and <code>\\\\</code>. Process these during tokenization, not later in parsing.</p>\n<p>⚠️ <strong>Pitfall: Multi-Character Operators</strong>\nOperators like <code>&lt;=</code>, <code>&gt;=</code>, <code>!=</code>, and <code>&lt;&gt;</code> require lookahead. Don&#39;t just tokenize <code>&lt;</code> and <code>=</code> separately - peek ahead to see if they form a compound operator.</p>\n<h2 id=\"data-model-and-ast-design\">Data Model and AST Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the foundational data structures for all milestones (1-4), defining the token types that milestone 1 produces and the AST node hierarchy that milestones 2-4 construct.</p>\n</blockquote>\n<h3 id=\"mental-model-language-structure-blueprint\">Mental Model: Language Structure Blueprint</h3>\n<p>Think of parsing SQL like understanding the grammatical structure of a sentence in human language. When you read &quot;The quick brown fox jumps over the lazy dog,&quot; your mind automatically breaks this into parts: articles, adjectives, nouns, verbs, and prepositions. You then organize these parts into a hierarchical structure - subject, predicate, object - that captures the meaning and relationships.</p>\n<p>Our SQL parser works similarly. First, the tokenizer acts like breaking a sentence into individual words and punctuation marks, identifying each piece&#39;s role (is &quot;SELECT&quot; a keyword? is &quot;users&quot; an identifier?). Then the parser acts like a grammar analyzer, taking these categorized pieces and organizing them into a tree structure that captures the SQL statement&#39;s meaning and execution intent.</p>\n<p>The <strong>tokens</strong> are like labeled word cards - each piece of the original SQL text tagged with its grammatical role. The <strong>Abstract Syntax Tree (AST)</strong> is like a sentence diagram - a hierarchical structure that shows how these pieces relate to each other and what the overall statement means. Just as a sentence diagram helps you understand &quot;who did what to whom,&quot; our AST helps the database understand &quot;select what from where with which conditions.&quot;</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Ftoken-types.svg\" alt=\"Token Type Classification\"></p>\n<h3 id=\"token-type-definitions\">Token Type Definitions</h3>\n<p>The tokenizer must categorize every meaningful piece of text in a SQL statement. Our token classification system divides the SQL language into distinct lexical categories, each serving a specific grammatical purpose in SQL statements. The <code>TokenType</code> enumeration defines all possible token categories our parser recognizes.</p>\n<p><strong>Keywords</strong> represent SQL&#39;s reserved vocabulary - the fundamental commands and clauses that define statement structure and behavior. Unlike human languages where context often determines meaning, SQL keywords have fixed, unambiguous meanings that cannot be redefined by users.</p>\n<p><strong>Identifiers</strong> represent user-defined names within the database schema - table names, column names, and aliases that users create to organize their data. These tokens must be distinguished from keywords, even when they might spell the same characters in different cases.</p>\n<p><strong>Literals</strong> represent concrete values embedded directly in the SQL text - numbers, strings, and special constants like NULL that specify actual data rather than references to stored data.</p>\n<p><strong>Operators</strong> represent the mathematical, logical, and comparison operations that can be performed on data - the computational vocabulary that transforms and filters information.</p>\n<p><strong>Punctuation</strong> provides the structural grammar that separates and groups other tokens - commas that separate list items, parentheses that group expressions, and semicolons that terminate statements.</p>\n<table>\n<thead>\n<tr>\n<th>Token Type</th>\n<th>Purpose</th>\n<th>Examples</th>\n<th>Recognition Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>KEYWORD_SELECT</code></td>\n<td>Query projection clause</td>\n<td><code>SELECT</code>, <code>select</code>, <code>Select</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_FROM</code></td>\n<td>Table specification clause</td>\n<td><code>FROM</code>, <code>from</code>, <code>From</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_WHERE</code></td>\n<td>Condition filtering clause</td>\n<td><code>WHERE</code>, <code>where</code>, <code>Where</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_INSERT</code></td>\n<td>Data insertion command</td>\n<td><code>INSERT</code>, <code>insert</code>, <code>Insert</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_UPDATE</code></td>\n<td>Data modification command</td>\n<td><code>UPDATE</code>, <code>update</code>, <code>Update</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_DELETE</code></td>\n<td>Data removal command</td>\n<td><code>DELETE</code>, <code>delete</code>, <code>Delete</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_INTO</code></td>\n<td>Insertion target specification</td>\n<td><code>INTO</code>, <code>into</code>, <code>Into</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_VALUES</code></td>\n<td>Literal value list indicator</td>\n<td><code>VALUES</code>, <code>values</code>, <code>Values</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_SET</code></td>\n<td>Assignment operation indicator</td>\n<td><code>SET</code>, <code>set</code>, <code>Set</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_AND</code></td>\n<td>Logical conjunction operator</td>\n<td><code>AND</code>, <code>and</code>, <code>And</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_OR</code></td>\n<td>Logical disjunction operator</td>\n<td><code>OR</code>, <code>or</code>, <code>Or</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_NOT</code></td>\n<td>Logical negation operator</td>\n<td><code>NOT</code>, <code>not</code>, <code>Not</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_NULL</code></td>\n<td>Null value literal</td>\n<td><code>NULL</code>, <code>null</code>, <code>Null</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_IS</code></td>\n<td>Null comparison operator</td>\n<td><code>IS</code>, <code>is</code>, <code>Is</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>KEYWORD_AS</code></td>\n<td>Alias definition indicator</td>\n<td><code>AS</code>, <code>as</code>, <code>As</code></td>\n<td>Case-insensitive match against keyword dictionary</td>\n</tr>\n<tr>\n<td><code>IDENTIFIER</code></td>\n<td>User-defined names</td>\n<td><code>users</code>, <code>first_name</code>, <code>u1</code></td>\n<td>Starts with letter/underscore, contains alphanumeric/underscore</td>\n</tr>\n<tr>\n<td><code>STRING_LITERAL</code></td>\n<td>Text values</td>\n<td><code>&#39;John&#39;</code>, <code>&quot;Hello&quot;</code>, <code>&#39;Don&#39;&#39;t&#39;</code></td>\n<td>Single or double quotes with escape sequence support</td>\n</tr>\n<tr>\n<td><code>INTEGER_LITERAL</code></td>\n<td>Whole number values</td>\n<td><code>42</code>, <code>0</code>, <code>-123</code>, <code>999999</code></td>\n<td>Optional minus sign followed by digits</td>\n</tr>\n<tr>\n<td><code>FLOAT_LITERAL</code></td>\n<td>Decimal number values</td>\n<td><code>3.14</code>, <code>-0.5</code>, <code>123.456</code></td>\n<td>Integer pattern with decimal point and fractional digits</td>\n</tr>\n<tr>\n<td><code>OPERATOR_EQUALS</code></td>\n<td>Equality comparison</td>\n<td><code>=</code></td>\n<td>Single equals character</td>\n</tr>\n<tr>\n<td><code>OPERATOR_NOT_EQUALS</code></td>\n<td>Inequality comparison</td>\n<td><code>!=</code>, <code>&lt;&gt;</code></td>\n<td>Exclamation-equals or angle bracket pair</td>\n</tr>\n<tr>\n<td><code>OPERATOR_LESS_THAN</code></td>\n<td>Magnitude comparison</td>\n<td><code>&lt;</code></td>\n<td>Single less-than character</td>\n</tr>\n<tr>\n<td><code>OPERATOR_GREATER_THAN</code></td>\n<td>Magnitude comparison</td>\n<td><code>&gt;</code></td>\n<td>Single greater-than character</td>\n</tr>\n<tr>\n<td><code>OPERATOR_LESS_EQUAL</code></td>\n<td>Inclusive magnitude comparison</td>\n<td><code>&lt;=</code></td>\n<td>Less-than followed by equals</td>\n</tr>\n<tr>\n<td><code>OPERATOR_GREATER_EQUAL</code></td>\n<td>Inclusive magnitude comparison</td>\n<td><code>&gt;=</code></td>\n<td>Greater-than followed by equals</td>\n</tr>\n<tr>\n<td><code>OPERATOR_PLUS</code></td>\n<td>Addition operator</td>\n<td><code>+</code></td>\n<td>Single plus character</td>\n</tr>\n<tr>\n<td><code>OPERATOR_MINUS</code></td>\n<td>Subtraction operator</td>\n<td><code>-</code></td>\n<td>Single minus character</td>\n</tr>\n<tr>\n<td><code>OPERATOR_MULTIPLY</code></td>\n<td>Multiplication operator</td>\n<td><code>*</code></td>\n<td>Single asterisk character</td>\n</tr>\n<tr>\n<td><code>OPERATOR_DIVIDE</code></td>\n<td>Division operator</td>\n<td><code>/</code></td>\n<td>Single forward slash character</td>\n</tr>\n<tr>\n<td><code>PUNCTUATION_COMMA</code></td>\n<td>List item separator</td>\n<td><code>,</code></td>\n<td>Single comma character</td>\n</tr>\n<tr>\n<td><code>PUNCTUATION_SEMICOLON</code></td>\n<td>Statement terminator</td>\n<td><code>;</code></td>\n<td>Single semicolon character</td>\n</tr>\n<tr>\n<td><code>PUNCTUATION_LEFT_PAREN</code></td>\n<td>Expression grouping start</td>\n<td><code>(</code></td>\n<td>Single left parenthesis character</td>\n</tr>\n<tr>\n<td><code>PUNCTUATION_RIGHT_PAREN</code></td>\n<td>Expression grouping end</td>\n<td><code>)</code></td>\n<td>Single right parenthesis character</td>\n</tr>\n<tr>\n<td><code>PUNCTUATION_DOT</code></td>\n<td>Qualified name separator</td>\n<td><code>.</code></td>\n<td>Single period character</td>\n</tr>\n<tr>\n<td><code>WHITESPACE</code></td>\n<td>Token separation</td>\n<td><code> </code>, <code>\\t</code>, <code>\\n</code>, <code>\\r</code></td>\n<td>Spaces, tabs, newlines, carriage returns</td>\n</tr>\n<tr>\n<td><code>EOF</code></td>\n<td>End of input marker</td>\n<td>(implicit)</td>\n<td>Reached end of input string</td>\n</tr>\n</tbody></table>\n<p>The <code>Token</code> data structure encapsulates each recognized lexical unit along with essential metadata for error reporting and debugging. Position information enables the parser to provide meaningful error messages that pinpoint exactly where problems occur in the original SQL text.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>type</code></td>\n<td><code>TokenType</code></td>\n<td>The grammatical category this token belongs to</td>\n</tr>\n<tr>\n<td><code>value</code></td>\n<td><code>str</code></td>\n<td>The original text from the SQL input that this token represents</td>\n</tr>\n<tr>\n<td><code>line</code></td>\n<td><code>int</code></td>\n<td>Line number where this token appears (1-based for human readability)</td>\n</tr>\n<tr>\n<td><code>column</code></td>\n<td><code>int</code></td>\n<td>Column position where this token starts (1-based for human readability)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The position tracking in tokens serves two critical purposes beyond simple error reporting. First, it enables sophisticated IDE features like syntax highlighting and auto-completion by providing precise location information. Second, it supports advanced error recovery techniques where the parser can suggest corrections based on the surrounding context and common patterns at specific positions within SQL statements.</p>\n</blockquote>\n<p><strong>Multi-character Operator Recognition Strategy</strong></p>\n<p>Several SQL operators consist of multiple characters that must be recognized as single tokens rather than separate punctuation marks. The tokenizer must use <strong>maximal munch</strong> strategy - always consume the longest possible token sequence that forms a valid token.</p>\n<table>\n<thead>\n<tr>\n<th>Multi-Character Operator</th>\n<th>Individual Characters</th>\n<th>Correct Tokenization</th>\n<th>Incorrect Tokenization</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>!=</code></td>\n<td><code>!</code> and <code>=</code></td>\n<td><code>OPERATOR_NOT_EQUALS</code></td>\n<td><code>INVALID</code> + <code>OPERATOR_EQUALS</code></td>\n</tr>\n<tr>\n<td><code>&lt;&gt;</code></td>\n<td><code>&lt;</code> and <code>&gt;</code></td>\n<td><code>OPERATOR_NOT_EQUALS</code></td>\n<td><code>OPERATOR_LESS_THAN</code> + <code>OPERATOR_GREATER_THAN</code></td>\n</tr>\n<tr>\n<td><code>&lt;=</code></td>\n<td><code>&lt;</code> and <code>=</code></td>\n<td><code>OPERATOR_LESS_EQUAL</code></td>\n<td><code>OPERATOR_LESS_THAN</code> + <code>OPERATOR_EQUALS</code></td>\n</tr>\n<tr>\n<td><code>&gt;=</code></td>\n<td><code>&gt;</code> and <code>=</code></td>\n<td><code>OPERATOR_GREATER_EQUAL</code></td>\n<td><code>OPERATOR_GREATER_THAN</code> + <code>OPERATOR_EQUALS</code></td>\n</tr>\n</tbody></table>\n<p>The tokenizer implements maximal munch by examining the current character and looking ahead to determine if a longer token is possible. For example, when encountering <code>&lt;</code>, the tokenizer checks the next character: if it&#39;s <code>=</code>, the complete token becomes <code>OPERATOR_LESS_EQUAL</code>; if it&#39;s <code>&gt;</code>, the complete token becomes <code>OPERATOR_NOT_EQUALS</code>; otherwise, the token is simply <code>OPERATOR_LESS_THAN</code>.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Fast-node-hierarchy.svg\" alt=\"AST Node Type Hierarchy\"></p>\n<h3 id=\"ast-node-hierarchy\">AST Node Hierarchy</h3>\n<p>The Abstract Syntax Tree represents the parsed SQL statement as a hierarchical data structure that captures both the syntactic structure and semantic relationships within the query. Our AST design follows the <strong>Composite Pattern</strong> from object-oriented design, where every node shares common properties and behaviors while specialized node types handle specific SQL constructs.</p>\n<p>The base <code>ASTNode</code> class establishes the fundamental interface that all AST nodes implement. This common interface enables uniform traversal, debugging, and future extensions like query optimization or code generation. Each node maintains essential metadata about its source location and provides standardized methods for tree navigation and introspection.</p>\n<p><strong>Base AST Node Interface</strong></p>\n<p>Every AST node, regardless of its specific type or purpose, implements the fundamental <code>ASTNode</code> interface. This interface provides the essential operations needed for tree traversal, debugging, serialization, and future compiler phases like semantic analysis or code generation.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>node_type()</code></td>\n<td>None</td>\n<td><code>str</code></td>\n<td>Returns string identifying the specific node type (e.g., &quot;SelectStatement&quot;, &quot;BinaryOperation&quot;)</td>\n</tr>\n<tr>\n<td><code>children()</code></td>\n<td>None</td>\n<td><code>List[ASTNode]</code></td>\n<td>Returns list of direct child nodes in left-to-right order for tree traversal</td>\n</tr>\n<tr>\n<td><code>accept(visitor)</code></td>\n<td><code>visitor: ASTVisitor</code></td>\n<td><code>Any</code></td>\n<td>Implements visitor pattern for extensible tree operations like pretty-printing or analysis</td>\n</tr>\n<tr>\n<td><code>source_location()</code></td>\n<td>None</td>\n<td><code>SourceLocation</code></td>\n<td>Returns line/column information for error reporting and debugging</td>\n</tr>\n<tr>\n<td><code>__str__()</code></td>\n<td>None</td>\n<td><code>str</code></td>\n<td>Human-readable string representation for debugging and testing</td>\n</tr>\n<tr>\n<td><code>__repr__()</code></td>\n<td>None</td>\n<td><code>str</code></td>\n<td>Developer-friendly string representation showing node type and key properties</td>\n</tr>\n</tbody></table>\n<p>The <code>SourceLocation</code> helper class encapsulates position information that traces each AST node back to its original position in the SQL text. This information proves invaluable during error reporting, allowing the parser to show users exactly where problems occur in their queries.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>start_line</code></td>\n<td><code>int</code></td>\n<td>Line number where this construct begins (1-based)</td>\n</tr>\n<tr>\n<td><code>start_column</code></td>\n<td><code>int</code></td>\n<td>Column position where this construct begins (1-based)</td>\n</tr>\n<tr>\n<td><code>end_line</code></td>\n<td><code>int</code></td>\n<td>Line number where this construct ends (1-based)</td>\n</tr>\n<tr>\n<td><code>end_column</code></td>\n<td><code>int</code></td>\n<td>Column position where this construct ends (1-based)</td>\n</tr>\n</tbody></table>\n<p><strong>Node Classification Strategy</strong></p>\n<p>Our AST nodes organize into three primary categories based on their role in SQL statement structure:</p>\n<p><strong>Statement Nodes</strong> represent complete SQL commands that can be executed independently. These top-level nodes correspond to the major SQL statement types our parser supports. Each statement node contains the specific clauses and expressions that define the statement&#39;s behavior.</p>\n<p><strong>Expression Nodes</strong> represent computational units that evaluate to values during query execution. These nodes handle literals, identifiers, operators, function calls, and any other construct that produces a value. Expression nodes form the computational vocabulary of SQL queries.</p>\n<p><strong>Clause Nodes</strong> represent the major structural components within statements - SELECT lists, FROM clauses, WHERE conditions, and similar constructs that organize and contain expressions. Clause nodes bridge the gap between high-level statement structure and low-level expression details.</p>\n<table>\n<thead>\n<tr>\n<th>Node Category</th>\n<th>Purpose</th>\n<th>Examples</th>\n<th>Parent-Child Relationships</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statement</td>\n<td>Complete executable commands</td>\n<td><code>SelectStatement</code>, <code>InsertStatement</code></td>\n<td>Root nodes with clause children</td>\n</tr>\n<tr>\n<td>Expression</td>\n<td>Value-producing computations</td>\n<td><code>Literal</code>, <code>Identifier</code>, <code>BinaryOperation</code></td>\n<td>Leaf or internal nodes within clauses</td>\n</tr>\n<tr>\n<td>Clause</td>\n<td>Statement structural components</td>\n<td><code>SelectClause</code>, <code>FromClause</code>, <code>WhereClause</code></td>\n<td>Children of statements, parents of expressions</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The three-tier hierarchy (Statement → Clause → Expression) mirrors how humans naturally think about SQL queries. We start with the overall intent (SELECT data), then specify the major components (which columns FROM which tables WHERE certain conditions), and finally fill in the computational details (specific column names, comparison values, logical operators).</p>\n</blockquote>\n<h3 id=\"statement-ast-nodes\">Statement AST Nodes</h3>\n<p>Statement nodes represent the top-level executable commands in SQL. Each statement type corresponds to one of the major data manipulation operations our parser supports. These nodes serve as the root of AST trees and contain all the information necessary to understand and execute the complete SQL command.</p>\n<p><strong>SELECT Statement Structure</strong></p>\n<p>The <code>SelectStatement</code> node represents queries that retrieve and project data from tables. SELECT statements demonstrate the most complex AST structure in our parser, incorporating multiple optional clauses that can appear in specific orders with interdependent relationships.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>select_clause</code></td>\n<td><code>SelectClause</code></td>\n<td>Specifies which columns or expressions to retrieve from the result set</td>\n</tr>\n<tr>\n<td><code>from_clause</code></td>\n<td><code>FromClause | None</code></td>\n<td>Identifies source tables and their relationships (optional for scalar SELECT)</td>\n</tr>\n<tr>\n<td><code>where_clause</code></td>\n<td><code>WhereClause | None</code></td>\n<td>Filters rows based on boolean conditions (optional)</td>\n</tr>\n<tr>\n<td><code>distinct</code></td>\n<td><code>bool</code></td>\n<td>Whether to eliminate duplicate rows from the result set</td>\n</tr>\n</tbody></table>\n<p>The SELECT statement&#39;s clause structure reflects SQL&#39;s declarative nature - each clause specifies &quot;what&quot; rather than &quot;how,&quot; leaving execution strategy decisions to the database engine. The optional nature of FROM and WHERE clauses accommodates both simple scalar queries (<code>SELECT 1 + 1</code>) and complex filtered table queries (<code>SELECT name FROM users WHERE age &gt; 18</code>).</p>\n<p><strong>INSERT Statement Structure</strong></p>\n<p>The <code>InsertStatement</code> node represents commands that add new rows to tables. INSERT statements require careful coordination between column specifications and value lists to ensure data integrity and proper schema compliance.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>table_name</code></td>\n<td><code>Identifier</code></td>\n<td>Target table that will receive the new rows</td>\n</tr>\n<tr>\n<td><code>column_list</code></td>\n<td><code>List[Identifier] | None</code></td>\n<td>Explicit column names for value mapping (optional, defaults to table order)</td>\n</tr>\n<tr>\n<td><code>values_clause</code></td>\n<td><code>ValuesClause</code></td>\n<td>Source of data for the new rows (VALUES list or SELECT statement)</td>\n</tr>\n</tbody></table>\n<p>The relationship between <code>column_list</code> and <code>values_clause</code> requires validation during semantic analysis. When <code>column_list</code> is specified, each value tuple in the VALUES clause must contain exactly the same number of expressions. When <code>column_list</code> is omitted, the VALUES clause must provide values for all columns in the table&#39;s schema order.</p>\n<p><strong>UPDATE Statement Structure</strong></p>\n<p>The <code>UpdateStatement</code> node represents commands that modify existing table data. UPDATE statements combine assignment operations with conditional filtering to specify both what changes and which rows receive those changes.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>table_name</code></td>\n<td><code>Identifier</code></td>\n<td>Target table containing rows to modify</td>\n</tr>\n<tr>\n<td><code>set_clause</code></td>\n<td><code>SetClause</code></td>\n<td>List of column assignments that define the modifications</td>\n</tr>\n<tr>\n<td><code>where_clause</code></td>\n<td><code>WhereClause | None</code></td>\n<td>Conditions that determine which rows to modify (optional but dangerous without)</td>\n</tr>\n</tbody></table>\n<p>The <code>SetClause</code> contains a list of assignment expressions, each specifying a column name and the new value expression. The WHERE clause serves as a critical safety mechanism - UPDATE statements without WHERE clauses modify ALL rows in the target table, which rarely represents the intended behavior.</p>\n<p><strong>DELETE Statement Structure</strong></p>\n<p>The <code>DeleteStatement</code> node represents commands that remove rows from tables. DELETE statements demonstrate the simplest statement structure in our parser, requiring only a target table and optional filtering conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>table_name</code></td>\n<td><code>Identifier</code></td>\n<td>Target table containing rows to remove</td>\n</tr>\n<tr>\n<td><code>where_clause</code></td>\n<td><code>WhereClause | None</code></td>\n<td>Conditions that determine which rows to remove (optional but dangerous without)</td>\n</tr>\n</tbody></table>\n<p>DELETE statements share the same safety concerns as UPDATE statements - omitting the WHERE clause removes ALL rows from the target table. Many production database systems require explicit confirmation or administrative privileges for unfiltered DELETE operations.</p>\n<blockquote>\n<p><strong>Architecture Decision: Statement Node Completeness</strong></p>\n<ul>\n<li><strong>Context</strong>: SQL statements can have many optional clauses, and we must decide how much complexity to handle in our initial parser implementation.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Minimal statement support (only required clauses)</li>\n<li>Complete SQL standard compliance (all possible clauses)  </li>\n<li>Practical subset based on common usage patterns</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement practical subset covering the most frequently used clauses</li>\n<li><strong>Rationale</strong>: This approach provides immediate value for common queries while maintaining extensibility for future enhancements. Full SQL compliance would require months of development effort with diminishing returns for learning purposes.</li>\n<li><strong>Consequences</strong>: Users can parse and understand the majority of real-world SQL queries, but some advanced features like JOINs, subqueries, and window functions require future extensions.</li>\n</ul>\n</blockquote>\n<h3 id=\"expression-ast-nodes\">Expression AST Nodes</h3>\n<p>Expression nodes represent the computational components of SQL statements - any construct that evaluates to a value during query execution. These nodes form the building blocks for conditions, calculations, and data transformations within SQL queries. Expression nodes must support type information and evaluation contexts to enable future semantic analysis and optimization phases.</p>\n<p><strong>Literal Expression Nodes</strong></p>\n<p>Literal nodes represent constant values embedded directly in the SQL text. These nodes store the parsed value along with type information that enables proper comparison operations and prevents type-related errors during query execution.</p>\n<p>The <code>StringLiteral</code> node handles text constants with proper escape sequence processing. SQL string literals can use either single quotes (<code>&#39;text&#39;</code>) or double quotes (<code>&quot;text&quot;</code>), and they support escape sequences for embedded quotes and special characters.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>value</code></td>\n<td><code>str</code></td>\n<td>The processed string content with escape sequences resolved</td>\n</tr>\n<tr>\n<td><code>quote_char</code></td>\n<td><code>str</code></td>\n<td>Original quote character used (<code>&#39;</code> or <code>&quot;</code>) for round-trip preservation</td>\n</tr>\n<tr>\n<td><code>raw_value</code></td>\n<td><code>str</code></td>\n<td>Original text including quotes for debugging and error reporting</td>\n</tr>\n</tbody></table>\n<p>The <code>IntegerLiteral</code> node represents whole number constants with support for both positive and negative values. Integer literals must handle potential overflow conditions and provide appropriate type information for arithmetic operations.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>value</code></td>\n<td><code>int</code></td>\n<td>The parsed numeric value</td>\n</tr>\n<tr>\n<td><code>raw_value</code></td>\n<td><code>str</code></td>\n<td>Original text representation for debugging and formatting preservation</td>\n</tr>\n</tbody></table>\n<p>The <code>FloatLiteral</code> node represents decimal number constants with fractional components. Float literals require careful precision handling and must support scientific notation in future extensions.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>value</code></td>\n<td><code>float</code></td>\n<td>The parsed numeric value with decimal precision</td>\n</tr>\n<tr>\n<td><code>raw_value</code></td>\n<td><code>str</code></td>\n<td>Original text representation for precision preservation</td>\n</tr>\n</tbody></table>\n<p>The <code>NullLiteral</code> node represents the special NULL value that indicates missing or undefined data. NULL values require special comparison semantics (NULL = NULL is unknown, not true) and must be handled carefully in all expression evaluations.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>value</code></td>\n<td><code>None</code></td>\n<td>Always None to represent the NULL value</td>\n</tr>\n</tbody></table>\n<p><strong>Identifier Expression Nodes</strong></p>\n<p>The <code>Identifier</code> node represents references to database objects like tables, columns, and aliases. Identifiers can be simple names (<code>username</code>) or qualified names (<code>users.username</code>) that specify both table and column components.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>name</code></td>\n<td><code>str</code></td>\n<td>The identifier name as it appears in the SQL (case-preserved)</td>\n</tr>\n<tr>\n<td><code>table_qualifier</code></td>\n<td><code>str | None</code></td>\n<td>Optional table prefix for qualified column references</td>\n</tr>\n<tr>\n<td><code>quoted</code></td>\n<td><code>bool</code></td>\n<td>Whether this identifier was quoted in the original SQL (affects case sensitivity)</td>\n</tr>\n</tbody></table>\n<p>Quoted identifiers (enclosed in backticks, square brackets, or double quotes depending on SQL dialect) preserve exact case and allow reserved words to be used as identifiers. Unquoted identifiers typically follow case-insensitive matching rules, though this varies by database system.</p>\n<p><strong>Binary Operation Expression Nodes</strong></p>\n<p>The <code>BinaryOperation</code> node represents expressions that combine two operands with a single operator. These nodes form the backbone of arithmetic calculations, comparisons, and logical operations in SQL queries.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>left</code></td>\n<td><code>ASTNode</code></td>\n<td>Left operand expression (can be any expression type)</td>\n</tr>\n<tr>\n<td><code>operator</code></td>\n<td><code>TokenType</code></td>\n<td>The operation to perform (PLUS, EQUALS, AND, etc.)</td>\n</tr>\n<tr>\n<td><code>right</code></td>\n<td><code>ASTNode</code></td>\n<td>Right operand expression (can be any expression type)</td>\n</tr>\n</tbody></table>\n<p>Binary operations require careful precedence and associativity handling during parsing. The AST structure must reflect the correct evaluation order, with higher-precedence operations appearing deeper in the tree (closer to the leaves).</p>\n<p><strong>Arithmetic Operations</strong> include addition (<code>+</code>), subtraction (<code>-</code>), multiplication (<code>*</code>), and division (<code>/</code>). These operations typically require numeric operands and produce numeric results, though some SQL systems support string concatenation using the <code>+</code> operator.</p>\n<p><strong>Comparison Operations</strong> include equality (<code>=</code>), inequality (<code>!=</code>, <code>&lt;&gt;</code>), and magnitude comparisons (<code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>). Comparison operations accept various operand types but always produce boolean results.</p>\n<p><strong>Logical Operations</strong> include conjunction (<code>AND</code>), disjunction (<code>OR</code>), and negation (<code>NOT</code>). Logical operations require boolean operands and produce boolean results, with special handling for NULL values and three-valued logic.</p>\n<blockquote>\n<p><strong>Expression Tree Structure Example</strong>: The condition <code>age &gt; 18 AND name = &#39;John&#39;</code> produces a binary operation tree where:</p>\n<ul>\n<li>Root: BinaryOperation(operator=AND)</li>\n<li>Left child: BinaryOperation(operator=GREATER_THAN, left=Identifier(&#39;age&#39;), right=IntegerLiteral(18))</li>\n<li>Right child: BinaryOperation(operator=EQUALS, left=Identifier(&#39;name&#39;), right=StringLiteral(&#39;John&#39;))</li>\n</ul>\n</blockquote>\n<p><strong>Unary Operation Expression Nodes</strong></p>\n<p>The <code>UnaryOperation</code> node represents expressions with a single operand and operator. The most common unary operation in SQL is logical negation (<code>NOT</code>), though arithmetic negation (<code>-</code>) for negative numbers is handled during tokenization as part of numeric literals.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>operator</code></td>\n<td><code>TokenType</code></td>\n<td>The operation to perform (typically NOT)</td>\n</tr>\n<tr>\n<td><code>operand</code></td>\n<td><code>ASTNode</code></td>\n<td>The expression to apply the operation to</td>\n</tr>\n</tbody></table>\n<p>Unary operations have higher precedence than most binary operations, so <code>NOT age &gt; 18</code> parses as <code>NOT (age &gt; 18)</code> rather than <code>(NOT age) &gt; 18</code>.</p>\n<p><strong>Function Call Expression Nodes</strong></p>\n<p>The <code>FunctionCall</code> node represents invocations of SQL functions, both built-in functions (like <code>COUNT</code>, <code>SUM</code>, <code>UPPER</code>) and user-defined functions. Function calls include the function name and an argument list that can contain any number of expressions.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>function_name</code></td>\n<td><code>Identifier</code></td>\n<td>Name of the function to invoke</td>\n</tr>\n<tr>\n<td><code>arguments</code></td>\n<td><code>List[ASTNode]</code></td>\n<td>List of argument expressions (can be empty)</td>\n</tr>\n<tr>\n<td><code>distinct</code></td>\n<td><code>bool</code></td>\n<td>Whether DISTINCT was specified for aggregate functions</td>\n</tr>\n</tbody></table>\n<p>Function calls require semantic analysis to validate argument counts, types, and contexts. Some functions like <code>COUNT(*)</code> have special syntax rules, while aggregate functions like <code>SUM(DISTINCT column)</code> support optional DISTINCT qualifiers.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Fparse-tree-example.svg\" alt=\"Example Parse Tree for Complex Query\"></p>\n<blockquote>\n<p><strong>Architecture Decision: Expression Node Extensibility</strong></p>\n<ul>\n<li><strong>Context</strong>: SQL expressions can become arbitrarily complex with nested function calls, subqueries, and case expressions. We must balance initial simplicity with future extensibility needs.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Flat expression model (limited nesting depth)</li>\n<li>Fully recursive expression model (unlimited nesting)</li>\n<li>Hybrid model (recursive with practical depth limits)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement fully recursive expression model with clear extension points</li>\n<li><strong>Rationale</strong>: Real SQL queries often contain deeply nested expressions, and artificial limitations would restrict the parser&#39;s utility. The recursive model aligns naturally with the recursive descent parsing algorithm.</li>\n<li><strong>Consequences</strong>: Parser can handle complex expressions from real applications, but stack overflow protection and depth limiting may be needed for malicious inputs.</li>\n</ul>\n</blockquote>\n<p><strong>Common AST Construction Pitfalls</strong></p>\n<p>⚠️ <strong>Pitfall: Incorrect Operator Precedence in AST Structure</strong>\nMany implementers create AST trees that don&#39;t reflect proper operator precedence, leading to incorrect expression evaluation. For example, the expression <code>a + b * c</code> should parse as <code>a + (b * c)</code>, not <code>(a + b) * c</code>. The AST structure must place higher-precedence operations (multiplication) deeper in the tree than lower-precedence operations (addition). During parsing, ensure that precedence climbing or recursive descent properly structures the tree according to operator precedence rules, not just left-to-right parsing order.</p>\n<p>⚠️ <strong>Pitfall: Missing Position Information in AST Nodes</strong>\nForgetting to propagate source position information from tokens to AST nodes makes debugging and error reporting nearly impossible. Each AST node should maintain line and column information that traces back to the original SQL text. Without this information, error messages become generic and unhelpful (&quot;syntax error&quot; instead of &quot;syntax error at line 3, column 15: expected comma after column name&quot;). Always copy position information from tokens during AST node construction, and consider spanning positions for nodes that cover multiple tokens.</p>\n<p>⚠️ <strong>Pitfall: Mutable AST Node Fields</strong>\nMaking AST node fields mutable creates opportunities for bugs where tree structure gets accidentally modified during traversal or analysis. AST nodes should be immutable data structures - once constructed during parsing, their content should never change. Use read-only properties or immutable data structures to prevent accidental modifications. If you need to transform the AST, create new nodes rather than modifying existing ones.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Node Type Hierarchies</strong>\nCreating AST node classes without consistent interfaces makes tree traversal and analysis code brittle. All AST nodes should implement the same base interface with consistent method names and behaviors. Don&#39;t create special cases where some nodes lack common methods like <code>children()</code> or <code>accept()</code>. Consistent interfaces enable generic tree operations like pretty-printing, serialization, and visitor-based analysis.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Type Information in Expression Nodes</strong>\nExpression nodes that don&#39;t maintain sufficient type information make semantic analysis and code generation difficult or impossible. While our basic parser focuses on syntactic structure, including type hints and value information in expression nodes enables future enhancements. At minimum, distinguish between different literal types (string, integer, float) and maintain enough information to reconstruct the original SQL text.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides Python-specific code for the token types and AST node hierarchy defined above. The code emphasizes clarity and extensibility while following Python best practices for data classes and type hints.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Data Structure</td>\n<td>Named tuples (<code>collections.namedtuple</code>)</td>\n<td>Data classes with <code>@dataclass</code> decorator</td>\n</tr>\n<tr>\n<td>AST Node Base Class</td>\n<td>Abstract base class (<code>abc.ABC</code>)</td>\n<td>Protocol classes for structural typing</td>\n</tr>\n<tr>\n<td>Enum Definitions</td>\n<td>Standard <code>enum.Enum</code></td>\n<td><code>enum.auto()</code> for automatic value assignment</td>\n</tr>\n<tr>\n<td>Type Checking</td>\n<td>Built-in <code>typing</code> module</td>\n<td>Third-party <code>mypy</code> for static analysis</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql_parser/\n├── __init__.py\n├── tokens.py              ← Token types and Token data class\n├── ast_nodes/\n│   ├── __init__.py        ← Import all node types for convenience\n│   ├── base.py           ← Base ASTNode interface and common utilities\n│   ├── statements.py     ← Statement node classes (SELECT, INSERT, etc.)\n│   ├── expressions.py    ← Expression node classes (literals, operators, etc.)\n│   └── clauses.py        ← Clause node classes (FROM, WHERE, etc.)\n├── exceptions.py         ← All parser exception classes\n└── utils.py             ← Common utilities and helper functions</code></pre></div>\n\n<p><strong>Complete Token Infrastructure (Ready to Use)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tokens.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Set</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Keywords</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_SELECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_FROM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_WHERE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_INSERT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_UPDATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_DELETE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_INTO</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_VALUES</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_SET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_AND</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_OR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_NOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_NULL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_IS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_AS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD_DISTINCT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Identifiers and Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTEGER_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FLOAT_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_NOT_EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_LESS_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_GREATER_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_LESS_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_GREATER_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_PLUS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_MINUS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_MULTIPLY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR_DIVIDE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION_COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION_SEMICOLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION_LEFT_PAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION_RIGHT_PAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION_DOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Special</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WHITESPACE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INVALID</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># SQL keyword mapping for case-insensitive recognition</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SQL_KEYWORDS</span><span style=\"color:#E1E4E8\">: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'select'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_SELECT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'from'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_FROM</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'where'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_WHERE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'insert'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_INSERT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'update'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_UPDATE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'delete'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_DELETE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'into'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_INTO</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'values'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_VALUES</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'set'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_SET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'and'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_AND</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'or'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_OR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'not'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_NOT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'null'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_NULL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'is'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_IS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'as'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_AS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'distinct'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_DISTINCT</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Immutable token with position information for error reporting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_keyword</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if this token represents a SQL keyword.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.type.name.startswith(</span><span style=\"color:#9ECBFF\">'KEYWORD_'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_operator</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if this token represents an operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.type.name.startswith(</span><span style=\"color:#9ECBFF\">'OPERATOR_'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_literal</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if this token represents a literal value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">STRING_LITERAL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">INTEGER_LITERAL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">FLOAT_LITERAL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_NULL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SourceLocation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Source position information for AST nodes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_token</span><span style=\"color:#E1E4E8\">(cls, token: Token) -> </span><span style=\"color:#9ECBFF\">'SourceLocation'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create source location from a single token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            start_line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">token.line,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            start_column</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">token.column,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            end_line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">token.line,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            end_column</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">token.column </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(token.value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> span_tokens</span><span style=\"color:#E1E4E8\">(cls, start_token: Token, end_token: Token) -> </span><span style=\"color:#9ECBFF\">'SourceLocation'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create source location spanning from start token to end token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            start_line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">start_token.line,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            start_column</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">start_token.column,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            end_line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">end_token.line,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            end_column</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">end_token.column </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(end_token.value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<p><strong>Complete Exception Hierarchy (Ready to Use)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># exceptions.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .tokens </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, SourceLocation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for all parsing errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.location </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.location:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Parse error at line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.location.start_line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.location.start_column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Parse error: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised during tokenization phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised during parsing phase due to invalid syntax.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UnexpectedTokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised when parser encounters unexpected token type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, actual: Token):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        location </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SourceLocation.from_token(actual)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, but found </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.actual </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual</span></span></code></pre></div>\n\n<p><strong>Base AST Node Infrastructure (Ready to Use)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># ast_nodes/base.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tokens </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SourceLocation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ASTVisitor</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Visitor interface for AST traversal and analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> visit_select_statement</span><span style=\"color:#E1E4E8\">(self, node: </span><span style=\"color:#9ECBFF\">'SelectStatement'</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> visit_identifier</span><span style=\"color:#E1E4E8\">(self, node: </span><span style=\"color:#9ECBFF\">'Identifier'</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> visit_string_literal</span><span style=\"color:#E1E4E8\">(self, node: </span><span style=\"color:#9ECBFF\">'StringLiteral'</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Add visit methods for each AST node type as you implement them</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ASTNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all AST nodes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._location </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return string identifier for this node type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#9ECBFF\">'ASTNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return list of direct child nodes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Accept visitor for tree traversal.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> source_location</span><span style=\"color:#E1E4E8\">(self) -> Optional[SourceLocation]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return source location information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Human-readable representation for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        children_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> ', '</span><span style=\"color:#E1E4E8\">.join(</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(child) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> child </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.children())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.node_type()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">(</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">children_str</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Developer-friendly representation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"&#x3C;</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.node_type()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.source_location()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">>\"</span></span></code></pre></div>\n\n<p><strong>Statement AST Node Skeletons (Core Logic for Students to Implement)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># ast_nodes/statements.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .base </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ASTNode, ASTVisitor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tokens </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SourceLocation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SelectStatement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AST node representing a SELECT query statement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 select_clause: </span><span style=\"color:#9ECBFF\">'SelectClause'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 from_clause: Optional[</span><span style=\"color:#9ECBFF\">'FromClause'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 where_clause: Optional[</span><span style=\"color:#9ECBFF\">'WhereClause'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 distinct: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(location)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store the provided clause nodes in instance variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that select_clause is not None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: self.select_clause = select_clause</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"SelectStatement\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return list of non-None clause nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include select_clause (always present)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include from_clause if not None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include where_clause if not None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use list comprehension with filter(None, [clause1, clause2, ...])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> visitor.visit_select_statement(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InsertStatement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AST node representing an INSERT statement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 table_name: </span><span style=\"color:#9ECBFF\">'Identifier'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 values_clause: </span><span style=\"color:#9ECBFF\">'ValuesClause'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 column_list: Optional[List[</span><span style=\"color:#9ECBFF\">'Identifier'</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(location)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store table_name and values_clause (both required)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store column_list (optional, defaults to empty list if None)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that table_name and values_clause are not None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"InsertStatement\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return list starting with table_name, then values_clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add all identifiers from column_list if present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: [self.table_name, self.values_clause] + self.column_list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> visitor.visit_insert_statement(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UpdateStatement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AST node representing an UPDATE statement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 table_name: </span><span style=\"color:#9ECBFF\">'Identifier'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 set_clause: </span><span style=\"color:#9ECBFF\">'SetClause'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 where_clause: Optional[</span><span style=\"color:#9ECBFF\">'WhereClause'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(location)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store table_name and set_clause (both required)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store where_clause (optional for UPDATE, but dangerous without it)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider adding a warning when where_clause is None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"UpdateStatement\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return table_name, set_clause, and where_clause (if present)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use filter(None, ...) to exclude None values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> visitor.visit_update_statement(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DeleteStatement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AST node representing a DELETE statement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 table_name: </span><span style=\"color:#9ECBFF\">'Identifier'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 where_clause: Optional[</span><span style=\"color:#9ECBFF\">'WhereClause'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(location)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store table_name (required)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store where_clause (optional but dangerous without it)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider logging a warning when where_clause is None (deletes all rows)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"DeleteStatement\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return table_name and where_clause (if present)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: where_clause can be None, so filter it out if missing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> visitor.visit_delete_statement(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Expression AST Node Skeletons (Core Logic for Students to Implement)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># ast_nodes/expressions.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .base </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ASTNode, ASTVisitor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tokens </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SourceLocation, TokenType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Identifier</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AST node representing a table or column identifier.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 table_qualifier: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 quoted: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(location)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store the name (required - the identifier text)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store table_qualifier (optional - for qualified names like table.column)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store quoted flag (whether identifier was quoted in SQL)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that name is not empty</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"Identifier\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identifiers are leaf nodes - return empty list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: return []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> visitor.visit_identifier(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> qualified_name</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return the full qualified name (table.column or just column).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If table_qualifier exists, return f\"{table_qualifier}.{name}\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Otherwise, return just the name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use conditional expression or if statement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StringLiteral</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AST node representing a string literal value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"'\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 raw_value: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(location)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store the processed string value (with escape sequences resolved)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store the quote character used (' or \")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store raw_value for debugging (original text including quotes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If raw_value not provided, construct it from quote_char and value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"StringLiteral\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> []  </span><span style=\"color:#6A737D\"># Literals are leaf nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> visitor.visit_string_literal(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IntegerLiteral</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AST node representing an integer literal value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 raw_value: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(location)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store the parsed integer value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store raw_value (original text) for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If raw_value not provided, convert value to string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"IntegerLiteral\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> visitor.visit_integer_literal(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BinaryOperation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AST node representing a binary operation (left operator right).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 left: ASTNode,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 operator: TokenType,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 right: ASTNode,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 location: Optional[SourceLocation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(location)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store left operand (required ASTNode)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store operator (required TokenType like OPERATOR_EQUALS)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store right operand (required ASTNode)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that all parameters are not None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"BinaryOperation\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return list containing left operand and right operand</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Order matters for tree traversal - left first, then right</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: return [self.left, self.right]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> accept</span><span style=\"color:#E1E4E8\">(self, visitor: ASTVisitor) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> visitor.visit_binary_operation(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_comparison</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if this operation is a comparison (=, &#x3C;, >, etc.).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if operator is any comparison type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check against OPERATOR_EQUALS, OPERATOR_LESS_THAN, etc.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use operator in {TokenType.OPERATOR_EQUALS, ...}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_arithmetic</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if this operation is arithmetic (+, -, *, /).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if operator is arithmetic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check against OPERATOR_PLUS, OPERATOR_MINUS, etc.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_logical</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if this operation is logical (AND, OR).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if operator is KEYWORD_AND or KEYWORD_OR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Python Hints</strong></p>\n<ul>\n<li>Use <code>@dataclass(frozen=True)</code> for immutable AST nodes to prevent accidental modification</li>\n<li>Leverage <code>typing.Optional[T]</code> and <code>typing.List[T]</code> for clear type specifications</li>\n<li>Use <code>enum.auto()</code> for TokenType values to avoid manual numbering</li>\n<li>Implement <code>__str__</code> and <code>__repr__</code> methods for better debugging experience</li>\n<li>Consider using <code>functools.cached_property</code> for expensive computed properties</li>\n<li>Use <code>isinstance()</code> checks instead of string comparisons for node type testing</li>\n</ul>\n<p><strong>Milestone Checkpoint: Data Model Validation</strong></p>\n<p>After implementing the data structures above, validate your implementation with these tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test basic token creation and properties</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD_SELECT</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"SELECT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> token.is_keyword() </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> token.is_operator() </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test source location spanning</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">start </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"users\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">end </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION_SEMICOLON</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\";\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">25</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">location </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SourceLocation.span_tokens(start, end)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> location.start_column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> location.end_column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 26</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test AST node creation and relationships</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">table_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Identifier(</span><span style=\"color:#9ECBFF\">\"users\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">column_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Identifier(</span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">table_qualifier</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"users\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">select_clause </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SelectClause([column_id])  </span><span style=\"color:#6A737D\"># You'll implement SelectClause</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">from_clause </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FromClause(table_id)         </span><span style=\"color:#6A737D\"># You'll implement FromClause</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">select_stmt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SelectStatement(select_clause, from_clause)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(select_stmt.children()) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> select_stmt.node_type() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"SelectStatement\"</span></span></code></pre></div>\n\n<p><strong>Debugging Tips for Data Model Issues</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AttributeError</code> on AST node</td>\n<td>Missing field initialization</td>\n<td>Check <code>__init__</code> method, print <code>self.__dict__</code></td>\n<td>Add missing field assignments in constructor</td>\n</tr>\n<tr>\n<td>Incorrect tree traversal</td>\n<td>Wrong <code>children()</code> implementation</td>\n<td>Print node.children() for each node type</td>\n<td>Return all child nodes in correct order</td>\n</tr>\n<tr>\n<td>Position information missing</td>\n<td>Not propagating SourceLocation</td>\n<td>Check if location passed to nodes during parsing</td>\n<td>Copy location from tokens during AST construction</td>\n</tr>\n<tr>\n<td>Token type confusion</td>\n<td>Wrong TokenType enum values</td>\n<td>Print token.type for each token</td>\n<td>Verify SQL_KEYWORDS mapping and tokenizer logic</td>\n</tr>\n<tr>\n<td>Immutability violations</td>\n<td>Modifying AST after creation</td>\n<td>Use <code>@dataclass(frozen=True)</code> or check for assignments</td>\n<td>Make all fields read-only properties</td>\n</tr>\n</tbody></table>\n<h2 id=\"tokenizer-component-design\">Tokenizer Component Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (SQL Tokenizer) - This section provides the detailed design for the lexical analysis component that converts SQL text into classified tokens, forming the foundation for all subsequent parsing operations.</p>\n</blockquote>\n<h3 id=\"mental-model-reading-word-by-word\">Mental Model: Reading Word by Word</h3>\n<p>Understanding tokenization becomes intuitive when we think about how humans read written language. When you encounter the sentence &quot;The quick brown fox jumps over the lazy dog,&quot; your brain doesn&#39;t process individual letters in isolation. Instead, it automatically groups characters into meaningful units—words—and then classifies each word by its role: articles, adjectives, nouns, verbs. Your mental lexicon helps you recognize &quot;quick&quot; as an adjective and &quot;jumps&quot; as a verb, even before you understand the complete sentence structure.</p>\n<p>SQL tokenization follows this exact same pattern, but with database query language instead of natural language. When our tokenizer encounters the SQL text <code>SELECT name FROM users WHERE age &gt; 25</code>, it performs the same word-by-word recognition process. It groups the characters &#39;S&#39;, &#39;E&#39;, &#39;L&#39;, &#39;E&#39;, &#39;C&#39;, &#39;T&#39; into the unit &quot;SELECT&quot; and immediately recognizes this as a SQL keyword token. Similarly, it identifies &quot;name&quot; as an identifier token, &quot;FROM&quot; as another keyword, and &quot;&gt;&quot; as a comparison operator token.</p>\n<p>The key insight is that tokenization is fundamentally about <strong>pattern recognition and classification</strong>. Just as your brain has learned to distinguish between different word types in English, our tokenizer must learn to distinguish between SQL keywords (<code>SELECT</code>, <code>WHERE</code>), identifiers (table names, column names), literals (strings, numbers), operators (<code>=</code>, <code>&lt;</code>, <code>AND</code>), and punctuation (commas, parentheses). The tokenizer serves as the &quot;reading comprehension&quot; layer that transforms raw character sequences into meaningful linguistic units that the parser can then assemble into complete grammatical structures.</p>\n<p>This analogy also reveals why tokenization must happen before parsing. You cannot understand the grammatical structure of &quot;The quick brown fox jumps&quot; until you first recognize that &quot;quick&quot; and &quot;brown&quot; are adjectives modifying &quot;fox&quot; as a noun. Similarly, a SQL parser cannot build an Abstract Syntax Tree for <code>SELECT name FROM users</code> until the tokenizer has first identified <code>SELECT</code> as a statement keyword, <code>name</code> as a column identifier, <code>FROM</code> as a table reference keyword, and <code>users</code> as a table identifier.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Ftoken-types.svg\" alt=\"Token Type Classification\"></p>\n<h3 id=\"tokenization-algorithm\">Tokenization Algorithm</h3>\n<p>The tokenization algorithm implements a <strong>character-by-character scanning process</strong> that maintains state about the current position in the SQL text and accumulates characters into complete tokens. The algorithm follows the <strong>maximal munch principle</strong>, always consuming the longest possible sequence of characters that forms a valid token. This prevents ambiguities where shorter token matches might incorrectly break up longer valid tokens.</p>\n<p>The core tokenization process operates through these fundamental steps:</p>\n<ol>\n<li><p><strong>Initialize Scanner State</strong>: The tokenizer maintains a current position pointer in the input SQL string, along with line and column counters for error reporting. It also maintains an empty token list that will accumulate the final results. The scanner starts at position zero with line 1, column 1.</p>\n</li>\n<li><p><strong>Character Classification Loop</strong>: The algorithm enters a main loop that continues until it reaches the end of the input string. At each iteration, it examines the current character and determines what type of token might be starting at this position. This classification drives the decision about which specialized parsing routine to invoke.</p>\n</li>\n<li><p><strong>Whitespace Skipping</strong>: Before attempting any token recognition, the scanner checks if the current character is whitespace (space, tab, newline, carriage return). If so, it advances the position counter and updates line/column tracking as appropriate. Whitespace serves as token separators but does not generate tokens themselves in our SQL parser.</p>\n</li>\n<li><p><strong>Token Type Dispatch</strong>: Based on the current character, the scanner dispatches to specialized parsing routines. Alphabetic characters typically start keywords or identifiers, digit characters start numeric literals, quote characters start string literals, and special symbol characters start operators or punctuation tokens.</p>\n</li>\n<li><p><strong>Maximal Munch Token Building</strong>: Each specialized parsing routine implements maximal munch by consuming characters as long as they can extend the current token. For example, when parsing an identifier that starts with &#39;u&#39;, the routine continues consuming characters through &#39;users&#39; rather than stopping at just &#39;u&#39;. This ensures that longer valid tokens take precedence over shorter ones.</p>\n</li>\n<li><p><strong>Token Classification and Creation</strong>: Once a complete token has been consumed, the parsing routine determines the appropriate <code>TokenType</code> for the accumulated characters. Keywords are distinguished from identifiers through dictionary lookup, operators are classified by their symbol patterns, and literals are typed based on their content format.</p>\n</li>\n<li><p><strong>Position Tracking and Error Context</strong>: Throughout token creation, the scanner maintains precise position information including start and end coordinates. This enables high-quality error messages that can point users to the exact location of tokenization problems.</p>\n</li>\n<li><p><strong>Token List Accumulation</strong>: Each successfully created token is appended to the growing token list. The scanner then returns to the main character classification loop to process the next token in the input stream.</p>\n</li>\n</ol>\n<p>The algorithm handles <strong>multi-character operators</strong> like <code>&gt;=</code> and <code>&lt;=</code> by implementing look-ahead logic. When the scanner encounters a <code>&gt;</code> character, it peeks at the next character to determine whether to create a simple <code>GREATER_THAN</code> token or a compound <code>GREATER_THAN_OR_EQUAL</code> token. This look-ahead prevents the incorrect tokenization of <code>&gt;=</code> as separate <code>&gt;</code> and <code>=</code> tokens.</p>\n<p><strong>Error handling</strong> integrates directly into the tokenization algorithm. When the scanner encounters an unexpected character that cannot start any valid token type, it creates a <code>TokenizerError</code> with precise position information and a descriptive message. The error includes the problematic character, its line and column position, and context about what types of tokens were expected at that location.</p>\n<p>The algorithm maintains <strong>line and column counters</strong> by tracking newline characters during scanning. Each time the scanner encounters a <code>\\n</code> character, it increments the line counter and resets the column counter to 1. For all other characters, it increments the column counter. This position tracking is essential for providing helpful error messages and debugging information.</p>\n<h3 id=\"keyword-recognition-strategy\">Keyword Recognition Strategy</h3>\n<p>SQL keyword recognition presents unique challenges because SQL keywords are <strong>case-insensitive</strong> but must be distinguished from regular identifiers that happen to use the same character sequences. The keyword recognition strategy implements a two-phase approach: first identifying potential keywords through pattern matching, then confirming their keyword status through dictionary lookup.</p>\n<p>The <strong>case-insensitive matching</strong> requirement means that <code>SELECT</code>, <code>select</code>, <code>Select</code>, and <code>SeLeCt</code> must all be recognized as the same <code>SELECT</code> keyword token. However, the tokenizer must preserve the original case of the characters for potential pretty-printing or error message display. This dual requirement drives the design of our keyword recognition algorithm.</p>\n<p><strong>Phase 1: Identifier Pattern Recognition</strong> begins when the scanner encounters an alphabetic character or underscore. The tokenizer enters identifier parsing mode and consumes all subsequent characters that match the SQL identifier character set: letters, digits, and underscores. This phase produces a complete character sequence like &quot;SELECT&quot; or &quot;user_id&quot; without yet determining whether it represents a keyword or regular identifier.</p>\n<p><strong>Phase 2: Keyword Dictionary Lookup</strong> takes the accumulated character sequence and performs a case-insensitive comparison against the <code>SQL_KEYWORDS</code> dictionary. The lookup converts the candidate string to uppercase before dictionary access, ensuring that &quot;select&quot; matches the dictionary key &quot;SELECT&quot;. If a match is found, the tokenizer creates a keyword token with the appropriate <code>TokenType</code>. If no match is found, the tokenizer creates an <code>IDENTIFIER</code> token instead.</p>\n<p>The <code>SQL_KEYWORDS</code> dictionary maps uppercase keyword strings to their corresponding <code>TokenType</code> enumeration values. This design provides several advantages: fast lookup performance through hash table access, easy extensibility for additional keywords, and clear separation between keyword recognition logic and keyword definitions. The dictionary structure appears as follows:</p>\n<table>\n<thead>\n<tr>\n<th>Keyword String</th>\n<th>Token Type</th>\n<th>Usage Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SELECT</td>\n<td>SELECT_KEYWORD</td>\n<td>Beginning of SELECT statements</td>\n</tr>\n<tr>\n<td>FROM</td>\n<td>FROM_KEYWORD</td>\n<td>Table reference clauses</td>\n</tr>\n<tr>\n<td>WHERE</td>\n<td>WHERE_KEYWORD</td>\n<td>Conditional filter clauses</td>\n</tr>\n<tr>\n<td>INSERT</td>\n<td>INSERT_KEYWORD</td>\n<td>Beginning of INSERT statements</td>\n</tr>\n<tr>\n<td>UPDATE</td>\n<td>UPDATE_KEYWORD</td>\n<td>Beginning of UPDATE statements</td>\n</tr>\n<tr>\n<td>DELETE</td>\n<td>DELETE_KEYWORD</td>\n<td>Beginning of DELETE statements</td>\n</tr>\n<tr>\n<td>INTO</td>\n<td>INTO_KEYWORD</td>\n<td>INSERT target specification</td>\n</tr>\n<tr>\n<td>VALUES</td>\n<td>VALUES_KEYWORD</td>\n<td>INSERT value lists</td>\n</tr>\n<tr>\n<td>SET</td>\n<td>SET_KEYWORD</td>\n<td>UPDATE assignment clauses</td>\n</tr>\n<tr>\n<td>AND</td>\n<td>AND_OPERATOR</td>\n<td>Logical conjunction in expressions</td>\n</tr>\n<tr>\n<td>OR</td>\n<td>OR_OPERATOR</td>\n<td>Logical disjunction in expressions</td>\n</tr>\n<tr>\n<td>NOT</td>\n<td>NOT_OPERATOR</td>\n<td>Logical negation in expressions</td>\n</tr>\n<tr>\n<td>IS</td>\n<td>IS_OPERATOR</td>\n<td>NULL comparison operator</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>NULL_LITERAL</td>\n<td>NULL value literal</td>\n</tr>\n<tr>\n<td>AS</td>\n<td>AS_KEYWORD</td>\n<td>Alias declarations</td>\n</tr>\n</tbody></table>\n<p>The <strong>context-sensitive keyword</strong> challenge arises because some SQL terms can function as keywords in certain contexts and identifiers in others. For example, &quot;ORDER&quot; is a keyword in <code>ORDER BY</code> clauses but could theoretically be used as a table name or column name in other contexts. Our tokenizer takes a <strong>context-free approach</strong> and always recognizes &quot;ORDER&quot; as a keyword token, leaving context-sensitive validation to the parser layer.</p>\n<p><strong>Reserved word handling</strong> follows SQL standards by treating all recognized keywords as reserved words that cannot be used as regular identifiers. This simplifies parsing logic and prevents ambiguous situations where the parser cannot determine whether a token represents a keyword or identifier based on context alone. Users who need to use reserved words as identifiers must employ quoted identifiers (though our parser does not support quoted identifiers in this implementation).</p>\n<p><strong>Extensibility</strong> for additional keywords requires only adding entries to the <code>SQL_KEYWORDS</code> dictionary and defining corresponding <code>TokenType</code> enumeration values. This design allows easy extension as new SQL features are added to the parser. The keyword recognition algorithm itself requires no modification to support additional keywords.</p>\n<h3 id=\"string-literal-parsing\">String Literal Parsing</h3>\n<p>String literal parsing handles the complex process of extracting quoted text values from SQL input while properly processing <strong>escape sequences</strong>, <strong>quote character variations</strong>, and <strong>embedded quotes</strong>. SQL string literals can use either single quotes (<code>&#39;text&#39;</code>) or double quotes (<code>&quot;text&quot;</code>), and the parsing algorithm must handle both quote types while maintaining the semantic differences between them.</p>\n<p>The <strong>quote character detection</strong> phase begins when the tokenizer encounters either a single quote (<code>&#39;</code>) or double quote (<code>&quot;</code>) character. The tokenizer records which quote character initiated the string and enters string literal parsing mode. The opening quote character determines both the expected closing quote character and the escape sequence rules that apply within the string content.</p>\n<p><strong>Character accumulation</strong> proceeds character by character from the opening quote until the matching closing quote is found. However, this process must handle several complex cases that make string parsing more sophisticated than simple character collection. The algorithm must distinguish between quote characters that end the string and quote characters that represent literal quote content within the string.</p>\n<p><strong>Escape sequence processing</strong> handles special character representations within string literals. SQL supports several standard escape sequences that allow representing characters that would otherwise be difficult to include in quoted strings:</p>\n<table>\n<thead>\n<tr>\n<th>Escape Sequence</th>\n<th>Literal Character</th>\n<th>Usage Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>\\&#39;</code></td>\n<td>Single quote</td>\n<td><code>&#39;Don\\&#39;t stop&#39;</code></td>\n</tr>\n<tr>\n<td><code>\\&quot;</code></td>\n<td>Double quote</td>\n<td><code>&quot;She said \\&quot;Hello\\&quot;&quot;</code></td>\n</tr>\n<tr>\n<td><code>\\\\</code></td>\n<td>Backslash</td>\n<td><code>&#39;C:\\\\Users\\\\data&#39;</code></td>\n</tr>\n<tr>\n<td><code>\\n</code></td>\n<td>Newline</td>\n<td><code>&#39;Line 1\\nLine 2&#39;</code></td>\n</tr>\n<tr>\n<td><code>\\t</code></td>\n<td>Tab character</td>\n<td><code>&#39;Column1\\tColumn2&#39;</code></td>\n</tr>\n<tr>\n<td><code>\\r</code></td>\n<td>Carriage return</td>\n<td><code>&#39;Windows\\r\\nNewline&#39;</code></td>\n</tr>\n</tbody></table>\n<p>The <strong>doubled quote convention</strong> provides an alternative escape mechanism where two consecutive quote characters represent a single literal quote. This convention applies when the doubled quotes match the string&#39;s opening quote character. For example, <code>&#39;Don&#39;&#39;t stop&#39;</code> represents the string value &quot;Don&#39;t stop&quot; using single-quote doubling, while <code>&quot;She said &quot;&quot;Hello&quot;&quot;&quot;</code> represents &quot;She said &quot;Hello&quot;&quot; using double-quote doubling.</p>\n<p><strong>Algorithm implementation</strong> for string parsing follows these detailed steps:</p>\n<ol>\n<li><p><strong>Quote Detection</strong>: When the scanner encounters a quote character, it records the quote type and advances past the opening quote. The quote type determines the termination condition and escape rules for the remainder of the parsing process.</p>\n</li>\n<li><p><strong>Content Accumulation Loop</strong>: The algorithm enters a loop that examines each subsequent character. Regular characters are directly appended to the string content. When escape characters or quote characters are encountered, special processing logic is invoked.</p>\n</li>\n<li><p><strong>Escape Sequence Recognition</strong>: When a backslash character is encountered, the algorithm examines the following character to determine the appropriate escape sequence. Valid escape sequences are converted to their literal character representations. Invalid escape sequences trigger tokenization errors with descriptive messages.</p>\n</li>\n<li><p><strong>Quote Character Handling</strong>: When a quote character matching the opening quote is encountered, the algorithm must determine whether this represents string termination or an escaped literal quote. If the next character is also a matching quote, this represents an escaped quote via the doubled quote convention. Otherwise, this represents string termination.</p>\n</li>\n<li><p><strong>Termination and Token Creation</strong>: When a closing quote is successfully identified, the algorithm advances past the closing quote and creates a <code>STRING_LITERAL</code> token containing the accumulated string content. The token value contains the processed string content with escape sequences resolved to their literal characters.</p>\n</li>\n</ol>\n<p><strong>Error conditions</strong> in string parsing include several scenarios that require clear error messages:</p>\n<ul>\n<li><strong>Unterminated strings</strong> occur when the input ends before a closing quote is found. The error message should indicate the line and column where the string started and note that no closing quote was found.</li>\n<li><strong>Invalid escape sequences</strong> occur when a backslash is followed by a character that does not form a valid escape sequence. The error should identify the invalid sequence and list the valid escape sequences.</li>\n<li><strong>Embedded newlines</strong> in string literals may or may not be supported depending on SQL dialect. Our parser treats unescaped newlines within strings as errors and suggests using <code>\\n</code> escape sequences instead.</li>\n</ul>\n<h3 id=\"architecture-decision-state-machine-vs-character-by-character\">Architecture Decision: State Machine vs Character-by-Character</h3>\n<p>The tokenizer implementation approach represents a fundamental architectural decision that affects code complexity, performance characteristics, extensibility, and debugging ease. Two primary approaches were considered: a <strong>formal state machine</strong> implementation and a <strong>character-by-character scanning</strong> approach with method dispatch.</p>\n<blockquote>\n<p><strong>Decision: Character-by-Character Scanning with Method Dispatch</strong></p>\n<ul>\n<li><strong>Context</strong>: The tokenizer needs to handle multiple token types (keywords, identifiers, numbers, strings, operators) with different parsing rules and varying levels of complexity. The approach must be understandable to intermediate developers while remaining extensible for future SQL features.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Formal finite state machine with explicit state transitions</li>\n<li>Character-by-character scanning with specialized parsing methods</li>\n<li>Regular expression-based tokenization</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Character-by-character scanning with method dispatch to specialized parsing functions</li>\n<li><strong>Rationale</strong>: This approach provides the best balance of simplicity and power for our educational context. It allows each token type to have dedicated parsing logic without the complexity of managing explicit state transitions. The code structure directly mirrors the conceptual understanding of tokenization, making it easier for learners to understand and extend.</li>\n<li><strong>Consequences</strong>: Trade-off of some theoretical elegance and performance optimization potential in exchange for significantly improved code readability and maintainability. Each token type parsing can be understood in isolation.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Formal State Machine</strong></td>\n<td>Theoretically elegant, potentially faster, handles ambiguous cases well</td>\n<td>Complex state transition management, harder to debug, steeper learning curve</td>\n<td>No</td>\n</tr>\n<tr>\n<td><strong>Character-by-Character + Dispatch</strong></td>\n<td>Clear separation of concerns, easy to understand and extend, straightforward debugging</td>\n<td>Slightly more method call overhead, less formally rigorous</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td><strong>Regular Expression Based</strong></td>\n<td>Very concise code, leverages proven regex engines</td>\n<td>Poor error messages, harder to customize, regex complexity for SQL grammar</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>The <strong>formal state machine approach</strong> would implement tokenization as an explicit finite automaton with defined states (Normal, InIdentifier, InString, InNumber, InOperator) and transition functions. While this approach offers theoretical elegance and can handle complex tokenization scenarios efficiently, it introduces significant complexity in state management and transition logic. The resulting code would be harder for intermediate developers to understand and modify, working against our educational goals.</p>\n<p><strong>Character-by-character scanning with method dispatch</strong> implements tokenization through a main scanning loop that examines the current character and dispatches to specialized parsing methods based on character type. This approach aligns naturally with how developers think about tokenization: &quot;if I see a letter, parse an identifier; if I see a digit, parse a number; if I see a quote, parse a string.&quot; Each parsing method can focus entirely on its specific token type without worrying about global state management.</p>\n<p>The <strong>method dispatch strategy</strong> uses character classification to determine the appropriate parsing method:</p>\n<table>\n<thead>\n<tr>\n<th>Character Type</th>\n<th>Dispatch Method</th>\n<th>Handles Token Types</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Alphabetic or underscore</td>\n<td><code>_parse_identifier_or_keyword()</code></td>\n<td>IDENTIFIER, keywords</td>\n</tr>\n<tr>\n<td>Digit</td>\n<td><code>_parse_number()</code></td>\n<td>INTEGER_LITERAL, FLOAT_LITERAL</td>\n</tr>\n<tr>\n<td>Single or double quote</td>\n<td><code>_parse_string_literal()</code></td>\n<td>STRING_LITERAL</td>\n</tr>\n<tr>\n<td>Operator symbols</td>\n<td><code>_parse_operator()</code></td>\n<td>All operator tokens</td>\n</tr>\n<tr>\n<td>Punctuation</td>\n<td><code>_parse_punctuation()</code></td>\n<td>COMMA, SEMICOLON, etc.</td>\n</tr>\n</tbody></table>\n<p><strong>Error handling benefits</strong> from the character-by-character approach because each parsing method can provide specific error messages related to its token type. When string parsing fails due to an unterminated quote, the string parsing method can provide a message like &quot;Unterminated string literal starting at line 5, column 12&quot; rather than a generic state machine error like &quot;Unexpected end of input in state InString.&quot;</p>\n<p><strong>Extensibility advantages</strong> emerge because adding support for new token types requires only implementing a new parsing method and adding its dispatch condition to the main scanning loop. The existing parsing methods require no modification, and the overall tokenizer structure remains unchanged. This modularity supports incremental development and makes the codebase easier to understand for learners.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Ftokenizer-state-machine.svg\" alt=\"Tokenizer State Machine\"></p>\n<p><strong>Performance considerations</strong> for the character-by-character approach involve slightly higher method call overhead compared to a state machine&#39;s direct transitions. However, for the scope of our SQL parser and typical query sizes, this overhead is negligible compared to the benefits in code clarity and maintainability. The tokenization phase typically represents a small fraction of total query processing time.</p>\n<h3 id=\"common-tokenizer-pitfalls\">Common Tokenizer Pitfalls</h3>\n<p>Tokenizer implementation presents several recurring challenges that can trap intermediate developers. Understanding these pitfalls and their solutions prevents hours of debugging and helps build robust lexical analysis components.</p>\n<p>⚠️ <strong>Pitfall: Case Sensitivity Inconsistency</strong></p>\n<p>Many developers initially implement keyword recognition with exact string matching, causing <code>SELECT</code> to be recognized as a keyword while <code>select</code> is treated as a regular identifier. This creates confusing behavior where syntactically identical queries behave differently based on capitalization.</p>\n<p><strong>Why this fails</strong>: SQL is case-insensitive for keywords by specification, but exact string matching is case-sensitive by default in most programming languages. When the tokenizer uses direct dictionary lookup with mixed-case user input, keywords fail to match their dictionary entries.</p>\n<p><strong>How to fix</strong>: Implement case-insensitive keyword lookup by converting candidate strings to uppercase before dictionary comparison. Store all keyword dictionary keys in uppercase, and normalize input strings to uppercase before lookup. Preserve original case in token values for display purposes.</p>\n<p>⚠️ <strong>Pitfall: Incomplete Escape Sequence Handling</strong></p>\n<p>String literal parsing often fails to handle all required escape sequences, particularly the doubled quote convention (<code>&#39;&#39;</code> within single-quoted strings). Developers frequently implement only backslash escapes, causing parse failures when SQL queries use the doubled quote style.</p>\n<p><strong>Why this fails</strong>: SQL supports two different escape mechanisms for quotes within strings: backslash escapes (<code>\\&#39;</code>) and doubled quotes (<code>&#39;&#39;</code>). Many tokenizer implementations only support one mechanism, causing valid SQL strings to be rejected or incorrectly parsed.</p>\n<p><strong>How to fix</strong>: Implement both escape mechanisms in string parsing logic. When encountering a quote character that matches the opening quote, check if the next character is also a matching quote (doubled convention) before treating it as string termination.</p>\n<p>⚠️ <strong>Pitfall: Multi-Character Operator Fragmentation</strong></p>\n<p>Tokenizers often incorrectly split multi-character operators like <code>&gt;=</code>, <code>&lt;=</code>, <code>&lt;&gt;</code>, and <code>!=</code> into separate single-character tokens. This occurs when the tokenizer processes each character independently without considering longer operator sequences.</p>\n<p><strong>Why this fails</strong>: The parser expects compound operators to arrive as single tokens matching the SQL grammar. When <code>&gt;=</code> arrives as separate <code>&gt;</code> and <code>=</code> tokens, the parser cannot match this sequence to any valid grammar rule, resulting in syntax errors for valid SQL expressions.</p>\n<p><strong>How to fix</strong>: Implement look-ahead logic in operator parsing. When encountering a character that could start a multi-character operator, examine the following character to determine whether to create a compound operator token or a simple single-character token.</p>\n<p>⚠️ <strong>Pitfall: Position Tracking Errors</strong></p>\n<p>Line and column position tracking often becomes inaccurate due to incorrect handling of different newline conventions (<code>\\n</code>, <code>\\r\\n</code>, <code>\\r</code>), leading to error messages that point to wrong locations in the source code.</p>\n<p><strong>Why this fails</strong>: Different operating systems use different newline conventions, and SQL text might originate from various sources. If the tokenizer only recognizes one newline style, position tracking becomes inaccurate when processing text with different newline conventions.</p>\n<p><strong>How to fix</strong>: Implement comprehensive newline handling that recognizes all common newline conventions. When advancing past newline characters, increment line counters and reset column counters regardless of the specific newline style encountered.</p>\n<p>⚠️ <strong>Pitfall: Whitespace Preservation in Tokens</strong></p>\n<p>Token values sometimes include leading or trailing whitespace characters, particularly when parsing identifiers that are adjacent to whitespace. This causes string comparisons and keyword lookups to fail unexpectedly.</p>\n<p><strong>Why this fails</strong>: If the tokenizer includes whitespace characters in token values, subsequent processing that expects clean token values will fail. For example, a keyword lookup for &quot; SELECT &quot; (with spaces) will not match the dictionary entry &quot;SELECT&quot;.</p>\n<p><strong>How to fix</strong>: Ensure that token parsing methods exclude whitespace from token values. Skip whitespace before beginning token parsing, and stop token accumulation when whitespace is encountered after the token content.</p>\n<p>⚠️ <strong>Pitfall: Numeric Literal Type Confusion</strong></p>\n<p>Number parsing often fails to distinguish between integer and floating-point literals, either treating all numbers as the same type or incorrectly classifying numbers based on incomplete parsing logic.</p>\n<p><strong>Why this fails</strong>: SQL distinguishes between integer literals (123) and floating-point literals (123.45, 1.23e10), and this distinction affects query semantics. If the tokenizer misclassifies numeric literals, the parser may generate incorrect AST nodes or fail to enforce proper type checking.</p>\n<p><strong>How to fix</strong>: Implement comprehensive numeric parsing that detects decimal points and scientific notation to distinguish between integer and floating-point formats. Create appropriate <code>INTEGER_LITERAL</code> or <code>FLOAT_LITERAL</code> tokens based on the detected format.</p>\n<p>⚠️ <strong>Pitfall: Error Recovery Absence</strong></p>\n<p>Tokenizers often terminate immediately upon encountering unexpected characters, providing minimal error context and preventing the detection of additional errors elsewhere in the SQL text.</p>\n<p><strong>Why this fails</strong>: Immediate termination on tokenization errors prevents users from seeing all problems in their SQL text at once. This creates a frustrating development experience where users must fix errors one at a time through multiple parse attempts.</p>\n<p><strong>How to fix</strong>: Implement error recovery that creates error tokens for problematic characters while continuing to process the remainder of the input. Collect all tokenization errors and report them together to give users a complete picture of all issues.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides concrete Python implementation guidance for building the SQL tokenizer component, bridging the gap between the design concepts and working code.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Character Processing</strong></td>\n<td>String indexing with manual position tracking</td>\n<td>Custom StringScanner class with position management</td>\n</tr>\n<tr>\n<td><strong>Keyword Lookup</strong></td>\n<td>Python dict with uppercase keys</td>\n<td>collections.defaultdict with case-insensitive wrapper</td>\n</tr>\n<tr>\n<td><strong>Token Storage</strong></td>\n<td>Python list of Token objects</td>\n<td>collections.deque for efficient append operations</td>\n</tr>\n<tr>\n<td><strong>Error Handling</strong></td>\n<td>Custom exception classes with position info</td>\n<td>Rich error context with source snippet display</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td>unittest with hand-written test cases</td>\n<td>pytest with parameterized tests and property-based testing</td>\n</tr>\n</tbody></table>\n<p>For this educational implementation, we recommend the <strong>simple options</strong> to maintain focus on core tokenization concepts rather than advanced Python features.</p>\n<p><strong>B. Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql-parser/\n  src/\n    tokenizer/\n      __init__.py              ← Module exports\n      token_types.py           ← TokenType enum and Token class\n      tokenizer.py             ← Main Tokenizer class\n      exceptions.py            ← TokenizerError and related exceptions\n      keywords.py              ← SQL_KEYWORDS dictionary\n    tests/\n      test_tokenizer.py        ← Comprehensive tokenizer tests\n      test_token_types.py      ← Token class tests\n    examples/\n      tokenize_examples.py     ← Usage examples\n  README.md</code></pre></div>\n\n<p>This structure separates concerns clearly: token definitions, tokenization logic, error handling, and keyword recognition each live in dedicated modules.</p>\n<p><strong>C. Infrastructure Starter Code (Complete)</strong></p>\n<p>The following complete infrastructure code handles token definitions, exceptions, and keyword mappings. Learners can use this code as-is and focus on implementing the core tokenization algorithm.</p>\n<p><strong>File: <code>src/tokenizer/token_types.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Token type definitions and Token class for SQL parsing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enumeration of all SQL token types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Keywords</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SELECT_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FROM_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WHERE_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INSERT_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UPDATE_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DELETE_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTO_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VALUES_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SET_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AS_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AND_OPERATOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OR_OPERATOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_OPERATOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_THAN_OR_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_THAN_OR_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IS_OPERATOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTEGER_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FLOAT_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NULL_LITERAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Identifiers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEMICOLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LEFT_PAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RIGHT_PAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STAR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Special</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Represents a single token from SQL lexical analysis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Attributes:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        type: The classification of this token</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        value: The original text content</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        line: Line number (1-based) where token begins</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        column: Column number (1-based) where token begins</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_keyword</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns True if this token is a SQL keyword.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.type.name.endswith(</span><span style=\"color:#9ECBFF\">'_KEYWORD'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_operator</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns True if this token is an operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.type.name.endswith(</span><span style=\"color:#9ECBFF\">'_OPERATOR'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUALS</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN_OR_EQUAL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN_OR_EQUAL</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IS_OPERATOR</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_literal</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns True if this token is a literal value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.type.name.endswith(</span><span style=\"color:#9ECBFF\">'_LITERAL'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">('</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">') at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>File: <code>src/tokenizer/exceptions.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Exception classes for SQL tokenization errors.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception class for all parsing errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> column </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            full_message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Column </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            full_message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(full_message)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised during tokenization/lexical analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised during parsing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UnexpectedTokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exception raised when parser encounters unexpected token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, actual_token, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.actual_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual_token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> message </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, but found </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, actual_token.line, actual_token.column)</span></span></code></pre></div>\n\n<p><strong>File: <code>src/tokenizer/keywords.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">SQL keyword definitions and lookup utilities.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .token_types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Mapping from uppercase keyword strings to their TokenType values</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SQL_KEYWORDS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'SELECT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SELECT_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'FROM'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">FROM_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'WHERE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">WHERE_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'INSERT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">INSERT_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'UPDATE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">UPDATE_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'DELETE'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">DELETE_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'INTO'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">INTO_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'VALUES'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">VALUES_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'SET'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SET_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'AS'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">AS_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'AND'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">AND_OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'OR'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OR_OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'NOT'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">NOT_OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'IS'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">IS_OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'NULL'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">NULL_LITERAL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> lookup_keyword</span><span style=\"color:#E1E4E8\">(text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> TokenType:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Look up a token type for potential keyword text.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        text: The text to check (case-insensitive)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        TokenType.IDENTIFIER if not a keyword, otherwise the keyword TokenType</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> SQL_KEYWORDS</span><span style=\"color:#E1E4E8\">.get(text.upper(), TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>The following skeleton provides the main Tokenizer class structure with detailed TODO comments that map to the algorithm steps described in the design section.</p>\n<p><strong>File: <code>src/tokenizer/tokenizer.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Main SQL tokenizer implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .token_types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, TokenType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenizerError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .keywords </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> lookup_keyword</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Tokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    SQL tokenizer that converts SQL text into a list of classified tokens.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    The tokenizer uses character-by-character scanning with method dispatch</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    to specialized parsing functions for different token types.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize tokenizer with SQL text to process.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            sql_text: The SQL query string to tokenize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.sql_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sql_text</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Main entry point that tokenizes the SQL text and returns token list.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of Token objects representing the tokenized SQL</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            TokenizerError: If tokenization fails due to invalid input</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Reset tokenizer state (position, line, column, tokens list)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Enter main scanning loop while not at end of input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Skip whitespace characters and update position tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check if at end of input, break if so</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Dispatch to appropriate token parsing method based on current character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If no parsing method handles current character, raise TokenizerError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Add EOF token to end of token list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return completed token list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use _current_char() to get current character, None if at end</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Dispatch based on character type - letters->identifier, digits->number, etc.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _current_char</span><span style=\"color:#E1E4E8\">(self) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns the current character or None if at end of input.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Current character or None if position >= len(sql_text)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if position is beyond end of input string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return None if at end, otherwise return character at current position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _peek_char</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Look ahead at future characters without advancing position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            offset: How many characters ahead to look (default 1)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Character at position + offset, or None if beyond end</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate target position as current position + offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return None if target position is beyond input length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return character at target position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Move to the next character and update position tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if current character is newline (\\n)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If newline: increment line counter, reset column to 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If not newline: increment column counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Increment position counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle \\r\\n sequences correctly for Windows line endings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Advance past all whitespace characters at current position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Loop while current character exists and is whitespace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call _advance() to move past each whitespace character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use char.isspace() to check for whitespace</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_token</span><span style=\"color:#E1E4E8\">(self, token_type: TokenType, value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     start_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, start_column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Create a new token and add it to the token list.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            token_type: The type classification for this token</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            value: The original text content</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            start_line: Line where token begins</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            start_column: Column where token begins</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            The created Token object</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Token(token_type, value, start_line, start_column)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_identifier_or_keyword</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse an identifier or keyword token starting at current position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Token with IDENTIFIER type or specific keyword type</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_line, start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Loop while current character is letter, digit, or underscore</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Accumulate characters into value string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Advance position after adding each character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Use lookup_keyword() to determine if this is a keyword</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create token with appropriate type (IDENTIFIER or keyword type)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return the created token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: First character must be letter or underscore, subsequent can include digits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse a numeric literal (integer or float) starting at current position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Token with INTEGER_LITERAL or FLOAT_LITERAL type</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_line, start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        is_float </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Loop while current character is digit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Accumulate digits into value string, advance position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if next character is decimal point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If decimal point found, set is_float=True, add to value, advance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Continue accumulating digits after decimal point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Determine token type based on is_float flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create and return appropriate numeric literal token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Don't consume decimal point if not followed by digit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_string_literal</span><span style=\"color:#E1E4E8\">(self, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse a string literal enclosed in quotes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            quote_char: The opening quote character (' or \")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Token with STRING_LITERAL type</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            TokenizerError: If string is unterminated or has invalid escapes</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_line, start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Advance past opening quote character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Loop until closing quote found or end of input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle escape sequences (backslash escapes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle doubled quote escapes ('' within single quotes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Accumulate processed characters (with escapes resolved)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check for unterminated string error condition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Advance past closing quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Create and return STRING_LITERAL token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Process \\n, \\t, \\', \\\" escape sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle quote_char + quote_char as single literal quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_operator</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse an operator token starting at current position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Token with appropriate operator type</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_line, start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._current_char()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Handle single-character operators: =, &#x3C;, >, !, (, ), *, comma, semicolon</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle multi-character operators: &#x3C;=, >=, &#x3C;>, !=</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use _peek_char() to look ahead for multi-character sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Advance position appropriately (1 char for single, 2 for multi)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create token with correct operator TokenType</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return the created token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check for >= before >, &#x3C;= before &#x3C;, != and &#x3C;> for not-equals</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Python Hints</strong></p>\n<ul>\n<li><strong>String methods</strong>: Use <code>char.isalpha()</code>, <code>char.isdigit()</code>, <code>char.isspace()</code> for character classification</li>\n<li><strong>String building</strong>: Accumulate token characters with <code>value += char</code> or use <code>list</code> and <code>&#39;&#39;.join()</code>  for better performance</li>\n<li><strong>Dictionary lookup</strong>: <code>SQL_KEYWORDS.get(text.upper(), TokenType.IDENTIFIER)</code> handles case-insensitive keyword lookup with default</li>\n<li><strong>Exception handling</strong>: Create TokenizerError with precise position: <code>TokenizerError(f&quot;Unexpected character &#39;{char}&#39;&quot;, self.line, self.column)</code></li>\n<li><strong>Enum comparison</strong>: Compare TokenType values directly: <code>if token_type == TokenType.SELECT_KEYWORD:</code></li>\n<li><strong>Optional handling</strong>: Check <code>if self._current_char() is not None:</code> before character operations</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the tokenizer, verify correct operation with these checkpoints:</p>\n<p><strong>Test Command</strong>: </p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_tokenizer.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>Expected Behavior</strong>: \nRun this manual test to verify tokenizer functionality:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.tokenizer.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tokenizer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test basic SELECT query</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"SELECT name FROM users WHERE id = 42\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tokenizer(sql)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer.tokenize()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should produce these tokens:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">expected_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">SELECT_KEYWORD</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">FROM_KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">WHERE_KEYWORD</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">INTEGER_LITERAL</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Tokenization successful!\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected_types) </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"Token count mismatch\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Signs of Success</strong>:</p>\n<ul>\n<li>Keywords recognized regardless of case: <code>SELECT</code>, <code>select</code>, <code>Select</code> all produce <code>SELECT_KEYWORD</code></li>\n<li>String literals parse correctly: <code>&#39;John&#39;&#39;s Data&#39;</code> produces value <code>John&#39;s Data</code></li>\n<li>Multi-character operators work: <code>&gt;=</code> produces <code>GREATER_THAN_OR_EQUAL</code>, not separate <code>&gt;</code> and <code>=</code></li>\n<li>Position tracking accurate: Error messages point to correct line and column</li>\n<li>All test cases pass without assertion errors</li>\n</ul>\n<p><strong>Common Issues and Fixes</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Keywords treated as identifiers</td>\n<td>Case-sensitive keyword lookup</td>\n<td>Use <code>text.upper()</code> before dictionary lookup</td>\n</tr>\n<tr>\n<td>String parsing fails on escaped quotes</td>\n<td>Incomplete escape handling</td>\n<td>Implement both backslash and doubled quote escapes</td>\n</tr>\n<tr>\n<td>Wrong token positions in errors</td>\n<td>Incorrect line/column tracking</td>\n<td>Handle newlines properly in <code>_advance()</code> method</td>\n</tr>\n<tr>\n<td><code>&gt;=</code> tokenized as separate <code>&gt;</code> and <code>=</code></td>\n<td>No look-ahead for multi-char operators</td>\n<td>Use <code>_peek_char()</code> in operator parsing</td>\n</tr>\n<tr>\n<td>Tests fail with &quot;unexpected character&quot;</td>\n<td>Character dispatch missing cases</td>\n<td>Add dispatch cases for all valid starting characters</td>\n</tr>\n</tbody></table>\n<h2 id=\"select-parser-component-design\">SELECT Parser Component Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 (SELECT Parser) - This section provides the detailed design for the recursive descent parser that transforms tokens into Abstract Syntax Trees for SELECT statements.</p>\n</blockquote>\n<p>The SELECT parser represents the heart of our SQL parser, transforming a flat sequence of tokens into a meaningful hierarchical structure that captures the semantic intent of the query. Unlike the tokenizer which operates character-by-character in a linear fashion, the parser must understand the grammatical relationships between tokens and construct a tree that reflects SQL&#39;s complex syntax rules. This component bridges the gap between lexical analysis and semantic understanding, creating the foundation for query execution engines to interpret and process SQL statements.</p>\n<h3 id=\"mental-model-grammar-rules-as-functions\">Mental Model: Grammar Rules as Functions</h3>\n<p>Think of parsing a SELECT statement like teaching someone to understand the structure of English sentences. When we read &quot;The quick brown fox jumps over the lazy dog,&quot; we instinctively recognize patterns: &quot;The quick brown fox&quot; is the subject (a noun phrase), &quot;jumps over&quot; is the verb phrase, and &quot;the lazy dog&quot; is the object. We learned these patterns as grammar rules in school - a sentence consists of a subject and predicate, a noun phrase can have adjectives, and so on.</p>\n<p>Recursive descent parsing works exactly like this grammatical analysis, but formalized into code. Each grammar rule becomes a function that knows how to recognize and process its specific pattern. Just as &quot;noun phrase&quot; might call &quot;adjective&quot; and &quot;noun&quot; sub-rules, our <code>parse_select_statement()</code> function calls <code>parse_column_list()</code>, <code>parse_from_clause()</code>, and <code>parse_where_clause()</code> functions. Each function is responsible for understanding its piece of the grammar puzzle and returning a structured representation of what it found.</p>\n<p>The &quot;recursive&quot; part comes into play when rules reference themselves or each other. In English, we can have sentences within sentences (&quot;The dog that bit the cat ran away&quot;). In SQL, we can have expressions within expressions (<code>(age &gt; 18) AND (status = &#39;active&#39;)</code>). The parser handles this by having functions call other functions, building up a tree of understanding just like we mentally parse complex sentences by breaking them into familiar patterns.</p>\n<p>The key insight is that each parsing function has a single, focused responsibility: recognize its specific grammar pattern, consume the appropriate tokens, and build the correct AST node. When a function encounters something outside its domain (like <code>parse_column_list()</code> seeing a <code>WHERE_KEYWORD</code>), it simply stops and lets the calling function handle the next piece. This creates a natural, modular way to handle SQL&#39;s complex but structured syntax.</p>\n<h3 id=\"select-statement-grammar-rules\">SELECT Statement Grammar Rules</h3>\n<p>Our SELECT parser must understand the formal grammar rules that define valid SELECT statements. We express these rules in a BNF-like notation that directly translates to our recursive descent parsing functions. Each rule defines what tokens and sub-rules can appear in what order, along with which elements are required versus optional.</p>\n<p>The core SELECT statement grammar follows this hierarchical structure:</p>\n<table>\n<thead>\n<tr>\n<th>Rule Name</th>\n<th>Grammar Definition</th>\n<th>Required Elements</th>\n<th>Optional Elements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>select_statement</code></td>\n<td><code>SELECT column_list FROM table_reference [WHERE expression]</code></td>\n<td>SELECT keyword, column_list, FROM keyword, table_reference</td>\n<td>WHERE clause</td>\n</tr>\n<tr>\n<td><code>column_list</code></td>\n<td><code>column_spec [, column_spec]*</code> or <code>*</code></td>\n<td>At least one column_spec or star</td>\n<td>Additional comma-separated columns</td>\n</tr>\n<tr>\n<td><code>column_spec</code></td>\n<td><code>[table_name .] column_name [AS alias]</code></td>\n<td>column_name</td>\n<td>table qualifier, alias</td>\n</tr>\n<tr>\n<td><code>table_reference</code></td>\n<td><code>table_name [AS alias]</code></td>\n<td>table_name</td>\n<td>alias (with or without AS)</td>\n</tr>\n<tr>\n<td><code>expression</code></td>\n<td><code>comparison_expr [(AND|OR) expression]*</code></td>\n<td>At least one comparison</td>\n<td>Additional logical operators</td>\n</tr>\n<tr>\n<td><code>comparison_expr</code></td>\n<td><code>column_spec (= | != | &lt; | &gt; | &lt;= | &gt;=) literal</code></td>\n<td>Left operand, operator, right operand</td>\n<td>Parentheses for grouping</td>\n</tr>\n</tbody></table>\n<p>These grammar rules establish the parsing precedence and structure. The <code>select_statement</code> rule is our top-level entry point - it must start with a <code>SELECT_KEYWORD</code>, followed by a column list, then a <code>FROM_KEYWORD</code>, then a table reference, and optionally a WHERE clause. Each sub-rule like <code>column_list</code> defines its own internal structure and delegates to further sub-rules as needed.</p>\n<p>The grammar handles several important SQL features that make parsing challenging. Column specifications can be qualified with table names (<code>users.name</code>) or unqualified (<code>name</code>), and can have aliases either with the <code>AS</code> keyword (<code>name AS full_name</code>) or without (<code>name full_name</code>). Table references follow similar aliasing rules. The star wildcard (<code>SELECT *</code>) represents a special case in the column list that our parser must recognize and handle differently from explicit column names.</p>\n<p>Expression parsing within WHERE clauses introduces additional complexity through operator precedence. While we show a simplified version here, the full expression grammar must handle comparison operators, logical operators (<code>AND</code>, <code>OR</code>, <code>NOT</code>), parentheses for precedence override, and various literal types (strings, integers, floats, NULL). Each level of precedence becomes its own grammar rule, creating a hierarchy that naturally enforces correct parsing order.</p>\n<h3 id=\"recursive-descent-algorithm\">Recursive Descent Algorithm</h3>\n<p>The recursive descent algorithm transforms each grammar rule into a dedicated parsing function that follows a consistent pattern: check for expected tokens, consume them if found, recursively call sub-parsers for complex elements, build and return the appropriate AST node. This approach creates a direct correspondence between our grammar rules and code structure, making the parser both intuitive to understand and straightforward to extend.</p>\n<p>Every parsing function in our recursive descent parser follows the same fundamental algorithm steps:</p>\n<ol>\n<li><p><strong>Token Validation</strong>: Check that the current token matches what this grammar rule expects. If parsing a SELECT statement, verify the current token is <code>SELECT_KEYWORD</code>. If not, raise an <code>UnexpectedTokenError</code> with details about what was expected versus what was found.</p>\n</li>\n<li><p><strong>Token Consumption</strong>: Advance the parser position to consume the expected token. This moves the parser forward through the token stream and positions it for the next parsing step. Track the consumed token&#39;s position information for error reporting and AST node location data.</p>\n</li>\n<li><p><strong>Sub-rule Delegation</strong>: For each complex element in the grammar rule, call the appropriate parsing function. When parsing a SELECT statement, call <code>parse_column_list()</code> for the column specification, <code>parse_from_clause()</code> for the table reference, and conditionally <code>parse_where_clause()</code> if a WHERE keyword is present.</p>\n</li>\n<li><p><strong>AST Node Construction</strong>: Create the appropriate AST node type with all parsed child elements. For a SELECT statement, instantiate a <code>SelectStatement</code> object with the parsed column list, table reference, and optional WHERE expression as child nodes.</p>\n</li>\n<li><p><strong>Error Recovery</strong>: If parsing fails at any step, generate descriptive error messages that include the current position, what was expected, and what was actually found. Advanced parsers attempt to recover and continue parsing to find additional errors, but our implementation focuses on clear error reporting for the first issue encountered.</p>\n</li>\n<li><p><strong>Return Value</strong>: Return the constructed AST node to the calling function. This allows parent parsing functions to incorporate the parsed element into their own AST nodes, building up the complete tree structure.</p>\n</li>\n</ol>\n<p>The recursive nature emerges when parsing functions call each other to handle sub-elements. When <code>parse_select_statement()</code> calls <code>parse_column_list()</code>, and <code>parse_column_list()</code> calls <code>parse_column_spec()</code> for each column, we&#39;re building a call stack that mirrors the hierarchical structure of the SQL statement. Each function focuses solely on its grammar rule, trusting that called functions will correctly handle their responsibilities.</p>\n<p>This algorithm handles optional elements through conditional parsing. When a grammar rule has optional components (like the WHERE clause in SELECT statements), the parsing function checks if the current token matches the optional element&#39;s start token. If so, it parses the optional element; if not, it leaves that element as null in the AST node and continues with required elements.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Fast-node-hierarchy.svg\" alt=\"AST Node Type Hierarchy\"></p>\n<h3 id=\"column-list-parsing\">Column List Parsing</h3>\n<p>Column list parsing represents one of the more complex aspects of SELECT statement processing because it must handle multiple syntactic variations while building a consistent AST representation. The parser must recognize star wildcards (<code>SELECT *</code>), qualified column names (<code>SELECT users.name, orders.total</code>), aliases with and without the AS keyword (<code>SELECT name AS full_name, total order_amount</code>), and complex expressions within column specifications.</p>\n<p>The column list parsing algorithm begins by checking for the special case of a star wildcard. If the current token is an <code>ASTERISK</code> token, the parser creates a special <code>StarExpression</code> AST node that represents the &quot;select all columns&quot; semantic. This distinguishes between explicit column lists and wildcard selection at the AST level, allowing query execution engines to handle each case appropriately. The star wildcard cannot be combined with explicit column names in standard SQL, so finding an asterisk ends column list parsing immediately.</p>\n<p>For explicit column lists, the parser enters a comma-separated parsing loop that continues until it encounters a token that cannot be part of a column specification (typically the <code>FROM_KEYWORD</code>). Each iteration of this loop calls <code>parse_column_spec()</code> to handle individual column elements, then checks for a <code>COMMA</code> token to determine whether to continue parsing additional columns. The parser must be careful to distinguish between commas that separate columns and other commas that might appear in expressions or function calls.</p>\n<p>Individual column specification parsing handles the most complex aspects of column list syntax. Each column spec can begin with either a simple identifier (<code>name</code>) or a qualified identifier (<code>users.name</code>). The parser uses lookahead to distinguish these cases - if an identifier is followed by a <code>DOT</code> token, it treats the identifier as a table qualifier and expects another identifier for the actual column name. This creates either an <code>Identifier</code> AST node (for simple columns) or a <code>QualifiedIdentifier</code> node (for table.column references).</p>\n<p>Alias parsing adds another layer of complexity because SQL supports both explicit aliases (using the <code>AS</code> keyword) and implicit aliases (where the alias immediately follows the column specification). The parser checks for an <code>AS_KEYWORD</code> token after each column specification, and if found, expects an identifier token for the alias name. However, it must also handle implicit aliases where an identifier token follows the column specification without an intervening AS keyword. This requires careful lookahead to avoid conflicting with subsequent clauses.</p>\n<table>\n<thead>\n<tr>\n<th>Column List Pattern</th>\n<th>Example</th>\n<th>AST Node Structure</th>\n<th>Parsing Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Star wildcard</td>\n<td><code>SELECT *</code></td>\n<td><code>StarExpression()</code></td>\n<td>Simple - single token</td>\n</tr>\n<tr>\n<td>Simple column</td>\n<td><code>SELECT name</code></td>\n<td><code>Identifier(&quot;name&quot;)</code></td>\n<td>Simple - single identifier</td>\n</tr>\n<tr>\n<td>Qualified column</td>\n<td><code>SELECT users.name</code></td>\n<td><code>QualifiedIdentifier(&quot;users&quot;, &quot;name&quot;)</code></td>\n<td>Medium - requires DOT lookahead</td>\n</tr>\n<tr>\n<td>Explicit alias</td>\n<td><code>SELECT name AS full_name</code></td>\n<td><code>AliasedExpression(Identifier(&quot;name&quot;), &quot;full_name&quot;)</code></td>\n<td>Medium - AS keyword detection</td>\n</tr>\n<tr>\n<td>Implicit alias</td>\n<td><code>SELECT name full_name</code></td>\n<td><code>AliasedExpression(Identifier(&quot;name&quot;), &quot;full_name&quot;)</code></td>\n<td>High - requires FROM lookahead</td>\n</tr>\n<tr>\n<td>Mixed list</td>\n<td><code>SELECT id, users.name AS full_name, status</code></td>\n<td>Array of mixed AST nodes</td>\n<td>High - combines all patterns</td>\n</tr>\n</tbody></table>\n<h3 id=\"table-reference-and-alias-parsing\">Table Reference and Alias Parsing</h3>\n<p>Table reference parsing in the FROM clause handles the foundation of SQL query processing by identifying the data sources and their optional aliases. While conceptually simpler than column list parsing, table reference parsing must still handle qualified table names (for databases that support schema prefixes), implicit and explicit aliasing, and proper error detection when table names are malformed or missing.</p>\n<p>The FROM clause parsing begins with mandatory keyword recognition. After the SELECT statement parser identifies the column list, it expects to find a <code>FROM_KEYWORD</code> token. If this token is missing, the parser generates an <code>UnexpectedTokenError</code> indicating that a FROM clause is required. SQL syntax demands that every SELECT statement specify its data source, making the FROM clause non-optional in our grammar.</p>\n<p>Table name parsing follows similar patterns to column name parsing but with different semantic meaning. The parser expects an <code>IDENTIFIER</code> token that represents the table name, creating an <code>Identifier</code> AST node to hold the table reference. For databases that support schema qualification, the parser uses lookahead to detect <code>schema.table</code> patterns, creating a <code>QualifiedIdentifier</code> node when a DOT token follows the first identifier. This allows the AST to preserve schema information for query execution engines that need to resolve table references across multiple schemas.</p>\n<p>Table aliasing provides a mechanism for referencing tables with shorter names or disambiguating multiple references to the same table (in self-joins or subqueries). The parser handles both explicit aliases (<code>FROM users AS u</code>) and implicit aliases (<code>FROM users u</code>) using the same lookahead strategy as column aliases. However, table alias parsing has a simpler termination condition because the FROM clause is typically followed by easily recognizable keywords like WHERE, ORDER BY, or statement end.</p>\n<p>The table reference parser must also handle error cases gracefully. Common errors include missing table names after the FROM keyword, invalid characters in table identifiers, and malformed schema qualifiers. Each error case generates specific error messages that help developers identify and fix syntax issues. The parser tracks position information throughout the process to provide accurate error locations.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Table aliases become crucial for self-joins and complex queries, even though our initial parser only handles simple single-table SELECT statements. By building alias support into the fundamental table reference parsing, we create a foundation that easily extends to support multi-table queries in future parser versions.</p>\n</blockquote>\n<h3 id=\"architecture-decision-look-ahead-strategy\">Architecture Decision: Look-ahead Strategy</h3>\n<p>One of the most critical architectural decisions in recursive descent parser design involves determining how much token look-ahead the parser needs to make correct parsing decisions. This decision affects both parser complexity and the types of SQL syntax the parser can handle unambiguously. Our parser must balance implementation simplicity with the ability to correctly distinguish between syntactically similar constructs.</p>\n<blockquote>\n<p><strong>Decision: Single Token Look-ahead with Backtrack-free Parsing</strong></p>\n<ul>\n<li><strong>Context</strong>: SQL syntax contains several ambiguous constructs where the parser cannot determine the correct interpretation without examining future tokens. Column aliases without AS keywords (<code>SELECT name full_name</code>) look identical to column lists (<code>SELECT name, full_name</code>) until the parser sees the next token. Similarly, qualified identifiers (<code>table.column</code>) cannot be distinguished from separate identifiers until the DOT token is examined.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>No look-ahead</strong>: Make parsing decisions based solely on the current token, requiring strict syntax rules that eliminate ambiguity</li>\n<li><strong>Single token look-ahead</strong>: Examine the next token when needed to resolve ambiguous constructs, allowing more flexible SQL syntax</li>\n<li><strong>Unlimited look-ahead with backtracking</strong>: Allow the parser to examine arbitrarily many future tokens and backtrack when parsing decisions prove incorrect</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Single token look-ahead with carefully designed grammar rules that avoid backtracking requirements</li>\n<li><strong>Rationale</strong>: Single token look-ahead provides sufficient power to handle all standard SQL constructs in our scope while maintaining parser simplicity and performance predictability. Most SQL ambiguities resolve within one token of context. Unlimited look-ahead adds significant complexity for minimal benefit given our parser&#39;s scope, while no look-ahead forces overly restrictive syntax rules that don&#39;t match standard SQL expectations.</li>\n<li><strong>Consequences</strong>: The parser can handle standard SQL syntax naturally while maintaining O(n) linear time complexity. However, some exotic SQL constructs that require deeper look-ahead may need grammar modifications or may be unsupported. Parser functions must be carefully designed to avoid look-ahead conflicts.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Look-ahead Strategy</th>\n<th>Parsing Power</th>\n<th>Implementation Complexity</th>\n<th>Performance</th>\n<th>Memory Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No look-ahead</td>\n<td>Limited - requires restrictive grammar</td>\n<td>Very Low</td>\n<td>Excellent - O(n)</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>Single token</td>\n<td>High - handles most SQL constructs</td>\n<td>Low-Medium</td>\n<td>Excellent - O(n)</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Unlimited + backtrack</td>\n<td>Maximum - any context-free grammar</td>\n<td>High</td>\n<td>Variable - O(n) to O(n²)</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<p>The single token look-ahead strategy requires careful design of parsing functions to use look-ahead consistently and efficiently. Our parser implements look-ahead through a <code>peek_token()</code> method that examines the next token without consuming it, allowing parsing functions to make informed decisions about syntax disambiguation. This approach avoids the complexity and performance overhead of backtracking while providing sufficient parsing power for standard SQL constructs.</p>\n<p>Look-ahead usage follows specific patterns throughout the parser. When parsing column specifications, the parser uses <code>peek_token()</code> to distinguish between qualified identifiers (next token is DOT) and simple identifiers (next token is not DOT). When handling aliases, it looks ahead to distinguish between explicit column separators (next token is COMMA or FROM keyword) and implicit aliases (next token is an identifier). These patterns create predictable look-ahead usage that doesn&#39;t cascade into complex decision trees.</p>\n<h3 id=\"common-select-parser-pitfalls\">Common SELECT Parser Pitfalls</h3>\n<p>Implementing a recursive descent parser for SELECT statements involves several subtle challenges that frequently trip up developers. These pitfalls often stem from the mismatch between SQL&#39;s flexible syntax and the structured approach required by recursive descent parsing. Understanding these common mistakes and their solutions helps developers build robust parsers that handle real-world SQL correctly.</p>\n<p>⚠️ <strong>Pitfall: Token Consumption Tracking Errors</strong></p>\n<p>One of the most frequent mistakes involves inconsistent token consumption - either forgetting to advance the parser position after recognizing expected tokens, or accidentally consuming tokens multiple times. This creates parsing errors where the parser becomes out of sync with the token stream, leading to unexpected token errors or infinite parsing loops.</p>\n<p>The root cause typically lies in parsing functions that check for expected tokens but forget to call the token advance method, or functions that advance the parser position speculatively and fail to handle backtracking correctly. For example, a column list parser might check for a COMMA token to continue parsing additional columns, but forget to consume the comma before calling <code>parse_column_spec()</code> for the next column.</p>\n<p>To avoid this pitfall, establish consistent token handling patterns throughout all parsing functions. Every token recognition should immediately be followed by token consumption. Use helper methods like <code>expect_token(token_type)</code> that both verify the expected token type and advance the parser position atomically. This prevents the separation between token checking and token consumption that leads to synchronization errors.</p>\n<p>⚠️ <strong>Pitfall: Incomplete AST Node Construction</strong></p>\n<p>Another common mistake involves creating AST nodes with missing or incorrect child node assignments. This typically happens when parsing functions successfully parse all required elements but fail to properly attach parsed sub-elements to the parent AST node, or when optional elements are handled inconsistently.</p>\n<p>For example, a SELECT statement parser might successfully parse the column list, FROM clause, and WHERE clause, but forget to assign the WHERE clause AST node to the <code>SelectStatement</code> object&#39;s where_clause field. This creates an AST that loses important query information, even though parsing completed without syntax errors.</p>\n<p>The solution involves creating comprehensive AST node constructors that require all necessary child nodes as parameters, making it impossible to create incomplete AST nodes. Additionally, use builder patterns for complex AST nodes with optional elements, ensuring that all parsed elements are properly incorporated into the final node structure.</p>\n<p>⚠️ <strong>Pitfall: Look-ahead Token Consumption</strong></p>\n<p>A subtle but critical error occurs when parsing functions accidentally consume look-ahead tokens instead of just examining them. This happens when developers use the token advance method instead of the peek method for look-ahead operations, causing the parser to skip tokens and misinterpret subsequent syntax.</p>\n<p>For instance, when parsing column aliases, a function might advance the parser position to check if the next token is an identifier (indicating an implicit alias), but forget that advancing the position consumes the token. If the token turns out not to be an alias, the parser has already moved past it and cannot recover the correct parsing position.</p>\n<p>Prevent this pitfall by maintaining strict separation between look-ahead operations (which only examine tokens) and parsing operations (which consume tokens). Use clearly named methods like <code>peek_token()</code> for examination and <code>consume_token()</code> for advancement. Never mix these operations within a single logical parsing step.</p>\n<p>⚠️ <strong>Pitfall: Error Recovery State Corruption</strong></p>\n<p>When parsing errors occur, poorly designed error handling can leave the parser in an inconsistent state that causes cascading errors or incorrect error messages. This typically happens when parsing functions partially modify parser state before encountering errors, then fail to properly clean up or reset to a known good state.</p>\n<p>A column list parser might successfully parse several columns, adding them to a result list, then encounter a syntax error on the fourth column. If the error handling doesn&#39;t properly reset the parser state, subsequent parsing attempts might include the partial results from the failed parse, leading to confused error messages or incorrect AST structures.</p>\n<p>Address this pitfall by implementing atomic parsing operations that either succeed completely or fail without side effects. Use temporary data structures during parsing and only commit results to the final AST after successful completion of entire grammar rules. This creates clean error boundaries that don&#39;t pollute subsequent parsing attempts.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Fselect-parsing-flow.svg\" alt=\"SELECT Statement Parsing Flow\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The SELECT parser implementation requires careful coordination between token stream management, recursive descent parsing logic, and AST node construction. This section provides the concrete Python implementation framework that transforms the design concepts into working code, with particular attention to the parsing patterns and error handling strategies that make the parser robust and maintainable.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Stream</td>\n<td>Simple list index tracking</td>\n<td>Iterator-based stream with buffering</td>\n</tr>\n<tr>\n<td>AST Construction</td>\n<td>Direct object instantiation</td>\n<td>Builder pattern with validation</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception-based with position tracking</td>\n<td>Error collection with recovery attempts</td>\n</tr>\n<tr>\n<td>Look-ahead</td>\n<td>Single token peek with manual management</td>\n<td>Configurable look-ahead buffer</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql_parser/\n  parser/\n    __init__.py                    ← Parser module exports\n    select_parser.py              ← SELECT statement parsing (this component)\n    base_parser.py                ← Common parsing utilities and base classes\n    expression_parser.py          ← WHERE clause expression parsing (milestone 3)\n    dml_parser.py                 ← INSERT/UPDATE/DELETE parsing (milestone 4)\n  ast/\n    __init__.py                   ← AST node exports\n    nodes.py                      ← AST node class definitions\n    visitors.py                   ← AST traversal and manipulation\n  tokenizer/\n    __init__.py                   ← Tokenizer exports (from milestone 1)\n    tokenizer.py                  ← Token generation (from milestone 1)\n  errors/\n    __init__.py                   ← Error class exports\n    parse_errors.py               ← Parsing error definitions\n  tests/\n    test_select_parser.py         ← SELECT parser unit tests\n    test_integration.py           ← End-to-end parsing tests</code></pre></div>\n\n<p><strong>Core Parsing Infrastructure (Complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># base_parser.py - Common utilities for all parser components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tokenizer.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, TokenType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..errors.parse_errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> UnexpectedTokenError, </span><span style=\"color:#79B8FF\">SyntaxError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..ast.nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ASTNode, SourceLocation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class providing common parsing utilities and token stream management.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokens: List[Token]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek_token</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> Optional[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look ahead at future tokens without consuming them.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peek_pos </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> offset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens[peek_pos] </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> peek_pos </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tokens) </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> consume_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Advance parser position and return the consumed token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        consumed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.position] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tokens) </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> consumed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> expect_token</span><span style=\"color:#E1E4E8\">(self, expected_type: TokenType) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify current token matches expected type and consume it.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected_type:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#E1E4E8\"> UnexpectedTokenError(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.current_token.type.name </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'EOF'</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.current_token.line </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.current_token.column </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                expected_type.name,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.current_token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.consume_token()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if parser has reached end of token stream.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tokens)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_source_location</span><span style=\"color:#E1E4E8\">(self, start_token: Token, end_token: Optional[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> SourceLocation:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create source location from token positions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        end </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> end_token </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> start_token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> SourceLocation(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            start_line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">start_token.line,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            start_column</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">start_token.column,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            end_line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">end.line,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            end_column</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">end.column </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(end.value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<p><strong>SELECT Parser Core Logic (Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># select_parser.py - SELECT statement recursive descent parser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .base_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tokenizer.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..ast.nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SelectStatement, Identifier, QualifiedIdentifier, StarExpression,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AliasedExpression, TableReference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..errors.parse_errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> SyntaxError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SelectParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseParser</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Recursive descent parser for SELECT statements following SQL grammar rules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_select_statement</span><span style=\"color:#E1E4E8\">(self) -> SelectStatement:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse complete SELECT statement: SELECT column_list FROM table_reference [WHERE expression]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Grammar: select_statement ::= SELECT column_list FROM table_reference [WHERE expression]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns: SelectStatement AST node with all parsed components</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Verify current token is SELECT_KEYWORD and consume it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse column list by calling parse_column_list()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify FROM_KEYWORD is present and consume it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse table reference by calling parse_table_reference()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for optional WHERE_KEYWORD and parse WHERE clause if present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create SelectStatement AST node with all parsed components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Set source location information from start/end tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use expect_token() for required keywords, peek_token() for optional clauses</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_column_list</span><span style=\"color:#E1E4E8\">(self) -> Union[StarExpression, List[AliasedExpression]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse column list: either '*' or comma-separated column specifications</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Grammar: column_list ::= '*' | column_spec (',' column_spec)*</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns: StarExpression for SELECT *, or list of column specifications</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if current token is ASTERISK for SELECT * case</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If asterisk, consume token and return StarExpression AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Otherwise, initialize empty column list for explicit columns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse first column specification by calling parse_column_spec()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Loop while current token is COMMA - consume comma and parse next column</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return list of parsed column specifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use peek_token() to check for FROM_KEYWORD to end column list parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_column_spec</span><span style=\"color:#E1E4E8\">(self) -> AliasedExpression:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse individual column specification with optional alias</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Grammar: column_spec ::= [table_name '.'] column_name [AS alias | alias]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns: AliasedExpression wrapping Identifier or QualifiedIdentifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse base identifier (could be column name or table qualifier)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if next token is DOT for qualified name (table.column)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If DOT found, consume it and parse actual column name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create Identifier or QualifiedIdentifier based on qualification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for alias - either AS keyword followed by identifier, or just identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create AliasedExpression with parsed column and optional alias</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use peek_token() to distinguish aliases from next clause keywords</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_table_reference</span><span style=\"color:#E1E4E8\">(self) -> TableReference:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse table reference with optional alias</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Grammar: table_reference ::= [schema_name '.'] table_name [AS alias | alias]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns: TableReference AST node with table identifier and optional alias</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse base identifier (could be table name or schema qualifier)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if next token is DOT for schema-qualified table (schema.table)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If DOT found, consume it and parse actual table name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create Identifier or QualifiedIdentifier for table reference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for optional table alias using same logic as column aliases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create and return TableReference AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Table aliases help with self-joins and complex queries in future extensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_identifier</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse identifier token and return its string value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns: String value of identifier token</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises: UnexpectedTokenError if current token is not IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Verify current token is IDENTIFIER type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Consume the identifier token and return its value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use expect_token() for automatic type checking and consumption</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Python Hints:</strong></p>\n<ul>\n<li>Use <code>dataclasses</code> or <code>attrs</code> for AST node definitions to reduce boilerplate and get automatic <code>__init__</code>, <code>__repr__</code>, and comparison methods</li>\n<li>Leverage <code>typing.Union</code> for parsing methods that can return different AST node types based on syntax variations</li>\n<li>Use <code>enum.Enum</code> for token types to get automatic string representation and comparison safety</li>\n<li>Consider <code>contextlib.contextmanager</code> for parsing state management that automatically handles cleanup on errors</li>\n<li>Use <code>pytest.parametrize</code> for testing multiple SQL statement variations with the same test logic</li>\n</ul>\n<p><strong>Milestone 2 Checkpoint:</strong></p>\n<p>After implementing the SELECT parser, verify functionality with these specific tests:</p>\n<ol>\n<li><p><strong>Basic SELECT parsing</strong>: <code>python -m pytest tests/test_select_parser.py::test_basic_select -v</code></p>\n<ul>\n<li>Expected: All tests pass showing correct AST node creation for simple SELECT statements</li>\n<li>Verify: <code>SELECT name FROM users</code> creates SelectStatement with Identifier column and TableReference</li>\n</ul>\n</li>\n<li><p><strong>Column list variations</strong>: Test these statements manually in a Python REPL:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> sql_parser.parser.select_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SelectParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> sql_parser.tokenizer.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Test star wildcard</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tokenizer(</span><span style=\"color:#9ECBFF\">\"SELECT * FROM users\"</span><span style=\"color:#E1E4E8\">).tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SelectParser(tokens).parse_select_statement()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(ast.columns, StarExpression)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Test multiple columns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tokenizer(</span><span style=\"color:#9ECBFF\">\"SELECT name, email, age FROM users\"</span><span style=\"color:#E1E4E8\">).tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SelectParser(tokens).parse_select_statement()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.columns) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><p><strong>Alias parsing</strong>: Verify both explicit and implicit aliases work correctly</p>\n<ul>\n<li><code>SELECT name AS full_name FROM users AS u</code> should parse without errors</li>\n<li><code>SELECT name full_name FROM users u</code> should create the same AST structure</li>\n</ul>\n</li>\n<li><p><strong>Error reporting</strong>: Test invalid syntax produces helpful error messages:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Missing FROM keyword should raise UnexpectedTokenError</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tokenizer(</span><span style=\"color:#9ECBFF\">\"SELECT name WHERE id = 1\"</span><span style=\"color:#E1E4E8\">).tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       SelectParser(tokens).parse_select_statement()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       assert</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Should have raised UnexpectedTokenError\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   except</span><span style=\"color:#E1E4E8\"> UnexpectedTokenError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       assert</span><span style=\"color:#9ECBFF\"> \"Expected FROM_KEYWORD\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(e)</span></span></code></pre></div>\n\n<p><strong>Signs of Implementation Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parser hangs in infinite loop</td>\n<td>Token consumption not advancing position</td>\n<td>Add debug prints showing current token and position</td>\n<td>Ensure every parsing method advances position or exits loop</td>\n</tr>\n<tr>\n<td>&quot;Unexpected EOF&quot; errors</td>\n<td>Parser consuming more tokens than available</td>\n<td>Check <code>is_at_end()</code> before token access</td>\n<td>Add bounds checking in all token access methods</td>\n</tr>\n<tr>\n<td>Wrong AST node types in tree</td>\n<td>Incorrect node construction in parsing methods</td>\n<td>Print AST structure after parsing</td>\n<td>Verify each parsing method returns documented node type</td>\n</tr>\n<tr>\n<td>Missing child nodes in AST</td>\n<td>Parsed elements not assigned to parent nodes</td>\n<td>Walk AST tree and print all child relationships</td>\n<td>Check all AST node constructors receive parsed children</td>\n</tr>\n</tbody></table>\n<h2 id=\"where-clause-expression-parser-design\">WHERE Clause Expression Parser Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (WHERE Clause) - This section provides the detailed design for parsing complex expressions with proper operator precedence, building upon the tokenizer from Milestone 1 and the basic parsing infrastructure from Milestone 2.</p>\n</blockquote>\n<h3 id=\"mental-model-mathematical-expression-evaluation\">Mental Model: Mathematical Expression Evaluation</h3>\n<p>Understanding expression parsing requires thinking about how humans naturally process mathematical expressions and apply precedence rules. When you see the expression <code>2 + 3 * 4</code>, you instinctively know to multiply first, then add, yielding 14 rather than 20. This intuitive understanding comes from years of learning mathematical precedence rules that dictate the order of operations.</p>\n<p>SQL WHERE clause expressions work identically to mathematical expressions, but with additional operators for comparisons (<code>=</code>, <code>&lt;</code>, <code>&gt;</code>), logical operations (<code>AND</code>, <code>OR</code>, <code>NOT</code>), and SQL-specific constructs like <code>IS NULL</code>. Just as mathematical expressions form tree structures where higher-precedence operations appear deeper in the tree, SQL expressions create Abstract Syntax Tree nodes where the root represents the lowest-precedence operation and leaves represent the highest-precedence operations or literal values.</p>\n<p>Consider the WHERE clause <code>age &gt; 18 AND salary &gt;= 50000 OR department = &#39;Engineering&#39;</code>. A human reader naturally groups this as <code>((age &gt; 18) AND (salary &gt;= 50000)) OR (department = &#39;Engineering&#39;)</code> because comparison operators (<code>&gt;</code>, <code>&gt;=</code>, <code>=</code>) bind more tightly than logical operators (<code>AND</code>), and <code>AND</code> binds more tightly than <code>OR</code>. The expression parser must replicate this human understanding by building an AST where the <code>OR</code> node sits at the root, with an <code>AND</code> node as its left child and an equality comparison as its right child.</p>\n<p>The key insight is that <strong>precedence determines tree depth</strong>. Higher-precedence operators appear deeper in the tree because they must be evaluated first. Lower-precedence operators appear closer to the root because they operate on the results of higher-precedence sub-expressions. Parentheses override natural precedence by forcing certain sub-expressions to be treated as atomic units, effectively &quot;raising&quot; their precedence to the highest level.</p>\n<p>This mental model guides our algorithm design: we need a parsing strategy that naturally builds trees where precedence determines depth, handles parentheses as precedence overrides, and processes left-to-right evaluation for operators of equal precedence (associativity).</p>\n<h3 id=\"sql-operator-precedence-rules\">SQL Operator Precedence Rules</h3>\n<p>SQL expressions in WHERE clauses involve multiple categories of operators, each with specific precedence and associativity rules. Understanding these rules is crucial because they determine how ambiguous expressions are resolved and how the AST should be structured.</p>\n<p>The following table presents the complete operator precedence hierarchy for SQL expressions, ordered from highest precedence (evaluated first) to lowest precedence (evaluated last):</p>\n<table>\n<thead>\n<tr>\n<th>Precedence Level</th>\n<th>Operators</th>\n<th>Associativity</th>\n<th>Description</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1 (Highest)</td>\n<td><code>()</code></td>\n<td>N/A</td>\n<td>Parentheses override all precedence</td>\n<td><code>(age + 5) * 2</code></td>\n</tr>\n<tr>\n<td>2</td>\n<td><code>+</code>, <code>-</code> (unary)</td>\n<td>Right</td>\n<td>Unary plus and minus</td>\n<td><code>-salary</code>, <code>+bonus</code></td>\n</tr>\n<tr>\n<td>3</td>\n<td><code>*</code>, <code>/</code>, <code>%</code></td>\n<td>Left</td>\n<td>Multiplicative arithmetic</td>\n<td><code>salary * 0.1</code></td>\n</tr>\n<tr>\n<td>4</td>\n<td><code>+</code>, <code>-</code> (binary)</td>\n<td>Left</td>\n<td>Additive arithmetic</td>\n<td><code>base_pay + bonus</code></td>\n</tr>\n<tr>\n<td>5</td>\n<td><code>=</code>, <code>!=</code>, <code>&lt;&gt;</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code></td>\n<td>Left</td>\n<td>Comparison operators</td>\n<td><code>age &gt;= 21</code></td>\n</tr>\n<tr>\n<td>6</td>\n<td><code>IS NULL</code>, <code>IS NOT NULL</code></td>\n<td>Left</td>\n<td>NULL testing</td>\n<td><code>email IS NOT NULL</code></td>\n</tr>\n<tr>\n<td>7</td>\n<td><code>NOT</code></td>\n<td>Right</td>\n<td>Logical negation</td>\n<td><code>NOT active</code></td>\n</tr>\n<tr>\n<td>8</td>\n<td><code>AND</code></td>\n<td>Left</td>\n<td>Logical conjunction</td>\n<td><code>age &gt; 18 AND active</code></td>\n</tr>\n<tr>\n<td>9 (Lowest)</td>\n<td><code>OR</code></td>\n<td>Left</td>\n<td>Logical disjunction</td>\n<td><code>admin OR manager</code></td>\n</tr>\n</tbody></table>\n<p>The <strong>associativity</strong> column determines how operators of equal precedence are grouped. Left associativity means <code>a + b + c</code> groups as <code>(a + b) + c</code>, while right associativity means <code>a = b = c</code> would group as <code>a = (b = c)</code> if SQL supported chained assignments (which it doesn&#39;t in WHERE clauses).</p>\n<p>Several important edge cases and special rules apply to SQL operator precedence:</p>\n<p><strong>Comparison Chaining</strong>: Unlike mathematical expressions, SQL does not allow chained comparisons like <code>18 &lt; age &lt; 65</code>. Each comparison must be explicit: <code>age &gt; 18 AND age &lt; 65</code>. The parser should detect and reject chained comparisons with appropriate error messages.</p>\n<p><strong>NULL Handling</strong>: SQL uses three-valued logic where expressions can evaluate to TRUE, FALSE, or NULL. The <code>IS NULL</code> and <code>IS NOT NULL</code> operators have special precedence because they bind tightly to their operand. The expression <code>salary + bonus IS NULL</code> groups as <code>(salary + bonus) IS NULL</code>, not <code>salary + (bonus IS NULL)</code>.</p>\n<p><strong>Negation Scope</strong>: The <code>NOT</code> operator has right associativity and applies to the immediately following expression. <code>NOT age &gt; 18 AND active</code> groups as <code>(NOT (age &gt; 18)) AND active</code>, not <code>NOT ((age &gt; 18) AND active)</code>. This frequently causes logical errors in query writing.</p>\n<p><strong>Parentheses Override</strong>: Parentheses have the highest precedence and create atomic sub-expressions. The parser must handle nested parentheses correctly and detect unmatched parentheses as syntax errors.</p>\n<p>Understanding these precedence rules enables the expression parser to build AST structures that correctly represent the intended evaluation order. The precedence climbing algorithm uses these numeric precedence levels to determine when to recurse deeper into the expression tree versus when to return control to handle lower-precedence operators.</p>\n<h3 id=\"precedence-climbing-parser-algorithm\">Precedence Climbing Parser Algorithm</h3>\n<p>The <strong>precedence climbing algorithm</strong> provides an elegant solution to expression parsing that naturally handles operator precedence and associativity without requiring complex grammar transformations or separate parsing phases. This algorithm works by recursively parsing sub-expressions while maintaining a minimum precedence threshold that determines which operators can be consumed at each recursion level.</p>\n<p>The core insight behind precedence climbing is that higher-precedence operators should be parsed by deeper recursive calls, while lower-precedence operators are handled by shallower calls. By passing a minimum precedence parameter to each recursive call, we ensure that operators below this threshold are left for parent calls to handle, while operators at or above the threshold are consumed by the current call.</p>\n<p>The algorithm follows these fundamental steps for parsing any expression:</p>\n<ol>\n<li><p><strong>Parse Primary Expression</strong>: Begin by parsing a primary expression (literal, identifier, or parenthesized expression) that serves as the left operand for any following binary operations.</p>\n</li>\n<li><p><strong>Enter Binary Operation Loop</strong>: Check if the current token is a binary operator and compare its precedence to the minimum precedence threshold passed to this function call.</p>\n</li>\n<li><p><strong>Precedence Decision</strong>: If the operator&#39;s precedence is below the minimum threshold, return the current expression without consuming the operator (let a parent call handle it). If at or above threshold, consume the operator and continue.</p>\n</li>\n<li><p><strong>Parse Right Operand</strong>: Recursively parse the right operand with an adjusted minimum precedence based on the current operator&#39;s precedence and associativity.</p>\n</li>\n<li><p><strong>Build AST Node</strong>: Create a binary operation AST node with the left expression, operator, and right expression as children.</p>\n</li>\n<li><p><strong>Continue or Return</strong>: Repeat the loop to handle additional operators of sufficient precedence, or return the completed expression.</p>\n</li>\n</ol>\n<p>The following table shows how the algorithm handles different parsing scenarios:</p>\n<table>\n<thead>\n<tr>\n<th>Current Min Precedence</th>\n<th>Next Operator</th>\n<th>Operator Precedence</th>\n<th>Action Taken</th>\n<th>Reason</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>5</td>\n<td><code>+</code></td>\n<td>4</td>\n<td>Consume operator, recurse</td>\n<td>Operator meets threshold</td>\n</tr>\n<tr>\n<td>5</td>\n<td><code>*</code></td>\n<td>3</td>\n<td>Consume operator, recurse</td>\n<td>Higher precedence than threshold</td>\n</tr>\n<tr>\n<td>5</td>\n<td><code>OR</code></td>\n<td>9</td>\n<td>Return current expression</td>\n<td>Below threshold, parent handles</td>\n</tr>\n<tr>\n<td>3</td>\n<td><code>AND</code></td>\n<td>8</td>\n<td>Return current expression</td>\n<td>Below threshold, parent handles</td>\n</tr>\n<tr>\n<td>8</td>\n<td><code>AND</code></td>\n<td>8</td>\n<td>Consume operator, recurse</td>\n<td>Meets threshold exactly</td>\n</tr>\n</tbody></table>\n<p><strong>Associativity Handling</strong>: Left-associative operators increment the minimum precedence for the right operand recursive call (<code>min_precedence = current_precedence + 1</code>), ensuring that operators of equal precedence in the right operand are handled by this call rather than the recursive call. Right-associative operators use the same precedence (<code>min_precedence = current_precedence</code>), allowing equal-precedence operators to be consumed by the recursive call.</p>\n<p><strong>Unary Operator Integration</strong>: Unary operators are handled during primary expression parsing rather than in the binary operator loop. When encountering a unary operator token (<code>NOT</code>, unary <code>-</code>, unary <code>+</code>), the parser recursively calls the expression parser with the unary operator&#39;s precedence, then wraps the result in a <code>UnaryOperation</code> AST node.</p>\n<p><strong>Parentheses Processing</strong>: Parentheses are treated as primary expressions with maximum precedence. When encountering an opening parenthesis, the parser recursively calls the expression parser with minimum precedence (allowing all operators), consumes the closing parenthesis, and returns the sub-expression as an atomic unit.</p>\n<p>The precedence climbing approach elegantly handles complex expressions like <code>NOT active AND salary &gt; base_pay + bonus * rate OR manager</code> by naturally building the correct AST structure through recursive precedence-based decisions, without requiring the parser to explicitly encode complex grammar rules or perform multiple parsing passes.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Fexpression-parsing-sequence.svg\" alt=\"WHERE Clause Expression Parsing Sequence\"></p>\n<h3 id=\"expression-type-handling\">Expression Type Handling</h3>\n<p>SQL WHERE clause expressions encompass several distinct categories of operations, each requiring specialized parsing logic while fitting within the overall precedence climbing framework. The expression parser must recognize and correctly handle each expression type to build appropriate AST nodes and validate syntax rules specific to each category.</p>\n<p><strong>Comparison Expressions</strong> form the foundation of most WHERE clauses, testing relationships between values using operators like <code>=</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, and <code>&gt;=</code>. These expressions always produce boolean results and require two operands of compatible types. The parser creates <code>BinaryOperation</code> AST nodes with the comparison operator and validates that both operands are valid expressions.</p>\n<table>\n<thead>\n<tr>\n<th>Comparison Type</th>\n<th>Operator</th>\n<th>AST Node Structure</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Equality</td>\n<td><code>=</code>, <code>!=</code>, <code>&lt;&gt;</code></td>\n<td><code>BinaryOperation(left, op, right)</code></td>\n<td>Compatible operand types</td>\n</tr>\n<tr>\n<td>Ordering</td>\n<td><code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code></td>\n<td><code>BinaryOperation(left, op, right)</code></td>\n<td>Comparable operand types</td>\n</tr>\n<tr>\n<td>Pattern Matching</td>\n<td><code>LIKE</code> (future)</td>\n<td><code>BinaryOperation(left, LIKE, pattern)</code></td>\n<td>Left: string, Right: pattern</td>\n</tr>\n</tbody></table>\n<p><strong>Logical Expressions</strong> combine boolean values using <code>AND</code>, <code>OR</code>, and <code>NOT</code> operators, implementing SQL&#39;s three-valued logic where expressions can evaluate to TRUE, FALSE, or NULL. The <code>AND</code> and <code>OR</code> operators create <code>BinaryOperation</code> nodes, while <code>NOT</code> creates <code>UnaryOperation</code> nodes. These operators have specific short-circuit evaluation semantics that affect query optimization.</p>\n<p><strong>NULL Testing Expressions</strong> use the special <code>IS NULL</code> and <code>IS NOT NULL</code> operators to test for NULL values, which cannot be tested using regular comparison operators due to SQL&#39;s three-valued logic. These expressions require special parsing because they involve multi-token operators (<code>IS NULL</code> is two tokens but represents a single operation).</p>\n<p>The following table shows how different NULL testing expressions are parsed and represented:</p>\n<table>\n<thead>\n<tr>\n<th>Expression Pattern</th>\n<th>Tokens Consumed</th>\n<th>AST Node Type</th>\n<th>Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>column IS NULL</code></td>\n<td><code>IS</code>, <code>NULL</code></td>\n<td><code>UnaryOperation(IS_NULL, column)</code></td>\n<td>Operand must be expression</td>\n</tr>\n<tr>\n<td><code>expr IS NOT NULL</code></td>\n<td><code>IS</code>, <code>NOT</code>, <code>NULL</code></td>\n<td><code>UnaryOperation(IS_NOT_NULL, expr)</code></td>\n<td>Operand must be expression</td>\n</tr>\n<tr>\n<td><code>NOT column IS NULL</code></td>\n<td><code>NOT</code>, <code>column</code>, <code>IS</code>, <code>NULL</code></td>\n<td><code>UnaryOperation(NOT, UnaryOperation(IS_NULL, column))</code></td>\n<td>Nested unary operations</td>\n</tr>\n</tbody></table>\n<p><strong>Arithmetic Expressions</strong> support mathematical calculations using <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>%</code> operators, following standard mathematical precedence rules. These expressions can appear within comparisons (e.g., <code>salary * 1.1 &gt; 50000</code>) and require numeric operands. The parser validates operand types where possible and creates appropriate <code>BinaryOperation</code> nodes.</p>\n<p><strong>Primary Expressions</strong> represent the atomic building blocks of larger expressions: literals, identifiers, and parenthesized sub-expressions. Each primary expression type requires specialized parsing logic:</p>\n<ul>\n<li><p><strong>Literals</strong>: The parser recognizes <code>INTEGER_LITERAL</code>, <code>FLOAT_LITERAL</code>, <code>STRING_LITERAL</code>, and <code>NULL_LITERAL</code> tokens and creates corresponding AST nodes (<code>IntegerLiteral</code>, <code>FloatLiteral</code>, <code>StringLiteral</code>, <code>NullLiteral</code>).</p>\n</li>\n<li><p><strong>Identifiers</strong>: Simple identifiers (<code>column_name</code>) and qualified identifiers (<code>table.column</code>) are parsed into <code>Identifier</code> and <code>QualifiedIdentifier</code> nodes respectively. The parser must distinguish between these forms by looking ahead for dot tokens.</p>\n</li>\n<li><p><strong>Parenthesized Expressions</strong>: Opening parentheses trigger recursive expression parsing with minimum precedence, allowing any operator to be parsed within the parentheses. The parser validates that closing parentheses match opening ones.</p>\n</li>\n</ul>\n<p><strong>Function Calls</strong> (if supported) follow the pattern <code>function_name(arg1, arg2, ...)</code> and require parsing the function name, opening parenthesis, comma-separated argument list, and closing parenthesis. Each argument is a full expression parsed recursively.</p>\n<p>The expression parser dispatches to appropriate specialized parsing functions based on the current token type, ensuring that each expression category receives proper syntactic and semantic validation while maintaining the overall precedence climbing algorithm structure.</p>\n<h3 id=\"parentheses-and-precedence-override\">Parentheses and Precedence Override</h3>\n<p>Parentheses serve as the most powerful precedence override mechanism in SQL expressions, allowing developers to explicitly control evaluation order and create sub-expressions that are treated as atomic units regardless of the natural operator precedence. Understanding how parentheses interact with the precedence climbing algorithm is essential for building correct expression parsers.</p>\n<p><strong>Conceptual Model</strong>: Parentheses effectively &quot;reset&quot; the precedence context, creating an isolated parsing environment where any operators can appear regardless of the minimum precedence threshold of the surrounding context. When the parser encounters an opening parenthesis, it saves the current parsing state, recursively parses the enclosed expression with minimum precedence (allowing all operators), then restores the original context after consuming the closing parenthesis.</p>\n<p>The parentheses parsing process follows these detailed steps:</p>\n<ol>\n<li><p><strong>Recognition</strong>: During primary expression parsing, detect the <code>LEFT_PAREN</code> token type, which indicates the start of a parenthesized sub-expression.</p>\n</li>\n<li><p><strong>Context Save</strong>: Store the current token position and any relevant parser state that might be needed for error recovery.</p>\n</li>\n<li><p><strong>Consume Opening Parenthesis</strong>: Advance the parser position past the <code>LEFT_PAREN</code> token and update position tracking for error reporting.</p>\n</li>\n<li><p><strong>Recursive Expression Parse</strong>: Call the main expression parsing function with minimum precedence level 0 (or the lowest precedence value), allowing any operators to be parsed within the parentheses.</p>\n</li>\n<li><p><strong>Validate Closing Parenthesis</strong>: Expect and consume a <code>RIGHT_PAREN</code> token, generating a specific error message if the closing parenthesis is missing.</p>\n</li>\n<li><p><strong>Return Sub-Expression</strong>: Return the parsed sub-expression as an atomic primary expression that can participate in higher-level operations.</p>\n</li>\n</ol>\n<p><strong>Nested Parentheses Handling</strong>: The recursive nature of the precedence climbing algorithm naturally handles nested parentheses without additional complexity. Each level of nesting creates its own parsing context, and the recursive calls unwind in the correct order as closing parentheses are consumed.</p>\n<p>Consider the expression <code>((age + 5) * 2) &gt; ((salary / 12) + bonus)</code>. The parser handles this through multiple levels of recursive calls:</p>\n<table>\n<thead>\n<tr>\n<th>Nesting Level</th>\n<th>Expression Being Parsed</th>\n<th>Parser Context</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0</td>\n<td><code>((age + 5) * 2) &gt; ((salary / 12) + bonus)</code></td>\n<td>Main expression, min_prec=0</td>\n<td>Binary comparison</td>\n</tr>\n<tr>\n<td>1</td>\n<td><code>(age + 5) * 2</code></td>\n<td>Left operand of <code>&gt;</code>, min_prec=0</td>\n<td>Binary multiplication</td>\n</tr>\n<tr>\n<td>2</td>\n<td><code>age + 5</code></td>\n<td>Within first parentheses, min_prec=0</td>\n<td>Binary addition</td>\n</tr>\n<tr>\n<td>1</td>\n<td><code>(salary / 12) + bonus</code></td>\n<td>Right operand of <code>&gt;</code>, min_prec=0</td>\n<td>Binary addition</td>\n</tr>\n<tr>\n<td>2</td>\n<td><code>salary / 12</code></td>\n<td>Within second parentheses, min_prec=0</td>\n<td>Binary division</td>\n</tr>\n</tbody></table>\n<p><strong>Error Handling for Unbalanced Parentheses</strong>: Parentheses introduce several error conditions that require careful handling and clear error messages:</p>\n<ul>\n<li><p><strong>Missing Closing Parenthesis</strong>: When a parenthesized expression reaches the end of input or encounters an unexpected token before finding the closing parenthesis, generate an error with the position of the opening parenthesis.</p>\n</li>\n<li><p><strong>Unexpected Closing Parenthesis</strong>: When encountering a closing parenthesis outside of a parenthesized context, report that no matching opening parenthesis exists.</p>\n</li>\n<li><p><strong>Empty Parentheses</strong>: SQL expressions cannot contain empty parentheses <code>()</code>, so the parser should detect this condition and provide an appropriate error message.</p>\n</li>\n</ul>\n<p>The following table shows error detection and recovery strategies for parentheses-related issues:</p>\n<table>\n<thead>\n<tr>\n<th>Error Condition</th>\n<th>Detection Method</th>\n<th>Error Message</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing <code>)</code></td>\n<td>End of input during parentheses parsing</td>\n<td>&quot;Expected &#39;)&#39; to match &#39;(&#39; at line X:Y&quot;</td>\n<td>Report error, don&#39;t attempt recovery</td>\n</tr>\n<tr>\n<td>Unexpected <code>)</code></td>\n<td><code>)</code> token outside parentheses context</td>\n<td>&quot;Unexpected &#39;)&#39; - no matching &#39;(&#39;&quot;</td>\n<td>Skip token, continue parsing</td>\n</tr>\n<tr>\n<td>Empty <code>()</code></td>\n<td><code>)</code> immediately follows <code>(</code></td>\n<td>&quot;Empty parentheses are not allowed&quot;</td>\n<td>Skip both tokens, continue parsing</td>\n</tr>\n<tr>\n<td>Nested overflow</td>\n<td>Too many nesting levels</td>\n<td>&quot;Expression nesting too deep&quot;</td>\n<td>Limit recursion depth</td>\n</tr>\n</tbody></table>\n<p><strong>AST Representation</strong>: Parentheses themselves do not create AST nodes because they are purely syntactic constructs that influence parsing behavior without representing semantic operations. The sub-expression within parentheses becomes the AST node, with the parentheses&#39; effect preserved through the correct tree structure that reflects the overridden precedence.</p>\n<p>This approach ensures that semantically equivalent expressions like <code>(a + b) * c</code> and the hypothetical high-precedence version produce identical AST structures, while maintaining the ability to reconstruct the original expression with parentheses when needed for pretty-printing or query optimization analysis.</p>\n<h3 id=\"architecture-decision-precedence-climbing-vs-shunting-yard\">Architecture Decision: Precedence Climbing vs Shunting Yard</h3>\n<p>The choice of expression parsing algorithm significantly impacts the parser&#39;s complexity, maintainability, and extensibility. Two primary algorithms dominate expression parsing: precedence climbing (also known as operator precedence parsing) and the Shunting Yard algorithm. Understanding the trade-offs between these approaches guides the architectural decision for our SQL parser.</p>\n<blockquote>\n<p><strong>Decision: Precedence Climbing Algorithm for Expression Parsing</strong></p>\n<ul>\n<li><strong>Context</strong>: SQL WHERE clause expressions involve multiple operator types with complex precedence and associativity rules. The parsing algorithm must handle unary operators, binary operators, function calls, and parentheses while building correct AST structures. The chosen algorithm affects code maintainability, debugging difficulty, and future extensibility for additional SQL operators.</li>\n<li><strong>Options Considered</strong>: Precedence climbing algorithm, Shunting Yard algorithm with separate AST construction phase, and recursive descent with explicit precedence grammar rules</li>\n<li><strong>Decision</strong>: Implement precedence climbing algorithm for all expression parsing</li>\n<li><strong>Rationale</strong>: Precedence climbing provides the optimal balance of simplicity, directness, and maintainability for our use case. It builds AST nodes during parsing rather than requiring separate phases, integrates naturally with the existing recursive descent parser structure, and offers straightforward debugging and testing capabilities. The algorithm&#39;s recursive nature aligns with our overall parser architecture.</li>\n<li><strong>Consequences</strong>: Enables direct AST construction during parsing, simplifies debugging with clear call stack correlation to expression structure, and allows easy extension for new operators by updating the precedence table. The trade-off is slightly higher memory usage due to recursive calls compared to iterative approaches.</li>\n</ul>\n</blockquote>\n<p>The following comparison table evaluates the key characteristics of each algorithmic approach:</p>\n<table>\n<thead>\n<tr>\n<th>Criteria</th>\n<th>Precedence Climbing</th>\n<th>Shunting Yard</th>\n<th>Recursive Descent Grammar</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Complexity</strong></td>\n<td>Medium - single recursive function</td>\n<td>Medium - two phases (infix→postfix, postfix→AST)</td>\n<td>High - many grammar rules</td>\n</tr>\n<tr>\n<td><strong>AST Construction</strong></td>\n<td>Direct during parsing</td>\n<td>Separate phase after parsing</td>\n<td>Direct during parsing</td>\n</tr>\n<tr>\n<td><strong>Memory Usage</strong></td>\n<td>Medium - recursive call stack</td>\n<td>Low - single evaluation stack</td>\n<td>High - deep call stack</td>\n</tr>\n<tr>\n<td><strong>Debugging Ease</strong></td>\n<td>High - call stack matches expression</td>\n<td>Medium - trace through two phases</td>\n<td>Low - complex control flow</td>\n</tr>\n<tr>\n<td><strong>Extensibility</strong></td>\n<td>High - add operators to precedence table</td>\n<td>Medium - update both phases</td>\n<td>Low - modify grammar rules</td>\n</tr>\n<tr>\n<td><strong>Error Messages</strong></td>\n<td>Excellent - precise location context</td>\n<td>Good - requires position tracking</td>\n<td>Fair - can be vague</td>\n</tr>\n<tr>\n<td><strong>Learning Curve</strong></td>\n<td>Medium - understand precedence concept</td>\n<td>High - understand two algorithms</td>\n<td>High - complex grammar theory</td>\n</tr>\n</tbody></table>\n<p><strong>Precedence Climbing Advantages</strong>: The precedence climbing algorithm excels in several areas critical to our educational SQL parser project. Its single-pass nature eliminates the complexity of coordinating multiple parsing phases while building AST nodes directly at the point where expressions are recognized. This directness greatly simplifies debugging because the recursive call stack directly mirrors the expression&#39;s structural hierarchy - when debugging a parsing error in <code>a + b * c</code>, the call stack shows exactly how <code>b * c</code> was parsed as a sub-expression before being used as the right operand of the addition.</p>\n<p>The algorithm&#39;s extensibility particularly suits our milestone-based development approach. Adding support for new operators (like <code>LIKE</code> or <code>IN</code> in future milestones) requires only adding entries to the operator precedence table and potentially new AST node types. The core parsing logic remains unchanged, reducing the risk of introducing bugs when extending functionality.</p>\n<p><strong>Shunting Yard Considerations</strong>: While the Shunting Yard algorithm offers theoretical elegance and potentially better performance for very complex expressions, it introduces unnecessary complexity for our use case. The two-phase approach (converting infix notation to postfix, then building AST from postfix) creates additional opportunities for bugs and makes debugging more challenging because errors can occur in either phase with less clear relationships between the phases.</p>\n<p><strong>Integration with Recursive Descent</strong>: Precedence climbing integrates seamlessly with our existing recursive descent parser architecture. The expression parser can be called from any point in the grammar that expects an expression (WHERE clauses, SET clause values, etc.) and returns an appropriate AST node. This integration model would be more complex with Shunting Yard due to its multi-phase nature.</p>\n<p><strong>Educational Value</strong>: For developers learning parsing techniques, precedence climbing provides better pedagogical value. The algorithm&#39;s behavior directly reflects how humans think about mathematical expressions and operator precedence, making it more intuitive to understand and debug. Students can easily trace through the algorithm manually to predict its behavior, building confidence in their parser implementation.</p>\n<p>The precedence climbing algorithm thus emerges as the optimal choice for our SQL parser project, balancing implementation complexity with maintainability, debugging ease, and educational clarity while providing a solid foundation for future extensions to more complex SQL expression types.</p>\n<h3 id=\"common-expression-parser-pitfalls\">Common Expression Parser Pitfalls</h3>\n<p>Expression parsing introduces numerous subtle bugs and edge cases that frequently trip up developers, especially those new to parsing techniques. These pitfalls often stem from misunderstanding precedence rules, incorrectly handling associativity, or failing to properly validate expression syntax. Understanding these common mistakes helps avoid them during implementation and provides debugging guidance when issues arise.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Precedence Level Assignment</strong></p>\n<p>Many developers incorrectly assign precedence levels based on intuition rather than formal SQL specification, leading to expressions that parse but evaluate in unexpected order. The most common error is making logical operators (<code>AND</code>, <code>OR</code>) have higher precedence than comparison operators (<code>=</code>, <code>&lt;</code>, <code>&gt;</code>), when the reverse is actually correct.</p>\n<p>Consider the expression <code>age &gt;= 18 AND department = &#39;Sales&#39;</code>. With incorrect precedence where <code>AND</code> binds tighter than <code>=</code>, this would parse as <code>age &gt;= (18 AND department) = &#39;Sales&#39;</code>, which is nonsensical. The correct precedence makes comparisons bind tighter, yielding <code>(age &gt;= 18) AND (department = &#39;Sales&#39;)</code>.</p>\n<p><strong>Fix</strong>: Always consult the official SQL specification for precedence levels. Create comprehensive test cases that verify precedence behavior for all operator combinations. Never guess at precedence levels.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Associativity Rules</strong></p>\n<p>Left and right associativity determine how operators of equal precedence are grouped, but many implementations fail to correctly implement associativity in the precedence climbing algorithm. This typically manifests as incorrect AST structures for expressions with repeated operators.</p>\n<p>The expression <code>10 - 5 - 2</code> should parse as <code>(10 - 5) - 2 = 3</code> with left associativity, but incorrect implementations might parse it as <code>10 - (5 - 2) = 7</code>. The error occurs when the recursive call for the right operand uses the same minimum precedence as the current operator instead of incrementing it for left-associative operators.</p>\n<p><strong>Fix</strong>: For left-associative operators, increment the minimum precedence when recursively parsing the right operand (<code>parse_expression(current_precedence + 1)</code>). For right-associative operators, use the same precedence (<code>parse_expression(current_precedence)</code>).</p>\n<p>⚠️ <strong>Pitfall: Mishandling Multi-Token Operators</strong></p>\n<p>SQL includes operators composed of multiple tokens, such as <code>IS NULL</code>, <code>IS NOT NULL</code>, and <code>NOT LIKE</code>. Naive implementations often treat these as separate operations rather than atomic operators, leading to incorrect parsing and AST structures.</p>\n<p>The expression <code>email IS NOT NULL</code> should create a single <code>UnaryOperation</code> node with operator <code>IS_NOT_NULL</code>, not a <code>UnaryOperation(NOT, UnaryOperation(IS_NULL, email))</code> structure. Treating multi-token operators as separate tokens creates semantic ambiguity and complicates query optimization.</p>\n<p><strong>Fix</strong>: Implement lookahead logic to recognize multi-token operators during tokenization or parsing. Create specific token types for compound operators (<code>IS_NULL_KEYWORD</code>, <code>IS_NOT_NULL_KEYWORD</code>) or handle them as special cases in the expression parser.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Unary Operator Handling</strong></p>\n<p>Unary operators (<code>-</code>, <code>+</code>, <code>NOT</code>) require different parsing logic than binary operators, but many implementations incorrectly try to handle them within the binary operator precedence climbing loop. This leads to parsing failures or incorrect AST structures.</p>\n<p>The expression <code>-salary &gt; 1000</code> should parse as <code>(-salary) &gt; 1000</code>, but incorrect implementations might fail to recognize the unary minus or incorrectly try to parse it as a binary operator with a missing left operand.</p>\n<p><strong>Fix</strong>: Handle unary operators during primary expression parsing, not in the binary operator loop. When encountering a unary operator token, recursively call the expression parser with the unary operator&#39;s precedence level and wrap the result in a <code>UnaryOperation</code> node.</p>\n<p>⚠️ <strong>Pitfall: Poor Parentheses Error Recovery</strong></p>\n<p>Unbalanced parentheses create parsing errors that cascade through the entire expression, often producing confusing error messages that don&#39;t clearly indicate the root problem. Many implementations fail to provide helpful error messages or attempt misguided error recovery that makes the situation worse.</p>\n<p>When encountering missing closing parentheses in <code>(age &gt; 18 AND salary</code>, naive error recovery might attempt to insert the missing parenthesis automatically, leading to incorrect parsing of subsequent expressions and masking the real error location.</p>\n<p><strong>Fix</strong>: Detect unbalanced parentheses at the point where they occur and provide precise error messages with line and column information for both the expected closing parenthesis and the location of the unmatched opening parenthesis. Avoid automatic error recovery for structural syntax errors.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Error Context in Messages</strong></p>\n<p>Expression parsing errors often provide generic messages like &quot;unexpected token&quot; without sufficient context about what the parser expected or where the expression parsing began. This makes debugging difficult, especially in complex nested expressions.</p>\n<p><strong>Fix</strong>: Maintain parsing context throughout expression parsing, including the starting position of the current expression and the type of expression being parsed (WHERE clause, SET clause value, etc.). Include this context in error messages along with specific expectations based on the current parsing state.</p>\n<p>The following table summarizes debugging strategies for common expression parsing symptoms:</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Wrong evaluation order</td>\n<td>Incorrect precedence levels</td>\n<td>Check precedence table against SQL spec</td>\n<td>Update precedence assignments</td>\n</tr>\n<tr>\n<td>Left-associative parsed as right</td>\n<td>Missing precedence increment</td>\n<td>Trace recursive calls for equal operators</td>\n<td>Add +1 to min_precedence for left-assoc</td>\n</tr>\n<tr>\n<td>Multi-token operators fail</td>\n<td>Token-by-token parsing</td>\n<td>Check if <code>IS NULL</code> creates two operations</td>\n<td>Implement compound operator recognition</td>\n</tr>\n<tr>\n<td>Unary operators don&#39;t work</td>\n<td>Binary operator parsing logic</td>\n<td>Verify unary ops handled in primary parsing</td>\n<td>Move to primary expression parser</td>\n</tr>\n<tr>\n<td>Cascading parentheses errors</td>\n<td>Poor error recovery</td>\n<td>Check error message clarity and location</td>\n<td>Improve error reporting, avoid recovery</td>\n</tr>\n<tr>\n<td>Vague error messages</td>\n<td>Missing parsing context</td>\n<td>Review error message content and detail</td>\n<td>Add context about expression type and expectations</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete Python implementation guidance for building the WHERE clause expression parser using the precedence climbing algorithm. The implementation builds upon the tokenizer from Milestone 1 and integrates with the basic parser infrastructure from Milestone 2.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Precedence Table</td>\n<td>Python dictionary mapping operators to integers</td>\n<td>Dataclass with precedence and associativity</td>\n</tr>\n<tr>\n<td>AST Node Creation</td>\n<td>Simple factory functions</td>\n<td>Abstract factory pattern with node registry</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Basic exception raising with message</td>\n<td>Rich error objects with source location</td>\n</tr>\n<tr>\n<td>Expression Validation</td>\n<td>Runtime type checking in AST nodes</td>\n<td>Static analysis during parsing</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure Integration:</strong></p>\n<p>The expression parser integrates into the existing parser module structure established in previous milestones:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql_parser/\n  tokenizer.py              ← Milestone 1: Token definitions and lexical analysis\n  ast_nodes.py             ← AST node classes for all statement and expression types\n  base_parser.py           ← Base parser infrastructure with token management\n  select_parser.py         ← Milestone 2: SELECT statement parsing\n  expression_parser.py     ← NEW: WHERE clause expression parsing (this milestone)\n  parser.py               ← Main parser entry point coordinating all components\n  test_expression_parser.py ← Comprehensive expression parser tests</code></pre></div>\n\n<p><strong>Infrastructure Code - Precedence Table and Operator Definitions:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># expression_parser.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> IntEnum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, TokenType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .ast_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#F97583\"> *</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .base_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseParser, ParseError, UnexpectedTokenError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Precedence</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">IntEnum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Operator precedence levels for expression parsing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Higher numeric values indicate higher precedence (evaluated first).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This enum provides clear naming and easy comparison for precedence levels.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOWEST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">       # Used for parentheses and top-level parsing</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">          # Logical OR - lowest precedence operator</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AND</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">         # Logical AND</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">         # Logical NOT (unary)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IS_NULL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">     # IS NULL, IS NOT NULL operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPARISON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">   # =, !=, &#x3C;>, &#x3C;, &#x3C;=, >, >=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ADDITION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">     # Binary + and - operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MULTIPLICATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 7</span><span style=\"color:#6A737D\">  # *, /, % operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNARY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#6A737D\">       # Unary + and - operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PRIMARY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 9</span><span style=\"color:#6A737D\">     # Literals, identifiers, parentheses</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperatorInfo</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete operator information including precedence and associativity.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This structure allows easy extension for additional operator properties</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    like operator symbol, AST node type, and validation rules.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    precedence: Precedence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_right_associative: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> right_binding_power</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate binding power for right operand in precedence climbing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Left-associative operators increment binding power to ensure</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        equal-precedence operators in right operand are handled by parent call.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Right-associative operators maintain same binding power.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_right_associative:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.precedence.value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.precedence.value </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Complete operator precedence table for SQL expressions</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">OPERATOR_PRECEDENCE</span><span style=\"color:#E1E4E8\">: Dict[TokenType, OperatorInfo] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Logical operators</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">OR_KEYWORD</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">OR</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">AND_KEYWORD</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">AND</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">NOT_KEYWORD</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">NOT</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">is_right_associative</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Comparison operators  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">COMPARISON</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUALS</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">COMPARISON</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">COMPARISON</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">LESS_EQUAL</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">COMPARISON</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">COMPARISON</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">COMPARISON</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Arithmetic operators</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">ADDITION</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">ADDITION</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">MULTIPLY</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">MULTIPLICATION</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">DIVIDE</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">MULTIPLICATION</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TokenType.</span><span style=\"color:#79B8FF\">MODULO</span><span style=\"color:#E1E4E8\">: OperatorInfo(Precedence.</span><span style=\"color:#79B8FF\">MULTIPLICATION</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> is_binary_operator</span><span style=\"color:#E1E4E8\">(token_type: TokenType) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check if token type represents a binary operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> token_type </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> OPERATOR_PRECEDENCE</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> is_unary_operator</span><span style=\"color:#E1E4E8\">(token_type: TokenType) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check if token type can be used as unary operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> token_type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (TokenType.</span><span style=\"color:#79B8FF\">NOT_KEYWORD</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton - Expression Parser Class:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExpressionParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseParser</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Precedence climbing parser for SQL WHERE clause expressions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This parser handles all expression types including comparisons, logical operations,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    arithmetic, literals, identifiers, and parenthesized sub-expressions with correct</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    operator precedence and associativity.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_expression</span><span style=\"color:#E1E4E8\">(self, min_precedence: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Precedence.</span><span style=\"color:#79B8FF\">LOWEST</span><span style=\"color:#E1E4E8\">) -> ASTNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse expression using precedence climbing algorithm.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            min_precedence: Minimum operator precedence to handle at this level.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                           Operators below this level are left for parent calls.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ASTNode representing the parsed expression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            UnexpectedTokenError: When encountering invalid expression syntax</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ParseError: When expression parsing fails due to syntax errors</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse left-hand side primary expression (literal, identifier, or parenthesized)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Enter binary operator parsing loop - continue while operators meet precedence threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each qualifying operator, determine right operand binding power based on associativity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Recursively parse right operand with calculated binding power</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create BinaryOperation AST node with left expr, operator, right expr</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update left expression to newly created binary operation for next iteration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return final expression when no more qualifying operators found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use while loop checking current_token type and precedence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle end-of-input gracefully by checking is_at_end()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_primary_expression</span><span style=\"color:#E1E4E8\">(self) -> ASTNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse primary expressions: literals, identifiers, unary ops, parentheses.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Primary expressions are the atomic building blocks that cannot be broken</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        down further by operator precedence rules. This includes all literal values,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        identifiers, unary operations, and parenthesized sub-expressions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ASTNode representing the primary expression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            UnexpectedTokenError: When current token cannot start primary expression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check current token type to determine primary expression type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle INTEGER_LITERAL -> create IntegerLiteral AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle FLOAT_LITERAL -> create FloatLiteral AST node  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle STRING_LITERAL -> create StringLiteral AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle NULL_KEYWORD -> create NullLiteral AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle IDENTIFIER -> parse as simple or qualified identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Handle unary operators (NOT, +, -) -> recursively parse operand with unary precedence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Handle LEFT_PAREN -> recursively parse inner expression, expect RIGHT_PAREN</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Raise UnexpectedTokenError for any other token type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use token consumption pattern: token = consume_token(), then process token.value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: For parentheses, call parse_expression(Precedence.LOWEST) for inner expression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_identifier_expression</span><span style=\"color:#E1E4E8\">(self) -> ASTNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse identifier that might be qualified (table.column) or simple (column).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handles both simple identifiers and qualified identifiers by looking ahead</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        for dot tokens. Also handles potential IS NULL / IS NOT NULL operators</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        that follow identifiers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Identifier or QualifiedIdentifier AST node</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Consume IDENTIFIER token and store name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if next token is DOT for qualified identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If qualified, consume DOT and next IDENTIFIER, create QualifiedIdentifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If simple, create basic Identifier node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for IS NULL / IS NOT NULL operators following identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return appropriate identifier node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use peek_token() to check for DOT without consuming</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle IS NULL as post-processing of identifier rather than separate operator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_null_test_expression</span><span style=\"color:#E1E4E8\">(self, operand: ASTNode) -> ASTNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse IS NULL or IS NOT NULL operations following an expression.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            operand: Expression being tested for NULL value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            UnaryOperation node for IS NULL or IS NOT NULL</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Expect and consume IS_KEYWORD token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if next token is NOT_KEYWORD for IS NOT NULL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If NOT found, consume it and expect NULL_KEYWORD -> create IS_NOT_NULL operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If no NOT, expect NULL_KEYWORD -> create IS_NULL operation  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return UnaryOperation with appropriate operator type and operand</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Create custom UnaryOperationType enum values for IS_NULL and IS_NOT_NULL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_parenthesized_expression</span><span style=\"color:#E1E4E8\">(self) -> ASTNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse parenthesized sub-expression with precedence override.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parentheses create isolated parsing context where any operators can appear</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        regardless of surrounding minimum precedence. This implements precedence override.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ASTNode representing the sub-expression within parentheses</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            UnexpectedTokenError: When closing parenthesis is missing</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Consume LEFT_PAREN token (already verified by caller)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for empty parentheses () -> raise appropriate error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Recursively call parse_expression(Precedence.LOWEST) for inner expression  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Expect and consume RIGHT_PAREN token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the inner expression (parentheses don't create AST nodes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Save opening parenthesis token position for error reporting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use expect_token(TokenType.RIGHT_PAREN) with descriptive error message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_operator_precedence</span><span style=\"color:#E1E4E8\">(self, token_type: TokenType) -> Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get precedence level for operator token type.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            token_type: Token type to look up</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Precedence level integer, or None if not an operator</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        operator_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> OPERATOR_PRECEDENCE</span><span style=\"color:#E1E4E8\">.get(token_type)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> operator_info.precedence.value </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> operator_info </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_right_associative</span><span style=\"color:#E1E4E8\">(self, token_type: TokenType) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if operator is right-associative.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            token_type: Operator token type</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if right-associative, False if left-associative</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        operator_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> OPERATOR_PRECEDENCE</span><span style=\"color:#E1E4E8\">.get(token_type)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> operator_info.is_right_associative </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> operator_info </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong>Integration with Main Parser:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># In select_parser.py - integrate expression parser for WHERE clauses</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SelectParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseParser</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokens: List[Token]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(tokens)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expression_parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ExpressionParser(tokens)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_where_clause</span><span style=\"color:#E1E4E8\">(self) -> Optional[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse WHERE clause with complete expression support.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Expression AST node, or None if no WHERE clause present</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if current token is WHERE_KEYWORD</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If no WHERE clause, return None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Consume WHERE_KEYWORD token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Delegate to expression parser for condition parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return expression AST node from expression parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: self.expression_parser.parse_expression() handles full WHERE condition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the expression parser, verify correct functionality with these specific tests:</p>\n<p><strong>Test Command</strong>: <code>python -m pytest test_expression_parser.py -v</code></p>\n<p><strong>Expected Behavior Verification:</strong></p>\n<ol>\n<li><strong>Precedence Test</strong>: Parse <code>age &gt; 18 AND salary &gt;= 50000 OR department = &#39;Engineering&#39;</code> and verify AST structure shows <code>OR</code> at root with <code>AND</code> as left child</li>\n<li><strong>Associativity Test</strong>: Parse <code>10 - 5 - 2</code> and verify it creates <code>((10 - 5) - 2)</code> structure, not <code>(10 - (5 - 2))</code></li>\n<li><strong>Parentheses Test</strong>: Parse <code>(age + 5) * 2 &gt; salary</code> and verify multiplication has higher effective precedence than comparison</li>\n<li><strong>NULL Test</strong>: Parse <code>email IS NOT NULL AND active</code> and verify <code>IS NOT NULL</code> creates single unary operation</li>\n<li><strong>Unary Test</strong>: Parse <code>NOT active AND salary &gt; 0</code> and verify <code>NOT</code> applies only to <code>active</code>, not entire expression</li>\n</ol>\n<p><strong>Debug Output</strong>: Enable debug logging to see precedence climbing decisions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Add to parse_expression method for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.debug_enabled:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Parsing with min_precedence=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">min_precedence</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, current_token=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.current_token</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Signs of Issues and Diagnostics:</strong></p>\n<ul>\n<li><strong>Wrong precedence</strong>: Check precedence table values against SQL specification</li>\n<li><strong>Associativity errors</strong>: Verify binding power calculation in recursive calls  </li>\n<li><strong>Parentheses issues</strong>: Ensure recursive call uses <code>Precedence.LOWEST</code> for inner expressions</li>\n<li><strong>Unary operator problems</strong>: Confirm unary operators handled in primary parsing, not binary loop</li>\n<li><strong>Token consumption errors</strong>: Verify <code>consume_token()</code> called exactly once per expected token</li>\n</ul>\n<h2 id=\"data-modification-statement-parser-design\">Data Modification Statement Parser Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 (INSERT/UPDATE/DELETE) - This section provides the detailed design for parsing data modification statements that transform database state, building on the expression parsing capabilities from milestone 3.</p>\n</blockquote>\n<h3 id=\"mental-model-data-manipulation-commands\">Mental Model: Data Manipulation Commands</h3>\n<p>Understanding data modification language (DML) statements requires thinking about them as <strong>database operation commands</strong> rather than queries. While SELECT statements are questions asking &quot;what data exists?&quot;, DML statements are instructions declaring &quot;change the data in this specific way.&quot;</p>\n<p>Consider the analogy of <strong>giving instructions to a warehouse manager</strong>. When you want to retrieve information, you ask questions: &quot;Show me all products from supplier X.&quot; But when you want to modify inventory, you give commands: &quot;Add these new products to shelf 5,&quot; &quot;Update the price of item 123 to $50,&quot; or &quot;Remove all expired items from the refrigerated section.&quot; Each command has a specific structure and requires different validation steps.</p>\n<p>INSERT statements are like <strong>delivery instructions</strong> - they specify what new items to add and where to place them. You must provide both a destination (which shelves/columns) and the actual items (what values). The warehouse manager needs to verify that you&#39;re providing the right number of items for the available shelf spaces.</p>\n<p>UPDATE statements are like <strong>modification orders</strong> - they identify existing items by description (WHERE clause) and specify how to change them (SET clauses). You&#39;re telling the manager &quot;find all items matching these criteria, then change their properties in these ways.&quot;</p>\n<p>DELETE statements are like <strong>removal orders</strong> - they identify items by description and request their elimination. The simplest but most dangerous instruction, since there&#39;s no way to undo a deletion without external backups.</p>\n<p>This mental model helps us understand why DML parsers need different validation logic than SELECT parsers. While SELECT parsing focuses on expression evaluation and result formatting, DML parsing must validate <strong>data integrity constraints</strong> and <strong>structural consistency</strong> between different parts of the statement.</p>\n<h3 id=\"insert-statement-parsing\">INSERT Statement Parsing</h3>\n<p>INSERT statements follow a structured pattern that requires careful coordination between multiple parser components. The basic syntax <code>INSERT INTO table (columns) VALUES (values)</code> appears simple, but contains several parsing challenges that require specialized handling.</p>\n<p>The parser must handle <strong>column list specification</strong> where users can either provide an explicit list of target columns or omit the column specification entirely. When columns are explicitly specified, the parser validates that each identifier refers to a valid column name. When omitted, the parser assumes all columns in their table definition order, which requires the parser to understand default column ordering.</p>\n<p><strong>Multiple value row parsing</strong> presents additional complexity since INSERT statements can specify multiple rows in a single command: <code>INSERT INTO users (name, email) VALUES (&#39;Alice&#39;, &#39;alice@example.com&#39;), (&#39;Bob&#39;, &#39;bob@example.com&#39;)</code>. The parser must track value list boundaries and ensure each row contains the same number of values as the column specification.</p>\n<p>The <code>InsertStatement</code> AST node structure captures this complexity through several key components:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>table_reference</td>\n<td>TableReference</td>\n<td>Target table identifier with optional alias</td>\n</tr>\n<tr>\n<td>column_list</td>\n<td>Optional[List[Identifier]]</td>\n<td>Explicit column names or None for all columns</td>\n</tr>\n<tr>\n<td>value_rows</td>\n<td>List[List[Expression]]</td>\n<td>List of value lists, one per row being inserted</td>\n</tr>\n<tr>\n<td>source_location</td>\n<td>SourceLocation</td>\n<td>Position information for error reporting</td>\n</tr>\n</tbody></table>\n<p>The parsing algorithm follows a structured sequence that validates syntax while building the appropriate AST structure:</p>\n<ol>\n<li><strong>Parse INSERT keyword</strong> and verify the statement begins with the expected token type</li>\n<li><strong>Parse INTO keyword</strong> which is required in SQL INSERT syntax, though some dialects make it optional</li>\n<li><strong>Parse table reference</strong> using the existing table parsing logic from SELECT statements</li>\n<li><strong>Detect column list presence</strong> by looking ahead for an opening parenthesis token</li>\n<li><strong>Parse column list</strong> if present, validating each identifier and handling comma separation</li>\n<li><strong>Parse VALUES keyword</strong> which introduces the actual data values being inserted</li>\n<li><strong>Parse value row list</strong> handling multiple rows separated by commas, with each row containing parenthesized value expressions</li>\n<li><strong>Validate column-value consistency</strong> ensuring each value row contains the same number of expressions as the column specification</li>\n</ol>\n<p>The <strong>column list parsing</strong> logic must handle both explicit and implicit column specifications. When columns are explicitly provided, the parser builds a list of <code>Identifier</code> nodes representing the target columns. When omitted, the parser sets the column_list field to None, indicating that the consuming application should use default column ordering.</p>\n<p><strong>Value expression parsing</strong> within each row leverages the existing expression parser developed for WHERE clauses, since INSERT values can include literals, arithmetic expressions, and function calls. However, the parser must be careful to handle <strong>type validation</strong> at the AST level rather than during parsing, since type checking requires schema information not available to the syntax parser.</p>\n<p><strong>Multi-row value parsing</strong> requires careful state management as the parser alternates between comma tokens separating rows and comma tokens separating values within rows. The parser tracks nesting level using parentheses to distinguish between these two contexts.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: INSERT parsing reuses expression parsing logic from WHERE clauses, but requires additional structural validation to ensure column-value alignment. This demonstrates how the recursive descent architecture enables component reuse across different statement types.</p>\n</blockquote>\n<h3 id=\"update-statement-parsing\">UPDATE Statement Parsing</h3>\n<p>UPDATE statements combine table references, assignment expressions, and conditional filtering in a single command, making them the most syntactically complex DML statement. The parser must coordinate between multiple parsing subsystems while maintaining proper precedence and associativity rules.</p>\n<p>The basic UPDATE syntax <code>UPDATE table SET column = value WHERE condition</code> requires parsing three distinct sections that interact with different parts of the existing parser infrastructure. The SET clause introduces <strong>assignment expressions</strong> which differ from boolean expressions used in WHERE clauses, requiring specialized parsing logic.</p>\n<p>The <code>UpdateStatement</code> AST node captures the structural relationships between these components:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>table_reference</td>\n<td>TableReference</td>\n<td>Target table with optional alias</td>\n</tr>\n<tr>\n<td>set_assignments</td>\n<td>List[AssignmentExpression]</td>\n<td>Column-value assignments from SET clause</td>\n</tr>\n<tr>\n<td>where_clause</td>\n<td>Optional[Expression]</td>\n<td>Filtering condition or None for all rows</td>\n</tr>\n<tr>\n<td>source_location</td>\n<td>SourceLocation</td>\n<td>Position information for error reporting</td>\n</tr>\n</tbody></table>\n<p>The <code>AssignmentExpression</code> AST node represents individual SET clause assignments:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>target_column</td>\n<td>Identifier</td>\n<td>Column being assigned new value</td>\n</tr>\n<tr>\n<td>assigned_value</td>\n<td>Expression</td>\n<td>Expression computing the new value</td>\n</tr>\n<tr>\n<td>source_location</td>\n<td>SourceLocation</td>\n<td>Position information for error reporting</td>\n</tr>\n</tbody></table>\n<p>The parsing algorithm coordinates multiple specialized parsing functions:</p>\n<ol>\n<li><strong>Parse UPDATE keyword</strong> and validate statement type identification</li>\n<li><strong>Parse table reference</strong> using existing table parsing infrastructure</li>\n<li><strong>Parse SET keyword</strong> which introduces the assignment section</li>\n<li><strong>Parse assignment list</strong> handling multiple column assignments separated by commas</li>\n<li><strong>Parse WHERE keyword</strong> if present, as WHERE clauses are optional in UPDATE statements</li>\n<li><strong>Parse conditional expression</strong> using existing WHERE clause parsing logic</li>\n<li><strong>Validate assignment targets</strong> ensuring all assigned columns are valid identifiers</li>\n</ol>\n<p><strong>Assignment expression parsing</strong> requires specialized logic since assignments use single equals (<code>=</code>) rather than comparison operators. The parser must distinguish between assignment context and boolean expression context to handle the equals sign correctly.</p>\n<p>The assignment parsing process follows this sequence for each SET clause entry:</p>\n<ol>\n<li><strong>Parse target column identifier</strong> which must be a simple or qualified column name</li>\n<li><strong>Expect assignment operator</strong> (single equals sign) and consume the token</li>\n<li><strong>Parse assigned value expression</strong> which can include literals, column references, arithmetic, and function calls</li>\n<li><strong>Handle comma separation</strong> between multiple assignments while detecting end of SET clause</li>\n</ol>\n<p><strong>WHERE clause integration</strong> reuses the expression parser from milestone 3, but the UPDATE parser must handle the <strong>optional WHERE clause</strong> carefully. When WHERE is omitted, the update applies to all rows in the table, which can be dangerous but is syntactically valid.</p>\n<p>The parser implements <strong>WHERE clause detection</strong> by checking for the WHERE keyword after parsing all SET assignments. If found, it delegates to the expression parser. If not found, it sets the where_clause field to None and continues parsing any remaining statement components.</p>\n<blockquote>\n<p><strong>Critical Safety Consideration</strong>: UPDATE statements without WHERE clauses affect all table rows. While syntactically valid, this represents a common source of data modification errors. The parser should provide clear AST representation allowing downstream tools to detect and warn about such cases.</p>\n</blockquote>\n<h3 id=\"delete-statement-parsing\">DELETE Statement Parsing</h3>\n<p>DELETE statements represent the simplest DML syntax but require careful safety considerations due to their destructive nature. The basic form <code>DELETE FROM table WHERE condition</code> focuses parsing complexity on the conditional expression rather than structural validation.</p>\n<p>The <code>DeleteStatement</code> AST node reflects this structural simplicity:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>table_reference</td>\n<td>TableReference</td>\n<td>Target table identifier</td>\n</tr>\n<tr>\n<td>where_clause</td>\n<td>Optional[Expression]</td>\n<td>Row selection condition or None for all rows</td>\n</tr>\n<tr>\n<td>source_location</td>\n<td>SourceLocation</td>\n<td>Position information for error reporting</td>\n</tr>\n</tbody></table>\n<p>The parsing algorithm emphasizes safety validation alongside syntax processing:</p>\n<ol>\n<li><strong>Parse DELETE keyword</strong> confirming statement type identification</li>\n<li><strong>Parse FROM keyword</strong> which is required in standard SQL DELETE syntax</li>\n<li><strong>Parse table reference</strong> identifying the target table for row deletion</li>\n<li><strong>Check for WHERE clause</strong> by looking ahead for the WHERE keyword</li>\n<li><strong>Parse conditional expression</strong> if WHERE clause is present</li>\n<li><strong>Validate statement completeness</strong> ensuring no unexpected tokens remain</li>\n<li><strong>Flag safety concerns</strong> when WHERE clause is omitted, indicating all-row deletion</li>\n</ol>\n<p><strong>Table reference parsing</strong> for DELETE statements reuses the existing table parsing logic, but DELETE statements typically do not support table aliases since there&#39;s no column selection or JOIN operations involved.</p>\n<p><strong>WHERE clause handling</strong> follows the same optional pattern as UPDATE statements. When present, the parser delegates to the expression parsing subsystem developed for milestone 3. When omitted, the parser sets where_clause to None, but this represents a <strong>high-risk operation</strong> that affects all table rows.</p>\n<p>The parser implements <strong>safety-conscious parsing</strong> by including metadata in the AST that downstream tools can use to detect potentially dangerous operations:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">markdown</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">Safety Analysis Table:</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| WHERE Clause Status | Risk Level | AST Indicator | Recommended Action |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">|-------------------|------------|---------------|-------------------|</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| WHERE clause present | Low | where_clause field populated | Normal processing |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| WHERE clause omitted | High | where_clause field is None | Warn or require confirmation |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| WHERE with constant true | Medium | where_clause is BooleanLiteral(true) | Warn about full table deletion |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| WHERE with impossible condition | Low | Expression analysis required | Optimize to no-op |</span></span></code></pre></div>\n\n<p><strong>Error detection</strong> for DELETE statements focuses on structural validation rather than complex expression parsing, since the conditional logic reuses existing WHERE clause infrastructure. The primary parsing errors involve missing FROM keywords or malformed table references.</p>\n<blockquote>\n<p><strong>Design Philosophy</strong>: DELETE statement parsing prioritizes safety analysis over syntactic complexity. The AST structure enables downstream tools to implement appropriate safety checks and user confirmations before executing destructive operations.</p>\n</blockquote>\n<h3 id=\"value-list-and-type-validation\">Value List and Type Validation</h3>\n<p>Value list parsing requires coordination between syntax analysis and semantic validation to ensure data integrity while building correct AST structures. The parser must handle <strong>type-aware literal parsing</strong> and <strong>column-value correspondence checking</strong> without requiring full schema information during the parsing phase.</p>\n<p><strong>Literal type inference</strong> occurs during tokenization and parsing phases, where the parser identifies basic data types based on syntax patterns. However, full type validation requires schema information not available to the syntax parser, creating a separation of concerns between parsing and semantic analysis.</p>\n<p>The parser handles several distinct literal types with different parsing requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Literal Type</th>\n<th>Token Recognition</th>\n<th>AST Node Type</th>\n<th>Validation Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Integer</td>\n<td>Digit sequences</td>\n<td>IntegerLiteral</td>\n<td>Range checking</td>\n</tr>\n<tr>\n<td>Float</td>\n<td>Decimal point + digits</td>\n<td>FloatLiteral</td>\n<td>Precision validation</td>\n</tr>\n<tr>\n<td>String</td>\n<td>Quoted character sequences</td>\n<td>StringLiteral</td>\n<td>Escape sequence processing</td>\n</tr>\n<tr>\n<td>NULL</td>\n<td>NULL keyword</td>\n<td>NullLiteral</td>\n<td>Context appropriateness</td>\n</tr>\n<tr>\n<td>Boolean</td>\n<td>TRUE/FALSE keywords</td>\n<td>BooleanLiteral</td>\n<td>Logical consistency</td>\n</tr>\n</tbody></table>\n<p><strong>Column count validation</strong> ensures that each value row in INSERT statements contains the same number of expressions as the column specification. This validation occurs during parsing since it involves purely structural consistency rather than semantic type checking.</p>\n<p>The validation algorithm processes value lists using a two-phase approach:</p>\n<ol>\n<li><strong>Parse all value rows</strong> into expression lists without type validation</li>\n<li><strong>Verify structural consistency</strong> ensuring equal expression counts across all rows</li>\n<li><strong>Flag type validation requirements</strong> in AST metadata for semantic analysis phase</li>\n<li><strong>Preserve position information</strong> enabling detailed error reporting for type mismatches</li>\n</ol>\n<p><strong>Expression count validation</strong> compares the length of each value row against either the explicit column list length or the implied column count from schema information:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">markdown</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">Validation Cases:</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| Column Specification | Value Row Count | Validation Result | Error Type |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">|--------------------|----------------|------------------|------------|</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| 3 explicit columns | 3 values | Valid | None |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| 3 explicit columns | 2 values | Invalid | Too few values |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| 3 explicit columns | 4 values | Invalid | Too many values |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| No column list | Any count | Deferred | Schema validation required |</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">| Empty column list | 0 values | Valid | No-op statement |</span></span></code></pre></div>\n\n<p><strong>Type hint preservation</strong> ensures that the AST retains enough information for downstream semantic analysis to perform complete type validation. The parser includes literal type information and expression structure without requiring schema access.</p>\n<p>The parser implements <strong>progressive validation</strong> where syntactic validation occurs immediately while semantic validation is deferred to later phases. This separation enables the parser to work without database connections while still providing meaningful error messages for structural problems.</p>\n<p><strong>Value expression complexity</strong> requires the parser to handle not just simple literals but also arithmetic expressions, function calls, and column references within INSERT and UPDATE statements. These expressions use the same parsing infrastructure as WHERE clause expressions, ensuring consistent behavior across statement types.</p>\n<blockquote>\n<p><strong>Architecture Principle</strong>: The parser performs structural validation immediately but defers semantic type validation to later phases. This design enables parsing without schema access while preserving all information needed for comprehensive type checking.</p>\n</blockquote>\n<h3 id=\"architecture-decision-statement-specific-vs-unified-parser\">Architecture Decision: Statement-Specific vs Unified Parser</h3>\n<p>The choice between specialized parsers for each statement type versus a single unified parser affects code organization, maintainability, and extensibility throughout the parser architecture.</p>\n<blockquote>\n<p><strong>Decision: Statement-Specific Parser Classes</strong></p>\n<ul>\n<li><strong>Context</strong>: DML statements (INSERT, UPDATE, DELETE) have significantly different syntax structures, validation requirements, and AST node types compared to SELECT statements and each other</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Single unified parser class with method dispatch based on statement type</li>\n<li>Statement-specific parser classes inheriting from common base</li>\n<li>Completely independent parser classes with no shared infrastructure</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement statement-specific parser classes (<code>SelectParser</code>, <code>DMLParser</code>) that inherit from a common <code>BaseParser</code> class</li>\n<li><strong>Rationale</strong>: Each statement type has unique parsing requirements that benefit from specialized logic, while shared infrastructure (token management, error handling, expression parsing) reduces code duplication</li>\n<li><strong>Consequences</strong>: Enables focused parser logic for each statement type while maintaining code reuse for common operations</li>\n</ul>\n</blockquote>\n<p>The architecture decision analysis reveals several important trade-offs:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Code Organization Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unified Parser</td>\n<td>Single class to maintain</td>\n<td>Complex method dispatch</td>\n<td>Monolithic class structure</td>\n</tr>\n<tr>\n<td>Statement-Specific</td>\n<td>Focused logic per type</td>\n<td>Potential code duplication</td>\n<td>Modular class hierarchy</td>\n</tr>\n<tr>\n<td>Independent Parsers</td>\n<td>Complete isolation</td>\n<td>Maximum duplication</td>\n<td>Separate file per parser</td>\n</tr>\n</tbody></table>\n<p><strong>Statement-specific parser architecture</strong> provides the optimal balance between code organization and functionality. The <code>DMLParser</code> class handles INSERT, UPDATE, and DELETE statements while the <code>SelectParser</code> handles SELECT queries, with both inheriting common functionality from <code>BaseParser</code>.</p>\n<p>The <code>BaseParser</code> class provides shared infrastructure:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Purpose</th>\n<th>Shared Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>peek_token(offset)</code></td>\n<td>Token lookahead</td>\n<td>Consistent across all statement types</td>\n</tr>\n<tr>\n<td><code>consume_token()</code></td>\n<td>Token advancement</td>\n<td>Uniform token consumption</td>\n</tr>\n<tr>\n<td><code>expect_token(type)</code></td>\n<td>Validation and consumption</td>\n<td>Standard error handling</td>\n</tr>\n<tr>\n<td><code>create_source_location()</code></td>\n<td>Position tracking</td>\n<td>Consistent error reporting</td>\n</tr>\n<tr>\n<td><code>parse_expression()</code></td>\n<td>Expression parsing</td>\n<td>Reused in WHERE, SET, VALUES clauses</td>\n</tr>\n</tbody></table>\n<p><strong>Specialized parser methods</strong> in each parser class focus on statement-specific syntax:</p>\n<p>DMLParser specialized methods:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Statement Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>parse_insert_statement()</code></td>\n<td>INSERT</td>\n<td>Handle column lists and value rows</td>\n</tr>\n<tr>\n<td><code>parse_update_statement()</code></td>\n<td>UPDATE</td>\n<td>Handle SET assignments and WHERE clauses</td>\n</tr>\n<tr>\n<td><code>parse_delete_statement()</code></td>\n<td>DELETE</td>\n<td>Handle table references and WHERE clauses</td>\n</tr>\n<tr>\n<td><code>parse_assignment_expression()</code></td>\n<td>UPDATE</td>\n<td>Handle column = value assignments</td>\n</tr>\n<tr>\n<td><code>parse_value_list()</code></td>\n<td>INSERT</td>\n<td>Handle comma-separated value rows</td>\n</tr>\n</tbody></table>\n<p><strong>Parser coordination</strong> occurs through the main <code>SQLParser</code> class which analyzes the initial keyword and delegates to the appropriate specialized parser:</p>\n<ol>\n<li><strong>Analyze statement type</strong> by examining the first token (SELECT, INSERT, UPDATE, DELETE)</li>\n<li><strong>Instantiate appropriate parser</strong> based on statement type detection</li>\n<li><strong>Delegate parsing</strong> to the specialized parser instance</li>\n<li><strong>Return unified AST</strong> with consistent node interfaces</li>\n</ol>\n<p>This architecture enables <strong>independent evolution</strong> of each parser while maintaining <strong>consistent interfaces</strong> for error handling, AST construction, and token management.</p>\n<blockquote>\n<p><strong>Design Benefit</strong>: Statement-specific parsers allow each parser to focus on its unique requirements while sharing common infrastructure. This reduces complexity in each parser class while avoiding code duplication across parsers.</p>\n</blockquote>\n<h3 id=\"common-dml-parser-pitfalls\">Common DML Parser Pitfalls</h3>\n<p>Data modification statement parsing introduces several categories of errors that differ from SELECT statement parsing challenges. Understanding these pitfalls helps developers avoid subtle bugs that can lead to incorrect AST construction or runtime failures.</p>\n<p>⚠️ <strong>Pitfall: Column-Value Count Mismatch Detection</strong></p>\n<p>Many developers implement INSERT parsing without proper validation that the number of values matches the number of columns. The parser may successfully construct an AST that represents syntactically valid but semantically incorrect SQL.</p>\n<p><strong>Why it&#39;s wrong</strong>: INSERT statements with mismatched column and value counts will fail at execution time with unclear error messages. The parser should catch this structural problem early to provide better error reporting.</p>\n<p><strong>How to fix</strong>: Implement validation logic after parsing each value row that compares the expression count against the column list length. Store validation results in AST metadata for downstream error reporting.</p>\n<p>⚠️ <strong>Pitfall: Assignment vs Comparison Operator Confusion</strong></p>\n<p>UPDATE statement parsing must distinguish between assignment operators (<code>=</code>) and comparison operators (<code>=</code>) based on context. Developers often implement a single equals handler that doesn&#39;t account for this contextual difference.</p>\n<p><strong>Why it&#39;s wrong</strong>: The same symbol (<code>=</code>) has different meanings in SET clauses (assignment) versus WHERE clauses (comparison). Treating them identically leads to incorrect AST node types.</p>\n<p><strong>How to fix</strong>: Implement separate parsing contexts for SET clauses and WHERE clauses. The SET clause parser should create <code>AssignmentExpression</code> nodes while the WHERE clause parser creates <code>BinaryOperation</code> nodes for the same token.</p>\n<p>⚠️ <strong>Pitfall: Missing WHERE Clause Safety Validation</strong></p>\n<p>DELETE and UPDATE statements without WHERE clauses affect all table rows, which is often unintentional. Developers frequently parse these statements without flagging the safety implications.</p>\n<p><strong>Why it&#39;s wrong</strong>: While syntactically valid, DELETE or UPDATE statements without WHERE clauses represent high-risk operations that users often execute accidentally.</p>\n<p><strong>How to fix</strong>: Include safety metadata in the AST that indicates when WHERE clauses are omitted. Downstream tools can use this information to require user confirmation or implement safety checks.</p>\n<p>⚠️ <strong>Pitfall: Multi-Row INSERT Parsing State Confusion</strong></p>\n<p>INSERT statements with multiple value rows require careful state management to distinguish between commas separating rows and commas separating values within rows. Developers often lose track of nesting level during parsing.</p>\n<p><strong>Why it&#39;s wrong</strong>: Incorrect comma handling leads to malformed AST structures where value rows contain the wrong number of expressions or expressions span multiple rows.</p>\n<p><strong>How to fix</strong>: Use explicit state tracking with parentheses counting to maintain parsing context. Each opening parenthesis increments nesting level, each closing parenthesis decrements, and comma handling changes based on current nesting level.</p>\n<p>⚠️ <strong>Pitfall: Table Reference Aliasing in DML Statements</strong></p>\n<p>Some developers allow table aliases in DELETE and UPDATE statements without considering that aliases complicate column reference resolution and aren&#39;t typically needed for single-table operations.</p>\n<p><strong>Why it&#39;s wrong</strong>: Table aliases in DML statements add complexity without providing significant benefit, and can confuse column reference parsing in SET and WHERE clauses.</p>\n<p><strong>How to fix</strong>: Consider restricting table aliases in DML statements to simple table names only. If aliases are supported, ensure that column reference parsing in SET and WHERE clauses correctly resolves aliased table names.</p>\n<p>⚠️ <strong>Pitfall: Type Validation During Parsing</strong></p>\n<p>Developers sometimes attempt to perform type validation during the parsing phase, checking that string literals are assigned to string columns or integer literals to integer columns.</p>\n<p><strong>Why it&#39;s wrong</strong>: Type validation requires schema information that&#39;s not available during syntax parsing. Attempting type validation during parsing either fails or requires tight coupling between parser and database schema.</p>\n<p><strong>How to fix</strong>: Separate syntactic parsing from semantic validation. The parser should construct correctly typed literal AST nodes based on syntax alone, leaving type compatibility checking to a later semantic analysis phase.</p>\n<p>⚠️ <strong>Pitfall: Incomplete Error Recovery for DML Statements</strong></p>\n<p>DML statement parsing errors often leave the parser in an inconsistent state where subsequent statements cannot be parsed correctly, especially in batch processing scenarios.</p>\n<p><strong>Why it&#39;s wrong</strong>: Poor error recovery prevents parsing multiple statements in sequence and makes debugging difficult when only the first error is reported.</p>\n<p><strong>How to fix</strong>: Implement statement-level error recovery that can resynchronize on statement boundary keywords (INSERT, UPDATE, DELETE, SELECT) after encountering parsing errors.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides concrete Python implementation guidance for building DML statement parsers that integrate with the existing tokenizer and SELECT parser infrastructure from previous milestones.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>AST Node Implementation</td>\n<td>Python dataclasses with type hints</td>\n<td>Custom classes with visitor pattern support</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception hierarchy with message strings</td>\n<td>Structured error objects with recovery suggestions</td>\n</tr>\n<tr>\n<td>Type Validation</td>\n<td>Deferred to separate validation phase</td>\n<td>Integrated type hints with schema validation</td>\n</tr>\n<tr>\n<td>Statement Dispatch</td>\n<td>Simple if/elif chain on token type</td>\n<td>Strategy pattern with parser registry</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql_parser/\n  tokenizer.py              ← Token definitions and lexical analysis\n  ast_nodes.py              ← AST node classes for all statement types\n  base_parser.py            ← BaseParser with shared parsing infrastructure\n  select_parser.py          ← SelectParser class for SELECT statements\n  dml_parser.py             ← DMLParser class for INSERT/UPDATE/DELETE\n  expression_parser.py      ← ExpressionParser for WHERE/SET clause expressions\n  main_parser.py            ← SQLParser orchestrating all parser components\n  errors.py                 ← Exception hierarchy for parsing errors\n  tests/\n    test_dml_parser.py      ← Tests for DML statement parsing\n    test_integration.py     ← End-to-end parsing tests</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code (COMPLETE, ready to use):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># ast_nodes.py - DML statement AST node definitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Union, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SourceLocation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ASTNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_location: SourceLocation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#9ECBFF\">'ASTNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Identifier</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"Identifier\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TableReference</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    table: Union[Identifier, </span><span style=\"color:#9ECBFF\">'QualifiedIdentifier'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alias: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"TableReference\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.table] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.table </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AssignmentExpression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_column: Identifier</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assigned_value: </span><span style=\"color:#9ECBFF\">'Expression'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"AssignmentExpression\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.target_column, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.assigned_value]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InsertStatement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    table_reference: TableReference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column_list: Optional[List[Identifier]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value_rows: List[List[</span><span style=\"color:#9ECBFF\">'Expression'</span><span style=\"color:#E1E4E8\">]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"InsertStatement\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        children </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.table_reference]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column_list:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            children.extend(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column_list)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> row </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.value_rows:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            children.extend(row)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> children</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UpdateStatement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    table_reference: TableReference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    set_assignments: List[AssignmentExpression]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    where_clause: Optional[</span><span style=\"color:#9ECBFF\">'Expression'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"UpdateStatement\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        children </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.table_reference] </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.set_assignments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.where_clause:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            children.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.where_clause)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> children</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DeleteStatement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    table_reference: TableReference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    where_clause: Optional[</span><span style=\"color:#9ECBFF\">'Expression'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"DeleteStatement\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> children</span><span style=\"color:#E1E4E8\">(self) -> List[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        children </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.table_reference]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.where_clause:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            children.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.where_clause)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> children</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Type alias for expression nodes (defined in expression_parser.py)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">Expression </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Union[</span><span style=\"color:#9ECBFF\">'BinaryOperation'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'UnaryOperation'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Identifier'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                  'StringLiteral'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'IntegerLiteral'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'FloatLiteral'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'NullLiteral'</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># errors.py - Exception hierarchy for DML parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Parse error at line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UnexpectedTokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, actual_token: </span><span style=\"color:#9ECBFF\">'Token'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.actual_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual_token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, actual_token.line, actual_token.column)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructuralValidationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, source_location.start_line, source_location.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source_location </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source_location</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code (signature + TODOs only):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># dml_parser.py - DML statement parser implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> base_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ast_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#F97583\"> *</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType, Token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> expression_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExpressionParser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DMLParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseParser</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Parser for INSERT, UPDATE, DELETE statements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokens: List[Token]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(tokens)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expression_parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ExpressionParser(tokens)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_insert_statement</span><span style=\"color:#E1E4E8\">(self) -> InsertStatement:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse INSERT INTO table [(columns)] VALUES (values) statement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Consume and validate INSERT keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Consume and validate INTO keyword token  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse table reference using parse_table_reference()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for optional column list by looking ahead for LPAREN</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If column list present, parse comma-separated identifiers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Consume and validate VALUES keyword</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Parse value row list with parse_value_row_list()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Validate column count matches value count for each row</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Create and return InsertStatement AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.peek_token() to check for optional column list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_update_statement</span><span style=\"color:#E1E4E8\">(self) -> UpdateStatement:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse UPDATE table SET assignments [WHERE condition] statement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Consume and validate UPDATE keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse table reference using parse_table_reference()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Consume and validate SET keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse assignment list with parse_assignment_list()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for optional WHERE clause by looking ahead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If WHERE present, parse condition with expression_parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create and return UpdateStatement AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: WHERE clauses are optional in UPDATE statements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_delete_statement</span><span style=\"color:#E1E4E8\">(self) -> DeleteStatement:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse DELETE FROM table [WHERE condition] statement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Consume and validate DELETE keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Consume and validate FROM keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse table reference using parse_table_reference()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for optional WHERE clause by looking ahead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If WHERE present, parse condition with expression_parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Flag safety concern if WHERE clause is missing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create and return DeleteStatement AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Consider adding safety metadata to AST for missing WHERE</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_assignment_list</span><span style=\"color:#E1E4E8\">(self) -> List[AssignmentExpression]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse comma-separated list of column = value assignments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize empty assignment list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse first assignment with parse_assignment_expression()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add first assignment to list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: While next token is COMMA, continue parsing assignments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each comma, consume it and parse next assignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return completed assignment list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Similar structure to column list parsing but creates AssignmentExpression nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_assignment_expression</span><span style=\"color:#E1E4E8\">(self) -> AssignmentExpression:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse single column = value assignment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse target column as Identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Expect and consume EQUALS token for assignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse assigned value expression using expression_parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create source location from target column to end of value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return AssignmentExpression AST node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Assignment equals is different from comparison equals in context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_value_row_list</span><span style=\"color:#E1E4E8\">(self) -> List[List[Expression]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse comma-separated list of (value, value, ...) rows.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize empty row list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse first value row with parse_value_row()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add first row to list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: While next token is COMMA, continue parsing rows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each comma, consume it and parse next row</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return completed row list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Each row is parenthesized list of expressions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_value_row</span><span style=\"color:#E1E4E8\">(self) -> List[Expression]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse single (value, value, ...) row.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Expect and consume LPAREN token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize empty value list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse first value expression using expression_parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add first value to list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: While next token is COMMA, continue parsing values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: For each comma, consume it and parse next value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Expect and consume RPAREN token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return completed value list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle empty value lists for INSERT () VALUES () syntax</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_column_value_consistency</span><span style=\"color:#E1E4E8\">(self, column_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, value_rows: List[List[Expression]]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate that each value row has correct number of expressions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through each value row in value_rows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each row, compare len(row) with column_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If counts don't match, raise StructuralValidationError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Include row number and expected vs actual counts in error message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Include source location information for precise error reporting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This catches INSERT structural errors early with good error messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>Token Type Checking</strong>: Use <code>token.type == TokenType.INSERT_KEYWORD</code> for exact token type matching</li>\n<li><strong>Optional Parsing</strong>: Use <code>self.peek_token(0).type == TokenType.WHERE_KEYWORD</code> to detect optional clauses</li>\n<li><strong>Error Recovery</strong>: Wrap parsing operations in try/except blocks to catch and re-raise with additional context</li>\n<li><strong>List Comprehensions</strong>: Use <code>[self.parse_expression() for _ in range(value_count)]</code> for parsing known-length lists</li>\n<li><strong>Type Annotations</strong>: Include full type hints for better IDE support: <code>def parse_insert_statement(self) -&gt; InsertStatement:</code></li>\n<li><strong>Source Location Tracking</strong>: Use <code>self.create_source_location(start_token, self.current_token)</code> for position information</li>\n<li><strong>Expression Parser Integration</strong>: Call <code>self.expression_parser.parse_expression()</code> for WHERE and value expressions</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the DML parser, verify correct behavior with these tests:</p>\n<p><strong>INSERT Statement Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test case: Basic INSERT with explicit columns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com')\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(sql)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(ast, InsertStatement)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.table_reference.table.name </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"users\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.column_list) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.column_list[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].name </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"name\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.value_rows) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.value_rows[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span></code></pre></div>\n\n<p><strong>UPDATE Statement Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test case: UPDATE with WHERE clause</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"UPDATE users SET email = 'newemail@example.com' WHERE id = 1\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(sql)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(ast, UpdateStatement)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.set_assignments) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.set_assignments[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].target_column.name </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"email\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.where_clause </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<p><strong>DELETE Statement Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test case: DELETE with WHERE clause</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"DELETE FROM users WHERE active = false\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(sql)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(ast, DeleteStatement)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.where_clause </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test case: DELETE without WHERE (should parse but flag safety concern)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"DELETE FROM temp_table\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(sql)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> ast.where_clause </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Safety concern flagged</span></span></code></pre></div>\n\n<p><strong>Multi-Row INSERT Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test case: Multiple row INSERT</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"INSERT INTO users (name, age) VALUES ('Alice', 25), ('Bob', 30)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(sql)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.value_rows) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.value_rows[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(ast.value_rows[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span></code></pre></div>\n\n<p><strong>G. Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Expected VALUES keyword&quot; error</td>\n<td>Missing or misspelled VALUES in INSERT</td>\n<td>Check token stream after table reference</td>\n<td>Verify tokenizer recognizes VALUES as keyword</td>\n</tr>\n<tr>\n<td>Assignment parsing fails</td>\n<td>Confusion between assignment = and comparison =</td>\n<td>Print token types during SET clause parsing</td>\n<td>Use context-specific parsing for SET vs WHERE</td>\n</tr>\n<tr>\n<td>Multi-row INSERT creates wrong AST</td>\n<td>Comma separation logic incorrect</td>\n<td>Print nesting level during value parsing</td>\n<td>Track parentheses depth to distinguish comma contexts</td>\n</tr>\n<tr>\n<td>WHERE clause not parsed in UPDATE</td>\n<td>Optional WHERE logic missing</td>\n<td>Check lookahead token after SET clause</td>\n<td>Add proper optional clause detection</td>\n</tr>\n<tr>\n<td>Column count validation fails</td>\n<td>Off-by-one errors in list length comparison</td>\n<td>Print actual vs expected counts</td>\n<td>Verify column list length calculation</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-recovery\">Error Handling and Recovery</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-4), providing essential error handling strategies that ensure our parser produces meaningful feedback when SQL statements contain syntax errors or malformed constructs.</p>\n</blockquote>\n<h3 id=\"mental-model-medical-diagnosis-and-treatment\">Mental Model: Medical Diagnosis and Treatment</h3>\n<p>Think of parser error handling like a medical diagnosis system. When a patient visits a doctor with symptoms, the doctor doesn&#39;t just say &quot;you&#39;re sick&quot; and stop there. Instead, they perform a systematic examination to identify the specific problem, explain what&#39;s wrong in terms the patient can understand, suggest what might have caused it, and recommend treatment options. Similarly, our SQL parser must act as a diagnostic system for malformed queries.</p>\n<p>When the parser encounters invalid SQL, it should identify the specific syntax error, explain what was expected versus what was found, pinpoint the exact location in the query, suggest likely causes, and ideally continue analyzing the rest of the query to find additional problems. Just as a doctor might discover multiple health issues during a single examination, our parser should strive to identify multiple syntax errors in one pass rather than forcing the developer to fix errors one at a time.</p>\n<p>The key insight is that error handling isn&#39;t just about detecting problems—it&#39;s about providing enough information for the developer to understand and fix the issue quickly. A parser that simply says &quot;syntax error&quot; is like a doctor who says &quot;you&#39;re unwell&quot; without any further explanation or guidance.</p>\n<h3 id=\"parser-error-categories\">Parser Error Categories</h3>\n<p>Our SQL parser must handle several distinct categories of errors, each requiring different detection strategies and recovery approaches. Understanding these categories helps us design appropriate error handling mechanisms and provide targeted error messages.</p>\n<p><img src=\"/api/project/sql-parser/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Parser Error Handling Flow\"></p>\n<p>The following table categorizes the different types of errors our parser will encounter, organized by the component that detects them and their characteristics:</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Detection Component</th>\n<th>Timing</th>\n<th>Recovery Difficulty</th>\n<th>Example Scenarios</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenization Errors</td>\n<td>Tokenizer</td>\n<td>During lexical analysis</td>\n<td>Low - local to single token</td>\n<td>Unterminated strings, invalid characters, malformed numbers</td>\n</tr>\n<tr>\n<td>Syntax Errors</td>\n<td>Parser</td>\n<td>During parsing</td>\n<td>Medium - affects statement structure</td>\n<td>Missing keywords, unexpected tokens, malformed expressions</td>\n</tr>\n<tr>\n<td>Structural Errors</td>\n<td>Parser</td>\n<td>During AST construction</td>\n<td>High - affects statement semantics</td>\n<td>Column/value count mismatch, invalid table references</td>\n</tr>\n<tr>\n<td>Precedence Errors</td>\n<td>Expression Parser</td>\n<td>During expression parsing</td>\n<td>Medium - affects expression tree</td>\n<td>Mismatched parentheses, operator precedence violations</td>\n</tr>\n<tr>\n<td>Recovery Errors</td>\n<td>Error Recovery</td>\n<td>After error detection</td>\n<td>Variable - depends on context</td>\n<td>Failed synchronization, cascade failures</td>\n</tr>\n</tbody></table>\n<h4 id=\"tokenization-error-types\">Tokenization Error Types</h4>\n<p><strong>Tokenization errors</strong> occur during lexical analysis when the tokenizer encounters character sequences that cannot be classified into valid token types. These errors are typically the easiest to detect and report because they affect only a single token and don&#39;t propagate through the parsing process.</p>\n<p>The <code>TokenizerError</code> class extends <code>ParseError</code> to capture tokenization-specific information:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>message</td>\n<td>str</td>\n<td>Human-readable description of the tokenization problem</td>\n</tr>\n<tr>\n<td>line</td>\n<td>int</td>\n<td>Line number where the error occurred (1-based)</td>\n</tr>\n<tr>\n<td>column</td>\n<td>int</td>\n<td>Column position within the line (1-based)</td>\n</tr>\n<tr>\n<td>invalid_sequence</td>\n<td>str</td>\n<td>The character sequence that caused the error</td>\n</tr>\n<tr>\n<td>suggestion</td>\n<td>Optional[str]</td>\n<td>Suggested fix or likely intended token</td>\n</tr>\n</tbody></table>\n<p>Common tokenization errors include unterminated string literals where the closing quote is missing, invalid escape sequences within strings, malformed numeric literals with multiple decimal points, and unrecognized character sequences that don&#39;t match any token pattern. The tokenizer can often provide specific suggestions for these errors because the context is limited and the intended token is usually obvious.</p>\n<h4 id=\"syntax-error-types\">Syntax Error Types</h4>\n<p><strong>Syntax errors</strong> occur during parsing when the token sequence doesn&#39;t match the expected grammar rules for SQL statements. These errors are more complex than tokenization errors because they involve the relationship between multiple tokens and the overall structure of the statement.</p>\n<p>The <code>SyntaxError</code> class provides the foundation for syntax error reporting:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>message</td>\n<td>str</td>\n<td>Description of the syntax violation</td>\n</tr>\n<tr>\n<td>line</td>\n<td>int</td>\n<td>Line number where parsing failed</td>\n</tr>\n<tr>\n<td>column</td>\n<td>int</td>\n<td>Column position where parsing failed</td>\n</tr>\n<tr>\n<td>error_token</td>\n<td>Token</td>\n<td>The token that caused the parsing failure</td>\n</tr>\n<tr>\n<td>parser_state</td>\n<td>str</td>\n<td>Description of what the parser was trying to parse</td>\n</tr>\n</tbody></table>\n<p>The <code>UnexpectedTokenError</code> subclass handles the most common syntax error scenario where the parser encounters a token different from what the grammar rules expect:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>expected</td>\n<td>str</td>\n<td>Description of what token types were expected</td>\n</tr>\n<tr>\n<td>actual_token</td>\n<td>Token</td>\n<td>The token that was actually encountered</td>\n</tr>\n<tr>\n<td>context</td>\n<td>str</td>\n<td>Description of the parsing context (e.g., &quot;parsing SELECT clause&quot;)</td>\n</tr>\n<tr>\n<td>suggestions</td>\n<td>List[str]</td>\n<td>Possible corrections or likely intended tokens</td>\n</tr>\n</tbody></table>\n<h4 id=\"structural-validation-errors\">Structural Validation Errors</h4>\n<p><strong>Structural validation errors</strong> represent violations of SQL&#39;s semantic rules that can be detected without schema information. These errors involve the logical consistency of the parsed statement structure rather than pure syntax violations.</p>\n<p>The <code>StructuralValidationError</code> class captures these higher-level consistency problems:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>message</td>\n<td>str</td>\n<td>Description of the structural inconsistency</td>\n</tr>\n<tr>\n<td>source_location</td>\n<td>SourceLocation</td>\n<td>Span of tokens involved in the error</td>\n</tr>\n<tr>\n<td>violation_type</td>\n<td>str</td>\n<td>Category of structural rule violated</td>\n</tr>\n<tr>\n<td>related_elements</td>\n<td>List[ASTNode]</td>\n<td>AST nodes involved in the inconsistency</td>\n</tr>\n</tbody></table>\n<p>Examples include INSERT statements where the number of columns in the column list doesn&#39;t match the number of values in each value row, UPDATE statements with SET clauses that reference non-existent columns, and DELETE statements with malformed WHERE clauses that would affect statement safety.</p>\n<blockquote>\n<p><strong>Architecture Decision: Error Type Hierarchy</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to represent different categories of parsing errors with appropriate detail and recovery information</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Single generic error class with type field</li>\n<li>Inheritance hierarchy with specialized error classes</li>\n<li>Error union types with tagged variants</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use inheritance hierarchy with specialized error classes</li>\n<li><strong>Rationale</strong>: Inheritance allows each error type to carry appropriate contextual information while maintaining type safety and enabling specific error handling strategies</li>\n<li><strong>Consequences</strong>: More complex type system but better error reporting and recovery capabilities</li>\n</ul>\n</blockquote>\n<h3 id=\"error-message-design\">Error Message Design</h3>\n<p>Effective error messages are crucial for developer productivity. Our parser&#39;s error messages must provide enough information for developers to understand what went wrong, why it&#39;s a problem, and how to fix it. The design of error messages should follow principles of clarity, specificity, and actionability.</p>\n<h4 id=\"error-message-components\">Error Message Components</h4>\n<p>Each error message should contain several key components that together provide a complete picture of the problem. The structure ensures consistency across different error types and gives developers the information they need to resolve issues quickly.</p>\n<p>The core error message format includes these elements:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Type</td>\n<td>Categorize the kind of problem</td>\n<td>&quot;Syntax Error&quot;, &quot;Tokenization Error&quot;, &quot;Structural Error&quot;</td>\n</tr>\n<tr>\n<td>Location Information</td>\n<td>Pinpoint where the error occurred</td>\n<td>&quot;line 3, column 15&quot;</td>\n</tr>\n<tr>\n<td>Problem Description</td>\n<td>Explain what went wrong</td>\n<td>&quot;Expected FROM keyword after column list&quot;</td>\n</tr>\n<tr>\n<td>Context Information</td>\n<td>Describe what the parser was doing</td>\n<td>&quot;while parsing SELECT statement&quot;</td>\n</tr>\n<tr>\n<td>Actual vs Expected</td>\n<td>Show the mismatch</td>\n<td>&quot;found IDENTIFIER &#39;users&#39;, expected FROM&quot;</td>\n</tr>\n<tr>\n<td>Suggestion</td>\n<td>Provide actionable fix</td>\n<td>&quot;Try: SELECT name FROM users&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"position-information-and-context\">Position Information and Context</h4>\n<p>Accurate position information is essential for developers to locate errors quickly, especially in complex queries. Our error reporting system tracks both character-level position and structural context within the SQL statement.</p>\n<p>The <code>SourceLocation</code> type captures precise position information:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>start_line</td>\n<td>int</td>\n<td>Starting line number (1-based indexing)</td>\n</tr>\n<tr>\n<td>start_column</td>\n<td>int</td>\n<td>Starting column position (1-based indexing)</td>\n</tr>\n<tr>\n<td>end_line</td>\n<td>int</td>\n<td>Ending line number for multi-token errors</td>\n</tr>\n<tr>\n<td>end_column</td>\n<td>int</td>\n<td>Ending column position for error spans</td>\n</tr>\n</tbody></table>\n<p>Position tracking must account for SQL&#39;s formatting conventions. Many SQL queries are formatted across multiple lines with indentation, and developers expect error messages to reference the visual layout they see in their editor. The tokenizer maintains line and column counters that increment appropriately for newline characters and tab expansion.</p>\n<p>Context information helps developers understand not just where the error occurred, but what the parser was attempting to accomplish when it failed. This is particularly important for complex parsing scenarios where the immediate error location might not be the actual source of the problem.</p>\n<h4 id=\"suggestion-generation-strategies\">Suggestion Generation Strategies</h4>\n<p>High-quality error messages include actionable suggestions that help developers fix problems quickly. Our suggestion generation uses several strategies based on the type of error and the parsing context.</p>\n<p><strong>Keyword Suggestion Strategy</strong> uses fuzzy matching to identify likely intended keywords when the parser encounters unexpected identifiers. For example, if the parser expects a FROM keyword but finds &quot;FORM&quot;, the suggestion system can identify this as a likely typo and suggest the correction.</p>\n<p><strong>Token Sequence Analysis</strong> examines the tokens surrounding an error to infer the developer&#39;s intent. If a SELECT statement is missing a comma between column names, the parser can detect the pattern and suggest inserting the missing comma.</p>\n<p><strong>Grammar-Based Suggestions</strong> use knowledge of SQL grammar rules to propose valid continuations when the parser encounters unexpected end-of-input or invalid token sequences. When parsing reaches an incomplete statement, the suggestion system can list the valid ways to complete the current grammar rule.</p>\n<p><strong>Common Pattern Recognition</strong> identifies frequent error patterns and provides targeted suggestions. For example, when developers write <code>SELECT *</code> without a FROM clause, the suggestion system can recognize this common mistake and prompt for the missing table specification.</p>\n<p>The suggestion generation process follows this algorithm:</p>\n<ol>\n<li><strong>Analyze the error context</strong> to determine what grammar rule was being parsed and what tokens were expected</li>\n<li><strong>Examine surrounding tokens</strong> to identify patterns that might indicate the developer&#39;s intent</li>\n<li><strong>Apply fuzzy matching</strong> to identify likely corrections for misspelled keywords or identifiers</li>\n<li><strong>Generate multiple suggestions</strong> ranked by likelihood, with the most probable correction listed first</li>\n<li><strong>Validate suggestions</strong> by checking that the proposed corrections would allow parsing to continue successfully</li>\n</ol>\n<blockquote>\n<p><strong>Key Design Principle: Progressive Disclosure</strong></p>\n<p>Error messages should provide essential information immediately while allowing developers to access additional detail when needed. The primary error message should be concise and actionable, with supplementary information available for complex debugging scenarios.</p>\n</blockquote>\n<h4 id=\"example-error-message-formats\">Example Error Message Formats</h4>\n<p>The following examples illustrate how different error categories produce structured, helpful error messages:</p>\n<p><strong>Tokenization Error Example:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Tokenization Error at line 2, column 18:\n  Unterminated string literal starting with single quote\n  \n  SELECT name, 'John Doe FROM users;\n                       ^\n  Expected: Closing single quote (') before end of statement\n  Suggestion: Add closing quote → SELECT name, 'John Doe' FROM users;</code></pre></div>\n\n<p><strong>Syntax Error Example:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Syntax Error at line 1, column 21:\n  Expected FROM keyword after column list in SELECT statement\n  \n  SELECT name, email users WHERE active = 1;\n                     ^\n  Found: IDENTIFIER 'users'\n  Expected: FROM keyword\n  Suggestion: INSERT missing FROM → SELECT name, email FROM users WHERE active = 1;</code></pre></div>\n\n<p><strong>Structural Error Example:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Structural Error at line 3, column 1:\n  INSERT statement column count mismatch\n  \n  INSERT INTO users (name, email) \n  VALUES ('John', 'john@example.com', 'active');\n         ^\n  Column list specifies 2 columns but VALUES clause provides 3 values\n  Suggestion: Either add 'status' to column list or remove 'active' from VALUES</code></pre></div>\n\n<h3 id=\"error-recovery-techniques\">Error Recovery Techniques</h3>\n<p>Error recovery allows the parser to continue analyzing SQL statements after encountering errors, enabling the detection of multiple problems in a single parse attempt. Effective error recovery improves developer productivity by reducing the edit-compile-test cycle time when fixing complex SQL syntax errors.</p>\n<h4 id=\"recovery-strategy-overview\">Recovery Strategy Overview</h4>\n<p>Parser error recovery involves two main challenges: <strong>detecting that an error has occurred</strong> and <strong>resynchronizing the parser state</strong> to continue parsing from a known good position. The recovery strategy must balance between finding additional errors and avoiding cascade failures where one error causes spurious additional error reports.</p>\n<p>Our parser implements a <strong>panic-mode recovery</strong> strategy combined with <strong>synchronization points</strong> that represent reliable positions where parsing can safely resume. This approach provides good error detection coverage while minimizing false positive error reports.</p>\n<h4 id=\"synchronization-points-and-panic-mode\">Synchronization Points and Panic Mode</h4>\n<p><strong>Synchronization points</strong> are token positions where the parser can confidently resume parsing after an error. These points correspond to major statement boundaries and clause beginnings where the parser&#39;s internal state can be reset to a known configuration.</p>\n<p>The following table identifies key synchronization points for different statement types:</p>\n<table>\n<thead>\n<tr>\n<th>Statement Context</th>\n<th>Synchronization Tokens</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statement boundary</td>\n<td>Semicolon, EOF</td>\n<td>Reset to statement-level parsing</td>\n</tr>\n<tr>\n<td>SELECT statement</td>\n<td>FROM, WHERE, ORDER, GROUP</td>\n<td>Skip to clause and resume</td>\n</tr>\n<tr>\n<td>INSERT statement</td>\n<td>VALUES, SELECT</td>\n<td>Skip to data specification</td>\n</tr>\n<tr>\n<td>UPDATE statement</td>\n<td>SET, WHERE</td>\n<td>Skip to assignment or condition</td>\n</tr>\n<tr>\n<td>DELETE statement</td>\n<td>WHERE</td>\n<td>Skip to condition specification</td>\n</tr>\n<tr>\n<td>Expression parsing</td>\n<td>Comma, parentheses</td>\n<td>Reset expression parser state</td>\n</tr>\n</tbody></table>\n<p><strong>Panic mode recovery</strong> activates when the parser encounters an unexpected token that prevents normal parsing from continuing. The recovery algorithm follows these steps:</p>\n<ol>\n<li><strong>Record the error</strong> with complete position and context information, ensuring the original error is preserved for reporting</li>\n<li><strong>Enter panic mode</strong> by setting a recovery flag that changes the parser&#39;s behavior to focus on finding synchronization points</li>\n<li><strong>Skip tokens</strong> systematically until reaching a reliable synchronization point where parsing can resume</li>\n<li><strong>Reset parser state</strong> to match the synchronization point context, clearing any partial parse results that might be inconsistent</li>\n<li><strong>Resume normal parsing</strong> from the synchronization point, continuing to build the AST for the remainder of the statement</li>\n</ol>\n<p>The panic mode algorithm must be careful to avoid <strong>infinite loops</strong> where the parser repeatedly encounters errors without making progress. Recovery includes a <strong>maximum error threshold</strong> that prevents runaway error detection and ensures parsing terminates even for severely malformed input.</p>\n<h4 id=\"cascade-error-prevention\">Cascade Error Prevention</h4>\n<p><strong>Cascade errors</strong> occur when an initial parsing error causes subsequent spurious error reports as the parser attempts to continue with inconsistent state. Preventing cascade errors is crucial for producing useful error reports that help developers identify the actual problems in their SQL statements.</p>\n<p>Our cascade prevention strategy uses several techniques:</p>\n<p><strong>Error Context Marking</strong> tracks which parts of the input have been affected by error recovery. When the parser resumes from a synchronization point, it marks the recovered section as potentially unreliable for further error reporting. Subsequent errors in marked regions are classified as possible cascade effects and reported with lower confidence.</p>\n<p><strong>State Validation</strong> performs consistency checks after error recovery to ensure the parser&#39;s internal state matches the synchronization point assumptions. If state validation fails, the parser resets more aggressively rather than continuing with potentially corrupted state.</p>\n<p><strong>Error Suppression Heuristics</strong> identify common patterns of cascade errors and suppress obvious false positives. For example, if a SELECT statement is missing a FROM keyword, the parser might misinterpret the table name as a second column name, leading to cascade errors in the WHERE clause. The suppression system recognizes this pattern and avoids reporting the secondary errors.</p>\n<p><strong>Recovery Distance Tracking</strong> measures how far parsing has progressed since the last error recovery. Errors that occur immediately after recovery are more likely to be cascade effects, while errors that occur after successfully parsing several tokens are more likely to be independent problems.</p>\n<blockquote>\n<p><strong>Architecture Decision: Panic Mode vs Local Recovery</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose error recovery strategy that balances error detection with parsing robustness</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Panic mode with synchronization points</li>\n<li>Local recovery with token insertion/deletion</li>\n<li>Backtracking with alternative grammar rules</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Panic mode with synchronization points</li>\n<li><strong>Rationale</strong>: Simpler to implement correctly, less prone to cascade errors, works well with SQL&#39;s structured syntax</li>\n<li><strong>Consequences</strong>: May miss some recoverable errors but provides more reliable error reporting overall</li>\n</ul>\n</blockquote>\n<h4 id=\"recovery-state-management\">Recovery State Management</h4>\n<p>Effective error recovery requires careful management of the parser&#39;s internal state to ensure that recovery doesn&#39;t introduce inconsistencies that affect subsequent parsing. The parser maintains several state components that must be coordinated during recovery.</p>\n<p>The parser state includes these key elements:</p>\n<table>\n<thead>\n<tr>\n<th>State Component</th>\n<th>Description</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Position</td>\n<td>Current position in token stream</td>\n<td>Advance to synchronization point</td>\n</tr>\n<tr>\n<td>Parse Stack</td>\n<td>Recursive descent call context</td>\n<td>Unwind to appropriate level</td>\n</tr>\n<tr>\n<td>AST Construction</td>\n<td>Partially built syntax tree</td>\n<td>Mark incomplete nodes, continue building</td>\n</tr>\n<tr>\n<td>Symbol Context</td>\n<td>Current parsing context (statement type, clause)</td>\n<td>Reset to synchronization context</td>\n</tr>\n<tr>\n<td>Error Flags</td>\n<td>Recovery mode and error suppression state</td>\n<td>Update based on recovery success</td>\n</tr>\n</tbody></table>\n<p><strong>Parse Stack Management</strong> during error recovery must carefully unwind recursive descent parser calls to reach the appropriate level for the synchronization point. If an error occurs deep within expression parsing, the recovery mechanism must unwind through multiple call levels to reach statement-level parsing where synchronization can occur safely.</p>\n<p><strong>AST Node Handling</strong> during recovery presents a challenge because partially constructed AST nodes may be incomplete or inconsistent. Our strategy marks incomplete nodes with special error indicators rather than discarding them entirely. This preserves structural information that might be useful for error reporting while preventing incomplete nodes from affecting semantic analysis.</p>\n<p><strong>Recovery Validation</strong> occurs after each successful synchronization to ensure the parser has returned to a consistent state. The validation process checks that the current token position aligns with the expected synchronization point, the parse stack depth is appropriate for the current context, and any partially constructed AST nodes are properly marked as incomplete.</p>\n<h4 id=\"multi-error-reporting-strategy\">Multi-Error Reporting Strategy</h4>\n<p>When error recovery successfully identifies multiple problems in a single SQL statement, the parser must present these errors in a way that helps developers prioritize and fix them efficiently. The multi-error reporting strategy balances completeness with usability.</p>\n<p><strong>Error Prioritization</strong> ranks detected errors by their likely impact on statement correctness and their independence from other errors. Primary syntax errors that prevent basic statement structure recognition receive highest priority, while potential cascade errors or stylistic issues receive lower priority.</p>\n<p><strong>Error Grouping</strong> organizes related errors to avoid overwhelming developers with redundant information. If a missing FROM keyword causes multiple subsequent parsing problems, the error report groups these issues and identifies the FROM keyword as the primary fix.</p>\n<p><strong>Fix Suggestion Coordination</strong> ensures that suggested fixes for multiple errors are compatible with each other. The suggestion system validates that applying all recommended fixes would result in syntactically valid SQL rather than introducing new conflicts.</p>\n<p>The multi-error report format presents errors in order of priority while showing the relationships between different problems:</p>\n<ol>\n<li><strong>Primary errors</strong> that represent fundamental syntax violations affecting statement structure</li>\n<li><strong>Secondary errors</strong> that might be cascade effects but could also be independent problems  </li>\n<li><strong>Suggestions</strong> that address multiple errors simultaneously when possible</li>\n<li><strong>Recovery summary</strong> indicating which parts of the statement were successfully parsed despite errors</li>\n</ol>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete implementation patterns and starter code for building robust error handling into your SQL parser. The error handling system serves as the foundation for user-friendly parser feedback and reliable error recovery.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Error Handling Aspect</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Classes</td>\n<td>Simple inheritance with string messages</td>\n<td>Rich error objects with structured data and recovery hints</td>\n</tr>\n<tr>\n<td>Position Tracking</td>\n<td>Line/column counters in tokenizer</td>\n<td>Source span tracking with original text preservation</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Basic panic mode with semicolon sync</td>\n<td>Multi-level synchronization with context-aware recovery</td>\n</tr>\n<tr>\n<td>Message Formatting</td>\n<td>Template strings with substitution</td>\n<td>Structured error builders with suggestion generation</td>\n</tr>\n<tr>\n<td>Multiple Error Handling</td>\n<td>Collect errors in list, report at end</td>\n<td>Streaming error reporting with cascade detection</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/\n  parser/\n    errors.py                    ← Error class hierarchy and utilities\n    error_recovery.py           ← Recovery strategies and synchronization\n    error_reporter.py           ← Message formatting and multi-error handling\n    position_tracker.py         ← Source location and context tracking\n  tokenizer/\n    tokenizer.py                ← Main tokenizer with error detection\n  parser/\n    base_parser.py              ← Base parser with error handling integration\n    select_parser.py            ← SELECT parser with recovery points\n    expression_parser.py        ← Expression parser with precedence error handling\n    dml_parser.py              ← DML parsers with structural validation\n  tests/\n    test_error_handling.py      ← Comprehensive error handling test suite</code></pre></div>\n\n<h4 id=\"complete-error-class-infrastructure\">Complete Error Class Infrastructure</h4>\n<p>Here&#39;s the complete error handling infrastructure that provides the foundation for all parser error reporting:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorSeverity</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"error\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WARNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"warning\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INFO</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"info\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SourceLocation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.end_line:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.start_line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.start_column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"lines </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.start_line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">-</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.end_line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> span_length</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.end_line:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.end_column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Multi-line spans simplified</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 severity: ErrorSeverity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ErrorSeverity.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.severity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> severity</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> with_suggestion</span><span style=\"color:#E1E4E8\">(self, suggestion: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'ParseError'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggestion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> with_context</span><span style=\"color:#E1E4E8\">(self, context: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'ParseError'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> source_location</span><span style=\"color:#E1E4E8\">(self) -> SourceLocation:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> SourceLocation(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 invalid_sequence: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Tokenization Error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, line, column)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.invalid_sequence </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> invalid_sequence</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 parser_state: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Syntax Error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, line, column)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parser_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser_state</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> UnexpectedTokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, actual_token: </span><span style=\"color:#9ECBFF\">'Token'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 context: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, found </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, actual_token.line, actual_token.column, context)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.actual_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual_token</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestions: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructuralValidationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source_location: SourceLocation, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 violation_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Structural Error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        source_location.start_line, source_location.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source_location </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source_location</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.violation_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> violation_type</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.related_elements: List[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span></code></pre></div>\n\n<h4 id=\"error-recovery-infrastructure\">Error Recovery Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Set, Dict, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RecoveryMode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NORMAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"normal\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PANIC</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"panic\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUPPRESSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"suppressed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SynchronizationPoint</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, token_types: Set[TokenType], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 recovery_action: Callable[[], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.token_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> token_types</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_action </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> recovery_action</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.confidence_level </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorRecoveryManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_mode </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RecoveryMode.</span><span style=\"color:#79B8FF\">NORMAL</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.error_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.synchronization_points: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, SynchronizationPoint] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cascade_suppression_distance </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens_since_recovery </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_sync_point</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, sync_point: SynchronizationPoint):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Register a synchronization point for error recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that token types are appropriate for synchronization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> enter_panic_mode</span><span style=\"color:#E1E4E8\">(self, error: ParseError):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set recovery mode to PANIC</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record error for later reporting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize recovery state tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> attempt_synchronization</span><span style=\"color:#E1E4E8\">(self, current_token: Token) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if current token matches any synchronization point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute recovery action if synchronization point found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Reset to normal parsing mode on successful sync</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if synchronization successful, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_suppress_error</span><span style=\"color:#E1E4E8\">(self, error: ParseError) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if error is likely cascade effect</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider distance from last recovery point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply suppression heuristics based on error type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if error should be suppressed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-parser-error-integration-skeleton\">Core Parser Error Integration Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokens: List[Token]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors: List[ParseError] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ErrorRecoveryManager()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._setup_synchronization_points()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _setup_synchronization_points</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Register synchronization points for statement boundaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add synchronization for major clause keywords (FROM, WHERE, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure recovery actions for each synchronization type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> report_error</span><span style=\"color:#E1E4E8\">(self, error: ParseError):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if error should be suppressed due to cascade effects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add error to error list if not suppressed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Enter panic mode recovery if error is severe</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update recovery statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> expect_token</span><span style=\"color:#E1E4E8\">(self, expected_type: TokenType) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if current token matches expected type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If match, return token and advance position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If no match, create UnexpectedTokenError with suggestion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Attempt error recovery and return placeholder token if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> synchronize_after_error</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Skip tokens until synchronization point found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate parser state after synchronization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Reset parsing context appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Exit panic mode if synchronization successful</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_error_with_context</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                context: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create SyntaxError with current token position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add parsing context information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate suggestions based on current state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return fully configured error object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 - Tokenizer Error Handling:</strong>\nAfter implementing tokenizer error handling, test with these inputs:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test unterminated string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"SELECT 'unterminated string FROM users\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: TokenizerError with suggestion to add closing quote</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test invalid characters  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"SELECT name @ FROM users\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: TokenizerError identifying '@' as invalid character</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test malformed number</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"SELECT price WHERE cost > 12.34.56\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: TokenizerError for invalid number format</span></span></code></pre></div>\n\n<p><strong>Milestone 2-3 - Parser Error Recovery:</strong>\nTest error recovery with malformed SELECT statements:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test missing FROM keyword</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"SELECT name, email users WHERE active = 1\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: UnexpectedTokenError suggesting FROM insertion, continue parsing WHERE</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test unbalanced parentheses in WHERE</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"SELECT * FROM users WHERE (name = 'John' AND age > 25\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Error about unbalanced parentheses, attempt to recover at statement end</span></span></code></pre></div>\n\n<p><strong>Milestone 4 - Structural Validation:</strong>\nTest structural consistency errors:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test column/value mismatch in INSERT</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">test_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"INSERT INTO users (name, email) VALUES ('John', 'john@example.com', 'active')\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: StructuralValidationError identifying count mismatch with specific suggestion</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips-for-error-handling\">Debugging Tips for Error Handling</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parser stops at first error</td>\n<td>Error recovery not implemented</td>\n<td>Check if synchronization points registered</td>\n<td>Add sync points and recovery logic</td>\n</tr>\n<tr>\n<td>Cascade of false errors</td>\n<td>Recovery state not reset properly</td>\n<td>Examine parser state after recovery</td>\n<td>Reset all parser state at sync points</td>\n</tr>\n<tr>\n<td>Unhelpful error messages</td>\n<td>Missing context information</td>\n<td>Check error creation calls</td>\n<td>Add parser context and suggestions to errors</td>\n</tr>\n<tr>\n<td>Recovery loops infinitely</td>\n<td>Sync points not reachable</td>\n<td>Trace token skipping in panic mode</td>\n<td>Add more sync points, add loop detection</td>\n</tr>\n<tr>\n<td>Errors missing position info</td>\n<td>Position tracking broken</td>\n<td>Verify line/column updates</td>\n<td>Fix position tracking in tokenizer</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy-and-validation\">Testing Strategy and Validation</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-4), providing a comprehensive testing framework that validates parser correctness at each development stage and ensures milestone acceptance criteria are met.</p>\n</blockquote>\n<h3 id=\"mental-model-quality-control-assembly-line\">Mental Model: Quality Control Assembly Line</h3>\n<p>Think of testing a SQL parser like quality control in a manufacturing assembly line. Each component (tokenizer, parser, AST builder) must be tested in isolation before assembly, just as individual car parts are tested before installation. Then we test the complete assembled product (end-to-end parsing) under real-world conditions with various inputs. Finally, we have checkpoint inspections at each assembly stage (milestone validation) to ensure the product meets specifications before moving to the next phase. This multi-layered approach catches defects early when they&#39;re cheaper to fix, prevents faulty components from contaminating downstream processes, and validates that the final product meets customer requirements.</p>\n<p>The key insight in parser testing is that failures can occur at multiple levels - lexical analysis can produce wrong tokens, syntax analysis can build incorrect AST structures, and semantic validation can miss logical inconsistencies. Each level requires different testing strategies because the failure modes and detection methods are distinct. Just as a car manufacturer tests engine components separately from transmission components, we must isolate parser components to pinpoint exactly where failures originate.</p>\n<h3 id=\"component-unit-testing\">Component Unit Testing</h3>\n<p><strong>Component isolation testing</strong> focuses on verifying that individual parser components function correctly in controlled environments with carefully crafted inputs. This approach enables precise failure localization, comprehensive edge case coverage, and rapid test execution during development cycles.</p>\n<h4 id=\"tokenizer-unit-testing\">Tokenizer Unit Testing</h4>\n<p>The tokenizer transforms character sequences into classified tokens, making it the foundation of the entire parsing process. Tokenizer testing must verify both correct token recognition and proper error handling for invalid input sequences.</p>\n<p><strong>Core Tokenizer Test Categories:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Purpose</th>\n<th>Example Input</th>\n<th>Expected Output</th>\n<th>Failure Detection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Keyword Recognition</td>\n<td>Verify case-insensitive keyword detection</td>\n<td><code>&quot;SELECT&quot;</code>, <code>&quot;select&quot;</code>, <code>&quot;Select&quot;</code></td>\n<td><code>Token(SELECT_KEYWORD, &quot;SELECT&quot;, 1, 1)</code></td>\n<td>Wrong token type or case handling</td>\n</tr>\n<tr>\n<td>Identifier Parsing</td>\n<td>Validate identifier boundary detection</td>\n<td><code>&quot;table_name&quot;</code>, <code>&quot;column1&quot;</code></td>\n<td><code>Token(IDENTIFIER, &quot;table_name&quot;, 1, 1)</code></td>\n<td>Incorrect character inclusion/exclusion</td>\n</tr>\n<tr>\n<td>String Literal Handling</td>\n<td>Test quote parsing and escape sequences</td>\n<td><code>&quot;&#39;O&#39;&#39;Reilly&#39;&quot;</code>, <code>&quot;\\&quot;test\\&quot;&quot;</code></td>\n<td><code>Token(STRING_LITERAL, &quot;O&#39;Reilly&quot;, 1, 1)</code></td>\n<td>Malformed escape handling</td>\n</tr>\n<tr>\n<td>Numeric Literal Recognition</td>\n<td>Verify integer and float parsing</td>\n<td><code>&quot;123&quot;</code>, <code>&quot;45.67&quot;</code>, <code>&quot;.89&quot;</code></td>\n<td><code>Token(INTEGER_LITERAL, &quot;123&quot;, 1, 1)</code></td>\n<td>Wrong numeric type classification</td>\n</tr>\n<tr>\n<td>Operator Tokenization</td>\n<td>Test multi-character operator recognition</td>\n<td><code>&quot;&lt;=&quot;</code>, <code>&quot;!=&quot;</code>, <code>&quot;IS NULL&quot;</code></td>\n<td>Correct operator token type</td>\n<td>Incomplete operator recognition</td>\n</tr>\n<tr>\n<td>Position Tracking</td>\n<td>Validate line/column accuracy</td>\n<td>Multi-line input with various tokens</td>\n<td>Accurate line/column for each token</td>\n<td>Incorrect position information</td>\n</tr>\n</tbody></table>\n<p><strong>Tokenizer Error Handling Tests:</strong></p>\n<p>The tokenizer must gracefully handle malformed input while providing actionable error information. These tests verify that error detection and recovery mechanisms function correctly.</p>\n<table>\n<thead>\n<tr>\n<th>Error Scenario</th>\n<th>Input Example</th>\n<th>Expected Error Type</th>\n<th>Error Message Content</th>\n<th>Recovery Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unterminated String</td>\n<td><code>&quot;&#39;unclosed string</code></td>\n<td><code>TokenizerError</code></td>\n<td>Position of quote start, suggests closing quote</td>\n<td>Skip to next whitespace or semicolon</td>\n</tr>\n<tr>\n<td>Invalid Character</td>\n<td><code>&quot;SELECT @ FROM&quot;</code></td>\n<td><code>TokenizerError</code></td>\n<td>Character and position, suggests valid alternatives</td>\n<td>Continue after invalid character</td>\n</tr>\n<tr>\n<td>Malformed Number</td>\n<td><code>&quot;123.45.67&quot;</code></td>\n<td><code>TokenizerError</code></td>\n<td>Position of second decimal, explains numeric format</td>\n<td>Treat as separate tokens</td>\n</tr>\n<tr>\n<td>Unknown Operator</td>\n<td><code>&quot;column ~~ value&quot;</code></td>\n<td><code>TokenizerError</code></td>\n<td>Position of operator, lists valid operators</td>\n<td>Treat as separate characters</td>\n</tr>\n</tbody></table>\n<h4 id=\"parser-component-unit-testing\">Parser Component Unit Testing</h4>\n<p>Parser components transform token streams into AST nodes, requiring validation of both structural correctness and error handling behavior. Each parser component has distinct responsibilities and failure modes.</p>\n<p><strong>SELECT Parser Unit Tests:</strong></p>\n<p>The <code>SelectParser</code> handles column specifications, table references, and optional clauses. Testing must verify correct AST construction for various SELECT statement patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Token Input</th>\n<th>Expected AST Structure</th>\n<th>Validation Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Column List</td>\n<td><code>SELECT col1, col2 FROM table1</code></td>\n<td><code>SelectStatement</code> with column list containing two <code>AliasedExpression</code> nodes</td>\n<td>Column count, column names, no aliases</td>\n</tr>\n<tr>\n<td>Star Wildcard</td>\n<td><code>SELECT * FROM table1</code></td>\n<td><code>SelectStatement</code> with <code>StarExpression</code> in columns field</td>\n<td>Star expression type, table reference</td>\n</tr>\n<tr>\n<td>Column Aliases</td>\n<td><code>SELECT col1 AS alias1, col2 alias2</code></td>\n<td><code>AliasedExpression</code> nodes with explicit and implicit aliases</td>\n<td>Alias presence and values</td>\n</tr>\n<tr>\n<td>Qualified Columns</td>\n<td><code>SELECT table1.col1, t.col2 FROM table1 t</code></td>\n<td><code>QualifiedIdentifier</code> nodes with correct qualifier and name</td>\n<td>Qualifier extraction, name parsing</td>\n</tr>\n<tr>\n<td>Table Aliases</td>\n<td><code>SELECT * FROM table1 AS t</code></td>\n<td><code>TableReference</code> with alias field populated</td>\n<td>Table name, alias value</td>\n</tr>\n</tbody></table>\n<p><strong>Expression Parser Unit Tests:</strong></p>\n<p>The <code>ExpressionParser</code> handles operator precedence, associativity, and various expression types. These tests must verify that complex expressions produce correct AST structures with proper precedence relationships.</p>\n<table>\n<thead>\n<tr>\n<th>Expression Input</th>\n<th>Token Sequence</th>\n<th>Expected AST Structure</th>\n<th>Precedence Verification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>col1 = 5 AND col2 &gt; 10</code></td>\n<td>Identifier, Equals, Integer, AND, Identifier, Greater, Integer</td>\n<td><code>BinaryOperation(AND, BinaryOperation(EQUALS, col1, 5), BinaryOperation(GREATER, col2, 10))</code></td>\n<td>AND binds looser than comparison</td>\n</tr>\n<tr>\n<td><code>NOT col1 IS NULL</code></td>\n<td>NOT, Identifier, IS, NULL</td>\n<td><code>UnaryOperation(NOT, BinaryOperation(IS, col1, NULL))</code></td>\n<td>NOT binds tighter than IS</td>\n</tr>\n<tr>\n<td><code>(col1 + col2) * 3</code></td>\n<td>LParen, Identifier, Plus, Identifier, RParen, Multiply, Integer</td>\n<td><code>BinaryOperation(MULTIPLY, BinaryOperation(PLUS, col1, col2), 3)</code></td>\n<td>Parentheses override precedence</td>\n</tr>\n<tr>\n<td><code>col1 = col2 = col3</code></td>\n<td>Multiple equality operators</td>\n<td><code>BinaryOperation(EQUALS, col1, BinaryOperation(EQUALS, col2, col3))</code></td>\n<td>Right associativity for equality</td>\n</tr>\n</tbody></table>\n<p><strong>DML Parser Unit Tests:</strong></p>\n<p>Data modification statement parsers handle INSERT, UPDATE, and DELETE syntax with specific validation requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Statement Type</th>\n<th>Input Example</th>\n<th>AST Validation</th>\n<th>Structural Checks</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INSERT with columns</td>\n<td><code>INSERT INTO table (col1, col2) VALUES (1, &#39;test&#39;)</code></td>\n<td>Column list matches value count</td>\n<td>Column-value correspondence</td>\n</tr>\n<tr>\n<td>INSERT without columns</td>\n<td><code>INSERT INTO table VALUES (1, &#39;test&#39;, NULL)</code></td>\n<td>No column list, correct value parsing</td>\n<td>Value type recognition</td>\n</tr>\n<tr>\n<td>UPDATE with WHERE</td>\n<td><code>UPDATE table SET col1 = 5 WHERE col2 &gt; 10</code></td>\n<td>Assignment list and condition parsing</td>\n<td>SET clause structure</td>\n</tr>\n<tr>\n<td>DELETE with condition</td>\n<td><code>DELETE FROM table WHERE active = false</code></td>\n<td>Table reference and WHERE clause</td>\n<td>Condition expression validation</td>\n</tr>\n</tbody></table>\n<h4 id=\"ast-node-unit-testing\">AST Node Unit Testing</h4>\n<p>AST nodes must implement visitor patterns, provide accurate source location information, and support tree traversal operations. These tests verify that node implementations conform to interface contracts.</p>\n<p><strong>AST Node Interface Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Interface Method</th>\n<th>Test Scenario</th>\n<th>Expected Behavior</th>\n<th>Failure Detection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>node_type()</code></td>\n<td>Call on various node types</td>\n<td>Returns correct string identifier</td>\n<td>Wrong or missing type name</td>\n</tr>\n<tr>\n<td><code>children()</code></td>\n<td>Call on nodes with child nodes</td>\n<td>Returns list of direct children only</td>\n<td>Missing children or incorrect nesting</td>\n</tr>\n<tr>\n<td><code>accept(visitor)</code></td>\n<td>Pass visitor to each node type</td>\n<td>Calls correct visitor method</td>\n<td>Wrong method or missing implementation</td>\n</tr>\n<tr>\n<td><code>source_location()</code></td>\n<td>Nodes created from tokens</td>\n<td>Returns accurate position information</td>\n<td>Incorrect line/column data</td>\n</tr>\n</tbody></table>\n<h3 id=\"end-to-end-parser-testing\">End-to-End Parser Testing</h3>\n<p><strong>End-to-end testing</strong> validates the complete parsing pipeline from SQL text input to final AST output. These tests simulate real-world usage patterns and verify that component integration produces correct results for complex queries.</p>\n<h4 id=\"real-world-query-testing\">Real-World Query Testing</h4>\n<p>End-to-end tests use realistic SQL queries that combine multiple language features, testing the interaction between different parser components and validating that the complete system handles complexity correctly.</p>\n<p><strong>Representative Query Test Cases:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Query Category</th>\n<th>SQL Example</th>\n<th>Integration Points Tested</th>\n<th>Expected AST Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Complex SELECT</td>\n<td><code>SELECT t1.name AS customer_name, t2.total FROM customers t1, orders t2 WHERE t1.id = t2.customer_id AND t2.total &gt; 100</code></td>\n<td>Tokenizer → SELECT parser → Expression parser</td>\n<td>Qualified identifiers, aliases, complex WHERE conditions</td>\n</tr>\n<tr>\n<td>Multi-table operations</td>\n<td><code>UPDATE customers SET status = &#39;premium&#39; WHERE id IN (SELECT customer_id FROM orders WHERE total &gt; 1000)</code></td>\n<td>All parser components, nested expression handling</td>\n<td>Subquery parsing (future milestone), complex conditions</td>\n</tr>\n<tr>\n<td>Batch INSERT</td>\n<td><code>INSERT INTO products (name, price, category) VALUES (&#39;Product1&#39;, 19.99, &#39;electronics&#39;), (&#39;Product2&#39;, 29.99, &#39;books&#39;)</code></td>\n<td>Tokenizer → DML parser → Value list parsing</td>\n<td>Multiple value rows, type consistency</td>\n</tr>\n<tr>\n<td>Complex conditions</td>\n<td><code>DELETE FROM logs WHERE created_date &lt; &#39;2023-01-01&#39; OR (level = &#39;DEBUG&#39; AND archived IS NOT NULL)</code></td>\n<td>Expression precedence, multiple operators</td>\n<td>Precedence correctness, logical operator combination</td>\n</tr>\n</tbody></table>\n<p><strong>Query Complexity Progression:</strong></p>\n<p>End-to-end tests should progress from simple to complex queries, verifying that each level of complexity builds correctly on simpler foundations.</p>\n<table>\n<thead>\n<tr>\n<th>Complexity Level</th>\n<th>Query Characteristics</th>\n<th>Testing Focus</th>\n<th>Example Patterns</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic</td>\n<td>Single table, simple conditions</td>\n<td>Core functionality verification</td>\n<td><code>SELECT * FROM table WHERE id = 1</code></td>\n</tr>\n<tr>\n<td>Intermediate</td>\n<td>Multiple columns, aliases, AND/OR</td>\n<td>Component integration</td>\n<td><code>SELECT col1 AS c1, col2 FROM table t WHERE c1 &gt; 5 AND c2 IS NOT NULL</code></td>\n</tr>\n<tr>\n<td>Advanced</td>\n<td>Complex expressions, multiple tables</td>\n<td>Full system integration</td>\n<td><code>SELECT t1.col1, t2.col2 FROM table1 t1, table2 t2 WHERE t1.id = t2.ref_id AND (t1.status = &#39;active&#39; OR t2.priority &gt; 3)</code></td>\n</tr>\n<tr>\n<td>Edge Cases</td>\n<td>Boundary conditions, unusual syntax</td>\n<td>Robustness validation</td>\n<td><code>SELECT * FROM table WHERE col BETWEEN 1 AND (2 + 3) * 4</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"error-propagation-testing\">Error Propagation Testing</h4>\n<p>End-to-end error testing verifies that errors detected at any parsing stage propagate correctly through the system and produce helpful error messages for users.</p>\n<p><strong>Error Propagation Scenarios:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Origin</th>\n<th>SQL Input</th>\n<th>Error Detection Point</th>\n<th>Expected Error Information</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenizer</td>\n<td><code>SELECT * FROM table WHERE col = &#39;unterminated</code></td>\n<td>Tokenizer during string parsing</td>\n<td>Line/column of unterminated quote, suggestion to add closing quote</td>\n</tr>\n<tr>\n<td>SELECT Parser</td>\n<td><code>SELECT col1, FROM table</code></td>\n<td>SELECT parser during column list parsing</td>\n<td>Position of unexpected FROM, suggestion to remove comma or add column</td>\n</tr>\n<tr>\n<td>Expression Parser</td>\n<td><code>SELECT * FROM table WHERE col1 = = 5</code></td>\n<td>Expression parser during operator parsing</td>\n<td>Position of duplicate equals, explanation of valid syntax</td>\n</tr>\n<tr>\n<td>DML Parser</td>\n<td><code>INSERT INTO table (col1, col2) VALUES (1)</code></td>\n<td>DML parser during value validation</td>\n<td>Column count mismatch, specific counts and position information</td>\n</tr>\n</tbody></table>\n<h4 id=\"performance-characteristic-testing\">Performance Characteristic Testing</h4>\n<p>While not primary functional goals, end-to-end tests should verify that the parser maintains reasonable performance characteristics and doesn&#39;t exhibit pathological behavior with large inputs.</p>\n<p><strong>Performance Test Categories:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Performance Aspect</th>\n<th>Test Approach</th>\n<th>Acceptance Criteria</th>\n<th>Monitoring Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Linear scaling</td>\n<td>Queries with increasing column counts</td>\n<td>Parse time grows linearly with input size</td>\n<td>Token count vs. parse time</td>\n</tr>\n<tr>\n<td>Memory usage</td>\n<td>Large INSERT statements with many value rows</td>\n<td>Memory usage remains bounded</td>\n<td>Peak memory during parsing</td>\n</tr>\n<tr>\n<td>Error recovery</td>\n<td>Malformed queries with multiple errors</td>\n<td>Error detection doesn&#39;t degrade performance</td>\n<td>Error count vs. processing time</td>\n</tr>\n</tbody></table>\n<h3 id=\"milestone-validation-checkpoints\">Milestone Validation Checkpoints</h3>\n<p><strong>Milestone validation</strong> provides concrete checkpoints that verify specific project milestones are complete and functional. Each checkpoint includes automated tests, manual verification steps, and expected behavior descriptions.</p>\n<h4 id=\"milestone-1-sql-tokenizer-validation\">Milestone 1: SQL Tokenizer Validation</h4>\n<p>The tokenizer milestone focuses on lexical analysis correctness and establishes the foundation for all subsequent parsing stages.</p>\n<p><strong>Automated Tokenizer Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Name</th>\n<th>Input SQL</th>\n<th>Expected Token Sequence</th>\n<th>Pass/Fail Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_keyword_recognition</code></td>\n<td><code>&quot;SELECT FROM WHERE&quot;</code></td>\n<td><code>[SELECT_KEYWORD, FROM_KEYWORD, WHERE_KEYWORD]</code></td>\n<td>All tokens have correct types</td>\n</tr>\n<tr>\n<td><code>test_identifier_parsing</code></td>\n<td><code>&quot;table_name column1&quot;</code></td>\n<td><code>[IDENTIFIER(&quot;table_name&quot;), IDENTIFIER(&quot;column1&quot;)]</code></td>\n<td>Correct boundary detection</td>\n</tr>\n<tr>\n<td><code>test_string_literals</code></td>\n<td><code>&quot;&#39;O&#39;&#39;Reilly&#39; \\&quot;quoted\\&quot;&quot;</code></td>\n<td><code>[STRING_LITERAL(&quot;O&#39;Reilly&quot;), STRING_LITERAL(&quot;quoted&quot;)]</code></td>\n<td>Proper escape handling</td>\n</tr>\n<tr>\n<td><code>test_numeric_literals</code></td>\n<td><code>&quot;123 45.67 .89&quot;</code></td>\n<td><code>[INTEGER_LITERAL(&quot;123&quot;), FLOAT_LITERAL(&quot;45.67&quot;), FLOAT_LITERAL(&quot;.89&quot;)]</code></td>\n<td>Correct numeric type classification</td>\n</tr>\n<tr>\n<td><code>test_operator_recognition</code></td>\n<td><code>&quot;= &lt; &gt; &lt;= &gt;= != AND OR&quot;</code></td>\n<td>Correct operator token types for each</td>\n<td>Multi-character operators recognized</td>\n</tr>\n</tbody></table>\n<p><strong>Manual Tokenizer Verification:</strong></p>\n<blockquote>\n<p><strong>Checkpoint Command:</strong> <code>python -m tokenizer &quot;SELECT table1.column1 AS col1 FROM table1 WHERE value &gt; 100&quot;</code></p>\n<p><strong>Expected Output:</strong></p>\n<pre><code>Token(SELECT_KEYWORD, &#39;SELECT&#39;, line=1, column=1)\nToken(IDENTIFIER, &#39;table1&#39;, line=1, column=8)\nToken(DOT, &#39;.&#39;, line=1, column=14)\nToken(IDENTIFIER, &#39;column1&#39;, line=1, column=15)\nToken(AS_KEYWORD, &#39;AS&#39;, line=1, column=23)\nToken(IDENTIFIER, &#39;col1&#39;, line=1, column=26)\nToken(FROM_KEYWORD, &#39;FROM&#39;, line=1, column=31)\nToken(IDENTIFIER, &#39;table1&#39;, line=1, column=36)\nToken(WHERE_KEYWORD, &#39;WHERE&#39;, line=1, column=43)\nToken(IDENTIFIER, &#39;value&#39;, line=1, column=49)\nToken(GREATER, &#39;&gt;&#39;, line=1, column=55)\nToken(INTEGER_LITERAL, &#39;100&#39;, line=1, column=57)\n</code></pre>\n</blockquote>\n<p><strong>Milestone 1 Success Indicators:</strong></p>\n<ul>\n<li>All SQL keywords are recognized case-insensitively</li>\n<li>Identifiers are correctly distinguished from keywords</li>\n<li>String literals handle single and double quotes with escape sequences</li>\n<li>Numeric literals distinguish integers from floats</li>\n<li>Operators including multi-character ones (&lt;=, &gt;=, !=) are recognized</li>\n<li>Position tracking provides accurate line and column information</li>\n</ul>\n<h4 id=\"milestone-2-select-parser-validation\">Milestone 2: SELECT Parser Validation</h4>\n<p>The SELECT parser milestone builds AST structures from token streams and handles various SELECT statement patterns.</p>\n<p><strong>Automated SELECT Parser Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Name</th>\n<th>SQL Input</th>\n<th>Expected AST Structure</th>\n<th>Validation Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_simple_select</code></td>\n<td><code>&quot;SELECT * FROM table1&quot;</code></td>\n<td><code>SelectStatement(StarExpression(), TableReference(&quot;table1&quot;))</code></td>\n<td>Star expression and table parsing</td>\n</tr>\n<tr>\n<td><code>test_column_list</code></td>\n<td><code>&quot;SELECT col1, col2 FROM table1&quot;</code></td>\n<td>Column list with two <code>AliasedExpression</code> nodes</td>\n<td>Column count and names</td>\n</tr>\n<tr>\n<td><code>test_column_aliases</code></td>\n<td><code>&quot;SELECT col1 AS c1, col2 c2&quot;</code></td>\n<td>Explicit and implicit aliases in AST</td>\n<td>Alias detection and storage</td>\n</tr>\n<tr>\n<td><code>test_qualified_columns</code></td>\n<td><code>&quot;SELECT t.col1 FROM table1 t&quot;</code></td>\n<td><code>QualifiedIdentifier</code> with qualifier &quot;t&quot;</td>\n<td>Qualifier parsing and table alias</td>\n</tr>\n</tbody></table>\n<p><strong>Manual SELECT Parser Verification:</strong></p>\n<blockquote>\n<p><strong>Checkpoint Command:</strong> <code>python -m parser &quot;SELECT t1.name AS customer_name, t1.email FROM customers t1&quot;</code></p>\n<p><strong>Expected AST Structure:</strong></p>\n<pre><code>SelectStatement(\n  columns=[\n    AliasedExpression(\n      expression=QualifiedIdentifier(qualifier=&quot;t1&quot;, name=&quot;name&quot;),\n      alias=&quot;customer_name&quot;\n    ),\n    AliasedExpression(\n      expression=QualifiedIdentifier(qualifier=&quot;t1&quot;, name=&quot;email&quot;),\n      alias=None\n    )\n  ],\n  table_reference=TableReference(\n    table=Identifier(&quot;customers&quot;),\n    alias=&quot;t1&quot;\n  ),\n  where_clause=None\n)\n</code></pre>\n</blockquote>\n<p><strong>Milestone 2 Success Indicators:</strong></p>\n<ul>\n<li>SELECT statements parse into correct <code>SelectStatement</code> AST nodes</li>\n<li>Column specifications support star wildcards and explicit column lists</li>\n<li>Column aliases work with both explicit AS keyword and implicit syntax</li>\n<li>Table references support aliases and qualified identifier parsing</li>\n<li>AST nodes provide accurate source location information</li>\n</ul>\n<h4 id=\"milestone-3-where-clause-expression-parser-validation\">Milestone 3: WHERE Clause Expression Parser Validation</h4>\n<p>The expression parser milestone handles complex expressions with proper operator precedence and associativity rules.</p>\n<p><strong>Automated Expression Parser Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Name</th>\n<th>SQL Input</th>\n<th>Expected AST Structure</th>\n<th>Precedence Verification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_basic_comparison</code></td>\n<td><code>&quot;WHERE col1 = 5&quot;</code></td>\n<td><code>BinaryOperation(EQUALS, Identifier(&quot;col1&quot;), IntegerLiteral(5))</code></td>\n<td>Simple binary operation</td>\n</tr>\n<tr>\n<td><code>test_logical_operators</code></td>\n<td><code>&quot;WHERE col1 = 5 AND col2 &gt; 10&quot;</code></td>\n<td>AND operation with two comparison children</td>\n<td>AND binds looser than comparison</td>\n</tr>\n<tr>\n<td><code>test_precedence_override</code></td>\n<td><code>&quot;WHERE (col1 + col2) &gt; 10&quot;</code></td>\n<td>Addition as left child of comparison</td>\n<td>Parentheses override precedence</td>\n</tr>\n<tr>\n<td><code>test_null_checks</code></td>\n<td><code>&quot;WHERE col1 IS NULL OR col2 IS NOT NULL&quot;</code></td>\n<td>Null test expressions with OR combination</td>\n<td>IS NULL/IS NOT NULL parsing</td>\n</tr>\n</tbody></table>\n<p><strong>Manual Expression Parser Verification:</strong></p>\n<blockquote>\n<p><strong>Checkpoint Command:</strong> <code>python -m parser &quot;SELECT * FROM table WHERE col1 = 5 AND (col2 &gt; 10 OR col3 IS NULL)&quot;</code></p>\n<p><strong>Expected WHERE Clause AST:</strong></p>\n<pre><code>BinaryOperation(\n  operator=AND,\n  left=BinaryOperation(\n    operator=EQUALS,\n    left=Identifier(&quot;col1&quot;),\n    right=IntegerLiteral(5)\n  ),\n  right=BinaryOperation(\n    operator=OR,\n    left=BinaryOperation(\n      operator=GREATER,\n      left=Identifier(&quot;col2&quot;),\n      right=IntegerLiteral(10)\n    ),\n    right=UnaryOperation(\n      operator=IS_NULL,\n      operand=Identifier(&quot;col3&quot;)\n    )\n  )\n)\n</code></pre>\n</blockquote>\n<p><strong>Milestone 3 Success Indicators:</strong></p>\n<ul>\n<li>Complex expressions parse with correct operator precedence</li>\n<li>Boolean operators (AND, OR, NOT) have proper precedence relationships</li>\n<li>Parentheses correctly override default precedence rules</li>\n<li>NULL checks (IS NULL, IS NOT NULL) parse as unary operations</li>\n<li>Expression trees accurately represent the logical structure</li>\n</ul>\n<h4 id=\"milestone-4-data-modification-statement-validation\">Milestone 4: Data Modification Statement Validation</h4>\n<p>The DML parser milestone handles INSERT, UPDATE, and DELETE statements with their specific syntax requirements and validation rules.</p>\n<p><strong>Automated DML Parser Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Name</th>\n<th>SQL Input</th>\n<th>Expected AST Structure</th>\n<th>Validation Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_basic_insert</code></td>\n<td><code>&quot;INSERT INTO table (col1) VALUES (1)&quot;</code></td>\n<td>Column list matches value count</td>\n<td>Column-value correspondence</td>\n</tr>\n<tr>\n<td><code>test_multi_row_insert</code></td>\n<td><code>&quot;INSERT INTO table VALUES (1, &#39;a&#39;), (2, &#39;b&#39;)&quot;</code></td>\n<td>Multiple value rows with consistent structure</td>\n<td>Value row consistency</td>\n</tr>\n<tr>\n<td><code>test_update_statement</code></td>\n<td><code>&quot;UPDATE table SET col1 = 5 WHERE id = 1&quot;</code></td>\n<td>Assignment list and WHERE condition</td>\n<td>SET clause parsing</td>\n</tr>\n<tr>\n<td><code>test_delete_statement</code></td>\n<td><code>&quot;DELETE FROM table WHERE active = false&quot;</code></td>\n<td>Table reference and condition</td>\n<td>Simple DELETE structure</td>\n</tr>\n</tbody></table>\n<p><strong>Manual DML Parser Verification:</strong></p>\n<blockquote>\n<p><strong>Checkpoint Command:</strong> <code>python -m parser &quot;INSERT INTO products (name, price, category) VALUES (&#39;Laptop&#39;, 999.99, &#39;electronics&#39;), (&#39;Book&#39;, 19.99, &#39;education&#39;)&quot;</code></p>\n<p><strong>Expected AST Structure:</strong></p>\n<pre><code>InsertStatement(\n  table_reference=TableReference(\n    table=Identifier(&quot;products&quot;),\n    alias=None\n  ),\n  column_list=[\n    Identifier(&quot;name&quot;),\n    Identifier(&quot;price&quot;),\n    Identifier(&quot;category&quot;)\n  ],\n  value_rows=[\n    [StringLiteral(&quot;Laptop&quot;), FloatLiteral(999.99), StringLiteral(&quot;electronics&quot;)],\n    [StringLiteral(&quot;Book&quot;), FloatLiteral(19.99), StringLiteral(&quot;education&quot;)]\n  ]\n)\n</code></pre>\n</blockquote>\n<p><strong>Milestone 4 Success Indicators:</strong></p>\n<ul>\n<li>INSERT statements handle both column lists and value rows correctly</li>\n<li>UPDATE statements parse SET clauses with assignment expressions</li>\n<li>DELETE statements support WHERE conditions for selective deletion</li>\n<li>Column count validation ensures INSERT statements have consistent structure</li>\n<li>All DML statements integrate with expression parsing for conditions and values</li>\n</ul>\n<h3 id=\"common-testing-pitfalls\">Common Testing Pitfalls</h3>\n<p>Testing SQL parsers presents unique challenges due to the complexity of SQL syntax, the variety of valid statement patterns, and the need for comprehensive error handling.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Edge Case Coverage</strong>\nMany parser tests focus on &quot;happy path&quot; scenarios with well-formed SQL but fail to adequately test boundary conditions and unusual but valid syntax. For example, testing <code>SELECT col1, col2 FROM table</code> but not testing <code>SELECT col1,col2FROM table</code> (no spaces around comma and FROM). This leads to parsers that work with formatted SQL but fail with compressed or unusual formatting.</p>\n<p><strong>Why it&#39;s wrong:</strong> Real-world SQL often contains minimal whitespace, unusual casing, and edge cases that well-formatted test queries don&#39;t cover. Parsers must handle the full spectrum of valid SQL syntax, not just the pretty-printed versions.</p>\n<p><strong>How to fix:</strong> Create comprehensive test suites that systematically vary whitespace, casing, punctuation, and optional syntax elements. Use property-based testing to generate variations automatically.</p>\n<p>⚠️ <strong>Pitfall: Testing Implementation Details Instead of Behavior</strong>\nTests that verify specific internal parser states, function call sequences, or intermediate data structures couple tests too tightly to implementation details. For example, testing that the parser calls <code>parse_column_spec()</code> exactly three times instead of testing that three columns are correctly parsed.</p>\n<p><strong>Why it&#39;s wrong:</strong> Implementation-coupled tests break when internal refactoring occurs, even if external behavior remains correct. This creates maintenance overhead and discourages beneficial code improvements.</p>\n<p><strong>How to fix:</strong> Focus tests on observable outputs - the structure and content of produced AST nodes, error messages for invalid input, and behavioral characteristics rather than internal implementation steps.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Error Message Validation</strong>\nTests that only check whether an error occurs but don&#39;t validate the quality, accuracy, or helpfulness of error messages. For example, checking that <code>&quot;SELECT , FROM table&quot;</code> produces an error but not verifying that the error message identifies the unexpected comma and suggests valid alternatives.</p>\n<p><strong>Why it&#39;s wrong:</strong> Poor error messages significantly degrade the developer experience when using the parser. Users need specific, actionable information about what went wrong and how to fix it.</p>\n<p><strong>How to fix:</strong> Include error message content validation in all error test cases. Verify that error messages include position information, describe the specific problem, and suggest corrective actions.</p>\n<p>⚠️ <strong>Pitfall: Missing Integration Test Coverage</strong>\nTesting individual parser components thoroughly but failing to test their integration under realistic conditions. For example, testing the tokenizer and expression parser separately but not testing how tokenizer errors affect expression parsing.</p>\n<p><strong>Why it&#39;s wrong:</strong> Component integration often reveals issues that don&#39;t appear in isolated testing. Error propagation, resource management, and performance characteristics emerge only when components work together.</p>\n<p><strong>How to fix:</strong> Include comprehensive end-to-end test suites that exercise the complete parsing pipeline with realistic, complex SQL statements. Test error conditions that span multiple components.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"testing-framework-selection\">Testing Framework Selection</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing</td>\n<td>Python <code>unittest</code> with manual assertions</td>\n<td><code>pytest</code> with fixtures and parameterized tests</td>\n</tr>\n<tr>\n<td>Test Data</td>\n<td>Hardcoded SQL strings in test methods</td>\n<td>External SQL file fixtures with metadata</td>\n</tr>\n<tr>\n<td>AST Assertion</td>\n<td>Manual field-by-field comparison</td>\n<td>Custom AST equality matchers with diff output</td>\n</tr>\n<tr>\n<td>Error Testing</td>\n<td>Try/catch with exception type checking</td>\n<td>Dedicated error assertion helpers with message validation</td>\n</tr>\n<tr>\n<td>Coverage</td>\n<td>Manual test case enumeration</td>\n<td><code>coverage.py</code> with automated coverage reporting</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-test-file-structure\">Recommended Test File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql-parser/\n  tests/\n    unit/\n      test_tokenizer.py           ← Tokenizer component tests\n      test_select_parser.py       ← SELECT statement parser tests  \n      test_expression_parser.py   ← WHERE clause expression tests\n      test_dml_parser.py          ← INSERT/UPDATE/DELETE tests\n      test_ast_nodes.py           ← AST node interface tests\n    integration/\n      test_end_to_end.py          ← Complete parsing pipeline tests\n      test_error_propagation.py   ← Multi-component error handling\n    milestone/\n      test_milestone_1.py         ← Tokenizer milestone validation\n      test_milestone_2.py         ← SELECT parser milestone validation\n      test_milestone_3.py         ← Expression parser milestone validation\n      test_milestone_4.py         ← DML parser milestone validation\n    fixtures/\n      sql_samples/                ← Sample SQL files for testing\n        simple_queries.sql        ← Basic test cases\n        complex_queries.sql       ← Advanced integration tests\n        error_cases.sql           ← Invalid SQL for error testing\n      expected_ast/               ← Expected AST structures in JSON\n        simple_queries.json       ← Expected outputs for simple cases\n        complex_queries.json      ← Expected outputs for complex cases</code></pre></div>\n\n<h4 id=\"complete-test-utility-infrastructure\">Complete Test Utility Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/utils/test_helpers.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Complete testing utility infrastructure for SQL parser validation.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sql_parser.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, TokenType, Tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sql_parser.parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SQLParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sql_parser.ast_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ASTNode, SelectStatement, InsertStatement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sql_parser.errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseError, TokenizerError, </span><span style=\"color:#79B8FF\">SyntaxError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenExpectation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Expected token with type, value, and position information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_type: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> matches</span><span style=\"color:#E1E4E8\">(self, actual_token: Token) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if actual token matches this expectation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (actual_token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.token_type </span><span style=\"color:#F97583\">and</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                actual_token.value </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.value </span><span style=\"color:#F97583\">and</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                actual_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">and</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                actual_token.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ASTExpectation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Expected AST node structure with type and property validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    children: List[</span><span style=\"color:#9ECBFF\">'ASTExpectation'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> matches</span><span style=\"color:#E1E4E8\">(self, actual_node: ASTNode) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Recursively validate AST node structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> actual_node.node_type() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.node_type:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Validate properties</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> prop_name, expected_value </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.properties.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            actual_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(actual_node, prop_name, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> actual_value </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected_value:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Validate children</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        actual_children </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual_node.children()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(actual_children) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.children):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> expected_child, actual_child </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> zip</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.children, actual_children):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> expected_child.matches(actual_child):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerTestHelper</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper class for tokenizer testing with assertion methods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> assert_tokens_match</span><span style=\"color:#E1E4E8\">(expected: List[TokenExpectation], actual: List[Token]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Assert that actual tokens match expected token list.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(actual) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected), </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Token count mismatch: expected </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(expected)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(actual)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i, (expected_token, actual_token) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(expected, actual)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> expected_token.matches(actual_token), (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> mismatch: expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_token</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">', </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_token.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize_and_validate</span><span style=\"color:#E1E4E8\">(sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected_tokens: List[TokenExpectation]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Tokenize SQL and validate against expected tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Tokenizer(sql_text)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        actual_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer.tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenizerTestHelper.assert_tokens_match(expected_tokens, actual_tokens)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> actual_tokens</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParserTestHelper</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper class for parser testing with AST validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_and_validate</span><span style=\"color:#E1E4E8\">(sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected_ast: ASTExpectation) -> ASTNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse SQL and validate against expected AST structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SQLParser()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        actual_ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(sql_text)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> expected_ast.matches(actual_ast), (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"AST structure mismatch: expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_ast.node_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> with \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"properties </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_ast.properties</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_ast.node_type()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> actual_ast</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> assert_parse_error</span><span style=\"color:#E1E4E8\">(sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected_error_type: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          expected_message_content: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Assert that SQL parsing raises expected error with optional message validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SQLParser()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            parser.parse(sql_text)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_error_type.</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\"> but parsing succeeded\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> ParseError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(e, expected_error_type), (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_error_type.</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\"> but got </span><span style=\"color:#79B8FF\">{type</span><span style=\"color:#E1E4E8\">(e).</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> expected_message_content:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                assert</span><span style=\"color:#E1E4E8\"> expected_message_content </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> e.message, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"Expected error message to contain '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_message_content</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"but got '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FixtureLoader</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load test cases from external fixture files.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, fixture_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"tests/fixtures\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fixture_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(fixture_dir)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_sql_cases</span><span style=\"color:#E1E4E8\">(self, filename: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load SQL test cases from file, one per line or separated by semicolons.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fixture_dir </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"sql_samples\"</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> filename</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(file_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            content </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f.read()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Split on semicolons and clean up</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cases </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [case.strip() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> case </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> content.split(</span><span style=\"color:#9ECBFF\">';'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> case.strip()]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> cases</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_expected_ast</span><span style=\"color:#E1E4E8\">(self, filename: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ASTExpectation]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load expected AST structures from JSON file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fixture_dir </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"expected_ast\"</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> filename</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(file_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert JSON to ASTExpectation objects</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expectations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> test_name, ast_data </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            expectations[test_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._json_to_ast_expectation(ast_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> expectations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _json_to_ast_expectation</span><span style=\"color:#E1E4E8\">(self, json_data: Dict) -> ASTExpectation:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert JSON representation to ASTExpectation object.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ASTExpectation(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            node_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">json_data[</span><span style=\"color:#9ECBFF\">\"node_type\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            properties</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">json_data.get(</span><span style=\"color:#9ECBFF\">\"properties\"</span><span style=\"color:#E1E4E8\">, {}),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            children</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._json_to_ast_expectation(child) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                     for</span><span style=\"color:#E1E4E8\"> child </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> json_data.get(</span><span style=\"color:#9ECBFF\">\"children\"</span><span style=\"color:#E1E4E8\">, [])]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MilestoneValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validation logic for milestone acceptance criteria.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.test_helper </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ParserTestHelper()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fixture_loader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FixtureLoader()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_1</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate tokenizer milestone acceptance criteria.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load tokenizer test cases from fixtures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate keyword recognition (SELECT, FROM, WHERE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate identifier and number tokenization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate string literal parsing with quotes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate operator recognition (=, &#x3C;, >, AND, OR)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if all tests pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_2</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate SELECT parser milestone acceptance criteria.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load SELECT statement test cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate column list parsing (wildcard and explicit columns)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate FROM clause with table name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate comma-separated columns with AST generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate column and table aliases (AS keyword and implicit)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if all tests pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_3</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate WHERE clause expression parser milestone.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load WHERE clause test cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate comparison operators (=, &#x3C;, >, !=)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate boolean operators with precedence (AND, OR, NOT)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate parentheses override default precedence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate NULL checks (IS NULL, IS NOT NULL)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if all tests pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_4</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate INSERT/UPDATE/DELETE parser milestone.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load DML statement test cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate INSERT INTO table (cols) VALUES (vals)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate UPDATE table SET col=val WHERE condition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate DELETE FROM table WHERE condition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate multiple value rows in INSERT statements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if all tests pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-test-skeletons\">Core Logic Test Skeletons</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/unit/test_tokenizer.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Comprehensive tokenizer unit tests with edge case coverage.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tests.utils.test_helpers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenizerTestHelper, TokenExpectation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sql_parser.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType, TokenizerError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestTokenizerKeywords</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test SQL keyword recognition and case handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_basic_keywords_uppercase</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test recognition of uppercase SQL keywords.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test SELECT, FROM, WHERE, INSERT, UPDATE, DELETE keywords</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify each keyword produces correct TokenType</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate position tracking for each keyword</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_basic_keywords_lowercase</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test case-insensitive keyword recognition.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test select, from, where with lowercase input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify tokenizer normalizes to uppercase in value field</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure TokenType matches uppercase equivalent</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_mixed_case_keywords</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test keywords with mixed case like Select, From, Where.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test various case combinations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify consistent normalization behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestTokenizerIdentifiers</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test identifier parsing and boundary detection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_simple_identifiers</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test basic identifier patterns.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"table_name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"column1\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenizerTestHelper.tokenize_and_validate(</span><span style=\"color:#9ECBFF\">\"table_name column1\"</span><span style=\"color:#E1E4E8\">, expected)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_identifier_with_numbers</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test identifiers containing numeric characters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test patterns like table1, col_2, item123</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify numbers within identifiers don't split tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_identifier_keyword_boundaries</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test identifiers that start with keyword prefixes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test selectcolumn, fromtable (should be identifiers)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test column_select, table_from (should be identifiers)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestTokenizerStringLiterals</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test string literal parsing with quotes and escapes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_single_quoted_strings</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test basic single-quoted string literals.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">STRING_LITERAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"test value\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenizerTestHelper.tokenize_and_validate(</span><span style=\"color:#9ECBFF\">\"'test value'\"</span><span style=\"color:#E1E4E8\">, expected)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_double_quoted_strings</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test double-quoted string literals.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test \"test value\" produces STRING_LITERAL token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify quote characters are not included in value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_escaped_quotes_in_strings</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test escape sequence handling in string literals.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test 'O''Reilly' produces \"O'Reilly\" value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test \"She said \\\"hello\\\"\" with escaped quotes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_unterminated_string_error</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test error handling for unterminated strings.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test 'unterminated string without closing quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify TokenizerError is raised with correct position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check error message suggests adding closing quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestTokenizerOperators</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test operator recognition including multi-character operators.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_single_character_operators</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test basic single-character operators.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"= &#x3C; > +\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenizerTestHelper.tokenize_and_validate(sql, expected)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_multi_character_operators</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test multi-character operator recognition.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test &#x3C;= >= != operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify complete operator is recognized as single token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test that &#x3C; = is two tokens, &#x3C;= is one token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># tests/unit/test_select_parser.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"SELECT statement parser unit tests.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tests.utils.test_helpers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParserTestHelper, ASTExpectation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sql_parser.ast_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SelectStatement, StarExpression, AliasedExpression</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestSelectParserBasic</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test basic SELECT statement parsing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_select_star_from_table</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test simplest SELECT * FROM table pattern.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_ast </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ASTExpectation(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            node_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"SelectStatement\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            properties</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            children</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ASTExpectation(</span><span style=\"color:#9ECBFF\">\"StarExpression\"</span><span style=\"color:#E1E4E8\">, {}, []),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ASTExpectation(</span><span style=\"color:#9ECBFF\">\"TableReference\"</span><span style=\"color:#E1E4E8\">, {</span><span style=\"color:#9ECBFF\">\"alias\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">}, [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    ASTExpectation(</span><span style=\"color:#9ECBFF\">\"Identifier\"</span><span style=\"color:#E1E4E8\">, {</span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"table1\"</span><span style=\"color:#E1E4E8\">}, [])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ParserTestHelper.parse_and_validate(</span><span style=\"color:#9ECBFF\">\"SELECT * FROM table1\"</span><span style=\"color:#E1E4E8\">, expected_ast)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_select_column_list</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test SELECT with explicit column list.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test \"SELECT col1, col2 FROM table1\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify AliasedExpression nodes for each column</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check column names and absence of aliases</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_select_with_table_alias</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test SELECT with table alias.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test \"SELECT * FROM table1 t\" and \"SELECT * FROM table1 AS t\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify TableReference contains alias information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestSelectParserColumnSpecs</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test column specification parsing in SELECT statements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_qualified_column_names</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test table.column qualified identifiers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test \"SELECT t.col1, t.col2 FROM table1 t\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify QualifiedIdentifier nodes with correct qualifier</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_column_aliases_explicit</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test column aliases with AS keyword.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test \"SELECT col1 AS c1, col2 AS c2 FROM table1\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify AliasedExpression nodes contain alias values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_column_aliases_implicit</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test column aliases without AS keyword.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test \"SELECT col1 c1, col2 c2 FROM table1\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify implicit alias parsing works correctly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-implementations\">Milestone Checkpoint Implementations</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/milestone/test_milestone_1.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Milestone 1 validation: SQL Tokenizer acceptance criteria.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tests.utils.test_helpers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MilestoneValidator, TokenizerTestHelper, TokenExpectation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sql_parser.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestMilestone1Acceptance</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate all Milestone 1 acceptance criteria.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_keyword_recognition_criteria</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acceptance: Recognize SQL keywords (SELECT, FROM, WHERE).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        test_cases </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">\"SELECT\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">SELECT_KEYWORD</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">\"FROM\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">FROM_KEYWORD</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">\"WHERE\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">WHERE_KEYWORD</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">\"INSERT\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">INSERT_KEYWORD</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">\"UPDATE\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">UPDATE_KEYWORD</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">\"DELETE\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">DELETE_KEYWORD</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> keyword, expected_type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> test_cases:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [TokenExpectation(expected_type, keyword, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenizerTestHelper.tokenize_and_validate(keyword, expected)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_identifier_and_numbers_criteria</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acceptance: Handle identifiers and numbers with correct classification.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"table_name column1 123 45.67\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"table_name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"column1\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">INTEGER_LITERAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"123\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenExpectation(TokenType.</span><span style=\"color:#79B8FF\">FLOAT_LITERAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"45.67\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">24</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenizerTestHelper.tokenize_and_validate(sql, expected)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_string_literals_criteria</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acceptance: Parse string literals enclosed in single or double quotes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test single and double quoted strings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test escape sequence handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify STRING_LITERAL token type classification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_operators_criteria</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acceptance: Support operators (=, &#x3C;, >, AND, OR).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test all required operators from acceptance criteria</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify correct operator token type classification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include multi-character operators like &#x3C;= and >=</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_milestone_1_complete_validation</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Complete milestone 1 validation using comprehensive test.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        validator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MilestoneValidator()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This should pass if all individual criteria pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> validator.validate_milestone_1(), </span><span style=\"color:#9ECBFF\">\"Milestone 1 acceptance criteria not met\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># tests/milestone/test_milestone_2.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Milestone 2 validation: SELECT Parser acceptance criteria.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestMilestone2Acceptance</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate all Milestone 2 acceptance criteria.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_column_list_or_star_criteria</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acceptance: Parse column list or star wildcard in SELECT clause correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test SELECT * produces StarExpression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test SELECT col1, col2 produces column list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify correct AST node generation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_from_clause_criteria</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acceptance: Parse FROM clause with table name.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test FROM table_name produces TableReference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify table name extraction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_comma_separated_columns_criteria</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acceptance: Handle multiple comma-separated columns with correct AST.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test various column count combinations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify AST structure for each column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_aliases_criteria</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acceptance: Parse column and table aliases using AS keyword or implicit.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test explicit aliases with AS keyword</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test implicit aliases without AS keyword</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify alias information stored in AST</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-testing-hints\">Language-Specific Testing Hints</h4>\n<p><strong>Python Testing Best Practices:</strong></p>\n<ul>\n<li>Use <code>pytest</code> with parametrized tests for testing multiple similar cases</li>\n<li>Use <code>pytest.raises()</code> for error testing with message validation</li>\n<li>Create custom assertion helpers in <code>conftest.py</code> for AST comparison</li>\n<li>Use <code>dataclasses</code> for test expectation objects to reduce boilerplate</li>\n<li>Leverage <code>pathlib.Path</code> for fixture file loading across platforms</li>\n</ul>\n<p><strong>Test Data Management:</strong></p>\n<ul>\n<li>Store complex SQL samples in external <code>.sql</code> files rather than hardcoding strings</li>\n<li>Use JSON files for expected AST structures to enable test case reuse</li>\n<li>Implement test case generators for systematic variation testing</li>\n<li>Create helper functions that combine SQL input with expected output for parameterized tests</li>\n</ul>\n<p><strong>Performance Testing Integration:</strong></p>\n<ul>\n<li>Use <code>time.time()</code> or <code>timeit</code> module for basic performance benchmarking</li>\n<li>Set reasonable timeout limits for tests to catch infinite loops</li>\n<li>Monitor memory usage with <code>tracemalloc</code> for large input testing</li>\n<li>Include performance regression tests in continuous integration</li>\n</ul>\n<h4 id=\"debugging-test-failures\">Debugging Test Failures</h4>\n<table>\n<thead>\n<tr>\n<th>Test Failure Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token type mismatch</td>\n<td>Keyword lookup failure</td>\n<td>Check <code>SQL_KEYWORDS</code> mapping and case handling</td>\n<td>Add missing keywords or fix case normalization</td>\n</tr>\n<tr>\n<td>Wrong token positions</td>\n<td>Position tracking bugs</td>\n<td>Print actual line/column vs expected</td>\n<td>Fix <code>_advance()</code> and line counting logic</td>\n</tr>\n<tr>\n<td>AST structure differences</td>\n<td>Parser logic errors</td>\n<td>Use AST visitor to print actual structure</td>\n<td>Debug recursive descent function calls</td>\n</tr>\n<tr>\n<td>Missing AST children</td>\n<td>Incomplete parsing</td>\n<td>Check if parser consumes all expected tokens</td>\n<td>Verify parser methods call child parsing functions</td>\n</tr>\n<tr>\n<td>Error message confusion</td>\n<td>Poor error reporting</td>\n<td>Print actual error message and compare to expected</td>\n<td>Improve error message content and context</td>\n</tr>\n<tr>\n<td>Test case brittleness</td>\n<td>Over-specific assertions</td>\n<td>Review what properties actually matter for correctness</td>\n<td>Focus on semantic correctness, not formatting</td>\n</tr>\n</tbody></table>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-4), providing systematic debugging strategies that help developers diagnose and resolve common implementation issues across tokenization, parsing, and AST construction phases.</p>\n</blockquote>\n<h3 id=\"mental-model-detective-work\">Mental Model: Detective Work</h3>\n<p>Think of debugging a parser like being a detective investigating a crime scene. Just as a detective collects evidence, follows clues, and reconstructs the sequence of events that led to the crime, a parser debugger examines symptoms, traces execution paths, and reconstructs the sequence of tokenization and parsing decisions that led to incorrect behavior. The key insight is that parser bugs leave traces throughout the processing pipeline - a tokenization error creates malformed tokens that cascade into parsing errors, while a parsing logic error produces malformed AST structures that reveal the faulty decision points.</p>\n<p>Every parser bug tells a story through its symptoms. A <code>TokenType.IDENTIFIER</code> appearing where <code>TokenType.SELECT_KEYWORD</code> was expected might indicate case sensitivity issues in keyword recognition. An <code>UnexpectedTokenError</code> with suggestions pointing to multiple valid alternatives often reveals ambiguous grammar rules or insufficient lookahead. A well-constructed <code>ASTNode</code> hierarchy with incorrect child relationships typically indicates successful parsing with flawed recursive descent logic. Understanding these diagnostic patterns transforms debugging from random trial-and-error into systematic investigation.</p>\n<p>The debugging process mirrors the parser&#39;s own architecture: we start with symptoms at the surface (incorrect final output), trace backward through the AST construction phase to identify structural issues, then examine the parsing logic for faulty token consumption patterns, and finally investigate the tokenization phase for lexical analysis problems. This layered debugging approach ensures we identify root causes rather than treating symptoms, leading to more robust parser implementations.</p>\n<h3 id=\"tokenizer-debugging-techniques\">Tokenizer Debugging Techniques</h3>\n<p>Tokenizer debugging focuses on diagnosing issues in the lexical analysis phase where character sequences transform into classified tokens. The most effective debugging strategy begins with <strong>token stream inspection</strong> - examining the complete sequence of tokens produced by the <code>Tokenizer</code> to identify unexpected token types, missing tokens, or incorrect token boundaries. This inspection reveals whether problems originate in character scanning logic, keyword recognition, or token classification algorithms.</p>\n<p>The <code>TokenizerTestHelper</code> provides systematic debugging capabilities through its token inspection methods. The <code>tokenize_only()</code> function bypasses parsing entirely, allowing isolated examination of tokenization behavior without interference from downstream parser logic. This isolation proves essential when distinguishing between tokenization failures and parsing failures that might initially present similar symptoms.</p>\n<p><strong>Position tracking verification</strong> represents another critical debugging technique. Every <code>Token</code> instance contains <code>line</code> and <code>column</code> fields that must accurately reflect the token&#39;s position in the original SQL text. Incorrect position tracking typically manifests as off-by-one errors in error reporting or misaligned token boundaries when examining multi-line SQL statements. The <code>_advance()</code> method in the <code>Tokenizer</code> must correctly increment line numbers when encountering newline characters and reset column numbers to handle line boundaries properly.</p>\n<p><strong>Character boundary analysis</strong> helps diagnose issues where tokens appear truncated, extended beyond their proper boundaries, or merged with adjacent tokens. The <code>_current_char()</code> and <code>_peek_char()</code> methods provide the foundation for character-level debugging - inserting temporary logging statements that reveal the character scanning sequence can expose logic errors in character advancement, whitespace skipping, or string literal parsing.</p>\n<p><strong>Keyword recognition debugging</strong> addresses the common challenge of distinguishing SQL keywords from regular identifiers. The <code>lookup_keyword()</code> method performs case-insensitive matching against the <code>SQL_KEYWORDS</code> mapping, but bugs often emerge from incorrect case handling or incomplete keyword tables. Testing keyword recognition involves verifying that &quot;SELECT&quot;, &quot;select&quot;, &quot;Select&quot;, and &quot;sElEcT&quot; all produce <code>TokenType.SELECT_KEYWORD</code> tokens rather than <code>TokenType.IDENTIFIER</code> tokens.</p>\n<table>\n<thead>\n<tr>\n<th>Debugging Technique</th>\n<th>When to Apply</th>\n<th>Information Revealed</th>\n<th>Tools/Methods</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Stream Inspection</td>\n<td>Always first step</td>\n<td>Complete tokenization output</td>\n<td><code>tokenize_only()</code>, manual token examination</td>\n</tr>\n<tr>\n<td>Position Tracking Verification</td>\n<td>Error reporting issues</td>\n<td>Line/column accuracy</td>\n<td>Token position fields, multi-line test cases</td>\n</tr>\n<tr>\n<td>Character Boundary Analysis</td>\n<td>Token truncation/merging</td>\n<td>Character scanning behavior</td>\n<td><code>_current_char()</code>, <code>_peek_char()</code> logging</td>\n</tr>\n<tr>\n<td>Keyword Recognition Testing</td>\n<td>Identifier/keyword confusion</td>\n<td>Case sensitivity issues</td>\n<td><code>lookup_keyword()</code> with various case combinations</td>\n</tr>\n<tr>\n<td>String Literal Boundary Testing</td>\n<td>Quote parsing problems</td>\n<td>String parsing logic</td>\n<td>Test cases with embedded quotes, escape sequences</td>\n</tr>\n<tr>\n<td>Operator Recognition Testing</td>\n<td>Multi-character operator issues</td>\n<td>Character sequence handling</td>\n<td>Test &quot;&lt;=&quot;, &quot;&gt;=&quot;, &quot;!=&quot;, &quot;&lt;&gt;&quot; recognition</td>\n</tr>\n</tbody></table>\n<p><strong>String literal debugging</strong> requires special attention due to the complexity of quote character handling, escape sequences, and embedded quote recognition. The <code>_read_string_literal()</code> method must correctly handle both single and double quotes as string delimiters, process escape sequences like <code>\\&#39;</code> and <code>\\&quot;</code>, and implement the SQL standard&#39;s doubled quote convention where <code>&#39;&#39;</code> represents a literal single quote within a single-quoted string. Common bugs include incorrect termination detection, failure to advance past closing quotes, and improper escape sequence processing.</p>\n<p><strong>Operator tokenization debugging</strong> focuses on multi-character operators like <code>&lt;=</code>, <code>&gt;=</code>, <code>!=</code>, and <code>&lt;&gt;</code> where the tokenizer must implement maximal munch behavior - always preferring the longest possible token match. Debugging involves verifying that <code>&lt;=</code> produces a single <code>TokenType.LESS_THAN_OR_EQUAL</code> token rather than separate <code>TokenType.LESS_THAN</code> and <code>TokenType.EQUALS</code> tokens. The <code>_parse_operator()</code> method requires lookahead logic to distinguish single-character from multi-character operators.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Tokenizer bugs compound geometrically through the parser pipeline. A single misclassified token can trigger cascade failures in multiple parsing functions, making the original tokenization error difficult to identify from parser-level symptoms alone. Always verify tokenization correctness before investigating parser logic issues.</p>\n</blockquote>\n<h3 id=\"parser-logic-debugging\">Parser Logic Debugging</h3>\n<p>Parser logic debugging targets the recursive descent parsing phase where token sequences transform into AST structures. Unlike tokenization debugging, which focuses on linear token stream analysis, parser debugging requires understanding the <strong>call stack dynamics</strong> of recursive descent functions and the <strong>state evolution</strong> of the parser&#39;s token position and AST construction process.</p>\n<p><strong>Parse trace debugging</strong> provides the most comprehensive view of parser behavior by logging entry and exit points of every parsing function along with the current token position and consumed tokens. This technique reveals whether parsing functions correctly recognize their expected input patterns, properly advance the token position, and construct appropriate AST nodes. The <code>BaseParser</code> class maintains the <code>position</code> field and <code>current_token</code> reference that form the core of parser state - these values should advance monotonically through successful parsing operations.</p>\n<p><strong>Token consumption debugging</strong> examines the interaction between <code>expect_token()</code>, <code>consume_token()</code>, and <code>peek_token()</code> methods that control token stream advancement. Correct parser logic ensures that every consumed token contributes to AST construction and that lookahead operations using <code>peek_token()</code> don&#39;t inadvertently advance the parser position. Common bugs include forgetting to consume expected tokens, consuming tokens without incorporating them into the AST, or advancing past error positions without proper error reporting.</p>\n<p><strong>AST construction verification</strong> focuses on validating that recursive descent functions build correctly structured AST node hierarchies. Each parsing function should construct AST nodes that accurately represent the parsed SQL construct with proper parent-child relationships and complete field population. The <code>ASTNode.children()</code> method provides access to the node hierarchy for debugging purposes, while the <code>source_location()</code> method enables verification that AST nodes maintain accurate position information from their corresponding tokens.</p>\n<p><strong>Precedence climbing debugging</strong> specifically targets the <code>ExpressionParser.parse_expression()</code> method and its handling of operator precedence and associativity rules. The precedence climbing algorithm relies on correct implementation of <code>get_operator_precedence()</code> and <code>is_right_associative()</code> functions, along with precise calculation of binding powers for recursive calls. Debugging involves verifying that expressions like <code>a AND b OR c</code> produce AST trees where the <code>OR</code> operation becomes the root node with <code>AND</code> operation as its left child, reflecting the correct precedence relationship.</p>\n<table>\n<thead>\n<tr>\n<th>Debugging Category</th>\n<th>Focus Area</th>\n<th>Key Questions</th>\n<th>Investigation Methods</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parse Trace Analysis</td>\n<td>Function call sequence</td>\n<td>Which functions execute? Where do they fail?</td>\n<td>Logging entry/exit points, call stack examination</td>\n</tr>\n<tr>\n<td>Token Consumption</td>\n<td>Position advancement</td>\n<td>Are tokens consumed correctly? Is position tracking accurate?</td>\n<td>Monitor <code>position</code> field, verify <code>consume_token()</code> calls</td>\n</tr>\n<tr>\n<td>AST Construction</td>\n<td>Node hierarchy</td>\n<td>Do AST structures match expected patterns?</td>\n<td>Examine <code>children()</code> relationships, field population</td>\n</tr>\n<tr>\n<td>Precedence Climbing</td>\n<td>Expression parsing</td>\n<td>Are operators parsed with correct precedence?</td>\n<td>Test complex expressions, verify AST tree structure</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Failure handling</td>\n<td>How does parser respond to invalid input?</td>\n<td>Test malformed SQL, verify error messages</td>\n</tr>\n<tr>\n<td>Lookahead Logic</td>\n<td>Future token examination</td>\n<td>Does lookahead correctly predict parsing paths?</td>\n<td>Monitor <code>peek_token()</code> usage, verify prediction accuracy</td>\n</tr>\n</tbody></table>\n<p><strong>Grammar rule validation</strong> ensures that parsing functions correctly implement their corresponding grammar rules from the SQL specification. Each function like <code>parse_select_statement()</code>, <code>parse_column_list()</code>, and <code>parse_table_reference()</code> embodies specific grammar productions. Debugging involves comparing the function&#39;s implementation against its formal grammar definition to verify correct handling of optional elements, alternative productions, and sequencing requirements.</p>\n<p><strong>Synchronization point debugging</strong> addresses error recovery behavior when the parser encounters unexpected tokens or syntax violations. The <code>ErrorRecoveryManager</code> implements panic-mode recovery that skips tokens until reaching predetermined synchronization points like statement boundaries or major clause keywords. Debugging focuses on verifying that error recovery doesn&#39;t inadvertently skip valid tokens or fail to resume parsing at appropriate positions.</p>\n<p><strong>Context-dependent parsing debugging</strong> handles situations where the same token sequence might have different meanings depending on parsing context. SQL&#39;s grammar includes several context-dependent constructs - for example, identifiers that might represent column names, table names, or alias names depending on their position within the statement. The parser maintains context through its call stack and current parsing state, making context debugging particularly challenging.</p>\n<blockquote>\n<p><strong>Key Debugging Principle</strong>: Parser bugs often manifest far from their root cause due to the recursive nature of descent parsing. An error in <code>parse_primary_expression()</code> might not surface until <code>parse_select_statement()</code> attempts to construct its final AST node. Always trace backward from error symptoms to identify the earliest point of incorrect behavior.</p>\n</blockquote>\n<h3 id=\"common-bug-symptom-analysis\">Common Bug Symptom Analysis</h3>\n<p>Systematic bug analysis relies on recognizing patterns between observed symptoms and their underlying causes. The following diagnostic table provides a structured approach to identifying and resolving the most frequent parser implementation issues across all milestone phases.</p>\n<p><strong>Tokenization Phase Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Keywords parsed as <code>IDENTIFIER</code> tokens</td>\n<td>Case sensitivity in <code>lookup_keyword()</code></td>\n<td>Test &quot;SELECT&quot; vs &quot;select&quot; recognition</td>\n<td>Implement case-insensitive keyword matching with <code>str.upper()</code></td>\n</tr>\n<tr>\n<td>String literals missing closing quotes</td>\n<td>Incorrect termination logic in <code>_read_string_literal()</code></td>\n<td>Examine quote character advancement</td>\n<td>Fix quote consumption after string content parsing</td>\n</tr>\n<tr>\n<td>Numbers parsed as multiple tokens</td>\n<td>Character boundary detection failure</td>\n<td>Test &quot;123.45&quot; and &quot;1e10&quot; tokenization</td>\n<td>Improve decimal point and scientific notation handling</td>\n</tr>\n<tr>\n<td>Multi-character operators split incorrectly</td>\n<td>Missing lookahead in <code>_parse_operator()</code></td>\n<td>Test &quot;&lt;=&quot; becoming &quot;&lt;&quot; + &quot;=&quot;</td>\n<td>Implement maximal munch with <code>_peek_char()</code> lookahead</td>\n</tr>\n<tr>\n<td>Position tracking incorrect</td>\n<td>Line/column advancement logic errors</td>\n<td>Compare reported positions with actual text locations</td>\n<td>Fix newline detection and column reset logic</td>\n</tr>\n<tr>\n<td>Whitespace included in token values</td>\n<td>Incomplete <code>_skip_whitespace()</code> implementation</td>\n<td>Examine tokens for leading/trailing spaces</td>\n<td>Ensure whitespace skipping before token recognition</td>\n</tr>\n</tbody></table>\n<p><strong>Parsing Phase Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>UnexpectedTokenError</code> with correct tokens</td>\n<td>Incorrect <code>expect_token()</code> usage</td>\n<td>Verify expected vs actual token types</td>\n<td>Review grammar rule implementation for token sequence</td>\n</tr>\n<tr>\n<td>Parser infinite loop</td>\n<td>Missing <code>consume_token()</code> calls</td>\n<td>Monitor parser position advancement</td>\n<td>Add missing token consumption in parsing functions</td>\n</tr>\n<tr>\n<td>Incomplete AST nodes</td>\n<td>Partial field population</td>\n<td>Examine AST node field values</td>\n<td>Complete all required field assignments in constructors</td>\n</tr>\n<tr>\n<td>Incorrect operator precedence</td>\n<td>Wrong <code>OPERATOR_PRECEDENCE</code> mapping</td>\n<td>Test &quot;a AND b OR c&quot; expression parsing</td>\n<td>Correct precedence values in operator table</td>\n</tr>\n<tr>\n<td>Missing optional clauses</td>\n<td>Lookahead logic failures</td>\n<td>Test queries with/without optional elements</td>\n<td>Implement proper <code>peek_token()</code> for optional detection</td>\n</tr>\n<tr>\n<td>Malformed error messages</td>\n<td>Insufficient context in error creation</td>\n<td>Review error message content and position</td>\n<td>Enhance error creation with parsing context information</td>\n</tr>\n</tbody></table>\n<p><strong>Expression Parsing Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Right-associative operators parsed left-associatively</td>\n<td>Incorrect associativity handling</td>\n<td>Test &quot;a = b = c&quot; expression parsing</td>\n<td>Fix <code>is_right_associative()</code> and binding power calculation</td>\n</tr>\n<tr>\n<td>Parentheses not overriding precedence</td>\n<td>Parenthetical expression parsing bugs</td>\n<td>Test &quot;(a OR b) AND c&quot; vs &quot;a OR (b AND c)&quot;</td>\n<td>Debug <code>parse_parenthesized_expression()</code> implementation</td>\n</tr>\n<tr>\n<td>Unary operators binding incorrectly</td>\n<td>Wrong unary precedence assignment</td>\n<td>Test &quot;NOT a AND b&quot; expression</td>\n<td>Assign highest precedence to unary operators</td>\n</tr>\n<tr>\n<td>NULL comparisons parsed incorrectly</td>\n<td>Missing NULL test recognition</td>\n<td>Test &quot;column IS NULL&quot; expressions</td>\n<td>Implement <code>parse_null_test_expression()</code> properly</td>\n</tr>\n<tr>\n<td>Qualified identifiers not recognized</td>\n<td>Dot operator handling issues</td>\n<td>Test &quot;table.column&quot; parsing</td>\n<td>Fix qualified identifier parsing in <code>parse_identifier_expression()</code></td>\n</tr>\n<tr>\n<td>Function calls parsed as identifiers</td>\n<td>Missing function call detection</td>\n<td>Test &quot;COUNT(*)&quot; parsing</td>\n<td>Add function call recognition in primary expression parsing</td>\n</tr>\n</tbody></table>\n<p><strong>Statement Construction Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Column count mismatch in INSERT</td>\n<td>Missing validation in <code>parse_insert_statement()</code></td>\n<td>Test INSERT with mismatched columns/values</td>\n<td>Implement <code>validate_column_value_consistency()</code></td>\n</tr>\n<tr>\n<td>UPDATE without WHERE parsed incorrectly</td>\n<td>Optional WHERE clause handling</td>\n<td>Test UPDATE statements without WHERE</td>\n<td>Make WHERE clause properly optional in UPDATE parsing</td>\n</tr>\n<tr>\n<td>DELETE safety warnings not generated</td>\n<td>Missing safety flag assignment</td>\n<td>Test &quot;DELETE FROM table&quot; without WHERE</td>\n<td>Add safety warnings for DELETE without WHERE conditions</td>\n</tr>\n<tr>\n<td>Alias recognition inconsistent</td>\n<td>AS keyword optionality issues</td>\n<td>Test both &quot;table AS alias&quot; and &quot;table alias&quot;</td>\n<td>Implement both explicit and implicit alias recognition</td>\n</tr>\n<tr>\n<td>Multiple value rows not parsed</td>\n<td>Value list iteration logic</td>\n<td>Test &quot;VALUES (1,2), (3,4)&quot; parsing</td>\n<td>Fix <code>parse_value_row_list()</code> loop implementation</td>\n</tr>\n<tr>\n<td>Assignment expressions malformed</td>\n<td>SET clause parsing errors</td>\n<td>Test &quot;SET col1=val1, col2=val2&quot;</td>\n<td>Debug comma-separated assignment list parsing</td>\n</tr>\n</tbody></table>\n<p><strong>Error Recovery Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parser stops at first error</td>\n<td>No error recovery implementation</td>\n<td>Test multiple syntax errors in single statement</td>\n<td>Implement panic-mode recovery with synchronization points</td>\n</tr>\n<tr>\n<td>Cascade error avalanche</td>\n<td>Insufficient error suppression</td>\n<td>Examine error count and proximity</td>\n<td>Add cascade error detection and suppression</td>\n</tr>\n<tr>\n<td>Incorrect error positions</td>\n<td>Position tracking during recovery</td>\n<td>Verify error line/column accuracy</td>\n<td>Maintain accurate position during token skipping</td>\n</tr>\n<tr>\n<td>Recovery never synchronizes</td>\n<td>Wrong synchronization points</td>\n<td>Test recovery behavior after various error types</td>\n<td>Choose better synchronization token types</td>\n</tr>\n<tr>\n<td>False error suppression</td>\n<td>Overly aggressive cascade detection</td>\n<td>Test legitimate multiple errors</td>\n<td>Tune <code>CASCADE_SUPPRESSION_DISTANCE</code> parameter</td>\n</tr>\n<tr>\n<td>Missing error context</td>\n<td>Insufficient error message detail</td>\n<td>Review error message informativeness</td>\n<td>Add parsing context and suggestions to error messages</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Debugging Without Systematic Approach</strong></p>\n<p>Many developers attempt parser debugging through random code changes or by adding print statements throughout the codebase without understanding the specific failure mode. This approach often introduces additional bugs while failing to identify root causes. Instead, always begin with symptom classification using the diagnostic tables above, then apply targeted debugging techniques specific to the identified failure category.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Error Position Information</strong></p>\n<p>Parser errors often occur at positions different from where symptoms become apparent. The <code>source_location()</code> method and <code>Token</code> position fields provide critical debugging information that pinpoints exact failure locations. Developers frequently overlook this position data and instead focus on high-level symptoms, making bug localization unnecessarily difficult.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The debugging infrastructure requires systematic tooling that supports both automated testing and interactive investigation of parser behavior. The following implementation provides comprehensive debugging capabilities while maintaining clear separation between diagnostic tools and production parser code.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Debug Logging</td>\n<td>Python <code>logging</code> module with configurable levels</td>\n<td>Structured logging with JSON output and external log analysis</td>\n</tr>\n<tr>\n<td>Token Inspection</td>\n<td>Simple <code>print()</code> statements with token formatting</td>\n<td>Rich console output with syntax highlighting using <code>rich</code> library</td>\n</tr>\n<tr>\n<td>AST Visualization</td>\n<td>Text-based tree printing with indentation</td>\n<td>Graphical tree rendering with <code>graphviz</code> or web-based visualization</td>\n</tr>\n<tr>\n<td>Test Case Management</td>\n<td>Built-in <code>unittest</code> with manual test cases</td>\n<td><code>pytest</code> with parameterized tests and fixture management</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td>Basic timing with <code>time.time()</code></td>\n<td>Advanced profiling with <code>cProfile</code> and memory usage tracking</td>\n</tr>\n<tr>\n<td>Error Analysis</td>\n<td>Manual error message inspection</td>\n<td>Automated error pattern recognition and categorization</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Debug Infrastructure Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql_parser/\n  debug/\n    __init__.py                    ← debug utilities public interface\n    tokenizer_debugger.py         ← tokenizer-specific debugging tools\n    parser_debugger.py            ← parser-specific debugging tools\n    ast_inspector.py              ← AST structure analysis tools\n    error_analyzer.py             ← error pattern analysis and reporting\n    test_case_generator.py        ← automated test case generation\n  tests/\n    debug/\n      test_tokenizer_debugger.py  ← debug tool validation\n      test_parser_debugger.py     ← parser debugging validation\n    fixtures/\n      failing_sql_cases.txt       ← known problematic SQL examples\n      expected_error_patterns.json ← expected error message patterns\n  examples/\n    debug_session_examples.py     ← interactive debugging examples</code></pre></div>\n\n<p><strong>Complete Tokenizer Debugging Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tokenizer, Token, TokenType, TokenizerError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DebugLevel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BASIC</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"basic\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DETAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"detailed\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VERBOSE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"verbose\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerDiagnostic</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    character: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_start: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_value: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_type: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    issues: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokenizer: Tokenizer, debug_level: DebugLevel </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DebugLevel.</span><span style=\"color:#79B8FF\">BASIC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.debug_level </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> debug_level</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.diagnostics: List[TokenizerDiagnostic] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._setup_logger()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _setup_logger</span><span style=\"color:#E1E4E8\">(self) -> logging.Logger:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create logger with appropriate formatting for tokenizer debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set log level based on debug_level (INFO for BASIC, DEBUG for DETAILED/VERBOSE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add console handler with formatted output showing position and token info</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> debug_tokenization</span><span style=\"color:#E1E4E8\">(self, sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Perform tokenization with comprehensive debugging output.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Logs every character advancement, token recognition, and potential issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Reset diagnostics and configure tokenizer with sql_text</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Instrument tokenizer methods to capture character-level scanning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Log token recognition events with position information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Detect and report potential issues (case sensitivity, boundary problems)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate summary report of tokenization process</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use monkey patching or inheritance to add debug logging to tokenizer methods</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_keyword_recognition</span><span style=\"color:#E1E4E8\">(self, test_cases: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Test keyword recognition with various case combinations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns mapping of test case to pass/fail status.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through test_cases dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Tokenize each test case and extract first token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare actual token type with expected token type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log mismatches with detailed case information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return success/failure mapping for test suite analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inspect_token_boundaries</span><span style=\"color:#E1E4E8\">(self, sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[TokenizerDiagnostic]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze token boundary detection for potential truncation or extension issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Tokenize sql_text while tracking character-level position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each token, verify that token value matches expected character sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for unexpected whitespace inclusion in token values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify potential off-by-one errors in token start/end positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate diagnostic reports for boundary issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_position_tracking</span><span style=\"color:#E1E4E8\">(self, multiline_sql: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Verify that line and column tracking correctly handles multi-line SQL statements.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Tokenize multiline SQL with embedded newlines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each token, verify line/column accuracy against manual calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check that newline characters correctly increment line and reset column</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate that position information enables accurate error reporting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if all position tracking is accurate, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Complete Parser Debugging Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional, Any, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SQLParser, BaseParser, SelectParser, ExpressionParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..ast_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ASTNode, SelectStatement, BinaryOperation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseError, UnexpectedTokenError, </span><span style=\"color:#79B8FF\">SyntaxError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseTraceEntry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    function_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry_position: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry_token: Optional[Token]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exit_position: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exit_token: Optional[Token]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result_node: Optional[ASTNode]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error: Optional[ParseError]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParserDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, parser: SQLParser):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.trace_entries: List[ParseTraceEntry] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.ast_snapshots: Dict[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, ASTNode] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> debug_parse_statement</span><span style=\"color:#E1E4E8\">(self, sql_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[ASTNode]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse SQL statement with comprehensive debugging trace and AST inspection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize trace collection and instrument parser methods</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute parsing with entry/exit logging for each function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Capture AST node creation and modification events</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record error occurrences with full context information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate detailed parsing report with trace analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use decorators or context managers to instrument parsing functions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> trace_function</span><span style=\"color:#E1E4E8\">(self, function_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Context manager for tracing individual parsing function execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Record function entry with current parser state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start timing measurement for performance analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Yield control to traced function execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record function exit with result and final parser state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate execution time and update trace entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_precedence_parsing</span><span style=\"color:#E1E4E8\">(self, expression_sql: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Deep analysis of expression parsing with operator precedence validation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse expression while tracking precedence climbing decisions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Log each precedence comparison and binding power calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate final AST structure against expected operator precedence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify associativity handling for operators of equal precedence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate report with precedence decision tree visualization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inspect_ast_structure</span><span style=\"color:#E1E4E8\">(self, root_node: ASTNode, max_depth: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate detailed textual representation of AST structure for inspection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement recursive AST traversal with depth tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Format node information with type, fields, and child relationships</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Include source location information for error debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Detect and highlight potential structural issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return formatted string suitable for console or log output</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_error_recovery</span><span style=\"color:#E1E4E8\">(self, malformed_sql: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Test error recovery behavior with intentionally malformed SQL statements.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse malformed SQL and capture all error occurrences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Analyze error recovery synchronization point selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify that parser continues after errors when possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check error message quality and position accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate recovery behavior analysis report</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Debugging Checkpoints:</strong></p>\n<p><strong>Milestone 1 - Tokenizer Validation:</strong></p>\n<ul>\n<li><strong>Test Command</strong>: <code>python -m sql_parser.debug.tokenizer_debugger test_keyword_cases.py</code></li>\n<li><strong>Expected Output</strong>: All SQL keywords recognized correctly in upper, lower, and mixed case</li>\n<li><strong>Manual Verification</strong>: Tokenize &quot;SELECT * FROM users WHERE id = 123&quot; and verify 9 tokens with correct types</li>\n<li><strong>Failure Signs</strong>: Keywords appearing as IDENTIFIER tokens, missing tokens, incorrect position information</li>\n</ul>\n<p><strong>Milestone 2 - SELECT Parser Validation:</strong></p>\n<ul>\n<li><strong>Test Command</strong>: <code>python -m sql_parser.debug.parser_debugger test_select_parsing.py</code></li>\n<li><strong>Expected Output</strong>: Valid AST tree with SelectStatement root, column list, and table reference</li>\n<li><strong>Manual Verification</strong>: Parse &quot;SELECT a, b AS alias FROM table1&quot; and inspect AST structure</li>\n<li><strong>Failure Signs</strong>: UnexpectedTokenError for valid syntax, incomplete AST nodes, missing alias handling</li>\n</ul>\n<p><strong>Milestone 3 - Expression Parser Validation:</strong></p>\n<ul>\n<li><strong>Test Command</strong>: <code>python -m sql_parser.debug.parser_debugger test_expression_precedence.py</code></li>\n<li><strong>Expected Output</strong>: Expression AST with correct operator precedence and associativity</li>\n<li><strong>Manual Verification</strong>: Parse &quot;a AND b OR c = d&quot; and verify OR at root with AND as left child</li>\n<li><strong>Failure Signs</strong>: Incorrect precedence in AST tree, wrong associativity for equal precedence operators</li>\n</ul>\n<p><strong>Milestone 4 - DML Parser Validation:</strong></p>\n<ul>\n<li><strong>Test Command</strong>: <code>python -m sql_parser.debug.parser_debugger test_dml_statements.py</code></li>\n<li><strong>Expected Output</strong>: Valid INSERT, UPDATE, DELETE statement AST nodes with proper structure</li>\n<li><strong>Manual Verification</strong>: Parse all three DML statement types and verify complete field population</li>\n<li><strong>Failure Signs</strong>: Column/value count mismatches, missing WHERE clause handling, incomplete SET expressions</li>\n</ul>\n<h2 id=\"future-extensions-and-extensibility\">Future Extensions and Extensibility</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides guidance for extending all milestones (1-4), establishing extensibility patterns and future roadmap considerations that help developers build upon the foundational parser architecture.</p>\n</blockquote>\n<h3 id=\"mental-model-language-evolution\">Mental Model: Language Evolution</h3>\n<p>Think of our SQL parser like a city&#39;s infrastructure - we&#39;re building the initial roads, utilities, and zoning laws that will support future development. Just as a well-planned city can expand from a town to a metropolis without completely rebuilding its core infrastructure, our parser architecture must anticipate growth while maintaining its foundational integrity. The tokenizer is like the postal system that must handle new address formats, the parser is like the legal system that must accommodate new types of laws, and the AST is like the urban planning framework that must represent increasingly complex structures.</p>\n<p>The key insight is that <strong>extensibility isn&#39;t just about adding new features - it&#39;s about preserving the conceptual clarity and architectural integrity that made the original system comprehensible</strong>. Each extension should feel like a natural evolution of the existing patterns rather than an awkward bolt-on addition.</p>\n<h3 id=\"grammar-extension-patterns\">Grammar Extension Patterns</h3>\n<p>Our parser&#39;s extensibility depends on recognizing that SQL growth follows predictable patterns. New features typically fall into one of several categories: new keywords that introduce statement variations, new operators that extend expression capabilities, new data types that expand literal parsing, and new statement types that follow existing structural conventions.</p>\n<h4 id=\"keyword-extension-strategy\">Keyword Extension Strategy</h4>\n<p>The foundation of keyword extensibility lies in our centralized <code>SQL_KEYWORDS</code> mapping and the <code>TokenType</code> enum. When SQL standards introduce new keywords like <code>WITH</code> for common table expressions or <code>WINDOW</code> for window functions, our architecture can accommodate them through a systematic extension process.</p>\n<p><strong>New keywords follow this integration pattern</strong>:</p>\n<ol>\n<li>Add the new token type to the <code>TokenType</code> enum with a descriptive name following our <code>KEYWORD</code> suffix convention</li>\n<li>Insert the keyword-to-token mapping in the <code>SQL_KEYWORDS</code> dictionary with uppercase key for case-insensitive matching</li>\n<li>Update the tokenizer&#39;s <code>lookup_keyword()</code> method to recognize the new keyword automatically through the expanded mapping</li>\n<li>Extend the parser&#39;s grammar rules to incorporate the new keyword in appropriate contexts</li>\n<li>Create corresponding AST node types to represent the new syntax constructs</li>\n</ol>\n<p>The tokenizer requires no modifications beyond updating the keyword dictionary because it uses <strong>table-driven keyword recognition</strong> rather than hardcoded conditional logic. This design decision pays dividends during extension - adding <code>WINDOW_KEYWORD</code> is identical in complexity to adding any other keyword.</p>\n<p>However, keyword extension involves important <strong>precedence and context considerations</strong>. Some keywords are context-sensitive (like <code>AS</code> which can appear in multiple statement contexts), while others introduce new parsing contexts entirely (like <code>WITH</code> which creates a recursive parsing scenario). Our recursive descent parser handles these through <strong>context-aware parsing functions</strong> that examine the current parsing state to determine keyword interpretation.</p>\n<table>\n<thead>\n<tr>\n<th>Keyword Category</th>\n<th>Extension Complexity</th>\n<th>Example Keywords</th>\n<th>Integration Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statement Introducer</td>\n<td>Low</td>\n<td>WITH, MERGE, TRUNCATE</td>\n<td>Add new statement parser function</td>\n</tr>\n<tr>\n<td>Clause Modifier</td>\n<td>Medium</td>\n<td>DISTINCT, ALL, UNIQUE</td>\n<td>Extend existing clause parsers</td>\n</tr>\n<tr>\n<td>Expression Operator</td>\n<td>Medium</td>\n<td>LIKE, BETWEEN, IN</td>\n<td>Extend expression parser precedence table</td>\n</tr>\n<tr>\n<td>Context Sensitive</td>\n<td>High</td>\n<td>AS, JOIN, ON</td>\n<td>Update multiple parsing contexts</td>\n</tr>\n</tbody></table>\n<h4 id=\"operator-extension-strategy\">Operator Extension Strategy</h4>\n<p>Extending the parser with new operators requires careful integration with our <strong>precedence climbing expression parser</strong>. The <code>OPERATOR_PRECEDENCE</code> dictionary serves as the central configuration point for operator behavior, defining both precedence levels and associativity rules.</p>\n<p><strong>Binary operator extension follows this process</strong>:</p>\n<ol>\n<li>Add the operator token type to <code>TokenType</code> enum (e.g., <code>REGEXP_OPERATOR</code>)</li>\n<li>Update the tokenizer&#39;s <code>_parse_operator()</code> method to recognize the new operator symbol(s)</li>\n<li>Insert the operator into <code>OPERATOR_PRECEDENCE</code> with appropriate <code>OperatorInfo</code> containing precedence level and associativity</li>\n<li>Extend the <code>BinaryOperation</code> AST node to handle the new operator type in its semantic analysis</li>\n<li>Update expression parsing logic to construct appropriate AST nodes for the new operator</li>\n</ol>\n<p>The critical design decision involves <strong>precedence assignment</strong>. New operators must fit logically into our existing precedence hierarchy. For example, a hypothetical <code>REGEXP</code> operator would likely belong at <code>Precedence.COMPARISON</code> level (5) alongside other comparison operators, while a new arithmetic operator might fit at <code>Precedence.ADDITION</code> (6) or <code>Precedence.MULTIPLICATION</code> (7).</p>\n<p><strong>Unary operator extension</strong> requires additional considerations because unary operators appear in primary expression contexts. The <code>parse_primary_expression()</code> function must be extended to recognize new unary operator tokens and construct appropriate <code>UnaryOperation</code> AST nodes.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The precedence climbing algorithm&#39;s strength lies in its <strong>data-driven approach</strong> - new operators require configuration changes rather than algorithmic modifications. This separation of mechanism from policy ensures that operator extensions don&#39;t introduce subtle parsing bugs.</p>\n</blockquote>\n<h4 id=\"statement-type-extension-strategy\">Statement Type Extension Strategy</h4>\n<p>Adding new SQL statement types like <code>MERGE</code>, <code>TRUNCATE</code>, or <code>CREATE TABLE</code> leverages our <strong>statement-specific parser pattern</strong>. Each statement type gets its own dedicated parser class that inherits from <code>BaseParser</code> and implements the statement&#39;s unique grammar rules.</p>\n<p><strong>Statement extension architecture</strong>:</p>\n<ol>\n<li>Create a new statement parser class (e.g., <code>MergeParser</code>) extending <code>BaseParser</code></li>\n<li>Define the corresponding AST node type (e.g., <code>MergeStatement</code>) with appropriate fields for the statement&#39;s components</li>\n<li>Add the statement&#39;s introductory keyword to the tokenizer&#39;s keyword recognition</li>\n<li>Extend the main <code>SQLParser.parse()</code> method to dispatch to the new parser based on the leading keyword</li>\n<li>Implement the statement-specific parsing logic following our established recursive descent patterns</li>\n</ol>\n<p>The <code>SQLParser</code> class uses a <strong>dispatch table pattern</strong> to route different statement types to their appropriate parsers:</p>\n<table>\n<thead>\n<tr>\n<th>Statement Token</th>\n<th>Parser Class</th>\n<th>AST Node Type</th>\n<th>Complexity Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SELECT_KEYWORD</td>\n<td>SelectParser</td>\n<td>SelectStatement</td>\n<td>Reference Implementation</td>\n</tr>\n<tr>\n<td>INSERT_KEYWORD</td>\n<td>DMLParser</td>\n<td>InsertStatement</td>\n<td>Reference Implementation</td>\n</tr>\n<tr>\n<td>CREATE_KEYWORD</td>\n<td>DDLParser</td>\n<td>CreateStatement</td>\n<td>Future Extension</td>\n</tr>\n<tr>\n<td>WITH_KEYWORD</td>\n<td>CTEParser</td>\n<td>WithStatement</td>\n<td>Future Extension</td>\n</tr>\n</tbody></table>\n<p>Each statement parser maintains the same <strong>interface contract</strong>:</p>\n<ul>\n<li>Accept a token stream from the tokenizer</li>\n<li>Implement recursive descent parsing for the statement&#39;s grammar</li>\n<li>Construct and return an appropriate AST node representing the parsed statement</li>\n<li>Handle statement-specific error conditions and recovery scenarios</li>\n</ul>\n<h4 id=\"data-type-extension-strategy\">Data Type Extension Strategy</h4>\n<p>SQL&#39;s type system continues evolving with new data types like JSON, XML, arrays, and user-defined types. Our parser&#39;s literal parsing capabilities can extend to accommodate these new types through systematic tokenizer and parser enhancements.</p>\n<p><strong>Literal type extension process</strong>:</p>\n<ol>\n<li>Add new token types to represent the literals (e.g., <code>JSON_LITERAL</code>, <code>ARRAY_LITERAL</code>)</li>\n<li>Extend the tokenizer&#39;s character scanning logic to recognize the new literal formats</li>\n<li>Create corresponding AST node types (e.g., <code>JsonLiteral</code>, <code>ArrayLiteral</code>) with appropriate value representation</li>\n<li>Update the expression parser&#39;s <code>parse_primary_expression()</code> to construct the new literal nodes</li>\n<li>Consider type-specific validation rules that should be enforced during parsing</li>\n</ol>\n<p><strong>Complex literal types</strong> like arrays or JSON objects require more sophisticated tokenizer logic because they contain <strong>nested structure</strong>. The tokenizer must balance brackets, handle escape sequences, and potentially perform recursive parsing within the literal value. This complexity suggests that some advanced literals might require <strong>dedicated sub-parsers</strong> rather than simple character scanning.</p>\n<h3 id=\"advanced-sql-feature-roadmap\">Advanced SQL Feature Roadmap</h3>\n<p>Our parser architecture anticipates several major SQL feature categories that represent natural evolution paths from our current implementation. Each category presents unique architectural challenges and opportunities for demonstrating advanced parsing techniques.</p>\n<h4 id=\"join-operation-support\">JOIN Operation Support</h4>\n<p>JOIN operations represent the next logical extension beyond our current single-table <code>FROM</code> clause parsing. JOIN syntax introduces <strong>multiple parsing complexities</strong>: table relationship specifications, join condition expressions, and various join types (INNER, LEFT OUTER, RIGHT OUTER, FULL OUTER, CROSS).</p>\n<p><strong>JOIN parsing architectural requirements</strong>:</p>\n<ul>\n<li>Extend <code>TableReference</code> to support <strong>joined table expressions</strong> rather than single table identifiers</li>\n<li>Create new AST node types (<code>JoinExpression</code>, <code>JoinCondition</code>) to represent the relational algebra</li>\n<li>Modify the <code>FROM</code> clause parser to handle comma-separated table lists and explicit JOIN syntax</li>\n<li>Extend expression parsing to handle <strong>correlation names</strong> and <strong>qualified column references</strong> across multiple tables</li>\n<li>Implement <strong>semantic validation</strong> to ensure JOIN conditions reference valid table columns</li>\n</ul>\n<p>The parsing complexity arises from JOIN&#39;s <strong>left-associative grouping</strong> and <strong>precedence interactions</strong> with other clauses. Consider the statement: <code>SELECT * FROM table1 JOIN table2 ON condition1 JOIN table3 ON condition2</code>. The parser must construct a <strong>left-deep join tree</strong> where each JOIN operation builds upon the previous result.</p>\n<p><strong>JOIN precedence considerations</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Join Type</th>\n<th>Precedence Level</th>\n<th>Associativity</th>\n<th>Parsing Challenge</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CROSS JOIN</td>\n<td>Highest</td>\n<td>Left</td>\n<td>Simple table product</td>\n</tr>\n<tr>\n<td>INNER JOIN</td>\n<td>High</td>\n<td>Left</td>\n<td>Requires ON or USING clause</td>\n</tr>\n<tr>\n<td>OUTER JOIN</td>\n<td>High</td>\n<td>Left</td>\n<td>Complex condition validation</td>\n</tr>\n<tr>\n<td>NATURAL JOIN</td>\n<td>Medium</td>\n<td>Left</td>\n<td>Implicit column matching</td>\n</tr>\n</tbody></table>\n<h4 id=\"subquery-and-nested-expression-support\">Subquery and Nested Expression Support</h4>\n<p>Subqueries introduce <strong>recursive parsing scenarios</strong> where our parser must handle complete SELECT statements embedded within expression contexts. This feature tests our parser&#39;s architectural flexibility because subqueries can appear in SELECT lists, WHERE clauses, FROM clauses, and even within other subqueries.</p>\n<p><strong>Subquery parsing architectural challenges</strong>:</p>\n<ul>\n<li>Modify expression parsing to recognize parenthesized SELECT statements as <strong>scalar expressions</strong></li>\n<li>Extend <code>FROM</code> clause parsing to handle <strong>derived tables</strong> (subqueries in FROM clause)</li>\n<li>Implement <strong>scope management</strong> for column references that might resolve to outer query tables</li>\n<li>Handle <strong>correlated subqueries</strong> where inner queries reference outer query columns</li>\n<li>Manage <strong>recursion depth limits</strong> to prevent infinite parsing loops</li>\n</ul>\n<p>The critical architectural decision involves <strong>scope chain management</strong>. When parsing a subquery, column references might resolve to tables in the current query, parent queries, or even grandparent queries. Our parser needs a <strong>symbol table stack</strong> that tracks available column names at each nesting level.</p>\n<blockquote>\n<p><strong>Architecture Decision: Subquery Scope Management</strong></p>\n<ul>\n<li><strong>Context</strong>: Subqueries need access to column names from outer query scopes for correlation</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Global symbol table with query-specific namespaces</li>\n<li>Stack-based scope chain with parent query contexts</li>\n<li>Deferred resolution with post-parsing symbol analysis</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Stack-based scope chain with incremental symbol resolution</li>\n<li><strong>Rationale</strong>: Provides natural nesting semantics, enables early error detection, and integrates cleanly with recursive descent parsing</li>\n<li><strong>Consequences</strong>: Requires careful scope management during parsing but enables comprehensive semantic validation</li>\n</ul>\n</blockquote>\n<h4 id=\"window-function-integration\">Window Function Integration</h4>\n<p>Window functions like <code>ROW_NUMBER() OVER (PARTITION BY column ORDER BY column)</code> represent advanced expression types that combine function calls with specialized clause parsing. They demonstrate how our expression parser can extend to handle <strong>context-specific syntax</strong> within general expression contexts.</p>\n<p><strong>Window function parsing requirements</strong>:</p>\n<ul>\n<li>Extend function call parsing to recognize <code>OVER</code> clauses following function invocation</li>\n<li>Implement <strong>window specification parsing</strong> for PARTITION BY and ORDER BY clauses within window context</li>\n<li>Create specialized AST nodes (<code>WindowFunction</code>, <code>WindowSpecification</code>) to represent the complex syntax</li>\n<li>Handle <strong>window frame specifications</strong> (ROWS, RANGE, GROUPS) with their boundary expressions</li>\n<li>Integrate window functions into expression precedence hierarchy appropriately</li>\n</ul>\n<p>Window functions illustrate the <strong>composability principle</strong> in parser design - they&#39;re expressions that contain clause-like internal structure. Our expression parser must seamlessly transition between expression parsing and clause parsing modes when encountering window syntax.</p>\n<h4 id=\"common-table-expression-cte-support\">Common Table Expression (CTE) Support</h4>\n<p>WITH clauses for common table expressions introduce <strong>named subquery definitions</strong> that precede the main query. CTEs test our parser&#39;s ability to handle <strong>query-level preprocessing</strong> where named table expressions must be parsed, validated, and made available to the main query parser.</p>\n<p><strong>CTE parsing architectural implications</strong>:</p>\n<ul>\n<li>Extend the main parser to recognize WITH clauses before statement parsing</li>\n<li>Implement <strong>recursive CTE parsing</strong> where CTE definitions can reference previously defined CTEs</li>\n<li>Create symbol table management for <strong>temporary table names</strong> that exist only within query scope</li>\n<li>Handle <strong>recursive CTE syntax</strong> with UNION operations and termination conditions</li>\n<li>Validate <strong>CTE reference consistency</strong> ensuring all referenced CTEs are properly defined</li>\n</ul>\n<p>CTEs represent a <strong>preprocessing phase</strong> in SQL parsing where the parser must build a <strong>dependency graph</strong> of table definitions before parsing the main query. This architectural pattern extends naturally to other features like temporary tables or view definitions.</p>\n<h4 id=\"advanced-expression-features\">Advanced Expression Features</h4>\n<p>Several SQL expression extensions would demonstrate sophisticated parsing techniques: <strong>CASE expressions</strong> with their complex conditional logic, <strong>aggregate functions</strong> with DISTINCT and filtering clauses, <strong>type casting</strong> with explicit conversion syntax, and <strong>array operations</strong> with subscripting and slicing.</p>\n<p><strong>CASE expression parsing challenges</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>CASE Type</th>\n<th>Syntax Pattern</th>\n<th>Parsing Complexity</th>\n<th>AST Representation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple CASE</td>\n<td>CASE expr WHEN val THEN result</td>\n<td>Medium</td>\n<td>CaseExpression with value matching</td>\n</tr>\n<tr>\n<td>Searched CASE</td>\n<td>CASE WHEN condition THEN result</td>\n<td>High</td>\n<td>CaseExpression with boolean conditions</td>\n</tr>\n<tr>\n<td>Nested CASE</td>\n<td>CASE within WHEN/THEN clauses</td>\n<td>Very High</td>\n<td>Recursive CaseExpression nodes</td>\n</tr>\n</tbody></table>\n<p>Each advanced expression type requires <strong>specialized parsing logic</strong> while maintaining integration with our existing expression precedence system. The key architectural principle is ensuring that complex expressions <strong>compose naturally</strong> with simpler expressions in all syntactic contexts.</p>\n<h3 id=\"extension-implementation-patterns\">Extension Implementation Patterns</h3>\n<p>Successful parser extensions follow established <strong>architectural patterns</strong> that maintain system coherence while adding new capabilities. These patterns emerged from our design decisions around separation of concerns, recursive descent parsing, and AST-based representation.</p>\n<h4 id=\"plugin-architecture-pattern\">Plugin Architecture Pattern</h4>\n<p>For maximum extensibility, our parser can adopt a <strong>plugin architecture</strong> where new SQL features register themselves with the core parsing engine. This pattern enables <strong>feature modularity</strong> and <strong>selective compilation</strong> of SQL dialects.</p>\n<p><strong>Plugin registration mechanism</strong>:</p>\n<ul>\n<li>Define <code>ParserExtension</code> interface with methods for keyword registration, token type declaration, and parsing function contribution</li>\n<li>Implement <code>ExtensionRegistry</code> that manages feature registration and parser integration</li>\n<li>Create <strong>feature-specific modules</strong> (e.g., <code>json_extension.py</code>, <code>window_function_extension.py</code>) that implement the extension interface</li>\n<li>Enable <strong>runtime configuration</strong> where applications can enable/disable specific SQL feature sets</li>\n</ul>\n<p>This architecture supports <strong>SQL dialect customization</strong> where different applications might need different subsets of SQL functionality. A lightweight embedded application might enable only basic SELECT/INSERT operations, while a full database system might enable all extensions.</p>\n<h4 id=\"grammar-composition-pattern\">Grammar Composition Pattern</h4>\n<p>Complex SQL features often <strong>compose existing grammar rules</strong> in new combinations. Our parser architecture should support <strong>grammar rule reuse</strong> where new features can leverage existing parsing functions for their constituent parts.</p>\n<p><strong>Grammar composition examples</strong>:</p>\n<ul>\n<li>Window functions reuse ORDER BY parsing logic from the main query parser</li>\n<li>CTE definitions reuse complete SELECT statement parsing for their query definitions  </li>\n<li>MERGE statements combine INSERT, UPDATE, and DELETE parsing logic for their action clauses</li>\n<li>Complex literals (arrays, JSON) reuse expression parsing for their element values</li>\n</ul>\n<p>This pattern reduces code duplication and ensures consistent parsing behavior across SQL features that share syntactic elements.</p>\n<h4 id=\"semantic-validation-extension-pattern\">Semantic Validation Extension Pattern</h4>\n<p>As our parser grows more sophisticated, <strong>semantic validation</strong> becomes increasingly important. Extensions should provide <strong>pluggable validation rules</strong> that can analyze AST structures for logical consistency beyond pure syntax correctness.</p>\n<p><strong>Validation extension architecture</strong>:</p>\n<ul>\n<li>Define <code>ValidationRule</code> interface with methods for AST node analysis and error reporting</li>\n<li>Implement <strong>validation phases</strong> that traverse the complete AST after parsing completes</li>\n<li>Create <strong>context-sensitive validators</strong> that understand cross-references between AST nodes</li>\n<li>Enable <strong>progressive validation</strong> where basic syntax errors are caught during parsing, while complex semantic errors are caught during validation phases</li>\n</ul>\n<p>Examples of semantic validation rules include: ensuring INSERT column counts match value counts, validating that referenced table names exist in the FROM clause, checking that aggregate functions don&#39;t appear in WHERE clauses (except in subqueries), and verifying that window functions use valid column references in their PARTITION BY clauses.</p>\n<blockquote>\n<p><strong>Future Architecture Principle</strong>: Extensions should <strong>enhance the parser&#39;s capabilities without compromising its conceptual clarity</strong>. Each new feature should feel like a natural evolution of existing patterns rather than an architectural departure.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The following implementation guidance provides concrete patterns and starter code for extending our SQL parser architecture. These examples demonstrate how to add new language features while maintaining architectural consistency and code quality.</p>\n<h4 id=\"technology-recommendations-for-extensions\">Technology Recommendations for Extensions</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Type</th>\n<th>Simple Approach</th>\n<th>Advanced Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>New Keywords</td>\n<td>Direct enum/dictionary updates</td>\n<td>Plugin-based keyword registry</td>\n</tr>\n<tr>\n<td>New Operators</td>\n<td>Hardcoded precedence table entries</td>\n<td>Configurable precedence system</td>\n</tr>\n<tr>\n<td>New Statements</td>\n<td>Statement-specific parser classes</td>\n<td>Grammar rule composition framework</td>\n</tr>\n<tr>\n<td>Semantic Validation</td>\n<td>Post-parsing validation functions</td>\n<td>Visitor-based validation pipeline</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-extension-file-structure\">Recommended Extension File Structure</h4>\n<p>Extensions should follow a <strong>modular architecture</strong> that keeps new functionality separated from core parser logic while maintaining clear integration points:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>sql_parser/\n  core/\n    tokenizer.py              ← Core tokenization logic\n    base_parser.py            ← Base recursive descent parser\n    ast_nodes.py              ← Core AST node definitions\n  statements/\n    select_parser.py          ← SELECT statement parsing\n    dml_parser.py             ← INSERT/UPDATE/DELETE parsing\n    ddl_parser.py             ← Future: CREATE/DROP statements\n  expressions/\n    expression_parser.py      ← Core expression parsing\n    window_functions.py       ← Future: Window function extensions\n    case_expressions.py       ← Future: CASE expression parsing\n  extensions/\n    __init__.py\n    registry.py               ← Extension registration system\n    json_support.py           ← Future: JSON literal/function support\n    cte_support.py            ← Future: Common table expression support\n  validation/\n    semantic_validator.py     ← Post-parsing semantic validation\n    rules/\n      reference_validator.py  ← Column/table reference validation\n      type_validator.py       ← Type consistency validation</code></pre></div>\n\n<h4 id=\"extension-registry-infrastructure\">Extension Registry Infrastructure</h4>\n<p>This infrastructure provides the foundation for pluggable parser extensions. The registry manages feature registration and integration with the core parsing engine:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># extensions/registry.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Callable, Type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"keyword\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"operator\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STATEMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"statement\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXPRESSION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"expression\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VALIDATOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"validator\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionInfo</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    extension_type: ExtensionType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dependencies: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    registration_callback: Callable</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._extensions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ExtensionInfo] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._keyword_extensions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._operator_extensions: Dict[TokenType, OperatorInfo] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._statement_parsers: Dict[TokenType, Type[BaseParser]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._validation_rules: List[ValidationRule] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_extension</span><span style=\"color:#E1E4E8\">(self, extension_info: ExtensionInfo):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a new parser extension with dependency checking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate extension dependencies are satisfied</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for extension name conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Sort extensions by priority for ordered registration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call extension's registration callback to integrate with parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update internal extension registries based on extension type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_keyword_token_type</span><span style=\"color:#E1E4E8\">(self, keyword: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[TokenType]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look up extended keyword token types beyond core SQL keywords.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check core SQL_KEYWORDS first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Search registered keyword extensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return appropriate TokenType or None if not found</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_statement_parser</span><span style=\"color:#E1E4E8\">(self, token_type: TokenType) -> Optional[Type[BaseParser]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve parser class for extended statement types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look up token_type in statement parser registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return parser class or None if not registered</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_validation_rules</span><span style=\"color:#E1E4E8\">(self) -> List[ValidationRule]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return all registered validation rules in execution order.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return validation rules sorted by priority</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider rule dependencies and execution phases</span></span></code></pre></div>\n\n<h4 id=\"keyword-extension-pattern-implementation\">Keyword Extension Pattern Implementation</h4>\n<p>This pattern demonstrates how to add new SQL keywords while maintaining the tokenizer&#39;s case-insensitive recognition and the parser&#39;s grammar rule integration:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># extensions/json_support.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.ast_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ASTNode, Expression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.base_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> extensions.registry </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExtensionRegistry, ExtensionInfo, ExtensionType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># New token types for JSON support</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JsonTokenTypes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JSON_KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"JSON_KEYWORD\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JSON_EXTRACT_OPERATOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"JSON_EXTRACT_OPERATOR\"</span><span style=\"color:#6A737D\">  # -></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JSON_EXTRACT_TEXT_OPERATOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"JSON_EXTRACT_TEXT_OPERATOR\"</span><span style=\"color:#6A737D\">  # ->></span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># New AST nodes for JSON expressions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JsonLiteral</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Expression</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, json_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(source_location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.json_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json_text</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parsed_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Could parse JSON for validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"JsonLiteral\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JsonExtractExpression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Expression</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, json_expr: Expression, path_expr: Expression, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 extract_text: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(source_location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.json_expression </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json_expr</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.path_expression </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> path_expr</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.extract_as_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> extract_text  </span><span style=\"color:#6A737D\"># True for ->>, False for -></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"JsonExtractExpression\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> register_json_extension</span><span style=\"color:#E1E4E8\">(registry: ExtensionRegistry):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Register JSON parsing extensions with the parser.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Register new token types with tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add JSON literal recognition to tokenizer's literal parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Register JSON operators with appropriate precedence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extend expression parser to handle JSON extract operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add JSON function parsing (JSON_OBJECT, JSON_ARRAY, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Example operator precedence registration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json_operators </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        JsonTokenTypes.</span><span style=\"color:#79B8FF\">JSON_EXTRACT_OPERATOR</span><span style=\"color:#E1E4E8\">: OperatorInfo(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            precedence</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">Precedence.</span><span style=\"color:#79B8FF\">PRIMARY</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            is_right_associative</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        JsonTokenTypes.</span><span style=\"color:#79B8FF\">JSON_EXTRACT_TEXT_OPERATOR</span><span style=\"color:#E1E4E8\">: OperatorInfo(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            precedence</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">Precedence.</span><span style=\"color:#79B8FF\">PRIMARY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            is_right_associative</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> token_type, operator_info </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> json_operators.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        registry._operator_extensions[token_type] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operator_info</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Extension registration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">extension_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ExtensionInfo(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"json_support\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    extension_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">ExtensionType.</span><span style=\"color:#79B8FF\">EXPRESSION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    dependencies</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    registration_callback</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">register_json_extension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"statement-parser-extension-pattern\">Statement Parser Extension Pattern</h4>\n<p>This pattern shows how to add entirely new statement types while reusing existing parser infrastructure and following established recursive descent patterns:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># statements/merge_parser.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.base_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.ast_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ASTNode, Expression, Identifier, TableReference</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MergeAction</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for MERGE statement actions (INSERT/UPDATE/DELETE).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, condition: Optional[Expression], source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(source_location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.when_condition </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> condition</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MergeInsertAction</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MergeAction</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, condition: Optional[Expression], column_list: Optional[List[Identifier]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 value_list: List[Expression], source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(condition, source_location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column_list</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value_list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"MergeInsertAction\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MergeUpdateAction</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MergeAction</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, condition: Optional[Expression], assignments: List[AssignmentExpression],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(condition, source_location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.set_assignments </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> assignments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"MergeUpdateAction\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MergeStatement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, target_table: TableReference, source_table: TableReference,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 merge_condition: Expression, actions: List[MergeAction],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(source_location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.target_table </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> target_table</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source_table </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source_table  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.merge_condition </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> merge_condition</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.merge_actions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"MergeStatement\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MergeParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseParser</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Parser for MERGE statements following SQL standard syntax.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_merge_statement</span><span style=\"color:#E1E4E8\">(self) -> MergeStatement:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse: MERGE INTO target USING source ON condition </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">               WHEN [NOT] MATCHED [AND condition] THEN action [...]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume MERGE keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume INTO keyword token  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse target table reference using existing table parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume USING keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse source table reference (could be table or subquery)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume ON keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse merge condition expression using expression parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse one or more WHEN clauses using parse_when_clause()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create and return MergeStatement AST node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_when_clause</span><span style=\"color:#E1E4E8\">(self) -> MergeAction:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse individual WHEN [NOT] MATCHED [AND condition] THEN action clause.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume WHEN keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for optional NOT keyword (for WHEN NOT MATCHED)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume MATCHED keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse optional AND condition expression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume THEN keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Determine action type (INSERT/UPDATE/DELETE) from next token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Delegate to appropriate action parser method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return constructed MergeAction subclass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_merge_insert_action</span><span style=\"color:#E1E4E8\">(self, condition: Optional[Expression]) -> MergeInsertAction:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse INSERT action: INSERT [(columns)] VALUES (values).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume INSERT keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse optional column list in parentheses</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume VALUES keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse value list in parentheses using existing value parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create and return MergeInsertAction node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_merge_update_action</span><span style=\"color:#E1E4E8\">(self, condition: Optional[Expression]) -> MergeUpdateAction:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse UPDATE action: UPDATE SET assignments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume UPDATE keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume SET keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse assignment list using existing assignment parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create and return MergeUpdateAction node</span></span></code></pre></div>\n\n<h4 id=\"expression-parser-extension-pattern\">Expression Parser Extension Pattern</h4>\n<p>This pattern demonstrates how to extend expression parsing capabilities while maintaining proper operator precedence and AST construction:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># expressions/case_expressions.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.ast_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Expression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> expressions.expression_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExpressionParser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WhenClause</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents WHEN condition THEN result clause in CASE expression.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, condition: Expression, result: Expression, source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(source_location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.when_condition </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> condition</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.then_result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"WhenClause\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CaseExpression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Expression</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents both simple and searched CASE expressions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, case_expression: Optional[Expression], when_clauses: List[WhenClause],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 else_expression: Optional[Expression], source_location: SourceLocation):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(source_location)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.case_expression </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> case_expression  </span><span style=\"color:#6A737D\"># None for searched CASE</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.when_clauses </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> when_clauses</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.else_expression </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> else_expression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> node_type</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"CaseExpression\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_simple_case</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"True if this is simple CASE expr WHEN val, False if searched CASE WHEN condition.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.case_expression </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CaseExpressionParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Extension to ExpressionParser for handling CASE expressions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expression_parser: ExpressionParser):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expression_parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expression_parser</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_case_expression</span><span style=\"color:#E1E4E8\">(self) -> CaseExpression:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse complete CASE expression with optional ELSE clause.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.expression_parser.current_token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume CASE keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if next token starts an expression (simple CASE) or WHEN keyword (searched CASE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse optional case expression for simple CASE</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse one or more WHEN clauses using parse_when_clause()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse optional ELSE clause if ELSE keyword present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume END keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create and return CaseExpression AST node with source location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_when_clause</span><span style=\"color:#E1E4E8\">(self, is_simple_case: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">) -> WhenClause:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse WHEN condition/value THEN result clause.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume WHEN keyword token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse condition expression (for searched CASE) or value expression (for simple CASE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume THEN keyword token  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse result expression using expression parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create and return WhenClause AST node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> integrate_with_expression_parser</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register CASE expression parsing with main expression parser.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add CASE_KEYWORD to primary expression parsing in parse_primary_expression()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure CASE expressions compose properly with other expressions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle CASE expression precedence in expression hierarchy</span></span></code></pre></div>\n\n<h4 id=\"milestone-validation-for-extensions\">Milestone Validation for Extensions</h4>\n<p>Extensions should include validation checkpoints that verify correct integration with the core parser:</p>\n<p><strong>Extension Integration Checklist:</strong></p>\n<ul>\n<li>Tokenizer recognizes new keywords/operators without breaking existing functionality</li>\n<li>New AST nodes integrate properly with visitor pattern and tree traversal</li>\n<li>Parser handles new syntax without infinite loops or stack overflow</li>\n<li>Error messages for new syntax provide helpful guidance to users</li>\n<li>New features compose correctly with existing SQL constructs</li>\n</ul>\n<p><strong>Testing Commands for Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test tokenizer extension</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/extensions/test_json_tokenizer.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test parser extension integration  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/extensions/test_merge_parser.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test end-to-end functionality</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from sql_parser import SQLParser</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">parser = SQLParser()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">ast = parser.parse('MERGE INTO target USING source ON condition WHEN MATCHED THEN UPDATE SET col = val')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'AST root type: {ast.node_type()}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Extension Debugging Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Debugging Steps</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>New keywords tokenized as IDENTIFIER</td>\n<td>Missing keyword registration</td>\n<td>Check SQL_KEYWORDS dictionary</td>\n<td>Add keyword to tokenizer registry</td>\n</tr>\n<tr>\n<td>Parser fails on new syntax</td>\n<td>Grammar rules not implemented</td>\n<td>Trace recursive descent calls</td>\n<td>Implement missing parsing methods</td>\n</tr>\n<tr>\n<td>AST contains incorrect node types</td>\n<td>Wrong AST node construction</td>\n<td>Inspect parse tree structure</td>\n<td>Fix node creation in parser</td>\n</tr>\n<tr>\n<td>Extension breaks existing functionality</td>\n<td>Precedence/dispatch conflicts</td>\n<td>Run regression test suite</td>\n<td>Adjust extension integration points</td>\n</tr>\n</tbody></table>\n<p>These patterns provide a <strong>systematic approach</strong> to parser extension that maintains architectural consistency while enabling significant functionality growth. Each pattern demonstrates how to leverage existing parser infrastructure while introducing new capabilities that feel like natural language evolution rather than architectural departures.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section supports all milestones (1-4), providing essential terminology and concepts referenced throughout the design document.</p>\n</blockquote>\n<h3 id=\"mental-model-technical-dictionary\">Mental Model: Technical Dictionary</h3>\n<p>Think of this glossary as a technical dictionary specifically curated for SQL parsing concepts. Just as a foreign language dictionary helps you understand unfamiliar words while reading literature, this glossary helps you navigate the specialized terminology of compiler theory, parsing algorithms, and SQL language processing. Unlike a general programming dictionary, every term here is carefully selected because it appears in our parser design and has specific meaning within the context of SQL parsing.</p>\n<h3 id=\"core-parsing-and-compiler-theory-terms\">Core Parsing and Compiler Theory Terms</h3>\n<p>The foundation of any parser rests on fundamental concepts from compiler theory and formal language processing. These terms represent the theoretical underpinnings that guide our practical implementation decisions.</p>\n<p><strong>Abstract Syntax Tree (AST)</strong>: A hierarchical tree representation of the syntactic structure of SQL source code. Unlike a parse tree that includes every syntactic detail, an AST abstracts away punctuation and focuses on the semantic structure. Each node represents a language construct (statement, expression, literal) with child nodes representing sub-components. The AST serves as the primary output of our parser and the input to subsequent processing phases like query optimization or execution.</p>\n<p><strong>Tokenization</strong>: The process of breaking input text into a sequence of meaningful units called tokens. This is the first phase of parsing, where character sequences like &quot;SELECT&quot; become keyword tokens, &quot;customer_name&quot; becomes identifier tokens, and &quot;123&quot; becomes integer literal tokens. Tokenization handles whitespace removal, comment elimination, and initial classification of input elements.</p>\n<p><strong>Lexical Analysis</strong>: The formal term for the process of analyzing input characters to identify and classify tokens. This includes recognizing token boundaries, determining token types, and handling special cases like escape sequences in string literals. Lexical analysis is implemented by the tokenizer component and must handle SQL&#39;s specific rules for identifiers, keywords, and literal values.</p>\n<p><strong>Recursive Descent</strong>: A top-down parsing technique where each grammar rule is implemented as a function that calls other functions corresponding to sub-rules. The &quot;recursive&quot; aspect comes from grammar rules that reference themselves (directly or indirectly), while &quot;descent&quot; refers to starting from the top-level rule and working down to terminal symbols. This approach maps naturally to SQL&#39;s hierarchical grammar structure.</p>\n<p><strong>Precedence Climbing</strong>: An expression parsing algorithm that handles operator precedence by recursively parsing sub-expressions with minimum precedence thresholds. Instead of encoding precedence in the grammar structure, precedence climbing uses a table-driven approach where each recursive call specifies the minimum operator precedence it will consume. This technique elegantly handles SQL&#39;s complex operator precedence rules without requiring separate grammar rules for each precedence level.</p>\n<p><strong>Lookahead</strong>: The practice of examining future tokens without consuming them from the input stream. Single-token lookahead allows the parser to make parsing decisions based on the next token, while multi-token lookahead can resolve more complex ambiguities. SQL parsing often requires lookahead to distinguish between similar constructs like function calls and column references.</p>\n<h3 id=\"sql-specific-language-terms\">SQL-Specific Language Terms</h3>\n<p>SQL introduces domain-specific concepts that affect parsing decisions and AST structure. These terms reflect the unique characteristics of SQL as a declarative query language.</p>\n<p><strong>Three-Valued Logic</strong>: SQL&#39;s boolean logic system that includes TRUE, FALSE, and NULL values. This affects expression parsing because SQL comparisons can produce NULL results, and boolean operators like AND and OR have specific behavior when encountering NULL operands. The parser must represent these semantics accurately in expression AST nodes.</p>\n<p><strong>Qualified Identifier</strong>: A multi-part identifier using dot notation, such as <code>table.column</code> or <code>schema.table.column</code>. Parsing qualified identifiers requires recognizing dot separators and building AST nodes that preserve the hierarchical relationship between qualifiers and base names. SQL&#39;s scoping rules determine how qualified identifiers are resolved.</p>\n<p><strong>Star Wildcard</strong>: The asterisk symbol (*) used in SELECT clauses to represent all columns from specified tables. Parsing star wildcards requires distinguishing them from multiplication operators based on context. The AST representation must capture whether the star applies to all tables or is qualified to specific tables.</p>\n<p><strong>Implicit Alias</strong>: A column or table alias specified without the AS keyword, such as <code>SELECT name customer_name FROM users u</code>. Parsing implicit aliases requires recognizing identifier sequences where the second identifier serves as an alias. This creates parsing ambiguity that must be resolved through grammar rules and lookahead.</p>\n<p><strong>Explicit Alias</strong>: An alias specified using the AS keyword, such as <code>SELECT name AS customer_name FROM users AS u</code>. Explicit aliases are easier to parse because the AS keyword clearly signals the aliasing intent, but the parser must handle cases where AS is optional.</p>\n<h3 id=\"parser-architecture-and-design-terms\">Parser Architecture and Design Terms</h3>\n<p>These terms describe the structural organization and design patterns used in parser implementation. Understanding these concepts helps navigate the relationship between different parser components.</p>\n<p><strong>Visitor Pattern</strong>: A design pattern that separates algorithms from the AST structure they operate on. The pattern defines a visitor interface with methods for each AST node type, allowing external code to traverse and process AST nodes without modifying the node classes. This pattern enables extensibility for operations like code generation, optimization, and validation.</p>\n<p><strong>Grammar Rule</strong>: A formal specification of valid syntax patterns in SQL. Grammar rules define how tokens can be combined to form valid statements, expressions, and clauses. Each rule in our recursive descent parser corresponds to a parsing function that recognizes the rule&#39;s pattern and constructs appropriate AST nodes.</p>\n<p><strong>Token Consumption</strong>: The process of advancing the parser&#39;s position after successfully recognizing an expected token. Token consumption moves the parser forward through the input stream and typically involves updating position counters and fetching the next token. Proper token consumption is critical for maintaining parser state consistency.</p>\n<p><strong>Precedence Override</strong>: Using parentheses to change the natural evaluation order of operators in expressions. When the parser encounters parentheses, it must recursively parse the enclosed expression as a complete sub-expression before continuing with outer-level parsing. This mechanism allows users to override default operator precedence.</p>\n<p><strong>Maximal Munch</strong>: A tokenization strategy that always forms the longest possible token from the current input position. For example, when encountering &quot;&lt;=&quot;, maximal munch produces a single less-than-or-equal token rather than separate less-than and equals tokens. This strategy resolves tokenization ambiguities in favor of longer tokens.</p>\n<h3 id=\"error-handling-and-recovery-terms\">Error Handling and Recovery Terms</h3>\n<p>Parser error handling introduces specialized terminology for managing and recovering from syntax errors. These concepts enable robust parsing that continues after encountering problems.</p>\n<p><strong>Panic-Mode Recovery</strong>: An error recovery strategy where the parser discards tokens until it finds a reliable synchronization point where parsing can safely resume. Panic mode helps prevent cascading errors by establishing known-good parser states after encountering syntax problems.</p>\n<p><strong>Synchronization Points</strong>: Specific token positions where the parser can reliably resume parsing after error recovery. Common synchronization points include statement boundaries (semicolons) and major keyword tokens (SELECT, INSERT, UPDATE, DELETE). These points represent positions where the parser&#39;s context is well-defined.</p>\n<p><strong>Cascade Errors</strong>: Spurious error reports caused by earlier parsing failures. When the parser fails to recognize a construct correctly, subsequent parsing attempts may generate false positive errors. Error recovery mechanisms must distinguish between genuine syntax errors and cascade effects.</p>\n<p><strong>Error Suppression</strong>: The practice of preventing false positive error reports during error recovery phases. Suppression mechanisms track error recovery state and avoid reporting errors that are likely cascade effects from earlier problems.</p>\n<h3 id=\"testing-and-validation-terms\">Testing and Validation Terms</h3>\n<p>Parser testing requires specialized approaches that verify both positive cases (valid SQL) and negative cases (syntax errors). These terms describe testing strategies specific to parser development.</p>\n<p><strong>Component Unit Testing</strong>: Testing individual parser components in isolation from the rest of the system. Unit tests for tokenizers verify token type classification and boundary detection, while unit tests for parsers verify AST construction for specific grammar constructs. Component isolation enables focused testing of specific functionality.</p>\n<p><strong>End-to-End Parser Testing</strong>: Testing complete SQL statement parsing from input text to final AST output. End-to-end tests verify that all parser components work together correctly and that the final AST accurately represents the input SQL&#39;s semantic structure. These tests catch integration issues between parser phases.</p>\n<p><strong>Milestone Validation Checkpoints</strong>: Specific tests and expected outputs that verify each development milestone meets its acceptance criteria. Checkpoints provide objective measurements of progress and help identify when milestone requirements are fully satisfied. Each checkpoint includes both positive and negative test cases.</p>\n<p><strong>Edge Case Coverage</strong>: Testing boundary conditions and unusual syntax that might expose parser bugs. Edge cases include empty input, maximum-length identifiers, deeply nested expressions, and unusual but valid SQL constructs. Comprehensive edge case testing improves parser robustness.</p>\n<p><strong>Test Fixture</strong>: External test data files containing SQL examples and expected parsing results. Fixtures separate test data from test logic, making it easier to add new test cases and maintain large test suites. Fixture formats include raw SQL files and JSON files describing expected AST structures.</p>\n<h3 id=\"advanced-parsing-and-extension-terms\">Advanced Parsing and Extension Terms</h3>\n<p>These terms relate to parser extensibility and advanced parsing techniques that support future enhancement and customization.</p>\n<p><strong>Grammar Extension Patterns</strong>: Systematic approaches for adding new SQL features to the parser without breaking existing functionality. Extension patterns include keyword registration, operator precedence modification, and new statement type integration. Well-designed extension patterns maintain parser modularity and backward compatibility.</p>\n<p><strong>Plugin Architecture</strong>: A modular system that allows external code to extend parser functionality through well-defined interfaces. Plugin architectures enable adding domain-specific SQL extensions, custom validation rules, and specialized AST transformations without modifying core parser code.</p>\n<p><strong>Extension Registry</strong>: A central system for managing parser extensions and their dependencies. The registry handles extension registration, dependency resolution, and coordinated initialization of multiple extensions. This centralization prevents conflicts between extensions and ensures proper initialization ordering.</p>\n<p><strong>Context-Sensitive Parsing</strong>: Parsing behavior that depends on the current parsing context or previously parsed elements. SQL includes context-sensitive constructs where the same token sequence can have different meanings depending on the surrounding context. Context-sensitive parsing requires maintaining parser state across parsing function calls.</p>\n<p><strong>Symbol Table Stack</strong>: A hierarchical data structure for tracking symbol definitions and scopes during parsing. While our basic parser doesn&#39;t implement full symbol tables, understanding this concept helps when extending the parser to handle variable scoping, table name resolution, and semantic validation.</p>\n<h3 id=\"data-structure-and-algorithm-terms\">Data Structure and Algorithm Terms</h3>\n<p>Parser implementation relies on fundamental data structures and algorithms. These terms describe the computational foundations underlying parser operations.</p>\n<p><strong>Binding Power</strong>: A numerical precedence value used in precedence climbing algorithms to determine operator parsing order. Higher binding power values indicate higher precedence. The precedence climbing algorithm uses binding power comparisons to decide whether to continue parsing at the current level or recurse to a higher precedence level.</p>\n<p><strong>Associativity</strong>: The rule determining how operators of equal precedence group together. Left-associative operators group from left to right (a - b - c becomes (a - b) - c), while right-associative operators group from right to left (a = b = c becomes a = (b = c)). SQL has specific associativity rules for each operator class.</p>\n<p><strong>Character-by-Character Scanning</strong>: A tokenization approach that examines input one character at a time and maintains state to determine token boundaries and types. This approach provides fine-grained control over tokenization logic and handles complex cases like escape sequences and multi-character operators.</p>\n<p><strong>Method Dispatch</strong>: The process of calling different parsing functions based on input characteristics. Dispatch can be table-driven (using lookup tables to map token types to parsing functions) or conditional (using if-else chains to select appropriate parsing logic). Efficient dispatch improves parser performance.</p>\n<p><strong>State Machine</strong>: A computational model that transitions between discrete states based on input events. Tokenizers often use state machines to track parsing context (normal text, inside string literal, inside comment) and determine appropriate character handling logic for each state.</p>\n<h3 id=\"ast-construction-and-manipulation-terms\">AST Construction and Manipulation Terms</h3>\n<p>Building and working with Abstract Syntax Trees requires understanding tree structures and traversal patterns. These terms describe AST-related concepts and operations.</p>\n<p><strong>AST Composition</strong>: The process of building complex tree structures from simpler node components. AST composition involves creating parent-child relationships, maintaining node properties, and ensuring tree consistency. Proper composition techniques create ASTs that accurately represent SQL semantic structure.</p>\n<p><strong>Tree Traversal</strong>: Systematic methods for visiting all nodes in an AST. Common traversal patterns include depth-first (visiting children before siblings) and breadth-first (visiting all nodes at one level before descending). Different traversal patterns suit different AST processing tasks.</p>\n<p><strong>Node Type Classification</strong>: Organizing AST nodes into categories based on their semantic roles. Classification schemes typically include statement nodes (SELECT, INSERT, UPDATE, DELETE), expression nodes (binary operations, literals, identifiers), and utility nodes (lists, aliases). Clear classification guides AST design decisions.</p>\n<p><strong>Source Location Tracking</strong>: Maintaining information about where each AST node originated in the source code. Source locations include line numbers, column positions, and character ranges that enable precise error reporting and debugging support. Location tracking requires coordination between tokenizer and parser phases.</p>\n<h3 id=\"performance-and-optimization-terms\">Performance and Optimization Terms</h3>\n<p>Parser performance characteristics affect usability and scalability. These terms describe performance-related concepts and optimization strategies.</p>\n<p><strong>Linear Time Complexity</strong>: A performance characteristic where parsing time increases proportionally to input size. Well-designed recursive descent parsers achieve linear time complexity by making single forward passes through the input without backtracking. Linear complexity ensures predictable performance on large SQL statements.</p>\n<p><strong>Memory Allocation Patterns</strong>: The way parsers allocate and manage memory for tokens and AST nodes. Efficient allocation patterns minimize garbage collection pressure and memory fragmentation. Techniques include object pooling, arena allocation, and careful lifetime management of temporary objects.</p>\n<p><strong>Parse Table Generation</strong>: Pre-computing parsing decisions in lookup tables rather than using runtime conditionals. Table-driven parsing can improve performance by eliminating repeated decision logic, but requires careful design to maintain code readability and extensibility.</p>\n<h3 id=\"sql-language-feature-terms\">SQL Language Feature Terms</h3>\n<p>Specific SQL language constructs introduce terminology that affects parsing design and implementation decisions.</p>\n<p><strong>Data Modification Language (DML)</strong>: SQL statements that change database state, including INSERT, UPDATE, and DELETE. DML parsing requires handling value lists, assignment expressions, and conditional clauses. DML statements have different structural patterns than query statements like SELECT.</p>\n<p><strong>Assignment Expression</strong>: A column = value expression used in UPDATE statement SET clauses. Assignment expressions create AST nodes that capture both the target column and the assigned value expression. The parser must handle multiple assignments in comma-separated lists.</p>\n<p><strong>Value Row</strong>: A parenthesized list of expressions in INSERT statement VALUES clauses. Value rows can contain literals, expressions, and NULL values. The parser must validate that value row structure matches the column list structure and handle multiple rows in single INSERT statements.</p>\n<p><strong>Structural Validation</strong>: Post-parsing checks that verify logical consistency of parsed statements. Structural validation includes verifying that column counts match value counts in INSERT statements, checking that UPDATE statements have SET clauses, and ensuring DELETE statements don&#39;t accidentally omit WHERE clauses.</p>\n<p><strong>Safety-Conscious Parsing</strong>: Parser design that flags potentially dangerous operations in the AST. Safety features might include warnings for DELETE statements without WHERE clauses or INSERT statements with mismatched column and value counts. Safety consciousness improves parser usability and prevents common mistakes.</p>\n<p>This comprehensive glossary provides the terminology foundation needed to understand and implement our SQL parser. Each term connects to specific implementation decisions documented in previous sections and supports the technical discussions throughout the design document.</p>\n","toc":[{"level":1,"text":"SQL Parser: Design Document","id":"sql-parser-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: Language Translation","id":"mental-model-language-translation"},{"level":3,"text":"SQL Parsing Challenges","id":"sql-parsing-challenges"},{"level":3,"text":"Existing Parser Approaches","id":"existing-parser-approaches"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Mental Model: Building a Language Translator","id":"mental-model-building-a-language-translator"},{"level":3,"text":"Functional Goals","id":"functional-goals"},{"level":3,"text":"Non-Functional Goals","id":"non-functional-goals"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Overview","id":"component-overview"},{"level":3,"text":"Data Flow Pipeline","id":"data-flow-pipeline"},{"level":3,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model and AST Design","id":"data-model-and-ast-design"},{"level":3,"text":"Mental Model: Language Structure Blueprint","id":"mental-model-language-structure-blueprint"},{"level":3,"text":"Token Type Definitions","id":"token-type-definitions"},{"level":3,"text":"AST Node Hierarchy","id":"ast-node-hierarchy"},{"level":3,"text":"Statement AST Nodes","id":"statement-ast-nodes"},{"level":3,"text":"Expression AST Nodes","id":"expression-ast-nodes"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Tokenizer Component Design","id":"tokenizer-component-design"},{"level":3,"text":"Mental Model: Reading Word by Word","id":"mental-model-reading-word-by-word"},{"level":3,"text":"Tokenization Algorithm","id":"tokenization-algorithm"},{"level":3,"text":"Keyword Recognition Strategy","id":"keyword-recognition-strategy"},{"level":3,"text":"String Literal Parsing","id":"string-literal-parsing"},{"level":3,"text":"Architecture Decision: State Machine vs Character-by-Character","id":"architecture-decision-state-machine-vs-character-by-character"},{"level":3,"text":"Common Tokenizer Pitfalls","id":"common-tokenizer-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"SELECT Parser Component Design","id":"select-parser-component-design"},{"level":3,"text":"Mental Model: Grammar Rules as Functions","id":"mental-model-grammar-rules-as-functions"},{"level":3,"text":"SELECT Statement Grammar Rules","id":"select-statement-grammar-rules"},{"level":3,"text":"Recursive Descent Algorithm","id":"recursive-descent-algorithm"},{"level":3,"text":"Column List Parsing","id":"column-list-parsing"},{"level":3,"text":"Table Reference and Alias Parsing","id":"table-reference-and-alias-parsing"},{"level":3,"text":"Architecture Decision: Look-ahead Strategy","id":"architecture-decision-look-ahead-strategy"},{"level":3,"text":"Common SELECT Parser Pitfalls","id":"common-select-parser-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"WHERE Clause Expression Parser Design","id":"where-clause-expression-parser-design"},{"level":3,"text":"Mental Model: Mathematical Expression Evaluation","id":"mental-model-mathematical-expression-evaluation"},{"level":3,"text":"SQL Operator Precedence Rules","id":"sql-operator-precedence-rules"},{"level":3,"text":"Precedence Climbing Parser Algorithm","id":"precedence-climbing-parser-algorithm"},{"level":3,"text":"Expression Type Handling","id":"expression-type-handling"},{"level":3,"text":"Parentheses and Precedence Override","id":"parentheses-and-precedence-override"},{"level":3,"text":"Architecture Decision: Precedence Climbing vs Shunting Yard","id":"architecture-decision-precedence-climbing-vs-shunting-yard"},{"level":3,"text":"Common Expression Parser Pitfalls","id":"common-expression-parser-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Modification Statement Parser Design","id":"data-modification-statement-parser-design"},{"level":3,"text":"Mental Model: Data Manipulation Commands","id":"mental-model-data-manipulation-commands"},{"level":3,"text":"INSERT Statement Parsing","id":"insert-statement-parsing"},{"level":3,"text":"UPDATE Statement Parsing","id":"update-statement-parsing"},{"level":3,"text":"DELETE Statement Parsing","id":"delete-statement-parsing"},{"level":3,"text":"Value List and Type Validation","id":"value-list-and-type-validation"},{"level":3,"text":"Architecture Decision: Statement-Specific vs Unified Parser","id":"architecture-decision-statement-specific-vs-unified-parser"},{"level":3,"text":"Common DML Parser Pitfalls","id":"common-dml-parser-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Error Handling and Recovery","id":"error-handling-and-recovery"},{"level":3,"text":"Mental Model: Medical Diagnosis and Treatment","id":"mental-model-medical-diagnosis-and-treatment"},{"level":3,"text":"Parser Error Categories","id":"parser-error-categories"},{"level":4,"text":"Tokenization Error Types","id":"tokenization-error-types"},{"level":4,"text":"Syntax Error Types","id":"syntax-error-types"},{"level":4,"text":"Structural Validation Errors","id":"structural-validation-errors"},{"level":3,"text":"Error Message Design","id":"error-message-design"},{"level":4,"text":"Error Message Components","id":"error-message-components"},{"level":4,"text":"Position Information and Context","id":"position-information-and-context"},{"level":4,"text":"Suggestion Generation Strategies","id":"suggestion-generation-strategies"},{"level":4,"text":"Example Error Message Formats","id":"example-error-message-formats"},{"level":3,"text":"Error Recovery Techniques","id":"error-recovery-techniques"},{"level":4,"text":"Recovery Strategy Overview","id":"recovery-strategy-overview"},{"level":4,"text":"Synchronization Points and Panic Mode","id":"synchronization-points-and-panic-mode"},{"level":4,"text":"Cascade Error Prevention","id":"cascade-error-prevention"},{"level":4,"text":"Recovery State Management","id":"recovery-state-management"},{"level":4,"text":"Multi-Error Reporting Strategy","id":"multi-error-reporting-strategy"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Complete Error Class Infrastructure","id":"complete-error-class-infrastructure"},{"level":4,"text":"Error Recovery Infrastructure","id":"error-recovery-infrastructure"},{"level":4,"text":"Core Parser Error Integration Skeleton","id":"core-parser-error-integration-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips for Error Handling","id":"debugging-tips-for-error-handling"},{"level":2,"text":"Testing Strategy and Validation","id":"testing-strategy-and-validation"},{"level":3,"text":"Mental Model: Quality Control Assembly Line","id":"mental-model-quality-control-assembly-line"},{"level":3,"text":"Component Unit Testing","id":"component-unit-testing"},{"level":4,"text":"Tokenizer Unit Testing","id":"tokenizer-unit-testing"},{"level":4,"text":"Parser Component Unit Testing","id":"parser-component-unit-testing"},{"level":4,"text":"AST Node Unit Testing","id":"ast-node-unit-testing"},{"level":3,"text":"End-to-End Parser Testing","id":"end-to-end-parser-testing"},{"level":4,"text":"Real-World Query Testing","id":"real-world-query-testing"},{"level":4,"text":"Error Propagation Testing","id":"error-propagation-testing"},{"level":4,"text":"Performance Characteristic Testing","id":"performance-characteristic-testing"},{"level":3,"text":"Milestone Validation Checkpoints","id":"milestone-validation-checkpoints"},{"level":4,"text":"Milestone 1: SQL Tokenizer Validation","id":"milestone-1-sql-tokenizer-validation"},{"level":4,"text":"Milestone 2: SELECT Parser Validation","id":"milestone-2-select-parser-validation"},{"level":4,"text":"Milestone 3: WHERE Clause Expression Parser Validation","id":"milestone-3-where-clause-expression-parser-validation"},{"level":4,"text":"Milestone 4: Data Modification Statement Validation","id":"milestone-4-data-modification-statement-validation"},{"level":3,"text":"Common Testing Pitfalls","id":"common-testing-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Testing Framework Selection","id":"testing-framework-selection"},{"level":4,"text":"Recommended Test File Structure","id":"recommended-test-file-structure"},{"level":4,"text":"Complete Test Utility Infrastructure","id":"complete-test-utility-infrastructure"},{"level":4,"text":"Core Logic Test Skeletons","id":"core-logic-test-skeletons"},{"level":4,"text":"Milestone Checkpoint Implementations","id":"milestone-checkpoint-implementations"},{"level":4,"text":"Language-Specific Testing Hints","id":"language-specific-testing-hints"},{"level":4,"text":"Debugging Test Failures","id":"debugging-test-failures"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: Detective Work","id":"mental-model-detective-work"},{"level":3,"text":"Tokenizer Debugging Techniques","id":"tokenizer-debugging-techniques"},{"level":3,"text":"Parser Logic Debugging","id":"parser-logic-debugging"},{"level":3,"text":"Common Bug Symptom Analysis","id":"common-bug-symptom-analysis"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Future Extensions and Extensibility","id":"future-extensions-and-extensibility"},{"level":3,"text":"Mental Model: Language Evolution","id":"mental-model-language-evolution"},{"level":3,"text":"Grammar Extension Patterns","id":"grammar-extension-patterns"},{"level":4,"text":"Keyword Extension Strategy","id":"keyword-extension-strategy"},{"level":4,"text":"Operator Extension Strategy","id":"operator-extension-strategy"},{"level":4,"text":"Statement Type Extension Strategy","id":"statement-type-extension-strategy"},{"level":4,"text":"Data Type Extension Strategy","id":"data-type-extension-strategy"},{"level":3,"text":"Advanced SQL Feature Roadmap","id":"advanced-sql-feature-roadmap"},{"level":4,"text":"JOIN Operation Support","id":"join-operation-support"},{"level":4,"text":"Subquery and Nested Expression Support","id":"subquery-and-nested-expression-support"},{"level":4,"text":"Window Function Integration","id":"window-function-integration"},{"level":4,"text":"Common Table Expression (CTE) Support","id":"common-table-expression-cte-support"},{"level":4,"text":"Advanced Expression Features","id":"advanced-expression-features"},{"level":3,"text":"Extension Implementation Patterns","id":"extension-implementation-patterns"},{"level":4,"text":"Plugin Architecture Pattern","id":"plugin-architecture-pattern"},{"level":4,"text":"Grammar Composition Pattern","id":"grammar-composition-pattern"},{"level":4,"text":"Semantic Validation Extension Pattern","id":"semantic-validation-extension-pattern"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations for Extensions","id":"technology-recommendations-for-extensions"},{"level":4,"text":"Recommended Extension File Structure","id":"recommended-extension-file-structure"},{"level":4,"text":"Extension Registry Infrastructure","id":"extension-registry-infrastructure"},{"level":4,"text":"Keyword Extension Pattern Implementation","id":"keyword-extension-pattern-implementation"},{"level":4,"text":"Statement Parser Extension Pattern","id":"statement-parser-extension-pattern"},{"level":4,"text":"Expression Parser Extension Pattern","id":"expression-parser-extension-pattern"},{"level":4,"text":"Milestone Validation for Extensions","id":"milestone-validation-for-extensions"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mental Model: Technical Dictionary","id":"mental-model-technical-dictionary"},{"level":3,"text":"Core Parsing and Compiler Theory Terms","id":"core-parsing-and-compiler-theory-terms"},{"level":3,"text":"SQL-Specific Language Terms","id":"sql-specific-language-terms"},{"level":3,"text":"Parser Architecture and Design Terms","id":"parser-architecture-and-design-terms"},{"level":3,"text":"Error Handling and Recovery Terms","id":"error-handling-and-recovery-terms"},{"level":3,"text":"Testing and Validation Terms","id":"testing-and-validation-terms"},{"level":3,"text":"Advanced Parsing and Extension Terms","id":"advanced-parsing-and-extension-terms"},{"level":3,"text":"Data Structure and Algorithm Terms","id":"data-structure-and-algorithm-terms"},{"level":3,"text":"AST Construction and Manipulation Terms","id":"ast-construction-and-manipulation-terms"},{"level":3,"text":"Performance and Optimization Terms","id":"performance-and-optimization-terms"},{"level":3,"text":"SQL Language Feature Terms","id":"sql-language-feature-terms"}],"title":"SQL Parser: Design Document","markdown":"# SQL Parser: Design Document\n\n\n## Overview\n\nA SQL parser that transforms SQL query strings into Abstract Syntax Trees (ASTs) for SELECT, INSERT, UPDATE, and DELETE statements. The key architectural challenge is building a robust tokenizer and recursive descent parser that handles SQL's complex grammar rules, operator precedence, and various expression types while maintaining extensibility for future SQL features.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** This section provides foundational understanding for all milestones (1-4), establishing the conceptual framework and challenges that guide our parser design decisions.\n\n### Mental Model: Language Translation\n\nUnderstanding SQL parsing requires us to think about how humans process and understand language. When you read a sentence in a foreign language that you're learning, your brain goes through several distinct phases. First, you break the sentence into individual words and punctuation—this is **tokenization**. Then, you identify what type each word is: noun, verb, adjective, preposition. Finally, you apply grammar rules to understand how these words relate to each other and what the overall sentence means.\n\nSQL parsing works exactly the same way. Consider the SQL statement `SELECT name, age FROM users WHERE age > 18`. A human reading this intuitively understands that \"SELECT\" indicates we're retrieving data, \"name\" and \"age\" are the specific pieces of information we want, \"users\" is the source of that information, and \"age > 18\" is a condition that filters the results. But a computer program starts with just a string of characters and must systematically work through the same process your brain does automatically.\n\nThe **tokenizer** acts like someone learning to read who carefully identifies each word and punctuation mark. It scans through \"SELECT name, age FROM users WHERE age > 18\" character by character and produces a sequence of **tokens**: [SELECT_KEYWORD, IDENTIFIER(\"name\"), COMMA, IDENTIFIER(\"age\"), FROM_KEYWORD, IDENTIFIER(\"users\"), WHERE_KEYWORD, IDENTIFIER(\"age\"), GREATER_THAN, NUMBER(18)]. Each token includes not just the text but also its **type**—just like identifying parts of speech in human language.\n\nThe **parser** then acts like a grammar student applying sentence structure rules. It takes the token sequence and builds an **Abstract Syntax Tree (AST)**—a hierarchical representation that captures the logical structure and relationships in the query. The AST for our example would have a SELECT node at the root, with child nodes representing the column list (name, age), the table reference (users), and the WHERE condition (age > 18). This tree structure makes the query's meaning explicit and computable.\n\nJust as human language translation can be tricky due to ambiguous phrases, multiple meanings, and context-dependent interpretation, SQL parsing faces similar challenges that make it far more complex than it initially appears.\n\n### SQL Parsing Challenges\n\nSQL presents a unique combination of parsing challenges that make it significantly more difficult than many other programming languages. Unlike languages with rigid, unambiguous syntax, SQL was designed for readability and flexibility, which creates numerous complications for parser implementers.\n\n**Keyword Context Sensitivity** represents one of the most pervasive challenges in SQL parsing. Many SQL \"keywords\" can also function as valid identifiers depending on their position in the statement. For example, `ORDER` is a reserved keyword in `ORDER BY`, but it's also a perfectly valid table name in `SELECT * FROM ORDER`. The parser cannot determine whether `ORDER` should be treated as a keyword or identifier until it examines the surrounding context. This forces the parser to implement **lookahead** mechanisms or **backtracking** strategies, significantly complicating the parsing logic.\n\nConsider the statement `SELECT order FROM order ORDER BY order`. Here, the first `order` is a column name, `order` in the FROM clause is a table name, and the final `order` is the ORDER BY keyword—three different roles for the same text. The parser must track its current position in the grammar to make these distinctions correctly.\n\n**Case Insensitivity with Mixed Conventions** adds another layer of complexity. SQL keywords are case-insensitive (`SELECT`, `select`, and `Select` are equivalent), but the handling of identifiers varies between database systems. Some systems preserve identifier case, others fold to uppercase or lowercase, and some only fold unquoted identifiers while preserving quoted ones. This means the tokenizer must track whether identifiers are quoted and apply different normalization rules accordingly.\n\n**Quote Character Variations** create tokenization ambiguity. SQL supports multiple quote types: single quotes for string literals (`'hello'`), double quotes for identifiers (`\"table name\"`), and backticks for identifiers in MySQL (`` `table name` ``). Some systems allow these to be used interchangeably, while others have strict rules. The tokenizer must distinguish between `\"table\"` as a quoted identifier and `'table'` as a string literal, which requires different parsing logic and token types.\n\n**Operator Precedence and Associativity** in WHERE clauses mirror the complexity found in mathematical expressions but with SQL-specific twists. The expression `a = b AND c = d OR e = f` must be parsed as `((a = b) AND (c = d)) OR (e = f)` due to operator precedence rules (comparison operators bind tighter than AND, which binds tighter than OR). However, SQL also includes operators like `BETWEEN`, `IN`, `LIKE`, and `IS NULL` that don't exist in typical mathematical expressions and have their own precedence relationships.\n\n**Ambiguous Grammar Productions** occur when the same input can be parsed in multiple valid ways according to the grammar rules. The classic example is the **dangling ELSE problem** in programming languages, but SQL has its own version with expressions like `SELECT a, b FROM c`. Without additional tokens, it's unclear whether this is a complete statement or if a WHERE clause might follow. The parser must implement strategies to resolve these ambiguities consistently.\n\n**Expression vs Statement Boundary Confusion** happens because SQL allows complex expressions within statements, but the boundary between where expressions end and the next clause begins can be ambiguous. Consider `SELECT a + b * c FROM table WHERE x`. The parser must correctly identify that `a + b * c` is a single expression in the SELECT clause, and `x` is the beginning of a WHERE clause expression, not a continuation of the SELECT expression.\n\n**Whitespace and Comment Handling** requires careful consideration because SQL allows comments in multiple forms (`-- single line` and `/* multi-line */`) and has flexible whitespace rules. Comments can appear almost anywhere in a statement, including within expressions, and the parser must handle them without breaking the logical flow of the query. Additionally, some SQL dialects have specific rules about line continuation and whitespace significance.\n\n**Error Recovery Complexity** makes SQL parsing particularly challenging for practical tools. When a human writes malformed SQL, they expect helpful error messages that point to the specific problem and suggest corrections. However, SQL's flexible syntax means that by the time the parser detects an error, it may be far from the actual mistake. For example, a missing comma in a column list might not be detected until the parser tries to parse the FROM clause and encounters an unexpected keyword.\n\n### Existing Parser Approaches\n\nThe SQL parsing landscape has evolved over decades, with different approaches emerging to address the challenges outlined above. Understanding these existing approaches helps us make informed decisions about our parser architecture and avoid common pitfalls.\n\n> **Decision: Hand-Written vs Parser Generator**\n> - **Context**: SQL parsers can be implemented using traditional parser generators (ANTLR, Yacc, Bison) that generate parsing code from grammar files, or hand-written using techniques like recursive descent. This fundamental choice affects every aspect of the implementation.\n> - **Options Considered**: \n>   1. Parser Generator (ANTLR/Yacc) approach\n>   2. Hand-written Recursive Descent approach\n>   3. Hybrid approach with generated tokenizer and hand-written parser\n> - **Decision**: Hand-written Recursive Descent parser\n> - **Rationale**: Better error messages, easier debugging, more control over AST construction, and simpler integration with custom logic for SQL's context-sensitive features\n> - **Consequences**: More initial implementation work but greater flexibility and maintainability for educational purposes\n\n| Approach | Pros | Cons |\n|----------|------|------|\n| Parser Generator (ANTLR) | Fast development, formally verified grammar, handles complex precedence automatically | Hard to customize error messages, difficult to debug generated code, complex AST integration |\n| Hand-Written Recursive Descent | Complete control over parsing logic, excellent error messages, easy to debug and extend | More initial code to write, manual precedence handling, potential for grammar mistakes |\n| Hybrid (Generated + Hand-Written) | Combines benefits of both approaches | Complex integration, requires expertise in both techniques |\n\n**Parser Generator Approaches** use tools like ANTLR, Yacc, or Bison to automatically generate parsing code from formal grammar specifications. Major database systems like PostgreSQL use variations of this approach. The PostgreSQL parser uses a Yacc-based grammar with over 500 production rules that handle the full SQL standard plus PostgreSQL extensions. These tools excel at handling complex grammar rules automatically and can generate efficient parsing code.\n\nHowever, parser generators present significant challenges for SQL specifically. SQL's context-sensitive keywords require grammar hacks or post-processing steps that complicate the supposedly \"automatic\" generation. Error messages from generated parsers tend to be cryptic (\"expected IDENTIFIER but found ORDER\") rather than helpful (\"ORDER is a reserved keyword here; use quotes if you meant it as a table name\"). Additionally, integrating custom AST construction logic into generated parsers often requires learning the generator tool's specific templating language.\n\n**Hand-Written Recursive Descent** approaches implement parsing logic directly in the target programming language. Each grammar rule becomes a function that consumes tokens and builds AST nodes. SQLite uses this approach with a hand-written C parser that provides excellent error messages and tight integration with the query execution engine. The MySQL parser also uses hand-written techniques, allowing for fine-tuned error recovery and custom syntax extensions.\n\nThe recursive descent approach maps naturally to SQL's hierarchical structure. A `parseSelectStatement()` function calls `parseSelectList()`, `parseFromClause()`, and `parseWhereClause()` functions, each of which handles its specific grammar rules. This makes the parser logic transparent and debuggable—you can step through the parsing process in a debugger and see exactly which grammar rule is being applied at each step.\n\n**Precedence Climbing Parsers** represent a specialized technique for handling expression parsing within recursive descent parsers. Rather than encoding operator precedence into the grammar rules (which creates deep recursion and poor error messages), precedence climbing algorithms use a table-driven approach. The parser maintains a **precedence table** that assigns numeric priorities to operators, then uses a loop to build expression trees with correct precedence and associativity.\n\nThis technique is particularly valuable for SQL WHERE clauses, which can contain complex expressions with dozens of different operators. Rather than writing separate grammar rules for each precedence level (which would require functions like `parseOrExpression()`, `parseAndExpression()`, `parseComparisonExpression()`, etc.), a single precedence climbing function can handle the entire expression hierarchy.\n\n**Packrat Parsing** techniques use memoization to handle ambiguous grammars and provide unlimited lookahead. Some research SQL parsers use packrat parsing to handle SQL's most problematic ambiguities by trying multiple parse paths and selecting the successful one. While powerful, packrat parsing requires significantly more memory and implementation complexity than simpler approaches.\n\n**Multi-Pass Parsing** strategies separate different aspects of parsing into distinct phases. The first pass might handle only tokenization and basic statement structure, while subsequent passes refine the AST and resolve ambiguities with additional context. Some commercial SQL tools use this approach to provide features like syntax highlighting (first pass) and semantic validation (later passes) independently.\n\n> **Key Design Insight**: The choice of parsing approach significantly impacts not just implementation complexity, but also the quality of error messages, debugging experience, and extensibility for future SQL features. For educational purposes, the transparency and debuggability of hand-written recursive descent outweighs the initial implementation complexity.\n\n**Error Handling Strategies** vary dramatically between approaches. Production SQL parsers like those in PostgreSQL and MySQL implement sophisticated error recovery that attempts to continue parsing after encountering errors, allowing them to report multiple problems in a single statement. They maintain **error recovery points** in the grammar where parsing can restart after consuming tokens until a stable state is reached.\n\nEducational and development-focused parsers often use **panic mode recovery**, where parsing stops at the first error and reports it with maximum detail. This approach provides clearer error messages for learning purposes but doesn't help users who want to see all problems in their SQL at once.\n\n**Performance Considerations** distinguish production parsers from educational ones. Production systems must parse thousands of queries per second with minimal memory allocation. They often use techniques like **token pooling** (reusing token objects), **string interning** (avoiding duplicate identifier strings), and **AST node pooling** to minimize garbage collection pressure.\n\nFor our educational SQL parser, we prioritize clarity and debuggability over maximum performance, but understanding these optimization techniques helps explain why production parsers are structured the way they are.\n\nThe landscape of existing SQL parsers shows that there's no single \"correct\" approach—each technique involves trade-offs between development time, runtime performance, error message quality, and extensibility. Our design decisions must balance these factors while keeping our educational goals in mind.\n\n![SQL Parser System Components](./diagrams/system-components.svg)\n\n### Implementation Guidance\n\n**A. Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Tokenizer | Character-by-character scanning with string operations | State machine with lookup tables |\n| Parser | Direct recursive descent with manual precedence | Precedence climbing with operator tables |\n| AST Nodes | Simple classes with basic properties | Visitor pattern with type-safe node traversal |\n| Error Handling | Exception-based with position tracking | Error accumulation with recovery strategies |\n\n**B. Recommended File Structure:**\n\n```\nsql_parser/\n├── __init__.py              # Package initialization\n├── tokens.py                # Token type definitions and Token class\n├── tokenizer.py             # Lexical analysis (Milestone 1)\n├── ast_nodes.py             # AST node class hierarchy\n├── parser.py                # Main parser class and entry point\n├── select_parser.py         # SELECT statement parsing (Milestone 2)\n├── expression_parser.py     # WHERE clause expressions (Milestone 3)\n├── dml_parser.py           # INSERT/UPDATE/DELETE (Milestone 4)\n├── errors.py               # Parser exception classes\n└── tests/\n    ├── test_tokenizer.py    # Tokenizer tests\n    ├── test_select.py       # SELECT parsing tests\n    ├── test_expressions.py  # Expression parsing tests\n    ├── test_dml.py         # DML statement tests\n    └── test_integration.py  # End-to-end parser tests\n```\n\n**C. Infrastructure Starter Code:**\n\n```python\n# tokens.py - Complete token type definitions\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\n\nclass TokenType(Enum):\n    # Keywords\n    SELECT = auto()\n    FROM = auto()\n    WHERE = auto()\n    INSERT = auto()\n    UPDATE = auto()\n    DELETE = auto()\n    INTO = auto()\n    VALUES = auto()\n    SET = auto()\n    \n    # Identifiers and Literals\n    IDENTIFIER = auto()\n    STRING_LITERAL = auto()\n    NUMBER_LITERAL = auto()\n    \n    # Operators\n    EQUALS = auto()\n    NOT_EQUALS = auto()\n    LESS_THAN = auto()\n    GREATER_THAN = auto()\n    LESS_EQUAL = auto()\n    GREATER_EQUAL = auto()\n    \n    # Logical\n    AND = auto()\n    OR = auto()\n    NOT = auto()\n    \n    # Punctuation\n    COMMA = auto()\n    SEMICOLON = auto()\n    LEFT_PAREN = auto()\n    RIGHT_PAREN = auto()\n    ASTERISK = auto()\n    \n    # Special\n    EOF = auto()\n    UNKNOWN = auto()\n\n@dataclass\nclass Token:\n    type: TokenType\n    value: str\n    position: int\n    line: int = 1\n    column: int = 1\n\n# SQL keyword mapping for tokenizer\nSQL_KEYWORDS = {\n    'SELECT': TokenType.SELECT,\n    'FROM': TokenType.FROM,\n    'WHERE': TokenType.WHERE,\n    'INSERT': TokenType.INSERT,\n    'UPDATE': TokenType.UPDATE,\n    'DELETE': TokenType.DELETE,\n    'INTO': TokenType.INTO,\n    'VALUES': TokenType.VALUES,\n    'SET': TokenType.SET,\n    'AND': TokenType.AND,\n    'OR': TokenType.OR,\n    'NOT': TokenType.NOT,\n}\n```\n\n```python\n# errors.py - Complete error handling infrastructure\nclass ParseError(Exception):\n    \"\"\"Base class for all parsing errors\"\"\"\n    def __init__(self, message: str, position: int = 0, line: int = 1, column: int = 1):\n        self.message = message\n        self.position = position\n        self.line = line\n        self.column = column\n        super().__init__(f\"Line {line}, Column {column}: {message}\")\n\nclass TokenizerError(ParseError):\n    \"\"\"Errors during tokenization phase\"\"\"\n    pass\n\nclass SyntaxError(ParseError):\n    \"\"\"Errors during syntax parsing\"\"\"\n    pass\n\nclass UnexpectedTokenError(SyntaxError):\n    \"\"\"Specific error for unexpected tokens\"\"\"\n    def __init__(self, expected: str, actual: Token):\n        message = f\"Expected {expected}, but found {actual.type.name} '{actual.value}'\"\n        super().__init__(message, actual.position, actual.line, actual.column)\n        self.expected = expected\n        self.actual = actual\n```\n\n**D. Core Logic Skeleton Code:**\n\n```python\n# tokenizer.py - Tokenizer implementation skeleton\nfrom typing import List, Optional\nfrom .tokens import Token, TokenType, SQL_KEYWORDS\nfrom .errors import TokenizerError\n\nclass Tokenizer:\n    def __init__(self, sql_text: str):\n        self.text = sql_text\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n    \n    def tokenize(self) -> List[Token]:\n        \"\"\"\n        Main tokenization entry point. Converts SQL text into token list.\n        Returns complete list of tokens including EOF marker.\n        \"\"\"\n        # TODO: Implement main tokenization loop\n        # TODO: Call appropriate helper methods based on current character\n        # TODO: Handle whitespace and comments\n        # TODO: Add EOF token at end\n        # TODO: Return completed token list\n        pass\n    \n    def _current_char(self) -> Optional[str]:\n        \"\"\"Returns current character or None if at end\"\"\"\n        # TODO: Check if position is within text bounds\n        # TODO: Return character at current position or None\n        pass\n    \n    def _peek_char(self, offset: int = 1) -> Optional[str]:\n        \"\"\"Look ahead at character without advancing position\"\"\"\n        # TODO: Calculate peek position\n        # TODO: Return character if within bounds, None otherwise\n        pass\n    \n    def _advance(self) -> None:\n        \"\"\"Move to next character, updating position tracking\"\"\"\n        # TODO: Increment position\n        # TODO: Update line/column tracking for newlines\n        # TODO: Handle different line ending types (\\n, \\r\\n)\n        pass\n    \n    def _skip_whitespace(self) -> None:\n        \"\"\"Skip spaces, tabs, newlines\"\"\"\n        # TODO: Loop while current char is whitespace\n        # TODO: Call _advance() for each whitespace character\n        pass\n    \n    def _read_string_literal(self, quote_char: str) -> Token:\n        \"\"\"Read quoted string literal, handling escape sequences\"\"\"\n        # TODO: Store starting position for token\n        # TODO: Advance past opening quote\n        # TODO: Read characters until closing quote\n        # TODO: Handle escape sequences (\\', \\\", \\\\)\n        # TODO: Raise error if string is not terminated\n        # TODO: Return STRING_LITERAL token\n        pass\n```\n\n**E. Language-Specific Python Hints:**\n\n- Use `str.isalpha()`, `str.isdigit()`, and `str.isalnum()` for character classification during tokenization\n- Python's `enum.auto()` provides automatic enumeration values for token types\n- Use `@dataclass` for Token and AST node classes to get automatic `__init__`, `__repr__`, etc.\n- `typing.Optional[T]` indicates values that can be None (important for end-of-file conditions)\n- String slicing (`text[start:end]`) is efficient for extracting token values\n- Use `str.upper()` for case-insensitive keyword matching in `SQL_KEYWORDS` dictionary lookup\n- List comprehensions are useful for filtering tokens: `[t for t in tokens if t.type != TokenType.WHITESPACE]`\n\n**F. Initial Milestone Checkpoint:**\n\nAfter implementing the basic tokenizer structure:\n\n1. **Test Command**: `python -m pytest tests/test_tokenizer.py -v`\n2. **Expected Output**: Tests should pass for basic keyword recognition and identifier tokenization\n3. **Manual Verification**: Create a simple test script:\n   ```python\n   from sql_parser.tokenizer import Tokenizer\n   \n   tokenizer = Tokenizer(\"SELECT name FROM users\")\n   tokens = tokenizer.tokenize()\n   for token in tokens:\n       print(f\"{token.type.name}: '{token.value}'\")\n   ```\n   Expected output:\n   ```\n   SELECT: 'SELECT'\n   IDENTIFIER: 'name'\n   FROM: 'FROM'\n   IDENTIFIER: 'users'\n   EOF: ''\n   ```\n\n4. **Common Issues to Check**:\n   - If keywords appear as IDENTIFIER tokens, check case-insensitive lookup in `SQL_KEYWORDS`\n   - If position tracking is wrong, verify `_advance()` properly updates line/column\n   - If string literals fail, ensure quote character matching and escape sequence handling\n\n**G. Debugging Tips for Context and Problem Understanding:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Keywords recognized as identifiers | Case sensitivity in keyword lookup | Print token values and check SQL_KEYWORDS keys | Use `.upper()` before dictionary lookup |\n| Position tracking incorrect | Not updating line/column in _advance() | Add debug prints in _advance() method | Properly handle \\n and \\r\\n line endings |\n| String parsing fails | Quote character handling logic | Test with simple quoted strings first | Separate logic for single vs double quotes |\n| Tokenizer seems to hang | Infinite loop in character scanning | Add position debug prints in main loop | Ensure _advance() always moves position forward |\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** This section provides foundational understanding for all milestones (1-4), establishing the scope and boundaries that guide our parser implementation decisions throughout the project.\n\n### Mental Model: Building a Language Translator\n\nThink of our SQL parser as building a specialized language translator that only works with a specific dialect of SQL. Just as a human translator might specialize in translating business documents between English and Spanish (rather than translating poetry or slang), our parser will specialize in translating basic SQL statements into a structured format that programs can understand. We're not building Google Translate for SQL - we're building a focused, reliable translator for the most common SQL operations that a database application needs.\n\nThis translator has three key characteristics that define its scope. First, it has **vocabulary limitations** - it only understands certain SQL keywords and constructs, just like a business translator might not know specialized medical terminology. Second, it has **complexity boundaries** - it can handle straightforward sentences but not highly complex nested structures, similar to how a translator might handle business emails but struggle with legal documents. Third, it has **quality standards** - it must produce accurate, well-structured output for the cases it does handle, even if it can't handle every possible input.\n\nUnderstanding these boundaries upfront is crucial because they guide every design decision in our parser. When we encounter a choice between supporting more SQL features versus making our core features more robust, our goals will tell us which path to take. When we decide how much error recovery to implement, our non-goals will tell us where to stop. When we design our AST structure, our functional goals will tell us which nodes we need to include.\n\n### Functional Goals\n\nOur SQL parser must successfully handle the four fundamental categories of SQL statements that form the backbone of most database applications. These represent the core operations that developers use daily when building applications with database persistence.\n\n**SELECT Statement Parsing** forms the foundation of our query support. The parser must correctly handle basic SELECT statements with column lists, including both explicit column names and the star wildcard for selecting all columns. It must parse FROM clauses with table references, supporting both simple table names and table aliases using either the explicit AS keyword or implicit aliasing where the alias follows the table name directly. The parser must handle qualified column references using dot notation, such as `users.name` or `orders.total`, which are essential for queries involving multiple tables or for clarity in single-table queries.\n\nWithin SELECT statements, the parser must support comma-separated column lists of arbitrary length, maintaining the correct order and preserving any alias information. For example, parsing `SELECT id, name AS user_name, email FROM users AS u` should produce an AST that captures the three selected columns, the alias for the name column, and the alias for the users table. This level of detail in the AST enables downstream tools to understand not just what data is being retrieved, but how it should be labeled in result sets.\n\n**WHERE Clause Expression Parsing** represents one of the most complex functional requirements because it involves handling operator precedence, associativity, and multiple expression types. The parser must correctly parse comparison operators including equals, not equals, less than, greater than, less than or equal to, and greater than or equal to. It must handle logical operators AND, OR, and NOT with the correct precedence rules, where NOT binds most tightly, followed by AND, then OR. Parentheses must be supported to override default precedence, allowing developers to express complex logical conditions unambiguously.\n\nThe WHERE clause parser must also handle NULL checking operations, including IS NULL and IS NOT NULL, which have special semantics in SQL that differ from regular equality comparisons. String literals, numeric literals, and identifiers must all be supported as operands in expressions. For example, the parser should correctly handle complex conditions like `WHERE (age > 18 AND status = 'active') OR (type = 'premium' AND balance IS NOT NULL)`, building an expression tree that preserves the intended logical structure.\n\n**INSERT Statement Parsing** must handle the standard INSERT INTO syntax with explicit column lists and corresponding value lists. The parser should support inserting into a subset of table columns, with the column list and value list in the INSERT statement specifying which columns receive values. Multiple-row inserts using a single INSERT statement should be supported, where multiple value tuples are provided after the VALUES keyword. For example, `INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com'), ('Bob', 'bob@example.com')` should parse into an AST that captures both the column specification and the multiple value rows.\n\n**UPDATE and DELETE Statement Parsing** rounds out our data modification language support. UPDATE statements must parse SET clauses that specify column assignments using equals signs, with support for multiple column assignments separated by commas. Both UPDATE and DELETE statements must support WHERE clauses using the same expression parsing logic developed for SELECT statements. For DELETE statements, the parser must handle the simple `DELETE FROM table WHERE condition` syntax. The parser should build AST nodes that clearly distinguish between the target table, the modification operations (for UPDATE), and the filtering conditions.\n\n**Error Detection and Reporting** is a functional goal that spans all statement types. The parser must detect common syntax errors and report them with helpful error messages that include position information. When the parser encounters unexpected tokens, missing required keywords, or malformed expressions, it should provide error messages that help developers understand what went wrong and where. For example, if a user writes `SELECT name FROM WHERE age > 18` (missing the table name), the parser should report an error indicating that a table name is expected after the FROM keyword.\n\nThe following table summarizes the specific syntax elements that must be successfully parsed:\n\n| Statement Type | Required Syntax Elements | Examples |\n|---|---|---|\n| SELECT | Column lists, star wildcard, table references, aliases | `SELECT id, name FROM users`, `SELECT * FROM orders AS o` |\n| WHERE | Comparison operators, logical operators, parentheses, literals | `WHERE age > 18 AND status = 'active'` |\n| INSERT | Column lists, value lists, multiple rows | `INSERT INTO users (name) VALUES ('Alice'), ('Bob')` |\n| UPDATE | SET clauses, column assignments, WHERE conditions | `UPDATE users SET status = 'inactive' WHERE age < 13` |\n| DELETE | Table references, WHERE conditions | `DELETE FROM sessions WHERE expires < '2024-01-01'` |\n\n### Non-Functional Goals\n\n**Parse Speed and Memory Efficiency** are important non-functional requirements that affect the parser's usability in real applications. The parser should be able to handle typical SQL queries (those under 1000 characters) in under 10 milliseconds on modern hardware. While this isn't blazingly fast compared to production database parsers, it's sufficient for educational use, development tools, and small to medium applications. Memory usage should be proportional to query size, with the AST requiring roughly 10-50 times the memory of the original query string depending on the complexity of expressions and the number of nodes created.\n\nThe tokenizer should process characters in a single pass without backtracking, ensuring that tokenization time grows linearly with input size. The recursive descent parser should avoid excessive function call depth by limiting expression nesting to reasonable levels (around 50 levels of parentheses nesting). These constraints ensure that the parser remains responsive even when processing moderately complex queries.\n\n**Extensibility and Maintainability** form critical design requirements because this parser serves as an educational foundation that students and developers will modify and extend. The tokenizer design must make it straightforward to add new keywords by simply updating a keyword mapping table, without requiring changes to the core tokenization logic. Adding new operators should require only updates to operator precedence tables and the addition of corresponding token types.\n\nThe AST node hierarchy must be designed for extension, with a clear base class or interface that new node types can inherit from or implement. Adding support for new statement types should follow a consistent pattern where developers implement a new parsing function following the same recursive descent approach used for existing statements. The parser should be modular enough that developers can test individual components (tokenizer, expression parser, statement parsers) in isolation.\n\n**Educational Value and Debuggability** represent special non-functional requirements because this parser is designed for learning. The AST structure should be intuitive and easy to inspect, with node types and field names that clearly correspond to SQL concepts. When students print or debug the AST, they should be able to easily understand how their SQL query was interpreted by the parser.\n\nError messages should be educational, explaining not just what was wrong but providing hints about correct syntax. For example, instead of reporting \"unexpected token,\" the parser might say \"expected table name after FROM keyword, but found WHERE.\" Debug output should be available to show the tokenization process and parsing decisions, helping students understand how the parser works internally.\n\n**Testing and Validation Support** ensures that implementations can be verified for correctness. The parser design should facilitate comprehensive testing by making it easy to test individual components and verify specific aspects of the AST structure. The tokenizer output should be easily inspectable to verify that queries are being tokenized correctly. AST nodes should support equality comparison to enable straightforward assertion-based testing.\n\nThe following table outlines our non-functional requirements with specific metrics:\n\n| Requirement Category | Specific Goal | Success Metric |\n|---|---|---|\n| Performance | Parse speed for typical queries | Under 10ms for queries under 1000 characters |\n| Memory | Proportional memory usage | AST uses 10-50x the memory of query string |\n| Extensibility | Easy keyword addition | New keywords added by updating single mapping table |\n| Maintainability | Modular component testing | Each component (tokenizer, parser) testable in isolation |\n| Educational Value | Intuitive AST structure | AST node names directly correspond to SQL concepts |\n| Error Quality | Helpful error messages | Errors include position info and syntax hints |\n| Debugging | Inspection capabilities | AST and tokens easily printable for debugging |\n\n### Explicit Non-Goals\n\nUnderstanding what we are **not** building is just as important as understanding what we are building. These explicit non-goals help prevent scope creep and keep our implementation focused on the core learning objectives.\n\n**Advanced SQL Features** are explicitly excluded from our parser scope. We will not support JOIN operations of any kind, including INNER JOIN, LEFT JOIN, RIGHT JOIN, or FULL OUTER JOIN. While JOINs are crucial in production SQL, they add significant complexity to both parsing and AST representation that would distract from learning the fundamental concepts of lexical analysis and recursive descent parsing. Supporting JOINs properly requires handling table correlation, join conditions, and the interaction between WHERE clauses and JOIN conditions.\n\nSubqueries and nested SELECT statements are not supported. Subqueries introduce recursive parsing challenges where SELECT statements can appear within expressions, requiring the parser to handle statement nesting and scope resolution. Window functions, CTEs (Common Table Expressions), and advanced SQL constructs like CASE expressions are similarly excluded. These features, while powerful, represent advanced SQL concepts that would significantly complicate our parser without proportional educational benefit for the core parsing concepts we're teaching.\n\n**Production-Quality Features** are not included in our design goals. We are not building a parser that could be used in a production database system or SQL analysis tool. This means we explicitly exclude performance optimizations like parse result caching, incremental parsing, or advanced error recovery strategies that would allow parsing to continue after encountering errors.\n\nWe do not support SQL dialects or vendor-specific extensions. Our parser targets a simplified, generic SQL syntax that captures the essential elements of SELECT, INSERT, UPDATE, and DELETE statements. We will not handle the subtle syntax differences between MySQL, PostgreSQL, SQL Server, or Oracle. Features like MySQL's LIMIT clause, PostgreSQL's RETURNING clause, or SQL Server's TOP clause are not supported.\n\n**Advanced Error Handling** capabilities are intentionally limited. We will not implement sophisticated error recovery that attempts to continue parsing after encountering syntax errors to find additional problems in the same query. While production parsers often include such features to provide comprehensive error reporting, implementing robust error recovery is a complex topic that goes beyond our educational goals. Our parser will report the first error it encounters and stop parsing.\n\nWe do not support error correction or \"did you mean\" suggestions. While these features can be helpful in development tools, they require significant additional complexity including similarity algorithms and extensive knowledge of valid SQL constructs. Our error messages will be helpful and descriptive, but they will not attempt to guess what the user intended to write.\n\n**Schema Validation and Semantic Analysis** are completely outside our scope. Our parser builds a syntactic AST that represents the structure of the SQL query, but it does not validate that referenced tables exist, that column names are valid, or that data types are compatible. These semantic validation tasks require knowledge of database schema and type systems, which would require building a substantial metadata management system alongside our parser.\n\nWe will not validate that INSERT statements specify values for all required columns, that UPDATE statements don't attempt to modify primary keys, or that WHERE clause comparisons use compatible data types. These are all important validations in a production system, but they belong to the semantic analysis phase that occurs after parsing.\n\n**Performance Optimization** beyond basic efficiency is not a goal. We will not implement advanced parsing techniques like packrat parsing, parser combinators, or generated parsers from formal grammars. While these approaches have advantages in certain scenarios, they introduce complexity that would obscure the fundamental concepts of tokenization and recursive descent parsing that our project aims to teach.\n\nMemory optimization techniques like AST node pooling, lazy parsing, or compact AST representations are not included. Our AST will prioritize clarity and ease of understanding over memory efficiency. Similarly, we will not implement streaming parsing for very large queries or sophisticated tokenizer optimizations.\n\nThe following table summarizes our explicit non-goals and the rationale for excluding them:\n\n| Excluded Feature Category | Specific Examples | Exclusion Rationale |\n|---|---|---|\n| Advanced SQL | JOINs, subqueries, window functions, CTEs | Adds parsing complexity without educational benefit for core concepts |\n| Production Features | Parse caching, incremental parsing, dialect support | Beyond educational scope; would obscure fundamental techniques |\n| Error Recovery | Continue parsing after errors, error correction | Complex topic requiring extensive additional implementation |\n| Semantic Analysis | Schema validation, type checking, constraint validation | Requires metadata system; not a parsing concern |\n| Performance Optimization | Advanced parsing algorithms, memory optimization | Would obscure educational focus on basic recursive descent |\n| SQL Dialects | MySQL LIMIT, PostgreSQL RETURNING, vendor extensions | Scope limitation to keep grammar manageable |\n\n> **Design Insight**: These non-goals are not limitations of the approach, but conscious decisions to maintain educational focus. A student who masters tokenization, recursive descent parsing, operator precedence, and AST construction using our simplified SQL grammar will have learned transferable skills that apply to parsing any language, not just SQL.\n\n### Implementation Guidance\n\nThe goals and non-goals defined above have direct implications for how we structure our implementation and what technologies we choose. This section provides concrete guidance on translating these requirements into a working parser.\n\n**Technology Recommendations**\n\nFor implementing our SQL parser with the defined scope, we recommend straightforward approaches that prioritize clarity and educational value over performance optimization or advanced features.\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Tokenizer | Character-by-character scanning with string methods | Regex-based tokenization |\n| Parser | Hand-written recursive descent | Parser generator (ANTLR, PLY) |\n| AST Representation | Simple classes with inheritance | Visitor pattern with node interfaces |\n| Error Handling | Exception-based with position tracking | Error recovery with synchronization points |\n| Testing | Unit tests with direct assertions | Property-based testing with query generation |\n\nFor this educational project, we strongly recommend the simple options. Character-by-character scanning helps students understand how tokenizers work at a fundamental level. Hand-written recursive descent parsing makes the relationship between grammar rules and code explicit. Simple AST classes are easy to debug and understand.\n\n**Recommended Project Structure**\n\nOrganizing the parser implementation across multiple modules helps maintain clear separation of concerns and makes the codebase easier to understand and test.\n\n```\nsql-parser/\n├── sql_parser/\n│   ├── __init__.py              ← Main parser API\n│   ├── tokens.py                ← Token types and TokenType enum\n│   ├── tokenizer.py             ← Tokenizer class (Milestone 1)\n│   ├── ast_nodes.py             ← AST node class hierarchy\n│   ├── select_parser.py         ← SELECT statement parsing (Milestone 2)\n│   ├── expression_parser.py     ← WHERE clause expressions (Milestone 3)\n│   ├── dml_parser.py            ← INSERT/UPDATE/DELETE parsing (Milestone 4)\n│   └── exceptions.py            ← Parser exception classes\n├── tests/\n│   ├── test_tokenizer.py        ← Tokenizer tests\n│   ├── test_select_parser.py    ← SELECT parser tests\n│   ├── test_expressions.py      ← Expression parser tests\n│   ├── test_dml_parser.py       ← DML parser tests\n│   └── test_integration.py      ← End-to-end parser tests\n├── examples/\n│   ├── basic_usage.py           ← Simple parser usage examples\n│   └── query_samples/           ← Sample SQL files for testing\n└── README.md                    ← Project documentation\n```\n\nThis structure separates each major component into its own module, making it easy to work on individual milestones independently. The test structure mirrors the implementation structure, encouraging comprehensive testing of each component.\n\n**Exception Hierarchy Setup**\n\nBased on our error handling goals, implement a clear exception hierarchy that provides good error messages with position information.\n\n```python\n# exceptions.py - Complete exception hierarchy for the parser\n\nclass ParseError(Exception):\n    \"\"\"Base exception for all parser errors.\"\"\"\n    \n    def __init__(self, message, position=None, line=None, column=None):\n        super().__init__(message)\n        self.message = message\n        self.position = position\n        self.line = line\n        self.column = column\n    \n    def __str__(self):\n        if self.line is not None and self.column is not None:\n            return f\"Parse error at line {self.line}, column {self.column}: {self.message}\"\n        elif self.position is not None:\n            return f\"Parse error at position {self.position}: {self.message}\"\n        else:\n            return f\"Parse error: {self.message}\"\n\nclass TokenizerError(ParseError):\n    \"\"\"Exception raised during tokenization phase.\"\"\"\n    pass\n\nclass SyntaxError(ParseError):\n    \"\"\"Exception raised during parsing phase.\"\"\"\n    pass\n\nclass UnexpectedTokenError(SyntaxError):\n    \"\"\"Exception raised when parser encounters unexpected token.\"\"\"\n    \n    def __init__(self, expected, actual, position=None, line=None, column=None):\n        self.expected = expected\n        self.actual = actual\n        message = f\"Expected {expected}, but found {actual}\"\n        super().__init__(message, position, line, column)\n```\n\n**Main Parser API Design**\n\nCreate a simple, unified API that encapsulates the entire parsing process while maintaining access to intermediate steps for debugging and testing.\n\n```python\n# __init__.py - Main parser API that ties everything together\n\nfrom .tokenizer import Tokenizer\nfrom .select_parser import SelectParser\nfrom .dml_parser import DMLParser\nfrom .exceptions import ParseError\n\nclass SQLParser:\n    \"\"\"\n    Main SQL parser that coordinates tokenization and parsing phases.\n    \n    This class provides the primary interface for parsing SQL statements\n    and returning AST nodes that represent the query structure.\n    \"\"\"\n    \n    def __init__(self):\n        self.tokenizer = Tokenizer()\n        self.select_parser = SelectParser()\n        self.dml_parser = DMLParser()\n    \n    def parse(self, sql_text):\n        \"\"\"\n        Parse a SQL statement and return the corresponding AST node.\n        \n        Args:\n            sql_text (str): The SQL statement to parse\n            \n        Returns:\n            AST node representing the parsed statement\n            \n        Raises:\n            ParseError: If the SQL statement contains syntax errors\n        \"\"\"\n        # TODO: Tokenize the input SQL text using self.tokenizer.tokenize()\n        # TODO: Determine statement type from first token (SELECT, INSERT, UPDATE, DELETE)\n        # TODO: Delegate to appropriate parser based on statement type\n        # TODO: Return the resulting AST node\n        pass\n    \n    def tokenize_only(self, sql_text):\n        \"\"\"\n        Tokenize SQL text without parsing (useful for debugging).\n        \n        Returns:\n            List of Token objects\n        \"\"\"\n        return self.tokenizer.tokenize(sql_text)\n```\n\n**Milestone Validation Checkpoints**\n\nEach milestone should have clear checkpoints that verify the implementation is working correctly before moving to the next milestone.\n\n**Milestone 1 Checkpoint - Tokenizer:**\n```python\n# Test that tokenizer correctly handles basic SQL tokens\nparser = SQLParser()\ntokens = parser.tokenize_only(\"SELECT name FROM users WHERE id = 42\")\n\n# Expected: 8 tokens with correct types\n# TOKEN_SELECT, TOKEN_IDENTIFIER('name'), TOKEN_FROM, TOKEN_IDENTIFIER('users'),\n# TOKEN_WHERE, TOKEN_IDENTIFIER('id'), TOKEN_EQUALS, TOKEN_NUMBER(42)\n\nassert len(tokens) == 8\nassert tokens[0].type == TokenType.SELECT\nassert tokens[1].value == \"name\"\nassert tokens[7].value == \"42\"\n```\n\n**Milestone 2 Checkpoint - SELECT Parser:**\n```python\n# Test that SELECT statements parse into correct AST structure\nast = parser.parse(\"SELECT id, name FROM users AS u\")\n\n# Expected: SelectStatement node with columns list and table reference\nassert isinstance(ast, SelectStatement)\nassert len(ast.columns) == 2\nassert ast.columns[0].name == \"id\"\nassert ast.table.alias == \"u\"\n```\n\n**Milestone 3 Checkpoint - WHERE Expressions:**\n```python\n# Test complex WHERE clause with operator precedence\nast = parser.parse(\"SELECT * FROM users WHERE age > 18 AND status = 'active' OR type = 'admin'\")\n\n# Expected: Correct precedence with AND binding tighter than OR\nassert isinstance(ast.where_clause, BinaryOperation)\nassert ast.where_clause.operator == \"OR\"  # Top-level operator\n```\n\n**Milestone 4 Checkpoint - DML Statements:**\n```python\n# Test INSERT, UPDATE, and DELETE parsing\ninsert_ast = parser.parse(\"INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com')\")\nupdate_ast = parser.parse(\"UPDATE users SET status = 'inactive' WHERE age < 13\")\ndelete_ast = parser.parse(\"DELETE FROM sessions WHERE expires < '2024-01-01'\")\n\nassert isinstance(insert_ast, InsertStatement)\nassert isinstance(update_ast, UpdateStatement) \nassert isinstance(delete_ast, DeleteStatement)\n```\n\n**Common Implementation Pitfalls**\n\n⚠️ **Pitfall: Overly Complex Initial Design**\nMany students try to implement all features at once or design overly flexible AST nodes that can handle any possible SQL construct. Start simple and extend gradually. Implement the tokenizer completely for basic tokens before adding complex operators or string escape sequences.\n\n⚠️ **Pitfall: Ignoring Position Tracking**\nFailing to track line and column positions during tokenization makes debugging very difficult. Include position information in every token from the start - it's much harder to add later.\n\n⚠️ **Pitfall: Mixing Parsing Phases**\nDon't try to do semantic validation (checking if tables exist) during syntax parsing. Keep the parser focused on syntax and structure. Semantic analysis can be added as a separate phase later.\n\n⚠️ **Pitfall: Inadequate Error Messages**\nGeneric error messages like \"syntax error\" provide little help. Include what was expected, what was found, and position information in every error message.\n\n**Debugging Strategy**\n\nWhen implementing the parser, use a systematic debugging approach:\n\n1. **Debug tokenization first**: Always verify that your input is being tokenized correctly before debugging parser logic. Print token sequences to ensure they match expectations.\n\n2. **Test individual components**: Test the expression parser independently before integrating it with statement parsing. Test each statement type independently.\n\n3. **Use small test cases**: Start with the simplest possible examples (`SELECT * FROM users`) before attempting complex queries.\n\n4. **Print AST structures**: Implement `__str__` or `__repr__` methods on AST nodes to make debugging easier. You should be able to print the AST and understand the structure visually.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** This section establishes the foundational architecture for all milestones (1-4), defining the core components and their relationships that will be implemented throughout the project.\n\n### Component Overview\n\nThink of the SQL parser as a **language translation service** operating in a document processing office. Raw SQL text arrives like a foreign document that needs to be understood and translated into a structured format that computers can work with effectively. Just as a translation service has specialists for different stages—someone who identifies individual words and their types, someone who understands grammar rules and sentence structure, and someone who creates the final structured document—our SQL parser has three specialized components that work together in sequence.\n\nThe **Tokenizer** acts as the lexical analyst who examines the raw SQL text character by character and identifies distinct words, symbols, and punctuation marks. It's responsible for recognizing that `SELECT` is a keyword, `user_id` is an identifier, `'John'` is a string literal, and `=` is an operator. This component transforms the continuous stream of characters into a sequence of classified tokens, each tagged with its type and position information for error reporting.\n\n![SQL Parser System Components](./diagrams/system-components.svg)\n\nThe **Parser** functions as the grammar expert who understands SQL's syntactic rules and constructs a meaningful hierarchical representation of the query. It consumes the token sequence from the tokenizer and applies recursive descent parsing techniques to build an Abstract Syntax Tree that captures both the structure and semantics of the SQL statement. The parser knows that after a `SELECT` keyword, it should expect a column list, and after a `FROM` keyword, it should find table references.\n\nThe **AST Node** hierarchy represents the structured output—a tree-like data structure where each node encapsulates a specific SQL construct with its associated data and relationships. These nodes serve as the final parsed representation that can be easily traversed, analyzed, and transformed by other systems such as query optimizers or execution engines.\n\nThe three main components work with distinct responsibilities and interfaces:\n\n| Component | Primary Responsibility | Input Format | Output Format | Key Operations |\n|-----------|----------------------|--------------|---------------|----------------|\n| `Tokenizer` | Lexical analysis and token classification | Raw SQL string | Sequence of `Token` objects | Character scanning, keyword recognition, literal parsing |\n| `SQLParser` | Syntactic analysis and AST construction | Token sequence | `ASTNode` tree structure | Recursive descent parsing, precedence handling, error recovery |\n| AST Nodes | Structured representation of parsed SQL | Parser method calls | Immutable tree nodes | Node creation, property access, tree traversal |\n\n> **Architecture Principle**: Each component operates independently with clear interfaces, enabling isolated testing and future extensibility. The tokenizer never needs to understand SQL grammar rules, and the parser never manipulates raw character data.\n\nThe **Tokenizer** component encapsulates all lexical analysis logic within a single class that maintains scanning state and provides token generation methods. It recognizes SQL keywords through case-insensitive lookup tables, handles string literals with proper escape sequence processing, and identifies numeric literals with appropriate type classification. The tokenizer maintains position tracking for detailed error reporting and supports lookahead operations for complex token disambiguation.\n\nThe **SQLParser** component serves as the main parsing coordinator that orchestrates statement-specific parsers and manages the overall parsing process. It delegates to specialized parsers like `SelectParser` and `DMLParser` based on the statement type detected from the leading keywords. This component handles error recovery, maintains parsing context, and provides the public API that client code uses to convert SQL strings into AST representations.\n\nThe **AST Node hierarchy** consists of abstract base classes and concrete implementations for each SQL construct type. Statement nodes like `SelectStatement` and `InsertStatement` represent complete SQL statements, while expression nodes like `BinaryOperation` and `Literal` represent components within those statements. Each node type encapsulates its specific properties and provides methods for tree traversal and property access.\n\n### Data Flow Pipeline\n\nThe SQL parsing process follows a clear linear pipeline where data transforms through distinct stages, similar to an assembly line in manufacturing. Raw SQL text enters at one end and emerges as a structured AST at the other end, with each stage adding a layer of understanding and organization to the data.\n\n**Stage 1: Character-Level Scanning**\nThe process begins when raw SQL text enters the tokenizer as a simple string. The tokenizer initializes its scanning state with a character pointer positioned at the beginning of the input and begins systematic character-by-character analysis. It maintains several pieces of state information including current position, line number, column number, and accumulation buffers for building multi-character tokens.\n\nThe character scanning process follows a state-machine approach where the tokenizer's behavior depends on its current state and the character being examined. In the normal scanning state, encountering alphabetic characters triggers identifier or keyword recognition logic, while numeric characters initiate number parsing, and quote characters begin string literal processing. Whitespace characters are consumed and discarded, while operator characters are immediately classified and converted to tokens.\n\n**Stage 2: Token Classification and Generation**\nAs the tokenizer identifies complete lexical units, it creates `Token` objects that encapsulate both the token's content and its classification. Each token contains the original text value, its classified type from the `TokenType` enumeration, and precise position information including line and column numbers. This position information becomes crucial for generating helpful error messages when parsing fails.\n\nThe token generation process involves several classification decisions. When the tokenizer encounters alphabetic character sequences, it must determine whether the sequence represents a SQL keyword like `SELECT` or `WHERE`, or a user-defined identifier like a table or column name. This classification uses case-insensitive lookup in the `SQL_KEYWORDS` mapping, with unrecognized sequences defaulting to identifier tokens.\n\nString literals require more complex processing as the tokenizer must handle quote character variations, escape sequences, and embedded quotes. The tokenizer tracks the opening quote character and scans until it finds the matching closing quote, processing escape sequences like `\\'` and `\\\"` along the way. Numeric literals are classified as either integer or floating-point based on the presence of decimal points.\n\n**Stage 3: Syntactic Analysis and Tree Construction**\nThe parser receives the complete token sequence and begins syntactic analysis using recursive descent parsing techniques. The parsing process starts by examining the first token to determine the statement type—`SELECT` tokens trigger SELECT statement parsing, while `INSERT`, `UPDATE`, and `DELETE` tokens activate the appropriate data modification statement parsers.\n\nRecursive descent parsing mirrors the hierarchical structure of SQL grammar rules. Each grammar rule corresponds to a parsing method that consumes tokens and constructs appropriate AST nodes. For example, the `parse_select_statement` method calls `parse_select_clause` to handle the column list, then `parse_from_clause` to handle table references, and optionally `parse_where_clause` for filtering conditions.\n\nThe parsing process maintains a current token pointer and advances through the sequence as it consumes tokens. When a parsing method recognizes the expected token pattern, it creates the corresponding AST node and populates it with the parsed information. Methods use lookahead to make parsing decisions—examining upcoming tokens without consuming them to determine which parsing path to follow.\n\n**Stage 4: AST Construction and Validation**\nAs parsing methods complete their token consumption, they construct and return AST nodes that represent the parsed SQL constructs. These nodes form a tree structure that mirrors the hierarchical nature of SQL statements. A `SelectStatement` node contains references to its constituent parts: a column list, table references, and optional WHERE conditions.\n\nThe AST construction process validates syntactic correctness as it proceeds. If the parser encounters unexpected tokens or malformed syntax, it generates specific `SyntaxError` exceptions that include position information and helpful error messages. The parser can also perform basic semantic validation, such as ensuring that column and value counts match in INSERT statements.\n\n**Data Flow Summary**\n\n| Stage | Input | Processing | Output | Error Types |\n|-------|-------|------------|---------|-------------|\n| Character Scanning | Raw SQL string | State machine character analysis | Character classifications | `TokenizerError` for malformed literals |\n| Token Generation | Character stream | Lexical analysis and classification | `Token` sequence | `TokenizerError` for invalid syntax |\n| Syntactic Parsing | Token sequence | Recursive descent parsing | Partial AST nodes | `SyntaxError` for grammar violations |\n| AST Construction | Parsed components | Node creation and linking | Complete AST tree | `UnexpectedTokenError` for token mismatches |\n\n> **Critical Design Insight**: The pipeline stages are loosely coupled through well-defined data structures (`Token` and `ASTNode`), enabling independent testing and future enhancements. Each stage can be developed and debugged in isolation.\n\n### Recommended File Structure\n\nThe SQL parser implementation benefits from a modular file organization that mirrors the logical component separation and supports both development workflow and testing strategies. The structure follows Python package conventions while grouping related functionality and separating public interfaces from internal implementation details.\n\nThe recommended organization separates concerns across multiple dimensions: component responsibility (tokenizer vs parser vs AST), statement types (SELECT vs DML), and interface visibility (public API vs internal implementation). This structure supports incremental development where each milestone can be implemented and tested independently.\n\n```\nsql_parser/                           # Main package directory\n    __init__.py                       # Public API exports\n    exceptions.py                     # Error classes and exception hierarchy\n    \n    tokenizer/                        # Tokenization component (Milestone 1)\n        __init__.py                   # Tokenizer public interface\n        tokenizer.py                  # Main Tokenizer class implementation\n        token_types.py                # TokenType enum and Token dataclass\n        keywords.py                   # SQL_KEYWORDS mapping and constants\n    \n    parser/                           # Parsing components (Milestones 2-4)\n        __init__.py                   # Parser public interfaces\n        base_parser.py                # Common parsing utilities and base classes\n        sql_parser.py                 # Main SQLParser class and entry points\n        select_parser.py              # SelectParser for SELECT statements (Milestone 2)\n        expression_parser.py          # WHERE clause and expression parsing (Milestone 3)\n        dml_parser.py                 # DMLParser for INSERT/UPDATE/DELETE (Milestone 4)\n    \n    ast/                              # AST node definitions\n        __init__.py                   # AST node exports\n        base_nodes.py                 # Abstract base classes and common interfaces\n        statement_nodes.py            # Statement AST nodes (SelectStatement, etc.)\n        expression_nodes.py           # Expression AST nodes (BinaryOperation, etc.)\n    \n    tests/                            # Test suite organization\n        test_tokenizer.py             # Tokenizer unit tests (Milestone 1)\n        test_select_parser.py         # SELECT parsing tests (Milestone 2)\n        test_expression_parser.py     # Expression parsing tests (Milestone 3)\n        test_dml_parser.py            # DML parsing tests (Milestone 4)\n        test_integration.py           # End-to-end parser tests\n        fixtures/                     # Test SQL files and expected outputs\n            valid_queries.sql         # Valid SQL test cases\n            invalid_queries.sql       # Error condition test cases\n            expected_asts.json        # Expected AST outputs for validation\n```\n\n**Component Isolation Benefits**\nThe tokenizer directory contains all lexical analysis logic, enabling independent development and testing of character-level processing without concerning parser logic. The `token_types.py` file centralizes all token definitions, making it easy to add new token types as SQL support expands. The `keywords.py` file isolates the SQL keyword recognition logic, supporting case-insensitive matching and future keyword additions.\n\nThe parser directory separates different parsing concerns into focused modules. The `base_parser.py` file contains common utilities used by all parser types, such as token consumption methods, error handling helpers, and lookahead operations. Statement-specific parsers like `select_parser.py` and `dml_parser.py` can focus on their particular grammar rules without duplicating common functionality.\n\n**Testing Strategy Support**\nThe test organization enables both component-level unit testing and integration testing. Each parser component has its dedicated test file, allowing milestone-by-milestone validation. The fixtures directory provides a centralized location for test SQL queries and expected outputs, supporting both positive and negative test cases.\n\nThe structure supports test-driven development where developers can implement and validate each component independently. For example, `test_tokenizer.py` can thoroughly exercise tokenization logic using only the tokenizer component, without requiring parser implementation.\n\n**API Design and Imports**\nThe `__init__.py` files control the public API surface and support clean import statements. Client code can import the main functionality with simple statements like `from sql_parser import SQLParser` or `from sql_parser.tokenizer import Tokenizer`. Internal implementation details remain hidden, enabling future refactoring without breaking client code.\n\n**Development Workflow Support**\n\n| Milestone | Primary Files | Testing Files | Validation Approach |\n|-----------|--------------|---------------|-------------------|\n| 1: Tokenizer | `tokenizer/tokenizer.py`, `tokenizer/token_types.py` | `test_tokenizer.py` | Token sequence validation |\n| 2: SELECT | `parser/select_parser.py`, `ast/statement_nodes.py` | `test_select_parser.py` | AST structure validation |\n| 3: WHERE | `parser/expression_parser.py`, `ast/expression_nodes.py` | `test_expression_parser.py` | Expression tree validation |\n| 4: DML | `parser/dml_parser.py` | `test_dml_parser.py` | Statement parsing validation |\n\n> **Implementation Strategy**: Begin implementation with the tokenizer component, then build parsers incrementally. Each milestone should achieve working functionality for its specific SQL features before proceeding to the next milestone.\n\nThe file structure supports future extensibility by isolating concerns and providing clear extension points. New statement types can be added with new parser modules, new expression types can be added to the expression parser, and new token types can be added to the tokenizer without affecting existing functionality.\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|--------------|-----------------|\n| Tokenizer | Character-by-character scanning with string methods | RegEx-based tokenization with `re` module |\n| Parser | Recursive descent with manual token management | Parser generator with grammar files (PLY, ANTLR) |\n| AST Nodes | Simple dataclasses with basic properties | Rich node classes with visitor pattern support |\n| Error Handling | Exception raising with basic messages | Structured error reporting with position tracking |\n| Testing | Manual test cases with assertions | Property-based testing with Hypothesis |\n\n**B. Recommended File Structure Implementation:**\n\n```python\n# sql_parser/__init__.py\n\"\"\"SQL Parser - Main package entry point.\"\"\"\n\nfrom .parser.sql_parser import SQLParser\nfrom .tokenizer.tokenizer import Tokenizer\nfrom .exceptions import ParseError, TokenizerError, SyntaxError, UnexpectedTokenError\n\n# Public API - what client code should import\n__all__ = ['SQLParser', 'Tokenizer', 'ParseError', 'TokenizerError', 'SyntaxError', 'UnexpectedTokenError']\n\n# Convenience method for most common use case\ndef parse(sql_text: str):\n    \"\"\"Parse SQL text and return AST. Main entry point for client code.\"\"\"\n    parser = SQLParser()\n    return parser.parse(sql_text)\n\ndef tokenize_only(sql_text: str):\n    \"\"\"Tokenize SQL text without parsing. Useful for debugging tokenization.\"\"\"\n    tokenizer = Tokenizer()\n    return tokenizer.tokenize(sql_text)\n```\n\n```python\n# sql_parser/exceptions.py\n\"\"\"Exception classes for SQL parser errors.\"\"\"\n\nclass ParseError(Exception):\n    \"\"\"Base class for all SQL parsing errors.\"\"\"\n    def __init__(self, message: str, line: int = None, column: int = None):\n        self.message = message\n        self.line = line\n        self.column = column\n        super().__init__(self._format_message())\n    \n    def _format_message(self):\n        if self.line is not None and self.column is not None:\n            return f\"Line {self.line}, Column {self.column}: {self.message}\"\n        return self.message\n\nclass TokenizerError(ParseError):\n    \"\"\"Errors during tokenization (lexical analysis).\"\"\"\n    pass\n\nclass SyntaxError(ParseError):\n    \"\"\"Errors during parsing (syntactic analysis).\"\"\"\n    pass\n\nclass UnexpectedTokenError(SyntaxError):\n    \"\"\"Specific error for unexpected token encounters.\"\"\"\n    def __init__(self, expected: str, actual: str, line: int = None, column: int = None):\n        self.expected = expected\n        self.actual = actual\n        message = f\"Expected {expected}, but found {actual}\"\n        super().__init__(message, line, column)\n```\n\n```python\n# sql_parser/tokenizer/token_types.py\n\"\"\"Token type definitions and Token dataclass.\"\"\"\n\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Any\n\nclass TokenType(Enum):\n    # Keywords\n    SELECT = auto()\n    FROM = auto()\n    WHERE = auto()\n    INSERT = auto()\n    UPDATE = auto()\n    DELETE = auto()\n    INTO = auto()\n    VALUES = auto()\n    SET = auto()\n    AS = auto()\n    AND = auto()\n    OR = auto()\n    NOT = auto()\n    NULL = auto()\n    IS = auto()\n    \n    # Identifiers and literals\n    IDENTIFIER = auto()\n    STRING_LITERAL = auto()\n    INTEGER_LITERAL = auto()\n    FLOAT_LITERAL = auto()\n    \n    # Operators\n    EQUALS = auto()          # =\n    NOT_EQUALS = auto()      # != or <>\n    LESS_THAN = auto()       # <\n    GREATER_THAN = auto()    # >\n    LESS_EQUAL = auto()      # <=\n    GREATER_EQUAL = auto()   # >=\n    \n    # Punctuation\n    COMMA = auto()           # ,\n    SEMICOLON = auto()       # ;\n    LEFT_PAREN = auto()      # (\n    RIGHT_PAREN = auto()     # )\n    STAR = auto()            # *\n    \n    # Special\n    EOF = auto()             # End of input\n    UNKNOWN = auto()         # Unrecognized token\n\n@dataclass\nclass Token:\n    \"\"\"Represents a single token from SQL lexical analysis.\"\"\"\n    type: TokenType\n    value: str\n    line: int\n    column: int\n    \n    def __str__(self):\n        return f\"Token({self.type.name}, '{self.value}', {self.line}:{self.column})\"\n```\n\n**C. Infrastructure Starter Code:**\n\n```python\n# sql_parser/tokenizer/keywords.py\n\"\"\"SQL keyword recognition and mapping.\"\"\"\n\nfrom .token_types import TokenType\n\n# Case-insensitive keyword mapping\nSQL_KEYWORDS = {\n    'SELECT': TokenType.SELECT,\n    'FROM': TokenType.FROM,\n    'WHERE': TokenType.WHERE,\n    'INSERT': TokenType.INSERT,\n    'UPDATE': TokenType.UPDATE,\n    'DELETE': TokenType.DELETE,\n    'INTO': TokenType.INTO,\n    'VALUES': TokenType.VALUES,\n    'SET': TokenType.SET,\n    'AS': TokenType.AS,\n    'AND': TokenType.AND,\n    'OR': TokenType.OR,\n    'NOT': TokenType.NOT,\n    'NULL': TokenType.NULL,\n    'IS': TokenType.IS,\n}\n\ndef is_keyword(text: str) -> bool:\n    \"\"\"Check if text is a SQL keyword (case-insensitive).\"\"\"\n    return text.upper() in SQL_KEYWORDS\n\ndef get_keyword_token_type(text: str) -> TokenType:\n    \"\"\"Get TokenType for keyword, or None if not a keyword.\"\"\"\n    return SQL_KEYWORDS.get(text.upper())\n```\n\n**D. Core Logic Skeleton Code:**\n\n```python\n# sql_parser/tokenizer/tokenizer.py\n\"\"\"Main tokenizer implementation - STUDENT IMPLEMENTS THIS.\"\"\"\n\nfrom typing import List, Optional\nfrom .token_types import Token, TokenType\nfrom .keywords import get_keyword_token_type\nfrom ..exceptions import TokenizerError\n\nclass Tokenizer:\n    \"\"\"Converts SQL text into sequence of tokens through lexical analysis.\"\"\"\n    \n    def __init__(self):\n        self.text = \"\"\n        self.position = 0\n        self.line = 1\n        self.column = 1\n    \n    def tokenize(self, text: str) -> List[Token]:\n        \"\"\"Main entry point - tokenize SQL text into list of tokens.\n        \n        TODO 1: Initialize tokenizer state (text, position, line, column)\n        TODO 2: Create empty token list for results\n        TODO 3: Loop while not at end of text:\n               - Skip whitespace and update position tracking\n               - Identify next token type based on current character\n               - Call appropriate token parsing method\n               - Add resulting token to list\n        TODO 4: Add EOF token to mark end of input\n        TODO 5: Return complete token list\n        \"\"\"\n        pass\n    \n    def _current_char(self) -> Optional[str]:\n        \"\"\"Return current character or None if at end of input.\n        \n        TODO: Check if position is within text bounds, return char or None\n        \"\"\"\n        pass\n    \n    def _peek_char(self, offset: int = 1) -> Optional[str]:\n        \"\"\"Look ahead at character without advancing position.\n        \n        TODO: Calculate peek position, check bounds, return char or None\n        \"\"\"\n        pass\n    \n    def _advance(self):\n        \"\"\"Move to next character and update line/column tracking.\n        \n        TODO 1: Check for newline character and update line/column\n        TODO 2: Increment position\n        TODO 3: Update column (increment or reset to 1 for newlines)\n        \"\"\"\n        pass\n    \n    def _skip_whitespace(self):\n        \"\"\"Skip whitespace characters and update position tracking.\n        \n        TODO: Loop while current char is whitespace, call _advance()\n        \"\"\"\n        pass\n    \n    def _read_string_literal(self, quote_char: str) -> Token:\n        \"\"\"Parse quoted string literal handling escape sequences.\n        \n        Args:\n            quote_char: Opening quote character (' or \")\n        \n        TODO 1: Store starting position for token creation\n        TODO 2: Advance past opening quote\n        TODO 3: Build string content, handling escape sequences\n        TODO 4: Look for matching closing quote\n        TODO 5: Create and return STRING_LITERAL token\n        TODO 6: Raise TokenizerError if unterminated string\n        \"\"\"\n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- Use `str.isalpha()`, `str.isdigit()`, and `str.isalnum()` for character classification\n- Use `str.upper()` for case-insensitive keyword matching  \n- Use `enumerate()` when you need both index and character in loops\n- Use `dataclasses` for Token and AST node definitions - they provide `__init__`, `__repr__` automatically\n- Use `typing.Optional[T]` for values that might be None (like end-of-input scenarios)\n- Use `typing.List[T]` and `typing.Dict[K,V]` for type hints on collections\n- Python's `in` operator works efficiently with sets and dictionaries for keyword lookup\n- Use f-strings for error message formatting: `f\"Expected {expected}, got {actual}\"`\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the tokenizer (Milestone 1), verify with these tests:\n\n```python\n# Quick validation script\nfrom sql_parser import tokenize_only\n\n# Test 1: Basic SELECT statement\ntokens = tokenize_only(\"SELECT id, name FROM users WHERE age > 25\")\nexpected_types = [TokenType.SELECT, TokenType.IDENTIFIER, TokenType.COMMA, \n                  TokenType.IDENTIFIER, TokenType.FROM, TokenType.IDENTIFIER, \n                  TokenType.WHERE, TokenType.IDENTIFIER, TokenType.GREATER_THAN, \n                  TokenType.INTEGER_LITERAL, TokenType.EOF]\n\n# Verify token count and types match\nassert len(tokens) == len(expected_types)\nfor i, (token, expected_type) in enumerate(zip(tokens, expected_types)):\n    assert token.type == expected_type, f\"Token {i}: expected {expected_type}, got {token.type}\"\n\nprint(\"✓ Basic tokenization working\")\n\n# Test 2: String literals and operators\ntokens = tokenize_only(\"INSERT INTO users (name) VALUES ('John O\\\\'Connor')\")\nstring_token = next(t for t in tokens if t.type == TokenType.STRING_LITERAL)\nassert string_token.value == \"John O'Connor\", f\"String parsing failed: {string_token.value}\"\n\nprint(\"✓ String literal parsing working\")\n```\n\n**G. Common Pitfalls:**\n\n⚠️ **Pitfall: Forgetting Position Tracking**\nMany students focus on token recognition but forget to maintain accurate line and column numbers. Without position tracking, error messages become useless (\"Syntax error somewhere in your query\"). Always increment column for each character and reset to 1 when encountering newlines.\n\n⚠️ **Pitfall: Case Sensitivity in Keywords** \nSQL keywords are case-insensitive, but Python string comparison is case-sensitive. Always use `.upper()` when checking against your keyword dictionary: `SQL_KEYWORDS.get(text.upper())`.\n\n⚠️ **Pitfall: String Literal Escape Sequences**\nDon't forget to handle escape sequences in string literals. SQL supports `\\'`, `\\\"`, and `\\\\`. Process these during tokenization, not later in parsing.\n\n⚠️ **Pitfall: Multi-Character Operators**\nOperators like `<=`, `>=`, `!=`, and `<>` require lookahead. Don't just tokenize `<` and `=` separately - peek ahead to see if they form a compound operator.\n\n\n## Data Model and AST Design\n\n> **Milestone(s):** This section establishes the foundational data structures for all milestones (1-4), defining the token types that milestone 1 produces and the AST node hierarchy that milestones 2-4 construct.\n\n### Mental Model: Language Structure Blueprint\n\nThink of parsing SQL like understanding the grammatical structure of a sentence in human language. When you read \"The quick brown fox jumps over the lazy dog,\" your mind automatically breaks this into parts: articles, adjectives, nouns, verbs, and prepositions. You then organize these parts into a hierarchical structure - subject, predicate, object - that captures the meaning and relationships.\n\nOur SQL parser works similarly. First, the tokenizer acts like breaking a sentence into individual words and punctuation marks, identifying each piece's role (is \"SELECT\" a keyword? is \"users\" an identifier?). Then the parser acts like a grammar analyzer, taking these categorized pieces and organizing them into a tree structure that captures the SQL statement's meaning and execution intent.\n\nThe **tokens** are like labeled word cards - each piece of the original SQL text tagged with its grammatical role. The **Abstract Syntax Tree (AST)** is like a sentence diagram - a hierarchical structure that shows how these pieces relate to each other and what the overall statement means. Just as a sentence diagram helps you understand \"who did what to whom,\" our AST helps the database understand \"select what from where with which conditions.\"\n\n![Token Type Classification](./diagrams/token-types.svg)\n\n### Token Type Definitions\n\nThe tokenizer must categorize every meaningful piece of text in a SQL statement. Our token classification system divides the SQL language into distinct lexical categories, each serving a specific grammatical purpose in SQL statements. The `TokenType` enumeration defines all possible token categories our parser recognizes.\n\n**Keywords** represent SQL's reserved vocabulary - the fundamental commands and clauses that define statement structure and behavior. Unlike human languages where context often determines meaning, SQL keywords have fixed, unambiguous meanings that cannot be redefined by users.\n\n**Identifiers** represent user-defined names within the database schema - table names, column names, and aliases that users create to organize their data. These tokens must be distinguished from keywords, even when they might spell the same characters in different cases.\n\n**Literals** represent concrete values embedded directly in the SQL text - numbers, strings, and special constants like NULL that specify actual data rather than references to stored data.\n\n**Operators** represent the mathematical, logical, and comparison operations that can be performed on data - the computational vocabulary that transforms and filters information.\n\n**Punctuation** provides the structural grammar that separates and groups other tokens - commas that separate list items, parentheses that group expressions, and semicolons that terminate statements.\n\n| Token Type | Purpose | Examples | Recognition Pattern |\n|------------|---------|----------|-------------------|\n| `KEYWORD_SELECT` | Query projection clause | `SELECT`, `select`, `Select` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_FROM` | Table specification clause | `FROM`, `from`, `From` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_WHERE` | Condition filtering clause | `WHERE`, `where`, `Where` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_INSERT` | Data insertion command | `INSERT`, `insert`, `Insert` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_UPDATE` | Data modification command | `UPDATE`, `update`, `Update` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_DELETE` | Data removal command | `DELETE`, `delete`, `Delete` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_INTO` | Insertion target specification | `INTO`, `into`, `Into` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_VALUES` | Literal value list indicator | `VALUES`, `values`, `Values` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_SET` | Assignment operation indicator | `SET`, `set`, `Set` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_AND` | Logical conjunction operator | `AND`, `and`, `And` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_OR` | Logical disjunction operator | `OR`, `or`, `Or` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_NOT` | Logical negation operator | `NOT`, `not`, `Not` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_NULL` | Null value literal | `NULL`, `null`, `Null` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_IS` | Null comparison operator | `IS`, `is`, `Is` | Case-insensitive match against keyword dictionary |\n| `KEYWORD_AS` | Alias definition indicator | `AS`, `as`, `As` | Case-insensitive match against keyword dictionary |\n| `IDENTIFIER` | User-defined names | `users`, `first_name`, `u1` | Starts with letter/underscore, contains alphanumeric/underscore |\n| `STRING_LITERAL` | Text values | `'John'`, `\"Hello\"`, `'Don''t'` | Single or double quotes with escape sequence support |\n| `INTEGER_LITERAL` | Whole number values | `42`, `0`, `-123`, `999999` | Optional minus sign followed by digits |\n| `FLOAT_LITERAL` | Decimal number values | `3.14`, `-0.5`, `123.456` | Integer pattern with decimal point and fractional digits |\n| `OPERATOR_EQUALS` | Equality comparison | `=` | Single equals character |\n| `OPERATOR_NOT_EQUALS` | Inequality comparison | `!=`, `<>` | Exclamation-equals or angle bracket pair |\n| `OPERATOR_LESS_THAN` | Magnitude comparison | `<` | Single less-than character |\n| `OPERATOR_GREATER_THAN` | Magnitude comparison | `>` | Single greater-than character |\n| `OPERATOR_LESS_EQUAL` | Inclusive magnitude comparison | `<=` | Less-than followed by equals |\n| `OPERATOR_GREATER_EQUAL` | Inclusive magnitude comparison | `>=` | Greater-than followed by equals |\n| `OPERATOR_PLUS` | Addition operator | `+` | Single plus character |\n| `OPERATOR_MINUS` | Subtraction operator | `-` | Single minus character |\n| `OPERATOR_MULTIPLY` | Multiplication operator | `*` | Single asterisk character |\n| `OPERATOR_DIVIDE` | Division operator | `/` | Single forward slash character |\n| `PUNCTUATION_COMMA` | List item separator | `,` | Single comma character |\n| `PUNCTUATION_SEMICOLON` | Statement terminator | `;` | Single semicolon character |\n| `PUNCTUATION_LEFT_PAREN` | Expression grouping start | `(` | Single left parenthesis character |\n| `PUNCTUATION_RIGHT_PAREN` | Expression grouping end | `)` | Single right parenthesis character |\n| `PUNCTUATION_DOT` | Qualified name separator | `.` | Single period character |\n| `WHITESPACE` | Token separation | ` `, `\\t`, `\\n`, `\\r` | Spaces, tabs, newlines, carriage returns |\n| `EOF` | End of input marker | (implicit) | Reached end of input string |\n\nThe `Token` data structure encapsulates each recognized lexical unit along with essential metadata for error reporting and debugging. Position information enables the parser to provide meaningful error messages that pinpoint exactly where problems occur in the original SQL text.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `type` | `TokenType` | The grammatical category this token belongs to |\n| `value` | `str` | The original text from the SQL input that this token represents |\n| `line` | `int` | Line number where this token appears (1-based for human readability) |\n| `column` | `int` | Column position where this token starts (1-based for human readability) |\n\n> The position tracking in tokens serves two critical purposes beyond simple error reporting. First, it enables sophisticated IDE features like syntax highlighting and auto-completion by providing precise location information. Second, it supports advanced error recovery techniques where the parser can suggest corrections based on the surrounding context and common patterns at specific positions within SQL statements.\n\n**Multi-character Operator Recognition Strategy**\n\nSeveral SQL operators consist of multiple characters that must be recognized as single tokens rather than separate punctuation marks. The tokenizer must use **maximal munch** strategy - always consume the longest possible token sequence that forms a valid token.\n\n| Multi-Character Operator | Individual Characters | Correct Tokenization | Incorrect Tokenization |\n|-------------------------|---------------------|---------------------|----------------------|\n| `!=` | `!` and `=` | `OPERATOR_NOT_EQUALS` | `INVALID` + `OPERATOR_EQUALS` |\n| `<>` | `<` and `>` | `OPERATOR_NOT_EQUALS` | `OPERATOR_LESS_THAN` + `OPERATOR_GREATER_THAN` |\n| `<=` | `<` and `=` | `OPERATOR_LESS_EQUAL` | `OPERATOR_LESS_THAN` + `OPERATOR_EQUALS` |\n| `>=` | `>` and `=` | `OPERATOR_GREATER_EQUAL` | `OPERATOR_GREATER_THAN` + `OPERATOR_EQUALS` |\n\nThe tokenizer implements maximal munch by examining the current character and looking ahead to determine if a longer token is possible. For example, when encountering `<`, the tokenizer checks the next character: if it's `=`, the complete token becomes `OPERATOR_LESS_EQUAL`; if it's `>`, the complete token becomes `OPERATOR_NOT_EQUALS`; otherwise, the token is simply `OPERATOR_LESS_THAN`.\n\n![AST Node Type Hierarchy](./diagrams/ast-node-hierarchy.svg)\n\n### AST Node Hierarchy\n\nThe Abstract Syntax Tree represents the parsed SQL statement as a hierarchical data structure that captures both the syntactic structure and semantic relationships within the query. Our AST design follows the **Composite Pattern** from object-oriented design, where every node shares common properties and behaviors while specialized node types handle specific SQL constructs.\n\nThe base `ASTNode` class establishes the fundamental interface that all AST nodes implement. This common interface enables uniform traversal, debugging, and future extensions like query optimization or code generation. Each node maintains essential metadata about its source location and provides standardized methods for tree navigation and introspection.\n\n**Base AST Node Interface**\n\nEvery AST node, regardless of its specific type or purpose, implements the fundamental `ASTNode` interface. This interface provides the essential operations needed for tree traversal, debugging, serialization, and future compiler phases like semantic analysis or code generation.\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `node_type()` | None | `str` | Returns string identifying the specific node type (e.g., \"SelectStatement\", \"BinaryOperation\") |\n| `children()` | None | `List[ASTNode]` | Returns list of direct child nodes in left-to-right order for tree traversal |\n| `accept(visitor)` | `visitor: ASTVisitor` | `Any` | Implements visitor pattern for extensible tree operations like pretty-printing or analysis |\n| `source_location()` | None | `SourceLocation` | Returns line/column information for error reporting and debugging |\n| `__str__()` | None | `str` | Human-readable string representation for debugging and testing |\n| `__repr__()` | None | `str` | Developer-friendly string representation showing node type and key properties |\n\nThe `SourceLocation` helper class encapsulates position information that traces each AST node back to its original position in the SQL text. This information proves invaluable during error reporting, allowing the parser to show users exactly where problems occur in their queries.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `start_line` | `int` | Line number where this construct begins (1-based) |\n| `start_column` | `int` | Column position where this construct begins (1-based) |\n| `end_line` | `int` | Line number where this construct ends (1-based) |\n| `end_column` | `int` | Column position where this construct ends (1-based) |\n\n**Node Classification Strategy**\n\nOur AST nodes organize into three primary categories based on their role in SQL statement structure:\n\n**Statement Nodes** represent complete SQL commands that can be executed independently. These top-level nodes correspond to the major SQL statement types our parser supports. Each statement node contains the specific clauses and expressions that define the statement's behavior.\n\n**Expression Nodes** represent computational units that evaluate to values during query execution. These nodes handle literals, identifiers, operators, function calls, and any other construct that produces a value. Expression nodes form the computational vocabulary of SQL queries.\n\n**Clause Nodes** represent the major structural components within statements - SELECT lists, FROM clauses, WHERE conditions, and similar constructs that organize and contain expressions. Clause nodes bridge the gap between high-level statement structure and low-level expression details.\n\n| Node Category | Purpose | Examples | Parent-Child Relationships |\n|---------------|---------|----------|--------------------------|\n| Statement | Complete executable commands | `SelectStatement`, `InsertStatement` | Root nodes with clause children |\n| Expression | Value-producing computations | `Literal`, `Identifier`, `BinaryOperation` | Leaf or internal nodes within clauses |\n| Clause | Statement structural components | `SelectClause`, `FromClause`, `WhereClause` | Children of statements, parents of expressions |\n\n> **Design Insight**: The three-tier hierarchy (Statement → Clause → Expression) mirrors how humans naturally think about SQL queries. We start with the overall intent (SELECT data), then specify the major components (which columns FROM which tables WHERE certain conditions), and finally fill in the computational details (specific column names, comparison values, logical operators).\n\n### Statement AST Nodes\n\nStatement nodes represent the top-level executable commands in SQL. Each statement type corresponds to one of the major data manipulation operations our parser supports. These nodes serve as the root of AST trees and contain all the information necessary to understand and execute the complete SQL command.\n\n**SELECT Statement Structure**\n\nThe `SelectStatement` node represents queries that retrieve and project data from tables. SELECT statements demonstrate the most complex AST structure in our parser, incorporating multiple optional clauses that can appear in specific orders with interdependent relationships.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `select_clause` | `SelectClause` | Specifies which columns or expressions to retrieve from the result set |\n| `from_clause` | `FromClause \\| None` | Identifies source tables and their relationships (optional for scalar SELECT) |\n| `where_clause` | `WhereClause \\| None` | Filters rows based on boolean conditions (optional) |\n| `distinct` | `bool` | Whether to eliminate duplicate rows from the result set |\n\nThe SELECT statement's clause structure reflects SQL's declarative nature - each clause specifies \"what\" rather than \"how,\" leaving execution strategy decisions to the database engine. The optional nature of FROM and WHERE clauses accommodates both simple scalar queries (`SELECT 1 + 1`) and complex filtered table queries (`SELECT name FROM users WHERE age > 18`).\n\n**INSERT Statement Structure**\n\nThe `InsertStatement` node represents commands that add new rows to tables. INSERT statements require careful coordination between column specifications and value lists to ensure data integrity and proper schema compliance.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `table_name` | `Identifier` | Target table that will receive the new rows |\n| `column_list` | `List[Identifier] \\| None` | Explicit column names for value mapping (optional, defaults to table order) |\n| `values_clause` | `ValuesClause` | Source of data for the new rows (VALUES list or SELECT statement) |\n\nThe relationship between `column_list` and `values_clause` requires validation during semantic analysis. When `column_list` is specified, each value tuple in the VALUES clause must contain exactly the same number of expressions. When `column_list` is omitted, the VALUES clause must provide values for all columns in the table's schema order.\n\n**UPDATE Statement Structure**\n\nThe `UpdateStatement` node represents commands that modify existing table data. UPDATE statements combine assignment operations with conditional filtering to specify both what changes and which rows receive those changes.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `table_name` | `Identifier` | Target table containing rows to modify |\n| `set_clause` | `SetClause` | List of column assignments that define the modifications |\n| `where_clause` | `WhereClause \\| None` | Conditions that determine which rows to modify (optional but dangerous without) |\n\nThe `SetClause` contains a list of assignment expressions, each specifying a column name and the new value expression. The WHERE clause serves as a critical safety mechanism - UPDATE statements without WHERE clauses modify ALL rows in the target table, which rarely represents the intended behavior.\n\n**DELETE Statement Structure**\n\nThe `DeleteStatement` node represents commands that remove rows from tables. DELETE statements demonstrate the simplest statement structure in our parser, requiring only a target table and optional filtering conditions.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `table_name` | `Identifier` | Target table containing rows to remove |\n| `where_clause` | `WhereClause \\| None` | Conditions that determine which rows to remove (optional but dangerous without) |\n\nDELETE statements share the same safety concerns as UPDATE statements - omitting the WHERE clause removes ALL rows from the target table. Many production database systems require explicit confirmation or administrative privileges for unfiltered DELETE operations.\n\n> **Architecture Decision: Statement Node Completeness**\n> - **Context**: SQL statements can have many optional clauses, and we must decide how much complexity to handle in our initial parser implementation.\n> - **Options Considered**: \n>   1. Minimal statement support (only required clauses)\n>   2. Complete SQL standard compliance (all possible clauses)  \n>   3. Practical subset based on common usage patterns\n> - **Decision**: Implement practical subset covering the most frequently used clauses\n> - **Rationale**: This approach provides immediate value for common queries while maintaining extensibility for future enhancements. Full SQL compliance would require months of development effort with diminishing returns for learning purposes.\n> - **Consequences**: Users can parse and understand the majority of real-world SQL queries, but some advanced features like JOINs, subqueries, and window functions require future extensions.\n\n### Expression AST Nodes\n\nExpression nodes represent the computational components of SQL statements - any construct that evaluates to a value during query execution. These nodes form the building blocks for conditions, calculations, and data transformations within SQL queries. Expression nodes must support type information and evaluation contexts to enable future semantic analysis and optimization phases.\n\n**Literal Expression Nodes**\n\nLiteral nodes represent constant values embedded directly in the SQL text. These nodes store the parsed value along with type information that enables proper comparison operations and prevents type-related errors during query execution.\n\nThe `StringLiteral` node handles text constants with proper escape sequence processing. SQL string literals can use either single quotes (`'text'`) or double quotes (`\"text\"`), and they support escape sequences for embedded quotes and special characters.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `value` | `str` | The processed string content with escape sequences resolved |\n| `quote_char` | `str` | Original quote character used (`'` or `\"`) for round-trip preservation |\n| `raw_value` | `str` | Original text including quotes for debugging and error reporting |\n\nThe `IntegerLiteral` node represents whole number constants with support for both positive and negative values. Integer literals must handle potential overflow conditions and provide appropriate type information for arithmetic operations.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `value` | `int` | The parsed numeric value |\n| `raw_value` | `str` | Original text representation for debugging and formatting preservation |\n\nThe `FloatLiteral` node represents decimal number constants with fractional components. Float literals require careful precision handling and must support scientific notation in future extensions.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `value` | `float` | The parsed numeric value with decimal precision |\n| `raw_value` | `str` | Original text representation for precision preservation |\n\nThe `NullLiteral` node represents the special NULL value that indicates missing or undefined data. NULL values require special comparison semantics (NULL = NULL is unknown, not true) and must be handled carefully in all expression evaluations.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `value` | `None` | Always None to represent the NULL value |\n\n**Identifier Expression Nodes**\n\nThe `Identifier` node represents references to database objects like tables, columns, and aliases. Identifiers can be simple names (`username`) or qualified names (`users.username`) that specify both table and column components.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `name` | `str` | The identifier name as it appears in the SQL (case-preserved) |\n| `table_qualifier` | `str \\| None` | Optional table prefix for qualified column references |\n| `quoted` | `bool` | Whether this identifier was quoted in the original SQL (affects case sensitivity) |\n\nQuoted identifiers (enclosed in backticks, square brackets, or double quotes depending on SQL dialect) preserve exact case and allow reserved words to be used as identifiers. Unquoted identifiers typically follow case-insensitive matching rules, though this varies by database system.\n\n**Binary Operation Expression Nodes**\n\nThe `BinaryOperation` node represents expressions that combine two operands with a single operator. These nodes form the backbone of arithmetic calculations, comparisons, and logical operations in SQL queries.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `left` | `ASTNode` | Left operand expression (can be any expression type) |\n| `operator` | `TokenType` | The operation to perform (PLUS, EQUALS, AND, etc.) |\n| `right` | `ASTNode` | Right operand expression (can be any expression type) |\n\nBinary operations require careful precedence and associativity handling during parsing. The AST structure must reflect the correct evaluation order, with higher-precedence operations appearing deeper in the tree (closer to the leaves).\n\n**Arithmetic Operations** include addition (`+`), subtraction (`-`), multiplication (`*`), and division (`/`). These operations typically require numeric operands and produce numeric results, though some SQL systems support string concatenation using the `+` operator.\n\n**Comparison Operations** include equality (`=`), inequality (`!=`, `<>`), and magnitude comparisons (`<`, `>`, `<=`, `>=`). Comparison operations accept various operand types but always produce boolean results.\n\n**Logical Operations** include conjunction (`AND`), disjunction (`OR`), and negation (`NOT`). Logical operations require boolean operands and produce boolean results, with special handling for NULL values and three-valued logic.\n\n> **Expression Tree Structure Example**: The condition `age > 18 AND name = 'John'` produces a binary operation tree where:\n> - Root: BinaryOperation(operator=AND)\n> - Left child: BinaryOperation(operator=GREATER_THAN, left=Identifier('age'), right=IntegerLiteral(18))\n> - Right child: BinaryOperation(operator=EQUALS, left=Identifier('name'), right=StringLiteral('John'))\n\n**Unary Operation Expression Nodes**\n\nThe `UnaryOperation` node represents expressions with a single operand and operator. The most common unary operation in SQL is logical negation (`NOT`), though arithmetic negation (`-`) for negative numbers is handled during tokenization as part of numeric literals.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `operator` | `TokenType` | The operation to perform (typically NOT) |\n| `operand` | `ASTNode` | The expression to apply the operation to |\n\nUnary operations have higher precedence than most binary operations, so `NOT age > 18` parses as `NOT (age > 18)` rather than `(NOT age) > 18`.\n\n**Function Call Expression Nodes**\n\nThe `FunctionCall` node represents invocations of SQL functions, both built-in functions (like `COUNT`, `SUM`, `UPPER`) and user-defined functions. Function calls include the function name and an argument list that can contain any number of expressions.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `function_name` | `Identifier` | Name of the function to invoke |\n| `arguments` | `List[ASTNode]` | List of argument expressions (can be empty) |\n| `distinct` | `bool` | Whether DISTINCT was specified for aggregate functions |\n\nFunction calls require semantic analysis to validate argument counts, types, and contexts. Some functions like `COUNT(*)` have special syntax rules, while aggregate functions like `SUM(DISTINCT column)` support optional DISTINCT qualifiers.\n\n![Example Parse Tree for Complex Query](./diagrams/parse-tree-example.svg)\n\n> **Architecture Decision: Expression Node Extensibility**\n> - **Context**: SQL expressions can become arbitrarily complex with nested function calls, subqueries, and case expressions. We must balance initial simplicity with future extensibility needs.\n> - **Options Considered**:\n>   1. Flat expression model (limited nesting depth)\n>   2. Fully recursive expression model (unlimited nesting)\n>   3. Hybrid model (recursive with practical depth limits)\n> - **Decision**: Implement fully recursive expression model with clear extension points\n> - **Rationale**: Real SQL queries often contain deeply nested expressions, and artificial limitations would restrict the parser's utility. The recursive model aligns naturally with the recursive descent parsing algorithm.\n> - **Consequences**: Parser can handle complex expressions from real applications, but stack overflow protection and depth limiting may be needed for malicious inputs.\n\n**Common AST Construction Pitfalls**\n\n⚠️ **Pitfall: Incorrect Operator Precedence in AST Structure**\nMany implementers create AST trees that don't reflect proper operator precedence, leading to incorrect expression evaluation. For example, the expression `a + b * c` should parse as `a + (b * c)`, not `(a + b) * c`. The AST structure must place higher-precedence operations (multiplication) deeper in the tree than lower-precedence operations (addition). During parsing, ensure that precedence climbing or recursive descent properly structures the tree according to operator precedence rules, not just left-to-right parsing order.\n\n⚠️ **Pitfall: Missing Position Information in AST Nodes**\nForgetting to propagate source position information from tokens to AST nodes makes debugging and error reporting nearly impossible. Each AST node should maintain line and column information that traces back to the original SQL text. Without this information, error messages become generic and unhelpful (\"syntax error\" instead of \"syntax error at line 3, column 15: expected comma after column name\"). Always copy position information from tokens during AST node construction, and consider spanning positions for nodes that cover multiple tokens.\n\n⚠️ **Pitfall: Mutable AST Node Fields**\nMaking AST node fields mutable creates opportunities for bugs where tree structure gets accidentally modified during traversal or analysis. AST nodes should be immutable data structures - once constructed during parsing, their content should never change. Use read-only properties or immutable data structures to prevent accidental modifications. If you need to transform the AST, create new nodes rather than modifying existing ones.\n\n⚠️ **Pitfall: Inconsistent Node Type Hierarchies**\nCreating AST node classes without consistent interfaces makes tree traversal and analysis code brittle. All AST nodes should implement the same base interface with consistent method names and behaviors. Don't create special cases where some nodes lack common methods like `children()` or `accept()`. Consistent interfaces enable generic tree operations like pretty-printing, serialization, and visitor-based analysis.\n\n⚠️ **Pitfall: Inadequate Type Information in Expression Nodes**\nExpression nodes that don't maintain sufficient type information make semantic analysis and code generation difficult or impossible. While our basic parser focuses on syntactic structure, including type hints and value information in expression nodes enables future enhancements. At minimum, distinguish between different literal types (string, integer, float) and maintain enough information to reconstruct the original SQL text.\n\n### Implementation Guidance\n\nThis implementation guidance provides Python-specific code for the token types and AST node hierarchy defined above. The code emphasizes clarity and extensibility while following Python best practices for data classes and type hints.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|----------------|\n| Token Data Structure | Named tuples (`collections.namedtuple`) | Data classes with `@dataclass` decorator |\n| AST Node Base Class | Abstract base class (`abc.ABC`) | Protocol classes for structural typing |\n| Enum Definitions | Standard `enum.Enum` | `enum.auto()` for automatic value assignment |\n| Type Checking | Built-in `typing` module | Third-party `mypy` for static analysis |\n\n**Recommended File Structure**\n\n```\nsql_parser/\n├── __init__.py\n├── tokens.py              ← Token types and Token data class\n├── ast_nodes/\n│   ├── __init__.py        ← Import all node types for convenience\n│   ├── base.py           ← Base ASTNode interface and common utilities\n│   ├── statements.py     ← Statement node classes (SELECT, INSERT, etc.)\n│   ├── expressions.py    ← Expression node classes (literals, operators, etc.)\n│   └── clauses.py        ← Clause node classes (FROM, WHERE, etc.)\n├── exceptions.py         ← All parser exception classes\n└── utils.py             ← Common utilities and helper functions\n```\n\n**Complete Token Infrastructure (Ready to Use)**\n\n```python\n# tokens.py\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Dict, Set\n\nclass TokenType(Enum):\n    # Keywords\n    KEYWORD_SELECT = auto()\n    KEYWORD_FROM = auto()\n    KEYWORD_WHERE = auto()\n    KEYWORD_INSERT = auto()\n    KEYWORD_UPDATE = auto()\n    KEYWORD_DELETE = auto()\n    KEYWORD_INTO = auto()\n    KEYWORD_VALUES = auto()\n    KEYWORD_SET = auto()\n    KEYWORD_AND = auto()\n    KEYWORD_OR = auto()\n    KEYWORD_NOT = auto()\n    KEYWORD_NULL = auto()\n    KEYWORD_IS = auto()\n    KEYWORD_AS = auto()\n    KEYWORD_DISTINCT = auto()\n    \n    # Identifiers and Literals\n    IDENTIFIER = auto()\n    STRING_LITERAL = auto()\n    INTEGER_LITERAL = auto()\n    FLOAT_LITERAL = auto()\n    \n    # Operators\n    OPERATOR_EQUALS = auto()\n    OPERATOR_NOT_EQUALS = auto()\n    OPERATOR_LESS_THAN = auto()\n    OPERATOR_GREATER_THAN = auto()\n    OPERATOR_LESS_EQUAL = auto()\n    OPERATOR_GREATER_EQUAL = auto()\n    OPERATOR_PLUS = auto()\n    OPERATOR_MINUS = auto()\n    OPERATOR_MULTIPLY = auto()\n    OPERATOR_DIVIDE = auto()\n    \n    # Punctuation\n    PUNCTUATION_COMMA = auto()\n    PUNCTUATION_SEMICOLON = auto()\n    PUNCTUATION_LEFT_PAREN = auto()\n    PUNCTUATION_RIGHT_PAREN = auto()\n    PUNCTUATION_DOT = auto()\n    \n    # Special\n    WHITESPACE = auto()\n    EOF = auto()\n    INVALID = auto()\n\n# SQL keyword mapping for case-insensitive recognition\nSQL_KEYWORDS: Dict[str, TokenType] = {\n    'select': TokenType.KEYWORD_SELECT,\n    'from': TokenType.KEYWORD_FROM,\n    'where': TokenType.KEYWORD_WHERE,\n    'insert': TokenType.KEYWORD_INSERT,\n    'update': TokenType.KEYWORD_UPDATE,\n    'delete': TokenType.KEYWORD_DELETE,\n    'into': TokenType.KEYWORD_INTO,\n    'values': TokenType.KEYWORD_VALUES,\n    'set': TokenType.KEYWORD_SET,\n    'and': TokenType.KEYWORD_AND,\n    'or': TokenType.KEYWORD_OR,\n    'not': TokenType.KEYWORD_NOT,\n    'null': TokenType.KEYWORD_NULL,\n    'is': TokenType.KEYWORD_IS,\n    'as': TokenType.KEYWORD_AS,\n    'distinct': TokenType.KEYWORD_DISTINCT,\n}\n\n@dataclass(frozen=True)\nclass Token:\n    \"\"\"Immutable token with position information for error reporting.\"\"\"\n    type: TokenType\n    value: str\n    line: int\n    column: int\n    \n    def is_keyword(self) -> bool:\n        \"\"\"Check if this token represents a SQL keyword.\"\"\"\n        return self.type.name.startswith('KEYWORD_')\n    \n    def is_operator(self) -> bool:\n        \"\"\"Check if this token represents an operator.\"\"\"\n        return self.type.name.startswith('OPERATOR_')\n    \n    def is_literal(self) -> bool:\n        \"\"\"Check if this token represents a literal value.\"\"\"\n        return self.type in {\n            TokenType.STRING_LITERAL,\n            TokenType.INTEGER_LITERAL,\n            TokenType.FLOAT_LITERAL,\n            TokenType.KEYWORD_NULL\n        }\n\n@dataclass(frozen=True)\nclass SourceLocation:\n    \"\"\"Source position information for AST nodes.\"\"\"\n    start_line: int\n    start_column: int\n    end_line: int\n    end_column: int\n    \n    @classmethod\n    def from_token(cls, token: Token) -> 'SourceLocation':\n        \"\"\"Create source location from a single token.\"\"\"\n        return cls(\n            start_line=token.line,\n            start_column=token.column,\n            end_line=token.line,\n            end_column=token.column + len(token.value)\n        )\n    \n    @classmethod\n    def span_tokens(cls, start_token: Token, end_token: Token) -> 'SourceLocation':\n        \"\"\"Create source location spanning from start token to end token.\"\"\"\n        return cls(\n            start_line=start_token.line,\n            start_column=start_token.column,\n            end_line=end_token.line,\n            end_column=end_token.column + len(end_token.value)\n        )\n```\n\n**Complete Exception Hierarchy (Ready to Use)**\n\n```python\n# exceptions.py\nfrom typing import Optional\nfrom .tokens import Token, SourceLocation\n\nclass ParseError(Exception):\n    \"\"\"Base exception for all parsing errors.\"\"\"\n    def __init__(self, message: str, location: Optional[SourceLocation] = None):\n        super().__init__(message)\n        self.message = message\n        self.location = location\n    \n    def __str__(self) -> str:\n        if self.location:\n            return f\"Parse error at line {self.location.start_line}, column {self.location.start_column}: {self.message}\"\n        return f\"Parse error: {self.message}\"\n\nclass TokenizerError(ParseError):\n    \"\"\"Exception raised during tokenization phase.\"\"\"\n    pass\n\nclass SyntaxError(ParseError):\n    \"\"\"Exception raised during parsing phase due to invalid syntax.\"\"\"\n    pass\n\nclass UnexpectedTokenError(SyntaxError):\n    \"\"\"Exception raised when parser encounters unexpected token type.\"\"\"\n    def __init__(self, expected: str, actual: Token):\n        location = SourceLocation.from_token(actual)\n        message = f\"Expected {expected}, but found {actual.type.name} '{actual.value}'\"\n        super().__init__(message, location)\n        self.expected = expected\n        self.actual = actual\n```\n\n**Base AST Node Infrastructure (Ready to Use)**\n\n```python\n# ast_nodes/base.py\nfrom abc import ABC, abstractmethod\nfrom typing import List, Any, Optional\nfrom ..tokens import SourceLocation\n\nclass ASTVisitor(ABC):\n    \"\"\"Visitor interface for AST traversal and analysis.\"\"\"\n    \n    @abstractmethod\n    def visit_select_statement(self, node: 'SelectStatement') -> Any:\n        pass\n    \n    @abstractmethod\n    def visit_identifier(self, node: 'Identifier') -> Any:\n        pass\n    \n    @abstractmethod\n    def visit_string_literal(self, node: 'StringLiteral') -> Any:\n        pass\n    \n    # Add visit methods for each AST node type as you implement them\n\nclass ASTNode(ABC):\n    \"\"\"Base class for all AST nodes.\"\"\"\n    \n    def __init__(self, location: Optional[SourceLocation] = None):\n        self._location = location\n    \n    @abstractmethod\n    def node_type(self) -> str:\n        \"\"\"Return string identifier for this node type.\"\"\"\n        pass\n    \n    @abstractmethod\n    def children(self) -> List['ASTNode']:\n        \"\"\"Return list of direct child nodes.\"\"\"\n        pass\n    \n    @abstractmethod\n    def accept(self, visitor: ASTVisitor) -> Any:\n        \"\"\"Accept visitor for tree traversal.\"\"\"\n        pass\n    \n    def source_location(self) -> Optional[SourceLocation]:\n        \"\"\"Return source location information.\"\"\"\n        return self._location\n    \n    def __str__(self) -> str:\n        \"\"\"Human-readable representation for debugging.\"\"\"\n        children_str = ', '.join(str(child) for child in self.children())\n        return f\"{self.node_type()}({children_str})\"\n    \n    def __repr__(self) -> str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return f\"<{self.node_type()} at {self.source_location()}>\"\n```\n\n**Statement AST Node Skeletons (Core Logic for Students to Implement)**\n\n```python\n# ast_nodes/statements.py\nfrom typing import List, Optional, Any\nfrom .base import ASTNode, ASTVisitor\nfrom ..tokens import SourceLocation\n\nclass SelectStatement(ASTNode):\n    \"\"\"AST node representing a SELECT query statement.\"\"\"\n    \n    def __init__(self, \n                 select_clause: 'SelectClause',\n                 from_clause: Optional['FromClause'] = None,\n                 where_clause: Optional['WhereClause'] = None,\n                 distinct: bool = False,\n                 location: Optional[SourceLocation] = None):\n        super().__init__(location)\n        # TODO: Store the provided clause nodes in instance variables\n        # TODO: Validate that select_clause is not None\n        # Hint: self.select_clause = select_clause\n        \n    def node_type(self) -> str:\n        return \"SelectStatement\"\n    \n    def children(self) -> List[ASTNode]:\n        # TODO: Return list of non-None clause nodes\n        # TODO: Include select_clause (always present)\n        # TODO: Include from_clause if not None\n        # TODO: Include where_clause if not None\n        # Hint: Use list comprehension with filter(None, [clause1, clause2, ...])\n        pass\n    \n    def accept(self, visitor: ASTVisitor) -> Any:\n        return visitor.visit_select_statement(self)\n\nclass InsertStatement(ASTNode):\n    \"\"\"AST node representing an INSERT statement.\"\"\"\n    \n    def __init__(self,\n                 table_name: 'Identifier',\n                 values_clause: 'ValuesClause',\n                 column_list: Optional[List['Identifier']] = None,\n                 location: Optional[SourceLocation] = None):\n        super().__init__(location)\n        # TODO: Store table_name and values_clause (both required)\n        # TODO: Store column_list (optional, defaults to empty list if None)\n        # TODO: Validate that table_name and values_clause are not None\n        pass\n    \n    def node_type(self) -> str:\n        return \"InsertStatement\"\n    \n    def children(self) -> List[ASTNode]:\n        # TODO: Return list starting with table_name, then values_clause\n        # TODO: Add all identifiers from column_list if present\n        # Hint: [self.table_name, self.values_clause] + self.column_list\n        pass\n    \n    def accept(self, visitor: ASTVisitor) -> Any:\n        return visitor.visit_insert_statement(self)\n\nclass UpdateStatement(ASTNode):\n    \"\"\"AST node representing an UPDATE statement.\"\"\"\n    \n    def __init__(self,\n                 table_name: 'Identifier',\n                 set_clause: 'SetClause',\n                 where_clause: Optional['WhereClause'] = None,\n                 location: Optional[SourceLocation] = None):\n        super().__init__(location)\n        # TODO: Store table_name and set_clause (both required)\n        # TODO: Store where_clause (optional for UPDATE, but dangerous without it)\n        # TODO: Consider adding a warning when where_clause is None\n        pass\n    \n    def node_type(self) -> str:\n        return \"UpdateStatement\"\n    \n    def children(self) -> List[ASTNode]:\n        # TODO: Return table_name, set_clause, and where_clause (if present)\n        # TODO: Use filter(None, ...) to exclude None values\n        pass\n    \n    def accept(self, visitor: ASTVisitor) -> Any:\n        return visitor.visit_update_statement(self)\n\nclass DeleteStatement(ASTNode):\n    \"\"\"AST node representing a DELETE statement.\"\"\"\n    \n    def __init__(self,\n                 table_name: 'Identifier',\n                 where_clause: Optional['WhereClause'] = None,\n                 location: Optional[SourceLocation] = None):\n        super().__init__(location)\n        # TODO: Store table_name (required)\n        # TODO: Store where_clause (optional but dangerous without it)\n        # TODO: Consider logging a warning when where_clause is None (deletes all rows)\n        pass\n    \n    def node_type(self) -> str:\n        return \"DeleteStatement\"\n    \n    def children(self) -> List[ASTNode]:\n        # TODO: Return table_name and where_clause (if present)\n        # TODO: where_clause can be None, so filter it out if missing\n        pass\n    \n    def accept(self, visitor: ASTVisitor) -> Any:\n        return visitor.visit_delete_statement(self)\n```\n\n**Expression AST Node Skeletons (Core Logic for Students to Implement)**\n\n```python\n# ast_nodes/expressions.py\nfrom typing import List, Optional, Any\nfrom .base import ASTNode, ASTVisitor\nfrom ..tokens import SourceLocation, TokenType\n\nclass Identifier(ASTNode):\n    \"\"\"AST node representing a table or column identifier.\"\"\"\n    \n    def __init__(self,\n                 name: str,\n                 table_qualifier: Optional[str] = None,\n                 quoted: bool = False,\n                 location: Optional[SourceLocation] = None):\n        super().__init__(location)\n        # TODO: Store the name (required - the identifier text)\n        # TODO: Store table_qualifier (optional - for qualified names like table.column)\n        # TODO: Store quoted flag (whether identifier was quoted in SQL)\n        # TODO: Validate that name is not empty\n        pass\n    \n    def node_type(self) -> str:\n        return \"Identifier\"\n    \n    def children(self) -> List[ASTNode]:\n        # TODO: Identifiers are leaf nodes - return empty list\n        # Hint: return []\n        pass\n    \n    def accept(self, visitor: ASTVisitor) -> Any:\n        return visitor.visit_identifier(self)\n    \n    def qualified_name(self) -> str:\n        \"\"\"Return the full qualified name (table.column or just column).\"\"\"\n        # TODO: If table_qualifier exists, return f\"{table_qualifier}.{name}\"\n        # TODO: Otherwise, return just the name\n        # Hint: Use conditional expression or if statement\n        pass\n\nclass StringLiteral(ASTNode):\n    \"\"\"AST node representing a string literal value.\"\"\"\n    \n    def __init__(self,\n                 value: str,\n                 quote_char: str = \"'\",\n                 raw_value: Optional[str] = None,\n                 location: Optional[SourceLocation] = None):\n        super().__init__(location)\n        # TODO: Store the processed string value (with escape sequences resolved)\n        # TODO: Store the quote character used (' or \")\n        # TODO: Store raw_value for debugging (original text including quotes)\n        # TODO: If raw_value not provided, construct it from quote_char and value\n        pass\n    \n    def node_type(self) -> str:\n        return \"StringLiteral\"\n    \n    def children(self) -> List[ASTNode]:\n        return []  # Literals are leaf nodes\n    \n    def accept(self, visitor: ASTVisitor) -> Any:\n        return visitor.visit_string_literal(self)\n\nclass IntegerLiteral(ASTNode):\n    \"\"\"AST node representing an integer literal value.\"\"\"\n    \n    def __init__(self,\n                 value: int,\n                 raw_value: Optional[str] = None,\n                 location: Optional[SourceLocation] = None):\n        super().__init__(location)\n        # TODO: Store the parsed integer value\n        # TODO: Store raw_value (original text) for debugging\n        # TODO: If raw_value not provided, convert value to string\n        pass\n    \n    def node_type(self) -> str:\n        return \"IntegerLiteral\"\n    \n    def children(self) -> List[ASTNode]:\n        return []\n    \n    def accept(self, visitor: ASTVisitor) -> Any:\n        return visitor.visit_integer_literal(self)\n\nclass BinaryOperation(ASTNode):\n    \"\"\"AST node representing a binary operation (left operator right).\"\"\"\n    \n    def __init__(self,\n                 left: ASTNode,\n                 operator: TokenType,\n                 right: ASTNode,\n                 location: Optional[SourceLocation] = None):\n        super().__init__(location)\n        # TODO: Store left operand (required ASTNode)\n        # TODO: Store operator (required TokenType like OPERATOR_EQUALS)\n        # TODO: Store right operand (required ASTNode)\n        # TODO: Validate that all parameters are not None\n        pass\n    \n    def node_type(self) -> str:\n        return \"BinaryOperation\"\n    \n    def children(self) -> List[ASTNode]:\n        # TODO: Return list containing left operand and right operand\n        # TODO: Order matters for tree traversal - left first, then right\n        # Hint: return [self.left, self.right]\n        pass\n    \n    def accept(self, visitor: ASTVisitor) -> Any:\n        return visitor.visit_binary_operation(self)\n    \n    def is_comparison(self) -> bool:\n        \"\"\"Check if this operation is a comparison (=, <, >, etc.).\"\"\"\n        # TODO: Return True if operator is any comparison type\n        # TODO: Check against OPERATOR_EQUALS, OPERATOR_LESS_THAN, etc.\n        # Hint: Use operator in {TokenType.OPERATOR_EQUALS, ...}\n        pass\n    \n    def is_arithmetic(self) -> bool:\n        \"\"\"Check if this operation is arithmetic (+, -, *, /).\"\"\"\n        # TODO: Return True if operator is arithmetic\n        # TODO: Check against OPERATOR_PLUS, OPERATOR_MINUS, etc.\n        pass\n    \n    def is_logical(self) -> bool:\n        \"\"\"Check if this operation is logical (AND, OR).\"\"\"\n        # TODO: Return True if operator is KEYWORD_AND or KEYWORD_OR\n        pass\n```\n\n**Language-Specific Python Hints**\n\n- Use `@dataclass(frozen=True)` for immutable AST nodes to prevent accidental modification\n- Leverage `typing.Optional[T]` and `typing.List[T]` for clear type specifications\n- Use `enum.auto()` for TokenType values to avoid manual numbering\n- Implement `__str__` and `__repr__` methods for better debugging experience\n- Consider using `functools.cached_property` for expensive computed properties\n- Use `isinstance()` checks instead of string comparisons for node type testing\n\n**Milestone Checkpoint: Data Model Validation**\n\nAfter implementing the data structures above, validate your implementation with these tests:\n\n```python\n# Test basic token creation and properties\ntoken = Token(TokenType.KEYWORD_SELECT, \"SELECT\", 1, 1)\nassert token.is_keyword() == True\nassert token.is_operator() == False\n\n# Test source location spanning\nstart = Token(TokenType.IDENTIFIER, \"users\", 1, 8)\nend = Token(TokenType.PUNCTUATION_SEMICOLON, \";\", 1, 25)\nlocation = SourceLocation.span_tokens(start, end)\nassert location.start_column == 8\nassert location.end_column == 26\n\n# Test AST node creation and relationships\ntable_id = Identifier(\"users\")\ncolumn_id = Identifier(\"name\", table_qualifier=\"users\")\nselect_clause = SelectClause([column_id])  # You'll implement SelectClause\nfrom_clause = FromClause(table_id)         # You'll implement FromClause\nselect_stmt = SelectStatement(select_clause, from_clause)\n\nassert len(select_stmt.children()) == 2\nassert select_stmt.node_type() == \"SelectStatement\"\n```\n\n**Debugging Tips for Data Model Issues**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| `AttributeError` on AST node | Missing field initialization | Check `__init__` method, print `self.__dict__` | Add missing field assignments in constructor |\n| Incorrect tree traversal | Wrong `children()` implementation | Print node.children() for each node type | Return all child nodes in correct order |\n| Position information missing | Not propagating SourceLocation | Check if location passed to nodes during parsing | Copy location from tokens during AST construction |\n| Token type confusion | Wrong TokenType enum values | Print token.type for each token | Verify SQL_KEYWORDS mapping and tokenizer logic |\n| Immutability violations | Modifying AST after creation | Use `@dataclass(frozen=True)` or check for assignments | Make all fields read-only properties |\n\n\n## Tokenizer Component Design\n\n> **Milestone(s):** Milestone 1 (SQL Tokenizer) - This section provides the detailed design for the lexical analysis component that converts SQL text into classified tokens, forming the foundation for all subsequent parsing operations.\n\n### Mental Model: Reading Word by Word\n\nUnderstanding tokenization becomes intuitive when we think about how humans read written language. When you encounter the sentence \"The quick brown fox jumps over the lazy dog,\" your brain doesn't process individual letters in isolation. Instead, it automatically groups characters into meaningful units—words—and then classifies each word by its role: articles, adjectives, nouns, verbs. Your mental lexicon helps you recognize \"quick\" as an adjective and \"jumps\" as a verb, even before you understand the complete sentence structure.\n\nSQL tokenization follows this exact same pattern, but with database query language instead of natural language. When our tokenizer encounters the SQL text `SELECT name FROM users WHERE age > 25`, it performs the same word-by-word recognition process. It groups the characters 'S', 'E', 'L', 'E', 'C', 'T' into the unit \"SELECT\" and immediately recognizes this as a SQL keyword token. Similarly, it identifies \"name\" as an identifier token, \"FROM\" as another keyword, and \">\" as a comparison operator token.\n\nThe key insight is that tokenization is fundamentally about **pattern recognition and classification**. Just as your brain has learned to distinguish between different word types in English, our tokenizer must learn to distinguish between SQL keywords (`SELECT`, `WHERE`), identifiers (table names, column names), literals (strings, numbers), operators (`=`, `<`, `AND`), and punctuation (commas, parentheses). The tokenizer serves as the \"reading comprehension\" layer that transforms raw character sequences into meaningful linguistic units that the parser can then assemble into complete grammatical structures.\n\nThis analogy also reveals why tokenization must happen before parsing. You cannot understand the grammatical structure of \"The quick brown fox jumps\" until you first recognize that \"quick\" and \"brown\" are adjectives modifying \"fox\" as a noun. Similarly, a SQL parser cannot build an Abstract Syntax Tree for `SELECT name FROM users` until the tokenizer has first identified `SELECT` as a statement keyword, `name` as a column identifier, `FROM` as a table reference keyword, and `users` as a table identifier.\n\n![Token Type Classification](./diagrams/token-types.svg)\n\n### Tokenization Algorithm\n\nThe tokenization algorithm implements a **character-by-character scanning process** that maintains state about the current position in the SQL text and accumulates characters into complete tokens. The algorithm follows the **maximal munch principle**, always consuming the longest possible sequence of characters that forms a valid token. This prevents ambiguities where shorter token matches might incorrectly break up longer valid tokens.\n\nThe core tokenization process operates through these fundamental steps:\n\n1. **Initialize Scanner State**: The tokenizer maintains a current position pointer in the input SQL string, along with line and column counters for error reporting. It also maintains an empty token list that will accumulate the final results. The scanner starts at position zero with line 1, column 1.\n\n2. **Character Classification Loop**: The algorithm enters a main loop that continues until it reaches the end of the input string. At each iteration, it examines the current character and determines what type of token might be starting at this position. This classification drives the decision about which specialized parsing routine to invoke.\n\n3. **Whitespace Skipping**: Before attempting any token recognition, the scanner checks if the current character is whitespace (space, tab, newline, carriage return). If so, it advances the position counter and updates line/column tracking as appropriate. Whitespace serves as token separators but does not generate tokens themselves in our SQL parser.\n\n4. **Token Type Dispatch**: Based on the current character, the scanner dispatches to specialized parsing routines. Alphabetic characters typically start keywords or identifiers, digit characters start numeric literals, quote characters start string literals, and special symbol characters start operators or punctuation tokens.\n\n5. **Maximal Munch Token Building**: Each specialized parsing routine implements maximal munch by consuming characters as long as they can extend the current token. For example, when parsing an identifier that starts with 'u', the routine continues consuming characters through 'users' rather than stopping at just 'u'. This ensures that longer valid tokens take precedence over shorter ones.\n\n6. **Token Classification and Creation**: Once a complete token has been consumed, the parsing routine determines the appropriate `TokenType` for the accumulated characters. Keywords are distinguished from identifiers through dictionary lookup, operators are classified by their symbol patterns, and literals are typed based on their content format.\n\n7. **Position Tracking and Error Context**: Throughout token creation, the scanner maintains precise position information including start and end coordinates. This enables high-quality error messages that can point users to the exact location of tokenization problems.\n\n8. **Token List Accumulation**: Each successfully created token is appended to the growing token list. The scanner then returns to the main character classification loop to process the next token in the input stream.\n\nThe algorithm handles **multi-character operators** like `>=` and `<=` by implementing look-ahead logic. When the scanner encounters a `>` character, it peeks at the next character to determine whether to create a simple `GREATER_THAN` token or a compound `GREATER_THAN_OR_EQUAL` token. This look-ahead prevents the incorrect tokenization of `>=` as separate `>` and `=` tokens.\n\n**Error handling** integrates directly into the tokenization algorithm. When the scanner encounters an unexpected character that cannot start any valid token type, it creates a `TokenizerError` with precise position information and a descriptive message. The error includes the problematic character, its line and column position, and context about what types of tokens were expected at that location.\n\nThe algorithm maintains **line and column counters** by tracking newline characters during scanning. Each time the scanner encounters a `\\n` character, it increments the line counter and resets the column counter to 1. For all other characters, it increments the column counter. This position tracking is essential for providing helpful error messages and debugging information.\n\n### Keyword Recognition Strategy\n\nSQL keyword recognition presents unique challenges because SQL keywords are **case-insensitive** but must be distinguished from regular identifiers that happen to use the same character sequences. The keyword recognition strategy implements a two-phase approach: first identifying potential keywords through pattern matching, then confirming their keyword status through dictionary lookup.\n\nThe **case-insensitive matching** requirement means that `SELECT`, `select`, `Select`, and `SeLeCt` must all be recognized as the same `SELECT` keyword token. However, the tokenizer must preserve the original case of the characters for potential pretty-printing or error message display. This dual requirement drives the design of our keyword recognition algorithm.\n\n**Phase 1: Identifier Pattern Recognition** begins when the scanner encounters an alphabetic character or underscore. The tokenizer enters identifier parsing mode and consumes all subsequent characters that match the SQL identifier character set: letters, digits, and underscores. This phase produces a complete character sequence like \"SELECT\" or \"user_id\" without yet determining whether it represents a keyword or regular identifier.\n\n**Phase 2: Keyword Dictionary Lookup** takes the accumulated character sequence and performs a case-insensitive comparison against the `SQL_KEYWORDS` dictionary. The lookup converts the candidate string to uppercase before dictionary access, ensuring that \"select\" matches the dictionary key \"SELECT\". If a match is found, the tokenizer creates a keyword token with the appropriate `TokenType`. If no match is found, the tokenizer creates an `IDENTIFIER` token instead.\n\nThe `SQL_KEYWORDS` dictionary maps uppercase keyword strings to their corresponding `TokenType` enumeration values. This design provides several advantages: fast lookup performance through hash table access, easy extensibility for additional keywords, and clear separation between keyword recognition logic and keyword definitions. The dictionary structure appears as follows:\n\n| Keyword String | Token Type | Usage Context |\n|---|---|---|\n| SELECT | SELECT_KEYWORD | Beginning of SELECT statements |\n| FROM | FROM_KEYWORD | Table reference clauses |\n| WHERE | WHERE_KEYWORD | Conditional filter clauses |\n| INSERT | INSERT_KEYWORD | Beginning of INSERT statements |\n| UPDATE | UPDATE_KEYWORD | Beginning of UPDATE statements |\n| DELETE | DELETE_KEYWORD | Beginning of DELETE statements |\n| INTO | INTO_KEYWORD | INSERT target specification |\n| VALUES | VALUES_KEYWORD | INSERT value lists |\n| SET | SET_KEYWORD | UPDATE assignment clauses |\n| AND | AND_OPERATOR | Logical conjunction in expressions |\n| OR | OR_OPERATOR | Logical disjunction in expressions |\n| NOT | NOT_OPERATOR | Logical negation in expressions |\n| IS | IS_OPERATOR | NULL comparison operator |\n| NULL | NULL_LITERAL | NULL value literal |\n| AS | AS_KEYWORD | Alias declarations |\n\nThe **context-sensitive keyword** challenge arises because some SQL terms can function as keywords in certain contexts and identifiers in others. For example, \"ORDER\" is a keyword in `ORDER BY` clauses but could theoretically be used as a table name or column name in other contexts. Our tokenizer takes a **context-free approach** and always recognizes \"ORDER\" as a keyword token, leaving context-sensitive validation to the parser layer.\n\n**Reserved word handling** follows SQL standards by treating all recognized keywords as reserved words that cannot be used as regular identifiers. This simplifies parsing logic and prevents ambiguous situations where the parser cannot determine whether a token represents a keyword or identifier based on context alone. Users who need to use reserved words as identifiers must employ quoted identifiers (though our parser does not support quoted identifiers in this implementation).\n\n**Extensibility** for additional keywords requires only adding entries to the `SQL_KEYWORDS` dictionary and defining corresponding `TokenType` enumeration values. This design allows easy extension as new SQL features are added to the parser. The keyword recognition algorithm itself requires no modification to support additional keywords.\n\n### String Literal Parsing\n\nString literal parsing handles the complex process of extracting quoted text values from SQL input while properly processing **escape sequences**, **quote character variations**, and **embedded quotes**. SQL string literals can use either single quotes (`'text'`) or double quotes (`\"text\"`), and the parsing algorithm must handle both quote types while maintaining the semantic differences between them.\n\nThe **quote character detection** phase begins when the tokenizer encounters either a single quote (`'`) or double quote (`\"`) character. The tokenizer records which quote character initiated the string and enters string literal parsing mode. The opening quote character determines both the expected closing quote character and the escape sequence rules that apply within the string content.\n\n**Character accumulation** proceeds character by character from the opening quote until the matching closing quote is found. However, this process must handle several complex cases that make string parsing more sophisticated than simple character collection. The algorithm must distinguish between quote characters that end the string and quote characters that represent literal quote content within the string.\n\n**Escape sequence processing** handles special character representations within string literals. SQL supports several standard escape sequences that allow representing characters that would otherwise be difficult to include in quoted strings:\n\n| Escape Sequence | Literal Character | Usage Example |\n|---|---|---|\n| `\\'` | Single quote | `'Don\\'t stop'` |\n| `\\\"` | Double quote | `\"She said \\\"Hello\\\"\"` |\n| `\\\\` | Backslash | `'C:\\\\Users\\\\data'` |\n| `\\n` | Newline | `'Line 1\\nLine 2'` |\n| `\\t` | Tab character | `'Column1\\tColumn2'` |\n| `\\r` | Carriage return | `'Windows\\r\\nNewline'` |\n\nThe **doubled quote convention** provides an alternative escape mechanism where two consecutive quote characters represent a single literal quote. This convention applies when the doubled quotes match the string's opening quote character. For example, `'Don''t stop'` represents the string value \"Don't stop\" using single-quote doubling, while `\"She said \"\"Hello\"\"\"` represents \"She said \"Hello\"\" using double-quote doubling.\n\n**Algorithm implementation** for string parsing follows these detailed steps:\n\n1. **Quote Detection**: When the scanner encounters a quote character, it records the quote type and advances past the opening quote. The quote type determines the termination condition and escape rules for the remainder of the parsing process.\n\n2. **Content Accumulation Loop**: The algorithm enters a loop that examines each subsequent character. Regular characters are directly appended to the string content. When escape characters or quote characters are encountered, special processing logic is invoked.\n\n3. **Escape Sequence Recognition**: When a backslash character is encountered, the algorithm examines the following character to determine the appropriate escape sequence. Valid escape sequences are converted to their literal character representations. Invalid escape sequences trigger tokenization errors with descriptive messages.\n\n4. **Quote Character Handling**: When a quote character matching the opening quote is encountered, the algorithm must determine whether this represents string termination or an escaped literal quote. If the next character is also a matching quote, this represents an escaped quote via the doubled quote convention. Otherwise, this represents string termination.\n\n5. **Termination and Token Creation**: When a closing quote is successfully identified, the algorithm advances past the closing quote and creates a `STRING_LITERAL` token containing the accumulated string content. The token value contains the processed string content with escape sequences resolved to their literal characters.\n\n**Error conditions** in string parsing include several scenarios that require clear error messages:\n\n- **Unterminated strings** occur when the input ends before a closing quote is found. The error message should indicate the line and column where the string started and note that no closing quote was found.\n- **Invalid escape sequences** occur when a backslash is followed by a character that does not form a valid escape sequence. The error should identify the invalid sequence and list the valid escape sequences.\n- **Embedded newlines** in string literals may or may not be supported depending on SQL dialect. Our parser treats unescaped newlines within strings as errors and suggests using `\\n` escape sequences instead.\n\n### Architecture Decision: State Machine vs Character-by-Character\n\nThe tokenizer implementation approach represents a fundamental architectural decision that affects code complexity, performance characteristics, extensibility, and debugging ease. Two primary approaches were considered: a **formal state machine** implementation and a **character-by-character scanning** approach with method dispatch.\n\n> **Decision: Character-by-Character Scanning with Method Dispatch**\n> - **Context**: The tokenizer needs to handle multiple token types (keywords, identifiers, numbers, strings, operators) with different parsing rules and varying levels of complexity. The approach must be understandable to intermediate developers while remaining extensible for future SQL features.\n> - **Options Considered**: \n>   1. Formal finite state machine with explicit state transitions\n>   2. Character-by-character scanning with specialized parsing methods\n>   3. Regular expression-based tokenization\n> - **Decision**: Character-by-character scanning with method dispatch to specialized parsing functions\n> - **Rationale**: This approach provides the best balance of simplicity and power for our educational context. It allows each token type to have dedicated parsing logic without the complexity of managing explicit state transitions. The code structure directly mirrors the conceptual understanding of tokenization, making it easier for learners to understand and extend.\n> - **Consequences**: Trade-off of some theoretical elegance and performance optimization potential in exchange for significantly improved code readability and maintainability. Each token type parsing can be understood in isolation.\n\n| Approach | Pros | Cons | Chosen? |\n|---|---|---|---|\n| **Formal State Machine** | Theoretically elegant, potentially faster, handles ambiguous cases well | Complex state transition management, harder to debug, steeper learning curve | No |\n| **Character-by-Character + Dispatch** | Clear separation of concerns, easy to understand and extend, straightforward debugging | Slightly more method call overhead, less formally rigorous | **Yes** |\n| **Regular Expression Based** | Very concise code, leverages proven regex engines | Poor error messages, harder to customize, regex complexity for SQL grammar | No |\n\nThe **formal state machine approach** would implement tokenization as an explicit finite automaton with defined states (Normal, InIdentifier, InString, InNumber, InOperator) and transition functions. While this approach offers theoretical elegance and can handle complex tokenization scenarios efficiently, it introduces significant complexity in state management and transition logic. The resulting code would be harder for intermediate developers to understand and modify, working against our educational goals.\n\n**Character-by-character scanning with method dispatch** implements tokenization through a main scanning loop that examines the current character and dispatches to specialized parsing methods based on character type. This approach aligns naturally with how developers think about tokenization: \"if I see a letter, parse an identifier; if I see a digit, parse a number; if I see a quote, parse a string.\" Each parsing method can focus entirely on its specific token type without worrying about global state management.\n\nThe **method dispatch strategy** uses character classification to determine the appropriate parsing method:\n\n| Character Type | Dispatch Method | Handles Token Types |\n|---|---|---|\n| Alphabetic or underscore | `_parse_identifier_or_keyword()` | IDENTIFIER, keywords |\n| Digit | `_parse_number()` | INTEGER_LITERAL, FLOAT_LITERAL |\n| Single or double quote | `_parse_string_literal()` | STRING_LITERAL |\n| Operator symbols | `_parse_operator()` | All operator tokens |\n| Punctuation | `_parse_punctuation()` | COMMA, SEMICOLON, etc. |\n\n**Error handling benefits** from the character-by-character approach because each parsing method can provide specific error messages related to its token type. When string parsing fails due to an unterminated quote, the string parsing method can provide a message like \"Unterminated string literal starting at line 5, column 12\" rather than a generic state machine error like \"Unexpected end of input in state InString.\"\n\n**Extensibility advantages** emerge because adding support for new token types requires only implementing a new parsing method and adding its dispatch condition to the main scanning loop. The existing parsing methods require no modification, and the overall tokenizer structure remains unchanged. This modularity supports incremental development and makes the codebase easier to understand for learners.\n\n![Tokenizer State Machine](./diagrams/tokenizer-state-machine.svg)\n\n**Performance considerations** for the character-by-character approach involve slightly higher method call overhead compared to a state machine's direct transitions. However, for the scope of our SQL parser and typical query sizes, this overhead is negligible compared to the benefits in code clarity and maintainability. The tokenization phase typically represents a small fraction of total query processing time.\n\n### Common Tokenizer Pitfalls\n\nTokenizer implementation presents several recurring challenges that can trap intermediate developers. Understanding these pitfalls and their solutions prevents hours of debugging and helps build robust lexical analysis components.\n\n⚠️ **Pitfall: Case Sensitivity Inconsistency**\n\nMany developers initially implement keyword recognition with exact string matching, causing `SELECT` to be recognized as a keyword while `select` is treated as a regular identifier. This creates confusing behavior where syntactically identical queries behave differently based on capitalization.\n\n**Why this fails**: SQL is case-insensitive for keywords by specification, but exact string matching is case-sensitive by default in most programming languages. When the tokenizer uses direct dictionary lookup with mixed-case user input, keywords fail to match their dictionary entries.\n\n**How to fix**: Implement case-insensitive keyword lookup by converting candidate strings to uppercase before dictionary comparison. Store all keyword dictionary keys in uppercase, and normalize input strings to uppercase before lookup. Preserve original case in token values for display purposes.\n\n⚠️ **Pitfall: Incomplete Escape Sequence Handling**\n\nString literal parsing often fails to handle all required escape sequences, particularly the doubled quote convention (`''` within single-quoted strings). Developers frequently implement only backslash escapes, causing parse failures when SQL queries use the doubled quote style.\n\n**Why this fails**: SQL supports two different escape mechanisms for quotes within strings: backslash escapes (`\\'`) and doubled quotes (`''`). Many tokenizer implementations only support one mechanism, causing valid SQL strings to be rejected or incorrectly parsed.\n\n**How to fix**: Implement both escape mechanisms in string parsing logic. When encountering a quote character that matches the opening quote, check if the next character is also a matching quote (doubled convention) before treating it as string termination.\n\n⚠️ **Pitfall: Multi-Character Operator Fragmentation**\n\nTokenizers often incorrectly split multi-character operators like `>=`, `<=`, `<>`, and `!=` into separate single-character tokens. This occurs when the tokenizer processes each character independently without considering longer operator sequences.\n\n**Why this fails**: The parser expects compound operators to arrive as single tokens matching the SQL grammar. When `>=` arrives as separate `>` and `=` tokens, the parser cannot match this sequence to any valid grammar rule, resulting in syntax errors for valid SQL expressions.\n\n**How to fix**: Implement look-ahead logic in operator parsing. When encountering a character that could start a multi-character operator, examine the following character to determine whether to create a compound operator token or a simple single-character token.\n\n⚠️ **Pitfall: Position Tracking Errors**\n\nLine and column position tracking often becomes inaccurate due to incorrect handling of different newline conventions (`\\n`, `\\r\\n`, `\\r`), leading to error messages that point to wrong locations in the source code.\n\n**Why this fails**: Different operating systems use different newline conventions, and SQL text might originate from various sources. If the tokenizer only recognizes one newline style, position tracking becomes inaccurate when processing text with different newline conventions.\n\n**How to fix**: Implement comprehensive newline handling that recognizes all common newline conventions. When advancing past newline characters, increment line counters and reset column counters regardless of the specific newline style encountered.\n\n⚠️ **Pitfall: Whitespace Preservation in Tokens**\n\nToken values sometimes include leading or trailing whitespace characters, particularly when parsing identifiers that are adjacent to whitespace. This causes string comparisons and keyword lookups to fail unexpectedly.\n\n**Why this fails**: If the tokenizer includes whitespace characters in token values, subsequent processing that expects clean token values will fail. For example, a keyword lookup for \" SELECT \" (with spaces) will not match the dictionary entry \"SELECT\".\n\n**How to fix**: Ensure that token parsing methods exclude whitespace from token values. Skip whitespace before beginning token parsing, and stop token accumulation when whitespace is encountered after the token content.\n\n⚠️ **Pitfall: Numeric Literal Type Confusion**\n\nNumber parsing often fails to distinguish between integer and floating-point literals, either treating all numbers as the same type or incorrectly classifying numbers based on incomplete parsing logic.\n\n**Why this fails**: SQL distinguishes between integer literals (123) and floating-point literals (123.45, 1.23e10), and this distinction affects query semantics. If the tokenizer misclassifies numeric literals, the parser may generate incorrect AST nodes or fail to enforce proper type checking.\n\n**How to fix**: Implement comprehensive numeric parsing that detects decimal points and scientific notation to distinguish between integer and floating-point formats. Create appropriate `INTEGER_LITERAL` or `FLOAT_LITERAL` tokens based on the detected format.\n\n⚠️ **Pitfall: Error Recovery Absence**\n\nTokenizers often terminate immediately upon encountering unexpected characters, providing minimal error context and preventing the detection of additional errors elsewhere in the SQL text.\n\n**Why this fails**: Immediate termination on tokenization errors prevents users from seeing all problems in their SQL text at once. This creates a frustrating development experience where users must fix errors one at a time through multiple parse attempts.\n\n**How to fix**: Implement error recovery that creates error tokens for problematic characters while continuing to process the remainder of the input. Collect all tokenization errors and report them together to give users a complete picture of all issues.\n\n### Implementation Guidance\n\nThis subsection provides concrete Python implementation guidance for building the SQL tokenizer component, bridging the gap between the design concepts and working code.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| **Character Processing** | String indexing with manual position tracking | Custom StringScanner class with position management |\n| **Keyword Lookup** | Python dict with uppercase keys | collections.defaultdict with case-insensitive wrapper |\n| **Token Storage** | Python list of Token objects | collections.deque for efficient append operations |\n| **Error Handling** | Custom exception classes with position info | Rich error context with source snippet display |\n| **Testing** | unittest with hand-written test cases | pytest with parameterized tests and property-based testing |\n\nFor this educational implementation, we recommend the **simple options** to maintain focus on core tokenization concepts rather than advanced Python features.\n\n**B. Recommended File Structure**\n\n```\nsql-parser/\n  src/\n    tokenizer/\n      __init__.py              ← Module exports\n      token_types.py           ← TokenType enum and Token class\n      tokenizer.py             ← Main Tokenizer class\n      exceptions.py            ← TokenizerError and related exceptions\n      keywords.py              ← SQL_KEYWORDS dictionary\n    tests/\n      test_tokenizer.py        ← Comprehensive tokenizer tests\n      test_token_types.py      ← Token class tests\n    examples/\n      tokenize_examples.py     ← Usage examples\n  README.md\n```\n\nThis structure separates concerns clearly: token definitions, tokenization logic, error handling, and keyword recognition each live in dedicated modules.\n\n**C. Infrastructure Starter Code (Complete)**\n\nThe following complete infrastructure code handles token definitions, exceptions, and keyword mappings. Learners can use this code as-is and focus on implementing the core tokenization algorithm.\n\n**File: `src/tokenizer/token_types.py`**\n```python\n\"\"\"\nToken type definitions and Token class for SQL parsing.\n\"\"\"\nfrom enum import Enum, auto\nfrom typing import Any\nfrom dataclasses import dataclass\n\n\nclass TokenType(Enum):\n    \"\"\"Enumeration of all SQL token types.\"\"\"\n    \n    # Keywords\n    SELECT_KEYWORD = auto()\n    FROM_KEYWORD = auto()\n    WHERE_KEYWORD = auto()\n    INSERT_KEYWORD = auto()\n    UPDATE_KEYWORD = auto()\n    DELETE_KEYWORD = auto()\n    INTO_KEYWORD = auto()\n    VALUES_KEYWORD = auto()\n    SET_KEYWORD = auto()\n    AS_KEYWORD = auto()\n    \n    # Operators\n    AND_OPERATOR = auto()\n    OR_OPERATOR = auto()\n    NOT_OPERATOR = auto()\n    EQUALS = auto()\n    NOT_EQUALS = auto()\n    LESS_THAN = auto()\n    GREATER_THAN = auto()\n    LESS_THAN_OR_EQUAL = auto()\n    GREATER_THAN_OR_EQUAL = auto()\n    IS_OPERATOR = auto()\n    \n    # Literals\n    STRING_LITERAL = auto()\n    INTEGER_LITERAL = auto()\n    FLOAT_LITERAL = auto()\n    NULL_LITERAL = auto()\n    \n    # Identifiers\n    IDENTIFIER = auto()\n    \n    # Punctuation\n    COMMA = auto()\n    SEMICOLON = auto()\n    LEFT_PAREN = auto()\n    RIGHT_PAREN = auto()\n    STAR = auto()\n    \n    # Special\n    EOF = auto()\n    UNKNOWN = auto()\n\n\n@dataclass(frozen=True)\nclass Token:\n    \"\"\"\n    Represents a single token from SQL lexical analysis.\n    \n    Attributes:\n        type: The classification of this token\n        value: The original text content\n        line: Line number (1-based) where token begins\n        column: Column number (1-based) where token begins\n    \"\"\"\n    type: TokenType\n    value: str\n    line: int\n    column: int\n    \n    def is_keyword(self) -> bool:\n        \"\"\"Returns True if this token is a SQL keyword.\"\"\"\n        return self.type.name.endswith('_KEYWORD')\n    \n    def is_operator(self) -> bool:\n        \"\"\"Returns True if this token is an operator.\"\"\"\n        return self.type.name.endswith('_OPERATOR') or self.type in {\n            TokenType.EQUALS, TokenType.NOT_EQUALS, TokenType.LESS_THAN,\n            TokenType.GREATER_THAN, TokenType.LESS_THAN_OR_EQUAL,\n            TokenType.GREATER_THAN_OR_EQUAL, TokenType.IS_OPERATOR\n        }\n    \n    def is_literal(self) -> bool:\n        \"\"\"Returns True if this token is a literal value.\"\"\"\n        return self.type.name.endswith('_LITERAL')\n    \n    def __str__(self) -> str:\n        return f\"{self.type.name}('{self.value}') at {self.line}:{self.column}\"\n```\n\n**File: `src/tokenizer/exceptions.py`**\n```python\n\"\"\"\nException classes for SQL tokenization errors.\n\"\"\"\n\nclass ParseError(Exception):\n    \"\"\"Base exception class for all parsing errors.\"\"\"\n    \n    def __init__(self, message: str, line: int = None, column: int = None):\n        self.message = message\n        self.line = line\n        self.column = column\n        \n        if line is not None and column is not None:\n            full_message = f\"Line {line}, Column {column}: {message}\"\n        else:\n            full_message = message\n            \n        super().__init__(full_message)\n\n\nclass TokenizerError(ParseError):\n    \"\"\"Exception raised during tokenization/lexical analysis.\"\"\"\n    pass\n\n\nclass SyntaxError(ParseError):\n    \"\"\"Exception raised during parsing.\"\"\"\n    pass\n\n\nclass UnexpectedTokenError(SyntaxError):\n    \"\"\"Exception raised when parser encounters unexpected token.\"\"\"\n    \n    def __init__(self, expected: str, actual_token, message: str = None):\n        self.expected = expected\n        self.actual_token = actual_token\n        \n        if message is None:\n            message = f\"Expected {expected}, but found {actual_token.type.name}\"\n        \n        super().__init__(message, actual_token.line, actual_token.column)\n```\n\n**File: `src/tokenizer/keywords.py`**\n```python\n\"\"\"\nSQL keyword definitions and lookup utilities.\n\"\"\"\nfrom .token_types import TokenType\n\n# Mapping from uppercase keyword strings to their TokenType values\nSQL_KEYWORDS = {\n    'SELECT': TokenType.SELECT_KEYWORD,\n    'FROM': TokenType.FROM_KEYWORD,\n    'WHERE': TokenType.WHERE_KEYWORD,\n    'INSERT': TokenType.INSERT_KEYWORD,\n    'UPDATE': TokenType.UPDATE_KEYWORD,\n    'DELETE': TokenType.DELETE_KEYWORD,\n    'INTO': TokenType.INTO_KEYWORD,\n    'VALUES': TokenType.VALUES_KEYWORD,\n    'SET': TokenType.SET_KEYWORD,\n    'AS': TokenType.AS_KEYWORD,\n    'AND': TokenType.AND_OPERATOR,\n    'OR': TokenType.OR_OPERATOR,\n    'NOT': TokenType.NOT_OPERATOR,\n    'IS': TokenType.IS_OPERATOR,\n    'NULL': TokenType.NULL_LITERAL,\n}\n\ndef lookup_keyword(text: str) -> TokenType:\n    \"\"\"\n    Look up a token type for potential keyword text.\n    \n    Args:\n        text: The text to check (case-insensitive)\n        \n    Returns:\n        TokenType.IDENTIFIER if not a keyword, otherwise the keyword TokenType\n    \"\"\"\n    return SQL_KEYWORDS.get(text.upper(), TokenType.IDENTIFIER)\n```\n\n**D. Core Logic Skeleton Code**\n\nThe following skeleton provides the main Tokenizer class structure with detailed TODO comments that map to the algorithm steps described in the design section.\n\n**File: `src/tokenizer/tokenizer.py`**\n```python\n\"\"\"\nMain SQL tokenizer implementation.\n\"\"\"\nfrom typing import List, Optional\nfrom .token_types import Token, TokenType\nfrom .exceptions import TokenizerError\nfrom .keywords import lookup_keyword\n\n\nclass Tokenizer:\n    \"\"\"\n    SQL tokenizer that converts SQL text into a list of classified tokens.\n    \n    The tokenizer uses character-by-character scanning with method dispatch\n    to specialized parsing functions for different token types.\n    \"\"\"\n    \n    def __init__(self, sql_text: str):\n        \"\"\"\n        Initialize tokenizer with SQL text to process.\n        \n        Args:\n            sql_text: The SQL query string to tokenize\n        \"\"\"\n        self.sql_text = sql_text\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens = []\n    \n    def tokenize(self) -> List[Token]:\n        \"\"\"\n        Main entry point that tokenizes the SQL text and returns token list.\n        \n        Returns:\n            List of Token objects representing the tokenized SQL\n            \n        Raises:\n            TokenizerError: If tokenization fails due to invalid input\n        \"\"\"\n        # TODO 1: Reset tokenizer state (position, line, column, tokens list)\n        # TODO 2: Enter main scanning loop while not at end of input\n        # TODO 3: Skip whitespace characters and update position tracking\n        # TODO 4: Check if at end of input, break if so\n        # TODO 5: Dispatch to appropriate token parsing method based on current character\n        # TODO 6: If no parsing method handles current character, raise TokenizerError\n        # TODO 7: Add EOF token to end of token list\n        # TODO 8: Return completed token list\n        # Hint: Use _current_char() to get current character, None if at end\n        # Hint: Dispatch based on character type - letters->identifier, digits->number, etc.\n        pass\n    \n    def _current_char(self) -> Optional[str]:\n        \"\"\"\n        Returns the current character or None if at end of input.\n        \n        Returns:\n            Current character or None if position >= len(sql_text)\n        \"\"\"\n        # TODO 1: Check if position is beyond end of input string\n        # TODO 2: Return None if at end, otherwise return character at current position\n        pass\n    \n    def _peek_char(self, offset: int = 1) -> Optional[str]:\n        \"\"\"\n        Look ahead at future characters without advancing position.\n        \n        Args:\n            offset: How many characters ahead to look (default 1)\n            \n        Returns:\n            Character at position + offset, or None if beyond end\n        \"\"\"\n        # TODO 1: Calculate target position as current position + offset\n        # TODO 2: Return None if target position is beyond input length\n        # TODO 3: Return character at target position\n        pass\n    \n    def _advance(self) -> None:\n        \"\"\"\n        Move to the next character and update position tracking.\n        \"\"\"\n        # TODO 1: Check if current character is newline (\\n)\n        # TODO 2: If newline: increment line counter, reset column to 1\n        # TODO 3: If not newline: increment column counter\n        # TODO 4: Increment position counter\n        # Hint: Handle \\r\\n sequences correctly for Windows line endings\n        pass\n    \n    def _skip_whitespace(self) -> None:\n        \"\"\"\n        Advance past all whitespace characters at current position.\n        \"\"\"\n        # TODO 1: Loop while current character exists and is whitespace\n        # TODO 2: Call _advance() to move past each whitespace character\n        # Hint: Use char.isspace() to check for whitespace\n        pass\n    \n    def _create_token(self, token_type: TokenType, value: str, \n                     start_line: int, start_column: int) -> Token:\n        \"\"\"\n        Create a new token and add it to the token list.\n        \n        Args:\n            token_type: The type classification for this token\n            value: The original text content\n            start_line: Line where token begins\n            start_column: Column where token begins\n            \n        Returns:\n            The created Token object\n        \"\"\"\n        token = Token(token_type, value, start_line, start_column)\n        self.tokens.append(token)\n        return token\n    \n    def _parse_identifier_or_keyword(self) -> Token:\n        \"\"\"\n        Parse an identifier or keyword token starting at current position.\n        \n        Returns:\n            Token with IDENTIFIER type or specific keyword type\n        \"\"\"\n        start_line, start_column = self.line, self.column\n        value = \"\"\n        \n        # TODO 1: Loop while current character is letter, digit, or underscore\n        # TODO 2: Accumulate characters into value string\n        # TODO 3: Advance position after adding each character\n        # TODO 4: Use lookup_keyword() to determine if this is a keyword\n        # TODO 5: Create token with appropriate type (IDENTIFIER or keyword type)\n        # TODO 6: Return the created token\n        # Hint: First character must be letter or underscore, subsequent can include digits\n        pass\n    \n    def _parse_number(self) -> Token:\n        \"\"\"\n        Parse a numeric literal (integer or float) starting at current position.\n        \n        Returns:\n            Token with INTEGER_LITERAL or FLOAT_LITERAL type\n        \"\"\"\n        start_line, start_column = self.line, self.column\n        value = \"\"\n        is_float = False\n        \n        # TODO 1: Loop while current character is digit\n        # TODO 2: Accumulate digits into value string, advance position\n        # TODO 3: Check if next character is decimal point\n        # TODO 4: If decimal point found, set is_float=True, add to value, advance\n        # TODO 5: Continue accumulating digits after decimal point\n        # TODO 6: Determine token type based on is_float flag\n        # TODO 7: Create and return appropriate numeric literal token\n        # Hint: Don't consume decimal point if not followed by digit\n        pass\n    \n    def _parse_string_literal(self, quote_char: str) -> Token:\n        \"\"\"\n        Parse a string literal enclosed in quotes.\n        \n        Args:\n            quote_char: The opening quote character (' or \")\n            \n        Returns:\n            Token with STRING_LITERAL type\n            \n        Raises:\n            TokenizerError: If string is unterminated or has invalid escapes\n        \"\"\"\n        start_line, start_column = self.line, self.column\n        value = \"\"\n        \n        # TODO 1: Advance past opening quote character\n        # TODO 2: Loop until closing quote found or end of input\n        # TODO 3: Handle escape sequences (backslash escapes)\n        # TODO 4: Handle doubled quote escapes ('' within single quotes)\n        # TODO 5: Accumulate processed characters (with escapes resolved)\n        # TODO 6: Check for unterminated string error condition\n        # TODO 7: Advance past closing quote\n        # TODO 8: Create and return STRING_LITERAL token\n        # Hint: Process \\n, \\t, \\', \\\" escape sequences\n        # Hint: Handle quote_char + quote_char as single literal quote\n        pass\n    \n    def _parse_operator(self) -> Token:\n        \"\"\"\n        Parse an operator token starting at current position.\n        \n        Returns:\n            Token with appropriate operator type\n        \"\"\"\n        start_line, start_column = self.line, self.column\n        char = self._current_char()\n        \n        # TODO 1: Handle single-character operators: =, <, >, !, (, ), *, comma, semicolon\n        # TODO 2: Handle multi-character operators: <=, >=, <>, !=\n        # TODO 3: Use _peek_char() to look ahead for multi-character sequences\n        # TODO 4: Advance position appropriately (1 char for single, 2 for multi)\n        # TODO 5: Create token with correct operator TokenType\n        # TODO 6: Return the created token\n        # Hint: Check for >= before >, <= before <, != and <> for not-equals\n        pass\n```\n\n**E. Language-Specific Python Hints**\n\n- **String methods**: Use `char.isalpha()`, `char.isdigit()`, `char.isspace()` for character classification\n- **String building**: Accumulate token characters with `value += char` or use `list` and `''.join()`  for better performance\n- **Dictionary lookup**: `SQL_KEYWORDS.get(text.upper(), TokenType.IDENTIFIER)` handles case-insensitive keyword lookup with default\n- **Exception handling**: Create TokenizerError with precise position: `TokenizerError(f\"Unexpected character '{char}'\", self.line, self.column)`\n- **Enum comparison**: Compare TokenType values directly: `if token_type == TokenType.SELECT_KEYWORD:`\n- **Optional handling**: Check `if self._current_char() is not None:` before character operations\n\n**F. Milestone Checkpoint**\n\nAfter implementing the tokenizer, verify correct operation with these checkpoints:\n\n**Test Command**: \n```bash\npython -m pytest tests/test_tokenizer.py -v\n```\n\n**Expected Behavior**: \nRun this manual test to verify tokenizer functionality:\n\n```python\nfrom src.tokenizer.tokenizer import Tokenizer\n\n# Test basic SELECT query\nsql = \"SELECT name FROM users WHERE id = 42\"\ntokenizer = Tokenizer(sql)\ntokens = tokenizer.tokenize()\n\n# Should produce these tokens:\nexpected_types = [\n    TokenType.SELECT_KEYWORD, TokenType.IDENTIFIER, TokenType.FROM_KEYWORD,\n    TokenType.IDENTIFIER, TokenType.WHERE_KEYWORD, TokenType.IDENTIFIER,\n    TokenType.EQUALS, TokenType.INTEGER_LITERAL, TokenType.EOF\n]\n\nprint(\"Tokenization successful!\" if len(tokens) == len(expected_types) else \"Token count mismatch\")\n```\n\n**Signs of Success**:\n- Keywords recognized regardless of case: `SELECT`, `select`, `Select` all produce `SELECT_KEYWORD`\n- String literals parse correctly: `'John''s Data'` produces value `John's Data`\n- Multi-character operators work: `>=` produces `GREATER_THAN_OR_EQUAL`, not separate `>` and `=`\n- Position tracking accurate: Error messages point to correct line and column\n- All test cases pass without assertion errors\n\n**Common Issues and Fixes**:\n| Symptom | Likely Cause | How to Fix |\n|---|---|---|\n| Keywords treated as identifiers | Case-sensitive keyword lookup | Use `text.upper()` before dictionary lookup |\n| String parsing fails on escaped quotes | Incomplete escape handling | Implement both backslash and doubled quote escapes |\n| Wrong token positions in errors | Incorrect line/column tracking | Handle newlines properly in `_advance()` method |\n| `>=` tokenized as separate `>` and `=` | No look-ahead for multi-char operators | Use `_peek_char()` in operator parsing |\n| Tests fail with \"unexpected character\" | Character dispatch missing cases | Add dispatch cases for all valid starting characters |\n\n\n## SELECT Parser Component Design\n\n> **Milestone(s):** Milestone 2 (SELECT Parser) - This section provides the detailed design for the recursive descent parser that transforms tokens into Abstract Syntax Trees for SELECT statements.\n\nThe SELECT parser represents the heart of our SQL parser, transforming a flat sequence of tokens into a meaningful hierarchical structure that captures the semantic intent of the query. Unlike the tokenizer which operates character-by-character in a linear fashion, the parser must understand the grammatical relationships between tokens and construct a tree that reflects SQL's complex syntax rules. This component bridges the gap between lexical analysis and semantic understanding, creating the foundation for query execution engines to interpret and process SQL statements.\n\n### Mental Model: Grammar Rules as Functions\n\nThink of parsing a SELECT statement like teaching someone to understand the structure of English sentences. When we read \"The quick brown fox jumps over the lazy dog,\" we instinctively recognize patterns: \"The quick brown fox\" is the subject (a noun phrase), \"jumps over\" is the verb phrase, and \"the lazy dog\" is the object. We learned these patterns as grammar rules in school - a sentence consists of a subject and predicate, a noun phrase can have adjectives, and so on.\n\nRecursive descent parsing works exactly like this grammatical analysis, but formalized into code. Each grammar rule becomes a function that knows how to recognize and process its specific pattern. Just as \"noun phrase\" might call \"adjective\" and \"noun\" sub-rules, our `parse_select_statement()` function calls `parse_column_list()`, `parse_from_clause()`, and `parse_where_clause()` functions. Each function is responsible for understanding its piece of the grammar puzzle and returning a structured representation of what it found.\n\nThe \"recursive\" part comes into play when rules reference themselves or each other. In English, we can have sentences within sentences (\"The dog that bit the cat ran away\"). In SQL, we can have expressions within expressions (`(age > 18) AND (status = 'active')`). The parser handles this by having functions call other functions, building up a tree of understanding just like we mentally parse complex sentences by breaking them into familiar patterns.\n\nThe key insight is that each parsing function has a single, focused responsibility: recognize its specific grammar pattern, consume the appropriate tokens, and build the correct AST node. When a function encounters something outside its domain (like `parse_column_list()` seeing a `WHERE_KEYWORD`), it simply stops and lets the calling function handle the next piece. This creates a natural, modular way to handle SQL's complex but structured syntax.\n\n### SELECT Statement Grammar Rules\n\nOur SELECT parser must understand the formal grammar rules that define valid SELECT statements. We express these rules in a BNF-like notation that directly translates to our recursive descent parsing functions. Each rule defines what tokens and sub-rules can appear in what order, along with which elements are required versus optional.\n\nThe core SELECT statement grammar follows this hierarchical structure:\n\n| Rule Name | Grammar Definition | Required Elements | Optional Elements |\n|-----------|-------------------|-------------------|-------------------|\n| `select_statement` | `SELECT column_list FROM table_reference [WHERE expression]` | SELECT keyword, column_list, FROM keyword, table_reference | WHERE clause |\n| `column_list` | `column_spec [, column_spec]*` or `*` | At least one column_spec or star | Additional comma-separated columns |\n| `column_spec` | `[table_name .] column_name [AS alias]` | column_name | table qualifier, alias |\n| `table_reference` | `table_name [AS alias]` | table_name | alias (with or without AS) |\n| `expression` | `comparison_expr [(AND\\|OR) expression]*` | At least one comparison | Additional logical operators |\n| `comparison_expr` | `column_spec (= \\| != \\| < \\| > \\| <= \\| >=) literal` | Left operand, operator, right operand | Parentheses for grouping |\n\nThese grammar rules establish the parsing precedence and structure. The `select_statement` rule is our top-level entry point - it must start with a `SELECT_KEYWORD`, followed by a column list, then a `FROM_KEYWORD`, then a table reference, and optionally a WHERE clause. Each sub-rule like `column_list` defines its own internal structure and delegates to further sub-rules as needed.\n\nThe grammar handles several important SQL features that make parsing challenging. Column specifications can be qualified with table names (`users.name`) or unqualified (`name`), and can have aliases either with the `AS` keyword (`name AS full_name`) or without (`name full_name`). Table references follow similar aliasing rules. The star wildcard (`SELECT *`) represents a special case in the column list that our parser must recognize and handle differently from explicit column names.\n\nExpression parsing within WHERE clauses introduces additional complexity through operator precedence. While we show a simplified version here, the full expression grammar must handle comparison operators, logical operators (`AND`, `OR`, `NOT`), parentheses for precedence override, and various literal types (strings, integers, floats, NULL). Each level of precedence becomes its own grammar rule, creating a hierarchy that naturally enforces correct parsing order.\n\n### Recursive Descent Algorithm\n\nThe recursive descent algorithm transforms each grammar rule into a dedicated parsing function that follows a consistent pattern: check for expected tokens, consume them if found, recursively call sub-parsers for complex elements, build and return the appropriate AST node. This approach creates a direct correspondence between our grammar rules and code structure, making the parser both intuitive to understand and straightforward to extend.\n\nEvery parsing function in our recursive descent parser follows the same fundamental algorithm steps:\n\n1. **Token Validation**: Check that the current token matches what this grammar rule expects. If parsing a SELECT statement, verify the current token is `SELECT_KEYWORD`. If not, raise an `UnexpectedTokenError` with details about what was expected versus what was found.\n\n2. **Token Consumption**: Advance the parser position to consume the expected token. This moves the parser forward through the token stream and positions it for the next parsing step. Track the consumed token's position information for error reporting and AST node location data.\n\n3. **Sub-rule Delegation**: For each complex element in the grammar rule, call the appropriate parsing function. When parsing a SELECT statement, call `parse_column_list()` for the column specification, `parse_from_clause()` for the table reference, and conditionally `parse_where_clause()` if a WHERE keyword is present.\n\n4. **AST Node Construction**: Create the appropriate AST node type with all parsed child elements. For a SELECT statement, instantiate a `SelectStatement` object with the parsed column list, table reference, and optional WHERE expression as child nodes.\n\n5. **Error Recovery**: If parsing fails at any step, generate descriptive error messages that include the current position, what was expected, and what was actually found. Advanced parsers attempt to recover and continue parsing to find additional errors, but our implementation focuses on clear error reporting for the first issue encountered.\n\n6. **Return Value**: Return the constructed AST node to the calling function. This allows parent parsing functions to incorporate the parsed element into their own AST nodes, building up the complete tree structure.\n\nThe recursive nature emerges when parsing functions call each other to handle sub-elements. When `parse_select_statement()` calls `parse_column_list()`, and `parse_column_list()` calls `parse_column_spec()` for each column, we're building a call stack that mirrors the hierarchical structure of the SQL statement. Each function focuses solely on its grammar rule, trusting that called functions will correctly handle their responsibilities.\n\nThis algorithm handles optional elements through conditional parsing. When a grammar rule has optional components (like the WHERE clause in SELECT statements), the parsing function checks if the current token matches the optional element's start token. If so, it parses the optional element; if not, it leaves that element as null in the AST node and continues with required elements.\n\n![AST Node Type Hierarchy](./diagrams/ast-node-hierarchy.svg)\n\n### Column List Parsing\n\nColumn list parsing represents one of the more complex aspects of SELECT statement processing because it must handle multiple syntactic variations while building a consistent AST representation. The parser must recognize star wildcards (`SELECT *`), qualified column names (`SELECT users.name, orders.total`), aliases with and without the AS keyword (`SELECT name AS full_name, total order_amount`), and complex expressions within column specifications.\n\nThe column list parsing algorithm begins by checking for the special case of a star wildcard. If the current token is an `ASTERISK` token, the parser creates a special `StarExpression` AST node that represents the \"select all columns\" semantic. This distinguishes between explicit column lists and wildcard selection at the AST level, allowing query execution engines to handle each case appropriately. The star wildcard cannot be combined with explicit column names in standard SQL, so finding an asterisk ends column list parsing immediately.\n\nFor explicit column lists, the parser enters a comma-separated parsing loop that continues until it encounters a token that cannot be part of a column specification (typically the `FROM_KEYWORD`). Each iteration of this loop calls `parse_column_spec()` to handle individual column elements, then checks for a `COMMA` token to determine whether to continue parsing additional columns. The parser must be careful to distinguish between commas that separate columns and other commas that might appear in expressions or function calls.\n\nIndividual column specification parsing handles the most complex aspects of column list syntax. Each column spec can begin with either a simple identifier (`name`) or a qualified identifier (`users.name`). The parser uses lookahead to distinguish these cases - if an identifier is followed by a `DOT` token, it treats the identifier as a table qualifier and expects another identifier for the actual column name. This creates either an `Identifier` AST node (for simple columns) or a `QualifiedIdentifier` node (for table.column references).\n\nAlias parsing adds another layer of complexity because SQL supports both explicit aliases (using the `AS` keyword) and implicit aliases (where the alias immediately follows the column specification). The parser checks for an `AS_KEYWORD` token after each column specification, and if found, expects an identifier token for the alias name. However, it must also handle implicit aliases where an identifier token follows the column specification without an intervening AS keyword. This requires careful lookahead to avoid conflicting with subsequent clauses.\n\n| Column List Pattern | Example | AST Node Structure | Parsing Complexity |\n|-------------------|---------|-------------------|-------------------|\n| Star wildcard | `SELECT *` | `StarExpression()` | Simple - single token |\n| Simple column | `SELECT name` | `Identifier(\"name\")` | Simple - single identifier |\n| Qualified column | `SELECT users.name` | `QualifiedIdentifier(\"users\", \"name\")` | Medium - requires DOT lookahead |\n| Explicit alias | `SELECT name AS full_name` | `AliasedExpression(Identifier(\"name\"), \"full_name\")` | Medium - AS keyword detection |\n| Implicit alias | `SELECT name full_name` | `AliasedExpression(Identifier(\"name\"), \"full_name\")` | High - requires FROM lookahead |\n| Mixed list | `SELECT id, users.name AS full_name, status` | Array of mixed AST nodes | High - combines all patterns |\n\n### Table Reference and Alias Parsing\n\nTable reference parsing in the FROM clause handles the foundation of SQL query processing by identifying the data sources and their optional aliases. While conceptually simpler than column list parsing, table reference parsing must still handle qualified table names (for databases that support schema prefixes), implicit and explicit aliasing, and proper error detection when table names are malformed or missing.\n\nThe FROM clause parsing begins with mandatory keyword recognition. After the SELECT statement parser identifies the column list, it expects to find a `FROM_KEYWORD` token. If this token is missing, the parser generates an `UnexpectedTokenError` indicating that a FROM clause is required. SQL syntax demands that every SELECT statement specify its data source, making the FROM clause non-optional in our grammar.\n\nTable name parsing follows similar patterns to column name parsing but with different semantic meaning. The parser expects an `IDENTIFIER` token that represents the table name, creating an `Identifier` AST node to hold the table reference. For databases that support schema qualification, the parser uses lookahead to detect `schema.table` patterns, creating a `QualifiedIdentifier` node when a DOT token follows the first identifier. This allows the AST to preserve schema information for query execution engines that need to resolve table references across multiple schemas.\n\nTable aliasing provides a mechanism for referencing tables with shorter names or disambiguating multiple references to the same table (in self-joins or subqueries). The parser handles both explicit aliases (`FROM users AS u`) and implicit aliases (`FROM users u`) using the same lookahead strategy as column aliases. However, table alias parsing has a simpler termination condition because the FROM clause is typically followed by easily recognizable keywords like WHERE, ORDER BY, or statement end.\n\nThe table reference parser must also handle error cases gracefully. Common errors include missing table names after the FROM keyword, invalid characters in table identifiers, and malformed schema qualifiers. Each error case generates specific error messages that help developers identify and fix syntax issues. The parser tracks position information throughout the process to provide accurate error locations.\n\n> **Design Insight**: Table aliases become crucial for self-joins and complex queries, even though our initial parser only handles simple single-table SELECT statements. By building alias support into the fundamental table reference parsing, we create a foundation that easily extends to support multi-table queries in future parser versions.\n\n### Architecture Decision: Look-ahead Strategy\n\nOne of the most critical architectural decisions in recursive descent parser design involves determining how much token look-ahead the parser needs to make correct parsing decisions. This decision affects both parser complexity and the types of SQL syntax the parser can handle unambiguously. Our parser must balance implementation simplicity with the ability to correctly distinguish between syntactically similar constructs.\n\n> **Decision: Single Token Look-ahead with Backtrack-free Parsing**\n> - **Context**: SQL syntax contains several ambiguous constructs where the parser cannot determine the correct interpretation without examining future tokens. Column aliases without AS keywords (`SELECT name full_name`) look identical to column lists (`SELECT name, full_name`) until the parser sees the next token. Similarly, qualified identifiers (`table.column`) cannot be distinguished from separate identifiers until the DOT token is examined.\n> - **Options Considered**:\n>   1. **No look-ahead**: Make parsing decisions based solely on the current token, requiring strict syntax rules that eliminate ambiguity\n>   2. **Single token look-ahead**: Examine the next token when needed to resolve ambiguous constructs, allowing more flexible SQL syntax\n>   3. **Unlimited look-ahead with backtracking**: Allow the parser to examine arbitrarily many future tokens and backtrack when parsing decisions prove incorrect\n> - **Decision**: Single token look-ahead with carefully designed grammar rules that avoid backtracking requirements\n> - **Rationale**: Single token look-ahead provides sufficient power to handle all standard SQL constructs in our scope while maintaining parser simplicity and performance predictability. Most SQL ambiguities resolve within one token of context. Unlimited look-ahead adds significant complexity for minimal benefit given our parser's scope, while no look-ahead forces overly restrictive syntax rules that don't match standard SQL expectations.\n> - **Consequences**: The parser can handle standard SQL syntax naturally while maintaining O(n) linear time complexity. However, some exotic SQL constructs that require deeper look-ahead may need grammar modifications or may be unsupported. Parser functions must be carefully designed to avoid look-ahead conflicts.\n\n| Look-ahead Strategy | Parsing Power | Implementation Complexity | Performance | Memory Usage |\n|-------------------|---------------|--------------------------|-------------|--------------|\n| No look-ahead | Limited - requires restrictive grammar | Very Low | Excellent - O(n) | Minimal |\n| Single token | High - handles most SQL constructs | Low-Medium | Excellent - O(n) | Low |\n| Unlimited + backtrack | Maximum - any context-free grammar | High | Variable - O(n) to O(n²) | High |\n\nThe single token look-ahead strategy requires careful design of parsing functions to use look-ahead consistently and efficiently. Our parser implements look-ahead through a `peek_token()` method that examines the next token without consuming it, allowing parsing functions to make informed decisions about syntax disambiguation. This approach avoids the complexity and performance overhead of backtracking while providing sufficient parsing power for standard SQL constructs.\n\nLook-ahead usage follows specific patterns throughout the parser. When parsing column specifications, the parser uses `peek_token()` to distinguish between qualified identifiers (next token is DOT) and simple identifiers (next token is not DOT). When handling aliases, it looks ahead to distinguish between explicit column separators (next token is COMMA or FROM keyword) and implicit aliases (next token is an identifier). These patterns create predictable look-ahead usage that doesn't cascade into complex decision trees.\n\n### Common SELECT Parser Pitfalls\n\nImplementing a recursive descent parser for SELECT statements involves several subtle challenges that frequently trip up developers. These pitfalls often stem from the mismatch between SQL's flexible syntax and the structured approach required by recursive descent parsing. Understanding these common mistakes and their solutions helps developers build robust parsers that handle real-world SQL correctly.\n\n⚠️ **Pitfall: Token Consumption Tracking Errors**\n\nOne of the most frequent mistakes involves inconsistent token consumption - either forgetting to advance the parser position after recognizing expected tokens, or accidentally consuming tokens multiple times. This creates parsing errors where the parser becomes out of sync with the token stream, leading to unexpected token errors or infinite parsing loops.\n\nThe root cause typically lies in parsing functions that check for expected tokens but forget to call the token advance method, or functions that advance the parser position speculatively and fail to handle backtracking correctly. For example, a column list parser might check for a COMMA token to continue parsing additional columns, but forget to consume the comma before calling `parse_column_spec()` for the next column.\n\nTo avoid this pitfall, establish consistent token handling patterns throughout all parsing functions. Every token recognition should immediately be followed by token consumption. Use helper methods like `expect_token(token_type)` that both verify the expected token type and advance the parser position atomically. This prevents the separation between token checking and token consumption that leads to synchronization errors.\n\n⚠️ **Pitfall: Incomplete AST Node Construction**\n\nAnother common mistake involves creating AST nodes with missing or incorrect child node assignments. This typically happens when parsing functions successfully parse all required elements but fail to properly attach parsed sub-elements to the parent AST node, or when optional elements are handled inconsistently.\n\nFor example, a SELECT statement parser might successfully parse the column list, FROM clause, and WHERE clause, but forget to assign the WHERE clause AST node to the `SelectStatement` object's where_clause field. This creates an AST that loses important query information, even though parsing completed without syntax errors.\n\nThe solution involves creating comprehensive AST node constructors that require all necessary child nodes as parameters, making it impossible to create incomplete AST nodes. Additionally, use builder patterns for complex AST nodes with optional elements, ensuring that all parsed elements are properly incorporated into the final node structure.\n\n⚠️ **Pitfall: Look-ahead Token Consumption**\n\nA subtle but critical error occurs when parsing functions accidentally consume look-ahead tokens instead of just examining them. This happens when developers use the token advance method instead of the peek method for look-ahead operations, causing the parser to skip tokens and misinterpret subsequent syntax.\n\nFor instance, when parsing column aliases, a function might advance the parser position to check if the next token is an identifier (indicating an implicit alias), but forget that advancing the position consumes the token. If the token turns out not to be an alias, the parser has already moved past it and cannot recover the correct parsing position.\n\nPrevent this pitfall by maintaining strict separation between look-ahead operations (which only examine tokens) and parsing operations (which consume tokens). Use clearly named methods like `peek_token()` for examination and `consume_token()` for advancement. Never mix these operations within a single logical parsing step.\n\n⚠️ **Pitfall: Error Recovery State Corruption**\n\nWhen parsing errors occur, poorly designed error handling can leave the parser in an inconsistent state that causes cascading errors or incorrect error messages. This typically happens when parsing functions partially modify parser state before encountering errors, then fail to properly clean up or reset to a known good state.\n\nA column list parser might successfully parse several columns, adding them to a result list, then encounter a syntax error on the fourth column. If the error handling doesn't properly reset the parser state, subsequent parsing attempts might include the partial results from the failed parse, leading to confused error messages or incorrect AST structures.\n\nAddress this pitfall by implementing atomic parsing operations that either succeed completely or fail without side effects. Use temporary data structures during parsing and only commit results to the final AST after successful completion of entire grammar rules. This creates clean error boundaries that don't pollute subsequent parsing attempts.\n\n![SELECT Statement Parsing Flow](./diagrams/select-parsing-flow.svg)\n\n### Implementation Guidance\n\nThe SELECT parser implementation requires careful coordination between token stream management, recursive descent parsing logic, and AST node construction. This section provides the concrete Python implementation framework that transforms the design concepts into working code, with particular attention to the parsing patterns and error handling strategies that make the parser robust and maintainable.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Token Stream | Simple list index tracking | Iterator-based stream with buffering |\n| AST Construction | Direct object instantiation | Builder pattern with validation |\n| Error Handling | Exception-based with position tracking | Error collection with recovery attempts |\n| Look-ahead | Single token peek with manual management | Configurable look-ahead buffer |\n\n**Recommended File Structure:**\n\n```\nsql_parser/\n  parser/\n    __init__.py                    ← Parser module exports\n    select_parser.py              ← SELECT statement parsing (this component)\n    base_parser.py                ← Common parsing utilities and base classes\n    expression_parser.py          ← WHERE clause expression parsing (milestone 3)\n    dml_parser.py                 ← INSERT/UPDATE/DELETE parsing (milestone 4)\n  ast/\n    __init__.py                   ← AST node exports\n    nodes.py                      ← AST node class definitions\n    visitors.py                   ← AST traversal and manipulation\n  tokenizer/\n    __init__.py                   ← Tokenizer exports (from milestone 1)\n    tokenizer.py                  ← Token generation (from milestone 1)\n  errors/\n    __init__.py                   ← Error class exports\n    parse_errors.py               ← Parsing error definitions\n  tests/\n    test_select_parser.py         ← SELECT parser unit tests\n    test_integration.py           ← End-to-end parsing tests\n```\n\n**Core Parsing Infrastructure (Complete):**\n\n```python\n# base_parser.py - Common utilities for all parser components\nfrom typing import List, Optional, Union\nfrom ..tokenizer.tokenizer import Token, TokenType\nfrom ..errors.parse_errors import UnexpectedTokenError, SyntaxError\nfrom ..ast.nodes import ASTNode, SourceLocation\n\nclass BaseParser:\n    \"\"\"Base class providing common parsing utilities and token stream management.\"\"\"\n    \n    def __init__(self, tokens: List[Token]):\n        self.tokens = tokens\n        self.position = 0\n        self.current_token = tokens[0] if tokens else None\n    \n    def peek_token(self, offset: int = 1) -> Optional[Token]:\n        \"\"\"Look ahead at future tokens without consuming them.\"\"\"\n        peek_pos = self.position + offset\n        return self.tokens[peek_pos] if peek_pos < len(self.tokens) else None\n    \n    def consume_token(self) -> Token:\n        \"\"\"Advance parser position and return the consumed token.\"\"\"\n        consumed = self.current_token\n        self.position += 1\n        self.current_token = self.tokens[self.position] if self.position < len(self.tokens) else None\n        return consumed\n    \n    def expect_token(self, expected_type: TokenType) -> Token:\n        \"\"\"Verify current token matches expected type and consume it.\"\"\"\n        if not self.current_token or self.current_token.type != expected_type:\n            raise UnexpectedTokenError(\n                f\"Expected {expected_type.name}, got {self.current_token.type.name if self.current_token else 'EOF'}\",\n                self.current_token.line if self.current_token else 0,\n                self.current_token.column if self.current_token else 0,\n                expected_type.name,\n                self.current_token\n            )\n        return self.consume_token()\n    \n    def is_at_end(self) -> bool:\n        \"\"\"Check if parser has reached end of token stream.\"\"\"\n        return self.current_token is None or self.position >= len(self.tokens)\n    \n    def create_source_location(self, start_token: Token, end_token: Optional[Token] = None) -> SourceLocation:\n        \"\"\"Create source location from token positions.\"\"\"\n        end = end_token or self.current_token or start_token\n        return SourceLocation(\n            start_line=start_token.line,\n            start_column=start_token.column,\n            end_line=end.line,\n            end_column=end.column + len(end.value)\n        )\n```\n\n**SELECT Parser Core Logic (Skeleton):**\n\n```python\n# select_parser.py - SELECT statement recursive descent parser\nfrom typing import List, Optional\nfrom .base_parser import BaseParser\nfrom ..tokenizer.tokenizer import TokenType\nfrom ..ast.nodes import (\n    SelectStatement, Identifier, QualifiedIdentifier, StarExpression,\n    AliasedExpression, TableReference\n)\nfrom ..errors.parse_errors import SyntaxError\n\nclass SelectParser(BaseParser):\n    \"\"\"Recursive descent parser for SELECT statements following SQL grammar rules.\"\"\"\n    \n    def parse_select_statement(self) -> SelectStatement:\n        \"\"\"\n        Parse complete SELECT statement: SELECT column_list FROM table_reference [WHERE expression]\n        \n        Grammar: select_statement ::= SELECT column_list FROM table_reference [WHERE expression]\n        Returns: SelectStatement AST node with all parsed components\n        \"\"\"\n        # TODO 1: Verify current token is SELECT_KEYWORD and consume it\n        # TODO 2: Parse column list by calling parse_column_list()\n        # TODO 3: Verify FROM_KEYWORD is present and consume it\n        # TODO 4: Parse table reference by calling parse_table_reference()\n        # TODO 5: Check for optional WHERE_KEYWORD and parse WHERE clause if present\n        # TODO 6: Create SelectStatement AST node with all parsed components\n        # TODO 7: Set source location information from start/end tokens\n        # Hint: Use expect_token() for required keywords, peek_token() for optional clauses\n        pass\n    \n    def parse_column_list(self) -> Union[StarExpression, List[AliasedExpression]]:\n        \"\"\"\n        Parse column list: either '*' or comma-separated column specifications\n        \n        Grammar: column_list ::= '*' | column_spec (',' column_spec)*\n        Returns: StarExpression for SELECT *, or list of column specifications\n        \"\"\"\n        # TODO 1: Check if current token is ASTERISK for SELECT * case\n        # TODO 2: If asterisk, consume token and return StarExpression AST node\n        # TODO 3: Otherwise, initialize empty column list for explicit columns\n        # TODO 4: Parse first column specification by calling parse_column_spec()\n        # TODO 5: Loop while current token is COMMA - consume comma and parse next column\n        # TODO 6: Return list of parsed column specifications\n        # Hint: Use peek_token() to check for FROM_KEYWORD to end column list parsing\n        pass\n    \n    def parse_column_spec(self) -> AliasedExpression:\n        \"\"\"\n        Parse individual column specification with optional alias\n        \n        Grammar: column_spec ::= [table_name '.'] column_name [AS alias | alias]\n        Returns: AliasedExpression wrapping Identifier or QualifiedIdentifier\n        \"\"\"\n        # TODO 1: Parse base identifier (could be column name or table qualifier)\n        # TODO 2: Check if next token is DOT for qualified name (table.column)\n        # TODO 3: If DOT found, consume it and parse actual column name\n        # TODO 4: Create Identifier or QualifiedIdentifier based on qualification\n        # TODO 5: Check for alias - either AS keyword followed by identifier, or just identifier\n        # TODO 6: Create AliasedExpression with parsed column and optional alias\n        # Hint: Use peek_token() to distinguish aliases from next clause keywords\n        pass\n    \n    def parse_table_reference(self) -> TableReference:\n        \"\"\"\n        Parse table reference with optional alias\n        \n        Grammar: table_reference ::= [schema_name '.'] table_name [AS alias | alias]\n        Returns: TableReference AST node with table identifier and optional alias\n        \"\"\"\n        # TODO 1: Parse base identifier (could be table name or schema qualifier)\n        # TODO 2: Check if next token is DOT for schema-qualified table (schema.table)\n        # TODO 3: If DOT found, consume it and parse actual table name\n        # TODO 4: Create Identifier or QualifiedIdentifier for table reference\n        # TODO 5: Check for optional table alias using same logic as column aliases\n        # TODO 6: Create and return TableReference AST node\n        # Hint: Table aliases help with self-joins and complex queries in future extensions\n        pass\n    \n    def parse_identifier(self) -> str:\n        \"\"\"\n        Parse identifier token and return its string value\n        \n        Returns: String value of identifier token\n        Raises: UnexpectedTokenError if current token is not IDENTIFIER\n        \"\"\"\n        # TODO 1: Verify current token is IDENTIFIER type\n        # TODO 2: Consume the identifier token and return its value\n        # Hint: Use expect_token() for automatic type checking and consumption\n        pass\n```\n\n**Language-Specific Python Hints:**\n\n- Use `dataclasses` or `attrs` for AST node definitions to reduce boilerplate and get automatic `__init__`, `__repr__`, and comparison methods\n- Leverage `typing.Union` for parsing methods that can return different AST node types based on syntax variations\n- Use `enum.Enum` for token types to get automatic string representation and comparison safety\n- Consider `contextlib.contextmanager` for parsing state management that automatically handles cleanup on errors\n- Use `pytest.parametrize` for testing multiple SQL statement variations with the same test logic\n\n**Milestone 2 Checkpoint:**\n\nAfter implementing the SELECT parser, verify functionality with these specific tests:\n\n1. **Basic SELECT parsing**: `python -m pytest tests/test_select_parser.py::test_basic_select -v`\n   - Expected: All tests pass showing correct AST node creation for simple SELECT statements\n   - Verify: `SELECT name FROM users` creates SelectStatement with Identifier column and TableReference\n\n2. **Column list variations**: Test these statements manually in a Python REPL:\n   ```python\n   from sql_parser.parser.select_parser import SelectParser\n   from sql_parser.tokenizer.tokenizer import Tokenizer\n   \n   # Test star wildcard\n   tokens = Tokenizer(\"SELECT * FROM users\").tokenize()\n   ast = SelectParser(tokens).parse_select_statement()\n   assert isinstance(ast.columns, StarExpression)\n   \n   # Test multiple columns\n   tokens = Tokenizer(\"SELECT name, email, age FROM users\").tokenize()\n   ast = SelectParser(tokens).parse_select_statement()\n   assert len(ast.columns) == 3\n   ```\n\n3. **Alias parsing**: Verify both explicit and implicit aliases work correctly\n   - `SELECT name AS full_name FROM users AS u` should parse without errors\n   - `SELECT name full_name FROM users u` should create the same AST structure\n\n4. **Error reporting**: Test invalid syntax produces helpful error messages:\n   ```python\n   # Missing FROM keyword should raise UnexpectedTokenError\n   tokens = Tokenizer(\"SELECT name WHERE id = 1\").tokenize()\n   try:\n       SelectParser(tokens).parse_select_statement()\n       assert False, \"Should have raised UnexpectedTokenError\"\n   except UnexpectedTokenError as e:\n       assert \"Expected FROM_KEYWORD\" in str(e)\n   ```\n\n**Signs of Implementation Issues:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Parser hangs in infinite loop | Token consumption not advancing position | Add debug prints showing current token and position | Ensure every parsing method advances position or exits loop |\n| \"Unexpected EOF\" errors | Parser consuming more tokens than available | Check `is_at_end()` before token access | Add bounds checking in all token access methods |\n| Wrong AST node types in tree | Incorrect node construction in parsing methods | Print AST structure after parsing | Verify each parsing method returns documented node type |\n| Missing child nodes in AST | Parsed elements not assigned to parent nodes | Walk AST tree and print all child relationships | Check all AST node constructors receive parsed children |\n\n\n## WHERE Clause Expression Parser Design\n\n> **Milestone(s):** Milestone 3 (WHERE Clause) - This section provides the detailed design for parsing complex expressions with proper operator precedence, building upon the tokenizer from Milestone 1 and the basic parsing infrastructure from Milestone 2.\n\n### Mental Model: Mathematical Expression Evaluation\n\nUnderstanding expression parsing requires thinking about how humans naturally process mathematical expressions and apply precedence rules. When you see the expression `2 + 3 * 4`, you instinctively know to multiply first, then add, yielding 14 rather than 20. This intuitive understanding comes from years of learning mathematical precedence rules that dictate the order of operations.\n\nSQL WHERE clause expressions work identically to mathematical expressions, but with additional operators for comparisons (`=`, `<`, `>`), logical operations (`AND`, `OR`, `NOT`), and SQL-specific constructs like `IS NULL`. Just as mathematical expressions form tree structures where higher-precedence operations appear deeper in the tree, SQL expressions create Abstract Syntax Tree nodes where the root represents the lowest-precedence operation and leaves represent the highest-precedence operations or literal values.\n\nConsider the WHERE clause `age > 18 AND salary >= 50000 OR department = 'Engineering'`. A human reader naturally groups this as `((age > 18) AND (salary >= 50000)) OR (department = 'Engineering')` because comparison operators (`>`, `>=`, `=`) bind more tightly than logical operators (`AND`), and `AND` binds more tightly than `OR`. The expression parser must replicate this human understanding by building an AST where the `OR` node sits at the root, with an `AND` node as its left child and an equality comparison as its right child.\n\nThe key insight is that **precedence determines tree depth**. Higher-precedence operators appear deeper in the tree because they must be evaluated first. Lower-precedence operators appear closer to the root because they operate on the results of higher-precedence sub-expressions. Parentheses override natural precedence by forcing certain sub-expressions to be treated as atomic units, effectively \"raising\" their precedence to the highest level.\n\nThis mental model guides our algorithm design: we need a parsing strategy that naturally builds trees where precedence determines depth, handles parentheses as precedence overrides, and processes left-to-right evaluation for operators of equal precedence (associativity).\n\n### SQL Operator Precedence Rules\n\nSQL expressions in WHERE clauses involve multiple categories of operators, each with specific precedence and associativity rules. Understanding these rules is crucial because they determine how ambiguous expressions are resolved and how the AST should be structured.\n\nThe following table presents the complete operator precedence hierarchy for SQL expressions, ordered from highest precedence (evaluated first) to lowest precedence (evaluated last):\n\n| Precedence Level | Operators | Associativity | Description | Example |\n|------------------|-----------|---------------|-------------|---------|\n| 1 (Highest) | `()` | N/A | Parentheses override all precedence | `(age + 5) * 2` |\n| 2 | `+`, `-` (unary) | Right | Unary plus and minus | `-salary`, `+bonus` |\n| 3 | `*`, `/`, `%` | Left | Multiplicative arithmetic | `salary * 0.1` |\n| 4 | `+`, `-` (binary) | Left | Additive arithmetic | `base_pay + bonus` |\n| 5 | `=`, `!=`, `<>`, `<`, `<=`, `>`, `>=` | Left | Comparison operators | `age >= 21` |\n| 6 | `IS NULL`, `IS NOT NULL` | Left | NULL testing | `email IS NOT NULL` |\n| 7 | `NOT` | Right | Logical negation | `NOT active` |\n| 8 | `AND` | Left | Logical conjunction | `age > 18 AND active` |\n| 9 (Lowest) | `OR` | Left | Logical disjunction | `admin OR manager` |\n\nThe **associativity** column determines how operators of equal precedence are grouped. Left associativity means `a + b + c` groups as `(a + b) + c`, while right associativity means `a = b = c` would group as `a = (b = c)` if SQL supported chained assignments (which it doesn't in WHERE clauses).\n\nSeveral important edge cases and special rules apply to SQL operator precedence:\n\n**Comparison Chaining**: Unlike mathematical expressions, SQL does not allow chained comparisons like `18 < age < 65`. Each comparison must be explicit: `age > 18 AND age < 65`. The parser should detect and reject chained comparisons with appropriate error messages.\n\n**NULL Handling**: SQL uses three-valued logic where expressions can evaluate to TRUE, FALSE, or NULL. The `IS NULL` and `IS NOT NULL` operators have special precedence because they bind tightly to their operand. The expression `salary + bonus IS NULL` groups as `(salary + bonus) IS NULL`, not `salary + (bonus IS NULL)`.\n\n**Negation Scope**: The `NOT` operator has right associativity and applies to the immediately following expression. `NOT age > 18 AND active` groups as `(NOT (age > 18)) AND active`, not `NOT ((age > 18) AND active)`. This frequently causes logical errors in query writing.\n\n**Parentheses Override**: Parentheses have the highest precedence and create atomic sub-expressions. The parser must handle nested parentheses correctly and detect unmatched parentheses as syntax errors.\n\nUnderstanding these precedence rules enables the expression parser to build AST structures that correctly represent the intended evaluation order. The precedence climbing algorithm uses these numeric precedence levels to determine when to recurse deeper into the expression tree versus when to return control to handle lower-precedence operators.\n\n### Precedence Climbing Parser Algorithm\n\nThe **precedence climbing algorithm** provides an elegant solution to expression parsing that naturally handles operator precedence and associativity without requiring complex grammar transformations or separate parsing phases. This algorithm works by recursively parsing sub-expressions while maintaining a minimum precedence threshold that determines which operators can be consumed at each recursion level.\n\nThe core insight behind precedence climbing is that higher-precedence operators should be parsed by deeper recursive calls, while lower-precedence operators are handled by shallower calls. By passing a minimum precedence parameter to each recursive call, we ensure that operators below this threshold are left for parent calls to handle, while operators at or above the threshold are consumed by the current call.\n\nThe algorithm follows these fundamental steps for parsing any expression:\n\n1. **Parse Primary Expression**: Begin by parsing a primary expression (literal, identifier, or parenthesized expression) that serves as the left operand for any following binary operations.\n\n2. **Enter Binary Operation Loop**: Check if the current token is a binary operator and compare its precedence to the minimum precedence threshold passed to this function call.\n\n3. **Precedence Decision**: If the operator's precedence is below the minimum threshold, return the current expression without consuming the operator (let a parent call handle it). If at or above threshold, consume the operator and continue.\n\n4. **Parse Right Operand**: Recursively parse the right operand with an adjusted minimum precedence based on the current operator's precedence and associativity.\n\n5. **Build AST Node**: Create a binary operation AST node with the left expression, operator, and right expression as children.\n\n6. **Continue or Return**: Repeat the loop to handle additional operators of sufficient precedence, or return the completed expression.\n\nThe following table shows how the algorithm handles different parsing scenarios:\n\n| Current Min Precedence | Next Operator | Operator Precedence | Action Taken | Reason |\n|----------------------|---------------|-------------------|--------------|---------|\n| 5 | `+` | 4 | Consume operator, recurse | Operator meets threshold |\n| 5 | `*` | 3 | Consume operator, recurse | Higher precedence than threshold |\n| 5 | `OR` | 9 | Return current expression | Below threshold, parent handles |\n| 3 | `AND` | 8 | Return current expression | Below threshold, parent handles |\n| 8 | `AND` | 8 | Consume operator, recurse | Meets threshold exactly |\n\n**Associativity Handling**: Left-associative operators increment the minimum precedence for the right operand recursive call (`min_precedence = current_precedence + 1`), ensuring that operators of equal precedence in the right operand are handled by this call rather than the recursive call. Right-associative operators use the same precedence (`min_precedence = current_precedence`), allowing equal-precedence operators to be consumed by the recursive call.\n\n**Unary Operator Integration**: Unary operators are handled during primary expression parsing rather than in the binary operator loop. When encountering a unary operator token (`NOT`, unary `-`, unary `+`), the parser recursively calls the expression parser with the unary operator's precedence, then wraps the result in a `UnaryOperation` AST node.\n\n**Parentheses Processing**: Parentheses are treated as primary expressions with maximum precedence. When encountering an opening parenthesis, the parser recursively calls the expression parser with minimum precedence (allowing all operators), consumes the closing parenthesis, and returns the sub-expression as an atomic unit.\n\nThe precedence climbing approach elegantly handles complex expressions like `NOT active AND salary > base_pay + bonus * rate OR manager` by naturally building the correct AST structure through recursive precedence-based decisions, without requiring the parser to explicitly encode complex grammar rules or perform multiple parsing passes.\n\n![WHERE Clause Expression Parsing Sequence](./diagrams/expression-parsing-sequence.svg)\n\n### Expression Type Handling\n\nSQL WHERE clause expressions encompass several distinct categories of operations, each requiring specialized parsing logic while fitting within the overall precedence climbing framework. The expression parser must recognize and correctly handle each expression type to build appropriate AST nodes and validate syntax rules specific to each category.\n\n**Comparison Expressions** form the foundation of most WHERE clauses, testing relationships between values using operators like `=`, `!=`, `<`, `<=`, `>`, and `>=`. These expressions always produce boolean results and require two operands of compatible types. The parser creates `BinaryOperation` AST nodes with the comparison operator and validates that both operands are valid expressions.\n\n| Comparison Type | Operator | AST Node Structure | Validation Rules |\n|-----------------|----------|-------------------|------------------|\n| Equality | `=`, `!=`, `<>` | `BinaryOperation(left, op, right)` | Compatible operand types |\n| Ordering | `<`, `<=`, `>`, `>=` | `BinaryOperation(left, op, right)` | Comparable operand types |\n| Pattern Matching | `LIKE` (future) | `BinaryOperation(left, LIKE, pattern)` | Left: string, Right: pattern |\n\n**Logical Expressions** combine boolean values using `AND`, `OR`, and `NOT` operators, implementing SQL's three-valued logic where expressions can evaluate to TRUE, FALSE, or NULL. The `AND` and `OR` operators create `BinaryOperation` nodes, while `NOT` creates `UnaryOperation` nodes. These operators have specific short-circuit evaluation semantics that affect query optimization.\n\n**NULL Testing Expressions** use the special `IS NULL` and `IS NOT NULL` operators to test for NULL values, which cannot be tested using regular comparison operators due to SQL's three-valued logic. These expressions require special parsing because they involve multi-token operators (`IS NULL` is two tokens but represents a single operation).\n\nThe following table shows how different NULL testing expressions are parsed and represented:\n\n| Expression Pattern | Tokens Consumed | AST Node Type | Validation |\n|-------------------|-----------------|---------------|------------|\n| `column IS NULL` | `IS`, `NULL` | `UnaryOperation(IS_NULL, column)` | Operand must be expression |\n| `expr IS NOT NULL` | `IS`, `NOT`, `NULL` | `UnaryOperation(IS_NOT_NULL, expr)` | Operand must be expression |\n| `NOT column IS NULL` | `NOT`, `column`, `IS`, `NULL` | `UnaryOperation(NOT, UnaryOperation(IS_NULL, column))` | Nested unary operations |\n\n**Arithmetic Expressions** support mathematical calculations using `+`, `-`, `*`, `/`, and `%` operators, following standard mathematical precedence rules. These expressions can appear within comparisons (e.g., `salary * 1.1 > 50000`) and require numeric operands. The parser validates operand types where possible and creates appropriate `BinaryOperation` nodes.\n\n**Primary Expressions** represent the atomic building blocks of larger expressions: literals, identifiers, and parenthesized sub-expressions. Each primary expression type requires specialized parsing logic:\n\n- **Literals**: The parser recognizes `INTEGER_LITERAL`, `FLOAT_LITERAL`, `STRING_LITERAL`, and `NULL_LITERAL` tokens and creates corresponding AST nodes (`IntegerLiteral`, `FloatLiteral`, `StringLiteral`, `NullLiteral`).\n\n- **Identifiers**: Simple identifiers (`column_name`) and qualified identifiers (`table.column`) are parsed into `Identifier` and `QualifiedIdentifier` nodes respectively. The parser must distinguish between these forms by looking ahead for dot tokens.\n\n- **Parenthesized Expressions**: Opening parentheses trigger recursive expression parsing with minimum precedence, allowing any operator to be parsed within the parentheses. The parser validates that closing parentheses match opening ones.\n\n**Function Calls** (if supported) follow the pattern `function_name(arg1, arg2, ...)` and require parsing the function name, opening parenthesis, comma-separated argument list, and closing parenthesis. Each argument is a full expression parsed recursively.\n\nThe expression parser dispatches to appropriate specialized parsing functions based on the current token type, ensuring that each expression category receives proper syntactic and semantic validation while maintaining the overall precedence climbing algorithm structure.\n\n### Parentheses and Precedence Override\n\nParentheses serve as the most powerful precedence override mechanism in SQL expressions, allowing developers to explicitly control evaluation order and create sub-expressions that are treated as atomic units regardless of the natural operator precedence. Understanding how parentheses interact with the precedence climbing algorithm is essential for building correct expression parsers.\n\n**Conceptual Model**: Parentheses effectively \"reset\" the precedence context, creating an isolated parsing environment where any operators can appear regardless of the minimum precedence threshold of the surrounding context. When the parser encounters an opening parenthesis, it saves the current parsing state, recursively parses the enclosed expression with minimum precedence (allowing all operators), then restores the original context after consuming the closing parenthesis.\n\nThe parentheses parsing process follows these detailed steps:\n\n1. **Recognition**: During primary expression parsing, detect the `LEFT_PAREN` token type, which indicates the start of a parenthesized sub-expression.\n\n2. **Context Save**: Store the current token position and any relevant parser state that might be needed for error recovery.\n\n3. **Consume Opening Parenthesis**: Advance the parser position past the `LEFT_PAREN` token and update position tracking for error reporting.\n\n4. **Recursive Expression Parse**: Call the main expression parsing function with minimum precedence level 0 (or the lowest precedence value), allowing any operators to be parsed within the parentheses.\n\n5. **Validate Closing Parenthesis**: Expect and consume a `RIGHT_PAREN` token, generating a specific error message if the closing parenthesis is missing.\n\n6. **Return Sub-Expression**: Return the parsed sub-expression as an atomic primary expression that can participate in higher-level operations.\n\n**Nested Parentheses Handling**: The recursive nature of the precedence climbing algorithm naturally handles nested parentheses without additional complexity. Each level of nesting creates its own parsing context, and the recursive calls unwind in the correct order as closing parentheses are consumed.\n\nConsider the expression `((age + 5) * 2) > ((salary / 12) + bonus)`. The parser handles this through multiple levels of recursive calls:\n\n| Nesting Level | Expression Being Parsed | Parser Context | Result |\n|---------------|------------------------|----------------|---------|\n| 0 | `((age + 5) * 2) > ((salary / 12) + bonus)` | Main expression, min_prec=0 | Binary comparison |\n| 1 | `(age + 5) * 2` | Left operand of `>`, min_prec=0 | Binary multiplication |\n| 2 | `age + 5` | Within first parentheses, min_prec=0 | Binary addition |\n| 1 | `(salary / 12) + bonus` | Right operand of `>`, min_prec=0 | Binary addition |\n| 2 | `salary / 12` | Within second parentheses, min_prec=0 | Binary division |\n\n**Error Handling for Unbalanced Parentheses**: Parentheses introduce several error conditions that require careful handling and clear error messages:\n\n- **Missing Closing Parenthesis**: When a parenthesized expression reaches the end of input or encounters an unexpected token before finding the closing parenthesis, generate an error with the position of the opening parenthesis.\n\n- **Unexpected Closing Parenthesis**: When encountering a closing parenthesis outside of a parenthesized context, report that no matching opening parenthesis exists.\n\n- **Empty Parentheses**: SQL expressions cannot contain empty parentheses `()`, so the parser should detect this condition and provide an appropriate error message.\n\nThe following table shows error detection and recovery strategies for parentheses-related issues:\n\n| Error Condition | Detection Method | Error Message | Recovery Strategy |\n|-----------------|------------------|---------------|-------------------|\n| Missing `)` | End of input during parentheses parsing | \"Expected ')' to match '(' at line X:Y\" | Report error, don't attempt recovery |\n| Unexpected `)` | `)` token outside parentheses context | \"Unexpected ')' - no matching '('\" | Skip token, continue parsing |\n| Empty `()` | `)` immediately follows `(` | \"Empty parentheses are not allowed\" | Skip both tokens, continue parsing |\n| Nested overflow | Too many nesting levels | \"Expression nesting too deep\" | Limit recursion depth |\n\n**AST Representation**: Parentheses themselves do not create AST nodes because they are purely syntactic constructs that influence parsing behavior without representing semantic operations. The sub-expression within parentheses becomes the AST node, with the parentheses' effect preserved through the correct tree structure that reflects the overridden precedence.\n\nThis approach ensures that semantically equivalent expressions like `(a + b) * c` and the hypothetical high-precedence version produce identical AST structures, while maintaining the ability to reconstruct the original expression with parentheses when needed for pretty-printing or query optimization analysis.\n\n### Architecture Decision: Precedence Climbing vs Shunting Yard\n\nThe choice of expression parsing algorithm significantly impacts the parser's complexity, maintainability, and extensibility. Two primary algorithms dominate expression parsing: precedence climbing (also known as operator precedence parsing) and the Shunting Yard algorithm. Understanding the trade-offs between these approaches guides the architectural decision for our SQL parser.\n\n> **Decision: Precedence Climbing Algorithm for Expression Parsing**\n> - **Context**: SQL WHERE clause expressions involve multiple operator types with complex precedence and associativity rules. The parsing algorithm must handle unary operators, binary operators, function calls, and parentheses while building correct AST structures. The chosen algorithm affects code maintainability, debugging difficulty, and future extensibility for additional SQL operators.\n> - **Options Considered**: Precedence climbing algorithm, Shunting Yard algorithm with separate AST construction phase, and recursive descent with explicit precedence grammar rules\n> - **Decision**: Implement precedence climbing algorithm for all expression parsing\n> - **Rationale**: Precedence climbing provides the optimal balance of simplicity, directness, and maintainability for our use case. It builds AST nodes during parsing rather than requiring separate phases, integrates naturally with the existing recursive descent parser structure, and offers straightforward debugging and testing capabilities. The algorithm's recursive nature aligns with our overall parser architecture.\n> - **Consequences**: Enables direct AST construction during parsing, simplifies debugging with clear call stack correlation to expression structure, and allows easy extension for new operators by updating the precedence table. The trade-off is slightly higher memory usage due to recursive calls compared to iterative approaches.\n\nThe following comparison table evaluates the key characteristics of each algorithmic approach:\n\n| Criteria | Precedence Climbing | Shunting Yard | Recursive Descent Grammar |\n|----------|-------------------|---------------|---------------------------|\n| **Complexity** | Medium - single recursive function | Medium - two phases (infix→postfix, postfix→AST) | High - many grammar rules |\n| **AST Construction** | Direct during parsing | Separate phase after parsing | Direct during parsing |\n| **Memory Usage** | Medium - recursive call stack | Low - single evaluation stack | High - deep call stack |\n| **Debugging Ease** | High - call stack matches expression | Medium - trace through two phases | Low - complex control flow |\n| **Extensibility** | High - add operators to precedence table | Medium - update both phases | Low - modify grammar rules |\n| **Error Messages** | Excellent - precise location context | Good - requires position tracking | Fair - can be vague |\n| **Learning Curve** | Medium - understand precedence concept | High - understand two algorithms | High - complex grammar theory |\n\n**Precedence Climbing Advantages**: The precedence climbing algorithm excels in several areas critical to our educational SQL parser project. Its single-pass nature eliminates the complexity of coordinating multiple parsing phases while building AST nodes directly at the point where expressions are recognized. This directness greatly simplifies debugging because the recursive call stack directly mirrors the expression's structural hierarchy - when debugging a parsing error in `a + b * c`, the call stack shows exactly how `b * c` was parsed as a sub-expression before being used as the right operand of the addition.\n\nThe algorithm's extensibility particularly suits our milestone-based development approach. Adding support for new operators (like `LIKE` or `IN` in future milestones) requires only adding entries to the operator precedence table and potentially new AST node types. The core parsing logic remains unchanged, reducing the risk of introducing bugs when extending functionality.\n\n**Shunting Yard Considerations**: While the Shunting Yard algorithm offers theoretical elegance and potentially better performance for very complex expressions, it introduces unnecessary complexity for our use case. The two-phase approach (converting infix notation to postfix, then building AST from postfix) creates additional opportunities for bugs and makes debugging more challenging because errors can occur in either phase with less clear relationships between the phases.\n\n**Integration with Recursive Descent**: Precedence climbing integrates seamlessly with our existing recursive descent parser architecture. The expression parser can be called from any point in the grammar that expects an expression (WHERE clauses, SET clause values, etc.) and returns an appropriate AST node. This integration model would be more complex with Shunting Yard due to its multi-phase nature.\n\n**Educational Value**: For developers learning parsing techniques, precedence climbing provides better pedagogical value. The algorithm's behavior directly reflects how humans think about mathematical expressions and operator precedence, making it more intuitive to understand and debug. Students can easily trace through the algorithm manually to predict its behavior, building confidence in their parser implementation.\n\nThe precedence climbing algorithm thus emerges as the optimal choice for our SQL parser project, balancing implementation complexity with maintainability, debugging ease, and educational clarity while providing a solid foundation for future extensions to more complex SQL expression types.\n\n### Common Expression Parser Pitfalls\n\nExpression parsing introduces numerous subtle bugs and edge cases that frequently trip up developers, especially those new to parsing techniques. These pitfalls often stem from misunderstanding precedence rules, incorrectly handling associativity, or failing to properly validate expression syntax. Understanding these common mistakes helps avoid them during implementation and provides debugging guidance when issues arise.\n\n⚠️ **Pitfall: Incorrect Precedence Level Assignment**\n\nMany developers incorrectly assign precedence levels based on intuition rather than formal SQL specification, leading to expressions that parse but evaluate in unexpected order. The most common error is making logical operators (`AND`, `OR`) have higher precedence than comparison operators (`=`, `<`, `>`), when the reverse is actually correct.\n\nConsider the expression `age >= 18 AND department = 'Sales'`. With incorrect precedence where `AND` binds tighter than `=`, this would parse as `age >= (18 AND department) = 'Sales'`, which is nonsensical. The correct precedence makes comparisons bind tighter, yielding `(age >= 18) AND (department = 'Sales')`.\n\n**Fix**: Always consult the official SQL specification for precedence levels. Create comprehensive test cases that verify precedence behavior for all operator combinations. Never guess at precedence levels.\n\n⚠️ **Pitfall: Ignoring Associativity Rules**\n\nLeft and right associativity determine how operators of equal precedence are grouped, but many implementations fail to correctly implement associativity in the precedence climbing algorithm. This typically manifests as incorrect AST structures for expressions with repeated operators.\n\nThe expression `10 - 5 - 2` should parse as `(10 - 5) - 2 = 3` with left associativity, but incorrect implementations might parse it as `10 - (5 - 2) = 7`. The error occurs when the recursive call for the right operand uses the same minimum precedence as the current operator instead of incrementing it for left-associative operators.\n\n**Fix**: For left-associative operators, increment the minimum precedence when recursively parsing the right operand (`parse_expression(current_precedence + 1)`). For right-associative operators, use the same precedence (`parse_expression(current_precedence)`).\n\n⚠️ **Pitfall: Mishandling Multi-Token Operators**\n\nSQL includes operators composed of multiple tokens, such as `IS NULL`, `IS NOT NULL`, and `NOT LIKE`. Naive implementations often treat these as separate operations rather than atomic operators, leading to incorrect parsing and AST structures.\n\nThe expression `email IS NOT NULL` should create a single `UnaryOperation` node with operator `IS_NOT_NULL`, not a `UnaryOperation(NOT, UnaryOperation(IS_NULL, email))` structure. Treating multi-token operators as separate tokens creates semantic ambiguity and complicates query optimization.\n\n**Fix**: Implement lookahead logic to recognize multi-token operators during tokenization or parsing. Create specific token types for compound operators (`IS_NULL_KEYWORD`, `IS_NOT_NULL_KEYWORD`) or handle them as special cases in the expression parser.\n\n⚠️ **Pitfall: Incorrect Unary Operator Handling**\n\nUnary operators (`-`, `+`, `NOT`) require different parsing logic than binary operators, but many implementations incorrectly try to handle them within the binary operator precedence climbing loop. This leads to parsing failures or incorrect AST structures.\n\nThe expression `-salary > 1000` should parse as `(-salary) > 1000`, but incorrect implementations might fail to recognize the unary minus or incorrectly try to parse it as a binary operator with a missing left operand.\n\n**Fix**: Handle unary operators during primary expression parsing, not in the binary operator loop. When encountering a unary operator token, recursively call the expression parser with the unary operator's precedence level and wrap the result in a `UnaryOperation` node.\n\n⚠️ **Pitfall: Poor Parentheses Error Recovery**\n\nUnbalanced parentheses create parsing errors that cascade through the entire expression, often producing confusing error messages that don't clearly indicate the root problem. Many implementations fail to provide helpful error messages or attempt misguided error recovery that makes the situation worse.\n\nWhen encountering missing closing parentheses in `(age > 18 AND salary`, naive error recovery might attempt to insert the missing parenthesis automatically, leading to incorrect parsing of subsequent expressions and masking the real error location.\n\n**Fix**: Detect unbalanced parentheses at the point where they occur and provide precise error messages with line and column information for both the expected closing parenthesis and the location of the unmatched opening parenthesis. Avoid automatic error recovery for structural syntax errors.\n\n⚠️ **Pitfall: Insufficient Error Context in Messages**\n\nExpression parsing errors often provide generic messages like \"unexpected token\" without sufficient context about what the parser expected or where the expression parsing began. This makes debugging difficult, especially in complex nested expressions.\n\n**Fix**: Maintain parsing context throughout expression parsing, including the starting position of the current expression and the type of expression being parsed (WHERE clause, SET clause value, etc.). Include this context in error messages along with specific expectations based on the current parsing state.\n\nThe following table summarizes debugging strategies for common expression parsing symptoms:\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|--------------|------------------|-----|\n| Wrong evaluation order | Incorrect precedence levels | Check precedence table against SQL spec | Update precedence assignments |\n| Left-associative parsed as right | Missing precedence increment | Trace recursive calls for equal operators | Add +1 to min_precedence for left-assoc |\n| Multi-token operators fail | Token-by-token parsing | Check if `IS NULL` creates two operations | Implement compound operator recognition |\n| Unary operators don't work | Binary operator parsing logic | Verify unary ops handled in primary parsing | Move to primary expression parser |\n| Cascading parentheses errors | Poor error recovery | Check error message clarity and location | Improve error reporting, avoid recovery |\n| Vague error messages | Missing parsing context | Review error message content and detail | Add context about expression type and expectations |\n\n### Implementation Guidance\n\nThis section provides concrete Python implementation guidance for building the WHERE clause expression parser using the precedence climbing algorithm. The implementation builds upon the tokenizer from Milestone 1 and integrates with the basic parser infrastructure from Milestone 2.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Precedence Table | Python dictionary mapping operators to integers | Dataclass with precedence and associativity |\n| AST Node Creation | Simple factory functions | Abstract factory pattern with node registry |\n| Error Handling | Basic exception raising with message | Rich error objects with source location |\n| Expression Validation | Runtime type checking in AST nodes | Static analysis during parsing |\n\n**Recommended File Structure Integration:**\n\nThe expression parser integrates into the existing parser module structure established in previous milestones:\n\n```\nsql_parser/\n  tokenizer.py              ← Milestone 1: Token definitions and lexical analysis\n  ast_nodes.py             ← AST node classes for all statement and expression types\n  base_parser.py           ← Base parser infrastructure with token management\n  select_parser.py         ← Milestone 2: SELECT statement parsing\n  expression_parser.py     ← NEW: WHERE clause expression parsing (this milestone)\n  parser.py               ← Main parser entry point coordinating all components\n  test_expression_parser.py ← Comprehensive expression parser tests\n```\n\n**Infrastructure Code - Precedence Table and Operator Definitions:**\n\n```python\n# expression_parser.py\nfrom enum import IntEnum\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional, Union\nfrom .tokenizer import Token, TokenType\nfrom .ast_nodes import *\nfrom .base_parser import BaseParser, ParseError, UnexpectedTokenError\n\nclass Precedence(IntEnum):\n    \"\"\"Operator precedence levels for expression parsing.\n    \n    Higher numeric values indicate higher precedence (evaluated first).\n    This enum provides clear naming and easy comparison for precedence levels.\n    \"\"\"\n    LOWEST = 0       # Used for parentheses and top-level parsing\n    OR = 1          # Logical OR - lowest precedence operator\n    AND = 2         # Logical AND\n    NOT = 3         # Logical NOT (unary)\n    IS_NULL = 4     # IS NULL, IS NOT NULL operators\n    COMPARISON = 5   # =, !=, <>, <, <=, >, >=\n    ADDITION = 6     # Binary + and - operators\n    MULTIPLICATION = 7  # *, /, % operators\n    UNARY = 8       # Unary + and - operators\n    PRIMARY = 9     # Literals, identifiers, parentheses\n\n@dataclass\nclass OperatorInfo:\n    \"\"\"Complete operator information including precedence and associativity.\n    \n    This structure allows easy extension for additional operator properties\n    like operator symbol, AST node type, and validation rules.\n    \"\"\"\n    precedence: Precedence\n    is_right_associative: bool = False\n    \n    def right_binding_power(self) -> int:\n        \"\"\"Calculate binding power for right operand in precedence climbing.\n        \n        Left-associative operators increment binding power to ensure\n        equal-precedence operators in right operand are handled by parent call.\n        Right-associative operators maintain same binding power.\n        \"\"\"\n        if self.is_right_associative:\n            return self.precedence.value\n        return self.precedence.value + 1\n\n# Complete operator precedence table for SQL expressions\nOPERATOR_PRECEDENCE: Dict[TokenType, OperatorInfo] = {\n    # Logical operators\n    TokenType.OR_KEYWORD: OperatorInfo(Precedence.OR),\n    TokenType.AND_KEYWORD: OperatorInfo(Precedence.AND),\n    TokenType.NOT_KEYWORD: OperatorInfo(Precedence.NOT, is_right_associative=True),\n    \n    # Comparison operators  \n    TokenType.EQUALS: OperatorInfo(Precedence.COMPARISON),\n    TokenType.NOT_EQUALS: OperatorInfo(Precedence.COMPARISON),\n    TokenType.LESS_THAN: OperatorInfo(Precedence.COMPARISON),\n    TokenType.LESS_EQUAL: OperatorInfo(Precedence.COMPARISON), \n    TokenType.GREATER_THAN: OperatorInfo(Precedence.COMPARISON),\n    TokenType.GREATER_EQUAL: OperatorInfo(Precedence.COMPARISON),\n    \n    # Arithmetic operators\n    TokenType.PLUS: OperatorInfo(Precedence.ADDITION),\n    TokenType.MINUS: OperatorInfo(Precedence.ADDITION),\n    TokenType.MULTIPLY: OperatorInfo(Precedence.MULTIPLICATION),\n    TokenType.DIVIDE: OperatorInfo(Precedence.MULTIPLICATION),\n    TokenType.MODULO: OperatorInfo(Precedence.MULTIPLICATION),\n}\n\ndef is_binary_operator(token_type: TokenType) -> bool:\n    \"\"\"Check if token type represents a binary operator.\"\"\"\n    return token_type in OPERATOR_PRECEDENCE\n\ndef is_unary_operator(token_type: TokenType) -> bool:\n    \"\"\"Check if token type can be used as unary operator.\"\"\"\n    return token_type in (TokenType.NOT_KEYWORD, TokenType.PLUS, TokenType.MINUS)\n```\n\n**Core Logic Skeleton - Expression Parser Class:**\n\n```python\nclass ExpressionParser(BaseParser):\n    \"\"\"Precedence climbing parser for SQL WHERE clause expressions.\n    \n    This parser handles all expression types including comparisons, logical operations,\n    arithmetic, literals, identifiers, and parenthesized sub-expressions with correct\n    operator precedence and associativity.\n    \"\"\"\n    \n    def parse_expression(self, min_precedence: int = Precedence.LOWEST) -> ASTNode:\n        \"\"\"Parse expression using precedence climbing algorithm.\n        \n        Args:\n            min_precedence: Minimum operator precedence to handle at this level.\n                           Operators below this level are left for parent calls.\n        \n        Returns:\n            ASTNode representing the parsed expression\n            \n        Raises:\n            UnexpectedTokenError: When encountering invalid expression syntax\n            ParseError: When expression parsing fails due to syntax errors\n        \"\"\"\n        # TODO 1: Parse left-hand side primary expression (literal, identifier, or parenthesized)\n        # TODO 2: Enter binary operator parsing loop - continue while operators meet precedence threshold\n        # TODO 3: For each qualifying operator, determine right operand binding power based on associativity\n        # TODO 4: Recursively parse right operand with calculated binding power\n        # TODO 5: Create BinaryOperation AST node with left expr, operator, right expr\n        # TODO 6: Update left expression to newly created binary operation for next iteration\n        # TODO 7: Return final expression when no more qualifying operators found\n        # Hint: Use while loop checking current_token type and precedence\n        # Hint: Handle end-of-input gracefully by checking is_at_end()\n        pass\n    \n    def parse_primary_expression(self) -> ASTNode:\n        \"\"\"Parse primary expressions: literals, identifiers, unary ops, parentheses.\n        \n        Primary expressions are the atomic building blocks that cannot be broken\n        down further by operator precedence rules. This includes all literal values,\n        identifiers, unary operations, and parenthesized sub-expressions.\n        \n        Returns:\n            ASTNode representing the primary expression\n            \n        Raises:\n            UnexpectedTokenError: When current token cannot start primary expression\n        \"\"\"\n        # TODO 1: Check current token type to determine primary expression type\n        # TODO 2: Handle INTEGER_LITERAL -> create IntegerLiteral AST node\n        # TODO 3: Handle FLOAT_LITERAL -> create FloatLiteral AST node  \n        # TODO 4: Handle STRING_LITERAL -> create StringLiteral AST node\n        # TODO 5: Handle NULL_KEYWORD -> create NullLiteral AST node\n        # TODO 6: Handle IDENTIFIER -> parse as simple or qualified identifier\n        # TODO 7: Handle unary operators (NOT, +, -) -> recursively parse operand with unary precedence\n        # TODO 8: Handle LEFT_PAREN -> recursively parse inner expression, expect RIGHT_PAREN\n        # TODO 9: Raise UnexpectedTokenError for any other token type\n        # Hint: Use token consumption pattern: token = consume_token(), then process token.value\n        # Hint: For parentheses, call parse_expression(Precedence.LOWEST) for inner expression\n        pass\n    \n    def parse_identifier_expression(self) -> ASTNode:\n        \"\"\"Parse identifier that might be qualified (table.column) or simple (column).\n        \n        Handles both simple identifiers and qualified identifiers by looking ahead\n        for dot tokens. Also handles potential IS NULL / IS NOT NULL operators\n        that follow identifiers.\n        \n        Returns:\n            Identifier or QualifiedIdentifier AST node\n        \"\"\"\n        # TODO 1: Consume IDENTIFIER token and store name\n        # TODO 2: Check if next token is DOT for qualified identifier\n        # TODO 3: If qualified, consume DOT and next IDENTIFIER, create QualifiedIdentifier\n        # TODO 4: If simple, create basic Identifier node\n        # TODO 5: Check for IS NULL / IS NOT NULL operators following identifier\n        # TODO 6: Return appropriate identifier node\n        # Hint: Use peek_token() to check for DOT without consuming\n        # Hint: Handle IS NULL as post-processing of identifier rather than separate operator\n        pass\n        \n    def parse_null_test_expression(self, operand: ASTNode) -> ASTNode:\n        \"\"\"Parse IS NULL or IS NOT NULL operations following an expression.\n        \n        Args:\n            operand: Expression being tested for NULL value\n            \n        Returns:\n            UnaryOperation node for IS NULL or IS NOT NULL\n        \"\"\"\n        # TODO 1: Expect and consume IS_KEYWORD token\n        # TODO 2: Check if next token is NOT_KEYWORD for IS NOT NULL\n        # TODO 3: If NOT found, consume it and expect NULL_KEYWORD -> create IS_NOT_NULL operation\n        # TODO 4: If no NOT, expect NULL_KEYWORD -> create IS_NULL operation  \n        # TODO 5: Return UnaryOperation with appropriate operator type and operand\n        # Hint: Create custom UnaryOperationType enum values for IS_NULL and IS_NOT_NULL\n        pass\n    \n    def parse_parenthesized_expression(self) -> ASTNode:\n        \"\"\"Parse parenthesized sub-expression with precedence override.\n        \n        Parentheses create isolated parsing context where any operators can appear\n        regardless of surrounding minimum precedence. This implements precedence override.\n        \n        Returns:\n            ASTNode representing the sub-expression within parentheses\n            \n        Raises:\n            UnexpectedTokenError: When closing parenthesis is missing\n        \"\"\"\n        # TODO 1: Consume LEFT_PAREN token (already verified by caller)\n        # TODO 2: Check for empty parentheses () -> raise appropriate error\n        # TODO 3: Recursively call parse_expression(Precedence.LOWEST) for inner expression  \n        # TODO 4: Expect and consume RIGHT_PAREN token\n        # TODO 5: Return the inner expression (parentheses don't create AST nodes)\n        # Hint: Save opening parenthesis token position for error reporting\n        # Hint: Use expect_token(TokenType.RIGHT_PAREN) with descriptive error message\n        pass\n        \n    def get_operator_precedence(self, token_type: TokenType) -> Optional[int]:\n        \"\"\"Get precedence level for operator token type.\n        \n        Args:\n            token_type: Token type to look up\n            \n        Returns:\n            Precedence level integer, or None if not an operator\n        \"\"\"\n        operator_info = OPERATOR_PRECEDENCE.get(token_type)\n        return operator_info.precedence.value if operator_info else None\n        \n    def is_right_associative(self, token_type: TokenType) -> bool:\n        \"\"\"Check if operator is right-associative.\n        \n        Args:\n            token_type: Operator token type\n            \n        Returns:\n            True if right-associative, False if left-associative\n        \"\"\"\n        operator_info = OPERATOR_PRECEDENCE.get(token_type)\n        return operator_info.is_right_associative if operator_info else False\n```\n\n**Integration with Main Parser:**\n\n```python\n# In select_parser.py - integrate expression parser for WHERE clauses\nclass SelectParser(BaseParser):\n    def __init__(self, tokens: List[Token]):\n        super().__init__(tokens)\n        self.expression_parser = ExpressionParser(tokens)\n        \n    def parse_where_clause(self) -> Optional[ASTNode]:\n        \"\"\"Parse WHERE clause with complete expression support.\n        \n        Returns:\n            Expression AST node, or None if no WHERE clause present\n        \"\"\"\n        # TODO 1: Check if current token is WHERE_KEYWORD\n        # TODO 2: If no WHERE clause, return None\n        # TODO 3: Consume WHERE_KEYWORD token\n        # TODO 4: Delegate to expression parser for condition parsing\n        # TODO 5: Return expression AST node from expression parser\n        # Hint: self.expression_parser.parse_expression() handles full WHERE condition\n        pass\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the expression parser, verify correct functionality with these specific tests:\n\n**Test Command**: `python -m pytest test_expression_parser.py -v`\n\n**Expected Behavior Verification:**\n1. **Precedence Test**: Parse `age > 18 AND salary >= 50000 OR department = 'Engineering'` and verify AST structure shows `OR` at root with `AND` as left child\n2. **Associativity Test**: Parse `10 - 5 - 2` and verify it creates `((10 - 5) - 2)` structure, not `(10 - (5 - 2))`\n3. **Parentheses Test**: Parse `(age + 5) * 2 > salary` and verify multiplication has higher effective precedence than comparison\n4. **NULL Test**: Parse `email IS NOT NULL AND active` and verify `IS NOT NULL` creates single unary operation\n5. **Unary Test**: Parse `NOT active AND salary > 0` and verify `NOT` applies only to `active`, not entire expression\n\n**Debug Output**: Enable debug logging to see precedence climbing decisions:\n```python\n# Add to parse_expression method for debugging\nif self.debug_enabled:\n    print(f\"Parsing with min_precedence={min_precedence}, current_token={self.current_token}\")\n```\n\n**Signs of Issues and Diagnostics:**\n- **Wrong precedence**: Check precedence table values against SQL specification\n- **Associativity errors**: Verify binding power calculation in recursive calls  \n- **Parentheses issues**: Ensure recursive call uses `Precedence.LOWEST` for inner expressions\n- **Unary operator problems**: Confirm unary operators handled in primary parsing, not binary loop\n- **Token consumption errors**: Verify `consume_token()` called exactly once per expected token\n\n\n## Data Modification Statement Parser Design\n\n> **Milestone(s):** Milestone 4 (INSERT/UPDATE/DELETE) - This section provides the detailed design for parsing data modification statements that transform database state, building on the expression parsing capabilities from milestone 3.\n\n### Mental Model: Data Manipulation Commands\n\nUnderstanding data modification language (DML) statements requires thinking about them as **database operation commands** rather than queries. While SELECT statements are questions asking \"what data exists?\", DML statements are instructions declaring \"change the data in this specific way.\"\n\nConsider the analogy of **giving instructions to a warehouse manager**. When you want to retrieve information, you ask questions: \"Show me all products from supplier X.\" But when you want to modify inventory, you give commands: \"Add these new products to shelf 5,\" \"Update the price of item 123 to $50,\" or \"Remove all expired items from the refrigerated section.\" Each command has a specific structure and requires different validation steps.\n\nINSERT statements are like **delivery instructions** - they specify what new items to add and where to place them. You must provide both a destination (which shelves/columns) and the actual items (what values). The warehouse manager needs to verify that you're providing the right number of items for the available shelf spaces.\n\nUPDATE statements are like **modification orders** - they identify existing items by description (WHERE clause) and specify how to change them (SET clauses). You're telling the manager \"find all items matching these criteria, then change their properties in these ways.\"\n\nDELETE statements are like **removal orders** - they identify items by description and request their elimination. The simplest but most dangerous instruction, since there's no way to undo a deletion without external backups.\n\nThis mental model helps us understand why DML parsers need different validation logic than SELECT parsers. While SELECT parsing focuses on expression evaluation and result formatting, DML parsing must validate **data integrity constraints** and **structural consistency** between different parts of the statement.\n\n### INSERT Statement Parsing\n\nINSERT statements follow a structured pattern that requires careful coordination between multiple parser components. The basic syntax `INSERT INTO table (columns) VALUES (values)` appears simple, but contains several parsing challenges that require specialized handling.\n\nThe parser must handle **column list specification** where users can either provide an explicit list of target columns or omit the column specification entirely. When columns are explicitly specified, the parser validates that each identifier refers to a valid column name. When omitted, the parser assumes all columns in their table definition order, which requires the parser to understand default column ordering.\n\n**Multiple value row parsing** presents additional complexity since INSERT statements can specify multiple rows in a single command: `INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com'), ('Bob', 'bob@example.com')`. The parser must track value list boundaries and ensure each row contains the same number of values as the column specification.\n\nThe `InsertStatement` AST node structure captures this complexity through several key components:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| table_reference | TableReference | Target table identifier with optional alias |\n| column_list | Optional[List[Identifier]] | Explicit column names or None for all columns |\n| value_rows | List[List[Expression]] | List of value lists, one per row being inserted |\n| source_location | SourceLocation | Position information for error reporting |\n\nThe parsing algorithm follows a structured sequence that validates syntax while building the appropriate AST structure:\n\n1. **Parse INSERT keyword** and verify the statement begins with the expected token type\n2. **Parse INTO keyword** which is required in SQL INSERT syntax, though some dialects make it optional\n3. **Parse table reference** using the existing table parsing logic from SELECT statements\n4. **Detect column list presence** by looking ahead for an opening parenthesis token\n5. **Parse column list** if present, validating each identifier and handling comma separation\n6. **Parse VALUES keyword** which introduces the actual data values being inserted\n7. **Parse value row list** handling multiple rows separated by commas, with each row containing parenthesized value expressions\n8. **Validate column-value consistency** ensuring each value row contains the same number of expressions as the column specification\n\nThe **column list parsing** logic must handle both explicit and implicit column specifications. When columns are explicitly provided, the parser builds a list of `Identifier` nodes representing the target columns. When omitted, the parser sets the column_list field to None, indicating that the consuming application should use default column ordering.\n\n**Value expression parsing** within each row leverages the existing expression parser developed for WHERE clauses, since INSERT values can include literals, arithmetic expressions, and function calls. However, the parser must be careful to handle **type validation** at the AST level rather than during parsing, since type checking requires schema information not available to the syntax parser.\n\n**Multi-row value parsing** requires careful state management as the parser alternates between comma tokens separating rows and comma tokens separating values within rows. The parser tracks nesting level using parentheses to distinguish between these two contexts.\n\n> **Design Insight**: INSERT parsing reuses expression parsing logic from WHERE clauses, but requires additional structural validation to ensure column-value alignment. This demonstrates how the recursive descent architecture enables component reuse across different statement types.\n\n### UPDATE Statement Parsing\n\nUPDATE statements combine table references, assignment expressions, and conditional filtering in a single command, making them the most syntactically complex DML statement. The parser must coordinate between multiple parsing subsystems while maintaining proper precedence and associativity rules.\n\nThe basic UPDATE syntax `UPDATE table SET column = value WHERE condition` requires parsing three distinct sections that interact with different parts of the existing parser infrastructure. The SET clause introduces **assignment expressions** which differ from boolean expressions used in WHERE clauses, requiring specialized parsing logic.\n\nThe `UpdateStatement` AST node captures the structural relationships between these components:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| table_reference | TableReference | Target table with optional alias |\n| set_assignments | List[AssignmentExpression] | Column-value assignments from SET clause |\n| where_clause | Optional[Expression] | Filtering condition or None for all rows |\n| source_location | SourceLocation | Position information for error reporting |\n\nThe `AssignmentExpression` AST node represents individual SET clause assignments:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| target_column | Identifier | Column being assigned new value |\n| assigned_value | Expression | Expression computing the new value |\n| source_location | SourceLocation | Position information for error reporting |\n\nThe parsing algorithm coordinates multiple specialized parsing functions:\n\n1. **Parse UPDATE keyword** and validate statement type identification\n2. **Parse table reference** using existing table parsing infrastructure\n3. **Parse SET keyword** which introduces the assignment section\n4. **Parse assignment list** handling multiple column assignments separated by commas\n5. **Parse WHERE keyword** if present, as WHERE clauses are optional in UPDATE statements\n6. **Parse conditional expression** using existing WHERE clause parsing logic\n7. **Validate assignment targets** ensuring all assigned columns are valid identifiers\n\n**Assignment expression parsing** requires specialized logic since assignments use single equals (`=`) rather than comparison operators. The parser must distinguish between assignment context and boolean expression context to handle the equals sign correctly.\n\nThe assignment parsing process follows this sequence for each SET clause entry:\n\n1. **Parse target column identifier** which must be a simple or qualified column name\n2. **Expect assignment operator** (single equals sign) and consume the token\n3. **Parse assigned value expression** which can include literals, column references, arithmetic, and function calls\n4. **Handle comma separation** between multiple assignments while detecting end of SET clause\n\n**WHERE clause integration** reuses the expression parser from milestone 3, but the UPDATE parser must handle the **optional WHERE clause** carefully. When WHERE is omitted, the update applies to all rows in the table, which can be dangerous but is syntactically valid.\n\nThe parser implements **WHERE clause detection** by checking for the WHERE keyword after parsing all SET assignments. If found, it delegates to the expression parser. If not found, it sets the where_clause field to None and continues parsing any remaining statement components.\n\n> **Critical Safety Consideration**: UPDATE statements without WHERE clauses affect all table rows. While syntactically valid, this represents a common source of data modification errors. The parser should provide clear AST representation allowing downstream tools to detect and warn about such cases.\n\n### DELETE Statement Parsing\n\nDELETE statements represent the simplest DML syntax but require careful safety considerations due to their destructive nature. The basic form `DELETE FROM table WHERE condition` focuses parsing complexity on the conditional expression rather than structural validation.\n\nThe `DeleteStatement` AST node reflects this structural simplicity:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| table_reference | TableReference | Target table identifier |\n| where_clause | Optional[Expression] | Row selection condition or None for all rows |\n| source_location | SourceLocation | Position information for error reporting |\n\nThe parsing algorithm emphasizes safety validation alongside syntax processing:\n\n1. **Parse DELETE keyword** confirming statement type identification\n2. **Parse FROM keyword** which is required in standard SQL DELETE syntax\n3. **Parse table reference** identifying the target table for row deletion\n4. **Check for WHERE clause** by looking ahead for the WHERE keyword\n5. **Parse conditional expression** if WHERE clause is present\n6. **Validate statement completeness** ensuring no unexpected tokens remain\n7. **Flag safety concerns** when WHERE clause is omitted, indicating all-row deletion\n\n**Table reference parsing** for DELETE statements reuses the existing table parsing logic, but DELETE statements typically do not support table aliases since there's no column selection or JOIN operations involved.\n\n**WHERE clause handling** follows the same optional pattern as UPDATE statements. When present, the parser delegates to the expression parsing subsystem developed for milestone 3. When omitted, the parser sets where_clause to None, but this represents a **high-risk operation** that affects all table rows.\n\nThe parser implements **safety-conscious parsing** by including metadata in the AST that downstream tools can use to detect potentially dangerous operations:\n\n```markdown\nSafety Analysis Table:\n\n| WHERE Clause Status | Risk Level | AST Indicator | Recommended Action |\n|-------------------|------------|---------------|-------------------|\n| WHERE clause present | Low | where_clause field populated | Normal processing |\n| WHERE clause omitted | High | where_clause field is None | Warn or require confirmation |\n| WHERE with constant true | Medium | where_clause is BooleanLiteral(true) | Warn about full table deletion |\n| WHERE with impossible condition | Low | Expression analysis required | Optimize to no-op |\n```\n\n**Error detection** for DELETE statements focuses on structural validation rather than complex expression parsing, since the conditional logic reuses existing WHERE clause infrastructure. The primary parsing errors involve missing FROM keywords or malformed table references.\n\n> **Design Philosophy**: DELETE statement parsing prioritizes safety analysis over syntactic complexity. The AST structure enables downstream tools to implement appropriate safety checks and user confirmations before executing destructive operations.\n\n### Value List and Type Validation\n\nValue list parsing requires coordination between syntax analysis and semantic validation to ensure data integrity while building correct AST structures. The parser must handle **type-aware literal parsing** and **column-value correspondence checking** without requiring full schema information during the parsing phase.\n\n**Literal type inference** occurs during tokenization and parsing phases, where the parser identifies basic data types based on syntax patterns. However, full type validation requires schema information not available to the syntax parser, creating a separation of concerns between parsing and semantic analysis.\n\nThe parser handles several distinct literal types with different parsing requirements:\n\n| Literal Type | Token Recognition | AST Node Type | Validation Required |\n|-------------|------------------|---------------|-------------------|\n| Integer | Digit sequences | IntegerLiteral | Range checking |\n| Float | Decimal point + digits | FloatLiteral | Precision validation |\n| String | Quoted character sequences | StringLiteral | Escape sequence processing |\n| NULL | NULL keyword | NullLiteral | Context appropriateness |\n| Boolean | TRUE/FALSE keywords | BooleanLiteral | Logical consistency |\n\n**Column count validation** ensures that each value row in INSERT statements contains the same number of expressions as the column specification. This validation occurs during parsing since it involves purely structural consistency rather than semantic type checking.\n\nThe validation algorithm processes value lists using a two-phase approach:\n\n1. **Parse all value rows** into expression lists without type validation\n2. **Verify structural consistency** ensuring equal expression counts across all rows\n3. **Flag type validation requirements** in AST metadata for semantic analysis phase\n4. **Preserve position information** enabling detailed error reporting for type mismatches\n\n**Expression count validation** compares the length of each value row against either the explicit column list length or the implied column count from schema information:\n\n```markdown\nValidation Cases:\n\n| Column Specification | Value Row Count | Validation Result | Error Type |\n|--------------------|----------------|------------------|------------|\n| 3 explicit columns | 3 values | Valid | None |\n| 3 explicit columns | 2 values | Invalid | Too few values |\n| 3 explicit columns | 4 values | Invalid | Too many values |\n| No column list | Any count | Deferred | Schema validation required |\n| Empty column list | 0 values | Valid | No-op statement |\n```\n\n**Type hint preservation** ensures that the AST retains enough information for downstream semantic analysis to perform complete type validation. The parser includes literal type information and expression structure without requiring schema access.\n\nThe parser implements **progressive validation** where syntactic validation occurs immediately while semantic validation is deferred to later phases. This separation enables the parser to work without database connections while still providing meaningful error messages for structural problems.\n\n**Value expression complexity** requires the parser to handle not just simple literals but also arithmetic expressions, function calls, and column references within INSERT and UPDATE statements. These expressions use the same parsing infrastructure as WHERE clause expressions, ensuring consistent behavior across statement types.\n\n> **Architecture Principle**: The parser performs structural validation immediately but defers semantic type validation to later phases. This design enables parsing without schema access while preserving all information needed for comprehensive type checking.\n\n### Architecture Decision: Statement-Specific vs Unified Parser\n\nThe choice between specialized parsers for each statement type versus a single unified parser affects code organization, maintainability, and extensibility throughout the parser architecture.\n\n> **Decision: Statement-Specific Parser Classes**\n> - **Context**: DML statements (INSERT, UPDATE, DELETE) have significantly different syntax structures, validation requirements, and AST node types compared to SELECT statements and each other\n> - **Options Considered**: \n>   1. Single unified parser class with method dispatch based on statement type\n>   2. Statement-specific parser classes inheriting from common base\n>   3. Completely independent parser classes with no shared infrastructure\n> - **Decision**: Implement statement-specific parser classes (`SelectParser`, `DMLParser`) that inherit from a common `BaseParser` class\n> - **Rationale**: Each statement type has unique parsing requirements that benefit from specialized logic, while shared infrastructure (token management, error handling, expression parsing) reduces code duplication\n> - **Consequences**: Enables focused parser logic for each statement type while maintaining code reuse for common operations\n\nThe architecture decision analysis reveals several important trade-offs:\n\n| Approach | Pros | Cons | Code Organization Impact |\n|----------|------|------|-------------------------|\n| Unified Parser | Single class to maintain | Complex method dispatch | Monolithic class structure |\n| Statement-Specific | Focused logic per type | Potential code duplication | Modular class hierarchy |\n| Independent Parsers | Complete isolation | Maximum duplication | Separate file per parser |\n\n**Statement-specific parser architecture** provides the optimal balance between code organization and functionality. The `DMLParser` class handles INSERT, UPDATE, and DELETE statements while the `SelectParser` handles SELECT queries, with both inheriting common functionality from `BaseParser`.\n\nThe `BaseParser` class provides shared infrastructure:\n\n| Method | Purpose | Shared Behavior |\n|--------|---------|----------------|\n| `peek_token(offset)` | Token lookahead | Consistent across all statement types |\n| `consume_token()` | Token advancement | Uniform token consumption |\n| `expect_token(type)` | Validation and consumption | Standard error handling |\n| `create_source_location()` | Position tracking | Consistent error reporting |\n| `parse_expression()` | Expression parsing | Reused in WHERE, SET, VALUES clauses |\n\n**Specialized parser methods** in each parser class focus on statement-specific syntax:\n\nDMLParser specialized methods:\n\n| Method | Statement Type | Purpose |\n|--------|---------------|---------|\n| `parse_insert_statement()` | INSERT | Handle column lists and value rows |\n| `parse_update_statement()` | UPDATE | Handle SET assignments and WHERE clauses |\n| `parse_delete_statement()` | DELETE | Handle table references and WHERE clauses |\n| `parse_assignment_expression()` | UPDATE | Handle column = value assignments |\n| `parse_value_list()` | INSERT | Handle comma-separated value rows |\n\n**Parser coordination** occurs through the main `SQLParser` class which analyzes the initial keyword and delegates to the appropriate specialized parser:\n\n1. **Analyze statement type** by examining the first token (SELECT, INSERT, UPDATE, DELETE)\n2. **Instantiate appropriate parser** based on statement type detection\n3. **Delegate parsing** to the specialized parser instance\n4. **Return unified AST** with consistent node interfaces\n\nThis architecture enables **independent evolution** of each parser while maintaining **consistent interfaces** for error handling, AST construction, and token management.\n\n> **Design Benefit**: Statement-specific parsers allow each parser to focus on its unique requirements while sharing common infrastructure. This reduces complexity in each parser class while avoiding code duplication across parsers.\n\n### Common DML Parser Pitfalls\n\nData modification statement parsing introduces several categories of errors that differ from SELECT statement parsing challenges. Understanding these pitfalls helps developers avoid subtle bugs that can lead to incorrect AST construction or runtime failures.\n\n⚠️ **Pitfall: Column-Value Count Mismatch Detection**\n\nMany developers implement INSERT parsing without proper validation that the number of values matches the number of columns. The parser may successfully construct an AST that represents syntactically valid but semantically incorrect SQL.\n\n**Why it's wrong**: INSERT statements with mismatched column and value counts will fail at execution time with unclear error messages. The parser should catch this structural problem early to provide better error reporting.\n\n**How to fix**: Implement validation logic after parsing each value row that compares the expression count against the column list length. Store validation results in AST metadata for downstream error reporting.\n\n⚠️ **Pitfall: Assignment vs Comparison Operator Confusion**\n\nUPDATE statement parsing must distinguish between assignment operators (`=`) and comparison operators (`=`) based on context. Developers often implement a single equals handler that doesn't account for this contextual difference.\n\n**Why it's wrong**: The same symbol (`=`) has different meanings in SET clauses (assignment) versus WHERE clauses (comparison). Treating them identically leads to incorrect AST node types.\n\n**How to fix**: Implement separate parsing contexts for SET clauses and WHERE clauses. The SET clause parser should create `AssignmentExpression` nodes while the WHERE clause parser creates `BinaryOperation` nodes for the same token.\n\n⚠️ **Pitfall: Missing WHERE Clause Safety Validation**\n\nDELETE and UPDATE statements without WHERE clauses affect all table rows, which is often unintentional. Developers frequently parse these statements without flagging the safety implications.\n\n**Why it's wrong**: While syntactically valid, DELETE or UPDATE statements without WHERE clauses represent high-risk operations that users often execute accidentally.\n\n**How to fix**: Include safety metadata in the AST that indicates when WHERE clauses are omitted. Downstream tools can use this information to require user confirmation or implement safety checks.\n\n⚠️ **Pitfall: Multi-Row INSERT Parsing State Confusion**\n\nINSERT statements with multiple value rows require careful state management to distinguish between commas separating rows and commas separating values within rows. Developers often lose track of nesting level during parsing.\n\n**Why it's wrong**: Incorrect comma handling leads to malformed AST structures where value rows contain the wrong number of expressions or expressions span multiple rows.\n\n**How to fix**: Use explicit state tracking with parentheses counting to maintain parsing context. Each opening parenthesis increments nesting level, each closing parenthesis decrements, and comma handling changes based on current nesting level.\n\n⚠️ **Pitfall: Table Reference Aliasing in DML Statements**\n\nSome developers allow table aliases in DELETE and UPDATE statements without considering that aliases complicate column reference resolution and aren't typically needed for single-table operations.\n\n**Why it's wrong**: Table aliases in DML statements add complexity without providing significant benefit, and can confuse column reference parsing in SET and WHERE clauses.\n\n**How to fix**: Consider restricting table aliases in DML statements to simple table names only. If aliases are supported, ensure that column reference parsing in SET and WHERE clauses correctly resolves aliased table names.\n\n⚠️ **Pitfall: Type Validation During Parsing**\n\nDevelopers sometimes attempt to perform type validation during the parsing phase, checking that string literals are assigned to string columns or integer literals to integer columns.\n\n**Why it's wrong**: Type validation requires schema information that's not available during syntax parsing. Attempting type validation during parsing either fails or requires tight coupling between parser and database schema.\n\n**How to fix**: Separate syntactic parsing from semantic validation. The parser should construct correctly typed literal AST nodes based on syntax alone, leaving type compatibility checking to a later semantic analysis phase.\n\n⚠️ **Pitfall: Incomplete Error Recovery for DML Statements**\n\nDML statement parsing errors often leave the parser in an inconsistent state where subsequent statements cannot be parsed correctly, especially in batch processing scenarios.\n\n**Why it's wrong**: Poor error recovery prevents parsing multiple statements in sequence and makes debugging difficult when only the first error is reported.\n\n**How to fix**: Implement statement-level error recovery that can resynchronize on statement boundary keywords (INSERT, UPDATE, DELETE, SELECT) after encountering parsing errors.\n\n### Implementation Guidance\n\nThis subsection provides concrete Python implementation guidance for building DML statement parsers that integrate with the existing tokenizer and SELECT parser infrastructure from previous milestones.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| AST Node Implementation | Python dataclasses with type hints | Custom classes with visitor pattern support |\n| Error Handling | Exception hierarchy with message strings | Structured error objects with recovery suggestions |\n| Type Validation | Deferred to separate validation phase | Integrated type hints with schema validation |\n| Statement Dispatch | Simple if/elif chain on token type | Strategy pattern with parser registry |\n\n**B. Recommended File/Module Structure:**\n\n```\nsql_parser/\n  tokenizer.py              ← Token definitions and lexical analysis\n  ast_nodes.py              ← AST node classes for all statement types\n  base_parser.py            ← BaseParser with shared parsing infrastructure\n  select_parser.py          ← SelectParser class for SELECT statements\n  dml_parser.py             ← DMLParser class for INSERT/UPDATE/DELETE\n  expression_parser.py      ← ExpressionParser for WHERE/SET clause expressions\n  main_parser.py            ← SQLParser orchestrating all parser components\n  errors.py                 ← Exception hierarchy for parsing errors\n  tests/\n    test_dml_parser.py      ← Tests for DML statement parsing\n    test_integration.py     ← End-to-end parsing tests\n```\n\n**C. Infrastructure Starter Code (COMPLETE, ready to use):**\n\n```python\n# ast_nodes.py - DML statement AST node definitions\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Union, Any\nfrom abc import ABC, abstractmethod\n\n@dataclass\nclass SourceLocation:\n    start_line: int\n    start_column: int\n    end_line: int\n    end_column: int\n\n@dataclass\nclass ASTNode(ABC):\n    source_location: SourceLocation\n    \n    @abstractmethod\n    def node_type(self) -> str:\n        pass\n    \n    @abstractmethod\n    def children(self) -> List['ASTNode']:\n        pass\n\n@dataclass\nclass Identifier(ASTNode):\n    name: str\n    \n    def node_type(self) -> str:\n        return \"Identifier\"\n    \n    def children(self) -> List[ASTNode]:\n        return []\n\n@dataclass\nclass TableReference(ASTNode):\n    table: Union[Identifier, 'QualifiedIdentifier']\n    alias: Optional[str]\n    \n    def node_type(self) -> str:\n        return \"TableReference\"\n    \n    def children(self) -> List[ASTNode]:\n        return [self.table] if self.table else []\n\n@dataclass\nclass AssignmentExpression(ASTNode):\n    target_column: Identifier\n    assigned_value: 'Expression'\n    \n    def node_type(self) -> str:\n        return \"AssignmentExpression\"\n    \n    def children(self) -> List[ASTNode]:\n        return [self.target_column, self.assigned_value]\n\n@dataclass\nclass InsertStatement(ASTNode):\n    table_reference: TableReference\n    column_list: Optional[List[Identifier]]\n    value_rows: List[List['Expression']]\n    \n    def node_type(self) -> str:\n        return \"InsertStatement\"\n    \n    def children(self) -> List[ASTNode]:\n        children = [self.table_reference]\n        if self.column_list:\n            children.extend(self.column_list)\n        for row in self.value_rows:\n            children.extend(row)\n        return children\n\n@dataclass\nclass UpdateStatement(ASTNode):\n    table_reference: TableReference\n    set_assignments: List[AssignmentExpression]\n    where_clause: Optional['Expression']\n    \n    def node_type(self) -> str:\n        return \"UpdateStatement\"\n    \n    def children(self) -> List[ASTNode]:\n        children = [self.table_reference] + self.set_assignments\n        if self.where_clause:\n            children.append(self.where_clause)\n        return children\n\n@dataclass\nclass DeleteStatement(ASTNode):\n    table_reference: TableReference\n    where_clause: Optional['Expression']\n    \n    def node_type(self) -> str:\n        return \"DeleteStatement\"\n    \n    def children(self) -> List[ASTNode]:\n        children = [self.table_reference]\n        if self.where_clause:\n            children.append(self.where_clause)\n        return children\n\n# Type alias for expression nodes (defined in expression_parser.py)\nExpression = Union['BinaryOperation', 'UnaryOperation', 'Identifier', \n                  'StringLiteral', 'IntegerLiteral', 'FloatLiteral', 'NullLiteral']\n```\n\n```python\n# errors.py - Exception hierarchy for DML parsing\nclass ParseError(Exception):\n    def __init__(self, message: str, line: int, column: int):\n        self.message = message\n        self.line = line\n        self.column = column\n        super().__init__(f\"Parse error at line {line}, column {column}: {message}\")\n\nclass SyntaxError(ParseError):\n    pass\n\nclass UnexpectedTokenError(SyntaxError):\n    def __init__(self, expected: str, actual_token: 'Token'):\n        self.expected = expected\n        self.actual_token = actual_token\n        message = f\"Expected {expected}, got {actual_token.type.name} '{actual_token.value}'\"\n        super().__init__(message, actual_token.line, actual_token.column)\n\nclass StructuralValidationError(ParseError):\n    def __init__(self, message: str, source_location: SourceLocation):\n        super().__init__(message, source_location.start_line, source_location.start_column)\n        self.source_location = source_location\n```\n\n**D. Core Logic Skeleton Code (signature + TODOs only):**\n\n```python\n# dml_parser.py - DML statement parser implementation\nfrom typing import List, Optional\nfrom base_parser import BaseParser\nfrom ast_nodes import *\nfrom tokenizer import TokenType, Token\nfrom expression_parser import ExpressionParser\n\nclass DMLParser(BaseParser):\n    \"\"\"Parser for INSERT, UPDATE, DELETE statements.\"\"\"\n    \n    def __init__(self, tokens: List[Token]):\n        super().__init__(tokens)\n        self.expression_parser = ExpressionParser(tokens)\n    \n    def parse_insert_statement(self) -> InsertStatement:\n        \"\"\"Parse INSERT INTO table [(columns)] VALUES (values) statement.\"\"\"\n        # TODO 1: Consume and validate INSERT keyword token\n        # TODO 2: Consume and validate INTO keyword token  \n        # TODO 3: Parse table reference using parse_table_reference()\n        # TODO 4: Check for optional column list by looking ahead for LPAREN\n        # TODO 5: If column list present, parse comma-separated identifiers\n        # TODO 6: Consume and validate VALUES keyword\n        # TODO 7: Parse value row list with parse_value_row_list()\n        # TODO 8: Validate column count matches value count for each row\n        # TODO 9: Create and return InsertStatement AST node\n        # Hint: Use self.peek_token() to check for optional column list\n        pass\n    \n    def parse_update_statement(self) -> UpdateStatement:\n        \"\"\"Parse UPDATE table SET assignments [WHERE condition] statement.\"\"\"\n        # TODO 1: Consume and validate UPDATE keyword token\n        # TODO 2: Parse table reference using parse_table_reference()\n        # TODO 3: Consume and validate SET keyword token\n        # TODO 4: Parse assignment list with parse_assignment_list()\n        # TODO 5: Check for optional WHERE clause by looking ahead\n        # TODO 6: If WHERE present, parse condition with expression_parser\n        # TODO 7: Create and return UpdateStatement AST node\n        # Hint: WHERE clauses are optional in UPDATE statements\n        pass\n    \n    def parse_delete_statement(self) -> DeleteStatement:\n        \"\"\"Parse DELETE FROM table [WHERE condition] statement.\"\"\"\n        # TODO 1: Consume and validate DELETE keyword token\n        # TODO 2: Consume and validate FROM keyword token\n        # TODO 3: Parse table reference using parse_table_reference()\n        # TODO 4: Check for optional WHERE clause by looking ahead\n        # TODO 5: If WHERE present, parse condition with expression_parser\n        # TODO 6: Flag safety concern if WHERE clause is missing\n        # TODO 7: Create and return DeleteStatement AST node\n        # Hint: Consider adding safety metadata to AST for missing WHERE\n        pass\n    \n    def parse_assignment_list(self) -> List[AssignmentExpression]:\n        \"\"\"Parse comma-separated list of column = value assignments.\"\"\"\n        # TODO 1: Initialize empty assignment list\n        # TODO 2: Parse first assignment with parse_assignment_expression()\n        # TODO 3: Add first assignment to list\n        # TODO 4: While next token is COMMA, continue parsing assignments\n        # TODO 5: For each comma, consume it and parse next assignment\n        # TODO 6: Return completed assignment list\n        # Hint: Similar structure to column list parsing but creates AssignmentExpression nodes\n        pass\n    \n    def parse_assignment_expression(self) -> AssignmentExpression:\n        \"\"\"Parse single column = value assignment.\"\"\"\n        # TODO 1: Parse target column as Identifier\n        # TODO 2: Expect and consume EQUALS token for assignment\n        # TODO 3: Parse assigned value expression using expression_parser\n        # TODO 4: Create source location from target column to end of value\n        # TODO 5: Return AssignmentExpression AST node\n        # Hint: Assignment equals is different from comparison equals in context\n        pass\n    \n    def parse_value_row_list(self) -> List[List[Expression]]:\n        \"\"\"Parse comma-separated list of (value, value, ...) rows.\"\"\"\n        # TODO 1: Initialize empty row list\n        # TODO 2: Parse first value row with parse_value_row()\n        # TODO 3: Add first row to list\n        # TODO 4: While next token is COMMA, continue parsing rows\n        # TODO 5: For each comma, consume it and parse next row\n        # TODO 6: Return completed row list\n        # Hint: Each row is parenthesized list of expressions\n        pass\n    \n    def parse_value_row(self) -> List[Expression]:\n        \"\"\"Parse single (value, value, ...) row.\"\"\"\n        # TODO 1: Expect and consume LPAREN token\n        # TODO 2: Initialize empty value list\n        # TODO 3: Parse first value expression using expression_parser\n        # TODO 4: Add first value to list\n        # TODO 5: While next token is COMMA, continue parsing values\n        # TODO 6: For each comma, consume it and parse next value\n        # TODO 7: Expect and consume RPAREN token\n        # TODO 8: Return completed value list\n        # Hint: Handle empty value lists for INSERT () VALUES () syntax\n        pass\n    \n    def validate_column_value_consistency(self, column_count: int, value_rows: List[List[Expression]]) -> None:\n        \"\"\"Validate that each value row has correct number of expressions.\"\"\"\n        # TODO 1: Iterate through each value row in value_rows\n        # TODO 2: For each row, compare len(row) with column_count\n        # TODO 3: If counts don't match, raise StructuralValidationError\n        # TODO 4: Include row number and expected vs actual counts in error message\n        # TODO 5: Include source location information for precise error reporting\n        # Hint: This catches INSERT structural errors early with good error messages\n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- **Token Type Checking**: Use `token.type == TokenType.INSERT_KEYWORD` for exact token type matching\n- **Optional Parsing**: Use `self.peek_token(0).type == TokenType.WHERE_KEYWORD` to detect optional clauses\n- **Error Recovery**: Wrap parsing operations in try/except blocks to catch and re-raise with additional context\n- **List Comprehensions**: Use `[self.parse_expression() for _ in range(value_count)]` for parsing known-length lists\n- **Type Annotations**: Include full type hints for better IDE support: `def parse_insert_statement(self) -> InsertStatement:`\n- **Source Location Tracking**: Use `self.create_source_location(start_token, self.current_token)` for position information\n- **Expression Parser Integration**: Call `self.expression_parser.parse_expression()` for WHERE and value expressions\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the DML parser, verify correct behavior with these tests:\n\n**INSERT Statement Testing:**\n```python\n# Test case: Basic INSERT with explicit columns\nsql = \"INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com')\"\nast = parser.parse(sql)\nassert isinstance(ast, InsertStatement)\nassert ast.table_reference.table.name == \"users\"\nassert len(ast.column_list) == 2\nassert ast.column_list[0].name == \"name\"\nassert len(ast.value_rows) == 1\nassert len(ast.value_rows[0]) == 2\n```\n\n**UPDATE Statement Testing:**\n```python\n# Test case: UPDATE with WHERE clause\nsql = \"UPDATE users SET email = 'newemail@example.com' WHERE id = 1\"\nast = parser.parse(sql)\nassert isinstance(ast, UpdateStatement)\nassert len(ast.set_assignments) == 1\nassert ast.set_assignments[0].target_column.name == \"email\"\nassert ast.where_clause is not None\n```\n\n**DELETE Statement Testing:**\n```python\n# Test case: DELETE with WHERE clause\nsql = \"DELETE FROM users WHERE active = false\"\nast = parser.parse(sql)\nassert isinstance(ast, DeleteStatement)\nassert ast.where_clause is not None\n\n# Test case: DELETE without WHERE (should parse but flag safety concern)\nsql = \"DELETE FROM temp_table\"\nast = parser.parse(sql)\nassert ast.where_clause is None  # Safety concern flagged\n```\n\n**Multi-Row INSERT Testing:**\n```python\n# Test case: Multiple row INSERT\nsql = \"INSERT INTO users (name, age) VALUES ('Alice', 25), ('Bob', 30)\"\nast = parser.parse(sql)\nassert len(ast.value_rows) == 2\nassert len(ast.value_rows[0]) == 2\nassert len(ast.value_rows[1]) == 2\n```\n\n**G. Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"Expected VALUES keyword\" error | Missing or misspelled VALUES in INSERT | Check token stream after table reference | Verify tokenizer recognizes VALUES as keyword |\n| Assignment parsing fails | Confusion between assignment = and comparison = | Print token types during SET clause parsing | Use context-specific parsing for SET vs WHERE |\n| Multi-row INSERT creates wrong AST | Comma separation logic incorrect | Print nesting level during value parsing | Track parentheses depth to distinguish comma contexts |\n| WHERE clause not parsed in UPDATE | Optional WHERE logic missing | Check lookahead token after SET clause | Add proper optional clause detection |\n| Column count validation fails | Off-by-one errors in list length comparison | Print actual vs expected counts | Verify column list length calculation |\n\n\n## Error Handling and Recovery\n\n> **Milestone(s):** This section applies to all milestones (1-4), providing essential error handling strategies that ensure our parser produces meaningful feedback when SQL statements contain syntax errors or malformed constructs.\n\n### Mental Model: Medical Diagnosis and Treatment\n\nThink of parser error handling like a medical diagnosis system. When a patient visits a doctor with symptoms, the doctor doesn't just say \"you're sick\" and stop there. Instead, they perform a systematic examination to identify the specific problem, explain what's wrong in terms the patient can understand, suggest what might have caused it, and recommend treatment options. Similarly, our SQL parser must act as a diagnostic system for malformed queries.\n\nWhen the parser encounters invalid SQL, it should identify the specific syntax error, explain what was expected versus what was found, pinpoint the exact location in the query, suggest likely causes, and ideally continue analyzing the rest of the query to find additional problems. Just as a doctor might discover multiple health issues during a single examination, our parser should strive to identify multiple syntax errors in one pass rather than forcing the developer to fix errors one at a time.\n\nThe key insight is that error handling isn't just about detecting problems—it's about providing enough information for the developer to understand and fix the issue quickly. A parser that simply says \"syntax error\" is like a doctor who says \"you're unwell\" without any further explanation or guidance.\n\n### Parser Error Categories\n\nOur SQL parser must handle several distinct categories of errors, each requiring different detection strategies and recovery approaches. Understanding these categories helps us design appropriate error handling mechanisms and provide targeted error messages.\n\n![Parser Error Handling Flow](./diagrams/error-handling-flow.svg)\n\nThe following table categorizes the different types of errors our parser will encounter, organized by the component that detects them and their characteristics:\n\n| Error Category | Detection Component | Timing | Recovery Difficulty | Example Scenarios |\n|---|---|---|---|---|\n| Tokenization Errors | Tokenizer | During lexical analysis | Low - local to single token | Unterminated strings, invalid characters, malformed numbers |\n| Syntax Errors | Parser | During parsing | Medium - affects statement structure | Missing keywords, unexpected tokens, malformed expressions |\n| Structural Errors | Parser | During AST construction | High - affects statement semantics | Column/value count mismatch, invalid table references |\n| Precedence Errors | Expression Parser | During expression parsing | Medium - affects expression tree | Mismatched parentheses, operator precedence violations |\n| Recovery Errors | Error Recovery | After error detection | Variable - depends on context | Failed synchronization, cascade failures |\n\n#### Tokenization Error Types\n\n**Tokenization errors** occur during lexical analysis when the tokenizer encounters character sequences that cannot be classified into valid token types. These errors are typically the easiest to detect and report because they affect only a single token and don't propagate through the parsing process.\n\nThe `TokenizerError` class extends `ParseError` to capture tokenization-specific information:\n\n| Field | Type | Description |\n|---|---|---|\n| message | str | Human-readable description of the tokenization problem |\n| line | int | Line number where the error occurred (1-based) |\n| column | int | Column position within the line (1-based) |\n| invalid_sequence | str | The character sequence that caused the error |\n| suggestion | Optional[str] | Suggested fix or likely intended token |\n\nCommon tokenization errors include unterminated string literals where the closing quote is missing, invalid escape sequences within strings, malformed numeric literals with multiple decimal points, and unrecognized character sequences that don't match any token pattern. The tokenizer can often provide specific suggestions for these errors because the context is limited and the intended token is usually obvious.\n\n#### Syntax Error Types\n\n**Syntax errors** occur during parsing when the token sequence doesn't match the expected grammar rules for SQL statements. These errors are more complex than tokenization errors because they involve the relationship between multiple tokens and the overall structure of the statement.\n\nThe `SyntaxError` class provides the foundation for syntax error reporting:\n\n| Field | Type | Description |\n|---|---|---|\n| message | str | Description of the syntax violation |\n| line | int | Line number where parsing failed |\n| column | int | Column position where parsing failed |\n| error_token | Token | The token that caused the parsing failure |\n| parser_state | str | Description of what the parser was trying to parse |\n\nThe `UnexpectedTokenError` subclass handles the most common syntax error scenario where the parser encounters a token different from what the grammar rules expect:\n\n| Field | Type | Description |\n|---|---|---|\n| expected | str | Description of what token types were expected |\n| actual_token | Token | The token that was actually encountered |\n| context | str | Description of the parsing context (e.g., \"parsing SELECT clause\") |\n| suggestions | List[str] | Possible corrections or likely intended tokens |\n\n#### Structural Validation Errors\n\n**Structural validation errors** represent violations of SQL's semantic rules that can be detected without schema information. These errors involve the logical consistency of the parsed statement structure rather than pure syntax violations.\n\nThe `StructuralValidationError` class captures these higher-level consistency problems:\n\n| Field | Type | Description |\n|---|---|---|\n| message | str | Description of the structural inconsistency |\n| source_location | SourceLocation | Span of tokens involved in the error |\n| violation_type | str | Category of structural rule violated |\n| related_elements | List[ASTNode] | AST nodes involved in the inconsistency |\n\nExamples include INSERT statements where the number of columns in the column list doesn't match the number of values in each value row, UPDATE statements with SET clauses that reference non-existent columns, and DELETE statements with malformed WHERE clauses that would affect statement safety.\n\n> **Architecture Decision: Error Type Hierarchy**\n> - **Context**: We need to represent different categories of parsing errors with appropriate detail and recovery information\n> - **Options Considered**: \n>   - Single generic error class with type field\n>   - Inheritance hierarchy with specialized error classes\n>   - Error union types with tagged variants\n> - **Decision**: Use inheritance hierarchy with specialized error classes\n> - **Rationale**: Inheritance allows each error type to carry appropriate contextual information while maintaining type safety and enabling specific error handling strategies\n> - **Consequences**: More complex type system but better error reporting and recovery capabilities\n\n### Error Message Design\n\nEffective error messages are crucial for developer productivity. Our parser's error messages must provide enough information for developers to understand what went wrong, why it's a problem, and how to fix it. The design of error messages should follow principles of clarity, specificity, and actionability.\n\n#### Error Message Components\n\nEach error message should contain several key components that together provide a complete picture of the problem. The structure ensures consistency across different error types and gives developers the information they need to resolve issues quickly.\n\nThe core error message format includes these elements:\n\n| Component | Purpose | Example |\n|---|---|---|\n| Error Type | Categorize the kind of problem | \"Syntax Error\", \"Tokenization Error\", \"Structural Error\" |\n| Location Information | Pinpoint where the error occurred | \"line 3, column 15\" |\n| Problem Description | Explain what went wrong | \"Expected FROM keyword after column list\" |\n| Context Information | Describe what the parser was doing | \"while parsing SELECT statement\" |\n| Actual vs Expected | Show the mismatch | \"found IDENTIFIER 'users', expected FROM\" |\n| Suggestion | Provide actionable fix | \"Try: SELECT name FROM users\" |\n\n#### Position Information and Context\n\nAccurate position information is essential for developers to locate errors quickly, especially in complex queries. Our error reporting system tracks both character-level position and structural context within the SQL statement.\n\nThe `SourceLocation` type captures precise position information:\n\n| Field | Type | Description |\n|---|---|---|\n| start_line | int | Starting line number (1-based indexing) |\n| start_column | int | Starting column position (1-based indexing) |\n| end_line | int | Ending line number for multi-token errors |\n| end_column | int | Ending column position for error spans |\n\nPosition tracking must account for SQL's formatting conventions. Many SQL queries are formatted across multiple lines with indentation, and developers expect error messages to reference the visual layout they see in their editor. The tokenizer maintains line and column counters that increment appropriately for newline characters and tab expansion.\n\nContext information helps developers understand not just where the error occurred, but what the parser was attempting to accomplish when it failed. This is particularly important for complex parsing scenarios where the immediate error location might not be the actual source of the problem.\n\n#### Suggestion Generation Strategies\n\nHigh-quality error messages include actionable suggestions that help developers fix problems quickly. Our suggestion generation uses several strategies based on the type of error and the parsing context.\n\n**Keyword Suggestion Strategy** uses fuzzy matching to identify likely intended keywords when the parser encounters unexpected identifiers. For example, if the parser expects a FROM keyword but finds \"FORM\", the suggestion system can identify this as a likely typo and suggest the correction.\n\n**Token Sequence Analysis** examines the tokens surrounding an error to infer the developer's intent. If a SELECT statement is missing a comma between column names, the parser can detect the pattern and suggest inserting the missing comma.\n\n**Grammar-Based Suggestions** use knowledge of SQL grammar rules to propose valid continuations when the parser encounters unexpected end-of-input or invalid token sequences. When parsing reaches an incomplete statement, the suggestion system can list the valid ways to complete the current grammar rule.\n\n**Common Pattern Recognition** identifies frequent error patterns and provides targeted suggestions. For example, when developers write `SELECT *` without a FROM clause, the suggestion system can recognize this common mistake and prompt for the missing table specification.\n\nThe suggestion generation process follows this algorithm:\n\n1. **Analyze the error context** to determine what grammar rule was being parsed and what tokens were expected\n2. **Examine surrounding tokens** to identify patterns that might indicate the developer's intent\n3. **Apply fuzzy matching** to identify likely corrections for misspelled keywords or identifiers\n4. **Generate multiple suggestions** ranked by likelihood, with the most probable correction listed first\n5. **Validate suggestions** by checking that the proposed corrections would allow parsing to continue successfully\n\n> **Key Design Principle: Progressive Disclosure**\n> \n> Error messages should provide essential information immediately while allowing developers to access additional detail when needed. The primary error message should be concise and actionable, with supplementary information available for complex debugging scenarios.\n\n#### Example Error Message Formats\n\nThe following examples illustrate how different error categories produce structured, helpful error messages:\n\n**Tokenization Error Example:**\n```\nTokenization Error at line 2, column 18:\n  Unterminated string literal starting with single quote\n  \n  SELECT name, 'John Doe FROM users;\n                       ^\n  Expected: Closing single quote (') before end of statement\n  Suggestion: Add closing quote → SELECT name, 'John Doe' FROM users;\n```\n\n**Syntax Error Example:**\n```\nSyntax Error at line 1, column 21:\n  Expected FROM keyword after column list in SELECT statement\n  \n  SELECT name, email users WHERE active = 1;\n                     ^\n  Found: IDENTIFIER 'users'\n  Expected: FROM keyword\n  Suggestion: INSERT missing FROM → SELECT name, email FROM users WHERE active = 1;\n```\n\n**Structural Error Example:**\n```\nStructural Error at line 3, column 1:\n  INSERT statement column count mismatch\n  \n  INSERT INTO users (name, email) \n  VALUES ('John', 'john@example.com', 'active');\n         ^\n  Column list specifies 2 columns but VALUES clause provides 3 values\n  Suggestion: Either add 'status' to column list or remove 'active' from VALUES\n```\n\n### Error Recovery Techniques\n\nError recovery allows the parser to continue analyzing SQL statements after encountering errors, enabling the detection of multiple problems in a single parse attempt. Effective error recovery improves developer productivity by reducing the edit-compile-test cycle time when fixing complex SQL syntax errors.\n\n#### Recovery Strategy Overview\n\nParser error recovery involves two main challenges: **detecting that an error has occurred** and **resynchronizing the parser state** to continue parsing from a known good position. The recovery strategy must balance between finding additional errors and avoiding cascade failures where one error causes spurious additional error reports.\n\nOur parser implements a **panic-mode recovery** strategy combined with **synchronization points** that represent reliable positions where parsing can safely resume. This approach provides good error detection coverage while minimizing false positive error reports.\n\n#### Synchronization Points and Panic Mode\n\n**Synchronization points** are token positions where the parser can confidently resume parsing after an error. These points correspond to major statement boundaries and clause beginnings where the parser's internal state can be reset to a known configuration.\n\nThe following table identifies key synchronization points for different statement types:\n\n| Statement Context | Synchronization Tokens | Recovery Action |\n|---|---|---|\n| Statement boundary | Semicolon, EOF | Reset to statement-level parsing |\n| SELECT statement | FROM, WHERE, ORDER, GROUP | Skip to clause and resume |\n| INSERT statement | VALUES, SELECT | Skip to data specification |\n| UPDATE statement | SET, WHERE | Skip to assignment or condition |\n| DELETE statement | WHERE | Skip to condition specification |\n| Expression parsing | Comma, parentheses | Reset expression parser state |\n\n**Panic mode recovery** activates when the parser encounters an unexpected token that prevents normal parsing from continuing. The recovery algorithm follows these steps:\n\n1. **Record the error** with complete position and context information, ensuring the original error is preserved for reporting\n2. **Enter panic mode** by setting a recovery flag that changes the parser's behavior to focus on finding synchronization points\n3. **Skip tokens** systematically until reaching a reliable synchronization point where parsing can resume\n4. **Reset parser state** to match the synchronization point context, clearing any partial parse results that might be inconsistent\n5. **Resume normal parsing** from the synchronization point, continuing to build the AST for the remainder of the statement\n\nThe panic mode algorithm must be careful to avoid **infinite loops** where the parser repeatedly encounters errors without making progress. Recovery includes a **maximum error threshold** that prevents runaway error detection and ensures parsing terminates even for severely malformed input.\n\n#### Cascade Error Prevention\n\n**Cascade errors** occur when an initial parsing error causes subsequent spurious error reports as the parser attempts to continue with inconsistent state. Preventing cascade errors is crucial for producing useful error reports that help developers identify the actual problems in their SQL statements.\n\nOur cascade prevention strategy uses several techniques:\n\n**Error Context Marking** tracks which parts of the input have been affected by error recovery. When the parser resumes from a synchronization point, it marks the recovered section as potentially unreliable for further error reporting. Subsequent errors in marked regions are classified as possible cascade effects and reported with lower confidence.\n\n**State Validation** performs consistency checks after error recovery to ensure the parser's internal state matches the synchronization point assumptions. If state validation fails, the parser resets more aggressively rather than continuing with potentially corrupted state.\n\n**Error Suppression Heuristics** identify common patterns of cascade errors and suppress obvious false positives. For example, if a SELECT statement is missing a FROM keyword, the parser might misinterpret the table name as a second column name, leading to cascade errors in the WHERE clause. The suppression system recognizes this pattern and avoids reporting the secondary errors.\n\n**Recovery Distance Tracking** measures how far parsing has progressed since the last error recovery. Errors that occur immediately after recovery are more likely to be cascade effects, while errors that occur after successfully parsing several tokens are more likely to be independent problems.\n\n> **Architecture Decision: Panic Mode vs Local Recovery**\n> - **Context**: Need to choose error recovery strategy that balances error detection with parsing robustness\n> - **Options Considered**:\n>   - Panic mode with synchronization points\n>   - Local recovery with token insertion/deletion\n>   - Backtracking with alternative grammar rules\n> - **Decision**: Panic mode with synchronization points\n> - **Rationale**: Simpler to implement correctly, less prone to cascade errors, works well with SQL's structured syntax\n> - **Consequences**: May miss some recoverable errors but provides more reliable error reporting overall\n\n#### Recovery State Management\n\nEffective error recovery requires careful management of the parser's internal state to ensure that recovery doesn't introduce inconsistencies that affect subsequent parsing. The parser maintains several state components that must be coordinated during recovery.\n\nThe parser state includes these key elements:\n\n| State Component | Description | Recovery Action |\n|---|---|---|\n| Token Position | Current position in token stream | Advance to synchronization point |\n| Parse Stack | Recursive descent call context | Unwind to appropriate level |\n| AST Construction | Partially built syntax tree | Mark incomplete nodes, continue building |\n| Symbol Context | Current parsing context (statement type, clause) | Reset to synchronization context |\n| Error Flags | Recovery mode and error suppression state | Update based on recovery success |\n\n**Parse Stack Management** during error recovery must carefully unwind recursive descent parser calls to reach the appropriate level for the synchronization point. If an error occurs deep within expression parsing, the recovery mechanism must unwind through multiple call levels to reach statement-level parsing where synchronization can occur safely.\n\n**AST Node Handling** during recovery presents a challenge because partially constructed AST nodes may be incomplete or inconsistent. Our strategy marks incomplete nodes with special error indicators rather than discarding them entirely. This preserves structural information that might be useful for error reporting while preventing incomplete nodes from affecting semantic analysis.\n\n**Recovery Validation** occurs after each successful synchronization to ensure the parser has returned to a consistent state. The validation process checks that the current token position aligns with the expected synchronization point, the parse stack depth is appropriate for the current context, and any partially constructed AST nodes are properly marked as incomplete.\n\n#### Multi-Error Reporting Strategy\n\nWhen error recovery successfully identifies multiple problems in a single SQL statement, the parser must present these errors in a way that helps developers prioritize and fix them efficiently. The multi-error reporting strategy balances completeness with usability.\n\n**Error Prioritization** ranks detected errors by their likely impact on statement correctness and their independence from other errors. Primary syntax errors that prevent basic statement structure recognition receive highest priority, while potential cascade errors or stylistic issues receive lower priority.\n\n**Error Grouping** organizes related errors to avoid overwhelming developers with redundant information. If a missing FROM keyword causes multiple subsequent parsing problems, the error report groups these issues and identifies the FROM keyword as the primary fix.\n\n**Fix Suggestion Coordination** ensures that suggested fixes for multiple errors are compatible with each other. The suggestion system validates that applying all recommended fixes would result in syntactically valid SQL rather than introducing new conflicts.\n\nThe multi-error report format presents errors in order of priority while showing the relationships between different problems:\n\n1. **Primary errors** that represent fundamental syntax violations affecting statement structure\n2. **Secondary errors** that might be cascade effects but could also be independent problems  \n3. **Suggestions** that address multiple errors simultaneously when possible\n4. **Recovery summary** indicating which parts of the statement were successfully parsed despite errors\n\n### Implementation Guidance\n\nThis section provides concrete implementation patterns and starter code for building robust error handling into your SQL parser. The error handling system serves as the foundation for user-friendly parser feedback and reliable error recovery.\n\n#### Technology Recommendations\n\n| Error Handling Aspect | Simple Option | Advanced Option |\n|---|---|---|\n| Error Classes | Simple inheritance with string messages | Rich error objects with structured data and recovery hints |\n| Position Tracking | Line/column counters in tokenizer | Source span tracking with original text preservation |\n| Error Recovery | Basic panic mode with semicolon sync | Multi-level synchronization with context-aware recovery |\n| Message Formatting | Template strings with substitution | Structured error builders with suggestion generation |\n| Multiple Error Handling | Collect errors in list, report at end | Streaming error reporting with cascade detection |\n\n#### Recommended File Structure\n\n```\nsrc/\n  parser/\n    errors.py                    ← Error class hierarchy and utilities\n    error_recovery.py           ← Recovery strategies and synchronization\n    error_reporter.py           ← Message formatting and multi-error handling\n    position_tracker.py         ← Source location and context tracking\n  tokenizer/\n    tokenizer.py                ← Main tokenizer with error detection\n  parser/\n    base_parser.py              ← Base parser with error handling integration\n    select_parser.py            ← SELECT parser with recovery points\n    expression_parser.py        ← Expression parser with precedence error handling\n    dml_parser.py              ← DML parsers with structural validation\n  tests/\n    test_error_handling.py      ← Comprehensive error handling test suite\n```\n\n#### Complete Error Class Infrastructure\n\nHere's the complete error handling infrastructure that provides the foundation for all parser error reporting:\n\n```python\nfrom typing import Optional, List, Any\nfrom enum import Enum\nfrom dataclasses import dataclass\n\nclass ErrorSeverity(Enum):\n    ERROR = \"error\"\n    WARNING = \"warning\"\n    INFO = \"info\"\n\n@dataclass\nclass SourceLocation:\n    start_line: int\n    start_column: int\n    end_line: int\n    end_column: int\n    \n    def __str__(self) -> str:\n        if self.start_line == self.end_line:\n            return f\"line {self.start_line}, column {self.start_column}\"\n        return f\"lines {self.start_line}-{self.end_line}\"\n    \n    def span_length(self) -> int:\n        if self.start_line == self.end_line:\n            return self.end_column - self.start_column\n        return 1  # Multi-line spans simplified\n\nclass ParseError(Exception):\n    def __init__(self, message: str, line: int, column: int, \n                 severity: ErrorSeverity = ErrorSeverity.ERROR):\n        super().__init__(message)\n        self.message = message\n        self.line = line\n        self.column = column\n        self.severity = severity\n        self.suggestion: Optional[str] = None\n        self.context: Optional[str] = None\n    \n    def with_suggestion(self, suggestion: str) -> 'ParseError':\n        self.suggestion = suggestion\n        return self\n    \n    def with_context(self, context: str) -> 'ParseError':\n        self.context = context\n        return self\n    \n    def source_location(self) -> SourceLocation:\n        return SourceLocation(self.line, self.column, self.line, self.column + 1)\n\nclass TokenizerError(ParseError):\n    def __init__(self, message: str, line: int, column: int, \n                 invalid_sequence: str = \"\"):\n        super().__init__(f\"Tokenization Error: {message}\", line, column)\n        self.invalid_sequence = invalid_sequence\n\nclass SyntaxError(ParseError):\n    def __init__(self, message: str, line: int, column: int, \n                 parser_state: str = \"\"):\n        super().__init__(f\"Syntax Error: {message}\", line, column)\n        self.parser_state = parser_state\n\nclass UnexpectedTokenError(SyntaxError):\n    def __init__(self, expected: str, actual_token: 'Token', \n                 context: str = \"\"):\n        message = f\"Expected {expected}, found {actual_token.type.name} '{actual_token.value}'\"\n        super().__init__(message, actual_token.line, actual_token.column, context)\n        self.expected = expected\n        self.actual_token = actual_token\n        self.suggestions: List[str] = []\n\nclass StructuralValidationError(ParseError):\n    def __init__(self, message: str, source_location: SourceLocation, \n                 violation_type: str = \"\"):\n        super().__init__(f\"Structural Error: {message}\", \n                        source_location.start_line, source_location.start_column)\n        self.source_location = source_location\n        self.violation_type = violation_type\n        self.related_elements: List[Any] = []\n```\n\n#### Error Recovery Infrastructure\n\n```python\nfrom typing import Set, Dict, Callable\nfrom enum import Enum\n\nclass RecoveryMode(Enum):\n    NORMAL = \"normal\"\n    PANIC = \"panic\"\n    SUPPRESSED = \"suppressed\"\n\nclass SynchronizationPoint:\n    def __init__(self, token_types: Set[TokenType], \n                 recovery_action: Callable[[], None]):\n        self.token_types = token_types\n        self.recovery_action = recovery_action\n        self.confidence_level = 1.0\n\nclass ErrorRecoveryManager:\n    def __init__(self):\n        self.recovery_mode = RecoveryMode.NORMAL\n        self.error_count = 0\n        self.max_errors = 10\n        self.synchronization_points: Dict[str, SynchronizationPoint] = {}\n        self.cascade_suppression_distance = 3\n        self.tokens_since_recovery = 0\n    \n    def register_sync_point(self, name: str, sync_point: SynchronizationPoint):\n        # TODO: Register a synchronization point for error recovery\n        # TODO: Validate that token types are appropriate for synchronization\n        pass\n    \n    def enter_panic_mode(self, error: ParseError):\n        # TODO: Set recovery mode to PANIC\n        # TODO: Record error for later reporting\n        # TODO: Initialize recovery state tracking\n        pass\n    \n    def attempt_synchronization(self, current_token: Token) -> bool:\n        # TODO: Check if current token matches any synchronization point\n        # TODO: Execute recovery action if synchronization point found\n        # TODO: Reset to normal parsing mode on successful sync\n        # TODO: Return True if synchronization successful, False otherwise\n        pass\n    \n    def should_suppress_error(self, error: ParseError) -> bool:\n        # TODO: Check if error is likely cascade effect\n        # TODO: Consider distance from last recovery point\n        # TODO: Apply suppression heuristics based on error type\n        # TODO: Return True if error should be suppressed\n        pass\n```\n\n#### Core Parser Error Integration Skeleton\n\n```python\nclass BaseParser:\n    def __init__(self, tokens: List[Token]):\n        self.tokens = tokens\n        self.position = 0\n        self.current_token = tokens[0] if tokens else None\n        self.errors: List[ParseError] = []\n        self.recovery_manager = ErrorRecoveryManager()\n        self._setup_synchronization_points()\n    \n    def _setup_synchronization_points(self):\n        # TODO: Register synchronization points for statement boundaries\n        # TODO: Add synchronization for major clause keywords (FROM, WHERE, etc.)\n        # TODO: Configure recovery actions for each synchronization type\n        pass\n    \n    def report_error(self, error: ParseError):\n        # TODO: Check if error should be suppressed due to cascade effects\n        # TODO: Add error to error list if not suppressed\n        # TODO: Enter panic mode recovery if error is severe\n        # TODO: Update recovery statistics\n        pass\n    \n    def expect_token(self, expected_type: TokenType) -> Token:\n        # TODO: Check if current token matches expected type\n        # TODO: If match, return token and advance position\n        # TODO: If no match, create UnexpectedTokenError with suggestion\n        # TODO: Attempt error recovery and return placeholder token if needed\n        pass\n    \n    def synchronize_after_error(self):\n        # TODO: Skip tokens until synchronization point found\n        # TODO: Validate parser state after synchronization\n        # TODO: Reset parsing context appropriately\n        # TODO: Exit panic mode if synchronization successful\n        pass\n    \n    def create_error_with_context(self, message: str, \n                                context: str = \"\") -> SyntaxError:\n        # TODO: Create SyntaxError with current token position\n        # TODO: Add parsing context information\n        # TODO: Generate suggestions based on current state\n        # TODO: Return fully configured error object\n        pass\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 - Tokenizer Error Handling:**\nAfter implementing tokenizer error handling, test with these inputs:\n```python\n# Test unterminated string\ntest_sql = \"SELECT 'unterminated string FROM users\"\n# Expected: TokenizerError with suggestion to add closing quote\n\n# Test invalid characters  \ntest_sql = \"SELECT name @ FROM users\"\n# Expected: TokenizerError identifying '@' as invalid character\n\n# Test malformed number\ntest_sql = \"SELECT price WHERE cost > 12.34.56\"\n# Expected: TokenizerError for invalid number format\n```\n\n**Milestone 2-3 - Parser Error Recovery:**\nTest error recovery with malformed SELECT statements:\n```python\n# Test missing FROM keyword\ntest_sql = \"SELECT name, email users WHERE active = 1\"\n# Expected: UnexpectedTokenError suggesting FROM insertion, continue parsing WHERE\n\n# Test unbalanced parentheses in WHERE\ntest_sql = \"SELECT * FROM users WHERE (name = 'John' AND age > 25\"\n# Expected: Error about unbalanced parentheses, attempt to recover at statement end\n```\n\n**Milestone 4 - Structural Validation:**\nTest structural consistency errors:\n```python\n# Test column/value mismatch in INSERT\ntest_sql = \"INSERT INTO users (name, email) VALUES ('John', 'john@example.com', 'active')\"\n# Expected: StructuralValidationError identifying count mismatch with specific suggestion\n```\n\n#### Debugging Tips for Error Handling\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---|---|---|---|\n| Parser stops at first error | Error recovery not implemented | Check if synchronization points registered | Add sync points and recovery logic |\n| Cascade of false errors | Recovery state not reset properly | Examine parser state after recovery | Reset all parser state at sync points |\n| Unhelpful error messages | Missing context information | Check error creation calls | Add parser context and suggestions to errors |\n| Recovery loops infinitely | Sync points not reachable | Trace token skipping in panic mode | Add more sync points, add loop detection |\n| Errors missing position info | Position tracking broken | Verify line/column updates | Fix position tracking in tokenizer |\n\n\n## Testing Strategy and Validation\n\n> **Milestone(s):** This section applies to all milestones (1-4), providing a comprehensive testing framework that validates parser correctness at each development stage and ensures milestone acceptance criteria are met.\n\n### Mental Model: Quality Control Assembly Line\n\nThink of testing a SQL parser like quality control in a manufacturing assembly line. Each component (tokenizer, parser, AST builder) must be tested in isolation before assembly, just as individual car parts are tested before installation. Then we test the complete assembled product (end-to-end parsing) under real-world conditions with various inputs. Finally, we have checkpoint inspections at each assembly stage (milestone validation) to ensure the product meets specifications before moving to the next phase. This multi-layered approach catches defects early when they're cheaper to fix, prevents faulty components from contaminating downstream processes, and validates that the final product meets customer requirements.\n\nThe key insight in parser testing is that failures can occur at multiple levels - lexical analysis can produce wrong tokens, syntax analysis can build incorrect AST structures, and semantic validation can miss logical inconsistencies. Each level requires different testing strategies because the failure modes and detection methods are distinct. Just as a car manufacturer tests engine components separately from transmission components, we must isolate parser components to pinpoint exactly where failures originate.\n\n### Component Unit Testing\n\n**Component isolation testing** focuses on verifying that individual parser components function correctly in controlled environments with carefully crafted inputs. This approach enables precise failure localization, comprehensive edge case coverage, and rapid test execution during development cycles.\n\n#### Tokenizer Unit Testing\n\nThe tokenizer transforms character sequences into classified tokens, making it the foundation of the entire parsing process. Tokenizer testing must verify both correct token recognition and proper error handling for invalid input sequences.\n\n**Core Tokenizer Test Categories:**\n\n| Test Category | Purpose | Example Input | Expected Output | Failure Detection |\n|--------------|---------|---------------|-----------------|-------------------|\n| Keyword Recognition | Verify case-insensitive keyword detection | `\"SELECT\"`, `\"select\"`, `\"Select\"` | `Token(SELECT_KEYWORD, \"SELECT\", 1, 1)` | Wrong token type or case handling |\n| Identifier Parsing | Validate identifier boundary detection | `\"table_name\"`, `\"column1\"` | `Token(IDENTIFIER, \"table_name\", 1, 1)` | Incorrect character inclusion/exclusion |\n| String Literal Handling | Test quote parsing and escape sequences | `\"'O''Reilly'\"`, `\"\\\"test\\\"\"` | `Token(STRING_LITERAL, \"O'Reilly\", 1, 1)` | Malformed escape handling |\n| Numeric Literal Recognition | Verify integer and float parsing | `\"123\"`, `\"45.67\"`, `\".89\"` | `Token(INTEGER_LITERAL, \"123\", 1, 1)` | Wrong numeric type classification |\n| Operator Tokenization | Test multi-character operator recognition | `\"<=\"`, `\"!=\"`, `\"IS NULL\"` | Correct operator token type | Incomplete operator recognition |\n| Position Tracking | Validate line/column accuracy | Multi-line input with various tokens | Accurate line/column for each token | Incorrect position information |\n\n**Tokenizer Error Handling Tests:**\n\nThe tokenizer must gracefully handle malformed input while providing actionable error information. These tests verify that error detection and recovery mechanisms function correctly.\n\n| Error Scenario | Input Example | Expected Error Type | Error Message Content | Recovery Behavior |\n|----------------|---------------|-------------------|----------------------|-------------------|\n| Unterminated String | `\"'unclosed string` | `TokenizerError` | Position of quote start, suggests closing quote | Skip to next whitespace or semicolon |\n| Invalid Character | `\"SELECT @ FROM\"` | `TokenizerError` | Character and position, suggests valid alternatives | Continue after invalid character |\n| Malformed Number | `\"123.45.67\"` | `TokenizerError` | Position of second decimal, explains numeric format | Treat as separate tokens |\n| Unknown Operator | `\"column ~~ value\"` | `TokenizerError` | Position of operator, lists valid operators | Treat as separate characters |\n\n#### Parser Component Unit Testing\n\nParser components transform token streams into AST nodes, requiring validation of both structural correctness and error handling behavior. Each parser component has distinct responsibilities and failure modes.\n\n**SELECT Parser Unit Tests:**\n\nThe `SelectParser` handles column specifications, table references, and optional clauses. Testing must verify correct AST construction for various SELECT statement patterns.\n\n| Test Scenario | Token Input | Expected AST Structure | Validation Points |\n|--------------|-------------|----------------------|-------------------|\n| Simple Column List | `SELECT col1, col2 FROM table1` | `SelectStatement` with column list containing two `AliasedExpression` nodes | Column count, column names, no aliases |\n| Star Wildcard | `SELECT * FROM table1` | `SelectStatement` with `StarExpression` in columns field | Star expression type, table reference |\n| Column Aliases | `SELECT col1 AS alias1, col2 alias2` | `AliasedExpression` nodes with explicit and implicit aliases | Alias presence and values |\n| Qualified Columns | `SELECT table1.col1, t.col2 FROM table1 t` | `QualifiedIdentifier` nodes with correct qualifier and name | Qualifier extraction, name parsing |\n| Table Aliases | `SELECT * FROM table1 AS t` | `TableReference` with alias field populated | Table name, alias value |\n\n**Expression Parser Unit Tests:**\n\nThe `ExpressionParser` handles operator precedence, associativity, and various expression types. These tests must verify that complex expressions produce correct AST structures with proper precedence relationships.\n\n| Expression Input | Token Sequence | Expected AST Structure | Precedence Verification |\n|-----------------|----------------|----------------------|------------------------|\n| `col1 = 5 AND col2 > 10` | Identifier, Equals, Integer, AND, Identifier, Greater, Integer | `BinaryOperation(AND, BinaryOperation(EQUALS, col1, 5), BinaryOperation(GREATER, col2, 10))` | AND binds looser than comparison |\n| `NOT col1 IS NULL` | NOT, Identifier, IS, NULL | `UnaryOperation(NOT, BinaryOperation(IS, col1, NULL))` | NOT binds tighter than IS |\n| `(col1 + col2) * 3` | LParen, Identifier, Plus, Identifier, RParen, Multiply, Integer | `BinaryOperation(MULTIPLY, BinaryOperation(PLUS, col1, col2), 3)` | Parentheses override precedence |\n| `col1 = col2 = col3` | Multiple equality operators | `BinaryOperation(EQUALS, col1, BinaryOperation(EQUALS, col2, col3))` | Right associativity for equality |\n\n**DML Parser Unit Tests:**\n\nData modification statement parsers handle INSERT, UPDATE, and DELETE syntax with specific validation requirements.\n\n| Statement Type | Input Example | AST Validation | Structural Checks |\n|---------------|---------------|----------------|------------------|\n| INSERT with columns | `INSERT INTO table (col1, col2) VALUES (1, 'test')` | Column list matches value count | Column-value correspondence |\n| INSERT without columns | `INSERT INTO table VALUES (1, 'test', NULL)` | No column list, correct value parsing | Value type recognition |\n| UPDATE with WHERE | `UPDATE table SET col1 = 5 WHERE col2 > 10` | Assignment list and condition parsing | SET clause structure |\n| DELETE with condition | `DELETE FROM table WHERE active = false` | Table reference and WHERE clause | Condition expression validation |\n\n#### AST Node Unit Testing\n\nAST nodes must implement visitor patterns, provide accurate source location information, and support tree traversal operations. These tests verify that node implementations conform to interface contracts.\n\n**AST Node Interface Tests:**\n\n| Interface Method | Test Scenario | Expected Behavior | Failure Detection |\n|-----------------|---------------|-------------------|-------------------|\n| `node_type()` | Call on various node types | Returns correct string identifier | Wrong or missing type name |\n| `children()` | Call on nodes with child nodes | Returns list of direct children only | Missing children or incorrect nesting |\n| `accept(visitor)` | Pass visitor to each node type | Calls correct visitor method | Wrong method or missing implementation |\n| `source_location()` | Nodes created from tokens | Returns accurate position information | Incorrect line/column data |\n\n### End-to-End Parser Testing\n\n**End-to-end testing** validates the complete parsing pipeline from SQL text input to final AST output. These tests simulate real-world usage patterns and verify that component integration produces correct results for complex queries.\n\n#### Real-World Query Testing\n\nEnd-to-end tests use realistic SQL queries that combine multiple language features, testing the interaction between different parser components and validating that the complete system handles complexity correctly.\n\n**Representative Query Test Cases:**\n\n| Query Category | SQL Example | Integration Points Tested | Expected AST Characteristics |\n|----------------|-------------|---------------------------|----------------------------|\n| Complex SELECT | `SELECT t1.name AS customer_name, t2.total FROM customers t1, orders t2 WHERE t1.id = t2.customer_id AND t2.total > 100` | Tokenizer → SELECT parser → Expression parser | Qualified identifiers, aliases, complex WHERE conditions |\n| Multi-table operations | `UPDATE customers SET status = 'premium' WHERE id IN (SELECT customer_id FROM orders WHERE total > 1000)` | All parser components, nested expression handling | Subquery parsing (future milestone), complex conditions |\n| Batch INSERT | `INSERT INTO products (name, price, category) VALUES ('Product1', 19.99, 'electronics'), ('Product2', 29.99, 'books')` | Tokenizer → DML parser → Value list parsing | Multiple value rows, type consistency |\n| Complex conditions | `DELETE FROM logs WHERE created_date < '2023-01-01' OR (level = 'DEBUG' AND archived IS NOT NULL)` | Expression precedence, multiple operators | Precedence correctness, logical operator combination |\n\n**Query Complexity Progression:**\n\nEnd-to-end tests should progress from simple to complex queries, verifying that each level of complexity builds correctly on simpler foundations.\n\n| Complexity Level | Query Characteristics | Testing Focus | Example Patterns |\n|-----------------|----------------------|---------------|------------------|\n| Basic | Single table, simple conditions | Core functionality verification | `SELECT * FROM table WHERE id = 1` |\n| Intermediate | Multiple columns, aliases, AND/OR | Component integration | `SELECT col1 AS c1, col2 FROM table t WHERE c1 > 5 AND c2 IS NOT NULL` |\n| Advanced | Complex expressions, multiple tables | Full system integration | `SELECT t1.col1, t2.col2 FROM table1 t1, table2 t2 WHERE t1.id = t2.ref_id AND (t1.status = 'active' OR t2.priority > 3)` |\n| Edge Cases | Boundary conditions, unusual syntax | Robustness validation | `SELECT * FROM table WHERE col BETWEEN 1 AND (2 + 3) * 4` |\n\n#### Error Propagation Testing\n\nEnd-to-end error testing verifies that errors detected at any parsing stage propagate correctly through the system and produce helpful error messages for users.\n\n**Error Propagation Scenarios:**\n\n| Error Origin | SQL Input | Error Detection Point | Expected Error Information |\n|--------------|-----------|----------------------|----------------------------|\n| Tokenizer | `SELECT * FROM table WHERE col = 'unterminated` | Tokenizer during string parsing | Line/column of unterminated quote, suggestion to add closing quote |\n| SELECT Parser | `SELECT col1, FROM table` | SELECT parser during column list parsing | Position of unexpected FROM, suggestion to remove comma or add column |\n| Expression Parser | `SELECT * FROM table WHERE col1 = = 5` | Expression parser during operator parsing | Position of duplicate equals, explanation of valid syntax |\n| DML Parser | `INSERT INTO table (col1, col2) VALUES (1)` | DML parser during value validation | Column count mismatch, specific counts and position information |\n\n#### Performance Characteristic Testing\n\nWhile not primary functional goals, end-to-end tests should verify that the parser maintains reasonable performance characteristics and doesn't exhibit pathological behavior with large inputs.\n\n**Performance Test Categories:**\n\n| Performance Aspect | Test Approach | Acceptance Criteria | Monitoring Points |\n|-------------------|---------------|-------------------|------------------|\n| Linear scaling | Queries with increasing column counts | Parse time grows linearly with input size | Token count vs. parse time |\n| Memory usage | Large INSERT statements with many value rows | Memory usage remains bounded | Peak memory during parsing |\n| Error recovery | Malformed queries with multiple errors | Error detection doesn't degrade performance | Error count vs. processing time |\n\n### Milestone Validation Checkpoints\n\n**Milestone validation** provides concrete checkpoints that verify specific project milestones are complete and functional. Each checkpoint includes automated tests, manual verification steps, and expected behavior descriptions.\n\n#### Milestone 1: SQL Tokenizer Validation\n\nThe tokenizer milestone focuses on lexical analysis correctness and establishes the foundation for all subsequent parsing stages.\n\n**Automated Tokenizer Tests:**\n\n| Test Name | Input SQL | Expected Token Sequence | Pass/Fail Criteria |\n|-----------|-----------|------------------------|-------------------|\n| `test_keyword_recognition` | `\"SELECT FROM WHERE\"` | `[SELECT_KEYWORD, FROM_KEYWORD, WHERE_KEYWORD]` | All tokens have correct types |\n| `test_identifier_parsing` | `\"table_name column1\"` | `[IDENTIFIER(\"table_name\"), IDENTIFIER(\"column1\")]` | Correct boundary detection |\n| `test_string_literals` | `\"'O''Reilly' \\\"quoted\\\"\"` | `[STRING_LITERAL(\"O'Reilly\"), STRING_LITERAL(\"quoted\")]` | Proper escape handling |\n| `test_numeric_literals` | `\"123 45.67 .89\"` | `[INTEGER_LITERAL(\"123\"), FLOAT_LITERAL(\"45.67\"), FLOAT_LITERAL(\".89\")]` | Correct numeric type classification |\n| `test_operator_recognition` | `\"= < > <= >= != AND OR\"` | Correct operator token types for each | Multi-character operators recognized |\n\n**Manual Tokenizer Verification:**\n\n> **Checkpoint Command:** `python -m tokenizer \"SELECT table1.column1 AS col1 FROM table1 WHERE value > 100\"`\n> \n> **Expected Output:**\n> ```\n> Token(SELECT_KEYWORD, 'SELECT', line=1, column=1)\n> Token(IDENTIFIER, 'table1', line=1, column=8)\n> Token(DOT, '.', line=1, column=14)\n> Token(IDENTIFIER, 'column1', line=1, column=15)\n> Token(AS_KEYWORD, 'AS', line=1, column=23)\n> Token(IDENTIFIER, 'col1', line=1, column=26)\n> Token(FROM_KEYWORD, 'FROM', line=1, column=31)\n> Token(IDENTIFIER, 'table1', line=1, column=36)\n> Token(WHERE_KEYWORD, 'WHERE', line=1, column=43)\n> Token(IDENTIFIER, 'value', line=1, column=49)\n> Token(GREATER, '>', line=1, column=55)\n> Token(INTEGER_LITERAL, '100', line=1, column=57)\n> ```\n\n**Milestone 1 Success Indicators:**\n\n- All SQL keywords are recognized case-insensitively\n- Identifiers are correctly distinguished from keywords\n- String literals handle single and double quotes with escape sequences\n- Numeric literals distinguish integers from floats\n- Operators including multi-character ones (<=, >=, !=) are recognized\n- Position tracking provides accurate line and column information\n\n#### Milestone 2: SELECT Parser Validation\n\nThe SELECT parser milestone builds AST structures from token streams and handles various SELECT statement patterns.\n\n**Automated SELECT Parser Tests:**\n\n| Test Name | SQL Input | Expected AST Structure | Validation Points |\n|-----------|-----------|----------------------|------------------|\n| `test_simple_select` | `\"SELECT * FROM table1\"` | `SelectStatement(StarExpression(), TableReference(\"table1\"))` | Star expression and table parsing |\n| `test_column_list` | `\"SELECT col1, col2 FROM table1\"` | Column list with two `AliasedExpression` nodes | Column count and names |\n| `test_column_aliases` | `\"SELECT col1 AS c1, col2 c2\"` | Explicit and implicit aliases in AST | Alias detection and storage |\n| `test_qualified_columns` | `\"SELECT t.col1 FROM table1 t\"` | `QualifiedIdentifier` with qualifier \"t\" | Qualifier parsing and table alias |\n\n**Manual SELECT Parser Verification:**\n\n> **Checkpoint Command:** `python -m parser \"SELECT t1.name AS customer_name, t1.email FROM customers t1\"`\n> \n> **Expected AST Structure:**\n> ```\n> SelectStatement(\n>   columns=[\n>     AliasedExpression(\n>       expression=QualifiedIdentifier(qualifier=\"t1\", name=\"name\"),\n>       alias=\"customer_name\"\n>     ),\n>     AliasedExpression(\n>       expression=QualifiedIdentifier(qualifier=\"t1\", name=\"email\"),\n>       alias=None\n>     )\n>   ],\n>   table_reference=TableReference(\n>     table=Identifier(\"customers\"),\n>     alias=\"t1\"\n>   ),\n>   where_clause=None\n> )\n> ```\n\n**Milestone 2 Success Indicators:**\n\n- SELECT statements parse into correct `SelectStatement` AST nodes\n- Column specifications support star wildcards and explicit column lists\n- Column aliases work with both explicit AS keyword and implicit syntax\n- Table references support aliases and qualified identifier parsing\n- AST nodes provide accurate source location information\n\n#### Milestone 3: WHERE Clause Expression Parser Validation\n\nThe expression parser milestone handles complex expressions with proper operator precedence and associativity rules.\n\n**Automated Expression Parser Tests:**\n\n| Test Name | SQL Input | Expected AST Structure | Precedence Verification |\n|-----------|-----------|----------------------|------------------------|\n| `test_basic_comparison` | `\"WHERE col1 = 5\"` | `BinaryOperation(EQUALS, Identifier(\"col1\"), IntegerLiteral(5))` | Simple binary operation |\n| `test_logical_operators` | `\"WHERE col1 = 5 AND col2 > 10\"` | AND operation with two comparison children | AND binds looser than comparison |\n| `test_precedence_override` | `\"WHERE (col1 + col2) > 10\"` | Addition as left child of comparison | Parentheses override precedence |\n| `test_null_checks` | `\"WHERE col1 IS NULL OR col2 IS NOT NULL\"` | Null test expressions with OR combination | IS NULL/IS NOT NULL parsing |\n\n**Manual Expression Parser Verification:**\n\n> **Checkpoint Command:** `python -m parser \"SELECT * FROM table WHERE col1 = 5 AND (col2 > 10 OR col3 IS NULL)\"`\n> \n> **Expected WHERE Clause AST:**\n> ```\n> BinaryOperation(\n>   operator=AND,\n>   left=BinaryOperation(\n>     operator=EQUALS,\n>     left=Identifier(\"col1\"),\n>     right=IntegerLiteral(5)\n>   ),\n>   right=BinaryOperation(\n>     operator=OR,\n>     left=BinaryOperation(\n>       operator=GREATER,\n>       left=Identifier(\"col2\"),\n>       right=IntegerLiteral(10)\n>     ),\n>     right=UnaryOperation(\n>       operator=IS_NULL,\n>       operand=Identifier(\"col3\")\n>     )\n>   )\n> )\n> ```\n\n**Milestone 3 Success Indicators:**\n\n- Complex expressions parse with correct operator precedence\n- Boolean operators (AND, OR, NOT) have proper precedence relationships\n- Parentheses correctly override default precedence rules\n- NULL checks (IS NULL, IS NOT NULL) parse as unary operations\n- Expression trees accurately represent the logical structure\n\n#### Milestone 4: Data Modification Statement Validation\n\nThe DML parser milestone handles INSERT, UPDATE, and DELETE statements with their specific syntax requirements and validation rules.\n\n**Automated DML Parser Tests:**\n\n| Test Name | SQL Input | Expected AST Structure | Validation Points |\n|-----------|-----------|----------------------|------------------|\n| `test_basic_insert` | `\"INSERT INTO table (col1) VALUES (1)\"` | Column list matches value count | Column-value correspondence |\n| `test_multi_row_insert` | `\"INSERT INTO table VALUES (1, 'a'), (2, 'b')\"` | Multiple value rows with consistent structure | Value row consistency |\n| `test_update_statement` | `\"UPDATE table SET col1 = 5 WHERE id = 1\"` | Assignment list and WHERE condition | SET clause parsing |\n| `test_delete_statement` | `\"DELETE FROM table WHERE active = false\"` | Table reference and condition | Simple DELETE structure |\n\n**Manual DML Parser Verification:**\n\n> **Checkpoint Command:** `python -m parser \"INSERT INTO products (name, price, category) VALUES ('Laptop', 999.99, 'electronics'), ('Book', 19.99, 'education')\"`\n> \n> **Expected AST Structure:**\n> ```\n> InsertStatement(\n>   table_reference=TableReference(\n>     table=Identifier(\"products\"),\n>     alias=None\n>   ),\n>   column_list=[\n>     Identifier(\"name\"),\n>     Identifier(\"price\"),\n>     Identifier(\"category\")\n>   ],\n>   value_rows=[\n>     [StringLiteral(\"Laptop\"), FloatLiteral(999.99), StringLiteral(\"electronics\")],\n>     [StringLiteral(\"Book\"), FloatLiteral(19.99), StringLiteral(\"education\")]\n>   ]\n> )\n> ```\n\n**Milestone 4 Success Indicators:**\n\n- INSERT statements handle both column lists and value rows correctly\n- UPDATE statements parse SET clauses with assignment expressions\n- DELETE statements support WHERE conditions for selective deletion\n- Column count validation ensures INSERT statements have consistent structure\n- All DML statements integrate with expression parsing for conditions and values\n\n### Common Testing Pitfalls\n\nTesting SQL parsers presents unique challenges due to the complexity of SQL syntax, the variety of valid statement patterns, and the need for comprehensive error handling.\n\n⚠️ **Pitfall: Insufficient Edge Case Coverage**\nMany parser tests focus on \"happy path\" scenarios with well-formed SQL but fail to adequately test boundary conditions and unusual but valid syntax. For example, testing `SELECT col1, col2 FROM table` but not testing `SELECT col1,col2FROM table` (no spaces around comma and FROM). This leads to parsers that work with formatted SQL but fail with compressed or unusual formatting.\n\n**Why it's wrong:** Real-world SQL often contains minimal whitespace, unusual casing, and edge cases that well-formatted test queries don't cover. Parsers must handle the full spectrum of valid SQL syntax, not just the pretty-printed versions.\n\n**How to fix:** Create comprehensive test suites that systematically vary whitespace, casing, punctuation, and optional syntax elements. Use property-based testing to generate variations automatically.\n\n⚠️ **Pitfall: Testing Implementation Details Instead of Behavior**\nTests that verify specific internal parser states, function call sequences, or intermediate data structures couple tests too tightly to implementation details. For example, testing that the parser calls `parse_column_spec()` exactly three times instead of testing that three columns are correctly parsed.\n\n**Why it's wrong:** Implementation-coupled tests break when internal refactoring occurs, even if external behavior remains correct. This creates maintenance overhead and discourages beneficial code improvements.\n\n**How to fix:** Focus tests on observable outputs - the structure and content of produced AST nodes, error messages for invalid input, and behavioral characteristics rather than internal implementation steps.\n\n⚠️ **Pitfall: Inadequate Error Message Validation**\nTests that only check whether an error occurs but don't validate the quality, accuracy, or helpfulness of error messages. For example, checking that `\"SELECT , FROM table\"` produces an error but not verifying that the error message identifies the unexpected comma and suggests valid alternatives.\n\n**Why it's wrong:** Poor error messages significantly degrade the developer experience when using the parser. Users need specific, actionable information about what went wrong and how to fix it.\n\n**How to fix:** Include error message content validation in all error test cases. Verify that error messages include position information, describe the specific problem, and suggest corrective actions.\n\n⚠️ **Pitfall: Missing Integration Test Coverage**\nTesting individual parser components thoroughly but failing to test their integration under realistic conditions. For example, testing the tokenizer and expression parser separately but not testing how tokenizer errors affect expression parsing.\n\n**Why it's wrong:** Component integration often reveals issues that don't appear in isolated testing. Error propagation, resource management, and performance characteristics emerge only when components work together.\n\n**How to fix:** Include comprehensive end-to-end test suites that exercise the complete parsing pipeline with realistic, complex SQL statements. Test error conditions that span multiple components.\n\n### Implementation Guidance\n\n#### Testing Framework Selection\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Unit Testing | Python `unittest` with manual assertions | `pytest` with fixtures and parameterized tests |\n| Test Data | Hardcoded SQL strings in test methods | External SQL file fixtures with metadata |\n| AST Assertion | Manual field-by-field comparison | Custom AST equality matchers with diff output |\n| Error Testing | Try/catch with exception type checking | Dedicated error assertion helpers with message validation |\n| Coverage | Manual test case enumeration | `coverage.py` with automated coverage reporting |\n\n#### Recommended Test File Structure\n\n```\nsql-parser/\n  tests/\n    unit/\n      test_tokenizer.py           ← Tokenizer component tests\n      test_select_parser.py       ← SELECT statement parser tests  \n      test_expression_parser.py   ← WHERE clause expression tests\n      test_dml_parser.py          ← INSERT/UPDATE/DELETE tests\n      test_ast_nodes.py           ← AST node interface tests\n    integration/\n      test_end_to_end.py          ← Complete parsing pipeline tests\n      test_error_propagation.py   ← Multi-component error handling\n    milestone/\n      test_milestone_1.py         ← Tokenizer milestone validation\n      test_milestone_2.py         ← SELECT parser milestone validation\n      test_milestone_3.py         ← Expression parser milestone validation\n      test_milestone_4.py         ← DML parser milestone validation\n    fixtures/\n      sql_samples/                ← Sample SQL files for testing\n        simple_queries.sql        ← Basic test cases\n        complex_queries.sql       ← Advanced integration tests\n        error_cases.sql           ← Invalid SQL for error testing\n      expected_ast/               ← Expected AST structures in JSON\n        simple_queries.json       ← Expected outputs for simple cases\n        complex_queries.json      ← Expected outputs for complex cases\n```\n\n#### Complete Test Utility Infrastructure\n\n```python\n# tests/utils/test_helpers.py\n\"\"\"Complete testing utility infrastructure for SQL parser validation.\"\"\"\n\nimport json\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nfrom sql_parser.tokenizer import Token, TokenType, Tokenizer\nfrom sql_parser.parser import SQLParser\nfrom sql_parser.ast_nodes import ASTNode, SelectStatement, InsertStatement\nfrom sql_parser.errors import ParseError, TokenizerError, SyntaxError\n\n\n@dataclass\nclass TokenExpectation:\n    \"\"\"Expected token with type, value, and position information.\"\"\"\n    token_type: TokenType\n    value: str\n    line: int\n    column: int\n    \n    def matches(self, actual_token: Token) -> bool:\n        \"\"\"Check if actual token matches this expectation.\"\"\"\n        return (actual_token.type == self.token_type and\n                actual_token.value == self.value and\n                actual_token.line == self.line and\n                actual_token.column == self.column)\n\n\n@dataclass\nclass ASTExpectation:\n    \"\"\"Expected AST node structure with type and property validation.\"\"\"\n    node_type: str\n    properties: Dict[str, Any]\n    children: List['ASTExpectation']\n    \n    def matches(self, actual_node: ASTNode) -> bool:\n        \"\"\"Recursively validate AST node structure.\"\"\"\n        if actual_node.node_type() != self.node_type:\n            return False\n        \n        # Validate properties\n        for prop_name, expected_value in self.properties.items():\n            actual_value = getattr(actual_node, prop_name, None)\n            if actual_value != expected_value:\n                return False\n        \n        # Validate children\n        actual_children = actual_node.children()\n        if len(actual_children) != len(self.children):\n            return False\n        \n        for expected_child, actual_child in zip(self.children, actual_children):\n            if not expected_child.matches(actual_child):\n                return False\n        \n        return True\n\n\nclass TokenizerTestHelper:\n    \"\"\"Helper class for tokenizer testing with assertion methods.\"\"\"\n    \n    @staticmethod\n    def assert_tokens_match(expected: List[TokenExpectation], actual: List[Token]):\n        \"\"\"Assert that actual tokens match expected token list.\"\"\"\n        assert len(actual) == len(expected), f\"Token count mismatch: expected {len(expected)}, got {len(actual)}\"\n        \n        for i, (expected_token, actual_token) in enumerate(zip(expected, actual)):\n            assert expected_token.matches(actual_token), (\n                f\"Token {i} mismatch: expected {expected_token}, got \"\n                f\"Token({actual_token.type}, '{actual_token.value}', {actual_token.line}, {actual_token.column})\"\n            )\n    \n    @staticmethod\n    def tokenize_and_validate(sql_text: str, expected_tokens: List[TokenExpectation]):\n        \"\"\"Tokenize SQL and validate against expected tokens.\"\"\"\n        tokenizer = Tokenizer(sql_text)\n        actual_tokens = tokenizer.tokenize()\n        TokenizerTestHelper.assert_tokens_match(expected_tokens, actual_tokens)\n        return actual_tokens\n\n\nclass ParserTestHelper:\n    \"\"\"Helper class for parser testing with AST validation.\"\"\"\n    \n    @staticmethod\n    def parse_and_validate(sql_text: str, expected_ast: ASTExpectation) -> ASTNode:\n        \"\"\"Parse SQL and validate against expected AST structure.\"\"\"\n        parser = SQLParser()\n        actual_ast = parser.parse(sql_text)\n        \n        assert expected_ast.matches(actual_ast), (\n            f\"AST structure mismatch: expected {expected_ast.node_type} with \"\n            f\"properties {expected_ast.properties}, got {actual_ast.node_type()}\"\n        )\n        \n        return actual_ast\n    \n    @staticmethod\n    def assert_parse_error(sql_text: str, expected_error_type: type, \n                          expected_message_content: Optional[str] = None):\n        \"\"\"Assert that SQL parsing raises expected error with optional message validation.\"\"\"\n        parser = SQLParser()\n        try:\n            parser.parse(sql_text)\n            assert False, f\"Expected {expected_error_type.__name__} but parsing succeeded\"\n        except ParseError as e:\n            assert isinstance(e, expected_error_type), (\n                f\"Expected {expected_error_type.__name__} but got {type(e).__name__}\"\n            )\n            if expected_message_content:\n                assert expected_message_content in e.message, (\n                    f\"Expected error message to contain '{expected_message_content}' \"\n                    f\"but got '{e.message}'\"\n                )\n\n\nclass FixtureLoader:\n    \"\"\"Load test cases from external fixture files.\"\"\"\n    \n    def __init__(self, fixture_dir: str = \"tests/fixtures\"):\n        self.fixture_dir = Path(fixture_dir)\n    \n    def load_sql_cases(self, filename: str) -> List[str]:\n        \"\"\"Load SQL test cases from file, one per line or separated by semicolons.\"\"\"\n        file_path = self.fixture_dir / \"sql_samples\" / filename\n        with open(file_path, 'r') as f:\n            content = f.read()\n        \n        # Split on semicolons and clean up\n        cases = [case.strip() for case in content.split(';') if case.strip()]\n        return cases\n    \n    def load_expected_ast(self, filename: str) -> Dict[str, ASTExpectation]:\n        \"\"\"Load expected AST structures from JSON file.\"\"\"\n        file_path = self.fixture_dir / \"expected_ast\" / filename\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        \n        # Convert JSON to ASTExpectation objects\n        expectations = {}\n        for test_name, ast_data in data.items():\n            expectations[test_name] = self._json_to_ast_expectation(ast_data)\n        \n        return expectations\n    \n    def _json_to_ast_expectation(self, json_data: Dict) -> ASTExpectation:\n        \"\"\"Convert JSON representation to ASTExpectation object.\"\"\"\n        return ASTExpectation(\n            node_type=json_data[\"node_type\"],\n            properties=json_data.get(\"properties\", {}),\n            children=[self._json_to_ast_expectation(child) \n                     for child in json_data.get(\"children\", [])]\n        )\n\n\nclass MilestoneValidator:\n    \"\"\"Validation logic for milestone acceptance criteria.\"\"\"\n    \n    def __init__(self):\n        self.test_helper = ParserTestHelper()\n        self.fixture_loader = FixtureLoader()\n    \n    def validate_milestone_1(self) -> bool:\n        \"\"\"Validate tokenizer milestone acceptance criteria.\"\"\"\n        # TODO: Load tokenizer test cases from fixtures\n        # TODO: Validate keyword recognition (SELECT, FROM, WHERE)\n        # TODO: Validate identifier and number tokenization\n        # TODO: Validate string literal parsing with quotes\n        # TODO: Validate operator recognition (=, <, >, AND, OR)\n        # TODO: Return True if all tests pass\n        pass\n    \n    def validate_milestone_2(self) -> bool:\n        \"\"\"Validate SELECT parser milestone acceptance criteria.\"\"\"\n        # TODO: Load SELECT statement test cases\n        # TODO: Validate column list parsing (wildcard and explicit columns)\n        # TODO: Validate FROM clause with table name\n        # TODO: Validate comma-separated columns with AST generation\n        # TODO: Validate column and table aliases (AS keyword and implicit)\n        # TODO: Return True if all tests pass\n        pass\n    \n    def validate_milestone_3(self) -> bool:\n        \"\"\"Validate WHERE clause expression parser milestone.\"\"\"\n        # TODO: Load WHERE clause test cases\n        # TODO: Validate comparison operators (=, <, >, !=)\n        # TODO: Validate boolean operators with precedence (AND, OR, NOT)\n        # TODO: Validate parentheses override default precedence\n        # TODO: Validate NULL checks (IS NULL, IS NOT NULL)\n        # TODO: Return True if all tests pass\n        pass\n    \n    def validate_milestone_4(self) -> bool:\n        \"\"\"Validate INSERT/UPDATE/DELETE parser milestone.\"\"\"\n        # TODO: Load DML statement test cases\n        # TODO: Validate INSERT INTO table (cols) VALUES (vals)\n        # TODO: Validate UPDATE table SET col=val WHERE condition\n        # TODO: Validate DELETE FROM table WHERE condition\n        # TODO: Validate multiple value rows in INSERT statements\n        # TODO: Return True if all tests pass\n        pass\n```\n\n#### Core Logic Test Skeletons\n\n```python\n# tests/unit/test_tokenizer.py\n\"\"\"Comprehensive tokenizer unit tests with edge case coverage.\"\"\"\n\nimport pytest\nfrom tests.utils.test_helpers import TokenizerTestHelper, TokenExpectation\nfrom sql_parser.tokenizer import TokenType, TokenizerError\n\n\nclass TestTokenizerKeywords:\n    \"\"\"Test SQL keyword recognition and case handling.\"\"\"\n    \n    def test_basic_keywords_uppercase(self):\n        \"\"\"Test recognition of uppercase SQL keywords.\"\"\"\n        # TODO: Test SELECT, FROM, WHERE, INSERT, UPDATE, DELETE keywords\n        # TODO: Verify each keyword produces correct TokenType\n        # TODO: Validate position tracking for each keyword\n        pass\n    \n    def test_basic_keywords_lowercase(self):\n        \"\"\"Test case-insensitive keyword recognition.\"\"\"\n        # TODO: Test select, from, where with lowercase input\n        # TODO: Verify tokenizer normalizes to uppercase in value field\n        # TODO: Ensure TokenType matches uppercase equivalent\n        pass\n    \n    def test_mixed_case_keywords(self):\n        \"\"\"Test keywords with mixed case like Select, From, Where.\"\"\"\n        # TODO: Test various case combinations\n        # TODO: Verify consistent normalization behavior\n        pass\n\n\nclass TestTokenizerIdentifiers:\n    \"\"\"Test identifier parsing and boundary detection.\"\"\"\n    \n    def test_simple_identifiers(self):\n        \"\"\"Test basic identifier patterns.\"\"\"\n        expected = [\n            TokenExpectation(TokenType.IDENTIFIER, \"table_name\", 1, 1),\n            TokenExpectation(TokenType.IDENTIFIER, \"column1\", 1, 12)\n        ]\n        TokenizerTestHelper.tokenize_and_validate(\"table_name column1\", expected)\n    \n    def test_identifier_with_numbers(self):\n        \"\"\"Test identifiers containing numeric characters.\"\"\"\n        # TODO: Test patterns like table1, col_2, item123\n        # TODO: Verify numbers within identifiers don't split tokens\n        pass\n    \n    def test_identifier_keyword_boundaries(self):\n        \"\"\"Test identifiers that start with keyword prefixes.\"\"\"\n        # TODO: Test selectcolumn, fromtable (should be identifiers)\n        # TODO: Test column_select, table_from (should be identifiers)\n        pass\n\n\nclass TestTokenizerStringLiterals:\n    \"\"\"Test string literal parsing with quotes and escapes.\"\"\"\n    \n    def test_single_quoted_strings(self):\n        \"\"\"Test basic single-quoted string literals.\"\"\"\n        expected = [TokenExpectation(TokenType.STRING_LITERAL, \"test value\", 1, 1)]\n        TokenizerTestHelper.tokenize_and_validate(\"'test value'\", expected)\n    \n    def test_double_quoted_strings(self):\n        \"\"\"Test double-quoted string literals.\"\"\"\n        # TODO: Test \"test value\" produces STRING_LITERAL token\n        # TODO: Verify quote characters are not included in value\n        pass\n    \n    def test_escaped_quotes_in_strings(self):\n        \"\"\"Test escape sequence handling in string literals.\"\"\"\n        # TODO: Test 'O''Reilly' produces \"O'Reilly\" value\n        # TODO: Test \"She said \\\"hello\\\"\" with escaped quotes\n        pass\n    \n    def test_unterminated_string_error(self):\n        \"\"\"Test error handling for unterminated strings.\"\"\"\n        # TODO: Test 'unterminated string without closing quote\n        # TODO: Verify TokenizerError is raised with correct position\n        # TODO: Check error message suggests adding closing quote\n        pass\n\n\nclass TestTokenizerOperators:\n    \"\"\"Test operator recognition including multi-character operators.\"\"\"\n    \n    def test_single_character_operators(self):\n        \"\"\"Test basic single-character operators.\"\"\"\n        sql = \"= < > +\"\n        expected = [\n            TokenExpectation(TokenType.EQUALS, \"=\", 1, 1),\n            TokenExpectation(TokenType.LESS_THAN, \"<\", 1, 3),\n            TokenExpectation(TokenType.GREATER_THAN, \">\", 1, 5),\n            TokenExpectation(TokenType.PLUS, \"+\", 1, 7)\n        ]\n        TokenizerTestHelper.tokenize_and_validate(sql, expected)\n    \n    def test_multi_character_operators(self):\n        \"\"\"Test multi-character operator recognition.\"\"\"\n        # TODO: Test <= >= != operators\n        # TODO: Verify complete operator is recognized as single token\n        # TODO: Test that < = is two tokens, <= is one token\n        pass\n\n\n# tests/unit/test_select_parser.py\n\"\"\"SELECT statement parser unit tests.\"\"\"\n\nimport pytest\nfrom tests.utils.test_helpers import ParserTestHelper, ASTExpectation\nfrom sql_parser.ast_nodes import SelectStatement, StarExpression, AliasedExpression\n\n\nclass TestSelectParserBasic:\n    \"\"\"Test basic SELECT statement parsing.\"\"\"\n    \n    def test_select_star_from_table(self):\n        \"\"\"Test simplest SELECT * FROM table pattern.\"\"\"\n        expected_ast = ASTExpectation(\n            node_type=\"SelectStatement\",\n            properties={},\n            children=[\n                ASTExpectation(\"StarExpression\", {}, []),\n                ASTExpectation(\"TableReference\", {\"alias\": None}, [\n                    ASTExpectation(\"Identifier\", {\"name\": \"table1\"}, [])\n                ])\n            ]\n        )\n        ParserTestHelper.parse_and_validate(\"SELECT * FROM table1\", expected_ast)\n    \n    def test_select_column_list(self):\n        \"\"\"Test SELECT with explicit column list.\"\"\"\n        # TODO: Test \"SELECT col1, col2 FROM table1\"\n        # TODO: Verify AliasedExpression nodes for each column\n        # TODO: Check column names and absence of aliases\n        pass\n    \n    def test_select_with_table_alias(self):\n        \"\"\"Test SELECT with table alias.\"\"\"\n        # TODO: Test \"SELECT * FROM table1 t\" and \"SELECT * FROM table1 AS t\"\n        # TODO: Verify TableReference contains alias information\n        pass\n\n\nclass TestSelectParserColumnSpecs:\n    \"\"\"Test column specification parsing in SELECT statements.\"\"\"\n    \n    def test_qualified_column_names(self):\n        \"\"\"Test table.column qualified identifiers.\"\"\"\n        # TODO: Test \"SELECT t.col1, t.col2 FROM table1 t\"\n        # TODO: Verify QualifiedIdentifier nodes with correct qualifier\n        pass\n    \n    def test_column_aliases_explicit(self):\n        \"\"\"Test column aliases with AS keyword.\"\"\"\n        # TODO: Test \"SELECT col1 AS c1, col2 AS c2 FROM table1\"\n        # TODO: Verify AliasedExpression nodes contain alias values\n        pass\n    \n    def test_column_aliases_implicit(self):\n        \"\"\"Test column aliases without AS keyword.\"\"\"\n        # TODO: Test \"SELECT col1 c1, col2 c2 FROM table1\"\n        # TODO: Verify implicit alias parsing works correctly\n        pass\n```\n\n#### Milestone Checkpoint Implementations\n\n```python\n# tests/milestone/test_milestone_1.py\n\"\"\"Milestone 1 validation: SQL Tokenizer acceptance criteria.\"\"\"\n\nimport pytest\nfrom tests.utils.test_helpers import MilestoneValidator, TokenizerTestHelper, TokenExpectation\nfrom sql_parser.tokenizer import TokenType\n\n\nclass TestMilestone1Acceptance:\n    \"\"\"Validate all Milestone 1 acceptance criteria.\"\"\"\n    \n    def test_keyword_recognition_criteria(self):\n        \"\"\"Acceptance: Recognize SQL keywords (SELECT, FROM, WHERE).\"\"\"\n        test_cases = [\n            (\"SELECT\", TokenType.SELECT_KEYWORD),\n            (\"FROM\", TokenType.FROM_KEYWORD), \n            (\"WHERE\", TokenType.WHERE_KEYWORD),\n            (\"INSERT\", TokenType.INSERT_KEYWORD),\n            (\"UPDATE\", TokenType.UPDATE_KEYWORD),\n            (\"DELETE\", TokenType.DELETE_KEYWORD)\n        ]\n        \n        for keyword, expected_type in test_cases:\n            expected = [TokenExpectation(expected_type, keyword, 1, 1)]\n            TokenizerTestHelper.tokenize_and_validate(keyword, expected)\n    \n    def test_identifier_and_numbers_criteria(self):\n        \"\"\"Acceptance: Handle identifiers and numbers with correct classification.\"\"\"\n        sql = \"table_name column1 123 45.67\"\n        expected = [\n            TokenExpectation(TokenType.IDENTIFIER, \"table_name\", 1, 1),\n            TokenExpectation(TokenType.IDENTIFIER, \"column1\", 1, 12),\n            TokenExpectation(TokenType.INTEGER_LITERAL, \"123\", 1, 20),\n            TokenExpectation(TokenType.FLOAT_LITERAL, \"45.67\", 1, 24)\n        ]\n        TokenizerTestHelper.tokenize_and_validate(sql, expected)\n    \n    def test_string_literals_criteria(self):\n        \"\"\"Acceptance: Parse string literals enclosed in single or double quotes.\"\"\"\n        # TODO: Test single and double quoted strings\n        # TODO: Test escape sequence handling\n        # TODO: Verify STRING_LITERAL token type classification\n        pass\n    \n    def test_operators_criteria(self):\n        \"\"\"Acceptance: Support operators (=, <, >, AND, OR).\"\"\"\n        # TODO: Test all required operators from acceptance criteria\n        # TODO: Verify correct operator token type classification\n        # TODO: Include multi-character operators like <= and >=\n        pass\n\n    def test_milestone_1_complete_validation(self):\n        \"\"\"Complete milestone 1 validation using comprehensive test.\"\"\"\n        validator = MilestoneValidator()\n        # This should pass if all individual criteria pass\n        assert validator.validate_milestone_1(), \"Milestone 1 acceptance criteria not met\"\n\n\n# tests/milestone/test_milestone_2.py\n\"\"\"Milestone 2 validation: SELECT Parser acceptance criteria.\"\"\"\n\nclass TestMilestone2Acceptance:\n    \"\"\"Validate all Milestone 2 acceptance criteria.\"\"\"\n    \n    def test_column_list_or_star_criteria(self):\n        \"\"\"Acceptance: Parse column list or star wildcard in SELECT clause correctly.\"\"\"\n        # TODO: Test SELECT * produces StarExpression\n        # TODO: Test SELECT col1, col2 produces column list\n        # TODO: Verify correct AST node generation\n        pass\n    \n    def test_from_clause_criteria(self):\n        \"\"\"Acceptance: Parse FROM clause with table name.\"\"\"\n        # TODO: Test FROM table_name produces TableReference\n        # TODO: Verify table name extraction\n        pass\n    \n    def test_comma_separated_columns_criteria(self):\n        \"\"\"Acceptance: Handle multiple comma-separated columns with correct AST.\"\"\"\n        # TODO: Test various column count combinations\n        # TODO: Verify AST structure for each column\n        pass\n    \n    def test_aliases_criteria(self):\n        \"\"\"Acceptance: Parse column and table aliases using AS keyword or implicit.\"\"\"\n        # TODO: Test explicit aliases with AS keyword\n        # TODO: Test implicit aliases without AS keyword\n        # TODO: Verify alias information stored in AST\n        pass\n```\n\n#### Language-Specific Testing Hints\n\n**Python Testing Best Practices:**\n- Use `pytest` with parametrized tests for testing multiple similar cases\n- Use `pytest.raises()` for error testing with message validation\n- Create custom assertion helpers in `conftest.py` for AST comparison\n- Use `dataclasses` for test expectation objects to reduce boilerplate\n- Leverage `pathlib.Path` for fixture file loading across platforms\n\n**Test Data Management:**\n- Store complex SQL samples in external `.sql` files rather than hardcoding strings\n- Use JSON files for expected AST structures to enable test case reuse\n- Implement test case generators for systematic variation testing\n- Create helper functions that combine SQL input with expected output for parameterized tests\n\n**Performance Testing Integration:**\n- Use `time.time()` or `timeit` module for basic performance benchmarking\n- Set reasonable timeout limits for tests to catch infinite loops\n- Monitor memory usage with `tracemalloc` for large input testing\n- Include performance regression tests in continuous integration\n\n#### Debugging Test Failures\n\n| Test Failure Symptom | Likely Cause | Diagnostic Steps | Fix Approach |\n|---------------------|--------------|------------------|--------------|\n| Token type mismatch | Keyword lookup failure | Check `SQL_KEYWORDS` mapping and case handling | Add missing keywords or fix case normalization |\n| Wrong token positions | Position tracking bugs | Print actual line/column vs expected | Fix `_advance()` and line counting logic |\n| AST structure differences | Parser logic errors | Use AST visitor to print actual structure | Debug recursive descent function calls |\n| Missing AST children | Incomplete parsing | Check if parser consumes all expected tokens | Verify parser methods call child parsing functions |\n| Error message confusion | Poor error reporting | Print actual error message and compare to expected | Improve error message content and context |\n| Test case brittleness | Over-specific assertions | Review what properties actually matter for correctness | Focus on semantic correctness, not formatting |\n\n\n## Debugging Guide\n\n> **Milestone(s):** This section applies to all milestones (1-4), providing systematic debugging strategies that help developers diagnose and resolve common implementation issues across tokenization, parsing, and AST construction phases.\n\n### Mental Model: Detective Work\n\nThink of debugging a parser like being a detective investigating a crime scene. Just as a detective collects evidence, follows clues, and reconstructs the sequence of events that led to the crime, a parser debugger examines symptoms, traces execution paths, and reconstructs the sequence of tokenization and parsing decisions that led to incorrect behavior. The key insight is that parser bugs leave traces throughout the processing pipeline - a tokenization error creates malformed tokens that cascade into parsing errors, while a parsing logic error produces malformed AST structures that reveal the faulty decision points.\n\nEvery parser bug tells a story through its symptoms. A `TokenType.IDENTIFIER` appearing where `TokenType.SELECT_KEYWORD` was expected might indicate case sensitivity issues in keyword recognition. An `UnexpectedTokenError` with suggestions pointing to multiple valid alternatives often reveals ambiguous grammar rules or insufficient lookahead. A well-constructed `ASTNode` hierarchy with incorrect child relationships typically indicates successful parsing with flawed recursive descent logic. Understanding these diagnostic patterns transforms debugging from random trial-and-error into systematic investigation.\n\nThe debugging process mirrors the parser's own architecture: we start with symptoms at the surface (incorrect final output), trace backward through the AST construction phase to identify structural issues, then examine the parsing logic for faulty token consumption patterns, and finally investigate the tokenization phase for lexical analysis problems. This layered debugging approach ensures we identify root causes rather than treating symptoms, leading to more robust parser implementations.\n\n### Tokenizer Debugging Techniques\n\nTokenizer debugging focuses on diagnosing issues in the lexical analysis phase where character sequences transform into classified tokens. The most effective debugging strategy begins with **token stream inspection** - examining the complete sequence of tokens produced by the `Tokenizer` to identify unexpected token types, missing tokens, or incorrect token boundaries. This inspection reveals whether problems originate in character scanning logic, keyword recognition, or token classification algorithms.\n\nThe `TokenizerTestHelper` provides systematic debugging capabilities through its token inspection methods. The `tokenize_only()` function bypasses parsing entirely, allowing isolated examination of tokenization behavior without interference from downstream parser logic. This isolation proves essential when distinguishing between tokenization failures and parsing failures that might initially present similar symptoms.\n\n**Position tracking verification** represents another critical debugging technique. Every `Token` instance contains `line` and `column` fields that must accurately reflect the token's position in the original SQL text. Incorrect position tracking typically manifests as off-by-one errors in error reporting or misaligned token boundaries when examining multi-line SQL statements. The `_advance()` method in the `Tokenizer` must correctly increment line numbers when encountering newline characters and reset column numbers to handle line boundaries properly.\n\n**Character boundary analysis** helps diagnose issues where tokens appear truncated, extended beyond their proper boundaries, or merged with adjacent tokens. The `_current_char()` and `_peek_char()` methods provide the foundation for character-level debugging - inserting temporary logging statements that reveal the character scanning sequence can expose logic errors in character advancement, whitespace skipping, or string literal parsing.\n\n**Keyword recognition debugging** addresses the common challenge of distinguishing SQL keywords from regular identifiers. The `lookup_keyword()` method performs case-insensitive matching against the `SQL_KEYWORDS` mapping, but bugs often emerge from incorrect case handling or incomplete keyword tables. Testing keyword recognition involves verifying that \"SELECT\", \"select\", \"Select\", and \"sElEcT\" all produce `TokenType.SELECT_KEYWORD` tokens rather than `TokenType.IDENTIFIER` tokens.\n\n| Debugging Technique | When to Apply | Information Revealed | Tools/Methods |\n|---------------------|---------------|----------------------|---------------|\n| Token Stream Inspection | Always first step | Complete tokenization output | `tokenize_only()`, manual token examination |\n| Position Tracking Verification | Error reporting issues | Line/column accuracy | Token position fields, multi-line test cases |\n| Character Boundary Analysis | Token truncation/merging | Character scanning behavior | `_current_char()`, `_peek_char()` logging |\n| Keyword Recognition Testing | Identifier/keyword confusion | Case sensitivity issues | `lookup_keyword()` with various case combinations |\n| String Literal Boundary Testing | Quote parsing problems | String parsing logic | Test cases with embedded quotes, escape sequences |\n| Operator Recognition Testing | Multi-character operator issues | Character sequence handling | Test \"<=\", \">=\", \"!=\", \"<>\" recognition |\n\n**String literal debugging** requires special attention due to the complexity of quote character handling, escape sequences, and embedded quote recognition. The `_read_string_literal()` method must correctly handle both single and double quotes as string delimiters, process escape sequences like `\\'` and `\\\"`, and implement the SQL standard's doubled quote convention where `''` represents a literal single quote within a single-quoted string. Common bugs include incorrect termination detection, failure to advance past closing quotes, and improper escape sequence processing.\n\n**Operator tokenization debugging** focuses on multi-character operators like `<=`, `>=`, `!=`, and `<>` where the tokenizer must implement maximal munch behavior - always preferring the longest possible token match. Debugging involves verifying that `<=` produces a single `TokenType.LESS_THAN_OR_EQUAL` token rather than separate `TokenType.LESS_THAN` and `TokenType.EQUALS` tokens. The `_parse_operator()` method requires lookahead logic to distinguish single-character from multi-character operators.\n\n> **Critical Insight**: Tokenizer bugs compound geometrically through the parser pipeline. A single misclassified token can trigger cascade failures in multiple parsing functions, making the original tokenization error difficult to identify from parser-level symptoms alone. Always verify tokenization correctness before investigating parser logic issues.\n\n### Parser Logic Debugging\n\nParser logic debugging targets the recursive descent parsing phase where token sequences transform into AST structures. Unlike tokenization debugging, which focuses on linear token stream analysis, parser debugging requires understanding the **call stack dynamics** of recursive descent functions and the **state evolution** of the parser's token position and AST construction process.\n\n**Parse trace debugging** provides the most comprehensive view of parser behavior by logging entry and exit points of every parsing function along with the current token position and consumed tokens. This technique reveals whether parsing functions correctly recognize their expected input patterns, properly advance the token position, and construct appropriate AST nodes. The `BaseParser` class maintains the `position` field and `current_token` reference that form the core of parser state - these values should advance monotonically through successful parsing operations.\n\n**Token consumption debugging** examines the interaction between `expect_token()`, `consume_token()`, and `peek_token()` methods that control token stream advancement. Correct parser logic ensures that every consumed token contributes to AST construction and that lookahead operations using `peek_token()` don't inadvertently advance the parser position. Common bugs include forgetting to consume expected tokens, consuming tokens without incorporating them into the AST, or advancing past error positions without proper error reporting.\n\n**AST construction verification** focuses on validating that recursive descent functions build correctly structured AST node hierarchies. Each parsing function should construct AST nodes that accurately represent the parsed SQL construct with proper parent-child relationships and complete field population. The `ASTNode.children()` method provides access to the node hierarchy for debugging purposes, while the `source_location()` method enables verification that AST nodes maintain accurate position information from their corresponding tokens.\n\n**Precedence climbing debugging** specifically targets the `ExpressionParser.parse_expression()` method and its handling of operator precedence and associativity rules. The precedence climbing algorithm relies on correct implementation of `get_operator_precedence()` and `is_right_associative()` functions, along with precise calculation of binding powers for recursive calls. Debugging involves verifying that expressions like `a AND b OR c` produce AST trees where the `OR` operation becomes the root node with `AND` operation as its left child, reflecting the correct precedence relationship.\n\n| Debugging Category | Focus Area | Key Questions | Investigation Methods |\n|-------------------|-------------|---------------|----------------------|\n| Parse Trace Analysis | Function call sequence | Which functions execute? Where do they fail? | Logging entry/exit points, call stack examination |\n| Token Consumption | Position advancement | Are tokens consumed correctly? Is position tracking accurate? | Monitor `position` field, verify `consume_token()` calls |\n| AST Construction | Node hierarchy | Do AST structures match expected patterns? | Examine `children()` relationships, field population |\n| Precedence Climbing | Expression parsing | Are operators parsed with correct precedence? | Test complex expressions, verify AST tree structure |\n| Error Recovery | Failure handling | How does parser respond to invalid input? | Test malformed SQL, verify error messages |\n| Lookahead Logic | Future token examination | Does lookahead correctly predict parsing paths? | Monitor `peek_token()` usage, verify prediction accuracy |\n\n**Grammar rule validation** ensures that parsing functions correctly implement their corresponding grammar rules from the SQL specification. Each function like `parse_select_statement()`, `parse_column_list()`, and `parse_table_reference()` embodies specific grammar productions. Debugging involves comparing the function's implementation against its formal grammar definition to verify correct handling of optional elements, alternative productions, and sequencing requirements.\n\n**Synchronization point debugging** addresses error recovery behavior when the parser encounters unexpected tokens or syntax violations. The `ErrorRecoveryManager` implements panic-mode recovery that skips tokens until reaching predetermined synchronization points like statement boundaries or major clause keywords. Debugging focuses on verifying that error recovery doesn't inadvertently skip valid tokens or fail to resume parsing at appropriate positions.\n\n**Context-dependent parsing debugging** handles situations where the same token sequence might have different meanings depending on parsing context. SQL's grammar includes several context-dependent constructs - for example, identifiers that might represent column names, table names, or alias names depending on their position within the statement. The parser maintains context through its call stack and current parsing state, making context debugging particularly challenging.\n\n> **Key Debugging Principle**: Parser bugs often manifest far from their root cause due to the recursive nature of descent parsing. An error in `parse_primary_expression()` might not surface until `parse_select_statement()` attempts to construct its final AST node. Always trace backward from error symptoms to identify the earliest point of incorrect behavior.\n\n### Common Bug Symptom Analysis\n\nSystematic bug analysis relies on recognizing patterns between observed symptoms and their underlying causes. The following diagnostic table provides a structured approach to identifying and resolving the most frequent parser implementation issues across all milestone phases.\n\n**Tokenization Phase Issues:**\n\n| Symptom | Likely Root Cause | Diagnostic Steps | Resolution Strategy |\n|---------|-------------------|------------------|---------------------|\n| Keywords parsed as `IDENTIFIER` tokens | Case sensitivity in `lookup_keyword()` | Test \"SELECT\" vs \"select\" recognition | Implement case-insensitive keyword matching with `str.upper()` |\n| String literals missing closing quotes | Incorrect termination logic in `_read_string_literal()` | Examine quote character advancement | Fix quote consumption after string content parsing |\n| Numbers parsed as multiple tokens | Character boundary detection failure | Test \"123.45\" and \"1e10\" tokenization | Improve decimal point and scientific notation handling |\n| Multi-character operators split incorrectly | Missing lookahead in `_parse_operator()` | Test \"<=\" becoming \"<\" + \"=\" | Implement maximal munch with `_peek_char()` lookahead |\n| Position tracking incorrect | Line/column advancement logic errors | Compare reported positions with actual text locations | Fix newline detection and column reset logic |\n| Whitespace included in token values | Incomplete `_skip_whitespace()` implementation | Examine tokens for leading/trailing spaces | Ensure whitespace skipping before token recognition |\n\n**Parsing Phase Issues:**\n\n| Symptom | Likely Root Cause | Diagnostic Steps | Resolution Strategy |\n|---------|-------------------|------------------|---------------------|\n| `UnexpectedTokenError` with correct tokens | Incorrect `expect_token()` usage | Verify expected vs actual token types | Review grammar rule implementation for token sequence |\n| Parser infinite loop | Missing `consume_token()` calls | Monitor parser position advancement | Add missing token consumption in parsing functions |\n| Incomplete AST nodes | Partial field population | Examine AST node field values | Complete all required field assignments in constructors |\n| Incorrect operator precedence | Wrong `OPERATOR_PRECEDENCE` mapping | Test \"a AND b OR c\" expression parsing | Correct precedence values in operator table |\n| Missing optional clauses | Lookahead logic failures | Test queries with/without optional elements | Implement proper `peek_token()` for optional detection |\n| Malformed error messages | Insufficient context in error creation | Review error message content and position | Enhance error creation with parsing context information |\n\n**Expression Parsing Issues:**\n\n| Symptom | Likely Root Cause | Diagnostic Steps | Resolution Strategy |\n|---------|-------------------|------------------|---------------------|\n| Right-associative operators parsed left-associatively | Incorrect associativity handling | Test \"a = b = c\" expression parsing | Fix `is_right_associative()` and binding power calculation |\n| Parentheses not overriding precedence | Parenthetical expression parsing bugs | Test \"(a OR b) AND c\" vs \"a OR (b AND c)\" | Debug `parse_parenthesized_expression()` implementation |\n| Unary operators binding incorrectly | Wrong unary precedence assignment | Test \"NOT a AND b\" expression | Assign highest precedence to unary operators |\n| NULL comparisons parsed incorrectly | Missing NULL test recognition | Test \"column IS NULL\" expressions | Implement `parse_null_test_expression()` properly |\n| Qualified identifiers not recognized | Dot operator handling issues | Test \"table.column\" parsing | Fix qualified identifier parsing in `parse_identifier_expression()` |\n| Function calls parsed as identifiers | Missing function call detection | Test \"COUNT(*)\" parsing | Add function call recognition in primary expression parsing |\n\n**Statement Construction Issues:**\n\n| Symptom | Likely Root Cause | Diagnostic Steps | Resolution Strategy |\n|---------|-------------------|------------------|---------------------|\n| Column count mismatch in INSERT | Missing validation in `parse_insert_statement()` | Test INSERT with mismatched columns/values | Implement `validate_column_value_consistency()` |\n| UPDATE without WHERE parsed incorrectly | Optional WHERE clause handling | Test UPDATE statements without WHERE | Make WHERE clause properly optional in UPDATE parsing |\n| DELETE safety warnings not generated | Missing safety flag assignment | Test \"DELETE FROM table\" without WHERE | Add safety warnings for DELETE without WHERE conditions |\n| Alias recognition inconsistent | AS keyword optionality issues | Test both \"table AS alias\" and \"table alias\" | Implement both explicit and implicit alias recognition |\n| Multiple value rows not parsed | Value list iteration logic | Test \"VALUES (1,2), (3,4)\" parsing | Fix `parse_value_row_list()` loop implementation |\n| Assignment expressions malformed | SET clause parsing errors | Test \"SET col1=val1, col2=val2\" | Debug comma-separated assignment list parsing |\n\n**Error Recovery Issues:**\n\n| Symptom | Likely Root Cause | Diagnostic Steps | Resolution Strategy |\n|---------|-------------------|------------------|---------------------|\n| Parser stops at first error | No error recovery implementation | Test multiple syntax errors in single statement | Implement panic-mode recovery with synchronization points |\n| Cascade error avalanche | Insufficient error suppression | Examine error count and proximity | Add cascade error detection and suppression |\n| Incorrect error positions | Position tracking during recovery | Verify error line/column accuracy | Maintain accurate position during token skipping |\n| Recovery never synchronizes | Wrong synchronization points | Test recovery behavior after various error types | Choose better synchronization token types |\n| False error suppression | Overly aggressive cascade detection | Test legitimate multiple errors | Tune `CASCADE_SUPPRESSION_DISTANCE` parameter |\n| Missing error context | Insufficient error message detail | Review error message informativeness | Add parsing context and suggestions to error messages |\n\n⚠️ **Pitfall: Debugging Without Systematic Approach**\n\nMany developers attempt parser debugging through random code changes or by adding print statements throughout the codebase without understanding the specific failure mode. This approach often introduces additional bugs while failing to identify root causes. Instead, always begin with symptom classification using the diagnostic tables above, then apply targeted debugging techniques specific to the identified failure category.\n\n⚠️ **Pitfall: Ignoring Error Position Information**\n\nParser errors often occur at positions different from where symptoms become apparent. The `source_location()` method and `Token` position fields provide critical debugging information that pinpoints exact failure locations. Developers frequently overlook this position data and instead focus on high-level symptoms, making bug localization unnecessarily difficult.\n\n### Implementation Guidance\n\nThe debugging infrastructure requires systematic tooling that supports both automated testing and interactive investigation of parser behavior. The following implementation provides comprehensive debugging capabilities while maintaining clear separation between diagnostic tools and production parser code.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Debug Logging | Python `logging` module with configurable levels | Structured logging with JSON output and external log analysis |\n| Token Inspection | Simple `print()` statements with token formatting | Rich console output with syntax highlighting using `rich` library |\n| AST Visualization | Text-based tree printing with indentation | Graphical tree rendering with `graphviz` or web-based visualization |\n| Test Case Management | Built-in `unittest` with manual test cases | `pytest` with parameterized tests and fixture management |\n| Performance Profiling | Basic timing with `time.time()` | Advanced profiling with `cProfile` and memory usage tracking |\n| Error Analysis | Manual error message inspection | Automated error pattern recognition and categorization |\n\n**Recommended Debug Infrastructure Structure:**\n\n```\nsql_parser/\n  debug/\n    __init__.py                    ← debug utilities public interface\n    tokenizer_debugger.py         ← tokenizer-specific debugging tools\n    parser_debugger.py            ← parser-specific debugging tools\n    ast_inspector.py              ← AST structure analysis tools\n    error_analyzer.py             ← error pattern analysis and reporting\n    test_case_generator.py        ← automated test case generation\n  tests/\n    debug/\n      test_tokenizer_debugger.py  ← debug tool validation\n      test_parser_debugger.py     ← parser debugging validation\n    fixtures/\n      failing_sql_cases.txt       ← known problematic SQL examples\n      expected_error_patterns.json ← expected error message patterns\n  examples/\n    debug_session_examples.py     ← interactive debugging examples\n```\n\n**Complete Tokenizer Debugging Infrastructure:**\n\n```python\nimport logging\nfrom typing import List, Optional, Dict, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom ..tokenizer import Tokenizer, Token, TokenType, TokenizerError\n\nclass DebugLevel(Enum):\n    BASIC = \"basic\"\n    DETAILED = \"detailed\" \n    VERBOSE = \"verbose\"\n\n@dataclass\nclass TokenizerDiagnostic:\n    position: int\n    character: Optional[str]\n    token_start: int\n    token_value: str\n    token_type: TokenType\n    issues: List[str]\n\nclass TokenizerDebugger:\n    def __init__(self, tokenizer: Tokenizer, debug_level: DebugLevel = DebugLevel.BASIC):\n        self.tokenizer = tokenizer\n        self.debug_level = debug_level\n        self.diagnostics: List[TokenizerDiagnostic] = []\n        self.logger = self._setup_logger()\n\n    def _setup_logger(self) -> logging.Logger:\n        # TODO 1: Create logger with appropriate formatting for tokenizer debugging\n        # TODO 2: Set log level based on debug_level (INFO for BASIC, DEBUG for DETAILED/VERBOSE)\n        # TODO 3: Add console handler with formatted output showing position and token info\n        pass\n\n    def debug_tokenization(self, sql_text: str) -> List[Token]:\n        \"\"\"\n        Perform tokenization with comprehensive debugging output.\n        Logs every character advancement, token recognition, and potential issues.\n        \"\"\"\n        # TODO 1: Reset diagnostics and configure tokenizer with sql_text\n        # TODO 2: Instrument tokenizer methods to capture character-level scanning\n        # TODO 3: Log token recognition events with position information\n        # TODO 4: Detect and report potential issues (case sensitivity, boundary problems)\n        # TODO 5: Generate summary report of tokenization process\n        # Hint: Use monkey patching or inheritance to add debug logging to tokenizer methods\n        pass\n\n    def analyze_keyword_recognition(self, test_cases: Dict[str, TokenType]) -> Dict[str, bool]:\n        \"\"\"\n        Test keyword recognition with various case combinations.\n        Returns mapping of test case to pass/fail status.\n        \"\"\"\n        # TODO 1: Iterate through test_cases dictionary\n        # TODO 2: Tokenize each test case and extract first token\n        # TODO 3: Compare actual token type with expected token type\n        # TODO 4: Log mismatches with detailed case information\n        # TODO 5: Return success/failure mapping for test suite analysis\n        pass\n\n    def inspect_token_boundaries(self, sql_text: str) -> List[TokenizerDiagnostic]:\n        \"\"\"\n        Analyze token boundary detection for potential truncation or extension issues.\n        \"\"\"\n        # TODO 1: Tokenize sql_text while tracking character-level position\n        # TODO 2: For each token, verify that token value matches expected character sequence\n        # TODO 3: Check for unexpected whitespace inclusion in token values\n        # TODO 4: Identify potential off-by-one errors in token start/end positions\n        # TODO 5: Generate diagnostic reports for boundary issues\n        pass\n\n    def validate_position_tracking(self, multiline_sql: str) -> bool:\n        \"\"\"\n        Verify that line and column tracking correctly handles multi-line SQL statements.\n        \"\"\"\n        # TODO 1: Tokenize multiline SQL with embedded newlines\n        # TODO 2: For each token, verify line/column accuracy against manual calculation\n        # TODO 3: Check that newline characters correctly increment line and reset column\n        # TODO 4: Validate that position information enables accurate error reporting\n        # TODO 5: Return True if all position tracking is accurate, False otherwise\n        pass\n```\n\n**Complete Parser Debugging Infrastructure:**\n\n```python\nfrom typing import List, Dict, Optional, Any, Callable\nfrom contextlib import contextmanager\nfrom ..parser import SQLParser, BaseParser, SelectParser, ExpressionParser\nfrom ..ast_nodes import ASTNode, SelectStatement, BinaryOperation\nfrom ..errors import ParseError, UnexpectedTokenError, SyntaxError\n\n@dataclass\nclass ParseTraceEntry:\n    function_name: str\n    entry_position: int\n    entry_token: Optional[Token]\n    exit_position: Optional[int]\n    exit_token: Optional[Token]\n    result_node: Optional[ASTNode]\n    error: Optional[ParseError]\n    duration_ms: float\n\nclass ParserDebugger:\n    def __init__(self, parser: SQLParser):\n        self.parser = parser\n        self.trace_entries: List[ParseTraceEntry] = []\n        self.ast_snapshots: Dict[int, ASTNode] = {}\n        \n    def debug_parse_statement(self, sql_text: str) -> Optional[ASTNode]:\n        \"\"\"\n        Parse SQL statement with comprehensive debugging trace and AST inspection.\n        \"\"\"\n        # TODO 1: Initialize trace collection and instrument parser methods\n        # TODO 2: Execute parsing with entry/exit logging for each function\n        # TODO 3: Capture AST node creation and modification events\n        # TODO 4: Record error occurrences with full context information\n        # TODO 5: Generate detailed parsing report with trace analysis\n        # Hint: Use decorators or context managers to instrument parsing functions\n        pass\n\n    @contextmanager\n    def trace_function(self, function_name: str):\n        \"\"\"\n        Context manager for tracing individual parsing function execution.\n        \"\"\"\n        # TODO 1: Record function entry with current parser state\n        # TODO 2: Start timing measurement for performance analysis\n        # TODO 3: Yield control to traced function execution\n        # TODO 4: Record function exit with result and final parser state\n        # TODO 5: Calculate execution time and update trace entry\n        pass\n\n    def analyze_precedence_parsing(self, expression_sql: str) -> Dict[str, Any]:\n        \"\"\"\n        Deep analysis of expression parsing with operator precedence validation.\n        \"\"\"\n        # TODO 1: Parse expression while tracking precedence climbing decisions\n        # TODO 2: Log each precedence comparison and binding power calculation\n        # TODO 3: Validate final AST structure against expected operator precedence\n        # TODO 4: Identify associativity handling for operators of equal precedence\n        # TODO 5: Generate report with precedence decision tree visualization\n        pass\n\n    def inspect_ast_structure(self, root_node: ASTNode, max_depth: int = 5) -> str:\n        \"\"\"\n        Generate detailed textual representation of AST structure for inspection.\n        \"\"\"\n        # TODO 1: Implement recursive AST traversal with depth tracking\n        # TODO 2: Format node information with type, fields, and child relationships\n        # TODO 3: Include source location information for error debugging\n        # TODO 4: Detect and highlight potential structural issues\n        # TODO 5: Return formatted string suitable for console or log output\n        pass\n\n    def validate_error_recovery(self, malformed_sql: str) -> Dict[str, Any]:\n        \"\"\"\n        Test error recovery behavior with intentionally malformed SQL statements.\n        \"\"\"\n        # TODO 1: Parse malformed SQL and capture all error occurrences\n        # TODO 2: Analyze error recovery synchronization point selection\n        # TODO 3: Verify that parser continues after errors when possible\n        # TODO 4: Check error message quality and position accuracy\n        # TODO 5: Generate recovery behavior analysis report\n        pass\n```\n\n**Milestone Debugging Checkpoints:**\n\n**Milestone 1 - Tokenizer Validation:**\n- **Test Command**: `python -m sql_parser.debug.tokenizer_debugger test_keyword_cases.py`\n- **Expected Output**: All SQL keywords recognized correctly in upper, lower, and mixed case\n- **Manual Verification**: Tokenize \"SELECT * FROM users WHERE id = 123\" and verify 9 tokens with correct types\n- **Failure Signs**: Keywords appearing as IDENTIFIER tokens, missing tokens, incorrect position information\n\n**Milestone 2 - SELECT Parser Validation:**\n- **Test Command**: `python -m sql_parser.debug.parser_debugger test_select_parsing.py`\n- **Expected Output**: Valid AST tree with SelectStatement root, column list, and table reference\n- **Manual Verification**: Parse \"SELECT a, b AS alias FROM table1\" and inspect AST structure\n- **Failure Signs**: UnexpectedTokenError for valid syntax, incomplete AST nodes, missing alias handling\n\n**Milestone 3 - Expression Parser Validation:**\n- **Test Command**: `python -m sql_parser.debug.parser_debugger test_expression_precedence.py`\n- **Expected Output**: Expression AST with correct operator precedence and associativity\n- **Manual Verification**: Parse \"a AND b OR c = d\" and verify OR at root with AND as left child\n- **Failure Signs**: Incorrect precedence in AST tree, wrong associativity for equal precedence operators\n\n**Milestone 4 - DML Parser Validation:**\n- **Test Command**: `python -m sql_parser.debug.parser_debugger test_dml_statements.py`\n- **Expected Output**: Valid INSERT, UPDATE, DELETE statement AST nodes with proper structure\n- **Manual Verification**: Parse all three DML statement types and verify complete field population\n- **Failure Signs**: Column/value count mismatches, missing WHERE clause handling, incomplete SET expressions\n\n\n## Future Extensions and Extensibility\n\n> **Milestone(s):** This section provides guidance for extending all milestones (1-4), establishing extensibility patterns and future roadmap considerations that help developers build upon the foundational parser architecture.\n\n### Mental Model: Language Evolution\n\nThink of our SQL parser like a city's infrastructure - we're building the initial roads, utilities, and zoning laws that will support future development. Just as a well-planned city can expand from a town to a metropolis without completely rebuilding its core infrastructure, our parser architecture must anticipate growth while maintaining its foundational integrity. The tokenizer is like the postal system that must handle new address formats, the parser is like the legal system that must accommodate new types of laws, and the AST is like the urban planning framework that must represent increasingly complex structures.\n\nThe key insight is that **extensibility isn't just about adding new features - it's about preserving the conceptual clarity and architectural integrity that made the original system comprehensible**. Each extension should feel like a natural evolution of the existing patterns rather than an awkward bolt-on addition.\n\n### Grammar Extension Patterns\n\nOur parser's extensibility depends on recognizing that SQL growth follows predictable patterns. New features typically fall into one of several categories: new keywords that introduce statement variations, new operators that extend expression capabilities, new data types that expand literal parsing, and new statement types that follow existing structural conventions.\n\n#### Keyword Extension Strategy\n\nThe foundation of keyword extensibility lies in our centralized `SQL_KEYWORDS` mapping and the `TokenType` enum. When SQL standards introduce new keywords like `WITH` for common table expressions or `WINDOW` for window functions, our architecture can accommodate them through a systematic extension process.\n\n**New keywords follow this integration pattern**:\n1. Add the new token type to the `TokenType` enum with a descriptive name following our `KEYWORD` suffix convention\n2. Insert the keyword-to-token mapping in the `SQL_KEYWORDS` dictionary with uppercase key for case-insensitive matching\n3. Update the tokenizer's `lookup_keyword()` method to recognize the new keyword automatically through the expanded mapping\n4. Extend the parser's grammar rules to incorporate the new keyword in appropriate contexts\n5. Create corresponding AST node types to represent the new syntax constructs\n\nThe tokenizer requires no modifications beyond updating the keyword dictionary because it uses **table-driven keyword recognition** rather than hardcoded conditional logic. This design decision pays dividends during extension - adding `WINDOW_KEYWORD` is identical in complexity to adding any other keyword.\n\nHowever, keyword extension involves important **precedence and context considerations**. Some keywords are context-sensitive (like `AS` which can appear in multiple statement contexts), while others introduce new parsing contexts entirely (like `WITH` which creates a recursive parsing scenario). Our recursive descent parser handles these through **context-aware parsing functions** that examine the current parsing state to determine keyword interpretation.\n\n| Keyword Category | Extension Complexity | Example Keywords | Integration Points |\n|------------------|---------------------|------------------|-------------------|\n| Statement Introducer | Low | WITH, MERGE, TRUNCATE | Add new statement parser function |\n| Clause Modifier | Medium | DISTINCT, ALL, UNIQUE | Extend existing clause parsers |\n| Expression Operator | Medium | LIKE, BETWEEN, IN | Extend expression parser precedence table |\n| Context Sensitive | High | AS, JOIN, ON | Update multiple parsing contexts |\n\n#### Operator Extension Strategy\n\nExtending the parser with new operators requires careful integration with our **precedence climbing expression parser**. The `OPERATOR_PRECEDENCE` dictionary serves as the central configuration point for operator behavior, defining both precedence levels and associativity rules.\n\n**Binary operator extension follows this process**:\n1. Add the operator token type to `TokenType` enum (e.g., `REGEXP_OPERATOR`)\n2. Update the tokenizer's `_parse_operator()` method to recognize the new operator symbol(s)\n3. Insert the operator into `OPERATOR_PRECEDENCE` with appropriate `OperatorInfo` containing precedence level and associativity\n4. Extend the `BinaryOperation` AST node to handle the new operator type in its semantic analysis\n5. Update expression parsing logic to construct appropriate AST nodes for the new operator\n\nThe critical design decision involves **precedence assignment**. New operators must fit logically into our existing precedence hierarchy. For example, a hypothetical `REGEXP` operator would likely belong at `Precedence.COMPARISON` level (5) alongside other comparison operators, while a new arithmetic operator might fit at `Precedence.ADDITION` (6) or `Precedence.MULTIPLICATION` (7).\n\n**Unary operator extension** requires additional considerations because unary operators appear in primary expression contexts. The `parse_primary_expression()` function must be extended to recognize new unary operator tokens and construct appropriate `UnaryOperation` AST nodes.\n\n> **Design Insight**: The precedence climbing algorithm's strength lies in its **data-driven approach** - new operators require configuration changes rather than algorithmic modifications. This separation of mechanism from policy ensures that operator extensions don't introduce subtle parsing bugs.\n\n#### Statement Type Extension Strategy\n\nAdding new SQL statement types like `MERGE`, `TRUNCATE`, or `CREATE TABLE` leverages our **statement-specific parser pattern**. Each statement type gets its own dedicated parser class that inherits from `BaseParser` and implements the statement's unique grammar rules.\n\n**Statement extension architecture**:\n1. Create a new statement parser class (e.g., `MergeParser`) extending `BaseParser`\n2. Define the corresponding AST node type (e.g., `MergeStatement`) with appropriate fields for the statement's components\n3. Add the statement's introductory keyword to the tokenizer's keyword recognition\n4. Extend the main `SQLParser.parse()` method to dispatch to the new parser based on the leading keyword\n5. Implement the statement-specific parsing logic following our established recursive descent patterns\n\nThe `SQLParser` class uses a **dispatch table pattern** to route different statement types to their appropriate parsers:\n\n| Statement Token | Parser Class | AST Node Type | Complexity Level |\n|----------------|--------------|---------------|------------------|\n| SELECT_KEYWORD | SelectParser | SelectStatement | Reference Implementation |\n| INSERT_KEYWORD | DMLParser | InsertStatement | Reference Implementation |\n| CREATE_KEYWORD | DDLParser | CreateStatement | Future Extension |\n| WITH_KEYWORD | CTEParser | WithStatement | Future Extension |\n\nEach statement parser maintains the same **interface contract**:\n- Accept a token stream from the tokenizer\n- Implement recursive descent parsing for the statement's grammar\n- Construct and return an appropriate AST node representing the parsed statement\n- Handle statement-specific error conditions and recovery scenarios\n\n#### Data Type Extension Strategy\n\nSQL's type system continues evolving with new data types like JSON, XML, arrays, and user-defined types. Our parser's literal parsing capabilities can extend to accommodate these new types through systematic tokenizer and parser enhancements.\n\n**Literal type extension process**:\n1. Add new token types to represent the literals (e.g., `JSON_LITERAL`, `ARRAY_LITERAL`)\n2. Extend the tokenizer's character scanning logic to recognize the new literal formats\n3. Create corresponding AST node types (e.g., `JsonLiteral`, `ArrayLiteral`) with appropriate value representation\n4. Update the expression parser's `parse_primary_expression()` to construct the new literal nodes\n5. Consider type-specific validation rules that should be enforced during parsing\n\n**Complex literal types** like arrays or JSON objects require more sophisticated tokenizer logic because they contain **nested structure**. The tokenizer must balance brackets, handle escape sequences, and potentially perform recursive parsing within the literal value. This complexity suggests that some advanced literals might require **dedicated sub-parsers** rather than simple character scanning.\n\n### Advanced SQL Feature Roadmap\n\nOur parser architecture anticipates several major SQL feature categories that represent natural evolution paths from our current implementation. Each category presents unique architectural challenges and opportunities for demonstrating advanced parsing techniques.\n\n#### JOIN Operation Support\n\nJOIN operations represent the next logical extension beyond our current single-table `FROM` clause parsing. JOIN syntax introduces **multiple parsing complexities**: table relationship specifications, join condition expressions, and various join types (INNER, LEFT OUTER, RIGHT OUTER, FULL OUTER, CROSS).\n\n**JOIN parsing architectural requirements**:\n- Extend `TableReference` to support **joined table expressions** rather than single table identifiers\n- Create new AST node types (`JoinExpression`, `JoinCondition`) to represent the relational algebra\n- Modify the `FROM` clause parser to handle comma-separated table lists and explicit JOIN syntax\n- Extend expression parsing to handle **correlation names** and **qualified column references** across multiple tables\n- Implement **semantic validation** to ensure JOIN conditions reference valid table columns\n\nThe parsing complexity arises from JOIN's **left-associative grouping** and **precedence interactions** with other clauses. Consider the statement: `SELECT * FROM table1 JOIN table2 ON condition1 JOIN table3 ON condition2`. The parser must construct a **left-deep join tree** where each JOIN operation builds upon the previous result.\n\n**JOIN precedence considerations**:\n| Join Type | Precedence Level | Associativity | Parsing Challenge |\n|-----------|------------------|---------------|-------------------|\n| CROSS JOIN | Highest | Left | Simple table product |\n| INNER JOIN | High | Left | Requires ON or USING clause |\n| OUTER JOIN | High | Left | Complex condition validation |\n| NATURAL JOIN | Medium | Left | Implicit column matching |\n\n#### Subquery and Nested Expression Support\n\nSubqueries introduce **recursive parsing scenarios** where our parser must handle complete SELECT statements embedded within expression contexts. This feature tests our parser's architectural flexibility because subqueries can appear in SELECT lists, WHERE clauses, FROM clauses, and even within other subqueries.\n\n**Subquery parsing architectural challenges**:\n- Modify expression parsing to recognize parenthesized SELECT statements as **scalar expressions**\n- Extend `FROM` clause parsing to handle **derived tables** (subqueries in FROM clause)\n- Implement **scope management** for column references that might resolve to outer query tables\n- Handle **correlated subqueries** where inner queries reference outer query columns\n- Manage **recursion depth limits** to prevent infinite parsing loops\n\nThe critical architectural decision involves **scope chain management**. When parsing a subquery, column references might resolve to tables in the current query, parent queries, or even grandparent queries. Our parser needs a **symbol table stack** that tracks available column names at each nesting level.\n\n> **Architecture Decision: Subquery Scope Management**\n> - **Context**: Subqueries need access to column names from outer query scopes for correlation\n> - **Options Considered**: \n>   - Global symbol table with query-specific namespaces\n>   - Stack-based scope chain with parent query contexts\n>   - Deferred resolution with post-parsing symbol analysis\n> - **Decision**: Stack-based scope chain with incremental symbol resolution\n> - **Rationale**: Provides natural nesting semantics, enables early error detection, and integrates cleanly with recursive descent parsing\n> - **Consequences**: Requires careful scope management during parsing but enables comprehensive semantic validation\n\n#### Window Function Integration\n\nWindow functions like `ROW_NUMBER() OVER (PARTITION BY column ORDER BY column)` represent advanced expression types that combine function calls with specialized clause parsing. They demonstrate how our expression parser can extend to handle **context-specific syntax** within general expression contexts.\n\n**Window function parsing requirements**:\n- Extend function call parsing to recognize `OVER` clauses following function invocation\n- Implement **window specification parsing** for PARTITION BY and ORDER BY clauses within window context\n- Create specialized AST nodes (`WindowFunction`, `WindowSpecification`) to represent the complex syntax\n- Handle **window frame specifications** (ROWS, RANGE, GROUPS) with their boundary expressions\n- Integrate window functions into expression precedence hierarchy appropriately\n\nWindow functions illustrate the **composability principle** in parser design - they're expressions that contain clause-like internal structure. Our expression parser must seamlessly transition between expression parsing and clause parsing modes when encountering window syntax.\n\n#### Common Table Expression (CTE) Support\n\nWITH clauses for common table expressions introduce **named subquery definitions** that precede the main query. CTEs test our parser's ability to handle **query-level preprocessing** where named table expressions must be parsed, validated, and made available to the main query parser.\n\n**CTE parsing architectural implications**:\n- Extend the main parser to recognize WITH clauses before statement parsing\n- Implement **recursive CTE parsing** where CTE definitions can reference previously defined CTEs\n- Create symbol table management for **temporary table names** that exist only within query scope\n- Handle **recursive CTE syntax** with UNION operations and termination conditions\n- Validate **CTE reference consistency** ensuring all referenced CTEs are properly defined\n\nCTEs represent a **preprocessing phase** in SQL parsing where the parser must build a **dependency graph** of table definitions before parsing the main query. This architectural pattern extends naturally to other features like temporary tables or view definitions.\n\n#### Advanced Expression Features\n\nSeveral SQL expression extensions would demonstrate sophisticated parsing techniques: **CASE expressions** with their complex conditional logic, **aggregate functions** with DISTINCT and filtering clauses, **type casting** with explicit conversion syntax, and **array operations** with subscripting and slicing.\n\n**CASE expression parsing challenges**:\n| CASE Type | Syntax Pattern | Parsing Complexity | AST Representation |\n|-----------|---------------|-------------------|-------------------|\n| Simple CASE | CASE expr WHEN val THEN result | Medium | CaseExpression with value matching |\n| Searched CASE | CASE WHEN condition THEN result | High | CaseExpression with boolean conditions |\n| Nested CASE | CASE within WHEN/THEN clauses | Very High | Recursive CaseExpression nodes |\n\nEach advanced expression type requires **specialized parsing logic** while maintaining integration with our existing expression precedence system. The key architectural principle is ensuring that complex expressions **compose naturally** with simpler expressions in all syntactic contexts.\n\n### Extension Implementation Patterns\n\nSuccessful parser extensions follow established **architectural patterns** that maintain system coherence while adding new capabilities. These patterns emerged from our design decisions around separation of concerns, recursive descent parsing, and AST-based representation.\n\n#### Plugin Architecture Pattern\n\nFor maximum extensibility, our parser can adopt a **plugin architecture** where new SQL features register themselves with the core parsing engine. This pattern enables **feature modularity** and **selective compilation** of SQL dialects.\n\n**Plugin registration mechanism**:\n- Define `ParserExtension` interface with methods for keyword registration, token type declaration, and parsing function contribution\n- Implement `ExtensionRegistry` that manages feature registration and parser integration\n- Create **feature-specific modules** (e.g., `json_extension.py`, `window_function_extension.py`) that implement the extension interface\n- Enable **runtime configuration** where applications can enable/disable specific SQL feature sets\n\nThis architecture supports **SQL dialect customization** where different applications might need different subsets of SQL functionality. A lightweight embedded application might enable only basic SELECT/INSERT operations, while a full database system might enable all extensions.\n\n#### Grammar Composition Pattern\n\nComplex SQL features often **compose existing grammar rules** in new combinations. Our parser architecture should support **grammar rule reuse** where new features can leverage existing parsing functions for their constituent parts.\n\n**Grammar composition examples**:\n- Window functions reuse ORDER BY parsing logic from the main query parser\n- CTE definitions reuse complete SELECT statement parsing for their query definitions  \n- MERGE statements combine INSERT, UPDATE, and DELETE parsing logic for their action clauses\n- Complex literals (arrays, JSON) reuse expression parsing for their element values\n\nThis pattern reduces code duplication and ensures consistent parsing behavior across SQL features that share syntactic elements.\n\n#### Semantic Validation Extension Pattern\n\nAs our parser grows more sophisticated, **semantic validation** becomes increasingly important. Extensions should provide **pluggable validation rules** that can analyze AST structures for logical consistency beyond pure syntax correctness.\n\n**Validation extension architecture**:\n- Define `ValidationRule` interface with methods for AST node analysis and error reporting\n- Implement **validation phases** that traverse the complete AST after parsing completes\n- Create **context-sensitive validators** that understand cross-references between AST nodes\n- Enable **progressive validation** where basic syntax errors are caught during parsing, while complex semantic errors are caught during validation phases\n\nExamples of semantic validation rules include: ensuring INSERT column counts match value counts, validating that referenced table names exist in the FROM clause, checking that aggregate functions don't appear in WHERE clauses (except in subqueries), and verifying that window functions use valid column references in their PARTITION BY clauses.\n\n> **Future Architecture Principle**: Extensions should **enhance the parser's capabilities without compromising its conceptual clarity**. Each new feature should feel like a natural evolution of existing patterns rather than an architectural departure.\n\n### Implementation Guidance\n\nThe following implementation guidance provides concrete patterns and starter code for extending our SQL parser architecture. These examples demonstrate how to add new language features while maintaining architectural consistency and code quality.\n\n#### Technology Recommendations for Extensions\n\n| Extension Type | Simple Approach | Advanced Approach |\n|----------------|----------------|-------------------|\n| New Keywords | Direct enum/dictionary updates | Plugin-based keyword registry |\n| New Operators | Hardcoded precedence table entries | Configurable precedence system |\n| New Statements | Statement-specific parser classes | Grammar rule composition framework |\n| Semantic Validation | Post-parsing validation functions | Visitor-based validation pipeline |\n\n#### Recommended Extension File Structure\n\nExtensions should follow a **modular architecture** that keeps new functionality separated from core parser logic while maintaining clear integration points:\n\n```\nsql_parser/\n  core/\n    tokenizer.py              ← Core tokenization logic\n    base_parser.py            ← Base recursive descent parser\n    ast_nodes.py              ← Core AST node definitions\n  statements/\n    select_parser.py          ← SELECT statement parsing\n    dml_parser.py             ← INSERT/UPDATE/DELETE parsing\n    ddl_parser.py             ← Future: CREATE/DROP statements\n  expressions/\n    expression_parser.py      ← Core expression parsing\n    window_functions.py       ← Future: Window function extensions\n    case_expressions.py       ← Future: CASE expression parsing\n  extensions/\n    __init__.py\n    registry.py               ← Extension registration system\n    json_support.py           ← Future: JSON literal/function support\n    cte_support.py            ← Future: Common table expression support\n  validation/\n    semantic_validator.py     ← Post-parsing semantic validation\n    rules/\n      reference_validator.py  ← Column/table reference validation\n      type_validator.py       ← Type consistency validation\n```\n\n#### Extension Registry Infrastructure\n\nThis infrastructure provides the foundation for pluggable parser extensions. The registry manages feature registration and integration with the core parsing engine:\n\n```python\n# extensions/registry.py\nfrom typing import Dict, List, Callable, Type\nfrom enum import Enum\nfrom dataclasses import dataclass\n\nclass ExtensionType(Enum):\n    KEYWORD = \"keyword\"\n    OPERATOR = \"operator\" \n    STATEMENT = \"statement\"\n    EXPRESSION = \"expression\"\n    VALIDATOR = \"validator\"\n\n@dataclass\nclass ExtensionInfo:\n    name: str\n    extension_type: ExtensionType\n    priority: int\n    dependencies: List[str]\n    registration_callback: Callable\n\nclass ExtensionRegistry:\n    def __init__(self):\n        self._extensions: Dict[str, ExtensionInfo] = {}\n        self._keyword_extensions: Dict[str, TokenType] = {}\n        self._operator_extensions: Dict[TokenType, OperatorInfo] = {}\n        self._statement_parsers: Dict[TokenType, Type[BaseParser]] = {}\n        self._validation_rules: List[ValidationRule] = []\n    \n    def register_extension(self, extension_info: ExtensionInfo):\n        \"\"\"Register a new parser extension with dependency checking.\"\"\"\n        # TODO: Validate extension dependencies are satisfied\n        # TODO: Check for extension name conflicts\n        # TODO: Sort extensions by priority for ordered registration\n        # TODO: Call extension's registration callback to integrate with parser\n        # TODO: Update internal extension registries based on extension type\n        \n    def get_keyword_token_type(self, keyword: str) -> Optional[TokenType]:\n        \"\"\"Look up extended keyword token types beyond core SQL keywords.\"\"\"\n        # TODO: Check core SQL_KEYWORDS first\n        # TODO: Search registered keyword extensions\n        # TODO: Return appropriate TokenType or None if not found\n        \n    def get_statement_parser(self, token_type: TokenType) -> Optional[Type[BaseParser]]:\n        \"\"\"Retrieve parser class for extended statement types.\"\"\"\n        # TODO: Look up token_type in statement parser registry\n        # TODO: Return parser class or None if not registered\n        \n    def get_validation_rules(self) -> List[ValidationRule]:\n        \"\"\"Return all registered validation rules in execution order.\"\"\"\n        # TODO: Return validation rules sorted by priority\n        # TODO: Consider rule dependencies and execution phases\n```\n\n#### Keyword Extension Pattern Implementation\n\nThis pattern demonstrates how to add new SQL keywords while maintaining the tokenizer's case-insensitive recognition and the parser's grammar rule integration:\n\n```python\n# extensions/json_support.py\nfrom typing import Optional\nfrom core.ast_nodes import ASTNode, Expression\nfrom core.base_parser import BaseParser\nfrom extensions.registry import ExtensionRegistry, ExtensionInfo, ExtensionType\n\n# New token types for JSON support\nclass JsonTokenTypes:\n    JSON_KEYWORD = \"JSON_KEYWORD\"\n    JSON_EXTRACT_OPERATOR = \"JSON_EXTRACT_OPERATOR\"  # ->\n    JSON_EXTRACT_TEXT_OPERATOR = \"JSON_EXTRACT_TEXT_OPERATOR\"  # ->>\n\n# New AST nodes for JSON expressions\nclass JsonLiteral(Expression):\n    def __init__(self, json_text: str, source_location: SourceLocation):\n        super().__init__(source_location)\n        self.json_text = json_text\n        self.parsed_value = None  # Could parse JSON for validation\n    \n    def node_type(self) -> str:\n        return \"JsonLiteral\"\n\nclass JsonExtractExpression(Expression):\n    def __init__(self, json_expr: Expression, path_expr: Expression, \n                 extract_text: bool, source_location: SourceLocation):\n        super().__init__(source_location)\n        self.json_expression = json_expr\n        self.path_expression = path_expr\n        self.extract_as_text = extract_text  # True for ->>, False for ->\n    \n    def node_type(self) -> str:\n        return \"JsonExtractExpression\"\n\ndef register_json_extension(registry: ExtensionRegistry):\n    \"\"\"Register JSON parsing extensions with the parser.\"\"\"\n    # TODO: Register new token types with tokenizer\n    # TODO: Add JSON literal recognition to tokenizer's literal parsing\n    # TODO: Register JSON operators with appropriate precedence\n    # TODO: Extend expression parser to handle JSON extract operations\n    # TODO: Add JSON function parsing (JSON_OBJECT, JSON_ARRAY, etc.)\n    \n    # Example operator precedence registration\n    json_operators = {\n        JsonTokenTypes.JSON_EXTRACT_OPERATOR: OperatorInfo(\n            precedence=Precedence.PRIMARY, \n            is_right_associative=True\n        ),\n        JsonTokenTypes.JSON_EXTRACT_TEXT_OPERATOR: OperatorInfo(\n            precedence=Precedence.PRIMARY,\n            is_right_associative=True\n        )\n    }\n    \n    for token_type, operator_info in json_operators.items():\n        registry._operator_extensions[token_type] = operator_info\n\n# Extension registration\nextension_info = ExtensionInfo(\n    name=\"json_support\",\n    extension_type=ExtensionType.EXPRESSION,\n    priority=100,\n    dependencies=[],\n    registration_callback=register_json_extension\n)\n```\n\n#### Statement Parser Extension Pattern\n\nThis pattern shows how to add entirely new statement types while reusing existing parser infrastructure and following established recursive descent patterns:\n\n```python\n# statements/merge_parser.py\nfrom typing import List, Optional\nfrom core.base_parser import BaseParser\nfrom core.ast_nodes import ASTNode, Expression, Identifier, TableReference\nfrom core.tokenizer import TokenType\n\nclass MergeAction(ASTNode):\n    \"\"\"Base class for MERGE statement actions (INSERT/UPDATE/DELETE).\"\"\"\n    def __init__(self, condition: Optional[Expression], source_location: SourceLocation):\n        super().__init__(source_location)\n        self.when_condition = condition\n\nclass MergeInsertAction(MergeAction):\n    def __init__(self, condition: Optional[Expression], column_list: Optional[List[Identifier]], \n                 value_list: List[Expression], source_location: SourceLocation):\n        super().__init__(condition, source_location)\n        self.column_list = column_list\n        self.value_list = value_list\n    \n    def node_type(self) -> str:\n        return \"MergeInsertAction\"\n\nclass MergeUpdateAction(MergeAction):\n    def __init__(self, condition: Optional[Expression], assignments: List[AssignmentExpression],\n                 source_location: SourceLocation):\n        super().__init__(condition, source_location)\n        self.set_assignments = assignments\n    \n    def node_type(self) -> str:\n        return \"MergeUpdateAction\"\n\nclass MergeStatement(ASTNode):\n    def __init__(self, target_table: TableReference, source_table: TableReference,\n                 merge_condition: Expression, actions: List[MergeAction],\n                 source_location: SourceLocation):\n        super().__init__(source_location)\n        self.target_table = target_table\n        self.source_table = source_table  \n        self.merge_condition = merge_condition\n        self.merge_actions = actions\n    \n    def node_type(self) -> str:\n        return \"MergeStatement\"\n\nclass MergeParser(BaseParser):\n    \"\"\"Parser for MERGE statements following SQL standard syntax.\"\"\"\n    \n    def parse_merge_statement(self) -> MergeStatement:\n        \"\"\"\n        Parse: MERGE INTO target USING source ON condition \n               WHEN [NOT] MATCHED [AND condition] THEN action [...]\n        \"\"\"\n        start_token = self.current_token\n        \n        # TODO: Consume MERGE keyword token\n        # TODO: Consume INTO keyword token  \n        # TODO: Parse target table reference using existing table parser\n        # TODO: Consume USING keyword token\n        # TODO: Parse source table reference (could be table or subquery)\n        # TODO: Consume ON keyword token\n        # TODO: Parse merge condition expression using expression parser\n        # TODO: Parse one or more WHEN clauses using parse_when_clause()\n        # TODO: Create and return MergeStatement AST node\n        \n    def parse_when_clause(self) -> MergeAction:\n        \"\"\"Parse individual WHEN [NOT] MATCHED [AND condition] THEN action clause.\"\"\"\n        # TODO: Consume WHEN keyword token\n        # TODO: Check for optional NOT keyword (for WHEN NOT MATCHED)\n        # TODO: Consume MATCHED keyword token\n        # TODO: Parse optional AND condition expression\n        # TODO: Consume THEN keyword token\n        # TODO: Determine action type (INSERT/UPDATE/DELETE) from next token\n        # TODO: Delegate to appropriate action parser method\n        # TODO: Return constructed MergeAction subclass\n        \n    def parse_merge_insert_action(self, condition: Optional[Expression]) -> MergeInsertAction:\n        \"\"\"Parse INSERT action: INSERT [(columns)] VALUES (values).\"\"\"\n        # TODO: Consume INSERT keyword token\n        # TODO: Parse optional column list in parentheses\n        # TODO: Consume VALUES keyword token\n        # TODO: Parse value list in parentheses using existing value parser\n        # TODO: Create and return MergeInsertAction node\n        \n    def parse_merge_update_action(self, condition: Optional[Expression]) -> MergeUpdateAction:\n        \"\"\"Parse UPDATE action: UPDATE SET assignments.\"\"\"\n        # TODO: Consume UPDATE keyword token\n        # TODO: Consume SET keyword token\n        # TODO: Parse assignment list using existing assignment parser\n        # TODO: Create and return MergeUpdateAction node\n```\n\n#### Expression Parser Extension Pattern\n\nThis pattern demonstrates how to extend expression parsing capabilities while maintaining proper operator precedence and AST construction:\n\n```python\n# expressions/case_expressions.py\nfrom typing import List, Optional\nfrom core.ast_nodes import Expression\nfrom expressions.expression_parser import ExpressionParser\n\nclass WhenClause(ASTNode):\n    \"\"\"Represents WHEN condition THEN result clause in CASE expression.\"\"\"\n    def __init__(self, condition: Expression, result: Expression, source_location: SourceLocation):\n        super().__init__(source_location)\n        self.when_condition = condition\n        self.then_result = result\n    \n    def node_type(self) -> str:\n        return \"WhenClause\"\n\nclass CaseExpression(Expression):\n    \"\"\"Represents both simple and searched CASE expressions.\"\"\"\n    def __init__(self, case_expression: Optional[Expression], when_clauses: List[WhenClause],\n                 else_expression: Optional[Expression], source_location: SourceLocation):\n        super().__init__(source_location)\n        self.case_expression = case_expression  # None for searched CASE\n        self.when_clauses = when_clauses\n        self.else_expression = else_expression\n    \n    def node_type(self) -> str:\n        return \"CaseExpression\"\n    \n    def is_simple_case(self) -> bool:\n        \"\"\"True if this is simple CASE expr WHEN val, False if searched CASE WHEN condition.\"\"\"\n        return self.case_expression is not None\n\nclass CaseExpressionParser:\n    \"\"\"Extension to ExpressionParser for handling CASE expressions.\"\"\"\n    \n    def __init__(self, expression_parser: ExpressionParser):\n        self.expression_parser = expression_parser\n    \n    def parse_case_expression(self) -> CaseExpression:\n        \"\"\"Parse complete CASE expression with optional ELSE clause.\"\"\"\n        start_token = self.expression_parser.current_token\n        \n        # TODO: Consume CASE keyword token\n        # TODO: Check if next token starts an expression (simple CASE) or WHEN keyword (searched CASE)\n        # TODO: Parse optional case expression for simple CASE\n        # TODO: Parse one or more WHEN clauses using parse_when_clause()\n        # TODO: Parse optional ELSE clause if ELSE keyword present\n        # TODO: Consume END keyword token\n        # TODO: Create and return CaseExpression AST node with source location\n        \n    def parse_when_clause(self, is_simple_case: bool) -> WhenClause:\n        \"\"\"Parse WHEN condition/value THEN result clause.\"\"\"\n        # TODO: Consume WHEN keyword token\n        # TODO: Parse condition expression (for searched CASE) or value expression (for simple CASE)\n        # TODO: Consume THEN keyword token  \n        # TODO: Parse result expression using expression parser\n        # TODO: Create and return WhenClause AST node\n        \n    def integrate_with_expression_parser(self):\n        \"\"\"Register CASE expression parsing with main expression parser.\"\"\"\n        # TODO: Add CASE_KEYWORD to primary expression parsing in parse_primary_expression()\n        # TODO: Ensure CASE expressions compose properly with other expressions\n        # TODO: Handle CASE expression precedence in expression hierarchy\n```\n\n#### Milestone Validation for Extensions\n\nExtensions should include validation checkpoints that verify correct integration with the core parser:\n\n**Extension Integration Checklist:**\n- Tokenizer recognizes new keywords/operators without breaking existing functionality\n- New AST nodes integrate properly with visitor pattern and tree traversal\n- Parser handles new syntax without infinite loops or stack overflow\n- Error messages for new syntax provide helpful guidance to users\n- New features compose correctly with existing SQL constructs\n\n**Testing Commands for Validation:**\n```bash\n# Test tokenizer extension\npython -m pytest tests/extensions/test_json_tokenizer.py -v\n\n# Test parser extension integration  \npython -m pytest tests/extensions/test_merge_parser.py -v\n\n# Test end-to-end functionality\npython -c \"\nfrom sql_parser import SQLParser\nparser = SQLParser()\nast = parser.parse('MERGE INTO target USING source ON condition WHEN MATCHED THEN UPDATE SET col = val')\nprint(f'AST root type: {ast.node_type()}')\n\"\n```\n\n**Extension Debugging Strategy:**\n\n| Symptom | Likely Cause | Debugging Steps | Resolution |\n|---------|--------------|----------------|------------|\n| New keywords tokenized as IDENTIFIER | Missing keyword registration | Check SQL_KEYWORDS dictionary | Add keyword to tokenizer registry |\n| Parser fails on new syntax | Grammar rules not implemented | Trace recursive descent calls | Implement missing parsing methods |\n| AST contains incorrect node types | Wrong AST node construction | Inspect parse tree structure | Fix node creation in parser |\n| Extension breaks existing functionality | Precedence/dispatch conflicts | Run regression test suite | Adjust extension integration points |\n\nThese patterns provide a **systematic approach** to parser extension that maintains architectural consistency while enabling significant functionality growth. Each pattern demonstrates how to leverage existing parser infrastructure while introducing new capabilities that feel like natural language evolution rather than architectural departures.\n\n\n## Glossary\n\n> **Milestone(s):** This section supports all milestones (1-4), providing essential terminology and concepts referenced throughout the design document.\n\n### Mental Model: Technical Dictionary\n\nThink of this glossary as a technical dictionary specifically curated for SQL parsing concepts. Just as a foreign language dictionary helps you understand unfamiliar words while reading literature, this glossary helps you navigate the specialized terminology of compiler theory, parsing algorithms, and SQL language processing. Unlike a general programming dictionary, every term here is carefully selected because it appears in our parser design and has specific meaning within the context of SQL parsing.\n\n### Core Parsing and Compiler Theory Terms\n\nThe foundation of any parser rests on fundamental concepts from compiler theory and formal language processing. These terms represent the theoretical underpinnings that guide our practical implementation decisions.\n\n**Abstract Syntax Tree (AST)**: A hierarchical tree representation of the syntactic structure of SQL source code. Unlike a parse tree that includes every syntactic detail, an AST abstracts away punctuation and focuses on the semantic structure. Each node represents a language construct (statement, expression, literal) with child nodes representing sub-components. The AST serves as the primary output of our parser and the input to subsequent processing phases like query optimization or execution.\n\n**Tokenization**: The process of breaking input text into a sequence of meaningful units called tokens. This is the first phase of parsing, where character sequences like \"SELECT\" become keyword tokens, \"customer_name\" becomes identifier tokens, and \"123\" becomes integer literal tokens. Tokenization handles whitespace removal, comment elimination, and initial classification of input elements.\n\n**Lexical Analysis**: The formal term for the process of analyzing input characters to identify and classify tokens. This includes recognizing token boundaries, determining token types, and handling special cases like escape sequences in string literals. Lexical analysis is implemented by the tokenizer component and must handle SQL's specific rules for identifiers, keywords, and literal values.\n\n**Recursive Descent**: A top-down parsing technique where each grammar rule is implemented as a function that calls other functions corresponding to sub-rules. The \"recursive\" aspect comes from grammar rules that reference themselves (directly or indirectly), while \"descent\" refers to starting from the top-level rule and working down to terminal symbols. This approach maps naturally to SQL's hierarchical grammar structure.\n\n**Precedence Climbing**: An expression parsing algorithm that handles operator precedence by recursively parsing sub-expressions with minimum precedence thresholds. Instead of encoding precedence in the grammar structure, precedence climbing uses a table-driven approach where each recursive call specifies the minimum operator precedence it will consume. This technique elegantly handles SQL's complex operator precedence rules without requiring separate grammar rules for each precedence level.\n\n**Lookahead**: The practice of examining future tokens without consuming them from the input stream. Single-token lookahead allows the parser to make parsing decisions based on the next token, while multi-token lookahead can resolve more complex ambiguities. SQL parsing often requires lookahead to distinguish between similar constructs like function calls and column references.\n\n### SQL-Specific Language Terms\n\nSQL introduces domain-specific concepts that affect parsing decisions and AST structure. These terms reflect the unique characteristics of SQL as a declarative query language.\n\n**Three-Valued Logic**: SQL's boolean logic system that includes TRUE, FALSE, and NULL values. This affects expression parsing because SQL comparisons can produce NULL results, and boolean operators like AND and OR have specific behavior when encountering NULL operands. The parser must represent these semantics accurately in expression AST nodes.\n\n**Qualified Identifier**: A multi-part identifier using dot notation, such as `table.column` or `schema.table.column`. Parsing qualified identifiers requires recognizing dot separators and building AST nodes that preserve the hierarchical relationship between qualifiers and base names. SQL's scoping rules determine how qualified identifiers are resolved.\n\n**Star Wildcard**: The asterisk symbol (*) used in SELECT clauses to represent all columns from specified tables. Parsing star wildcards requires distinguishing them from multiplication operators based on context. The AST representation must capture whether the star applies to all tables or is qualified to specific tables.\n\n**Implicit Alias**: A column or table alias specified without the AS keyword, such as `SELECT name customer_name FROM users u`. Parsing implicit aliases requires recognizing identifier sequences where the second identifier serves as an alias. This creates parsing ambiguity that must be resolved through grammar rules and lookahead.\n\n**Explicit Alias**: An alias specified using the AS keyword, such as `SELECT name AS customer_name FROM users AS u`. Explicit aliases are easier to parse because the AS keyword clearly signals the aliasing intent, but the parser must handle cases where AS is optional.\n\n### Parser Architecture and Design Terms\n\nThese terms describe the structural organization and design patterns used in parser implementation. Understanding these concepts helps navigate the relationship between different parser components.\n\n**Visitor Pattern**: A design pattern that separates algorithms from the AST structure they operate on. The pattern defines a visitor interface with methods for each AST node type, allowing external code to traverse and process AST nodes without modifying the node classes. This pattern enables extensibility for operations like code generation, optimization, and validation.\n\n**Grammar Rule**: A formal specification of valid syntax patterns in SQL. Grammar rules define how tokens can be combined to form valid statements, expressions, and clauses. Each rule in our recursive descent parser corresponds to a parsing function that recognizes the rule's pattern and constructs appropriate AST nodes.\n\n**Token Consumption**: The process of advancing the parser's position after successfully recognizing an expected token. Token consumption moves the parser forward through the input stream and typically involves updating position counters and fetching the next token. Proper token consumption is critical for maintaining parser state consistency.\n\n**Precedence Override**: Using parentheses to change the natural evaluation order of operators in expressions. When the parser encounters parentheses, it must recursively parse the enclosed expression as a complete sub-expression before continuing with outer-level parsing. This mechanism allows users to override default operator precedence.\n\n**Maximal Munch**: A tokenization strategy that always forms the longest possible token from the current input position. For example, when encountering \"<=\", maximal munch produces a single less-than-or-equal token rather than separate less-than and equals tokens. This strategy resolves tokenization ambiguities in favor of longer tokens.\n\n### Error Handling and Recovery Terms\n\nParser error handling introduces specialized terminology for managing and recovering from syntax errors. These concepts enable robust parsing that continues after encountering problems.\n\n**Panic-Mode Recovery**: An error recovery strategy where the parser discards tokens until it finds a reliable synchronization point where parsing can safely resume. Panic mode helps prevent cascading errors by establishing known-good parser states after encountering syntax problems.\n\n**Synchronization Points**: Specific token positions where the parser can reliably resume parsing after error recovery. Common synchronization points include statement boundaries (semicolons) and major keyword tokens (SELECT, INSERT, UPDATE, DELETE). These points represent positions where the parser's context is well-defined.\n\n**Cascade Errors**: Spurious error reports caused by earlier parsing failures. When the parser fails to recognize a construct correctly, subsequent parsing attempts may generate false positive errors. Error recovery mechanisms must distinguish between genuine syntax errors and cascade effects.\n\n**Error Suppression**: The practice of preventing false positive error reports during error recovery phases. Suppression mechanisms track error recovery state and avoid reporting errors that are likely cascade effects from earlier problems.\n\n### Testing and Validation Terms\n\nParser testing requires specialized approaches that verify both positive cases (valid SQL) and negative cases (syntax errors). These terms describe testing strategies specific to parser development.\n\n**Component Unit Testing**: Testing individual parser components in isolation from the rest of the system. Unit tests for tokenizers verify token type classification and boundary detection, while unit tests for parsers verify AST construction for specific grammar constructs. Component isolation enables focused testing of specific functionality.\n\n**End-to-End Parser Testing**: Testing complete SQL statement parsing from input text to final AST output. End-to-end tests verify that all parser components work together correctly and that the final AST accurately represents the input SQL's semantic structure. These tests catch integration issues between parser phases.\n\n**Milestone Validation Checkpoints**: Specific tests and expected outputs that verify each development milestone meets its acceptance criteria. Checkpoints provide objective measurements of progress and help identify when milestone requirements are fully satisfied. Each checkpoint includes both positive and negative test cases.\n\n**Edge Case Coverage**: Testing boundary conditions and unusual syntax that might expose parser bugs. Edge cases include empty input, maximum-length identifiers, deeply nested expressions, and unusual but valid SQL constructs. Comprehensive edge case testing improves parser robustness.\n\n**Test Fixture**: External test data files containing SQL examples and expected parsing results. Fixtures separate test data from test logic, making it easier to add new test cases and maintain large test suites. Fixture formats include raw SQL files and JSON files describing expected AST structures.\n\n### Advanced Parsing and Extension Terms\n\nThese terms relate to parser extensibility and advanced parsing techniques that support future enhancement and customization.\n\n**Grammar Extension Patterns**: Systematic approaches for adding new SQL features to the parser without breaking existing functionality. Extension patterns include keyword registration, operator precedence modification, and new statement type integration. Well-designed extension patterns maintain parser modularity and backward compatibility.\n\n**Plugin Architecture**: A modular system that allows external code to extend parser functionality through well-defined interfaces. Plugin architectures enable adding domain-specific SQL extensions, custom validation rules, and specialized AST transformations without modifying core parser code.\n\n**Extension Registry**: A central system for managing parser extensions and their dependencies. The registry handles extension registration, dependency resolution, and coordinated initialization of multiple extensions. This centralization prevents conflicts between extensions and ensures proper initialization ordering.\n\n**Context-Sensitive Parsing**: Parsing behavior that depends on the current parsing context or previously parsed elements. SQL includes context-sensitive constructs where the same token sequence can have different meanings depending on the surrounding context. Context-sensitive parsing requires maintaining parser state across parsing function calls.\n\n**Symbol Table Stack**: A hierarchical data structure for tracking symbol definitions and scopes during parsing. While our basic parser doesn't implement full symbol tables, understanding this concept helps when extending the parser to handle variable scoping, table name resolution, and semantic validation.\n\n### Data Structure and Algorithm Terms\n\nParser implementation relies on fundamental data structures and algorithms. These terms describe the computational foundations underlying parser operations.\n\n**Binding Power**: A numerical precedence value used in precedence climbing algorithms to determine operator parsing order. Higher binding power values indicate higher precedence. The precedence climbing algorithm uses binding power comparisons to decide whether to continue parsing at the current level or recurse to a higher precedence level.\n\n**Associativity**: The rule determining how operators of equal precedence group together. Left-associative operators group from left to right (a - b - c becomes (a - b) - c), while right-associative operators group from right to left (a = b = c becomes a = (b = c)). SQL has specific associativity rules for each operator class.\n\n**Character-by-Character Scanning**: A tokenization approach that examines input one character at a time and maintains state to determine token boundaries and types. This approach provides fine-grained control over tokenization logic and handles complex cases like escape sequences and multi-character operators.\n\n**Method Dispatch**: The process of calling different parsing functions based on input characteristics. Dispatch can be table-driven (using lookup tables to map token types to parsing functions) or conditional (using if-else chains to select appropriate parsing logic). Efficient dispatch improves parser performance.\n\n**State Machine**: A computational model that transitions between discrete states based on input events. Tokenizers often use state machines to track parsing context (normal text, inside string literal, inside comment) and determine appropriate character handling logic for each state.\n\n### AST Construction and Manipulation Terms\n\nBuilding and working with Abstract Syntax Trees requires understanding tree structures and traversal patterns. These terms describe AST-related concepts and operations.\n\n**AST Composition**: The process of building complex tree structures from simpler node components. AST composition involves creating parent-child relationships, maintaining node properties, and ensuring tree consistency. Proper composition techniques create ASTs that accurately represent SQL semantic structure.\n\n**Tree Traversal**: Systematic methods for visiting all nodes in an AST. Common traversal patterns include depth-first (visiting children before siblings) and breadth-first (visiting all nodes at one level before descending). Different traversal patterns suit different AST processing tasks.\n\n**Node Type Classification**: Organizing AST nodes into categories based on their semantic roles. Classification schemes typically include statement nodes (SELECT, INSERT, UPDATE, DELETE), expression nodes (binary operations, literals, identifiers), and utility nodes (lists, aliases). Clear classification guides AST design decisions.\n\n**Source Location Tracking**: Maintaining information about where each AST node originated in the source code. Source locations include line numbers, column positions, and character ranges that enable precise error reporting and debugging support. Location tracking requires coordination between tokenizer and parser phases.\n\n### Performance and Optimization Terms\n\nParser performance characteristics affect usability and scalability. These terms describe performance-related concepts and optimization strategies.\n\n**Linear Time Complexity**: A performance characteristic where parsing time increases proportionally to input size. Well-designed recursive descent parsers achieve linear time complexity by making single forward passes through the input without backtracking. Linear complexity ensures predictable performance on large SQL statements.\n\n**Memory Allocation Patterns**: The way parsers allocate and manage memory for tokens and AST nodes. Efficient allocation patterns minimize garbage collection pressure and memory fragmentation. Techniques include object pooling, arena allocation, and careful lifetime management of temporary objects.\n\n**Parse Table Generation**: Pre-computing parsing decisions in lookup tables rather than using runtime conditionals. Table-driven parsing can improve performance by eliminating repeated decision logic, but requires careful design to maintain code readability and extensibility.\n\n### SQL Language Feature Terms\n\nSpecific SQL language constructs introduce terminology that affects parsing design and implementation decisions.\n\n**Data Modification Language (DML)**: SQL statements that change database state, including INSERT, UPDATE, and DELETE. DML parsing requires handling value lists, assignment expressions, and conditional clauses. DML statements have different structural patterns than query statements like SELECT.\n\n**Assignment Expression**: A column = value expression used in UPDATE statement SET clauses. Assignment expressions create AST nodes that capture both the target column and the assigned value expression. The parser must handle multiple assignments in comma-separated lists.\n\n**Value Row**: A parenthesized list of expressions in INSERT statement VALUES clauses. Value rows can contain literals, expressions, and NULL values. The parser must validate that value row structure matches the column list structure and handle multiple rows in single INSERT statements.\n\n**Structural Validation**: Post-parsing checks that verify logical consistency of parsed statements. Structural validation includes verifying that column counts match value counts in INSERT statements, checking that UPDATE statements have SET clauses, and ensuring DELETE statements don't accidentally omit WHERE clauses.\n\n**Safety-Conscious Parsing**: Parser design that flags potentially dangerous operations in the AST. Safety features might include warnings for DELETE statements without WHERE clauses or INSERT statements with mismatched column and value counts. Safety consciousness improves parser usability and prevents common mistakes.\n\nThis comprehensive glossary provides the terminology foundation needed to understand and implement our SQL parser. Each term connects to specific implementation decisions documented in previous sections and supports the technical discussions throughout the design document.\n"}