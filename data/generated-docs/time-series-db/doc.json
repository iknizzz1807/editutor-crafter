{"html":"<h1 id=\"tempodb-a-modern-time-series-database-design-document\">TempoDB: A Modern Time-Series Database - Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This document outlines the design of TempoDB, a specialized database for storing and querying high-volume, timestamped data. It solves the core architectural challenge of achieving high write throughput for sequential data while enabling efficient range queries and compressing massive datasets with predictable patterns, making it essential for metrics, IoT, and financial applications.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This foundational section establishes the core challenges addressed by all five milestones.</p>\n</blockquote>\n<p>Time-series data—sequential measurements indexed by time—is one of the fastest-growing data categories in modern computing. From monitoring server CPU usage every second to tracking sensor readings in industrial IoT and recording financial trades at millisecond intervals, this data forms the observational backbone of digital systems. However, storing and querying this data at scale presents unique architectural challenges that traditional relational or NoSQL databases struggle to address efficiently.</p>\n<p>The fundamental characteristics of time-series data create a distinct set of requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Characteristic</th>\n<th>Implication for Database Design</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>High Volume &amp; Velocity</strong></td>\n<td>Individual measurements are typically small (a timestamp and value), but arrive continuously in massive volumes—potentially millions of points per second. The database must sustain high write throughput without compromising durability.</td>\n</tr>\n<tr>\n<td><strong>Append-Heavy, Rarely Updated</strong></td>\n<td>Once recorded, a data point is almost never modified (though it may be deleted after retention periods). This allows for write-optimized storage structures that differ from update-friendly B-trees.</td>\n</tr>\n<tr>\n<td><strong>Time-Ordered Nature</strong></td>\n<td>Points arrive roughly in chronological order and are most frequently queried by time ranges (&quot;last hour,&quot; &quot;yesterday&quot;). Temporal locality can be leveraged for compression and query optimization.</td>\n</tr>\n<tr>\n<td><strong>Periodic Patterns</strong></td>\n<td>Many time series exhibit regular patterns (daily cycles, seasonal trends) where consecutive values change slowly, enabling specialized compression algorithms far more efficient than general-purpose compression.</td>\n</tr>\n<tr>\n<td><strong>Exploding Cardinality</strong></td>\n<td>The combination of measurement names and tag combinations creates potentially millions of unique time series, requiring careful indexing strategies to avoid exponential growth in metadata overhead.</td>\n</tr>\n<tr>\n<td><strong>Decreasing Value Over Time</strong></td>\n<td>Recent data is queried frequently with high resolution, while older data is accessed less often and can be aggregated or downsampled to save storage space. This enables tiered storage strategies.</td>\n</tr>\n</tbody></table>\n<p>These characteristics create a fundamental tension: write paths must handle a relentless stream of small, sequential inserts, while read paths must efficiently retrieve and aggregate potentially vast ranges of historical data. Traditional databases optimized for transactional workloads (ACID guarantees, random updates) or document stores (flexible schemas, point lookups) fail to deliver the required performance at scale without excessive operational overhead.</p>\n<h3 id=\"mental-model-the-data-stream-conveyor-belt\">Mental Model: The Data Stream Conveyor Belt</h3>\n<p>Imagine time-series data as a <strong>high-speed conveyor belt</strong> in a massive warehouse processing facility. Items (data points) arrive continuously, each with a timestamp (arrival time) and content (measurement value), plus labels indicating their type and origin (tags).</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>[2023-10-01 12:00:00, CPU=85%, server=web-01] ---&gt;\n[2023-10-01 12:00:01, CPU=87%, server=web-01] ---&gt;\n[2023-10-01 12:00:00, Temp=72.5F, sensor=A] ---&gt;\n[2023-10-01 12:00:01, Temp=72.6F, sensor=A] ---&gt;</code></pre></div>\n\n<p>Our database system acts as the entire warehouse logistics operation, which must:</p>\n<ol>\n<li><p><strong>Label and Sort</strong> Each item must be categorized (by measurement and tags) and sorted chronologically as it arrives. Items arriving out of order (late packages) must be inserted into their correct position.</p>\n</li>\n<li><p><strong>Compress Efficiently</strong> Since similar items arrive consecutively (temperature readings change slowly), we can pack them tightly using specialized compression—storing only the differences between consecutive items rather than full representations.</p>\n</li>\n<li><p><strong>Store in Time-Bound Containers</strong> Rather than placing items individually on shelves, we batch them into time-bound containers (e.g., &quot;12:00-12:05 container for server web-01 CPU&quot;). Each container has a clear time range label on the outside, so we can quickly locate relevant containers without opening them.</p>\n</li>\n<li><p><strong>Move Through Storage Tiers</strong> Hot items (recent arrivals) remain near the front on easily accessible shelves (memory, fast SSDs). As items age, they move to deeper warehouse aisles (slower disks) and eventually to archival storage (cold storage), possibly in aggregated form (e.g., &quot;average temperature per hour&quot; rather than per-second readings).</p>\n</li>\n<li><p><strong>Query with Temporal Precision</strong> When someone asks, &quot;What was the CPU usage between 12:00 and 12:30?&quot;, we don&#39;t scan every item in the warehouse. We first consult our container index, retrieve only containers overlapping that time range, then open and decompress just those containers.</p>\n</li>\n</ol>\n<p>This mental model highlights why generic storage systems struggle: they&#39;re designed for random access to individual items (like a traditional library where books are constantly reshelved), not for processing a continuous, time-ordered stream. Our design must embrace the conveyor belt nature—high-throughput ingestion at the front, intelligent batching and compression in the middle, and tiered archiving at the back.</p>\n<h3 id=\"existing-approaches-and-comparison\">Existing Approaches and Comparison</h3>\n<p>When faced with time-series data, architects typically consider three categories of storage solutions, each with distinct trade-offs:</p>\n<h4 id=\"option-1-generic-relational-databases-postgresql-mysql\">Option 1: Generic Relational Databases (PostgreSQL, MySQL)</h4>\n<p>These systems store time-series data in conventional tables, often with a schema like:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#F97583\"> TABLE</span><span style=\"color:#B392F0\"> metrics</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    timestamp</span><span style=\"color:#F97583\"> TIMESTAMP</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metric_name </span><span style=\"color:#F97583\">VARCHAR</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">255</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    value</span><span style=\"color:#F97583\"> DOUBLE PRECISION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags JSONB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">);</span></span></code></pre></div>\n\n<blockquote>\n<p><strong>ADR: Choosing a Specialized Time-Series Database Over Generic RDBMS</strong></p>\n<ul>\n<li><strong>Context</strong>: Teams often start with existing relational databases due to familiarity and existing infrastructure, but face scaling challenges as time-series data volume grows.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Naive table with indexes</strong>: Simple <code>CREATE INDEX ON metrics(timestamp, metric_name)</code> </li>\n<li><strong>Table partitioning by time</strong>: Manual or automatic partitioning by time ranges</li>\n<li><strong>TimescaleDB extension</strong>: PostgreSQL extension adding time-series optimizations</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Build a specialized time-series database rather than using a generic RDBMS.</li>\n<li><strong>Rationale</strong>:<ol>\n<li><strong>Write amplification</strong>: B-trees (the standard RDBMS index) incur significant overhead for append-heavy workloads due to random writes and page splits.</li>\n<li><strong>Storage inefficiency</strong>: Row-based storage duplicates timestamp and tag columns for every measurement, while time-series compression can achieve 10-100x better ratios.</li>\n<li><strong>Query performance</strong>: Range scans must traverse index trees not optimized for temporal locality, missing opportunities for block-level skipping.</li>\n<li><strong>Cardinality management</strong>: High cardinality from tag combinations creates index bloat in RDBMS indexes.</li>\n</ol>\n</li>\n<li><strong>Consequences</strong>: We forgo full SQL support and ACID transactions across arbitrary tables, but gain order-of-magnitude improvements in write throughput, storage efficiency, and time-range query performance.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not for High-Volume TSDB</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Plain RDBMS table</strong></td>\n<td>Full SQL, ACID transactions, well-understood</td>\n<td>Poor write performance, storage bloat, index overhead on timestamps</td>\n<td>B-tree index maintenance dominates write cost at high volumes</td>\n</tr>\n<tr>\n<td><strong>Time partitioning</strong></td>\n<td>Faster deletion of old data, some query pruning</td>\n<td>Manual management, still uses row storage, doesn&#39;t help compression</td>\n<td>Reduces but doesn&#39;t eliminate fundamental architectural mismatches</td>\n</tr>\n<tr>\n<td><strong>TimescaleDB</strong></td>\n<td>PostgreSQL compatibility, automated partitioning, some compression</td>\n<td>Still bound by PostgreSQL&#39;s storage engine, higher latency than specialized TSDB</td>\n<td>Good hybrid solution but not optimal for pure time-series workloads at extreme scale</td>\n</tr>\n</tbody></table>\n<h4 id=\"option-2-specialized-time-series-databases-influxdb-prometheus\">Option 2: Specialized Time-Series Databases (InfluxDB, Prometheus)</h4>\n<p>These systems are built from the ground up for time-series data, employing specialized storage engines:</p>\n<ul>\n<li><strong>InfluxDB TSM Engine</strong>: Uses a Time-Structured Merge Tree with columnar storage within blocks, Gorilla compression for floats, and time-range indexing.</li>\n<li><strong>Prometheus TSDB</strong>: Uses a custom storage format with chunk-based encoding, separate head block for recent data, and compaction to larger chunks.</li>\n</ul>\n<blockquote>\n<p><strong>ADR: Learning from Existing TSDB Architectures Without Direct Copying</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple mature time-series databases exist with proven designs. We must decide how closely to follow existing patterns versus innovating.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Implement InfluxDB&#39;s TSM engine exactly</strong>: Clone the open-source implementation</li>\n<li><strong>Implement Prometheus TSDB</strong>: Adopt its chunk-based approach</li>\n<li><strong>Synthesize patterns with educational clarity</strong>: Combine proven ideas with explicit design decisions and trade-offs</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Create a synthesis that highlights architectural decisions clearly while using proven patterns from industry.</li>\n<li><strong>Rationale</strong>:<ol>\n<li><strong>Educational value</strong>: Direct cloning teaches implementation but not reasoning. Our design should expose decision points.</li>\n<li><strong>Simplification for learning</strong>: Production systems include many optimizations for edge cases that obscure core concepts.</li>\n<li><strong>Pattern diversity</strong>: Different systems excel in different aspects—InfluxDB&#39;s compression, Prometheus&#39;s chunk management, TimescaleDB&#39;s SQL integration.</li>\n</ol>\n</li>\n<li><strong>Consequences</strong>: Our design will resemble InfluxDB&#39;s TSM engine in structure but with simplified compaction, explicit ADRs for each major choice, and clear mapping from concepts to implementation.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>System</th>\n<th>Storage Engine</th>\n<th>Compression</th>\n<th>Query Language</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>InfluxDB</strong></td>\n<td>Time-Structured Merge Tree (TSM)</td>\n<td>Gorilla XOR for floats, delta-of-delta for timestamps</td>\n<td>InfluxQL (SQL-like), Flux</td>\n<td>High-write environments, operational monitoring</td>\n</tr>\n<tr>\n<td><strong>Prometheus</strong></td>\n<td>Custom chunk-based TSDB</td>\n<td>Variable encoding based on data type</td>\n<td>PromQL (functional)</td>\n<td>Kubernetes/metrics monitoring, pull-based collection</td>\n</tr>\n<tr>\n<td><strong>TimescaleDB</strong></td>\n<td>PostgreSQL with hypertables</td>\n<td>Native compression, dictionary encoding</td>\n<td>Full PostgreSQL SQL</td>\n<td>Teams requiring SQL, mixed workload environments</td>\n</tr>\n<tr>\n<td><strong>ClickHouse</strong></td>\n<td>MergeTree with LSM-like merges</td>\n<td>Multiple codecs, delta encoding</td>\n<td>SQL</td>\n<td>Analytics, high-cardinality time-series</td>\n</tr>\n</tbody></table>\n<h4 id=\"option-3-general-purpose-columnar-stores-apache-parquet-arrow\">Option 3: General-Purpose Columnar Stores (Apache Parquet, Arrow)</h4>\n<p>Columnar storage formats naturally align with time-series characteristics by storing all timestamps together and all values together:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Alignment with Time-Series</th>\n<th>Challenges</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Columnar formats</strong></td>\n<td>Excellent for compression (similar values stored contiguously), efficient for range scans</td>\n<td>Not optimized for real-time ingestion, lack built-in time-series operations</td>\n</tr>\n<tr>\n<td><strong>Data lake architectures</strong></td>\n<td>Cost-effective at petabyte scale, good for batch analytics</td>\n<td>High query latency, not designed for real-time monitoring</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: While columnar storage is theoretically ideal for time-series data, the missing piece is <strong>ingestion efficiency</strong>. Traditional columnar stores expect batch writes, while time-series requires sustained high-velocity ingestion with immediate queryability.</p>\n</blockquote>\n<h4 id=\"synthesis-the-tempodb-approach\">Synthesis: The TempoDB Approach</h4>\n<p>TempoDB adopts a <strong>hybrid architecture</strong> that combines the best elements of these approaches:</p>\n<ol>\n<li><strong>Specialized storage engine</strong> based on Time-Structured Merge Tree principles (like InfluxDB) for high write throughput and compression</li>\n<li><strong>Columnar layout within blocks</strong> (like Parquet) to enable efficient compression and scanning</li>\n<li><strong>SQL-like query language with time extensions</strong> (bridging InfluxQL and TimescaleDB approaches) for familiarity</li>\n<li><strong>Built-in aggregation and downsampling</strong> (like Prometheus) for data lifecycle management</li>\n</ol>\n<p>The core innovation isn&#39;t in individual components—which are proven in production—but in their <strong>pedagogical integration</strong>: each design decision is explicitly justified, alternatives are documented, and implementation maps clearly to architectural concepts.</p>\n<h3 id=\"common-pitfalls-in-time-series-database-design\">Common Pitfalls in Time-Series Database Design</h3>\n<p>⚠️ <strong>Pitfall: Treating Timestamps as Just Another Column</strong></p>\n<ul>\n<li><strong>Description</strong>: Storing timestamps in generic indexes without leveraging their monotonic, range-query-dominant nature.</li>\n<li><strong>Why Wrong</strong>: Generic indexes (B-trees) optimized for equality lookups waste space and CPU on timestamp ordering that&#39;s already inherent in append pattern.</li>\n<li><strong>Fix</strong>: Implement time-range indexes that store minimum/maximum timestamps per block, enabling entire blocks to be skipped during queries.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Naive Tag Indexing Causing Cardinality Explosion</strong></p>\n<ul>\n<li><strong>Description</strong>: Creating separate indexes for each tag dimension leads to combinatorial explosion of index entries.</li>\n<li><strong>Why Wrong</strong>: With 10 servers, 5 regions, and 3 applications, you have 150 series. Each point writes to 150 index entries in naive implementation.</li>\n<li><strong>Fix</strong>: Use inverted index or tag-value pair indexing where each unique series key (measurement + tag combination) is indexed once, not per tag.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Applying General-Purpose Compression</strong></p>\n<ul>\n<li><strong>Description</strong>: Using gzip or Snappy on entire data blocks without considering time-series patterns.</li>\n<li><strong>Why Wrong</strong>: Misses opportunity for order-of-magnitude better compression using delta-of-delta (timestamps) and XOR (floating point values).</li>\n<li><strong>Fix</strong>: Implement specialized time-series compression algorithms like Gorilla for floats and delta encoding for integers.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring Out-of-Order Writes</strong></p>\n<ul>\n<li><strong>Description</strong>: Assuming timestamps always arrive in strictly increasing order.</li>\n<li><strong>Why Wrong</strong>: Network latency, clock skew, and batch processing can cause &quot;late&quot; data points that arrive after newer points.</li>\n<li><strong>Fix</strong>: Design write path to handle out-of-order data through buffering and periodic re-sorting during compaction.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Querying Full Resolution for Historical Data</strong></p>\n<ul>\n<li><strong>Description</strong>: Storing and querying millisecond data for years-old metrics.</li>\n<li><strong>Why Wrong</strong>: Wastes storage and query CPU on precision no longer needed for trend analysis.</li>\n<li><strong>Fix</strong>: Implement automatic downsampling and retention policies that aggregate old data to lower resolutions.</li>\n</ul>\n<p>The subsequent sections of this design document address these pitfalls through specific architectural choices, each documented with rationale and alternatives.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<blockquote>\n<p><strong>Note</strong>: This section focuses on conceptual understanding rather than implementation. Detailed implementation guidance appears in later sections corresponding to each milestone. However, we provide foundational guidance here for setting up the project structure.</p>\n</blockquote>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Recommended)</th>\n<th>Advanced Option (Alternative)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Language</strong></td>\n<td>Go (static typing, GC, good concurrency)</td>\n<td>Rust (zero-cost abstractions, no GC)</td>\n</tr>\n<tr>\n<td><strong>Storage Abstraction</strong></td>\n<td>Memory-mapped files with <code>mmap</code> syscall</td>\n<td>Direct I/O with aligned buffers</td>\n</tr>\n<tr>\n<td><strong>Concurrency Control</strong></td>\n<td><code>sync.RWMutex</code> for shared mutable state</td>\n<td>Lock-free ring buffers for writes</td>\n</tr>\n<tr>\n<td><strong>Serialization</strong></td>\n<td>Custom binary format with <code>encoding/binary</code></td>\n<td>Protocol Buffers for metadata</td>\n</tr>\n<tr>\n<td><strong>HTTP Server</strong></td>\n<td>Standard <code>net/http</code> with JSON/plaintext</td>\n<td>FastHTTP for higher performance</td>\n</tr>\n<tr>\n<td><strong>Compression</strong></td>\n<td>Custom Gorilla/delta implementations</td>\n<td>Integrate <code>github.com/klauspost/compress</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>Establish this directory structure from the beginning to maintain separation of concerns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   ├── tempo-server/          # Main server executable\n│   │   └── main.go\n│   └── tempo-cli/             # Command-line interface (optional)\n│       └── main.go\n├── internal/                  # Private application code\n│   ├── api/                   # HTTP/gRPC APIs (Milestone 5)\n│   │   ├── handler.go\n│   │   ├── query_parser.go\n│   │   └── line_protocol.go\n│   ├── storage/               # Storage engine (Milestone 1)\n│   │   ├── tsm/\n│   │   │   ├── writer.go\n│   │   │   ├── reader.go\n│   │   │   ├── compression.go\n│   │   │   └── file.go\n│   │   ├── wal/               # Write-ahead log (Milestone 2)\n│   │   │   ├── writer.go\n│   │   │   ├── reader.go\n│   │   │   └── segment.go\n│   │   ├── memtable/          # In-memory buffer (Milestone 2)\n│   │   │   ├── memtable.go\n│   │   │   └── series_key.go\n│   │   └── index/             # Series and tag indexing\n│   │       ├── inverted.go\n│   │       └── series_store.go\n│   ├── query/                 # Query engine (Milestone 3)\n│   │   ├── parser/\n│   │   │   ├── lexer.go\n│   │   │   └── parser.go\n│   │   ├── executor/\n│   │   │   ├── planner.go\n│   │   │   ├── iterator.go\n│   │   │   └── aggregator.go\n│   │   └── engine.go\n│   ├── compact/               # Compaction &amp; retention (Milestone 4)\n│   │   ├── compactor.go\n│   │   ├── retention.go\n│   │   └── downsampler.go\n│   └── models/                # Shared data structures\n│       ├── point.go\n│       ├── series.go\n│       └── query.go\n├── pkg/                       # Public libraries (if any)\n├── scripts/                   # Build/test scripts\n├── testdata/                  # Test fixtures\n├── go.mod\n├── go.sum\n└── README.md</code></pre></div>\n\n<h4 id=\"c-core-type-definitions-foundation\">C. Core Type Definitions (Foundation)</h4>\n<p>Before implementing any component, define these foundational types in <code>internal/models/</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: internal/models/point.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DataPoint represents a single time-series measurement at a specific timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DataPoint</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\"> // Precision: nanosecond</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\">   // Supports integers via conversion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Note: In a production system, we'd support multiple value types (int, bool, string)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesKey uniquely identifies a time series by measurement name and tags</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SeriesKey</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Measurement </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">            // e.g., \"cpu_usage\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tags        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // e.g., {\"host\": \"server-01\", \"region\": \"us-east\"}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Series represents a complete time series: its identity and data points</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Series</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key    </span><span style=\"color:#B392F0\">SeriesKey</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Points []</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#6A737D\"> // In practice, points are stored in blocks, not individually</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: internal/models/query.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange represents an inclusive time window [Start, End]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AggregateFunction defines supported aggregation operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AggregateFunction</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateSum</span><span style=\"color:#B392F0\">  AggregateFunction</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sum\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateAvg</span><span style=\"color:#B392F0\">  AggregateFunction</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"avg\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateMin</span><span style=\"color:#B392F0\">  AggregateFunction</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"min\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateMax</span><span style=\"color:#B392F0\">  AggregateFunction</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"max\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateCount</span><span style=\"color:#B392F0\"> AggregateFunction</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"count\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Query represents a parsed query request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Query</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Measurement   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tags          </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">      // Tag filters (AND relationship)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange     </span><span style=\"color:#B392F0\">TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Aggregate     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AggregateFunction</span><span style=\"color:#6A737D\">     // nil for raw points</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GroupByWindow </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\">          // e.g., 1h for hourly aggregation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Fields        []</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">               // For future: multiple fields per measurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-language-specific-hints-for-go\">D. Language-Specific Hints for Go</h4>\n<ol>\n<li><strong>Time Handling</strong>: Use <code>time.Time</code> for timestamps throughout. Convert to nanoseconds for storage: <code>timestamp.UnixNano()</code>.</li>\n<li><strong>Concurrent Maps</strong>: Use <code>sync.Map</code> or <code>sync.RWMutex</code> with regular maps for shared mutable state. For high-throughput write buffers, consider sharded maps.</li>\n<li><strong>Binary Encoding</strong>: Use <code>encoding/binary.BigEndian.PutUint64()</code> and similar for consistent cross-platform binary format.</li>\n<li><strong>Memory Management</strong>: Preallocate slices with known capacity to avoid reallocations: <code>points := make([]DataPoint, 0, 1024)</code>.</li>\n<li><strong>Error Handling</strong>: Use custom error types for domain-specific errors (e.g., <code>ErrOutOfOrderTimestamp</code>, <code>ErrSeriesNotFound</code>).</li>\n<li><strong>Testing</strong>: Use table-driven tests extensively. Create golden files for binary format verification.</li>\n</ol>\n<h4 id=\"e-first-milestone-checkpoint-conceptual\">E. First Milestone Checkpoint (Conceptual)</h4>\n<p>Before writing code, validate your understanding by answering:</p>\n<ol>\n<li><strong>Write down</strong> three reasons why B-trees are suboptimal for time-series writes compared to TSM trees.</li>\n<li><strong>Sketch</strong> how delta-of-delta encoding would compress this timestamp sequence (in nanoseconds): \n<code>[1000, 2000, 3000, 4500, 6000]</code></li>\n<li><strong>Explain</strong> in one sentence why columnar storage within blocks helps compression for time-series data.</li>\n</ol>\n<p>Expected answers:</p>\n<ol>\n<li>B-trees cause random writes for sequential data, have high overhead for small inserts, and don&#39;t leverage temporal locality for compression.</li>\n<li>First delta: <code>[1000, 1000, 1000, 1500, 1500]</code>; Delta-of-delta: <code>[1000, 0, 0, 500, 0]</code> (most values are 0, compressible).</li>\n<li>Columnar storage groups similar values (timestamps with timestamps, values with values) allowing specialized compression algorithms to exploit patterns within each column.</li>\n</ol>\n<p>This foundational understanding will guide implementation decisions throughout the project.</p>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the foundational scope that guides all five milestones.</p>\n</blockquote>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<p>This section clearly defines the boundaries of the TempoDB project. Establishing precise goals and, equally importantly, explicit non-goals is critical for building a focused, learnable system. Time-series databases are complex and can expand into numerous features (distributed scaling, full SQL, complex analytics). By defining a narrow but deep scope, we ensure we build a complete, working system that demonstrates the <em>core architectural patterns</em> of a time-series database without being overwhelmed by auxiliary concerns.</p>\n<h3 id=\"goals\">Goals</h3>\n<p>The primary goal of TempoDB is to create a single-node, educational time-series database that demonstrates the essential architectural patterns required to handle high-velocity, sequential data. The system must be <strong>complete enough to be useful</strong> for basic metrics collection and querying, and <strong>pedagogically sound</strong> to illustrate the key trade-offs in storage, ingestion, and query design. All goals are prioritized for clarity of implementation and learning value over production-grade robustness or extreme performance.</p>\n<p>The following table enumerates the specific functional and non-functional requirements that TempoDB <em>must</em> deliver.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Category</th>\n<th align=\"left\">Goal</th>\n<th align=\"left\">Rationale &amp; Learning Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Write Throughput</strong></td>\n<td align=\"left\">Sustain thousands of writes per second on commodity hardware.</td>\n<td align=\"left\">Demonstrates the importance of batching, buffering, and sequential I/O patterns for high-ingestion workloads, contrasting with random-write databases.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Storage Efficiency</strong></td>\n<td align=\"left\">Achieve significant compression (targeting 10x or better for regular metrics) via <strong>columnar storage</strong>, <strong>delta-of-delta encoding</strong> for timestamps, and <strong>Gorilla compression</strong> for floating-point values.</td>\n<td align=\"left\">Teaches how to exploit the predictable patterns (monotonic timestamps, slowly-changing values) inherent in time-series data to reduce storage costs.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Query Performance</strong></td>\n<td align=\"left\">Support efficient <strong>range queries</strong> (e.g., &quot;last hour of data&quot;) by scanning only relevant time blocks, using a <strong>block-based index</strong> with min/max timestamps.</td>\n<td align=\"left\">Illustrates the principle of <strong>temporal locality</strong> and how indexing by time primary key is fundamentally different from indexing by arbitrary keys.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Data Model</strong></td>\n<td align=\"left\">Implement an InfluxDB-like data model with <strong>measurements</strong>, <strong>tags</strong> (indexed key-value pairs), and <strong>fields</strong> (values). Support <strong>series cardinality</strong> tracking.</td>\n<td align=\"left\">Provides a practical, industry-relevant model for organizing metrics and demonstrates the performance implications of high cardinality.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Query Language</strong></td>\n<td align=\"left\">Provide a SQL-like query language supporting <code>SELECT</code>, <code>FROM</code>, <code>WHERE</code> (time and tag filters), and <code>GROUP BY time()</code> intervals with built-in <strong>aggregation functions</strong> (sum, avg, min, max, count).</td>\n<td align=\"left\">Teaches query parsing, planning, and the pushdown of time-range predicates and simple aggregations to the storage layer.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Durability</strong></td>\n<td align=\"left\">Guarantee write durability through a <strong>Write-Ahead Log (WAL)</strong>. Acknowledged writes must survive process crashes.</td>\n<td align=\"left\">Introduces the fundamental pattern of logging before applying changes, which is critical for any reliable database system.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Data Lifecycle</strong></td>\n<td align=\"left\">Enforce <strong>Time-To-Live (TTL)</strong> policies for automatic data expiration and implement <strong>background compaction</strong> to merge small files and reclaim space.</td>\n<td align=\"left\">Shows how to manage the lifecycle of time-series data, which is typically valued less as it ages, and how to maintain read performance over time.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Downsampling</strong></td>\n<td align=\"left\">Support <strong>downsampling</strong> queries that return lower-resolution, aggregated views of raw data (e.g., 1-minute averages from 1-second data).</td>\n<td align=\"left\">Demonstrates a key analytical operation for visualizing long time ranges and pre-computing rollups for performance.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Operational Simplicity</strong></td>\n<td align=\"left\">Expose a simple <strong>HTTP API</strong> for writes (compatible with InfluxDB line protocol) and queries, and <strong>Grafana compatibility</strong> for visualization.</td>\n<td align=\"left\">Makes the system tangible and testable with common tools, reinforcing the connection between architecture and user-facing value.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Code Clarity</strong></td>\n<td align=\"left\">The implementation should be well-structured, modular, and documented to serve as a learning artifact. Primary logic should avoid unnecessary concurrency complexity initially.</td>\n<td align=\"left\">The ultimate goal is education; the code must be readable and traceable to the design concepts explained in this document.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Design Principle:</strong> <em>Depth over Breadth.</em> Each implemented feature should be built to its logical conclusion, demonstrating the complete data path from API to disk and back, rather than sketching many half-features.</p>\n</blockquote>\n<h4 id=\"non-functional-goal-elaboration\">Non-Functional Goal Elaboration</h4>\n<ul>\n<li><strong>Performance Profile:</strong> Write latency should be predictable and dominated by periodic disk flushes rather than per-point overhead. Read latency for recent data should be sub-second, utilizing in-memory structures (memtable). Full historical scans will be slower but bounded by sequential disk I/O.</li>\n<li><strong>Resource Usage:</strong> The database should be memory-efficient, avoiding loading entire datasets into memory. Compression reduces disk footprint. The system should be stable under sustained write load, employing backpressure when necessary.</li>\n<li><strong>Correctness:</strong> The system must provide durable writes and correct query results. For the educational scope, we prioritize &quot;eventual&quot; correctness after crashes (via WAL replay) over complex transactional guarantees.</li>\n</ul>\n<h3 id=\"non-goals\">Non-Goals</h3>\n<p>Explicitly stating what TempoDB will <em>not</em> do is essential to prevent scope creep and to focus effort on the core learning objectives. The following features are deliberately excluded from the initial design. Many are important for a production system but represent orthogonal complexities that can be studied independently after mastering the fundamentals.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Feature</th>\n<th align=\"left\">Reason for Exclusion</th>\n<th align=\"left\">Potential Learning Extension</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Horizontal Scaling / Clustering</strong></td>\n<td align=\"left\">Distributing data across multiple nodes introduces complex problems of consistency, replication, and global querying (e.g., consensus, vector clocks, distributed query planning). This is a vast topic deserving its own dedicated study.</td>\n<td align=\"left\">A future extension could add a simple sharding layer based on measurement name or hash of series key.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Full SQL Support</strong></td>\n<td align=\"left\">Implementing a complete SQL parser, optimizer, and executor with joins, subqueries, and complex expressions is a massive undertaking that distracts from time-series-specific optimizations.</td>\n<td align=\"left\">The simple query language can be extended with more functions and syntax over time.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Cross-Series Transactions (ACID)</strong></td>\n<td align=\"left\">Time-series data is primarily append-only. Supporting multi-series, multi-statement transactions with rollback adds significant complexity (locking, undo logs) for limited benefit in the target use cases.</td>\n<td align=\"left\">Basic single-series atomicity is provided by the WAL.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Advanced Security (RBAC, Encryption)</strong></td>\n<td align=\"left\">Authentication, authorization, and encryption are critical for production but are generic infrastructure concerns not unique to time-series databases.</td>\n<td align=\"left\">Can be added via a middleware proxy or later integration.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Continuous Queries</strong></td>\n<td align=\"left\">While a common feature (pre-computing aggregations in real-time), they are essentially a streaming computation layer on top of the write path. They add significant scheduling and state management complexity.</td>\n<td align=\"left\">Can be implemented as a separate background job scheduler that runs periodic aggregation queries.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Tiered Storage (Auto-migration to S3)</strong></td>\n<td align=\"left\">Automatically moving cold data to object storage involves lifecycle management, network I/O, and possibly different file formats. It&#39;s a valuable production feature but an orthogonal storage layer concern.</td>\n<td align=\"left\">A manual &quot;export to S3&quot; command could be a simpler first step.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Advanced Data Types</strong></td>\n<td align=\"left\">Support for complex types (arrays, histograms, geospatial) or efficient compression for integers, booleans, and strings. We focus on floating-point numbers as the most common and pedagogically interesting case for compression.</td>\n<td align=\"left\">New compression schemes (e.g., for integers) can be added as pluggable codecs.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>High Availability (Replication, Failover)</strong></td>\n<td align=\"left\">Ensuring the database survives machine failures requires replication and automatic failover, which again enters the domain of distributed systems.</td>\n<td align=\"left\">A simple primary-standby replication using WAL shipping could be a follow-on project.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Sophisticated Memory Management</strong></td>\n<td align=\"left\">Production databases often use custom memory allocators, page caches, and direct I/O to optimize performance. These are advanced system programming topics.</td>\n<td align=\"left\">We rely on Go&#39;s runtime and standard library for memory and file I/O for simplicity.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Pluggable Storage Backends</strong></td>\n<td align=\"left\">Abstracting the storage engine to support multiple backends (e.g., PostgreSQL, cloud storage) adds abstraction overhead and obscures the core TSM engine design.</td>\n<td align=\"left\">The TSM engine <em>is</em> the core learning artifact.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Design Principle:</strong> <em>Defer Complexity.</em> The system is designed to be a <strong>vertical slice</strong> of a time-series database, not a <strong>horizontal platform</strong>. By deferring or omitting these features, we maintain a clear line of sight from the core data structures on disk to the user-facing API, which is the primary educational objective.</p>\n</blockquote>\n<h4 id=\"the-quotnot-nowquot-vs-quotnot-everquot-distinction\">The &quot;Not Now&quot; vs. &quot;Not Ever&quot; Distinction</h4>\n<p>For the purposes of this project and design document, the listed non-goals are considered out of scope. However, the architecture does not intentionally preclude their future addition. For example:</p>\n<ul>\n<li>The <strong><code>SeriesKey</code></strong> structure could be extended.</li>\n<li>The <strong>TSM file format</strong> includes versioning in its header for future evolution.</li>\n<li>The <strong>HTTP API</strong> could be extended with new endpoints.</li>\n</ul>\n<p>The decision to exclude a feature is based on its impact on the <strong>initial learning journey</strong>, not on its inherent value.</p>\n<h3 id=\"summary-of-scope\">Summary of Scope</h3>\n<p>TempoDB will be a fully functional, single-node time-series database that excels at ingesting and compressing regular measurements and answering time-range queries with aggregations. It will not be a distributed, highly available, general-purpose SQL database. This focused scope allows us to build a complete system that deeply explores the unique architectural patterns—such as columnar block storage, time-based compaction, and specialized compression—that make time-series databases a distinct and valuable category of data infrastructure.</p>\n<hr>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This architectural overview provides the foundation for all five milestones, showing how components interact to achieve the system&#39;s core capabilities.</p>\n</blockquote>\n<p>TempoDB follows a layered architecture that separates concerns between ingestion, storage, query processing, and maintenance. The core insight is the separation between write-optimized and read-optimized data structures: incoming writes are buffered in memory for fast acknowledgment, then periodically flushed to disk in an optimized columnar format. This pattern, common in modern databases, provides the dual benefits of high write throughput and efficient read performance.</p>\n<h3 id=\"mental-model-the-time-series-processing-pipeline\">Mental Model: The Time-Series Processing Pipeline</h3>\n<p>Imagine TempoDB as a specialized factory for processing time-stamped measurements. Raw data arrives continuously on a high-speed conveyor belt (the <strong>write path</strong>). The factory has several specialized stations:</p>\n<ol>\n<li><strong>Receiving Dock (Ingest API)</strong>: Validates and categorizes incoming shipments</li>\n<li><strong>Receipt Printer (Write-Ahead Log)</strong>: Creates durable records of each shipment before any processing</li>\n<li><strong>Sorting Buffer (Memtable)</strong>: Temporarily holds items while they&#39;re being organized</li>\n<li><strong>Packaging Department (Storage Engine)</strong>: Compresses and packages items into optimized containers (TSM files) for long-term storage</li>\n<li><strong>Retrieval Desk (Query Engine)</strong>: Fetches and processes packages based on customer requests</li>\n<li><strong>Warehouse Maintenance (Compactor)</strong>: Periodically reorganizes storage, discards expired items, and creates summary catalogs</li>\n</ol>\n<p>This mental model helps visualize how data flows from ingestion to storage and back out through queries, with each component having a clear responsibility in the pipeline.</p>\n<h3 id=\"component-overview-and-responsibilities\">Component Overview and Responsibilities</h3>\n<p>The following diagram shows the major components and their interactions:</p>\n<p><img src=\"/api/project/time-series-db/architecture-doc/asset?path=diagrams%2Fsystem-component.svg\" alt=\"TempoDB High-Level System Architecture\"></p>\n<p>Each component has specific responsibilities and interacts with others through well-defined interfaces. The table below summarizes these responsibilities:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Primary Responsibility</th>\n<th>Key Data It Owns</th>\n<th>Performance Critical Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Ingest API</strong></td>\n<td>Accept writes and queries from clients, validate input, format responses</td>\n<td>Client connections, request buffers</td>\n<td>Request parsing, response serialization</td>\n</tr>\n<tr>\n<td><strong>Write-Ahead Log (WAL)</strong></td>\n<td>Ensure write durability before acknowledging to client</td>\n<td>WAL segment files on disk</td>\n<td>Sequential append with <code>fsync</code>, quick recovery scan</td>\n</tr>\n<tr>\n<td><strong>Memtable</strong></td>\n<td>Buffer writes in memory for fast acknowledgment and batching</td>\n<td>In-memory sorted data structure per series</td>\n<td>Concurrent insert, range scan for flush</td>\n</tr>\n<tr>\n<td><strong>Storage Engine (TSM)</strong></td>\n<td>Persist data to disk in optimized columnar format, serve reads</td>\n<td>TSM files on disk, block cache</td>\n<td>Range scans with predicate pushdown, compression/decompression</td>\n</tr>\n<tr>\n<td><strong>Query Engine</strong></td>\n<td>Parse, plan, and execute queries, streaming results</td>\n<td>Query plans, execution state</td>\n<td>Index lookups, aggregation pushdown, result streaming</td>\n</tr>\n<tr>\n<td><strong>Compactor</strong></td>\n<td>Background maintenance: merge files, delete expired data, create rollups</td>\n<td>Compaction state, scheduling metadata</td>\n<td>Disk I/O scheduling, resource throttling</td>\n</tr>\n</tbody></table>\n<p>Let&#39;s examine each component in detail:</p>\n<h4 id=\"ingest-api\">Ingest API</h4>\n<p>The <strong>Ingest API</strong> serves as the system&#39;s front door, handling all external communication. It exposes two primary interfaces:</p>\n<ol>\n<li><strong>Write API</strong>: Accepts time-series data points, typically in InfluxDB line protocol format</li>\n<li><strong>Query API</strong>: Accepts query requests and returns results in JSON or other formats</li>\n</ol>\n<p>This component is responsible for request validation, authentication (if implemented), rate limiting, and connection management. It translates external requests into internal operations and formats responses for clients. The API supports both HTTP/REST and (optionally) gRPC interfaces.</p>\n<p><strong>Key Design Decisions</strong>:</p>\n<ul>\n<li>Use a connection pool to handle concurrent requests efficiently</li>\n<li>Validate all incoming data before processing (timestamp ranges, value types, etc.)</li>\n<li>Implement request timeouts to prevent hung connections from consuming resources</li>\n<li>Support content negotiation for different response formats (JSON, CSV, binary)</li>\n</ul>\n<h4 id=\"write-ahead-log-wal\">Write-Ahead Log (WAL)</h4>\n<p>The <strong>Write-Ahead Log</strong> provides durability guarantees by recording every write operation to disk before acknowledging it to the client. This follows the &quot;write-ahead logging&quot; principle common in database systems: no modification to persistent data structures occurs without first being logged.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The WAL acts as a &quot;flight recorder&quot; for the database. In the event of a crash, the system can replay the WAL to reconstruct the state of in-memory buffers that were lost, ensuring no acknowledged writes are lost.</p>\n</blockquote>\n<p>The WAL uses a segment-based architecture:</p>\n<ul>\n<li><strong>Active segment</strong>: Currently being written to</li>\n<li><strong>Closed segments</strong>: Previously filled segments that can be deleted after their contents are flushed to TSM files</li>\n<li><strong>Index file</strong>: Maps series keys to their positions in WAL segments for efficient recovery</li>\n</ul>\n<p>Each WAL entry contains the full <code>DataPoint</code> information (series key, timestamp, value) in a compact binary format. The WAL must support extremely fast sequential writes and periodic <code>fsync</code> operations to ensure durability.</p>\n<h4 id=\"memtable\">Memtable</h4>\n<p>The <strong>Memtable</strong> (memory table) is an in-memory buffer that holds recently written data points before they&#39;re flushed to disk. It serves several purposes:</p>\n<ol>\n<li><strong>Write amplification reduction</strong>: Batches multiple writes into single disk operations</li>\n<li><strong>Fast acknowledgment</strong>: Allows immediate acknowledgment of writes after WAL logging</li>\n<li><strong>Temporal locality</strong>: Recent data (often queried together) remains in memory</li>\n<li><strong>Sorting</strong>: Organizes data by series and timestamp before disk persistence</li>\n</ol>\n<p>The memtable uses a sorted data structure (typically a skip list or B-tree) keyed by <code>SeriesKey</code> and <code>Timestamp</code>. When it reaches a configurable size threshold (e.g., 64MB), it&#39;s marked as <strong>immutable</strong>, a new <strong>active</strong> memtable is created, and the immutable memtable is scheduled for flushing to disk.</p>\n<blockquote>\n<p><strong>ADR: Choosing Memtable Data Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: Need fast concurrent inserts and efficient range scans for flushing</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Concurrent skip list</strong>: Lock-free or fine-grained locking, excellent for range queries</li>\n<li><strong>B-tree with copy-on-write</strong>: Good for ordered traversal, moderate concurrency</li>\n<li><strong>Per-series sorted slice with global lock</strong>: Simple but poor concurrency</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Concurrent skip list implementation</li>\n<li><strong>Rationale</strong>: Skip lists provide O(log n) operations with good concurrency characteristics and excellent cache locality for range scans, which are critical during flush operations</li>\n<li><strong>Consequences</strong>: Higher memory overhead than B-trees (~50% more pointers) but simpler concurrent implementation without complex rebalancing logic</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent skip list</td>\n<td>Excellent concurrency, good range scan performance, simple implementation</td>\n<td>Higher memory overhead (~2N pointers), not cache-optimal</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>B-tree with copy-on-write</td>\n<td>Good memory efficiency, excellent for ordered traversal</td>\n<td>Complex concurrent modification, rebalancing overhead</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Per-series sorted slice</td>\n<td>Simple, excellent for flushing by series</td>\n<td>Poor concurrent write performance, expensive inserts</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h4 id=\"storage-engine-tsm\">Storage Engine (TSM)</h4>\n<p>The <strong>Storage Engine</strong> implements the Time-Structured Merge Tree (TSM) format for persistent storage. This is the core of TempoDB&#39;s read-optimized storage, where data is organized in a columnar layout with aggressive compression.</p>\n<p>The TSM engine manages:</p>\n<ol>\n<li><strong>TSM files</strong>: Immutable files containing compressed time-series data</li>\n<li><strong>Block cache</strong>: Frequently accessed data blocks kept in memory</li>\n<li><strong>File index</strong>: In-memory mapping of series keys to their locations in TSM files</li>\n<li><strong>Statistics</strong>: Min/max timestamps, value ranges, and other metadata per block</li>\n</ol>\n<p>Each TSM file contains multiple <strong>series blocks</strong>, each holding data for a single time series. Within a series block, data is organized into <strong>data blocks</strong> covering contiguous time ranges. Each data block stores timestamps and values in separate columns with specialized compression.</p>\n<p><strong>Data Flow Through Storage Engine</strong>:</p>\n<ol>\n<li>During flush, the memtable&#39;s data is sorted by series and timestamp</li>\n<li>Each series&#39; data is compressed using delta-of-delta (timestamps) and Gorilla XOR (values)</li>\n<li>Compressed blocks are written to a new TSM file along with index information</li>\n<li>The file index is updated to include the new file&#39;s time ranges</li>\n<li>Old TSM files (from previous compactions) may be marked for deletion</li>\n</ol>\n<h4 id=\"query-engine\">Query Engine</h4>\n<p>The <strong>Query Engine</strong> processes read requests, transforming high-level queries into efficient storage operations. It follows a traditional database query processing pipeline with time-series optimizations:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Parse → Plan → Optimize → Execute → Stream</code></pre></div>\n\n<p>The key optimization is <strong>predicate pushdown</strong>: time range and tag filters are applied as early as possible, ideally at the storage layer, to minimize data movement. The query engine also handles:</p>\n<ul>\n<li><strong>Aggregation</strong>: Built-in functions (sum, avg, min, max, count) with pushdown where possible</li>\n<li><strong>Downsampling</strong>: GROUP BY time() operations that bucket data into fixed intervals</li>\n<li><strong>Multi-series queries</strong>: Joining data from multiple series based on time alignment</li>\n<li><strong>Result streaming</strong>: Progressive return of results to avoid large memory allocations</li>\n</ul>\n<p>The engine uses an <strong>iterator model</strong> where each stage of query processing produces a stream of <code>DataPoint</code> records that flow through operators (filter, aggregate, transform).</p>\n<h4 id=\"compactor\">Compactor</h4>\n<p>The <strong>Compactor</strong> handles background maintenance tasks that optimize storage and enforce data lifecycle policies:</p>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th>Frequency</th>\n<th>Purpose</th>\n<th>Resource Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Memtable flush</strong></td>\n<td>When memtable reaches size threshold</td>\n<td>Move data from memory to persistent storage</td>\n<td>High I/O (write burst)</td>\n</tr>\n<tr>\n<td><strong>TSM compaction</strong></td>\n<td>Periodically or when many small files exist</td>\n<td>Merge small TSM files into larger ones, remove duplicates</td>\n<td>Moderate I/O and CPU</td>\n</tr>\n<tr>\n<td><strong>Retention enforcement</strong></td>\n<td>Scheduled (e.g., hourly)</td>\n<td>Delete data older than retention period</td>\n<td>Moderate I/O (file deletion)</td>\n</tr>\n<tr>\n<td><strong>Rollup generation</strong></td>\n<td>Scheduled for historical data</td>\n<td>Create pre-aggregated summaries for fast queries</td>\n<td>High CPU, moderate I/O</td>\n</tr>\n</tbody></table>\n<p>The compactor uses a <strong>leveled compaction strategy</strong> similar to LSM trees but optimized for time-series:</p>\n<ul>\n<li><strong>Level 0</strong>: Recently flushed TSM files (may have overlapping time ranges)</li>\n<li><strong>Level 1-3</strong>: Compacted files with non-overlapping time ranges</li>\n<li>Each level has a target file size, with older data in larger files</li>\n</ul>\n<blockquote>\n<p><strong>Design Insight</strong>: Compaction is scheduled based on both time and space heuristics. Time-based scheduling ensures predictable performance impact, while space-based triggers prevent storage inefficiency from accumulating too many small files.</p>\n</blockquote>\n<h3 id=\"component-interactions\">Component Interactions</h3>\n<p>The components interact through well-defined interfaces and protocols. The following tables describe the key interactions:</p>\n<p><strong>Write Path Interactions</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>From Component</th>\n<th>To Component</th>\n<th>Data Passed</th>\n<th>Trigger Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingest API</td>\n<td>WAL</td>\n<td><code>SeriesKey</code> + <code>DataPoint</code></td>\n<td>Every write request</td>\n</tr>\n<tr>\n<td>WAL</td>\n<td>Memtable</td>\n<td><code>SeriesKey</code> + <code>DataPoint</code></td>\n<td>After successful log append</td>\n</tr>\n<tr>\n<td>Ingest API</td>\n<td>Client</td>\n<td>Acknowledgment</td>\n<td>After WAL append (or async after memtable)</td>\n</tr>\n<tr>\n<td>Memtable</td>\n<td>Storage Engine</td>\n<td>Batch of <code>DataPoint</code> per series</td>\n<td>Memtable reaches size threshold</td>\n</tr>\n<tr>\n<td>Storage Engine</td>\n<td>Compactor</td>\n<td>TSM file metadata</td>\n<td>New TSM file created</td>\n</tr>\n</tbody></table>\n<p><strong>Read Path Interactions</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>From Component</th>\n<th>To Component</th>\n<th>Data Passed</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingest API</td>\n<td>Query Engine</td>\n<td><code>Query</code> structure</td>\n<td>Query request</td>\n</tr>\n<tr>\n<td>Query Engine</td>\n<td>Storage Engine</td>\n<td>Time range + series filter</td>\n<td>Data retrieval</td>\n</tr>\n<tr>\n<td>Storage Engine</td>\n<td>Query Engine</td>\n<td>Stream of <code>DataPoint</code></td>\n<td>Query results</td>\n</tr>\n<tr>\n<td>Query Engine</td>\n<td>Ingest API</td>\n<td>Formatted results</td>\n<td>Response to client</td>\n</tr>\n</tbody></table>\n<p><strong>Background Task Interactions</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>From Component</th>\n<th>To Component</th>\n<th>Data Passed</th>\n<th>Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compactor</td>\n<td>Storage Engine</td>\n<td>List of TSM files to merge</td>\n<td>Scheduled or threshold-based</td>\n</tr>\n<tr>\n<td>Storage Engine</td>\n<td>Compactor</td>\n<td>New TSM file after compaction</td>\n<td>After compaction completes</td>\n</tr>\n<tr>\n<td>Compactor</td>\n<td>File System</td>\n<td>Paths of expired TSM files</td>\n<td>Retention policy schedule</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls-in-architecture-design\">Common Pitfalls in Architecture Design</h3>\n<p>⚠️ <strong>Pitfall: Tight coupling between components</strong></p>\n<ul>\n<li><strong>Description</strong>: Making components directly depend on each other&#39;s internal implementations</li>\n<li><strong>Why it&#39;s wrong</strong>: Makes testing difficult, inhibits independent evolution, complicates debugging</li>\n<li><strong>Fix</strong>: Define clear interfaces between components, use dependency injection for testing</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Single-threaded bottleneck in write path</strong></p>\n<ul>\n<li><strong>Description</strong>: All writes go through a global lock or single goroutine</li>\n<li><strong>Why it&#39;s wrong</strong>: Limits write throughput, doesn&#39;t scale with cores</li>\n<li><strong>Fix</strong>: Use sharded memtables (by series hash), lock-free data structures, or partitioned WAL</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Inefficient memory management</strong></p>\n<ul>\n<li><strong>Description</strong>: Allocating individual <code>DataPoint</code> objects, causing GC pressure</li>\n<li><strong>Why it&#39;s wrong</strong>: High garbage collection overhead reduces throughput</li>\n<li><strong>Fix</strong>: Use memory pools, slice-based storage, or arena allocation for batched points</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Blocking writes during flush</strong></p>\n<ul>\n<li><strong>Description</strong>: Stopping write acceptance while memtable flushes to disk</li>\n<li><strong>Why it&#39;s wrong</strong>: Creates write latency spikes, reduces availability</li>\n<li><strong>Fix</strong>: Implement double buffering with active and immutable memtables</li>\n</ul>\n<p>⚠️ <strong>Pitfall: No backpressure mechanism</strong></p>\n<ul>\n<li><strong>Description</strong>: Accepting writes indefinitely even when system is overloaded</li>\n<li><strong>Why it&#39;s wrong</strong>: Leads to out-of-memory crashes or severe performance degradation</li>\n<li><strong>Fix</strong>: Implement write rejection or throttling when buffers exceed thresholds</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Recommended)</th>\n<th>Advanced Option (Extension)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Ingest API</strong></td>\n<td><code>net/http</code> with JSON/line protocol</td>\n<td>gRPC with Protocol Buffers</td>\n</tr>\n<tr>\n<td><strong>WAL Persistence</strong></td>\n<td>Direct file I/O with <code>os.File</code></td>\n<td>Memory-mapped files with <code>syscall.Mmap</code></td>\n</tr>\n<tr>\n<td><strong>Memtable</strong></td>\n<td>Skip list with <code>sync.RWMutex</code></td>\n<td>Lock-free skip list using atomic operations</td>\n</tr>\n<tr>\n<td><strong>TSM File I/O</strong></td>\n<td>Standard file operations</td>\n<td>Memory-mapped file access</td>\n</tr>\n<tr>\n<td><strong>Concurrency Control</strong></td>\n<td>Channel-based pipelines</td>\n<td>Work-stealing scheduler</td>\n</tr>\n<tr>\n<td><strong>Compression</strong></td>\n<td>Custom delta/Gorilla implementation</td>\n<td>Plug-in architecture for multiple algorithms</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<p>Organize the codebase as follows to maintain separation of concerns and enable clean testing:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   ├── tempo-server/\n│   │   └── main.go              # Server entry point\n│   └── tempo-cli/\n│       └── main.go              # CLI tool (optional)\n├── internal/\n│   ├── api/\n│   │   ├── http/\n│   │   │   ├── server.go        # HTTP server setup\n│   │   │   ├── handlers.go      # Request handlers\n│   │   │   └── middleware.go    # Auth, logging middleware\n│   │   └── query/\n│   │       ├── parser.go        # Query language parser\n│   │       ├── planner.go       # Query plan generation\n│   │       └── executor.go      # Query execution\n│   ├── storage/\n│   │   ├── wal/\n│   │   │   ├── wal.go           # Write-ahead log implementation\n│   │   │   ├── segment.go       # WAL segment management\n│   │   │   └── recovery.go      # WAL recovery on startup\n│   │   ├── memtable/\n│   │   │   ├── memtable.go      # In-memory table interface\n│   │   │   ├── skiplist.go      # Skip list implementation\n│   │   │   └── buffer_pool.go   # Memory pool for points\n│   │   ├── tsm/\n│   │   │   ├── writer.go        # TSM file writer\n│   │   │   ├── reader.go        # TSM file reader\n│   │   │   ├── index.go         # File index management\n│   │   │   └── compression.go   # Compression algorithms\n│   │   └── engine/\n│   │       ├── engine.go        # Storage engine facade\n│   │       ├── cache.go         # Block cache\n│   │       └── statistics.go    # Storage statistics\n│   ├── query/\n│   │   ├── engine.go            # Query engine coordination\n│   │   ├── iterator.go          # Iterator interfaces\n│   │   ├── aggregator.go        # Aggregation functions\n│   │   └── downsampler.go       # Downsampling logic\n│   ├── compaction/\n│   │   ├── manager.go           # Compaction coordination\n│   │   ├── planner.go           # Compaction planning\n│   │   ├── executor.go          # Compaction execution\n│   │   └── retention.go         # TTL enforcement\n│   └── meta/\n│       ├── catalog.go           # Series catalog\n│       ├── statistics.go        # System statistics\n│       └── sharding.go          # Shard management (future)\n├── pkg/\n│   ├── models/\n│   │   ├── point.go             # DataPoint, SeriesKey types\n│   │   ├── query.go             # Query structure\n│   │   └── tsm.go               # TSM block structures\n│   ├── protocol/\n│   │   ├── lineproto.go         # Line protocol parser\n│   │   └── prometheus.go        # Prometheus remote read/write\n│   └── utils/\n│       ├── timeutil.go          # Time utilities\n│       ├── bytesutil.go         # Byte manipulation\n│       └── pool.go              # Object pools\n└── test/\n    ├── integration/\n    │   └── end_to_end_test.go   # Integration tests\n    └── benchmarks/\n        └── write_bench_test.go  # Performance benchmarks</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Here&#39;s a complete, ready-to-use implementation for the WAL segment management, a common infrastructure component:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/wal/segment.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> wal</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Segment represents a single WAL segment file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Segment</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSize  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    closed   </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    firstID  </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#6A737D\">  // First entry ID in this segment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastID   </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#6A737D\">  // Last entry ID in this segment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SegmentConfig holds configuration for WAL segments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SegmentConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxSizeBytes </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SyncInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SyncOnWrite  </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewSegment creates or opens a WAL segment file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSegment</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">firstID</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> SegmentConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create directory if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">MkdirAll</span><span style=\"color:#E1E4E8\">(filepath.</span><span style=\"color:#B392F0\">Dir</span><span style=\"color:#E1E4E8\">(path), </span><span style=\"color:#79B8FF\">0755</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"create wal directory: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Open or create the file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">OpenFile</span><span style=\"color:#E1E4E8\">(path, os.O_CREATE</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_RDWR</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_APPEND, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"open wal segment: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Get current size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    info, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Stat</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"stat wal segment: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    segment </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        path:    path,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file:    file,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        size:    info.</span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxSize: config.MaxSizeBytes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        firstID: firstID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastID:  firstID </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// Will be incremented on first write</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // If file is not empty, scan to find the last entry ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> segment.size </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> segment.</span><span style=\"color:#B392F0\">scanLastID</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            segment.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"scan existing wal segment: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> segment, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteEntry appends an entry to the segment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.closed {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"segment is closed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check if we need to rotate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entrySize </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(data) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// 8 bytes for CRC32</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.size</span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\">entrySize </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> s.maxSize {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, ErrSegmentFull</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Generate entry ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.lastID</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entryID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.lastID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write entry: [4-byte CRC32][4-byte data length][data]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> buf [</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.LittleEndian.</span><span style=\"color:#B392F0\">PutUint32</span><span style=\"color:#E1E4E8\">(buf[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#B392F0\">crc32Checksum</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    binary.LittleEndian.</span><span style=\"color:#B392F0\">PutUint32</span><span style=\"color:#E1E4E8\">(buf[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(data)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write header</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buf[:]); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.lastID</span><span style=\"color:#F97583\">--</span><span style=\"color:#6A737D\"> // Rollback ID on error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"write wal entry header: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(data); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Try to truncate the partial write</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.file.</span><span style=\"color:#B392F0\">Truncate</span><span style=\"color:#E1E4E8\">(s.size)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.lastID</span><span style=\"color:#F97583\">--</span><span style=\"color:#6A737D\"> // Rollback ID on error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"write wal entry data: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.size </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(data) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Sync if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // (SyncOnWrite logic would go here)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entryID, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Scan reads all entries from the segment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Scan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">fn</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Seek to beginning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Seek</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"seek wal segment: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> nextID </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> s.firstID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Read header</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        var</span><span style=\"color:#E1E4E8\"> header [</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        n, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(header[:])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"EOF\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read wal entry header: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Extract length and checksum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expectedCRC </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.LittleEndian.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(header[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        dataLen </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.LittleEndian.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(header[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Read data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, dataLen)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        n, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(n) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> dataLen {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read wal entry data: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Verify checksum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        actualCRC </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> crc32Checksum</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> actualCRC </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expectedCRC {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"wal entry checksum mismatch: expected </span><span style=\"color:#79B8FF\">%x</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">%x</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                expectedCRC, actualCRC)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Call callback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> fn</span><span style=\"color:#E1E4E8\">(nextID, data); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nextID</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close closes the segment file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.closed {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.closed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Delete removes the segment file from disk</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Delete</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">s.closed {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Remove</span><span style=\"color:#E1E4E8\">(s.path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Helper methods</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">scanLastID</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation scans file to find last entry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Returns last ID found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> crc32Checksum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simple implementation - use actual crc32 in production</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> sum </span><span style=\"color:#F97583\">uint32</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, b </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> data {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sum </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (sum </span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">^</span><span style=\"color:#E1E4E8\"> (sum </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 27</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">^</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(b)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> sum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> ErrSegmentFull </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"wal segment is full\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>Here&#39;s skeleton code for the key coordination component, the Storage Engine facade:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/engine/engine.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> engine</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/storage/memtable</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/storage/tsm</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/storage/wal</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/pkg/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageEngine coordinates all storage components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Components</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">wal</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WAL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memtables </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemtableManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tsmReader </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">tsm</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tsmWriter </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">tsm</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Writer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // State</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seriesIndex </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesMetadata</span><span style=\"color:#6A737D\">  // SeriesKey hash -> metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tsmFiles    []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TSMFileRef</span><span style=\"color:#6A737D\">               // Active TSM files</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    blockCache  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BlockCache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Channels for coordination</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    flushCh     </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">memtable</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Memtable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    compactCh   </span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> compactionPlan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh      </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config holds storage engine configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataDir          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxMemtableSize  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WalSegmentSize   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BlockCacheSize   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FlushConcurrency </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewStorageEngine initializes the storage engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStorageEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create data directory structure if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize WAL for durability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize memtable manager with active memtable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Scan existing TSM files and load their indices</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Initialize block cache with configured size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Start background goroutines for flush and compaction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Recover any unflushed data from WAL on startup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#6A737D\"> // Replace with actual return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WritePoint writes a single data point</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WritePoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesKey</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">point</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if context is cancelled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate point (timestamp not in future, valid value, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write to WAL first for durability (this may block on fsync)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Insert into active memtable (concurrent safe)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check if memtable needs flushing (size threshold)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         If yes, trigger async flush and rotate to new memtable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update series metadata (last timestamp, point count)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return nil on success, error on failure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Query executes a range query</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse query time range and validate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Resolve series keys from measurement and tags</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each series, determine which TSM files contain relevant data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create query plan with predicate pushdown optimization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Execute plan: scan memtables first (recent data), then TSM files</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Apply any aggregations or downsampling in the plan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Stream results back through channel to avoid large allocations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FlushMemtable flushes an immutable memtable to TSM files</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">FlushMemtable</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">mt</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">memtable</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Memtable</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create new TSM file writer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through memtable data sorted by series and timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each series, compress data using delta-of-delta and Gorilla</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write compressed blocks to TSM file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Build index mapping series to block offsets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Finalize TSM file and sync to disk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update engine's file list and index</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Signal WAL that data is persisted (can delete old segments)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Update statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Compact runs compaction on selected TSM files</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Compact</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">plan</span><span style=\"color:#B392F0\"> compactionPlan</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if compaction should proceed (enough disk space, not too many files)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Read all data from source TSM files</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Merge data, removing duplicates and expired points</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write merged data to new TSM file(s)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Atomically switch to new files and mark old ones for deletion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Schedule deletion of old files after safe period</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update compaction statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close gracefully shuts down the storage engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Signal background goroutines to stop</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Wait for in-progress flushes and compactions to complete</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Flush any remaining data in memtables</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Close all components (WAL, block cache, file handles)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Persist any metadata needed for clean restart</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints-for-go\">Language-Specific Hints for Go</h4>\n<ol>\n<li><p><strong>Concurrency</strong>: Use <code>sync.RWMutex</code> for read-heavy structures like the series index. For write-heavy paths, consider sharding or lock-free data structures.</p>\n</li>\n<li><p><strong>Memory Management</strong>: Use <code>sync.Pool</code> for frequently allocated objects like <code>DataPoint</code> slices or compression buffers to reduce GC pressure.</p>\n</li>\n<li><p><strong>File I/O</strong>: Use <code>os.File</code> with appropriate buffering. For sequential writes, <code>bufio.Writer</code> can help batch small writes. Remember to call <code>Sync()</code> for durability.</p>\n</li>\n<li><p><strong>Error Handling</strong>: Use Go&#39;s error wrapping with <code>fmt.Errorf(&quot;... %w&quot;, err)</code> to preserve error context. Define sentinel errors for expected failure cases.</p>\n</li>\n<li><p><strong>Context Propagation</strong>: Pass <code>context.Context</code> through all I/O operations to support cancellation and timeouts.</p>\n</li>\n<li><p><strong>Testing</strong>: Use table-driven tests with <code>t.Run()</code> subtests. For integration tests, use temporary directories created with <code>t.TempDir()</code>.</p>\n</li>\n<li><p><strong>Profiling</strong>: Integrate <code>net/http/pprof</code> early for performance debugging. Use <code>runtime.MemStats</code> to track memory usage.</p>\n</li>\n</ol>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the foundational data structures that underpin all five milestones, particularly Milestone 1 (Storage Engine) and Milestone 2 (Write Path). The data model dictates how points are represented, organized, and stored, directly influencing compression efficiency, query performance, and storage layout.</p>\n</blockquote>\n<p>The data model is the fundamental blueprint for how TempoDB organizes and interprets the raw stream of timestamped data. It defines the vocabulary and grammar we use to structure observations from the physical world—like server CPU usage or sensor temperature—into a format the database can efficiently store and retrieve. A well-designed data model balances expressiveness for application developers with optimization potential for the storage engine.</p>\n<h3 id=\"mental-model-the-time-series-library-catalog\">Mental Model: The Time-Series Library Catalog</h3>\n<p>Imagine a vast library dedicated solely to scientific journals. Each journal (<strong>measurement</strong>) records a specific type of observation, like &quot;Temperature&quot; or &quot;Network Latency.&quot; Every individual issue of a journal corresponds to a unique <strong>time series</strong>, distinguished by a set of metadata labels on its spine, such as <code>location=server_room</code> and <code>sensor_id=A12</code> (<strong>tags</strong>). Inside each issue, the actual data is organized as a chronological list of entries (<strong>data points</strong>), where each entry contains a publication date (<strong>timestamp</strong>) and the recorded experimental value (<strong>field value</strong>).</p>\n<p>The library&#39;s catalog (<strong>series index</strong>) doesn&#39;t store the actual data points; instead, it maps each unique combination of journal title and metadata labels (e.g., <code>Temperature{location=server_room, sensor_id=A12}</code>) to a specific shelf location where all its issues (data blocks) are stored. This separation—metadata for identification and ordering, and columnar storage for the actual time-value pairs—is the core of an efficient time-series data model. It allows us to quickly find all series related to a <code>location</code>, and then, within a found series, efficiently scan through time to retrieve values.</p>\n<h3 id=\"core-concepts-measurement-tags-field\">Core Concepts: Measurement, Tags, Field</h3>\n<p>TempoDB adopts a tag-set data model inspired by InfluxDB and Prometheus. This model is exceptionally well-suited for operational monitoring and IoT scenarios where data is multi-dimensional.</p>\n<ul>\n<li><p><strong>Measurement:</strong> A measurement is a container for related data, akin to a table name in SQL. It represents the <em>what</em> is being measured (e.g., <code>cpu_usage</code>, <code>http_requests_total</code>, <code>temperature</code>). All data points within a measurement share the same semantic meaning but may come from different sources identified by tags. The measurement name is a string.</p>\n</li>\n<li><p><strong>Tags:</strong> Tags are key-value pairs of metadata used to identify and filter time series. They represent the <em>dimensions</em> of the data (e.g., <code>host=&quot;web-01&quot;</code>, <code>region=&quot;us-east-1&quot;</code>, <code>http_method=&quot;GET&quot;</code>). Tags are <strong>indexed</strong>, enabling fast queries like &quot;show CPU usage for all hosts in <code>region=us-east-1</code>.&quot; A set of tags uniquely defines a <strong>series</strong>. Tags are intended to have low cardinality relative to the dataset; a tag key like <code>host</code> might have hundreds of values, not millions. High-cardinality tags (like a unique <code>request_id</code>) are discouraged as they explode the series index size and degrade performance.</p>\n</li>\n<li><p><strong>Field:</strong> A field is the actual measured value. It is the <em>quantity</em> being recorded (e.g., <code>value=62.5</code> for a temperature reading). Fields are not indexed; they are stored in columnar format with their corresponding timestamps. While a data point can theoretically have multiple fields (e.g., <code>temperature=62.5, humidity=85</code>), <strong>TempoDB&#39;s initial implementation will support a single <code>float64</code> field per data point for simplicity</strong>. This aligns with common metrics use cases and simplifies the storage engine and compression design.</p>\n</li>\n</ul>\n<p>The combination of a <strong>Measurement</strong> and a complete set of <strong>Tags</strong> forms a <strong>Series Key</strong>, which is the unique identifier for a single time series stream. All data points with the same series key are stored together in time-sorted order.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Concept</th>\n<th align=\"left\">Analogy</th>\n<th align=\"left\">Purpose</th>\n<th align=\"left\">Indexed?</th>\n<th align=\"left\">Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Measurement</strong></td>\n<td align=\"left\">Journal Title / Table Name</td>\n<td align=\"left\">Groups related metrics</td>\n<td align=\"left\">No</td>\n<td align=\"left\"><code>cpu_usage</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Tag</strong></td>\n<td align=\"left\">Journal Metadata / Dimension</td>\n<td align=\"left\">Identifies the source/context of a series</td>\n<td align=\"left\"><strong>Yes</strong></td>\n<td align=\"left\"><code>host=&quot;web-01&quot;</code>, <code>region=&quot;us-east&quot;</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Field</strong></td>\n<td align=\"left\">Journal Article Content / Metric Value</td>\n<td align=\"left\">The actual numerical observation</td>\n<td align=\"left\">No</td>\n<td align=\"left\"><code>value=62.5</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Series Key</strong></td>\n<td align=\"left\">Unique Journal ID (Title + Metadata)</td>\n<td align=\"left\">Uniquely identifies a single data stream</td>\n<td align=\"left\">Via its tags</td>\n<td align=\"left\"><code>cpu_usage{host=&quot;web-01&quot;,region=&quot;us-east&quot;}</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Data Point</strong></td>\n<td align=\"left\">A single journal entry</td>\n<td align=\"left\">A timestamped observation</td>\n<td align=\"left\">-</td>\n<td align=\"left\"><code>(timestamp=2023-10-01:12:00:00, value=62.5)</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The decision to index tags but not fields is a critical performance trade-off. Indexing enables fast series selection but adds write overhead and storage cost. Numerical field values, which are often high-cardinality and compress well in columnar form, are poor candidates for indexing. This tag-set model allows applications to model rich metadata while giving the database a clear path for optimization.</p>\n</blockquote>\n<h3 id=\"type-definitions-and-relationships\">Type Definitions and Relationships</h3>\n<p>The core concepts are realized in Go as a set of concrete types. These types flow through the entire system, from the write API to the storage engine and query engine.</p>\n<p><strong>Primary Data Types</strong></p>\n<p>These types represent the core data entities.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Type Name</th>\n<th align=\"left\">Fields (Name &amp; Type)</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong><code>DataPoint</code></strong></td>\n<td align=\"left\"><code>Timestamp time.Time</code><br><code>Value float64</code></td>\n<td align=\"left\">The atomic unit of storage. Represents a single observation at a specific moment in time. <code>Timestamp</code> is a nanosecond-precision UTC time. <code>Value</code> is the observed float64 measurement.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>SeriesKey</code></strong></td>\n<td align=\"left\"><code>Measurement string</code><br><code>Tags map[string]string</code></td>\n<td align=\"left\">The unique identifier for a time series. The <code>Tags</code> map is <strong>sorted by key</strong> when serialized (e.g., to form a string key for a map) to ensure a consistent, canonical representation.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>Series</code></strong></td>\n<td align=\"left\"><code>Key SeriesKey</code><br><code>Points []DataPoint</code></td>\n<td align=\"left\">An in-memory collection of data points belonging to a single series. Used primarily within the memtable and during query result materialization. The <code>Points</code> slice is always sorted by <code>Timestamp</code> ascending.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>TimeRange</code></strong></td>\n<td align=\"left\"><code>Start time.Time</code><br><code>End time.Time</code></td>\n<td align=\"left\">Represents an inclusive-exclusive time interval <code>[Start, End)</code>. Used throughout the system to bound queries, define block ranges, and specify retention periods.</td>\n</tr>\n</tbody></table>\n<p><strong>Query and Configuration Types</strong></p>\n<p>These types define requests and system configuration.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Type Name</th>\n<th align=\"left\">Fields (Name &amp; Type)</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong><code>Query</code></strong></td>\n<td align=\"left\"><code>Measurement string</code><br><code>Tags map[string]string</code><br><code>TimeRange TimeRange</code><br><code>Aggregate *AggregateFunction</code><br><code>GroupByWindow time.Duration</code><br><code>Fields []string</code></td>\n<td align=\"left\">A request to retrieve data. <code>Tags</code> contains predicates (e.g., <code>{&quot;host&quot;: &quot;web-01&quot;}</code>). <code>Aggregate</code> is a pointer, as it&#39;s optional (nil for raw data). <code>GroupByWindow</code> is optional for windowed aggregations. <code>Fields</code> is reserved for future multi-field support.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>AggregateFunction</code></strong></td>\n<td align=\"left\">(Enum type)</td>\n<td align=\"left\">Specifies the aggregation operation. One of: <code>AggregateSum</code>, <code>AggregateAvg</code>, <code>AggregateMin</code>, <code>AggregateMax</code>, <code>AggregateCount</code>.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>Config</code></strong></td>\n<td align=\"left\"><code>DataDir string</code><br><code>MaxMemtableSize int64</code><br><code>WalSegmentSize int64</code><br><code>BlockCacheSize int64</code><br><code>FlushConcurrency int</code></td>\n<td align=\"left\">Root configuration for the <code>StorageEngine</code>. Controls thresholds for flushing, WAL file size, cache memory, and background job concurrency.</td>\n</tr>\n</tbody></table>\n<p><strong>Storage Engine Internal Types</strong></p>\n<p>These types manage the internal state and components of the storage engine.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Type Name</th>\n<th align=\"left\">Fields (Name &amp; Type)</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong><code>StorageEngine</code></strong></td>\n<td align=\"left\"><code>config Config</code><br><code>wal *wal.WAL</code><br><code>memtables *MemtableManager</code><br><code>tsmReader *tsm.Reader</code><br><code>tsmWriter *tsm.Writer</code><br><code>mu sync.RWMutex</code><br><code>seriesIndex map[string]*SeriesMetadata</code><br><code>tsmFiles []*TSMFileRef</code><br><code>blockCache *BlockCache</code><br><code>flushCh chan *memtable.Memtable</code><br><code>compactCh chan compactionPlan</code><br><code>stopCh chan struct{}</code></td>\n<td align=\"left\">The central coordinator. It owns all subcomponents: the WAL for durability, the memtable manager for writes, the TSM reader/writer for disk I/O, the series index for lookups, the list of active TSM files, and a block cache for hot data. Channels coordinate background flushing and compaction.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>SeriesMetadata</code></strong></td>\n<td align=\"left\">(Conceptual, not a concrete type in naming conventions)</td>\n<td align=\"left\">In-memory metadata for a series. Would typically contain fields like <code>ID uint64</code> (for integer-based references), <code>LastTimestamp time.Time</code>, and references to the TSM files containing its data.</td>\n</tr>\n</tbody></table>\n<p><strong>WAL Segment Types</strong></p>\n<p>These types manage Write-Ahead Log segments.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Type Name</th>\n<th align=\"left\">Fields (Name &amp; Type)</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong><code>Segment</code></strong></td>\n<td align=\"left\"><code>path string</code><br><code>file *os.File</code><br><code>mu sync.RWMutex</code><br><code>size int64</code><br><code>maxSize int64</code><br><code>closed bool</code><br><code>firstID uint64</code><br><code>lastID uint64</code></td>\n<td align=\"left\">Represents an active WAL segment file. <code>firstID</code> and <code>lastID</code> track the range of entry IDs stored in this segment, aiding in cleanup and recovery. <code>mu</code> protects concurrent writes and closure.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong><code>SegmentConfig</code></strong></td>\n<td align=\"left\"><code>MaxSizeBytes int64</code><br><code>SyncInterval time.Duration</code><br><code>SyncOnWrite bool</code></td>\n<td align=\"left\">Configuration for WAL segment behavior, controlling rotation size and durability guarantees.</td>\n</tr>\n</tbody></table>\n<p><strong>Relationship Diagram</strong></p>\n<p>The diagram below illustrates how these primary types relate to each other:</p>\n<p><img src=\"/api/project/time-series-db/architecture-doc/asset?path=diagrams%2Fdata-model-relationships.svg\" alt=\"Data Model: Core Types and Relationships\"></p>\n<p><strong>Key Relationships Explained:</strong></p>\n<ol>\n<li>A <strong><code>StorageEngine</code></strong> contains many <strong><code>TSMFileRef</code></strong> objects (pointing to on-disk TSM files) and an in-memory <strong><code>seriesIndex</code></strong> mapping from string representations of <code>SeriesKey</code> to <code>SeriesMetadata</code>.</li>\n<li>Each <strong><code>TSMFile</code></strong> on disk contains data for potentially many series. For each series, it stores one or more **<code>TSMBlock</code>**s (not a user-facing type), which are columnar chunks of compressed <code>DataPoint</code> values for a specific <code>TimeRange</code>.</li>\n<li>The <strong><code>SeriesKey</code></strong> is the glue. It is used to:<ul>\n<li>Look up series in the <code>seriesIndex</code>.</li>\n<li>Group incoming <code>DataPoint</code> writes in the <code>Memtable</code>.</li>\n<li>Locate the correct <code>TSMBlock</code>s during a query that filters by tags.</li>\n</ul>\n</li>\n<li>A <strong><code>Query</code></strong> specifies constraints on <code>Measurement</code> and <code>Tags</code> (to select series) and a <code>TimeRange</code> (to select points within those series). The result is a collection of <code>DataPoint</code> values, potentially aggregated.</li>\n</ol>\n<p><strong>Architecture Decision Record: Tag-Set Model vs. Wide-Table Model</strong></p>\n<blockquote>\n<p><strong>Decision: Adopt a Tag-Set Data Model</strong></p>\n<ul>\n<li><strong>Context:</strong> We need a data model that is intuitive for time-series data (e.g., metrics, IoT) and allows for efficient storage and querying based on multi-dimensional metadata.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Tag-Set (InfluxDB-like):</strong> A measurement with a set of key-value tag pairs and one or more field values.</li>\n<li><strong>Wide-Table (Classic SQL):</strong> A fixed-schema table where metadata columns become separate <code>VARCHAR</code> columns (e.g., <code>host</code>, <code>region</code>).</li>\n<li><strong>Simple Key-Value:</strong> A single string key representing the entire series (e.g., <code>cpu_usage.host.web-01.region.us-east</code>).</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Option 1, the Tag-Set model.</li>\n<li><strong>Rationale:</strong><ul>\n<li><strong>Dynamic Schema:</strong> Tags can be added or removed per data point without altering a table schema, which is ideal for evolving instrumentation.</li>\n<li><strong>Optimized Indexing:</strong> Tags are stored separately from fields and can be indexed using an inverted index, enabling fast <code>WHERE tag = &#39;value&#39;</code> queries without scanning field data.</li>\n<li><strong>Cardinality Management:</strong> The model clearly distinguishes indexed dimensions (tags) from non-indexed measurements (fields), guiding users to avoid high-cardinality indexes.</li>\n<li><strong>Industry Alignment:</strong> This model is used by InfluxDB, Prometheus, and VictoriaMetrics, ensuring familiarity for users and compatibility with existing ecosystems (e.g., Prometheus remote write).</li>\n</ul>\n</li>\n<li><strong>Consequences:</strong><ul>\n<li><strong>Positive:</strong> Enables flexible, schema-less metadata. Query language naturally supports filtering on multiple tags. Compression benefits from storing tags as dictionary-encoded integers.</li>\n<li><strong>Negative:</strong> Requires maintaining a series index mapping tag sets to internal IDs. Queries with many distinct tag value combinations (high cardinality) can stress this index.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Option</th>\n<th align=\"left\">Pros</th>\n<th align=\"left\">Cons</th>\n<th align=\"left\">Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Tag-Set Model</strong></td>\n<td align=\"left\">Dynamic schema, optimized indexing, clear cardinality guidance, industry standard.</td>\n<td align=\"left\">Requires series index management, can be misused with high-cardinality tags.</td>\n<td align=\"left\"><strong>Yes</strong></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Wide-Table SQL</strong></td>\n<td align=\"left\">Familiar to SQL users, strong typing.</td>\n<td align=\"left\">Schema migrations needed for new tags, less efficient for sparse metadata, harder to compress.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Simple Key-Value</strong></td>\n<td align=\"left\">Very simple to implement.</td>\n<td align=\"left\">No structured querying on metadata, inefficient for multi-dimensional filtering.</td>\n<td align=\"left\">No</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Using High-Cardinality Tags</strong></p>\n<ul>\n<li><strong>Mistake:</strong> Using a tag with a vast number of unique values, such as <code>request_id</code> or <code>user_id</code>, in a high-volume system. This creates a unique series for every value, exploding the size of the series index and the number of series on disk.</li>\n<li><strong>Why it&#39;s Wrong:</strong> It destroys write and query performance. The series index becomes too large to fit in memory, and every write must touch a different series, preventing efficient batching and compression. A single query might need to open thousands of series files.</li>\n<li><strong>How to Avoid:</strong> High-cardinality identifiers should be stored as <strong>field values</strong>, not tags. Use tags for <em>groupable</em> dimensions like <code>host</code>, <code>region</code>, <code>service</code>. If you must query by a high-cardinality ID, consider a separate indexing system or pre-filter your data.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Sorting Tags for the Series Key</strong></p>\n<ul>\n<li><strong>Mistake:</strong> When constructing the string key for the <code>seriesIndex</code> map (e.g., by concatenating measurement and tags), not first sorting the tag keys (and optionally values).</li>\n<li><strong>Why it&#39;s Wrong:</strong> The series <code>{host=a, region=b}</code> is semantically identical to <code>{region=b, host=a}</code>, but without sorting, they produce different string keys (<code>cpu_usage,host=a,region=b</code> vs <code>cpu_usage,region=b,host=a</code>). This results in the same logical series being stored twice, corrupting data and queries.</li>\n<li><strong>How to Avoid:</strong> Always sort tag keys (and be consistent about values) before serializing a <code>SeriesKey</code> for use as a map key or for storage. Implement a canonical form.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Storing Non-UTC Timestamps</strong></p>\n<ul>\n<li><strong>Mistake:</strong> Accepting timestamps with local timezones or storing them without converting to a canonical format (like UTC nanoseconds since epoch).</li>\n<li><strong>Why it&#39;s Wrong:</strong> Leads to confusion during queries, incorrect results when comparing times, and sorting issues. Daylight saving time transitions can cause duplicate or missing hours.</li>\n<li><strong>How to Avoid:</strong> In the <code>DataPoint</code> type, use <code>time.Time</code> (which internally stores UTC). In APIs, require timestamps to be specified in UTC or as a Unix epoch (nanoseconds), and convert immediately to UTC on ingestion.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete Go code to implement the foundational data model types and their associated helper logic.</p>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option</th>\n<th align=\"left\">Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Timestamp Storage</strong></td>\n<td align=\"left\"><code>int64</code> nanoseconds (Unix epoch)</td>\n<td align=\"left\"><code>time.Time</code> struct (wraps <code>int64</code>, more expressive)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Series Key Hashing</strong></td>\n<td align=\"left\">Sorted string concatenation + <code>string</code> map key</td>\n<td align=\"left\">Sorted string concatenation + <code>xxhash</code> for <code>uint64</code> map key</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Tag Map Sorting</strong></td>\n<td align=\"left\"><code>sort.Strings</code> on keys</td>\n<td align=\"left\">Use a <code>Pair</code> slice and custom sort, or a <code>SortedMap</code> wrapper</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<p>Place the core data model types in an <code>internal/models</code> package to separate them from the business logic of components like storage or querying.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   └── server/\n│       └── main.go\n├── internal/\n│   ├── models/           # ← Core data model types live here\n│   │   ├── point.go\n│   │   ├── series.go\n│   │   ├── query.go\n│   │   └── time_range.go\n│   ├── storage/\n│   ├── query/\n│   └── wal/\n└── go.mod</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Here is a complete, reusable implementation for the <code>TimeRange</code> type and a helper function to canonicalize a <code>SeriesKey</code>. This is boilerplate that can be copied directly.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/models/time_range.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange represents an inclusive-exclusive time interval [Start, End).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tStart </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tEnd   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Contains checks if a given timestamp t is within the range [Start, End).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">t.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(tr.Start) </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> t.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(tr.End)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Overlaps checks if this TimeRange overlaps with another.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Overlaps</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">other</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> tr.Start.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(other.End) </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> other.Start.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(tr.End)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Duration returns the length of the time range as a time.Duration.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> tr.End.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(tr.Start)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/models/series.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesKey is a unique identifier for a time series.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SeriesKey</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tMeasurement </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTags        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String returns a canonical string representation of the SeriesKey.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Tags are sorted by key to ensure a consistent output.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sk </span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tvar</span><span style=\"color:#E1E4E8\"> b </span><span style=\"color:#B392F0\">strings</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Builder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tb.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(sk.Measurement)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(sk.Tags) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tkeys </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(sk.Tags))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tfor</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> sk.Tags {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tkeys </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(keys, k)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tsort.</span><span style=\"color:#B392F0\">Strings</span><span style=\"color:#E1E4E8\">(keys)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tfor</span><span style=\"color:#E1E4E8\"> _, k </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> keys {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tb.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\",\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tb.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(k)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tb.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tb.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(sk.Tags[k])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> b.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewSeriesKey creates a SeriesKey and ensures its tags map is initialized.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSeriesKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">measurement</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tags</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> tags </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\ttags </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#B392F0\"> SeriesKey</span><span style=\"color:#E1E4E8\">{Measurement: measurement, Tags: tags}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>Below is the skeleton for the primary data types and a key function for parsing a data point from the line protocol (a common ingestion format). The learner should fill in the implementation.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/models/point.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DataPoint represents a single observation in a time series.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DataPoint</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTimestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tValue     </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Series represents an in-memory collection of points for a single series.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Series</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tKey    </span><span style=\"color:#B392F0\">SeriesKey</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tPoints []</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#6A737D\"> // Invariant: Points are sorted by Timestamp ascending.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewSeries creates a new Series with the given key.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#B392F0\"> SeriesKey</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Series</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Series</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tKey:    key,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tPoints: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InsertPoint inserts a DataPoint into the Series while maintaining sorted order.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It assumes points are generally inserted in chronological order (append).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// For out-of-order inserts, a more complex merge is needed.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Series</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InsertPoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">p</span><span style=\"color:#B392F0\"> DataPoint</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Handle the common case: if the new point's timestamp is after the last point's timestamp, simply append.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Otherwise, find the correct insertion index to maintain sorted order.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Insert the point at that index using slice operations (e.g., append and copy).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Use `len(s.Points) == 0` as a special case.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: For out-of-order inserts, consider binary search.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/models/query.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AggregateFunction represents the type of aggregation to perform.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AggregateFunction</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tAggregateSum</span><span style=\"color:#B392F0\"> AggregateFunction</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tAggregateAvg</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tAggregateMin</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tAggregateMax</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tAggregateCount</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String returns a string representation of the aggregate function.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">af </span><span style=\"color:#B392F0\">AggregateFunction</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tswitch</span><span style=\"color:#E1E4E8\"> af {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tcase</span><span style=\"color:#E1E4E8\"> AggregateSum:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#9ECBFF\"> \"sum\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tcase</span><span style=\"color:#E1E4E8\"> AggregateAvg:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#9ECBFF\"> \"avg\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tcase</span><span style=\"color:#E1E4E8\"> AggregateMin:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#9ECBFF\"> \"min\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tcase</span><span style=\"color:#E1E4E8\"> AggregateMax:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#9ECBFF\"> \"max\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tcase</span><span style=\"color:#E1E4E8\"> AggregateCount:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#9ECBFF\"> \"count\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefault</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Query represents a request for data.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Query</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tMeasurement   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTags          </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Tag filters (equality only in v1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTimeRange     </span><span style=\"color:#B392F0\">TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tAggregate     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AggregateFunction</span><span style=\"color:#6A737D\"> // nil for raw data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tGroupByWindow </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\">      // 0 for no grouping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tFields        []</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">           // Reserved for future multi-field support</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li><strong>Time Representation:</strong> Use <code>time.Time</code> for clarity in user-facing structs like <code>DataPoint</code>. Internally, for compression and storage, you will likely convert it to an <code>int64</code> (nanoseconds since Unix epoch) using <code>t.UnixNano()</code>.</li>\n<li><strong>Map Sorting:</strong> Go&#39;s <code>map</code> iteration order is non-deterministic. Always use <code>sort.Strings</code> on a slice of keys when you need a canonical representation (e.g., for the <code>SeriesKey.String()</code> method or when writing to disk).</li>\n<li><strong>Point Sorting:</strong> The <code>Series.Points</code> slice must remain sorted. The <code>InsertPoint</code> method should efficiently handle both in-order appends (the common case) and out-of-order inserts (which may require a binary search and slice insertion).</li>\n<li><strong>Constants as Iota:</strong> Define the <code>AggregateFunction</code> as an <code>iota</code> enumeration. This is type-safe and efficient.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>To verify your data model implementation, write a simple test.</p>\n<ol>\n<li><strong>Command:</strong> <code>go test ./internal/models/... -v</code></li>\n<li><strong>Expected Behavior:</strong> Tests should pass, demonstrating:<ul>\n<li><code>SeriesKey.String()</code> produces the same output for <code>{host=a, region=b}</code> and <code>{region=b, host=a}</code>.</li>\n<li><code>TimeRange.Contains()</code> correctly identifies points inside and outside the range.</li>\n<li><code>Series.InsertPoint()</code> maintains chronological order for both in-order and out-of-order inserts.</li>\n</ul>\n</li>\n<li><strong>Sample Test Output:</strong></li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>    === RUN   TestSeriesKeyCanonical\n    --- PASS: TestSeriesKeyCanonical (0.00s)\n    === RUN   TestTimeRangeContains\n    --- PASS: TestTimeRangeContains (0.00s)\n    === RUN   TestSeriesInsertPointInOrder\n    --- PASS: TestSeriesInsertPointInOrder (0.00s)\n    === RUN   TestSeriesInsertPointOutOfOrder\n    --- PASS: TestSeriesInsertPointOutOfOrder (0.00s)\n    PASS</code></pre></div>\n\n<p><strong>G. Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Symptom</th>\n<th align=\"left\">Likely Cause</th>\n<th align=\"left\">How to Diagnose</th>\n<th align=\"left\">Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Queries return duplicate data points for the same series.</strong></td>\n<td align=\"left\">Tags are not being canonicalized before being used as a map key, creating two entries for the same logical series.</td>\n<td align=\"left\">Log the string key used for the <code>seriesIndex</code> map for two writes with the same tags in different order. They will differ.</td>\n<td align=\"left\">Ensure <code>SeriesKey.String()</code> sorts tag keys (and values if needed) before concatenation.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Data points appear in the wrong time order in query results.</strong></td>\n<td align=\"left\">The <code>Series.Points</code> slice is not sorted, or <code>InsertPoint</code> logic is flawed.</td>\n<td align=\"left\">Write a unit test that inserts points with shuffled timestamps and check the final slice order.</td>\n<td align=\"left\">Implement and verify the <code>InsertPoint</code> logic, handling both append and binary search insertion paths.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>&quot;Out of memory&quot; errors when loading the series index.</strong></td>\n<td align=\"left\">High-cardinality tags are being used, creating millions of unique series keys.</td>\n<td align=\"left\">Instrument the <code>StorageEngine</code> to log the count of unique series keys. If it grows linearly with write volume, investigate the tag structure.</td>\n<td align=\"left\">Guide users to move high-cardinality identifiers to field values, not tags. Consider implementing soft limits on series creation.</td>\n</tr>\n</tbody></table>\n<h2 id=\"storage-engine-design\">Storage Engine Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1: Storage Engine</p>\n</blockquote>\n<p>The storage engine is the heart of TempoDB, responsible for efficiently persisting time-series data to disk while enabling fast range queries. Unlike traditional row-oriented databases that store entire records together, our Time-Structured Merge (TSM) tree engine employs a columnar layout that groups timestamps and values separately, optimizing for the sequential and append-heavy nature of time-series data.</p>\n<h3 id=\"mental-model-the-time-indexed-filing-cabinet\">Mental Model: The Time-Indexed Filing Cabinet</h3>\n<p>Imagine a massive filing cabinet organizing documents by time period. Each drawer represents a year, each folder within represents a month, and each document within a folder contains data for a specific day. Within each document, instead of narrative paragraphs, you have two columns: one listing every timestamp (when something happened) and another listing the corresponding measurement values (what happened). This is our TSM engine.</p>\n<p>The filing clerk (storage engine) follows strict rules:</p>\n<ol>\n<li><strong>Documents are immutable</strong>: Once filed, a document is never modified. Updates create new documents.</li>\n<li><strong>Documents are self-describing</strong>: Each document has a cover page listing exactly what time range it covers and where to find specific series inside.</li>\n<li><strong>Documents are consolidated</strong>: Periodically, the clerk merges several small documents into larger, better-organized ones (compaction).</li>\n<li><strong>Old documents are archived</strong>: After a certain time, documents move to deeper storage or are destroyed (retention).</li>\n</ol>\n<p>This mental model explains why TSM files are append-only, why they contain columnar data blocks, and how queries can quickly locate relevant data by checking the &quot;cover page&quot; (index) rather than scanning every document.</p>\n<h3 id=\"tsm-file-format-and-block-layout\">TSM File Format and Block Layout</h3>\n<p>A TSM (Time-Structured Merge) file is the immutable on-disk representation of time-series data for a specific time window. Each file contains data for multiple series, with each series&#39; data organized into compressed blocks of timestamps and values.</p>\n<h4 id=\"file-structure\">File Structure</h4>\n<p>The TSM file format follows this layout:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Size</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Header</strong></td>\n<td>4+8 bytes</td>\n<td>Magic number (<code>0x16D1D1A5</code>) + file version (uint64)</td>\n</tr>\n<tr>\n<td><strong>Series Block 1</strong></td>\n<td>Variable</td>\n<td>Series key + one or more data blocks for this series</td>\n</tr>\n<tr>\n<td><strong>Series Block 2</strong></td>\n<td>Variable</td>\n<td>Series key + data blocks for another series</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>Additional series blocks</td>\n</tr>\n<tr>\n<td><strong>Index</strong></td>\n<td>Variable</td>\n<td>Map from series keys to block metadata (offsets, time ranges)</td>\n</tr>\n<tr>\n<td><strong>Footer</strong></td>\n<td>8 bytes</td>\n<td>Offset to the start of the index (uint64)</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/time-series-db/architecture-doc/asset?path=diagrams%2Ftsm-file-layout.svg\" alt=\"TSM File Internal Layout\"></p>\n<h4 id=\"data-block-structure\">Data Block Structure</h4>\n<p>Within each series block, data is organized into one or more <strong>data blocks</strong>. Each data block contains:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Block Header</strong></td>\n<td>16 bytes: Min timestamp (uint64), Max timestamp (uint64)</td>\n</tr>\n<tr>\n<td><strong>Compressed Timestamps</strong></td>\n<td>Delta-of-delta encoded and compressed byte array</td>\n</tr>\n<tr>\n<td><strong>Compressed Values</strong></td>\n<td>Gorilla XOR compressed float64 values (or other encoding for future types)</td>\n</tr>\n<tr>\n<td><strong>Checksum</strong></td>\n<td>4 bytes: CRC32 of the entire block (header + compressed data)</td>\n</tr>\n</tbody></table>\n<h4 id=\"index-structure\">Index Structure</h4>\n<p>The index maps each series key to the location of its data blocks within the file:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Series Key</strong></td>\n<td>String</td>\n<td>Canonical string representation (measurement + tags)</td>\n</tr>\n<tr>\n<td><strong>Block Entry Count</strong></td>\n<td>uint32</td>\n<td>Number of data blocks for this series in the file</td>\n</tr>\n<tr>\n<td><strong>Block Entries</strong></td>\n<td>Array of:</td>\n<td></td>\n</tr>\n<tr>\n<td>- Min Time</td>\n<td>uint64</td>\n<td>Minimum timestamp in the block</td>\n</tr>\n<tr>\n<td>- Max Time</td>\n<td>uint64</td>\n<td>Maximum timestamp in the block</td>\n</tr>\n<tr>\n<td>- Offset</td>\n<td>uint64</td>\n<td>Byte offset from file start to block header</td>\n</tr>\n<tr>\n<td>- Size</td>\n<td>uint32</td>\n<td>Size of the entire block (header + data + checksum)</td>\n</tr>\n</tbody></table>\n<h4 id=\"key-data-structures\">Key Data Structures</h4>\n<p>The following types implement the TSM file format:</p>\n<table>\n<thead>\n<tr>\n<th>Type Name</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TSMFile</code></td>\n<td><code>path string</code><br><code>file *os.File</code><br><code>mmap []byte</code><br><code>index TSMIndex</code><br><code>mu sync.RWMutex</code></td>\n<td>Represents an open TSM file with memory-mapped access</td>\n</tr>\n<tr>\n<td><code>TSMIndex</code></td>\n<td><code>entries map[string][]IndexEntry</code></td>\n<td>In-memory index mapping series keys to block metadata</td>\n</tr>\n<tr>\n<td><code>IndexEntry</code></td>\n<td><code>MinTime uint64</code><br><code>MaxTime uint64</code><br><code>Offset uint64</code><br><code>Size uint32</code></td>\n<td>Metadata for a single data block</td>\n</tr>\n<tr>\n<td><code>BlockHeader</code></td>\n<td><code>MinTime uint64</code><br><code>MaxTime uint64</code></td>\n<td>Header for a data block</td>\n</tr>\n<tr>\n<td><code>CompressedBlock</code></td>\n<td><code>Timestamps []byte</code><br><code>Values []byte</code><br><code>Checksum uint32</code></td>\n<td>Compressed timestamps and values with integrity check</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-creation-process\">File Creation Process</h4>\n<p>When creating a TSM file from a memtable flush:</p>\n<ol>\n<li><strong>Sort and Group</strong>: Group <code>DataPoint</code> values by <code>SeriesKey</code>, sort each series by timestamp</li>\n<li><strong>Create Blocks</strong>: For each series, partition points into blocks based on:<ul>\n<li>Maximum block size (e.g., 1024 points)</li>\n<li>Time range limit (e.g., 1 hour of data per block)</li>\n</ul>\n</li>\n<li><strong>Compress Each Block</strong>:<ul>\n<li>Apply delta-of-delta encoding to timestamps</li>\n<li>Apply Gorilla XOR compression to float64 values</li>\n<li>Calculate CRC32 checksum</li>\n</ul>\n</li>\n<li><strong>Write Series Blocks</strong>: For each series:<ul>\n<li>Write series key (length-prefixed string)</li>\n<li>For each data block: write <code>BlockHeader</code>, compressed timestamps, compressed values, checksum</li>\n</ul>\n</li>\n<li><strong>Build Index</strong>: Record offset and time range for each block</li>\n<li><strong>Write Index and Footer</strong>: Append index to end of file, then write footer with index offset</li>\n<li><strong>Finalize</strong>: Sync to disk, close file, update file list in storage engine</li>\n</ol>\n<h3 id=\"adr-choosing-compression-algorithms\">ADR: Choosing Compression Algorithms</h3>\n<blockquote>\n<p><strong>Decision: Delta-of-Delta for Timestamps, Gorilla XOR for Values</strong></p>\n<ul>\n<li><strong>Context</strong>: Time-series data exhibits strong temporal patterns: timestamps arrive in monotonic (usually regular) intervals, and consecutive float values often change slowly. We need compression that exploits these patterns without sacrificing query performance.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>No compression</strong>: Store raw int64 timestamps and float64 values</li>\n<li><strong>Simple delta encoding</strong>: Store difference from previous value</li>\n<li><strong>Delta-of-delta encoding</strong>: Store difference between consecutive deltas</li>\n<li><strong>Snappy/GZIP block compression</strong>: Compress entire blocks with general-purpose algorithms</li>\n<li><strong>Gorilla XOR for floats</strong>: XOR current value with previous, encode leading/trailing zeros</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use <strong>delta-of-delta encoding for timestamps</strong> and <strong>Gorilla XOR compression for float64 values</strong>.</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Timestamps</strong>: When data arrives at regular intervals (e.g., every second), delta-of-delta yields zeros or small constants that compress to few bits. Even with irregular intervals, it typically produces smaller deltas than simple delta encoding.</li>\n<li><strong>Values</strong>: Gorilla XOR exploits the property that consecutive float values in metrics often change little (CPU usage, temperature). XOR-ing with previous value yields many leading/trailing zeros in the mantissa, which can be efficiently encoded.</li>\n<li><strong>Query performance</strong>: Both algorithms allow random access within a block without decompressing the entire block—critical for range queries that may start in the middle of a block.</li>\n<li><strong>Proven effectiveness</strong>: Facebook&#39;s Gorilla paper demonstrated 10x compression for timestamps and 2x for values in production metrics.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Significant storage reduction (typically 70-90% for timestamps, 50% for values)</li>\n<li><strong>Positive</strong>: Faster I/O due to less data read from disk</li>\n<li><strong>Negative</strong>: CPU overhead during compression/decompression</li>\n<li><strong>Negative</strong>: Compression effectiveness depends on data regularity (irregular timestamps or highly volatile values compress poorly)</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>No compression</strong></td>\n<td>Zero CPU overhead, simplest implementation</td>\n<td>Wastes storage (8 bytes per timestamp + 8 bytes per value), slow I/O</td>\n<td>Storage efficiency is primary goal</td>\n</tr>\n<tr>\n<td><strong>Simple delta encoding</strong></td>\n<td>Better than raw for regular intervals, simple to implement</td>\n<td>Less effective than delta-of-delta for regular intervals</td>\n<td>Delta-of-delta is strictly better for regular data</td>\n</tr>\n<tr>\n<td><strong>Delta-of-delta encoding</strong></td>\n<td>Excellent for regular intervals, allows partial decompression</td>\n<td>Slightly more complex, poor for completely irregular timestamps</td>\n<td><strong>CHOSEN</strong>: Best trade-off for typical time-series</td>\n</tr>\n<tr>\n<td><strong>Snappy/GZIP block compression</strong></td>\n<td>Good compression for any data, widely available</td>\n<td>Requires decompressing entire block for any access, higher CPU</td>\n<td>Kills random access performance</td>\n</tr>\n<tr>\n<td><strong>Gorilla XOR for floats</strong></td>\n<td>Excellent for slowly-changing values, allows partial decompression</td>\n<td>Only for floats, requires maintaining previous value state</td>\n<td><strong>CHOSEN</strong>: Best for metric-like data</td>\n</tr>\n</tbody></table>\n<h4 id=\"compression-algorithm-details\">Compression Algorithm Details</h4>\n<p><strong>Delta-of-Delta Timestamp Encoding</strong>:</p>\n<ol>\n<li>Store first timestamp as raw int64 (Unix nanoseconds)</li>\n<li>Store first delta as difference from previous (int64)</li>\n<li>For subsequent timestamps: <ul>\n<li>Calculate delta = current_timestamp - previous_timestamp</li>\n<li>Calculate delta-of-delta = delta - previous_delta</li>\n<li>Encode delta-of-delta using variable-length integer encoding (VarInt)</li>\n</ul>\n</li>\n</ol>\n<p><strong>Gorilla XOR Float Compression</strong>:</p>\n<ol>\n<li>Store first float64 value as raw bits</li>\n<li>For subsequent values:<ul>\n<li>XOR current value bits with previous value bits</li>\n<li>If XOR == 0: store single &#39;0&#39; bit</li>\n<li>Else:<ul>\n<li>Calculate leading zeros = count of leading zero bits in XOR</li>\n<li>Calculate trailing zeros = count of trailing zero bits in XOR</li>\n<li>Store &#39;1&#39; bit followed by:<ul>\n<li>5 bits encoding leading zeros count</li>\n<li>6 bits encoding (64 - leading_zeros - trailing_zeros) = meaningful bits length</li>\n<li>The meaningful bits themselves (excluding leading/trailing zeros)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"block-size-considerations\">Block Size Considerations</h4>\n<p>Choosing the right block size involves trade-offs:</p>\n<table>\n<thead>\n<tr>\n<th>Block Size</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Small (≤ 100 points)</strong></td>\n<td>Fast decompression, fine-grained skipping</td>\n<td>Poor compression ratio, more index entries</td>\n<td>Avoid: overhead dominates</td>\n</tr>\n<tr>\n<td><strong>Medium (1,024 points)</strong></td>\n<td>Good compression, reasonable decompression cost</td>\n<td>May need to decompress unneeded data</td>\n<td><strong>Default choice</strong></td>\n</tr>\n<tr>\n<td><strong>Large (10,000 points)</strong></td>\n<td>Excellent compression, fewer index entries</td>\n<td>Expensive to decompress for small queries</td>\n<td>Use for cold/historical data</td>\n</tr>\n</tbody></table>\n<p>We choose <strong>1,024 points per block</strong> as the default, aligning with filesystem page sizes (typically 4KB) and providing good compression while keeping decompression costs manageable.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Not aligning block sizes with filesystem page sizes</strong></p>\n<ul>\n<li><strong>Description</strong>: Creating blocks that straddle page boundaries (e.g., 1500-byte blocks when pages are 4096 bytes) causes read amplification.</li>\n<li><strong>Why it&#39;s wrong</strong>: The filesystem may read two pages from disk to access one block, doubling I/O.</li>\n<li><strong>How to fix</strong>: Round block sizes up to multiples of 4096 bytes, or ensure block headers start on page boundaries.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Forgetting to handle clock skew in timestamp encoding</strong></p>\n<ul>\n<li><strong>Description</strong>: Assuming timestamps always increase, but in distributed systems, clocks may drift or jump.</li>\n<li><strong>Why it&#39;s wrong</strong>: Delta-of-delta produces large values for negative deltas, hurting compression. Decoder may misinterpret.</li>\n<li><strong>How to fix</strong>: Include a flag in block header for &quot;unordered timestamps&quot; that disables compression for that block, or reset compression state when timestamp goes backwards.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Over-compressing hot data that needs frequent access</strong></p>\n<ul>\n<li><strong>Description</strong>: Applying aggressive compression to frequently queried recent data.</li>\n<li><strong>Why it&#39;s wrong</strong>: Compression/decompression CPU cost outweighs I/O savings for hot data.</li>\n<li><strong>How to fix</strong>: Implement tiered compression: no compression for L0 (hot), delta encoding for L1 (warm), full Gorilla for L2+ (cold).</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not leaving buffer space for block metadata</strong></p>\n<ul>\n<li><strong>Description</strong>: Allocating exact size for compressed data without room for headers and checksums.</li>\n<li><strong>Why it&#39;s wrong</strong>: Need to copy and reallocate when adding metadata, wasting CPU.</li>\n<li><strong>How to fix</strong>: Pre-allocate buffer with extra capacity (e.g., data size + 256 bytes) for headers and checksums.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Choosing fixed compression without profiling actual data patterns</strong></p>\n<ul>\n<li><strong>Description</strong>: Always using Gorilla XOR even for highly volatile data (e.g., cryptographic hashes as floats).</li>\n<li><strong>Why it&#39;s wrong</strong>: XOR compression expands data rather than compressing it.</li>\n<li><strong>How to fix</strong>: Implement compression selector that samples data, chooses best algorithm, stores algorithm ID in block header.</li>\n</ul>\n<h3 id=\"implementation-guidance-milestone-1\">Implementation Guidance (Milestone 1)</h3>\n<p>This section provides concrete implementation guidance for the TSM storage engine in Go.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>File I/O</strong></td>\n<td><code>os.File</code> with manual buffering</td>\n<td><strong>Memory-mapped files (<code>mmap</code>)</strong> for zero-copy reads</td>\n</tr>\n<tr>\n<td><strong>Compression</strong></td>\n<td>Implement delta-of-delta and Gorilla from scratch</td>\n<td>Use existing libraries but understand algorithms</td>\n</tr>\n<tr>\n<td><strong>Checksums</strong></td>\n<td><strong>CRC32</strong> using <code>hash/crc32</code></td>\n<td>More robust hash but CRC32 is sufficient</td>\n</tr>\n<tr>\n<td><strong>Concurrent Access</strong></td>\n<td><code>sync.RWMutex</code> per file</td>\n<td>Lock-free reads with atomic pointers</td>\n</tr>\n<tr>\n<td><strong>Block Cache</strong></td>\n<td>Simple <code>map</code> with LRU eviction</td>\n<td><strong><code>sync.Map</code> or specialized cache</strong> with size limits</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   └── server/\n│       └── main.go                 # Entry point\n├── internal/\n│   ├── storage/\n│   │   ├── engine.go              # StorageEngine implementation\n│   │   ├── tsm/\n│   │   │   ├── writer.go          # TSM file creation\n│   │   │   ├── reader.go          # TSM file reading\n│   │   │   ├── compression.go     # Delta-of-delta, Gorilla\n│   │   │   ├── file.go            # TSMFile struct and methods\n│   │   │   └── index.go           # TSMIndex and IndexEntry\n│   │   ├── block_cache.go         # LRU block cache\n│   │   └── memtable.go            # Memtable (for Milestone 2)\n│   ├── models/\n│   │   ├── datapoint.go           # DataPoint struct\n│   │   ├── series.go              # SeriesKey, Series\n│   │   └── query.go               # Query, TimeRange\n│   └── wal/                       # Write-ahead log (Milestone 2)\n│       └── wal.go\n└── pkg/\n    └── mmap/                      # Memory-mapping utilities\n        └── mmap.go</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Memory-mapped file utility (<code>pkg/mmap/mmap.go</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> mmap</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">syscall</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">unsafe</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Map memory-maps a file for read-only access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> Map</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Open</span><span style=\"color:#E1E4E8\">(path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stat, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Stat</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> stat.</span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Handle empty files</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> size </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Memory map the file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> syscall.</span><span style=\"color:#B392F0\">Mmap</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">(file.</span><span style=\"color:#B392F0\">Fd</span><span style=\"color:#E1E4E8\">()), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">(size), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        syscall.PROT_READ, syscall.MAP_SHARED)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> data, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Unmap unmaps a memory-mapped file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> Unmap</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(data) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> syscall.</span><span style=\"color:#B392F0\">Munmap</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReadUint64 reads a little-endian uint64 from mapped memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ReadUint64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use unsafe for performance; ensure data is long enough</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*uint64</span><span style=\"color:#E1E4E8\">)(unsafe.</span><span style=\"color:#B392F0\">Pointer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">data[offset]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReadUint32 reads a little-endian uint32 from mapped memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ReadUint32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*uint32</span><span style=\"color:#E1E4E8\">)(unsafe.</span><span style=\"color:#B392F0\">Pointer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">data[offset]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>TSM file constants and header (<code>internal/storage/tsm/file.go</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> tsm</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // MagicNumber identifies a TSM file</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MagicNumber</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#F97583\"> =</span><span style=\"color:#F97583\"> 0x</span><span style=\"color:#79B8FF\">16D1D1A5</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Version is the current TSM file format version</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    Version</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // BlockHeaderSize is the size of a block header in bytes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BlockHeaderSize</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#6A737D\"> // 2 x uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // IndexEntrySize is the size of a single index entry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IndexEntrySize</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\"> // min, max, offset, size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // DefaultMaxPointsPerBlock is the default number of points per block</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DefaultMaxPointsPerBlock</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1024</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TSMHeader represents the file header</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TSMHeader</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Magic   </span><span style=\"color:#F97583\">uint32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteHeader writes the TSM header to a writer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> WriteHeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Writer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    header </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> TSMHeader</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Magic:   MagicNumber,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Version: Version,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(w, binary.LittleEndian, header)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReadHeader reads and validates a TSM header</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ReadHeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">TSMHeader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> header </span><span style=\"color:#B392F0\">TSMHeader</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(r, binary.LittleEndian, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">header.Magic); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> header, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(r, binary.LittleEndian, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">header.Version); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> header, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> header.Magic </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> MagicNumber {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> header, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid magic number: </span><span style=\"color:#79B8FF\">%x</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, header.Magic)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> header.Version </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> Version {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> header, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unsupported version: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, header.Version)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> header, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>TSM Writer (<code>internal/storage/tsm/writer.go</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> tsm</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">hash/crc32</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TSMWriter creates TSM files from series data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TSMWriter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buf       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">IndexEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    blockSize </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTSMWriter creates a new TSM writer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTSMWriter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">blockSize</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TSMWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Create</span><span style=\"color:#E1E4E8\">(path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    writer </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TSMWriter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file:      file,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buf:       bytes.</span><span style=\"color:#B392F0\">NewBuffer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        index:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">IndexEntry</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        blockSize: blockSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Write TSM file header (magic + version)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use WriteHeader function from file.go</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> writer, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteSeries writes a series to the TSM file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TSMWriter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">points</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Sort points by timestamp (ensure ascending order)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Partition points into blocks of max blockSize points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For each partition:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Create a block with timestamps and values separated</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Apply delta-of-delta compression to timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Apply Gorilla XOR compression to values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Calculate CRC32 checksum of the compressed block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Write block header (min/max timestamps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Write compressed timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Write compressed values  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Write checksum</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Record index entry with min/max time, offset, size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write series key (length-prefixed string)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Write all blocks for this series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Finish writes the index and footer, then closes the file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TSMWriter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Finish</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Write index section:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For each series in w.index:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * Write series key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * Write count of blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * For each block: write min/max time, offset, size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Write footer: offset to start of index (uint64)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Sync file to disk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// compressBlock compresses timestamps and values for a block</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> compressBlock</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timestamps</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">values</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Implement delta-of-delta compression for timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - First timestamp stored as-is (uint64)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - First delta = timestamp[1] - timestamp[0]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For i > 1: delta = timestamp[i] - timestamp[i-1]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //                deltaOfDelta = delta - previousDelta</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //                Encode deltaOfDelta as VarInt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Implement Gorilla XOR compression for float64 values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - First value stored as raw bits (uint64)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For each subsequent value:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * XOR with previous value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * If XOR == 0: store single 0 bit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * Else: store 1 bit + leading zeros + meaningful bits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>TSM Reader (<code>internal/storage/tsm/reader.go</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> tsm</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/pkg/mmap</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TSMReader reads TSM files using memory-mapped I/O</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TSMReader</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#6A737D\">  // Memory-mapped file data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index </span><span style=\"color:#B392F0\">TSMIndex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OpenTSMReader opens and memory-maps a TSM file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> OpenTSMReader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TSMReader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Memory-map the file using mmap.Map()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Read and validate header from beginning of data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Read footer (last 8 bytes) to get index offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse index section into TSMIndex structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Iterate through index entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Build map[string][]IndexEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TSMReader</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        path:  path,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data:  data,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        index: index,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReadBlock reads and decompresses a specific block</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TSMReader</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ReadBlock</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seriesKey</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">blockIndex</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Look up series in index, get block metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Validate blockIndex is within bounds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Read block from offset in memory-mapped data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Read header (min/max timestamps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Read compressed timestamps and values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Verify CRC32 checksum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Decompress timestamps (delta-of-delta decoding)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Decompress values (Gorilla XOR decoding)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Reconstruct DataPoint slice from timestamps and values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange returns the minimum and maximum timestamps in the file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TSMReader</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 11: Iterate through all index entries to find global min/max</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return 0,0 if no data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close unmaps the memory and releases resources</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TSMReader</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 12: Unmap memory using mmap.Unmap()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// decompressTimestamps decodes delta-of-delta compressed timestamps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> decompressTimestamps</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 13: Read first timestamp (uint64)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 14: Read first delta (VarInt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 15: For remaining points:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Read delta-of-delta (VarInt)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Reconstruct delta = previousDelta + deltaOfDelta</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Reconstruct timestamp = previousTimestamp + delta</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// decompressValues decodes Gorilla XOR compressed float64 values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> decompressValues</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 16: Implement Gorilla XOR decompression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Read first value as raw bits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For each subsequent value:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * Read control bit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * If 0: value = previous value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     * If 1: read leading zeros, meaningful bits length, and bits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //            Reconstruct XOR value, then XOR with previous to get current</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ol>\n<li><strong>Memory-mapping</strong>: Use <code>syscall.Mmap</code> directly for control, but ensure proper error handling and <code>Munmap</code> on close.</li>\n<li><strong>VarInt encoding</strong>: Implement using loops shifting 7 bits at a time. The standard library&#39;s <code>binary.PutUvarint</code> and <code>binary.Uvarint</code> work but may be slower than custom implementation.</li>\n<li><strong>Bit-level operations</strong>: For Gorilla compression, you&#39;ll need to read/write individual bits. Use a <code>bitReader</code>/<code>bitWriter</code> wrapper around byte slices.</li>\n<li><strong>CRC32</strong>: Use <code>hash/crc32</code> with IEEE polynomial: <code>crc32.ChecksumIEEE(data)</code>.</li>\n<li><strong>Concurrency</strong>: Use <code>sync.RWMutex</code> for <code>TSMReader</code> if it might be accessed concurrently. Better: make <code>TSMReader</code> immutable after creation.</li>\n<li><strong>File I/O</strong>: Always check errors from file operations. Use <code>defer</code> for cleanup.</li>\n</ol>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p><strong>To verify Milestone 1 implementation</strong>:</p>\n<ol>\n<li><strong>Create a test TSM file</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/tsm/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestTSMWriter</span></span></code></pre></div>\n\n<p>Expected output should show:</p>\n<ul>\n<li>TSM file created successfully</li>\n<li>Compression ratio reported (e.g., &quot;Compressed 1024 points to 5120 bytes&quot;)</li>\n<li>No errors during write or read</li>\n</ul>\n<ol start=\"2\">\n<li><strong>Manual verification</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create a simple test program</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">cat</span><span style=\"color:#F97583\"> ></span><span style=\"color:#9ECBFF\"> test_tsm.go</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#9ECBFF\"> 'EOF'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">package main</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"fmt\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"tempo/internal/storage/tsm\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">func main() {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    writer, _ := tsm.NewTSMWriter(\"/tmp/test.tsm\", 1024)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    // Add test data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    writer.Finish()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    reader, _ := tsm.OpenTSMReader(\"/tmp/test.tsm\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    points, _ := reader.ReadBlock(\"cpu\", 0)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    fmt.Printf(\"Read %d points\\n\", len(points))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    reader.Close()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> test_tsm.go</span></span></code></pre></div>\n\n<p>Expected: &quot;Read 1024 points&quot; with correct timestamps and values.</p>\n<ol start=\"3\">\n<li><strong>Check for common issues</strong>:</li>\n</ol>\n<ul>\n<li><strong>Issue</strong>: &quot;invalid magic number&quot; error when reading<ul>\n<li><strong>Fix</strong>: Ensure binary.Write uses <code>binary.LittleEndian</code> consistently</li>\n</ul>\n</li>\n<li><strong>Issue</strong>: CRC32 mismatch when reading blocks<ul>\n<li><strong>Fix</strong>: Verify you&#39;re checksumming the exact bytes written (including header)</li>\n</ul>\n</li>\n<li><strong>Issue</strong>: Gorilla decompression produces NaN values<ul>\n<li><strong>Fix</strong>: Check bit manipulation logic; use <code>math.Float64frombits()</code> for conversion</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Query returns no data for valid time range</strong></td>\n<td>Block index min/max times incorrect</td>\n<td>Add debug logging to show which blocks are being selected</td>\n<td>Ensure min/max in index match actual data in block</td>\n</tr>\n<tr>\n<td><strong>Decompression produces wrong timestamps</strong></td>\n<td>VarInt encoding/decoding bug</td>\n<td>Write unit test with known sequence: [1000, 1001, 1003, 1006]</td>\n<td>Step through decompression with small test case</td>\n</tr>\n<tr>\n<td><strong>Memory usage grows with many TSM files</strong></td>\n<td>Not unmapping closed files</td>\n<td>Add finalizer or explicit Close() calls</td>\n<td>Ensure every OpenTSMReader has matching Close()</td>\n</tr>\n<tr>\n<td><strong>ReadBlock returns fewer points than expected</strong></td>\n<td>Block size miscalculation during write</td>\n<td>Check partition logic in WriteSeries</td>\n<td>Ensure points are sorted before partitioning</td>\n</tr>\n<tr>\n<td><strong>Gorilla compression makes data larger</strong></td>\n<td>Highly volatile values</td>\n<td>Test with constant values first, then random</td>\n<td>Fall back to no compression if XOR doesn&#39;t help</td>\n</tr>\n</tbody></table>\n<h2 id=\"write-path-design\">Write Path Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2: Write Path</p>\n</blockquote>\n<p>The write path is the critical pipeline through which all incoming data flows from ingestion to durable storage. This design must balance three competing requirements: <strong>low-latency acknowledgment</strong> to clients, <strong>strong durability guarantees</strong>, and <strong>efficient batching</strong> for storage optimization. In TempoDB, we achieve this through a multi-stage pipeline that validates, logs, buffers, and finally persists data points in optimized columnar format.</p>\n<h3 id=\"mental-model-the-airport-check-in-and-baggage-system\">Mental Model: The Airport Check-in and Baggage System</h3>\n<p>Imagine an airport&#39;s check-in and baggage handling system. Passengers (data points) arrive at the terminal (the API). Each passenger checks in at a counter (validation) and receives a boarding pass (acknowledgment) once their luggage is registered in the system. The luggage itself is placed on a conveyor belt (the Write-Ahead Log) that immediately moves it to a secure holding area (the memtable). Luggage from many flights accumulates in the holding area until a critical mass for a particular destination is reached. Then, all luggage for that flight is efficiently packed into containers (compressed blocks) and loaded onto the plane (TSM file) for the journey to long-term storage (disk).</p>\n<p>This analogy captures the key principles:</p>\n<ul>\n<li><strong>Immediate acknowledgment</strong>: Passengers receive their boarding pass quickly, without waiting for the plane to be loaded.</li>\n<li><strong>Durability via logging</strong>: The conveyor belt (WAL) ensures luggage isn&#39;t lost if the holding area has an issue.</li>\n<li><strong>Batched efficiency</strong>: Luggage is packed in bulk, not piece-by-piece, optimizing space and effort.</li>\n<li><strong>Temporal organization</strong>: Luggage is sorted by destination (series) and flight time (timestamp).</li>\n</ul>\n<h3 id=\"write-ahead-log-and-memtable\">Write-Ahead Log and Memtable</h3>\n<p>The first two components of the write path work in tandem to provide durability and in-memory buffering.</p>\n<h4 id=\"write-ahead-log-wal\">Write-Ahead Log (WAL)</h4>\n<p>The <strong>Write-Ahead Log</strong> is an append-only file that records every write operation before it&#39;s acknowledged to the client. This ensures that even if the process crashes after acknowledgment but before data reaches persistent storage, the operation can be replayed during recovery. The WAL&#39;s sole purpose is durability—it&#39;s not optimized for random reads.</p>\n<p><strong>Design Decisions:</strong></p>\n<blockquote>\n<p><strong>Decision: WAL Format and Segmentation</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a durable log that can handle high write throughput, support efficient recovery scans, and manage disk space without unbounded growth.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Single monolithic log file</strong>: Append all writes to one ever-growing file.</li>\n<li><strong>Segmented log with rotation</strong>: Split the log into fixed-size segments; close and create a new segment when current segment reaches size limit.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use segmented log with rotation.</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Easier space management</strong>: Old segments can be deleted after their data is flushed to TSM files.</li>\n<li><strong>Parallel recovery</strong>: Multiple segments can be scanned in parallel during recovery.</li>\n<li><strong>Simplified implementation</strong>: Each segment is a simple append-only file with a predictable maximum size.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li>Requires managing multiple files and cleaning up obsolete segments.</li>\n<li>Adds complexity for tracking which segments are still needed (not yet flushed).</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single monolithic file</td>\n<td>Simple to implement; no file management overhead</td>\n<td>Unbounded growth; difficult to clean up old data; recovery scans entire log</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Segmented rotation</td>\n<td>Bounded segment size; easy cleanup; parallel recovery possible</td>\n<td>Must manage multiple files; need to track segment state</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<p>Each WAL segment file contains a sequence of <strong>entries</strong>, where each entry corresponds to one or more data points written in a batch. The segment format is:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Entry Length</td>\n<td>uint32 (4 bytes)</td>\n<td>Length of the entire entry (including this field and checksum)</td>\n</tr>\n<tr>\n<td>Checksum</td>\n<td>uint32 (4 bytes)</td>\n<td>CRC32 checksum of the entry data (for corruption detection)</td>\n</tr>\n<tr>\n<td>Entry ID</td>\n<td>uint64 (8 bytes)</td>\n<td>Monotonically increasing identifier for ordering and deduplication</td>\n</tr>\n<tr>\n<td>Batch Data</td>\n<td>[]byte (variable)</td>\n<td>Serialized batch of data points (using protocol buffers or custom binary format)</td>\n</tr>\n</tbody></table>\n<p>The <code>Segment</code> type manages an individual segment file:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>path</code></td>\n<td><code>string</code></td>\n<td>Filesystem path to the segment file</td>\n</tr>\n<tr>\n<td><code>file</code></td>\n<td><code>*os.File</code></td>\n<td>Open file handle for appending</td>\n</tr>\n<tr>\n<td><code>mu</code></td>\n<td><code>sync.RWMutex</code></td>\n<td>Protects concurrent writes and closes</td>\n</tr>\n<tr>\n<td><code>size</code></td>\n<td><code>int64</code></td>\n<td>Current size of the segment in bytes</td>\n</tr>\n<tr>\n<td><code>maxSize</code></td>\n<td><code>int64</code></td>\n<td>Maximum size before rotation (from <code>SegmentConfig.MaxSizeBytes</code>)</td>\n</tr>\n<tr>\n<td><code>closed</code></td>\n<td><code>bool</code></td>\n<td>Whether the segment is closed for writing</td>\n</tr>\n<tr>\n<td><code>firstID</code></td>\n<td><code>uint64</code></td>\n<td>Entry ID of the first entry in this segment</td>\n</tr>\n<tr>\n<td><code>lastID</code></td>\n<td><code>uint64</code></td>\n<td>Entry ID of the last entry written</td>\n</tr>\n</tbody></table>\n<p><strong>WAL Operations:</strong></p>\n<ol>\n<li><strong>Write Entry</strong>: When a batch of points arrives, serialize them, calculate checksum, append to current segment with monotonic ID.</li>\n<li><strong>Sync Policy</strong>: Configure via <code>SegmentConfig.SyncInterval</code> (periodic sync) or <code>SyncOnWrite</code> (sync every write). Trade-off between durability and performance.</li>\n<li><strong>Segment Rotation</strong>: When <code>size &gt;= maxSize</code>, close current segment, create new one with <code>firstID = lastID + 1</code>.</li>\n<li><strong>Recovery Scan</strong>: On startup, scan all non-flushed segments in ID order, replay entries to rebuild memtable.</li>\n</ol>\n<blockquote>\n<p><strong>Key Insight</strong>: The WAL must be fsynced to disk before acknowledging writes to the client. Without this, a power loss could lose acknowledged writes, violating durability guarantees. In Go, use <code>file.Sync()</code> after writing the entry.</p>\n</blockquote>\n<h4 id=\"memtable\">Memtable</h4>\n<p>The <strong>memtable</strong> (memory table) is an in-memory buffer that holds recently written data points in sorted order, organized by series. It serves two purposes: (1) provides a fast read path for recent data, and (2) batches points for efficient flushing to disk.</p>\n<p><strong>Design Decisions:</strong></p>\n<blockquote>\n<p><strong>Decision: Memtable Data Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: We need an in-memory structure that supports fast inserts and range scans, organized by series key and timestamp.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Sorted map of maps</strong>: <code>map[SeriesKey]map[timestamp]value</code> (unordered timestamps within series).</li>\n<li><strong>Sorted map with slices</strong>: <code>map[SeriesKey][]DataPoint</code> with points kept sorted.</li>\n<li><strong>Skip list per series</strong>: <code>map[SeriesKey]*skiplist</code> for ordered points.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use sorted map with slices (option 2).</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Simplicity</strong>: Slices are native Go types with predictable memory overhead.</li>\n<li><strong>Good for sequential writes</strong>: Time-series data typically arrives in roughly chronological order; appending to slice is O(1).</li>\n<li><strong>Efficient flushing</strong>: When flushing, we can iterate series and timestamps in order without additional sorting.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li>Out-of-order writes require binary search and insertion (O(n) worst-case).</li>\n<li>Must handle slice growth and potential copying.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p>The memtable structure in practice:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Conceptual structure (not actual code in Layer 1)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Memtable</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#6A737D\">  // Series → sorted points</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                      // Approximate memory usage in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu   </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span><span style=\"color:#6A737D\">               // For concurrent access</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Memtable Operations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Insert</code></td>\n<td><code>SeriesKey</code>, <code>DataPoint</code></td>\n<td><code>error</code></td>\n<td>Adds a point to the appropriate series slice, maintaining sorted order by timestamp</td>\n</tr>\n<tr>\n<td><code>GetRange</code></td>\n<td><code>SeriesKey</code>, <code>TimeRange</code></td>\n<td><code>[]DataPoint</code></td>\n<td>Returns all points for a series within time range (binary search)</td>\n</tr>\n<tr>\n<td><code>Size</code></td>\n<td>-</td>\n<td><code>int64</code></td>\n<td>Returns approximate memory usage in bytes</td>\n</tr>\n<tr>\n<td><code>Flush</code></td>\n<td>-</td>\n<td><code>map[SeriesKey][]DataPoint</code></td>\n<td>Returns all data and clears the memtable (becomes immutable)</td>\n</tr>\n</tbody></table>\n<p><strong>Series Cardinality Tracking</strong>: As points are inserted, we track the number of unique series keys (<code>len(memtable.data)</code>). This is critical for monitoring and query planning, as high cardinality (millions of series) can impact performance.</p>\n<h3 id=\"flush-mechanism-and-out-of-order-writes\">Flush Mechanism and Out-of-Order Writes</h3>\n<p>The memtable is a temporary buffer; eventually, its contents must be written to persistent TSM files. This process is called <strong>flushing</strong>.</p>\n<h4 id=\"flush-triggers\">Flush Triggers</h4>\n<p>A flush is triggered by one or more conditions, implemented as a decision flowchart (see <img src=\"/api/project/time-series-db/architecture-doc/asset?path=diagrams%2Fmemtable-flush-flow.svg\" alt=\"Memtable Flush Decision Flowchart\">):</p>\n<ol>\n<li><strong>Size-based</strong>: When the memtable&#39;s estimated memory usage exceeds <code>Config.MaxMemtableSize</code> (e.g., 256 MB).</li>\n<li><strong>Time-based</strong>: A periodic timer (e.g., every 5 minutes) ensures data doesn&#39;t stay in memory too long.</li>\n<li><strong>Manual</strong>: Via administrative command or shutdown hook.</li>\n</ol>\n<p>When a flush is triggered, the current active memtable is <strong>marked as immutable</strong>, and a new empty memtable becomes active for new writes. This allows writes to continue uninterrupted while flushing occurs in the background.</p>\n<h4 id=\"flush-process\">Flush Process</h4>\n<p>The flush process converts an immutable memtable to one or more TSM files:</p>\n<ol>\n<li><strong>Sort and Partition</strong>: Group points by series key, ensuring timestamps are sorted within each series.</li>\n<li><strong>Create TSM Writer</strong>: For each series, write points in batches of <code>DefaultMaxPointsPerBlock</code> (1024) to a new TSM file.</li>\n<li><strong>Apply Compression</strong>: Use delta-of-delta encoding for timestamps and Gorilla XOR for values (as described in Storage Engine Design).</li>\n<li><strong>Write Index</strong>: After all series are written, build and append the index mapping series keys to block locations.</li>\n<li><strong>Persist</strong>: Call <code>Finish()</code> to write footer and sync the TSM file to disk.</li>\n<li><strong>Update Metadata</strong>: Update the storage engine&#39;s file list and series index.</li>\n<li><strong>Cleanup WAL</strong>: Once the TSM file is durably written, mark the corresponding WAL segments as safe to delete (up to the highest entry ID included in the flush).</li>\n</ol>\n<h4 id=\"handling-out-of-order-writes\">Handling Out-of-Order Writes</h4>\n<p>Time-series data may arrive with timestamps that are not monotonically increasing—common in distributed systems with clock skew or batch replays. TempoDB must handle these <strong>out-of-order writes</strong> correctly.</p>\n<p><strong>Strategy: Insertion Sort with Tolerance Window</strong></p>\n<p>For each series, we maintain points sorted by timestamp. When a new point arrives:</p>\n<ol>\n<li>Check if its timestamp is <strong>after the last point</strong> for that series → append (common case, O(1)).</li>\n<li>If before the last point, perform binary search to find insertion position.</li>\n<li>Insert into slice (may require shifting elements, O(n) worst-case).</li>\n</ol>\n<p>To limit the performance impact, we define a <strong>tolerance window</strong> (e.g., 1 hour). Points arriving more than this window before the latest point are rejected as &quot;too old&quot; (configurable). Points within the window are accepted but may incur the insertion cost.</p>\n<blockquote>\n<p><strong>Design Principle</strong>: Optimize for the common case (in-order writes) while supporting bounded out-of-order writes. This balances performance with practical requirements.</p>\n</blockquote>\n<p><strong>Late Arrivals During Flush</strong>: A more complex scenario occurs when a point arrives for a series that is currently being flushed (the memtable is immutable). Two approaches:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Description</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Block and Insert</strong></td>\n<td>Block writes to that series until flush completes, then insert into new memtable</td>\n<td>Simple; ensures correctness</td>\n<td>Blocks writes; poor concurrency</td>\n<td>No</td>\n</tr>\n<tr>\n<td><strong>Write to New Memtable with Tombstone</strong></td>\n<td>Write point to active memtable, mark flushed data with &quot;tombstone&quot; to indicate missing point</td>\n<td>Non-blocking; good performance</td>\n<td>Complex; requires compaction to merge; tombstone overhead</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<p>We choose the second approach: out-of-order points for series being flushed go to the active memtable. The flushed TSM file contains data up to time T. The new memtable contains points with timestamps &lt; T. During query execution, we must merge points from both TSM file and memtable. This is handled during compaction (Milestone 4), which will merge overlapping time ranges.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Not Fsyncing WAL Before Acknowledgment</strong></p>\n<ul>\n<li><strong>Description</strong>: Acknowledging writes to the client before calling <code>fsync()</code> on the WAL.</li>\n<li><strong>Why it&#39;s wrong</strong>: If the OS crashes or loses power, the write may exist only in page cache, not on durable storage. The client believes data is saved, but it&#39;s lost.</li>\n<li><strong>Fix</strong>: Always call <code>file.Sync()</code> (or set <code>SyncOnWrite = true</code>) before sending acknowledgment.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Unbounded Memtable Growth</strong></p>\n<ul>\n<li><strong>Description</strong>: Not enforcing size limits on memtable, allowing it to consume all available memory.</li>\n<li><strong>Why it&#39;s wrong</strong>: Can lead to OOM crashes, especially during write bursts.</li>\n<li><strong>Fix</strong>: Implement strict size-based flushing and backpressure (see below).</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring Clock Skew</strong></p>\n<ul>\n<li><strong>Description</strong>: Assuming all timestamps arrive in order, causing incorrectly sorted data.</li>\n<li><strong>Why it&#39;s wrong</strong>: Queries may return incorrect results (out-of-order points), and compression efficiency suffers.</li>\n<li><strong>Fix</strong>: Implement out-of-order insertion logic with a reasonable tolerance window.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Blocking Writes During Flush</strong></p>\n<ul>\n<li><strong>Description</strong>: Making the write path wait for flush completion.</li>\n<li><strong>Why it&#39;s wrong</strong>: Kills write throughput during heavy load.</li>\n<li><strong>Fix</strong>: Use immutable memtables and background flushing; writes continue to new active memtable.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Handling Backpressure</strong></p>\n<ul>\n<li><strong>Description</strong>: Accepting writes indefinitely even when system is overloaded.</li>\n<li><strong>Why it&#39;s wrong</strong>: Leads to uncontrolled memory growth and eventual crash.</li>\n<li><strong>Fix</strong>: Implement backpressure mechanism (e.g., pause accepting writes when memtable size reaches 90% of limit).</li>\n</ul>\n<h3 id=\"implementation-guidance-milestone-2\">Implementation Guidance (Milestone 2)</h3>\n<p>This section provides concrete implementation steps for the write path components.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>WAL Storage</td>\n<td>Append-only files with <code>os.File</code></td>\n<td>Memory-mapped files for zero-copy reads during recovery</td>\n</tr>\n<tr>\n<td>Memtable Structure</td>\n<td><code>map[string][]DataPoint</code> with binary search insert</td>\n<td>Partitioned memtables per series hash to reduce lock contention</td>\n</tr>\n<tr>\n<td>Concurrency Control</td>\n<td><code>sync.RWMutex</code> per memtable</td>\n<td>Lock-free ring buffer for writes, CAS for memtable swap</td>\n</tr>\n<tr>\n<td>Batch Serialization</td>\n<td>Custom binary format with <code>encoding/binary</code></td>\n<td>Protocol Buffers for forward compatibility</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   └── server/\n│       └── main.go                 # Entry point\n├── internal/\n│   ├── storage/\n│   │   ├── engine.go              # StorageEngine (coordinates WAL, memtable, TSM)\n│   │   ├── wal/\n│   │   │   ├── wal.go             # WAL manager\n│   │   │   ├── segment.go         # Segment implementation (starter code below)\n│   │   │   └── recovery.go        # Recovery scanning logic\n│   │   ├── memtable/\n│   │   │   ├── memtable.go        # Memtable implementation\n│   │   │   └── manager.go         # MemtableManager (handles immutable/flush)\n│   │   └── tsm/\n│   │       ├── writer.go          # TSMWriter (from Milestone 1)\n│   │       └── reader.go          # TSMReader (from Milestone 1)\n│   ├── api/\n│   │   ├── http.go                # HTTP write/query handlers\n│   │   └── ingest.go              # Batch ingestion logic\n│   └── models/\n│       ├── point.go               # DataPoint, SeriesKey types\n│       └── query.go               # Query, TimeRange types</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Complete WAL Segment Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/wal/segment.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> wal</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">hash/crc32</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    byteOrder </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> binary.BigEndian</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    crcTable  </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> crc32.</span><span style=\"color:#B392F0\">MakeTable</span><span style=\"color:#E1E4E8\">(crc32.Castagnoli)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SegmentConfig defines configuration for WAL segments.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SegmentConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxSizeBytes   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">         // Maximum size before rotation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SyncInterval   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\"> // How often to sync (0 = never auto-sync)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SyncOnWrite    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">          // Sync after every write (strongest durability)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Segment manages a single WAL segment file.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Segment</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSize </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    closed  </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    firstID </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastID  </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config  </span><span style=\"color:#B392F0\">SegmentConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    syncTicker </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Ticker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewSegment creates or opens a WAL segment file.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSegment</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">firstID</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> SegmentConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create directory if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">MkdirAll</span><span style=\"color:#E1E4E8\">(filepath.</span><span style=\"color:#B392F0\">Dir</span><span style=\"color:#E1E4E8\">(path), </span><span style=\"color:#79B8FF\">0755</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"create wal directory: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Open file in append mode, create if not exists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">OpenFile</span><span style=\"color:#E1E4E8\">(path, os.O_CREATE</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_RDWR</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_APPEND, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"open wal segment: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Get current size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    info, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Stat</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"stat wal segment: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seg </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        path:    path,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file:    file,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        size:    info.</span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxSize: config.MaxSizeBytes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        firstID: firstID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastID:  firstID </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// Will be incremented on first write</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:  config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // If file has existing data, scan to find lastID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> info.</span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> seg.</span><span style=\"color:#B392F0\">scanToFindLastID</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"recover segment lastID: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start periodic sync if configured</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.SyncInterval </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seg.syncTicker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(config.SyncInterval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        go</span><span style=\"color:#E1E4E8\"> seg.</span><span style=\"color:#B392F0\">periodicSync</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> seg, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteEntry appends an entry to the segment.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.closed {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"segment closed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check if we need to rotate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.size </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> s.maxSize {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"segment full\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entryID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.lastID </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> encodeEntry</span><span style=\"color:#E1E4E8\">(entryID, data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write to file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(entry)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"write entry: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Sync if configured</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.config.SyncOnWrite {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Sync</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // If sync fails, the write may not be durable.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // In production, we might want to truncate or mark segment as corrupt.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"sync entry: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.size </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(n)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.lastID </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> entryID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entryID, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Scan reads all entries from the segment.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Scan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">fn</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.closed {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"segment closed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Seek to beginning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Seek</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, io.SeekStart); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"seek to start: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    reader </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">(s.file)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry, id, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> decodeEntry</span><span style=\"color:#E1E4E8\">(reader)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> io.EOF {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"decode entry at offset </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, s.size, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> fn</span><span style=\"color:#E1E4E8\">(id, entry); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close unmaps memory and releases resources.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.syncTicker </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.syncTicker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.closed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Delete removes the segment file from disk.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Delete</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">s.closed {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Remove</span><span style=\"color:#E1E4E8\">(s.path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Helper functions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> encodeEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Entry format: [4 length][4 checksum][8 id][data]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Length includes all fields (4+4+8+len(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    totalLen </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buf </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, totalLen)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    byteOrder.</span><span style=\"color:#B392F0\">PutUint32</span><span style=\"color:#E1E4E8\">(buf[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">(totalLen))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write placeholder for checksum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checksumPos </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    byteOrder.</span><span style=\"color:#B392F0\">PutUint64</span><span style=\"color:#E1E4E8\">(buf[</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">], id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Copy data</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(buf[</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">:], data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Compute and write checksum (over id + data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checksum </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> crc32.</span><span style=\"color:#B392F0\">Checksum</span><span style=\"color:#E1E4E8\">(buf[</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">:], crcTable)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    byteOrder.</span><span style=\"color:#B392F0\">PutUint32</span><span style=\"color:#E1E4E8\">(buf[checksumPos:checksumPos</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">], checksum)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> buf</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> decodeEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read length</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> lenBuf [</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadFull</span><span style=\"color:#E1E4E8\">(r, lenBuf[:]); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    totalLen </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> byteOrder.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(lenBuf[:])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read entire entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, totalLen)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(entry[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">], lenBuf[:])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadFull</span><span style=\"color:#E1E4E8\">(r, entry[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">:]); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Verify checksum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storedChecksum </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> byteOrder.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(entry[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    computedChecksum </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> crc32.</span><span style=\"color:#B392F0\">Checksum</span><span style=\"color:#E1E4E8\">(entry[</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">:], crcTable)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> storedChecksum </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> computedChecksum {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"checksum mismatch: stored </span><span style=\"color:#79B8FF\">%x</span><span style=\"color:#9ECBFF\">, computed </span><span style=\"color:#79B8FF\">%x</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            storedChecksum, computedChecksum)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Extract ID and data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    id </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> byteOrder.</span><span style=\"color:#B392F0\">Uint64</span><span style=\"color:#E1E4E8\">(entry[</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> entry[</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> data, id, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">scanToFindLastID</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.firstID </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">Scan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> id </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> lastID</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"non-sequential ID: expected </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, lastID</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastID </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.lastID </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> lastID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Segment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">periodicSync</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> s.syncTicker.C {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">s.closed {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.file.</span><span style=\"color:#B392F0\">Sync</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#6A737D\">// Ignore error for periodic sync</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>Memtable Implementation Skeleton</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/memtable/memtable.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> memtable</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Memtable holds incoming data points in memory before flushing to disk.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Memtable</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#6A737D\"> // key: SeriesKey.String()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                         // Approximate memory usage in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu   </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For out-of-order handling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxOutOfOrderWindow </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMemtable creates a new empty memtable.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMemtable</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">maxOutOfOrderWindow</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Memtable</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Memtable</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        size: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxOutOfOrderWindow: maxOutOfOrderWindow,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Insert adds a point to the appropriate series slice, maintaining sorted order.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Memtable</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Insert</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seriesKey</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">point</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Convert seriesKey to string key using seriesKey.String()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Lock the memtable for writing (m.mu.Lock())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Get the slice for this series (create if doesn't exist)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check if point is out of order:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    - If slice is empty, append and update size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    - If point timestamp > last point timestamp, append (common case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    - Else, binary search to find insertion position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate out-of-order window: if point is too old (beyond maxOutOfOrderWindow </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         compared to latest point), return error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Insert at correct position (may need to shift elements)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update m.size with approximate size of new point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return any error (e.g., out of window)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetRange returns all points for a series within time range.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Memtable</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetRange</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seriesKey</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tr</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Convert seriesKey to string key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Lock for reading (m.mu.RLock())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Get slice for series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If no points, return empty slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Binary search to find start index (first point >= tr.Start)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Iterate from start index while point timestamp &#x3C;= tr.End</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Collect points into result slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Size returns approximate memory usage in bytes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Memtable</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> m.size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Flush returns all data and clears the memtable.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Memtable</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Flush</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Lock for writing (m.mu.Lock())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create copy of m.data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Reset m.data to new empty map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Reset m.size to 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return the copied data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Helper function for binary search in sorted DataPoint slice</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> findInsertIndex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">points</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ts</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement binary search returning index where point should be inserted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use sort.Search with comparison on point.Timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Storage Engine Write Path Skeleton</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/engine.go (partial)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/storage/memtable</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/storage/wal</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageEngine coordinates writes and reads.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">wal</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WAL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memtables </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemtableManager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... other fields from naming conventions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    flushCh </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">memtable</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Memtable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh  </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WritePoint writes a single data point.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WritePoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    seriesKey</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">point</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate point (timestamp not in future, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Batch this single point with others if possible, or create single-point batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Serialize batch to bytes (e.g., using encodeBatch helper)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write to WAL with wal.WriteEntry(serialized)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If WAL write succeeds, insert into active memtable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Check if memtable needs flush (size > threshold)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: If flush needed, trigger async flush via flushCh</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return any error (e.g., WAL write failed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WritePointsBatch writes multiple data points efficiently.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WritePointsBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    points</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Group points by series key for efficient serialization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Apply backpressure: check if memtable size > 90% of max</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         If yes, wait a bit or return error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Serialize batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write to WAL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Insert all points into memtable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Check flush condition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return any error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// flushMemtable flushes an immutable memtable to TSM files.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">flushMemtable</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">mt</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">memtable</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Memtable</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get all data from memtable via mt.Flush()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create new TSM writer (tsm.NewTSMWriter)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each series in memtable data:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    a. Sort points by timestamp (should already be sorted)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //    b. Write in batches of DefaultMaxPointsPerBlock using writer.WriteSeries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Finish TSM file (writer.Finish())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Register new TSM file in engine's file list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Notify WAL that entries up to certain ID are durable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Schedule deletion of old WAL segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update series index metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ol>\n<li><strong>Concurrency</strong>: Use <code>sync.RWMutex</code> for memtable access. For higher throughput, consider sharding memtables by series key hash to reduce lock contention.</li>\n<li><strong>Memory Management</strong>: Estimate memtable size as: <code>(len(seriesKey) + 16) * points</code> (timestamp: 8 bytes, value: 8 bytes). Use <code>runtime.MemStats</code> to monitor actual usage.</li>\n<li><strong>Backpressure</strong>: When memtable reaches 90% capacity, use a <code>sync.Cond</code> to make writers wait or return a &quot;too many requests&quot; error.</li>\n<li><strong>WAL Sync</strong>: Use <code>file.Sync()</code> for durability. For better performance, batch syncs with <code>SyncInterval</code> (e.g., 100ms) but acknowledge writes after WAL append (not after sync). This provides &quot;group commit&quot; semantics.</li>\n<li><strong>Error Handling</strong>: If WAL write fails, do NOT insert into memtable (data would be lost on crash). Return error to client for retry.</li>\n</ol>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing Milestone 2, verify with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run unit tests for write path components</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/wal/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/memtable/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/...</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestWritePath</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Start the server and test write throughput</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># In another terminal, send test writes</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/write</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> \"cpu,host=server01 value=0.64 $(</span><span style=\"color:#B392F0\">date</span><span style=\"color:#9ECBFF\"> +%s%N)\"</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/write</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> \"cpu,host=server01 value=0.73 $(</span><span style=\"color:#B392F0\">date</span><span style=\"color:#9ECBFF\"> +%s%N)\"</span></span></code></pre></div>\n\n<p><strong>Expected Behavior</strong>:</p>\n<ul>\n<li>Points are acknowledged immediately (HTTP 204).</li>\n<li>WAL files appear in data directory (<code>data/wal/segment-000001.wal</code>).</li>\n<li>Memtable grows in memory.</li>\n<li>When memtable reaches size threshold (or after 5 minutes), a TSM file is created in <code>data/tsm/</code>.</li>\n<li>After flush, corresponding WAL segments are deleted.</li>\n</ul>\n<p><strong>Signs of Trouble</strong>:</p>\n<ul>\n<li>No WAL files → WAL not being written before acknowledgment.</li>\n<li>TSM files not created → flush not triggering or failing silently.</li>\n<li>Memory grows unbounded → memtable size calculation or flush triggering broken.</li>\n</ul>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Write returns error &quot;segment full&quot; but no flush happening</td>\n<td>WAL rotation not working or max size too small</td>\n<td>Check <code>SegmentConfig.MaxSizeBytes</code>; look for logs about flush</td>\n<td>Increase max size or ensure flushes run</td>\n</tr>\n<tr>\n<td>High memory usage even after flush</td>\n<td>Memtable data not being cleared after flush</td>\n<td>Check <code>Memtable.Flush()</code> returns copy and resets internal map</td>\n<td>Ensure <code>Flush()</code> creates new map, not sharing reference</td>\n</tr>\n<tr>\n<td>Recovery loses recent writes on restart</td>\n<td>WAL not synced to disk before acknowledgment</td>\n<td>Check if <code>SyncOnWrite=true</code> or periodic sync interval</td>\n<td>Enable <code>SyncOnWrite</code> or decrease sync interval</td>\n</tr>\n<tr>\n<td>Out-of-order points cause panic or incorrect order</td>\n<td>Binary search or insertion logic bug</td>\n<td>Add test with random timestamp insertion; check sort order</td>\n<td>Fix <code>findInsertIndex</code> and slice insertion logic</td>\n</tr>\n<tr>\n<td>Write throughput decreases over time</td>\n<td>Lock contention on single memtable</td>\n<td>Profile with <code>pprof</code>; look for lock wait times</td>\n<td>Implement sharded memtables (advanced)</td>\n</tr>\n</tbody></table>\n<h2 id=\"query-engine-design\">Query Engine Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3: Query Engine</p>\n</blockquote>\n<p>The query engine is the brain of TempoDB, responsible for interpreting user requests, locating relevant data, and transforming raw points into meaningful results. Unlike write-optimized paths that handle high-velocity sequential ingestion, the query engine must excel at <strong>efficient scanning, filtering, and aggregation</strong> over potentially massive historical datasets. This section details how TempoDB translates a declarative query into an efficient execution plan that leverages the storage engine&#39;s columnar layout and indexing to minimize data movement and computational overhead.</p>\n<h3 id=\"mental-model-the-library-research-assistant\">Mental Model: The Library Research Assistant</h3>\n<p>Imagine you&#39;re a researcher in a vast library (the database) requesting information about &quot;average temperature in New York during July 2023.&quot; You don&#39;t want every single measurement; you need a summary. A skilled research assistant (the query engine) follows a systematic process:</p>\n<ol>\n<li><strong>Consult the Card Catalog (Index):</strong> First, they check the card catalog (the series index and TSM file indexes) to identify which books (TSM files) contain temperature data for New York. They note the specific shelves (block offsets) where July 2023 data is located.</li>\n<li><strong>Retrieve Relevant Pages (Block Pruning):</strong> They don&#39;t bring you entire books. Instead, they go to the shelves, pull only the relevant chapters (data blocks) that overlap with your time range, ignoring books about other cities or months outside July.</li>\n<li><strong>Photocopy Necessary Columns (Columnar Scan):</strong> Within each chapter, they don&#39;t copy every word. They use a photocopier that can extract only the timestamp and temperature value columns (columnar projection), skipping irrelevant fields.</li>\n<li><strong>Summarize Information (Aggregation):</strong> Back at your desk, they don&#39;t dump 10,000 individual readings. They calculate the average temperature for each day (GROUP BY time(1d)), producing a concise table of 31 daily averages.</li>\n<li><strong>Present Final Report (Streaming Results):</strong> They hand you the summary table one row at a time (streaming) so you can start analyzing immediately, without waiting for the entire calculation to finish.</li>\n</ol>\n<p>This mental model captures the essence of the query engine: <strong>intelligent indexing, selective data retrieval, columnar efficiency, and early aggregation</strong> to transform a broad request into a precise, resource-efficient answer.</p>\n<h3 id=\"query-parsing-planning-and-execution\">Query Parsing, Planning, and Execution</h3>\n<p>Query processing in TempoDB follows a classic three-stage pipeline: parse the user&#39;s request into an abstract syntax tree (AST), create an optimized execution plan, and then execute that plan against the storage engine. The critical optimization is <strong>predicate pushdown</strong>—applying filters for time and tags as early as possible, ideally at the storage block layer, to avoid moving unnecessary data into memory.</p>\n<h4 id=\"1-query-parsing\">1. Query Parsing</h4>\n<p>The query engine accepts queries in a SQL-like dialect extended with time-series specific constructs (e.g., <code>GROUP BY time(1h)</code>). The parser, typically implemented using a <strong>recursive descent</strong> approach, validates syntax and converts the textual query into a structured <code>Query</code> object defined in the data model.</p>\n<p><strong>Core Query Structure Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>Measurement</code></td>\n<td align=\"left\"><code>string</code></td>\n<td align=\"left\">The target measurement (e.g., &quot;cpu_usage&quot;).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Tags</code></td>\n<td align=\"left\"><code>map[string]string</code></td>\n<td align=\"left\">Equality filters on tags (e.g., <code>host=&quot;server1&quot;</code>). Multiple tags are combined with AND logic.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>TimeRange</code></td>\n<td align=\"left\"><code>TimeRange</code></td>\n<td align=\"left\">Mandatory start and end timestamps for the query. Supports relative times (e.g., <code>now() - 1h</code>).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Aggregate</code></td>\n<td align=\"left\"><code>*AggregateFunction</code></td>\n<td align=\"left\">Optional aggregate function (<code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>, <code>COUNT</code>). If <code>nil</code>, raw points are returned.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>GroupByWindow</code></td>\n<td align=\"left\"><code>time.Duration</code></td>\n<td align=\"left\">Optional time bucket width for <code>GROUP BY time(...)</code>. Must be &gt; 0 if aggregation is specified.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>Fields</code></td>\n<td align=\"left\"><code>[]string</code></td>\n<td align=\"left\">List of field names to return. For simplicity in v1, we assume a single numeric field (float64).</td>\n</tr>\n</tbody></table>\n<p>The parser handles expressions like:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#79B8FF\"> avg</span><span style=\"color:#E1E4E8\">(temperature) </span><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> weather </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#E1E4E8\"> city</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'NYC'</span><span style=\"color:#F97583\"> AND</span><span style=\"color:#F97583\"> time</span><span style=\"color:#F97583\"> >=</span><span style=\"color:#9ECBFF\"> '2023-07-01'</span><span style=\"color:#F97583\"> AND</span><span style=\"color:#F97583\"> time</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#9ECBFF\"> '2023-08-01'</span><span style=\"color:#F97583\"> GROUP BY</span><span style=\"color:#F97583\"> time</span><span style=\"color:#E1E4E8\">(1d)</span></span></code></pre></div>\n<p>This would produce a <code>Query</code> object with <code>Measurement=&quot;weather&quot;</code>, <code>Tags={&quot;city&quot;:&quot;NYC&quot;}</code>, the corresponding <code>TimeRange</code>, <code>Aggregate=AggregateAvg</code>, and <code>GroupByWindow=24*time.Hour</code>.</p>\n<blockquote>\n<p><strong>Key Insight:</strong> Parsing is a pure syntactic transformation. No optimization or storage access occurs at this stage. The goal is to produce a valid, unambiguous internal representation.</p>\n</blockquote>\n<h4 id=\"2-query-planning\">2. Query Planning</h4>\n<p>The planner takes the parsed <code>Query</code> and the current state of the storage engine (list of TSM files, series index) and produces an executable <code>QueryPlan</code>. The plan&#39;s primary job is to <strong>push predicates down</strong> to the lowest possible level.</p>\n<p><strong>Query Plan Data Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Responsibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Series Selection</strong></td>\n<td align=\"left\">Uses the <code>SeriesIndex</code> (map from series key to metadata) to find all <code>SeriesKey</code> values matching the <code>Measurement</code> and <code>Tags</code> predicates. This yields a list of series IDs or keys to fetch data for.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>TSM File Selection</strong></td>\n<td align=\"left\">For each candidate series, consults the <code>TSMIndex</code> of each TSM file to identify which files contain blocks overlapping with the query&#39;s <code>TimeRange</code>. Files with <code>MaxTime &lt; query.Start</code> or <code>MinTime &gt; query.End</code> are skipped entirely.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Block-Level Pruning</strong></td>\n<td align=\"left\">Within each selected TSM file and series, the plan examines individual <code>IndexEntry</code> blocks. Blocks that don&#39;t overlap the time range are skipped. The <code>MinTime</code>/<code>MaxTime</code> in each index entry make this a constant-time check.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Aggregation Strategy</strong></td>\n<td align=\"left\">Decides where to perform aggregation: <br> • <strong>Pushdown</strong>: For simple aggregates like <code>MIN</code>/<code>MAX</code>, the storage engine can return the pre-computed block-level min/max if available. <br> • <strong>Partial Aggregation</strong>: For <code>SUM</code>, <code>COUNT</code>, <code>AVG</code>, the plan may instruct scanners to compute intermediate sums/counts per block, which are then merged.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Execution Order</strong></td>\n<td align=\"left\">Determines the scan order (typically series-by-series, then time-ordered blocks) to maximize temporal locality and cache friendliness.</td>\n</tr>\n</tbody></table>\n<p>The output is a tree of primitive operations: <code>SeriesScan -&gt; TSMFileScan -&gt; BlockScan -&gt; [Filter] -&gt; [Aggregate]</code>. The plan is stateless; it&#39;s a recipe for execution.</p>\n<h4 id=\"3-query-execution\">3. Query Execution</h4>\n<p>The executor runs the plan, coordinating between components to stream results back to the client. It employs an <strong>iterator model</strong> (also known as the Volcano model) where each stage pulls data from the stage below it. This enables pipelining and limits memory usage.</p>\n<p><strong>Execution Step-by-Step:</strong></p>\n<ol>\n<li><strong>Initialize Scanners:</strong> For each series identified in the plan, create a <code>SeriesScanner</code> that manages scanning across multiple TSM files.</li>\n<li><strong>Block Iterator:</strong> Each <code>SeriesScanner</code> creates a <code>BlockIterator</code> for each relevant TSM file. The iterator uses the file&#39;s <code>TSMIndex</code> to seek to the first block with <code>MaxTime &gt;= query.Start</code>.</li>\n<li><strong>Block Decoding:</strong> The iterator reads the compressed block from the memory-mapped file (via <code>TSMReader.ReadBlock</code>), decompresses the timestamps (delta-of-delta) and values (Gorilla XOR), and yields individual <code>DataPoint</code> objects within the query&#39;s time range.</li>\n<li><strong>Filter Application:</strong> Any remaining tag filters (beyond the equality filters used for series selection) are applied point-by-point. In v1, all tag filters are equality-based and pushed to series selection, so this step is often a no-op.</li>\n<li><strong>Aggregation Window Management:</strong> If <code>GroupByWindow</code> is specified, the executor maintains a sliding or tumbling window accumulator. As points are streamed in chronological order, they are added to the current time bucket. When a point&#39;s timestamp crosses a bucket boundary, the current bucket&#39;s aggregate is finalized and emitted, and a new accumulator is started.</li>\n<li><strong>Streaming Output:</strong> Final results (raw points or aggregated buckets) are written to an output channel or buffer as they are produced. The HTTP/gRPC handler streams these results to the client, interleaving computation and network I/O.</li>\n</ol>\n<p><strong>Sequence of Operations:</strong>\nThe flow can be visualized in the sequence diagram: <img src=\"/api/project/time-series-db/architecture-doc/asset?path=diagrams%2Fquery-path-sequence.svg\" alt=\"Sequence Diagram: Query Path\"></p>\n<blockquote>\n<p><strong>Architecture Decision: Iterator-Based Execution Model</strong></p>\n<ul>\n<li><strong>Context:</strong> We need to execute queries that may scan millions of points without loading all data into memory at once.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Materialize-then-process:</strong> Read all relevant points into a slice, then apply filters/aggregation.</li>\n<li><strong>Iterator (Volcano) model:</strong> Each operator implements a <code>Next()</code> interface, pulling data through the pipeline.</li>\n<li><strong>Vectorized/batch model:</strong> Operators process chunks of data (e.g., 1024 points) at a time.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use the iterator model for v1.</li>\n<li><strong>Rationale:</strong> The iterator model is simpler to implement correctly, naturally supports streaming, and maps well to our block-by-block scanning pattern. While vectorized execution can be more CPU-efficient, the complexity outweighs the benefit for an educational codebase. The memory efficiency of streaming is critical for large range queries.</li>\n<li><strong>Consequences:</strong> We get low memory overhead and early result emission, but each point incurs virtual function call overhead. This can be optimized later by switching to batch processing within operators (e.g., process a full block of 1024 points at a time).</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Option</th>\n<th align=\"left\">Pros</th>\n<th align=\"left\">Cons</th>\n<th align=\"left\">Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Materialize-then-process</strong></td>\n<td align=\"left\">Simple imperative logic, easy debugging.</td>\n<td align=\"left\">Memory explosion risk, delays first result.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Iterator Model</strong></td>\n<td align=\"left\">Streams results, memory efficient, composable.</td>\n<td align=\"left\">Per-point call overhead, more complex state machines.</td>\n<td align=\"left\"><strong>Yes</strong></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Vectorized Model</strong></td>\n<td align=\"left\">Excellent CPU cache utilization, modern best practice.</td>\n<td align=\"left\">Significant implementation complexity, harder to debug.</td>\n<td align=\"left\">No (future extension)</td>\n</tr>\n</tbody></table>\n<h3 id=\"aggregations-and-downsampling\">Aggregations and Downsampling</h3>\n<p>Aggregation transforms high-resolution raw data into summarized insights, which is fundamental for time-series analysis. TempoDB supports built-in aggregates and <strong>tumbling window</strong> grouping.</p>\n<h4 id=\"built-in-aggregate-functions\">Built-in Aggregate Functions</h4>\n<p>The five core aggregates operate on the <code>Value</code> field of <code>DataPoint</code>:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Function</th>\n<th align=\"left\">Computation</th>\n<th align=\"left\">Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>COUNT</code></td>\n<td align=\"left\">Number of points in the group.</td>\n<td align=\"left\">Includes <code>null</code>/missing? In v1, all points have values.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>SUM</code></td>\n<td align=\"left\">Sum of all values.</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\"><code>AVG</code></td>\n<td align=\"left\"><code>SUM / COUNT</code>.</td>\n<td align=\"left\">Computed from intermediate sum and count to avoid precision loss.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>MIN</code></td>\n<td align=\"left\">Minimum value.</td>\n<td align=\"left\">Can use block-level min as an optimization.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>MAX</code></td>\n<td align=\"left\">Maximum value.</td>\n<td align=\"left\">Can use block-level max as an optimization.</td>\n</tr>\n</tbody></table>\n<p><strong>Aggregation Algorithm (per series, per time bucket):</strong></p>\n<ol>\n<li>Initialize accumulators: <code>sum = 0</code>, <code>count = 0</code>, <code>min = +Inf</code>, <code>max = -Inf</code>.</li>\n<li>For each <code>DataPoint</code> in chronological order within the bucket:\na. <code>sum += point.Value</code>\nb. <code>count++</code>\nc. <code>min = math.Min(min, point.Value)</code>\nd. <code>max = math.Max(max, point.Value)</code></li>\n<li>At bucket boundary, compute final aggregate:<ul>\n<li><code>COUNT</code> → <code>count</code></li>\n<li><code>SUM</code> → <code>sum</code></li>\n<li><code>AVG</code> → <code>sum / float64(count)</code></li>\n<li><code>MIN</code> → <code>min</code></li>\n<li><code>MAX</code> → <code>max</code></li>\n</ul>\n</li>\n<li>Emit a new <code>DataPoint</code> representing the bucket. Its timestamp is the <strong>start</strong> of the bucket window (alignment is configurable, but default to start). Its value is the aggregate result.</li>\n<li>Reset accumulators for the next bucket.</li>\n</ol>\n<h4 id=\"group-by-time-and-tumbling-windows\">GROUP BY time() and Tumbling Windows</h4>\n<p>The <code>GROUP BY time(&lt;interval&gt;)</code> clause creates <strong>tumbling windows</strong>—contiguous, non-overlapping time intervals that partition the data. A point belongs to exactly one bucket based on its timestamp.</p>\n<p><strong>Window Alignment:</strong> Buckets are aligned to a fixed epoch (e.g., Unix epoch: 1970-01-01T00:00:00Z). The bucket for a timestamp <code>t</code> is calculated as:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>bucket_start = epoch + floor((t - epoch) / interval) * interval\nbucket_end = bucket_start + interval</code></pre></div>\n<p>For example, with <code>interval=1h</code> and <code>epoch=0</code>, the timestamp <code>2023-07-01 10:22:00</code> falls into the bucket <code>[2023-07-01 10:00:00, 2023-07-01 11:00:00)</code>.</p>\n<p><strong>Execution with Streaming:</strong> The executor maintains current bucket state per series (or per group if grouping by tags). As points arrive in time order (guaranteed by the storage layout), the executor checks:</p>\n<ul>\n<li>If <code>point.Timestamp &lt; current_bucket_end</code>, add to current accumulators.</li>\n<li>If <code>point.Timestamp &gt;= current_bucket_end</code>, finalize and emit the current bucket, then advance the window (possibly by multiple intervals if there are gaps in data) and start a new accumulator.</li>\n</ul>\n<h4 id=\"downsampling-vs-on-the-fly-aggregation\">Downsampling vs. On-the-Fly Aggregation</h4>\n<p>Downsampling is a form of <strong>precomputed aggregation</strong> stored persistently, typically applied to older data to save space while preserving trends. It&#39;s distinct from on-the-fly aggregation performed at query time.</p>\n<blockquote>\n<p><strong>Architecture Decision: On-the-Fly Aggregation Only for Milestone 3</strong></p>\n<ul>\n<li><strong>Context:</strong> We need to support aggregate queries over historical data. Precomputing downsamples improves query performance but adds storage and complexity.</li>\n<li><strong>Options:</strong><ol>\n<li><strong>Always compute on-the-fly:</strong> Execute the full aggregation pipeline for every query.</li>\n<li><strong>Pre-compute and store downsampled data:</strong> Run background jobs to create lower-resolution aggregates, and route queries to appropriate resolution.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Implement only on-the-fly aggregation for Milestone 3. Downsampling is deferred to Milestone 4 (Retention &amp; Compaction).</li>\n<li><strong>Rationale:</strong> On-the-fly aggregation is a prerequisite for downsampling and allows us to validate the correctness of aggregation logic first. It also keeps the query engine design focused on execution rather than storage lifecycle. Performance for large historical queries will be addressed later with rollups.</li>\n<li><strong>Consequences:</strong> Queries over long time ranges will be slower initially, but the architecture is ready to incorporate pre-computed rollups as a transparent optimization layer later.</li>\n</ul>\n</blockquote>\n<h4 id=\"handling-gaps-in-data\">Handling Gaps in Data</h4>\n<p>Time-series data often has gaps (no points recorded for periods). The aggregation semantics must define behavior for empty buckets:</p>\n<ul>\n<li><code>COUNT</code> returns 0.</li>\n<li><code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code> return <code>null</code> (or skip the bucket). In v1, we may skip empty buckets entirely in the result set.</li>\n</ul>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Loading Entire Series into Memory Before Filtering</strong></p>\n<ul>\n<li><strong>Description:</strong> Implementing <code>SeriesScanner</code> by first reading <em>all</em> points for a series from all files into a slice, then applying time range filters.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Defeats the purpose of block-level pruning and can cause out-of-memory errors for long-running series. It also delays the first result.</li>\n<li><strong>Fix:</strong> Implement a lazy <code>BlockIterator</code> that only decompresses the next needed block. Use the index&#39;s <code>MinTime</code>/<code>MaxTime</code> to skip blocks entirely.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Incorrect Bucket Alignment for GROUP BY time()</strong></p>\n<ul>\n<li><strong>Description:</strong> Aligning buckets to the query&#39;s start time instead of a fixed epoch, causing non-deterministic results and breaking cacheability.</li>\n<li><strong>Why It&#39;s Wrong:</strong> The same absolute time interval queried at different times would produce different bucket boundaries, making results inconsistent and precomputed rollups impossible.</li>\n<li><strong>Fix:</strong> Always align to a global epoch (Unix epoch). Use <code>floor((timestamp_nanos - epoch_nanos) / interval_nanos) * interval_nanos</code> to calculate bucket start.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Forgetting to Handle Out-of-Order Points in Queries</strong></p>\n<ul>\n<li><strong>Description:</strong> Assuming points within a TSM file are perfectly sorted, but out-of-order writes (within tolerance) may have been inserted.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Aggregations that assume chronological order (like tumbling windows) may produce incorrect results if points arrive late within a block.</li>\n<li><strong>Fix:</strong> The storage engine should guarantee that TSM files store points in sorted order. The memtable flush and compaction processes must sort points before writing blocks. The query engine can then rely on sorted streams.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Pushing Time Predicate to Storage Index</strong></p>\n<ul>\n<li><strong>Description:</strong> Reading all blocks for a series and filtering points in memory after decompression.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Decompression is expensive. Wasting CPU and I/O on irrelevant data destroys performance.</li>\n<li><strong>Fix:</strong> The <code>QueryPlan</code> must use the <code>TSMIndex</code> <code>MinTime</code>/<code>MaxTime</code> to skip blocks before they are read. The <code>BlockIterator.Seek()</code> method should jump to the first block with <code>MaxTime &gt;= query.Start</code>.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Floating-Precision Issues in AVG</strong></p>\n<ul>\n<li><strong>Description:</strong> Calculating average by accumulating <code>sum/count</code> in a loop with floating-point additions, leading to precision loss for large counts.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Time-series aggregations often run over millions of points. Standard floating-point error can become significant.</li>\n<li><strong>Fix:</strong> Use Kahan summation or at least <code>float64</code> for accumulators. For v1, the error is acceptable for educational purposes, but document the limitation.</li>\n</ul>\n<h3 id=\"implementation-guidance-milestone-3\">Implementation Guidance (Milestone 3)</h3>\n<p>This section provides concrete Go code skeletons and organization tips to implement the query engine.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option</th>\n<th align=\"left\">Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Query Parsing</strong></td>\n<td align=\"left\">Handwritten recursive descent parser</td>\n<td align=\"left\">Parser generator (ANTLR, pigeon)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Expression Evaluation</strong></td>\n<td align=\"left\">Switch on <code>AggregateFunction</code> enum</td>\n<td align=\"left\">Abstract syntax tree (AST) interpreter</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Execution Engine</strong></td>\n<td align=\"left\">Iterator model with <code>Next()</code> interface</td>\n<td align=\"left\">Vectorized batch processing</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Result Streaming</strong></td>\n<td align=\"left\">Channel of <code>DataPoint</code></td>\n<td align=\"left\">Custom <code>io.Writer</code> interface</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   └── server/                 # Main entry point\n├── internal/\n│   ├── query/                  # Query engine components\n│   │   ├── parser.go           # Query language parser\n│   │   ├── planner.go          # Query plan generation\n│   │   ├── executor.go         # Plan execution and streaming\n│   │   ├── aggregator.go       # Aggregate function implementations\n│   │   ├── iterator.go         # Block and series iterator interfaces\n│   │   └── scanner.go          # TSM file scanner implementation\n│   ├── storage/                # From Milestone 1 &amp; 2\n│   │   ├── tsm/\n│   │   └── wal/\n│   └── models/                 # Shared data structures\n│       ├── query.go            # Query, TimeRange structs\n│       └── series.go           # DataPoint, SeriesKey\n└── pkg/\n    └── api/                    # HTTP/gRPC handlers (Milestone 5)</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Complete <code>models/query.go</code> (shared data structures):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AggregateFunction represents the built-in aggregation functions.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AggregateFunction</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateNone</span><span style=\"color:#B392F0\"> AggregateFunction</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateSum</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateAvg</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateMin</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateMax</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AggregateCount</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">af </span><span style=\"color:#B392F0\">AggregateFunction</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> af {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> AggregateSum:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"sum\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> AggregateAvg:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"avg\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> AggregateMin:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"min\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> AggregateMax:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"max\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> AggregateCount:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"count\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"none\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange represents an inclusive-exclusive time interval [Start, End).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Contains returns true if t is within [Start, End).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">t.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(tr.Start) </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> t.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(tr.End)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Overlaps returns true if tr and other have any intersection.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Overlaps</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">other</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">tr.Start.</span><span style=\"color:#B392F0\">After</span><span style=\"color:#E1E4E8\">(other.End) </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">other.Start.</span><span style=\"color:#B392F0\">After</span><span style=\"color:#E1E4E8\">(tr.End)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Duration returns End - Start.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tr.End.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(tr.Start)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Query represents a parsed query ready for planning.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Query</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Measurement   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tags          </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange     </span><span style=\"color:#B392F0\">TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Aggregate     </span><span style=\"color:#B392F0\">AggregateFunction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GroupByWindow </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\"> // 0 means no grouping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Field         </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">        // For v1, we assume a single field</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>1. Query Parser Skeleton (<code>internal/query/parser.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Parser holds the state for parsing a query string.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Parser</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Scanner</span><span style=\"color:#6A737D\"> // Tokenizer (lexer) you need to implement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok     </span><span style=\"color:#B392F0\">Token</span><span style=\"color:#6A737D\">    // current token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lit     </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">   // current literal</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ParseQuery parses the input string and returns a models.Query or an error.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ParseQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">input</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">{scanner: </span><span style=\"color:#B392F0\">NewScanner</span><span style=\"color:#E1E4E8\">(strings.</span><span style=\"color:#B392F0\">NewReader</span><span style=\"color:#E1E4E8\">(input))}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.</span><span style=\"color:#B392F0\">next</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#6A737D\">// prime the pump</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> p.</span><span style=\"color:#B392F0\">parseSelectStatement</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// parseSelectStatement parses a SELECT ... FROM ... WHERE ... GROUP BY ... query.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">parseSelectStatement</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    query </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">{Tags: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse SELECT clause</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> p.</span><span style=\"color:#B392F0\">parseSelectClause</span><span style=\"color:#E1E4E8\">(query); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse FROM clause</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> p.</span><span style=\"color:#B392F0\">parseFromClause</span><span style=\"color:#E1E4E8\">(query); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse WHERE clause (optional)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> p.tok </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> WHERE {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> p.</span><span style=\"color:#B392F0\">parseWhereClause</span><span style=\"color:#E1E4E8\">(query); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse GROUP BY clause (optional)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> p.tok </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> GROUP {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> p.</span><span style=\"color:#B392F0\">parseGroupByClause</span><span style=\"color:#E1E4E8\">(query); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ensure we've consumed all input</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> p.tok </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> EOF {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unexpected token </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, expected end of input\"</span><span style=\"color:#E1E4E8\">, p.tok)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> query, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">parseSelectClause</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">q</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Expect SELECT token, consume it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse aggregate function (e.g., avg(temperature)) or field name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set q.Aggregate and q.Field accordingly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If no aggregate, set q.Aggregate = models.AggregateNone</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">parseFromClause</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">q</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Expect FROM token, consume it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Expect IDENTIFIER token, set q.Measurement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">parseWhereClause</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">q</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Expect WHERE token, consume it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse one or more tag equality conditions (tag = 'value') joined by AND</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each condition, add to q.Tags map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse time range condition: time >= &#x3C;start> AND time &#x3C; &#x3C;end></span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse absolute timestamps or relative expressions like now() - 1h</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Create models.TimeRange and assign to q.TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">parseGroupByClause</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">q</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Expect GROUP BY token, consume it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Expect TIME token and '('</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse duration literal (e.g., 1h, 30m)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Convert to time.Duration, assign to q.GroupByWindow</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">next</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.tok, p.lit </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> p.scanner.</span><span style=\"color:#B392F0\">Scan</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>2. Query Planner Skeleton (<code>internal/query/planner.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryPlan is an executable plan for a query.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryPlan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SeriesKeys []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For each series key, list of (TSM file path, block index entries)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FileBlocks </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">BlockRef</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query      </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Planner creates a QueryPlan from a parsed Query and storage engine state.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Planner</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    engine </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Planner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Plan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryPlan</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plan </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryPlan</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Query:      </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">query,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SeriesKeys: []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        FileBlocks: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">BlockRef</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Step 1: Series Selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Use engine.seriesIndex to find all series with matching measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Filter those series by matching all tags in query.Tags</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Append matching series keys to plan.SeriesKeys</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Step 2: TSM File and Block Selection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, seriesKey </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> plan.SeriesKeys {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        var</span><span style=\"color:#E1E4E8\"> blocks []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">BlockRef</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: For each TSM file in engine.tsmFiles</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Use tsmFile.IndexForSeries(seriesKey) to get block index entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: For each index entry, check if entry.Overlaps(query.TimeRange)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 7: If overlapping, add a BlockRef{FilePath, Entry} to blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plan.FileBlocks[seriesKey.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">()] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> plan, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BlockRef references a specific block in a TSM file.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BlockRef</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FilePath </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Entry    </span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">IndexEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>3. Iterator Interface and Series Scanner (<code>internal/query/iterator.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Iterator is the core interface for pulling data through the execution pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Iterator</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Next</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">             // Advances to the next point, returns false if no more</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    At</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#6A737D\">   // Returns the current point (valid only after Next() == true)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Err</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#6A737D\">             // Returns any error encountered</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#6A737D\">           // Releases resources</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesScanner implements Iterator for a single series across multiple TSM files.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SeriesScanner</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seriesKey </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plan      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryPlan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current   </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Internal state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fileIndex </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    blockIter </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BlockIterator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    points    []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#6A737D\"> // decompressed points from current block</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pointIdx  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSeriesScanner</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seriesKey</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">plan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">QueryPlan</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesScanner</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">SeriesScanner</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seriesKey: seriesKey,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plan:      plan,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fileIndex: </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesScanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // If we have points in the current block buffer, return the next one</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> s.points </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> s.pointIdx </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(s.points) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> s.points[s.pointIdx]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.pointIdx</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Apply time filter (in case block had points outside range due to min/max approximation)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> s.plan.Query.TimeRange.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(s.current.Timestamp) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span><span style=\"color:#6A737D\"> // point outside range, skip to next</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Need to load the next block</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> s.blockIter </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">s.blockIter.</span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Move to next file or next block list for this series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.fileIndex</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            blocks </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.plan.FileBlocks[s.seriesKey.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">()]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> s.fileIndex </span><span style=\"color:#F97583\">>=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(blocks) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#6A737D\"> // No more files/blocks for this series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO: Open TSM file reader for blocks[s.fileIndex].FilePath</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO: Create BlockIterator for the specific block index entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s.blockIter </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> NewBlockIterator</span><span style=\"color:#E1E4E8\">(reader, blocks[s.fileIndex].Entry)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // blockIter.Next() returned true, decompress the block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Call blockIter.At() to get compressed data, decompress into s.points</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.pointIdx </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesScanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">At</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> s.current }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesScanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">           { </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> } </span><span style=\"color:#6A737D\">// propagate errors from blockIter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesScanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.blockIter </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> s.blockIter.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>4. Aggregator Skeleton (<code>internal/query/aggregator.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WindowAggregator aggregates points into fixed-time windows.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WindowAggregator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input    </span><span style=\"color:#B392F0\">Iterator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    window   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    aggregate </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">AggregateFunction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Current window state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    windowStart </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sum         </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    count       </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    min         </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max         </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Result to emit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ready  </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewWindowAggregator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">input</span><span style=\"color:#B392F0\"> Iterator</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">agg</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">AggregateFunction</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WindowAggregator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">WindowAggregator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input:     input,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        window:    window,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        aggregate: agg,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        min:       math.</span><span style=\"color:#B392F0\">Inf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max:       math.</span><span style=\"color:#B392F0\">Inf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WindowAggregator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> a.input.</span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        point </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> a.input.</span><span style=\"color:#B392F0\">At</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bucketStart </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> alignToWindow</span><span style=\"color:#E1E4E8\">(point.Timestamp, a.window)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> a.windowStart.</span><span style=\"color:#B392F0\">IsZero</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // First point, initialize window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            a.windowStart </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucketStart</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> bucketStart </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> a.windowStart {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Point crossed into a new window, emit current aggregate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            a.</span><span style=\"color:#B392F0\">finalizeWindow</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            a.ready </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            a.windowStart </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucketStart</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            a.</span><span style=\"color:#B392F0\">resetAccumulators</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Add the current point to the new window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            a.</span><span style=\"color:#B392F0\">addPoint</span><span style=\"color:#E1E4E8\">(point)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Add point to current window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a.</span><span style=\"color:#B392F0\">addPoint</span><span style=\"color:#E1E4E8\">(point)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // End of input, emit final window if we have data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> a.count </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a.</span><span style=\"color:#B392F0\">finalizeWindow</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a.ready </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WindowAggregator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">At</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">a.ready {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"At() called without successful Next()\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.ready </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> a.result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WindowAggregator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">addPoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">p</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.sum </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> p.Value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.count</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.min </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Min</span><span style=\"color:#E1E4E8\">(a.min, p.Value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.max </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Max</span><span style=\"color:#E1E4E8\">(a.max, p.Value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WindowAggregator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">resetAccumulators</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.sum </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.min </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Inf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.max </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Inf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WindowAggregator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">finalizeWindow</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> a.aggregate {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> models.AggregateCount:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(a.count)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> models.AggregateSum:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a.sum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> models.AggregateAvg:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a.sum </span><span style=\"color:#F97583\">/</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(a.count)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> models.AggregateMin:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a.min</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> models.AggregateMax:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a.max</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.result </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">{Timestamp: a.windowStart, Value: value}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// alignToWindow returns the start time of the window containing t.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> alignToWindow</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    epoch </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Unix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(epoch).</span><span style=\"color:#B392F0\">Nanoseconds</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    windowNs </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> window.</span><span style=\"color:#B392F0\">Nanoseconds</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ns </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> windowNs </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> windowNs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> epoch.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(bucket) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> time.Nanosecond)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Memory Mapping for TSM Files:</strong> Use <code>mmap</code> via <code>golang.org/x/exp/mmap</code> or <code>syscall.Mmap</code> for zero-copy reads. Remember to <code>Munmap</code> when closing readers.</li>\n<li><strong>Efficient Iterators:</strong> Implement <code>Seek</code> methods on iterators to jump to a specific timestamp using binary search on block index entries.</li>\n<li><strong>Concurrent Scanning:</strong> For queries spanning many series, you can scan each series in a separate goroutine and merge results using a fan-in pattern. Use <code>sync.Pool</code> for recycling <code>DataPoint</code> slices.</li>\n<li><strong>Profiling:</strong> Use <code>pprof</code> to identify bottlenecks. Likely hotspots: decompression routines and iterator <code>Next()</code> calls.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>To verify your query engine implementation:</p>\n<ol>\n<li><strong>Unit Test Parsing:</strong> Run <code>go test ./internal/query -run TestParser</code>. You should see successful parsing of various query forms and appropriate errors for malformed queries.</li>\n<li><strong>Integration Test End-to-End:</strong><ul>\n<li>Start a test server with an in-memory storage engine.</li>\n<li>Write sample data: <code>curl -X POST http://localhost:8080/write -d &quot;cpu,host=server1 value=42.5 1672531200000000000&quot;</code> (points for different times and series).</li>\n<li>Execute a query: <code>curl &quot;http://localhost:8080/query?q=SELECT avg(value) FROM cpu WHERE host=&#39;server1&#39; AND time &gt;= 1672531200s AND time &lt; 1672617600s GROUP BY time(1h)&quot;</code>.</li>\n<li><strong>Expected:</strong> A JSON response containing aggregated values per hour. Verify the averages are mathematically correct.</li>\n</ul>\n</li>\n<li><strong>Performance Smoke Test:</strong> Write 100,000 points for a single series, then query a 24-hour range. The query should return within a few seconds (most time spent in decompression). Use <code>time</code> command to measure.</li>\n</ol>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Symptom</th>\n<th align=\"left\">Likely Cause</th>\n<th align=\"left\">How to Diagnose</th>\n<th align=\"left\">Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Query returns no data for a valid time range</td>\n<td align=\"left\">Block index min/max times incorrect</td>\n<td align=\"left\">Print index entries for the series; verify <code>MinTime</code>/<code>MaxTime</code> cover written points.</td>\n<td align=\"left\">Ensure <code>TSMWriter</code> correctly computes and writes block min/max.</td>\n</tr>\n<tr>\n<td align=\"left\">GROUP BY time() produces misaligned buckets</td>\n<td align=\"left\">Bucket alignment using query start time</td>\n<td align=\"left\">Log <code>alignToWindow</code> output for sample timestamps.</td>\n<td align=\"left\">Change alignment to fixed epoch (Unix).</td>\n</tr>\n<tr>\n<td align=\"left\">Aggregated values are wrong (e.g., avg off)</td>\n<td align=\"left\">Accumulator reset incorrectly across buckets</td>\n<td align=\"left\">Log accumulator state before finalizing each bucket.</td>\n<td align=\"left\">Ensure <code>resetAccumulators()</code> is called when window changes.</td>\n</tr>\n<tr>\n<td align=\"left\">Query runs out of memory for large ranges</td>\n<td align=\"left\">Loading all points before aggregation</td>\n<td align=\"left\">Add memory profiling (<code>pprof</code>). Check if <code>SeriesScanner</code> buffers entire series.</td>\n<td align=\"left\">Ensure iterator pattern streams points; decompress one block at a time.</td>\n</tr>\n<tr>\n<td align=\"left\">Decompression errors during query</td>\n<td align=\"left\">Corruption in TSM block or wrong compression algorithm</td>\n<td align=\"left\">Write a small tool to read and print raw block bytes, verify checksum.</td>\n<td align=\"left\">Check that <code>compressBlock</code> and <code>decompress</code> use same algorithms; verify CRC32.</td>\n</tr>\n</tbody></table>\n<h2 id=\"retention-and-compaction-design\">Retention and Compaction Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4: Retention &amp; Compaction</p>\n</blockquote>\n<p>The retention and compaction subsystem is TempoDB&#39;s housekeeping department—responsible for maintaining data quality, reclaiming storage space, and ensuring the system remains performant as data ages. Without effective lifecycle management, storage costs would balloon, query performance would degrade for historical data, and disk space would eventually be exhausted. This section details how TempoDB automatically manages data from &quot;hot&quot; recent writes to &quot;cold&quot; historical archives through time-based expiration, file consolidation, and pre-computed summarization.</p>\n<h3 id=\"mental-model-the-warehouse-archivist\">Mental Model: The Warehouse Archivist</h3>\n<p>Imagine a massive warehouse storing physical records. New boxes arrive constantly on a conveyor belt (the write path), each containing documents from a specific day. An archivist is responsible for:</p>\n<ol>\n<li><strong>Cleaning out old boxes:</strong> Every month, they check expiration dates and shred boxes older than the retention policy (TTL enforcement).</li>\n<li><strong>Consolidating partially-filled boxes:</strong> When multiple boxes contain documents from the same time period but are only half-full, they merge them into fewer, fully-packed boxes to save shelf space (compaction).</li>\n<li><strong>Creating summary catalogs:</strong> For very old records, they don&#39;t keep every document but instead create summary binders with daily or weekly totals (downsampling/rollups).</li>\n</ol>\n<p>This archivist works in the background, carefully scheduling their work during quiet periods to avoid disrupting people who need to access current records (queries). They maintain an inventory system (metadata) that tracks which boxes are where and what time periods they cover, updating it whenever they reorganize. This mental model captures the essence of TempoDB&#39;s retention and compaction: systematic, background reorganization of data to balance accessibility, storage efficiency, and performance.</p>\n<h3 id=\"ttl-enforcement-and-compaction-strategy\">TTL Enforcement and Compaction Strategy</h3>\n<p>Time-to-live (TTL) enforcement and compaction are two complementary processes that work together to manage data lifecycle. TTL removes data that&#39;s no longer needed, while compaction optimizes the storage of data that remains.</p>\n<h4 id=\"ttl-enforcement-time-based-data-expiration\">TTL Enforcement: Time-Based Data Expiration</h4>\n<p>TTL policies specify how long data should be retained before automatic deletion. In TempoDB, TTL is configured per measurement (similar to a table) with a maximum age. The system periodically scans for data blocks that have exceeded their retention period and deletes entire TSM files when all their data is expired.</p>\n<p><strong>TTL Enforcement Algorithm:</strong></p>\n<ol>\n<li><strong>Policy Configuration:</strong> Each measurement can have a TTL duration (e.g., 30 days, 1 year, or &quot;infinite&quot; for no expiration). The <code>Config</code> struct includes a default TTL applied to measurements without explicit policies.</li>\n<li><strong>Periodic Scanning:</strong> A background goroutine runs at configurable intervals (default: 1 hour) to check for expired data.</li>\n<li><strong>File-Level Deletion:</strong> Since TSM files contain immutable blocks covering specific time ranges, the system checks each file&#39;s maximum timestamp. If <code>file.max_timestamp &lt; (current_time - TTL)</code>, the entire file is marked for deletion.</li>\n<li><strong>Safe Deletion:</strong> Files aren&#39;t immediately removed from disk. Instead, they&#39;re moved to a &quot;tombstoned&quot; state in the metadata, excluded from queries, and physically deleted after a grace period (e.g., 5 minutes) to handle any in-flight queries.</li>\n<li><strong>Metadata Cleanup:</strong> After file deletion, the series index is updated to remove references to the deleted blocks, and the block cache evicts any cached data from those files.</li>\n</ol>\n<p><strong>Data Structures for TTL Management:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Type Name</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RetentionPolicy</code></td>\n<td><code>Measurement string</code><br><code>Duration time.Duration</code><br><code>Default bool</code></td>\n<td>Defines how long data for a specific measurement should be retained</td>\n</tr>\n<tr>\n<td><code>TTLEnforcer</code></td>\n<td><code>policies map[string]RetentionPolicy</code><br><code>interval time.Duration</code><br><code>stopCh chan struct{}</code><br><code>wg sync.WaitGroup</code></td>\n<td>Manages TTL enforcement across all measurements</td>\n</tr>\n<tr>\n<td><code>TSMFileRef</code> (extended)</td>\n<td><code>Path string</code><br><code>MinTime time.Time</code><br><code>MaxTime time.Time</code><br><code>Measurement string</code><br><code>Tombstoned bool</code><br><code>TombstoneTime time.Time</code></td>\n<td>Reference to a TSM file with metadata needed for TTL decisions</td>\n</tr>\n</tbody></table>\n<p><strong>TTL Enforcement State Machine:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Active</code></td>\n<td>File&#39;s max time &lt; (now - TTL)</td>\n<td><code>Tombstoned</code></td>\n<td>Mark file as tombstoned, set tombstone time, exclude from query plans</td>\n</tr>\n<tr>\n<td><code>Tombstoned</code></td>\n<td>Tombstone time &gt; grace period (e.g., 5 min)</td>\n<td><code>Deleting</code></td>\n<td>Schedule physical file deletion, update metadata</td>\n</tr>\n<tr>\n<td><code>Tombstoned</code></td>\n<td>Query references file (late arrival)</td>\n<td><code>Active</code></td>\n<td>Clear tombstone flag, include in queries</td>\n</tr>\n<tr>\n<td><code>Deleting</code></td>\n<td>File successfully deleted</td>\n<td><code>Deleted</code></td>\n<td>Remove from file list, update series index</td>\n</tr>\n<tr>\n<td><code>Deleting</code></td>\n<td>Deletion fails (e.g., permission)</td>\n<td><code>Tombstoned</code></td>\n<td>Log error, retry on next cycle</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> TTL operates at the file granularity rather than individual points because:</p>\n<ol>\n<li>TSM files are immutable, making partial deletion within a file complex</li>\n<li>Time-series data naturally clusters temporally, so entire files typically expire together</li>\n<li>File deletion is atomic and efficient compared to rewriting files</li>\n</ol>\n</blockquote>\n<h4 id=\"compaction-strategy-level-based-file-consolidation\">Compaction Strategy: Level-Based File Consolidation</h4>\n<p>Compaction merges smaller TSM files into larger, more efficient ones to reduce file count, improve read performance, and reclaim space from deleted or overwritten data. TempoDB uses a level-based compaction strategy inspired by LSM trees but optimized for time-series data.</p>\n<p><strong>Compaction Levels:</strong></p>\n<ul>\n<li><strong>Level 0 (L0):</strong> Freshly flushed memtables (1-2 files per flush). Files may have overlapping time ranges.</li>\n<li><strong>Level 1 (L1):</strong> Compacted from L0, non-overlapping time ranges within each file, up to 100MB each.</li>\n<li><strong>Level 2 (L2):</strong> Further compacted from L1, larger files (up to 500MB) covering broader time ranges.</li>\n<li><strong>Level N (LN):</strong> Continuing hierarchy; higher levels have larger files covering longer time spans.</li>\n</ul>\n<p><img src=\"/api/project/time-series-db/architecture-doc/asset?path=diagrams%2Fcompaction-state.svg\" alt=\"Compaction Tier State Machine\"></p>\n<p><strong>Compaction Triggers:</strong></p>\n<ol>\n<li><strong>Count-based:</strong> When a level accumulates too many files (e.g., L0 &gt; 4 files)</li>\n<li><strong>Size-based:</strong> When files in a level exceed size thresholds</li>\n<li><strong>Time-based:</strong> Scheduled compaction during low-activity periods</li>\n<li><strong>Manual:</strong> Admin-triggered compaction</li>\n</ol>\n<p><strong>Compaction Algorithm Steps:</strong></p>\n<ol>\n<li><strong>Plan Generation:</strong> The compaction planner examines files in a level, selecting those with overlapping time ranges or small sizes.</li>\n<li><strong>Series Grouping:</strong> For each series across selected files, collect all blocks, merging points by timestamp (handling duplicates).</li>\n<li><strong>Time Range Splitting:</strong> If the combined data exceeds the target file size, split by time boundaries to create multiple output files.</li>\n<li><strong>New File Creation:</strong> Write merged data to new TSM files using the standard <code>TSMWriter</code>, applying compression.</li>\n<li><strong>Atomic Switch:</strong> Update the metadata to reference new files, mark old files as &quot;compacted.&quot;</li>\n<li><strong>Cleanup:</strong> Delete old files after confirming new files are successfully written and referenced.</li>\n</ol>\n<p><strong>Compaction Data Structures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Type Name</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CompactionPlan</code></td>\n<td><code>Level int</code><br><code>InputFiles []string</code><br><code>OutputFile string</code><br><code>TimeRange TimeRange</code><br><code>Series []string</code></td>\n<td>Defines which files to compact and where to write output</td>\n</tr>\n<tr>\n<td><code>CompactionStats</code></td>\n<td><code>Level int</code><br><code>FilesIn int</code><br><code>FilesOut int</code><br><code>BytesIn int64</code><br><code>BytesOut int64</code><br><code>Duration time.Duration</code></td>\n<td>Tracks compaction performance and effectiveness</td>\n</tr>\n<tr>\n<td><code>CompactionManager</code></td>\n<td><code>plans chan CompactionPlan</code><br><code>stats map[int]CompactionStats</code><br><code>mu sync.RWMutex</code><br><code>stopCh chan struct{}</code></td>\n<td>Orchestrates compaction across levels</td>\n</tr>\n</tbody></table>\n<p><strong>Handling Special Cases During Compaction:</strong></p>\n<ul>\n<li><strong>Out-of-order points:</strong> Already resolved in memtable or handled during merge by sorting</li>\n<li><strong>Deleted points (tombstones):</strong> Points marked for deletion are omitted from new files</li>\n<li><strong>Partial series overlap:</strong> Only merge blocks for series present in input files</li>\n<li><strong>Compaction failures:</strong> Roll back by deleting partially written output, keeping input files</li>\n</ul>\n<blockquote>\n<p><strong>Critical Consideration:</strong> Compaction must not block write operations. TempoDB achieves this by:</p>\n<ol>\n<li>Performing compaction on immutable, already-persisted TSM files</li>\n<li>Using copy-on-write semantics—new files are created before old ones are deleted</li>\n<li>Allowing reads to continue from old files until the atomic metadata switch</li>\n</ol>\n</blockquote>\n<h4 id=\"common-pitfalls-in-retention-and-compaction\">Common Pitfalls in Retention and Compaction</h4>\n<p>⚠️ <strong>Pitfall: Blocking writes during aggressive compaction</strong></p>\n<ul>\n<li><strong>Description:</strong> Running compaction with too many files or too frequently can consume I/O bandwidth, causing write latency spikes.</li>\n<li><strong>Why it&#39;s wrong:</strong> Time-series databases must sustain high write throughput; blocking writes defeats their primary purpose.</li>\n<li><strong>Fix:</strong> Implement compaction throttling based on system load, schedule compaction during off-peak hours, and limit concurrent compaction jobs.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Losing data during TTL/compaction race conditions</strong></p>\n<ul>\n<li><strong>Description:</strong> If compaction runs on a file while TTL is deleting it, data may be lost or corruption may occur.</li>\n<li><strong>Why it&#39;s wrong:</strong> Data integrity is paramount; any race condition risking data loss is unacceptable.</li>\n<li><strong>Fix:</strong> Use a global lock or versioning system for file state changes. Mark files as &quot;in compaction&quot; to prevent concurrent TTL deletion.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Infinite compaction loops</strong></p>\n<ul>\n<li><strong>Description:</strong> Poorly configured compaction thresholds cause continuous recompaction of the same data.</li>\n<li><strong>Why it&#39;s wrong:</strong> Wastes CPU and I/O, causes write amplification, and provides no benefit.</li>\n<li><strong>Fix:</strong> Implement generation tracking—once a file is compacted to a higher level, don&#39;t recompact it unless new data arrives for overlapping time ranges.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not monitoring disk space during retention</strong></p>\n<ul>\n<li><strong>Description:</strong> TTL deletes files but doesn&#39;t check if disk space is actually reclaimed (some filesystems delay space reclamation).</li>\n<li><strong>Why it&#39;s wrong:</strong> May lead to disk full errors even though TTL is running.</li>\n<li><strong>Fix:</strong> Monitor actual disk free space, implement emergency compaction if space doesn&#39;t recover after deletion, and consider filesystem-specific operations (like <code>fallocate</code> or TRIM).</li>\n</ul>\n<h3 id=\"adr-continuous-vs-scheduled-downsampling\">ADR: Continuous vs Scheduled Downsampling</h3>\n<blockquote>\n<p><strong>Decision: Continuous Downsampling During Compaction</strong></p>\n<ul>\n<li><strong>Context:</strong> Historical data (e.g., older than 30 days) rarely needs millisecond precision but is valuable for trend analysis. Storing full-resolution data for all history consumes excessive storage. We need to automatically reduce data resolution over time while preserving statistical properties.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Continuous downsampling during compaction:</strong> Generate rollups as part of the standard compaction process when data reaches certain age thresholds.</li>\n<li><strong>Scheduled batch downsampling:</strong> Run separate periodic jobs (e.g., nightly) that scan for data eligible for downsampling and create rollups.</li>\n<li><strong>On-demand downsampling:</strong> Generate rollups only when queried, caching results for future use.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Option 1—continuous downsampling during compaction.</li>\n<li><strong>Rationale:</strong> <ul>\n<li><strong>Efficiency:</strong> Leverages existing data movement during compaction—data is already being read and rewritten, so adding rollup computation adds minimal overhead.</li>\n<li><strong>Simplicity:</strong> No separate scheduler, job tracking, or coordination needed.</li>\n<li><strong>Predictability:</strong> Rollups are created deterministically as data ages through levels, ensuring consistent availability of downsampled data.</li>\n<li><strong>Storage optimization:</strong> Immediate space savings as data is downsampled, rather than waiting for a scheduled job.</li>\n</ul>\n</li>\n<li><strong>Consequences:</strong><ul>\n<li><strong>Positive:</strong> Simplified architecture, efficient use of compaction I/O, deterministic rollup creation.</li>\n<li><strong>Negative:</strong> Compaction becomes more CPU-intensive, rollup granularity is tied to compaction levels, harder to change downsampling policies without recompaction.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><strong>Downsampling Options Comparison:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Continuous during compaction</td>\n<td>- No additional I/O<br>- Deterministic timing<br>- Simplified architecture</td>\n<td>- Couples compaction and downsampling<br>- Harder to change policies later</td>\n<td><strong>CHOSEN</strong> for simplicity and efficiency</td>\n</tr>\n<tr>\n<td>Scheduled batch jobs</td>\n<td>- Flexible scheduling<br>- Independent policy updates<br>- Can run during specific off-hours</td>\n<td>- Additional I/O (re-reading data)<br>- Complex job coordination<br>- Delay in space reclamation</td>\n<td>Adds operational complexity</td>\n</tr>\n<tr>\n<td>On-demand (lazy)</td>\n<td>- No upfront computation cost<br>- Adapts to query patterns</td>\n<td>- First query pays heavy cost<br>- Unpredictable performance<br>- No storage savings until queried</td>\n<td>Unpredictable performance unacceptable</td>\n</tr>\n</tbody></table>\n<p><strong>Continuous Downsampling Implementation:</strong></p>\n<ol>\n<li><strong>Policy Definition:</strong> Each measurement can define downsampling rules: e.g., &quot;after 7 days, keep 1-minute averages; after 30 days, keep 5-minute averages; after 1 year, keep 1-hour averages.&quot;</li>\n<li><strong>Level-to-Granularity Mapping:</strong> Associate compaction levels with downsampling granularities: L0-1 (raw), L2 (1-min), L3 (5-min), L4 (1-hour).</li>\n<li><strong>Compaction-Time Rollup:</strong> When compacting files from level N to N+1, apply the appropriate aggregation function (avg, sum, min, max, count) to create downsampled points.</li>\n<li><strong>Metadata Tracking:</strong> Store rollup series with special naming convention: <code>original_series:1min_avg</code>, <code>original_series:5min_max</code>, etc.</li>\n<li><strong>Query Routing:</strong> The query engine automatically selects the appropriate rollup series based on the query&#39;s time range and granularity requirements.</li>\n</ol>\n<p><strong>Downsampling Data Structures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Type Name</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>DownsamplePolicy</code></td>\n<td><code>Measurement string</code><br><code>AfterDuration time.Duration</code><br><code>Granularity time.Duration</code><br><code>Aggregate AggregateFunction</code></td>\n<td>Defines when and how to downsample data</td>\n</tr>\n<tr>\n<td><code>RollupSeriesKey</code></td>\n<td><code>OriginalKey SeriesKey</code><br><code>Granularity time.Duration</code><br><code>Function AggregateFunction</code></td>\n<td>Unique identifier for a rollup series</td>\n</tr>\n<tr>\n<td><code>Downsampler</code></td>\n<td><code>policies []DownsamplePolicy</code><br><code>levelMap map[int]time.Duration</code></td>\n<td>Applies downsampling during compaction</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance-milestone-4\">Implementation Guidance (Milestone 4)</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Background Scheduler</td>\n<td>Go&#39;s <code>time.Ticker</code> with goroutine</td>\n<td>Dedicated worker pool with priority queues</td>\n</tr>\n<tr>\n<td>File Deletion</td>\n<td><code>os.Remove()</code></td>\n<td>Atomic rename + background delete with retry</td>\n</tr>\n<tr>\n<td>Compaction Planning</td>\n<td>Simple level-based with fixed thresholds</td>\n<td>Cost-based planner analyzing overlap ratios</td>\n</tr>\n<tr>\n<td>Downsampling</td>\n<td>Fixed aggregation during level transitions</td>\n<td>Adaptive sampling based on data variance</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/server/main.go\n  internal/\n    retention/\n      ttl_enforcer.go      # TTL enforcement logic\n      ttl_enforcer_test.go\n      policies.go          # Retention policy definitions\n    compaction/\n      manager.go           # Compaction orchestration\n      planner.go           # Plan generation\n      executor.go          # Plan execution\n      stats.go             # Compaction statistics\n      levels.go            # Level configuration\n      compaction_test.go\n    downsampling/\n      policy.go            # Downsampling policy definitions\n      rollup_writer.go     # Write rollup series\n      aggregator.go        # Aggregation during downsampling\n    storage/\n      tsm/                 # Existing TSM implementation\n      block_cache.go\n    engine.go              # Updated StorageEngine with compaction hooks</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Background Job Scheduler (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/retention/scheduler.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> retention</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Job represents a background task to be executed periodically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Job</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tName     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tRun      </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Optional jitter to spread out jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tJitter </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Scheduler manages periodic background jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Scheduler</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tjobs   []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Job</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tcancel </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\twg     </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tmu     </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewScheduler creates a new scheduler</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewScheduler</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Scheduler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Scheduler</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tjobs: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Job</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddJob registers a job to be run periodically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Scheduler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddJob</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">job</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Job</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\ts.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> s.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\ts.jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(s.jobs, job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins executing all registered jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Scheduler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithCancel</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\ts.cancel </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cancel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tfor</span><span style=\"color:#E1E4E8\"> _, job </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> s.jobs {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\ts.wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tgo</span><span style=\"color:#E1E4E8\"> s.</span><span style=\"color:#B392F0\">runJob</span><span style=\"color:#E1E4E8\">(ctx, job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully shuts down all jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Scheduler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> s.cancel </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\ts.</span><span style=\"color:#B392F0\">cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\ts.wg.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// runJob executes a single job periodically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Scheduler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">runJob</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">job</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Job</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> s.wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Initial jitter to spread job starts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> job.Jitter </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tselect</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tcase</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">time.</span><span style=\"color:#B392F0\">After</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(job.Jitter) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> rand.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">())):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tcase</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(job.Interval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Run immediately on start</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> job.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(ctx); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tlog.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Job </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, job.Name, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tfor</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tselect</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tcase</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> job.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(ctx); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\t\tlog.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Job </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, job.Name, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tcase</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Example usage in main.go:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// scheduler := retention.NewScheduler()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// scheduler.AddJob(&#x26;retention.Job{</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">//     Name: \"ttl-enforcement\",</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">//     Interval: 1 * time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">//     Run: ttlEnforcer.Run,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// })</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// scheduler.Start()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// defer scheduler.Stop()</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>TTL Enforcement Sweeper:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/retention/ttl_enforcer.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> retention</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">tempo/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TTLEnforcer periodically removes expired data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TTLEnforcer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tstorage     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Engine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tpolicies    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\"> // measurement -> TTL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tinterval    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tgracePeriod </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tstopCh      </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\twg          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tmu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTTLEnforcer creates a new TTL enforcer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTTLEnforcer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Engine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">defaultTTL</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TTLEnforcer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TTLEnforcer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tstorage:     storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tpolicies:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tinterval:    </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tgracePeriod: </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Minute,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tstopCh:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetPolicy sets a TTL policy for a measurement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TTLEnforcer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetPolicy</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">measurement</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\te.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> e.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\te.policies[measurement] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ttl</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Run executes one TTL enforcement cycle</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TTLEnforcer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Get current time for cutoff calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Iterate through all TSM files in storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: For each file, determine its measurement and applicable TTL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Check if file's MaxTime &#x3C; (now - TTL)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: If expired, mark file as tombstoned (not deleted yet)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Check tombstoned files older than grace period for actual deletion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: Delete files from disk and update storage metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 8: Clean up series index entries pointing to deleted files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 9: Return statistics (files deleted, bytes freed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins periodic TTL enforcement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TTLEnforcer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\te.wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tgo</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tdefer</span><span style=\"color:#E1E4E8\"> e.wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(e.interval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tdefer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\tfor</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\tselect</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\tcase</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\t\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">()); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\t\t\tlog.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TTL enforcement failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\tcase</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">e.stopCh:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\t\t\treturn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully stops TTL enforcement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TTLEnforcer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">\tclose</span><span style=\"color:#E1E4E8\">(e.stopCh)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\te.wg.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Compaction Planner:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/compaction/planner.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> compaction</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">tempo/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CompactionPlanner decides which files to compact</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CompactionPlanner</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tstorage     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Engine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tlevelConfig </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">LevelConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LevelConfig defines compaction parameters for a level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LevelConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tMaxFiles     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tMaxSizeBytes </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTargetSize   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Downsampling granularity for this level (if any)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tDownsampleGranularity </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Plan generates compaction plans for overdue levels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CompactionPlanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Plan</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#B392F0\">CompactionPlan</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tplans </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">CompactionPlan</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Get current TSM files grouped by level from storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: For each level, check if compaction is needed:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Too many files (count > MaxFiles)?</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Files too small (total size &#x3C; threshold)?</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Time-based trigger (oldest file in level > age threshold)?</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: For levels needing compaction, select candidate files:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Prefer files with overlapping time ranges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Prefer smaller files first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Exclude files currently being written to</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Group selected files by time range and series overlap</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Create CompactionPlan for each group:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Set input files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Determine output file path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Set target level (current level + 1)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Include downsampling config if moving to downsampling level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Return list of plans</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> plans, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ShouldDownsample checks if compaction to next level requires downsampling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CompactionPlanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ShouldDownsample</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">currentLevel</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nextLevel</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Check level config for both levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Return true if next level has downsampling granularity configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Handle edge cases (e.g., level doesn't exist)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Compaction Executor:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/compaction/executor.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> compaction</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">tempo/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">tempo/internal/storage/tsm</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Executor runs compaction plans</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Executor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tstorage      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Engine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tdataDir      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tmaxConcurrent </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Execute runs a single compaction plan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Executor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Execute</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">plan</span><span style=\"color:#B392F0\"> CompactionPlan</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Validate all input files exist and are readable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Create temporary output file path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: For each series in input files:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Read all points across all input blocks for that series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Sort points by timestamp (handle duplicates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - If downsampling required, apply aggregation to create rollup points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Split points into blocks if exceeding block size limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Write to TSM writer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Finalize TSM file (write index, footer)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Atomically rename temporary file to final location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Update storage metadata to:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Add reference to new file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Mark old files as \"compacted\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Update series index to point to new blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: Schedule old files for deletion (after safe period)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 8: Update compaction statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// mergeSeriesPoints combines points for a series across multiple files</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Executor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">mergeSeriesPoints</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seriesKey</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">inputFiles</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tpoints </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: For each input file:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Open TSM reader</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Read all blocks for the series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Decompress blocks to get points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Append to points slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Sort all points by timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Remove duplicate timestamps (keep latest write)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Return merged, sorted points</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> points, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// applyDownsampling reduces point granularity through aggregation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Executor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">applyDownsampling</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">points</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">window</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">aggFunc</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">AggregateFunction</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tdownsampled </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: If window is zero or aggFunc is AggregateNone, return original points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Sort points by timestamp (should already be sorted)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Initialize window state:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - currentWindowStart = alignToWindow(points[0].Timestamp, window)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Initialize aggregator (sum, count, min, max)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: For each point:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - If point.Timestamp &#x3C; currentWindowStart + window:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//       * Add point to current window aggregation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//   - Else:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//       * Finalize current window (compute result based on aggFunc)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//       * Append result to downsampled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//       * Advance window (handle gaps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t//       * Reset aggregator for new window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Finalize last window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Return downsampled points</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> downsampled, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ol>\n<li><p><strong>File Operations:</strong> Use <code>os.Rename()</code> for atomic file replacement rather than copy-then-delete. On Windows, you may need to handle &quot;access denied&quot; errors by retrying.</p>\n</li>\n<li><p><strong>Concurrency Control:</strong> Use <code>sync.RWMutex</code> for metadata that&#39;s frequently read (by queries) but infrequently written (during compaction). Consider using a versioned metadata system to avoid blocking reads during updates.</p>\n</li>\n<li><p><strong>Error Handling in Background Jobs:</strong> Always recover from panics in goroutines:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       if</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> recover</span><span style=\"color:#E1E4E8\">(); r </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Compaction panicked: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }()</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Memory Management:</strong> When merging points from multiple large files, stream using iterators rather than loading all points into memory:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   // Use heap-based merging for large datasets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   type</span><span style=\"color:#B392F0\"> pointIterator</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">       Next</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">       At</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">       Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n\n<ol start=\"5\">\n<li><strong>Testing:</strong> Use temporary directories for compaction tests and verify both content and metadata after operations:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   func</span><span style=\"color:#B392F0\"> TestCompaction</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">testing.T) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       tmpDir </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.</span><span style=\"color:#B392F0\">TempDir</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       // ... test compaction ...</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       // Verify: file count decreased, total points preserved</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p><strong>Verification Command:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run retention and compaction tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/retention/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -count=1</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/compaction/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -count=1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Integration test: Write data, wait for TTL, verify deletion</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/integration_test/main.go</span><span style=\"color:#79B8FF\"> --test-ttl</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/compact\"</span><span style=\"color:#6A737D\">  # Trigger manual compaction</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/stats\"</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> jq</span><span style=\"color:#9ECBFF\"> '.compaction'</span><span style=\"color:#6A737D\">  # Check compaction stats</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ol>\n<li>TTL enforcement should delete files older than the retention period (check disk space reduction).</li>\n<li>Compaction should merge small files into larger ones (observe decreasing file count in data directory).</li>\n<li>Downsampling should create rollup series with <code>:1min_avg</code> suffix for older data.</li>\n<li>Queries spanning long time ranges should automatically use downsampled data for older portions.</li>\n</ol>\n<p><strong>Debugging Signs:</strong></p>\n<ul>\n<li><strong>Symptom:</strong> Disk space not freeing up after TTL.<ul>\n<li><strong>Check:</strong> Files may be locked by open readers. Ensure all <code>TSMReader</code> instances are closed.</li>\n</ul>\n</li>\n<li><strong>Symptom:</strong> Compaction running continuously without progress.<ul>\n<li><strong>Check:</strong> Thresholds may be too aggressive. Increase level size thresholds.</li>\n</ul>\n</li>\n<li><strong>Symptom:</strong> Queries returning incorrect aggregates after downsampling.<ul>\n<li><strong>Check:</strong> Verify aggregation logic matches query semantics (e.g., average of averages vs. true average).</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"query-language-and-api-design\">Query Language and API Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 5: Query Language &amp; API</p>\n</blockquote>\n<p>The Query Language and API Design defines how external systems and users interact with TempoDB. This is the face of the database—the contracts and interfaces that developers will use daily. A well-designed API must balance expressiveness with performance, provide clear abstractions that match user mental models, and integrate seamlessly with existing ecosystems. This section specifies TempoDB&#39;s query language syntax, HTTP/gRPC APIs, and implementation strategies for parsing and serving requests.</p>\n<h3 id=\"mental-model-the-restaurant-menu-and-kitchen-window\">Mental Model: The Restaurant Menu and Kitchen Window</h3>\n<p>Imagine a restaurant with a detailed menu (the API) that customers use to place orders. The menu categorizes dishes (queries) by type—appetizers (simple range queries), main courses (aggregations), and desserts (downsampling). Each menu item has a precise description (the query language) that the kitchen staff (the database engine) follows to prepare the dish. The kitchen window separates the dining area (client applications) from the cooking area (internal database components), ensuring clean separation of concerns. When a customer orders &quot;SELECT avg(temperature) FROM sensors WHERE time &gt; now() - 1h GROUP BY time(5m)&quot;, they&#39;re asking for a specific recipe. The waiter (HTTP handler) takes the order to the kitchen, where chefs (query parser, planner, executor) prepare it using ingredients (data blocks) from the pantry (storage engine). The finished dish (query results) is then served on a plate (HTTP/JSON response). This mental model emphasizes that the API should be intuitive and complete (like a good menu), while the query language must be unambiguous and executable (like a kitchen recipe), with clear separation between external interfaces and internal processing.</p>\n<h3 id=\"query-language-specification\">Query Language Specification</h3>\n<p>TempoDB implements a SQL-inspired query language specifically designed for time-series patterns. The language supports filtering by time range and tags, aggregating values over windows, and grouping by fixed intervals—the core operations needed for time-series analysis. The design prioritizes clarity for time-series use cases over generic SQL completeness, avoiding the complexity of joins or nested subqueries that are less common in time-series workloads.</p>\n<h4 id=\"core-grammar\">Core Grammar</h4>\n<p>The query language follows a modified BNF grammar with the following productions:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Query          = SelectClause FromClause [ WhereClause ] [ GroupByClause ] [ LimitClause ]\nSelectClause   = &quot;SELECT&quot; ( AggregateFunc &quot;(&quot; Field &quot;)&quot; | Field | &quot;*&quot; )\nAggregateFunc  = &quot;sum&quot; | &quot;avg&quot; | &quot;min&quot; | &quot;max&quot; | &quot;count&quot; | &quot;first&quot; | &quot;last&quot;\nFromClause     = &quot;FROM&quot; Measurement\nWhereClause    = &quot;WHERE&quot; Condition ( ( &quot;AND&quot; | &quot;OR&quot; ) Condition )*\nCondition      = TimeCondition | TagCondition\nTimeCondition  = &quot;time&quot; CompareOp ( AbsoluteTime | RelativeTime )\nTagCondition   = TagKey CompareOp ( StringLiteral | NumberLiteral )\nCompareOp      = &quot;&gt;&quot; | &quot;&gt;=&quot; | &quot;&lt;&quot; | &quot;&lt;=&quot; | &quot;=&quot; | &quot;!=&quot;\nGroupByClause  = &quot;GROUP BY&quot; &quot;time&quot; &quot;(&quot; Duration &quot;)&quot;\nLimitClause    = &quot;LIMIT&quot; Integer</code></pre></div>\n\n<p><strong>Key Design Decisions:</strong></p>\n<ol>\n<li><strong>Field vs. Tag References</strong>: Fields (the actual numeric values) can be aggregated and selected directly. Tags are only referenced in WHERE clauses for filtering—they cannot appear in the SELECT clause because they&#39;re metadata, not measured values.</li>\n<li><strong>Time Literals</strong>: Supports both absolute timestamps (RFC3339: <code>2023-10-05T14:30:00Z</code>) and relative expressions (<code>now() - 2h</code>). The <code>now()</code> function represents the current server time.</li>\n<li><strong>Single Measurement</strong>: Each query targets exactly one measurement, simplifying execution and aligning with time-series organization patterns.</li>\n<li><strong>Limited Aggregation Placement</strong>: Aggregate functions can only appear in the SELECT clause, not in WHERE—this prevents ambiguous semantics and matches typical time-series query patterns.</li>\n</ol>\n<h4 id=\"query-structure-and-semantics\">Query Structure and Semantics</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>SELECT</strong></td>\n<td>Specifies which field(s) to return and any aggregation</td>\n<td><code>SELECT avg(temperature)</code>, <code>SELECT *</code></td>\n<td><code>*</code> returns all fields (currently only one supported)</td>\n</tr>\n<tr>\n<td><strong>FROM</strong></td>\n<td>Identifies the measurement (container) to query</td>\n<td><code>FROM server_metrics</code></td>\n<td>Measurement names are case-sensitive</td>\n</tr>\n<tr>\n<td><strong>WHERE</strong></td>\n<td>Filters series by tags and time range</td>\n<td><code>WHERE host=&#39;web01&#39; AND time &gt; now() - 1h</code></td>\n<td>Multiple conditions combined with AND/OR</td>\n</tr>\n<tr>\n<td><strong>GROUP BY time()</strong></td>\n<td>Aggregates results into fixed-width time buckets</td>\n<td><code>GROUP BY time(5m)</code></td>\n<td>Creates tumbling windows aligned to epoch</td>\n</tr>\n<tr>\n<td><strong>LIMIT</strong></td>\n<td>Restricts number of returned points</td>\n<td><code>LIMIT 1000</code></td>\n<td>Applied after aggregation, useful for preview</td>\n</tr>\n</tbody></table>\n<p><strong>Example Query Walkthrough:</strong> Consider the query <code>SELECT max(cpu_usage), min(cpu_usage) FROM servers WHERE region=&#39;us-east&#39; AND time &gt;= &#39;2023-10-05T00:00:00Z&#39; AND time &lt; &#39;2023-10-05T01:00:00Z&#39; GROUP BY time(5m)</code>. This requests the maximum and minimum CPU usage for all servers in the us-east region during a specific hour, with results aggregated into 5-minute buckets. The query engine will:</p>\n<ol>\n<li>Identify all series where the <code>region</code> tag equals <code>&#39;us-east&#39;</code></li>\n<li>Locate data blocks overlapping the specified one-hour time range</li>\n<li>For each 5-minute window, compute both the maximum and minimum values across all points in that window</li>\n<li>Return one result point per window containing both aggregated values</li>\n</ol>\n<blockquote>\n<p><strong>Design Insight:</strong> The query language intentionally omits features like subqueries, JOINs, and complex expressions in the SELECT clause. Time-series queries overwhelmingly follow simple patterns: filter by time and tags, then aggregate over windows. Supporting only these patterns simplifies implementation while covering &gt;90% of real-world use cases.</p>\n</blockquote>\n<h4 id=\"architecture-decision-sql-like-vs-flux-style-query-language\">Architecture Decision: SQL-Like vs. Flux-Style Query Language</h4>\n<blockquote>\n<p><strong>Decision: SQL-Like Syntax with Time Extensions</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a query language that is immediately familiar to developers (most know SQL) while efficiently expressing time-series operations like time ranges and windowed aggregations. The language must be simple enough to implement within our educational scope yet powerful enough for real use.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Full SQL with time extensions</strong> (like TimescaleDB): Supports full SQL with time-specific functions and hypertables.</li>\n<li><strong>SQL-like subset with time primitives</strong> (like InfluxQL): Simplified SQL dialect with first-class time ranges and GROUP BY time().</li>\n<li><strong>Functional pipeline language</strong> (like Flux): Chainable operations (filter, map, aggregate) expressed as function calls.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Option 2—SQL-like subset with time primitives.</li>\n<li><strong>Rationale</strong>: <ul>\n<li><strong>Familiarity</strong>: Developers already understand SELECT-FROM-WHERE patterns, reducing learning curve.</li>\n<li><strong>Implementation Simplicity</strong>: Parsing a constrained SQL dialect is easier than building a full SQL parser or a functional language runtime.</li>\n<li><strong>Industry Alignment</strong>: InfluxQL and similar dialects have proven effective for time-series queries.</li>\n<li><strong>Performance</strong>: Simple structure enables straightforward predicate pushdown and execution planning.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Quick adoption, easier parsing/planning, covers common use cases.</li>\n<li><strong>Negative</strong>: Cannot express complex multi-measurement correlations or custom transformations; users needing advanced operations must post-process results.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Full SQL with extensions</td>\n<td>Maximum expressiveness, joins, subqueries</td>\n<td>Extremely complex parser/planner, overkill for TS</td>\n<td>Scope too large for educational project</td>\n</tr>\n<tr>\n<td>SQL-like subset</td>\n<td>Familiar, simpler implementation, covers 90% of TS queries</td>\n<td>Limited expressiveness for complex transformations</td>\n<td><strong>CHOSEN</strong> - Best trade-off</td>\n</tr>\n<tr>\n<td>Functional pipeline</td>\n<td>Extremely flexible, composable, ideal for complex data flows</td>\n<td>Steep learning curve, complex execution engine</td>\n<td>Better suited for advanced analytics systems</td>\n</tr>\n</tbody></table>\n<h3 id=\"write-and-read-apis\">Write and Read APIs</h3>\n<p>TempoDB exposes two primary interfaces: a write API for data ingestion and a read API for queries. Both use HTTP/REST for simplicity and broad compatibility, with plans for optional gRPC endpoints for high-performance scenarios. The APIs follow industry conventions to ease integration with existing monitoring stacks like Prometheus and Grafana.</p>\n<h4 id=\"write-api\">Write API</h4>\n<p>The write API accepts time-series data points in batches using a line protocol format similar to InfluxDB&#39;s. This protocol is efficient for network transmission and parsing, supporting thousands of points per request.</p>\n<p><strong>Endpoint:</strong> <code>POST /api/v1/write</code></p>\n<p><strong>Headers:</strong></p>\n<ul>\n<li><code>Content-Type: text/plain</code> (for line protocol)</li>\n<li>Optional: <code>Content-Encoding: gzip</code> for compressed payloads</li>\n</ul>\n<p><strong>Request Body (Line Protocol Format):</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>&lt;measurement&gt;[,&lt;tag_key&gt;=&lt;tag_value&gt;[,&lt;tag_key&gt;=&lt;tag_value&gt;]] &lt;field_key&gt;=&lt;field_value&gt;[,&lt;field_key&gt;=&lt;field_value&gt;] &lt;timestamp&gt;</code></pre></div>\n\n<p>Each line represents a single data point. Tags are optional but must be sorted by key for canonical series key generation. The timestamp is an integer representing nanoseconds since the Unix epoch (optional; defaults to server time).</p>\n<p><strong>Example Request:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">http</span><pre class=\"arch-pre shiki-highlighted\"><code>POST /api/v1/write HTTP/1.1\nContent-Type: text/plain\n\nserver,host=web01,region=us-east cpu_usage=42.5,memory_usage=38.2 1696501200000000000\nserver,host=web02,region=us-east cpu_usage=35.1 1696501260000000000\ntemperature,sensor_id=temp001 value=22.4 1696501200000000000</code></pre></div>\n\n<p><strong>Response Codes:</strong></p>\n<ul>\n<li><code>204 No Content</code>: Success, all points written</li>\n<li><code>400 Bad Request</code>: Malformed line protocol or invalid timestamp</li>\n<li><code>429 Too Many Requests</code>: Backpressure applied, client should retry</li>\n<li><code>500 Internal Server Error</code>: Unexpected server error</li>\n</ul>\n<p><strong>Design Rationale:</strong> The line protocol is text-based for human readability and debuggability, yet compact enough for efficient parsing. It matches industry standards, allowing easy integration with tools like Telegraf. The API accepts batches to amortize write overhead and WAL sync operations.</p>\n<h4 id=\"read-api\">Read API</h4>\n<p>The read API accepts queries in the TempoDB query language and returns results in structured JSON format suitable for visualization tools.</p>\n<p><strong>Endpoint:</strong> <code>POST /api/v1/query</code> or <code>GET /api/v1/query?q=...</code></p>\n<p><strong>For POST (recommended):</strong></p>\n<ul>\n<li><code>Content-Type: application/json</code></li>\n<li>Body: <code>{&quot;query&quot;: &quot;SELECT avg(cpu_usage) FROM servers WHERE time &gt; now() - 1h GROUP BY time(5m)&quot;}</code></li>\n</ul>\n<p><strong>For GET:</strong></p>\n<ul>\n<li>URL-encoded query parameter: <code>GET /api/v1/query?q=SELECT+avg%28cpu_usage%29+FROM+servers+WHERE+time+%3E+now%28%29+-+1h+GROUP+BY+time%285m%29</code></li>\n</ul>\n<p><strong>Response Format (JSON):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"results\"</span><span style=\"color:#E1E4E8\">: [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"series\"</span><span style=\"color:#E1E4E8\">: [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"servers\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"columns\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"time\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"avg\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"values\"</span><span style=\"color:#E1E4E8\">: [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            [</span><span style=\"color:#9ECBFF\">\"2023-10-05T14:00:00Z\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">42.5</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            [</span><span style=\"color:#9ECBFF\">\"2023-10-05T14:05:00Z\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">43.1</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FDAEB7;font-style:italic\">            ...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          ],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"tags\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"host\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"web01\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"region\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"us-east\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      ],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">null</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Pagination Support:</strong> For large result sets, the API supports limit/offset via query parameters or continuation tokens to prevent memory exhaustion.</p>\n<h4 id=\"prometheus-remote-readwrite-compatibility\">Prometheus Remote Read/Write Compatibility</h4>\n<p>To integrate with the Prometheus ecosystem, TempoDB implements the Prometheus remote read/write API endpoints. This allows Prometheus to use TempoDB as long-term storage.</p>\n<p><strong>Prometheus Write Endpoint:</strong> <code>POST /api/v1/prom/write</code></p>\n<ul>\n<li>Accepts Prometheus remote write protocol (snappy-compressed protobuf)</li>\n<li>Converts Prometheus samples to TempoDB data points (metric name → measurement, labels → tags)</li>\n<li>Handles Prometheus&#39;s specific timestamp granularity (milliseconds)</li>\n</ul>\n<p><strong>Prometheus Read Endpoint:</strong> <code>POST /api/v1/prom/read</code></p>\n<ul>\n<li>Accepts Prometheus remote read requests</li>\n<li>Translates Prometheus matchers and time ranges to TempoDB queries</li>\n<li>Returns results in Prometheus remote read response format</li>\n</ul>\n<p><strong>Grafana Data Source Compatibility:</strong> TempoDB implements the simple JSON datasource API that Grafana expects, allowing direct querying from Grafana panels. The endpoint <code>/api/v1/grafana/query</code> accepts Grafana&#39;s time range and target format, translating to TempoDB queries.</p>\n<h4 id=\"api-architecture-components\">API Architecture Components</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Implementation Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP Router</strong></td>\n<td>Routes requests to appropriate handlers</td>\n<td>Use <code>gorilla/mux</code> or <code>chi</code> for flexibility</td>\n</tr>\n<tr>\n<td><strong>Write Handler</strong></td>\n<td>Parses line protocol, validates points, passes to storage engine</td>\n<td>Must handle gzip decompression, batch validation</td>\n</tr>\n<tr>\n<td><strong>Query Handler</strong></td>\n<td>Parses query string, calls query engine, formats response</td>\n<td>Supports both GET and POST, handles timeout cancellation</td>\n</tr>\n<tr>\n<td><strong>Prometheus Adapter</strong></td>\n<td>Translates Prometheus protobuf to internal format</td>\n<td>Uses <code>prompb</code> generated Go code for protocol buffers</td>\n</tr>\n<tr>\n<td><strong>Grafana Adapter</strong></td>\n<td>Converts Grafana request format to TempoDB queries</td>\n<td>Implements <code>/search</code>, <code>/query</code>, <code>/annotations</code> endpoints</td>\n</tr>\n<tr>\n<td><strong>Authentication Middleware</strong></td>\n<td>(Future) Validates API keys or tokens</td>\n<td>Currently placeholder for educational purposes</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Exposing Internal Errors to Clients</strong></p>\n<ul>\n<li><strong>Description</strong>: Returning raw Go errors (like &quot;panic: nil pointer dereference&quot;) in HTTP responses.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Reveals implementation details, security risk, poor user experience.</li>\n<li><strong>Fix</strong>: Create a layer of well-defined HTTP error responses. Catch panics with middleware, log internal errors server-side, return generic 500 errors to clients.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Validating Time Ranges Early</strong></p>\n<ul>\n<li><strong>Description</strong>: Allowing queries like <code>SELECT * FROM metrics WHERE time &gt; now() - 1000y</code> that would scan the entire dataset.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Could cause memory exhaustion, disk thrashing, denial of service.</li>\n<li><strong>Fix</strong>: Implement query validation layer that rejects queries with unbounded or extremely large time ranges. Set configurable maximum time range limits.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Poor Line Protocol Parsing Performance</strong></p>\n<ul>\n<li><strong>Description</strong>: Using naive string splitting and conversion for each point in large batches.</li>\n<li><strong>Why It&#39;s Wrong</strong>: CPU becomes bottleneck, limits write throughput.</li>\n<li><strong>Fix</strong>: Use optimized parsing with minimal allocations: pre-allocate slices, reuse buffers, use byte-level scanning instead of regex.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Forgetting Content-Type Handling</strong></p>\n<ul>\n<li><strong>Description</strong>: Only accepting <code>text/plain</code> for writes, not supporting <code>gzip</code> compression.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Clients may send compressed data or different content types, causing 400 errors.</li>\n<li><strong>Fix</strong>: Check <code>Content-Encoding</code> header, decompress transparently. Be liberal in what you accept (within security bounds).</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Implementing Query Cancellation</strong></p>\n<ul>\n<li><strong>Description</strong>: Long-running queries continue processing even after client disconnects.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Wastes server resources, could lead to resource exhaustion.</li>\n<li><strong>Fix</strong>: Use request context that cancels when client disconnects. Periodically check <code>ctx.Done()</code> during query execution.</li>\n</ul>\n<h3 id=\"implementation-guidance-milestone-5\">Implementation Guidance (Milestone 5)</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP Framework</strong></td>\n<td>Standard <code>net/http</code> with <code>gorilla/mux</code></td>\n<td><code>chi</code> router with middleware chain</td>\n</tr>\n<tr>\n<td><strong>Query Parsing</strong></td>\n<td>Recursive descent parser manually written</td>\n<td>ANTLR grammar with generated parser</td>\n</tr>\n<tr>\n<td><strong>JSON Marshaling</strong></td>\n<td>Standard <code>encoding/json</code></td>\n<td><code>json-iterator/go</code> for faster performance</td>\n</tr>\n<tr>\n<td><strong>Protocol Buffers</strong></td>\n<td><code>gogo/protobuf</code> for Prometheus compatibility</td>\n<td>Standard <code>google.golang.org/protobuf</code></td>\n</tr>\n<tr>\n<td><strong>Compression</strong></td>\n<td>Standard <code>compress/gzip</code></td>\n<td>Also support <code>snappy</code> for Prometheus</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>Environment variables + YAML config file</td>\n<td>Viper for multi-format config support</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   └── tempo-server/\n│       └── main.go                 # Server entry point\n├── internal/\n│   ├── api/\n│   │   ├── handler.go              # HTTP handler registration\n│   │   ├── write_handler.go        # Line protocol parsing and write handling\n│   │   ├── query_handler.go        # Query API endpoint\n│   │   ├── prometheus_handler.go   # Prometheus remote read/write\n│   │   ├── grafana_handler.go      # Grafana data source API\n│   │   └── middleware.go           # Logging, panic recovery, CORS\n│   ├── query/\n│   │   ├── parser/\n│   │   │   ├── parser.go           # Query language parser\n│   │   │   ├── scanner.go          # Token scanner\n│   │   │   ├── ast.go              # Abstract syntax tree types\n│   │   │   └── parse_test.go\n│   │   └── executor/               # (From Milestone 3)\n│   ├── storage/                    # (From Milestone 1 &amp; 2)\n│   └── models/                     # Shared data structures\n└── pkg/\n    └── lineprotocol/               # Reusable line protocol parser\n        ├── parser.go\n        └── parser_test.go</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Line Protocol Parser (<code>pkg/lineprotocol/parser.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> lineprotocol</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInvalidFormat   </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid line protocol format\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInvalidFloat    </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid float value\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInvalidInt      </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid integer value\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInvalidBoolean  </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid boolean value\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInvalidTime     </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid timestamp\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Point represents a single data point parsed from line protocol</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Point</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Measurement </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tags        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Fields      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Parser parses line protocol format</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Parser</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buf []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pos </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewParser creates a new parser for the given byte slice</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">{buf: data}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Next parses the next point from the buffer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Point</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> p.pos </span><span style=\"color:#F97583\">>=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(p.buf) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#6A737D\"> // EOF</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Skip empty lines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> p.pos </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(p.buf) </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> (p.buf[p.pos] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> p.buf[p.pos] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        p.pos</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> p.pos </span><span style=\"color:#F97583\">>=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(p.buf) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> p.pos</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Find end of line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> p.pos </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(p.buf) </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> p.buf[p.pos] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        p.pos</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> p.buf[start:p.pos]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.pos</span><span style=\"color:#F97583\">++</span><span style=\"color:#6A737D\"> // Skip newline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> p.</span><span style=\"color:#B392F0\">parseLine</span><span style=\"color:#E1E4E8\">(line)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">parseLine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">line</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Point</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pt </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Point</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Tags:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Fields: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse measurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    measEnd </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">IndexByte</span><span style=\"color:#E1E4E8\">(line, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> measEnd </span><span style=\"color:#F97583\">==</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        measEnd </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">IndexByte</span><span style=\"color:#E1E4E8\">(line, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">,</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> measEnd </span><span style=\"color:#F97583\">==</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidFormat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pt.Measurement </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">(line[:measEnd])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    remaining </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> line[measEnd:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse tags (optional)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(remaining) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> remaining[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">,</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tagEnd </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">IndexByte</span><span style=\"color:#E1E4E8\">(remaining, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tagEnd </span><span style=\"color:#F97583\">==</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidFormat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tagsStr </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> remaining[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:tagEnd]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        remaining </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> remaining[tagEnd:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Parse comma-separated tags</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _, tag </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">Split</span><span style=\"color:#E1E4E8\">(tagsStr, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">,</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            kv </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">SplitN</span><span style=\"color:#E1E4E8\">(tag, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">=</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(kv) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidFormat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pt.Tags[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(kv[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">(kv[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Skip space before fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(remaining) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> remaining[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidFormat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    remaining </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> remaining[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fieldEnd </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">LastIndexByte</span><span style=\"color:#E1E4E8\">(remaining, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> fieldEnd </span><span style=\"color:#F97583\">==</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // No timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fieldEnd </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(remaining)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fieldsStr </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> remaining[:fieldEnd]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    remaining </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> remaining[fieldEnd:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, field </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">Split</span><span style=\"color:#E1E4E8\">(fieldsStr, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">,</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kv </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">SplitN</span><span style=\"color:#E1E4E8\">(field, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">=</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(kv) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidFormat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        key </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">(kv[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        val </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> kv[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Determine field type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(val) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidFormat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Try parsing as float first (most common)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> f, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strconv.</span><span style=\"color:#B392F0\">ParseFloat</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(val), </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pt.Fields[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Try integer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> i, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strconv.</span><span style=\"color:#B392F0\">ParseInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(val), </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pt.Fields[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(i)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Try boolean</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">(val) </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"true\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pt.Fields[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">(val) </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"false\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pt.Fields[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // String value (must be quoted)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(val) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> val[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> val[</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(val)</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                pt.Fields[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">(val[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\"> : </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(val)</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidFormat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse timestamp (optional)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(remaining) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> remaining[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidFormat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tsStr </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">TrimSpace</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(remaining))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tsStr </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Parse as nanoseconds since epoch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ns, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strconv.</span><span style=\"color:#B392F0\">ParseInt</span><span style=\"color:#E1E4E8\">(tsStr, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrInvalidTime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pt.Timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Unix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, ns)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> pt.Timestamp.</span><span style=\"color:#B392F0\">IsZero</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pt.Timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> pt, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>HTTP Server with Middleware (<code>internal/api/handler.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> api</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">runtime/debug</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/mux</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">go.uber.org/zap</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Server</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">mux</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Router</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage     </span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryEngine </span><span style=\"color:#B392F0\">QueryEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">zap</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpServer  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    WritePoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesKey</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">point</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    WritePointsBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">points</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Query</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryEngine</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">q</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryEngine</span><span style=\"color:#B392F0\"> QueryEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">zap</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        router:      mux.</span><span style=\"color:#B392F0\">NewRouter</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:     storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryEngine: queryEngine,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:      logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.</span><span style=\"color:#B392F0\">setupRoutes</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">setupRoutes</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Recovery middleware for all routes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">Use</span><span style=\"color:#E1E4E8\">(s.recoveryMiddleware)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">Use</span><span style=\"color:#E1E4E8\">(s.loggingMiddleware)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // API routes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/write\"</span><span style=\"color:#E1E4E8\">, s.handleWrite).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/query\"</span><span style=\"color:#E1E4E8\">, s.handleQuery).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/prom/write\"</span><span style=\"color:#E1E4E8\">, s.handlePrometheusWrite).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/prom/read\"</span><span style=\"color:#E1E4E8\">, s.handlePrometheusRead).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/grafana/query\"</span><span style=\"color:#E1E4E8\">, s.handleGrafanaQuery).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/grafana/search\"</span><span style=\"color:#E1E4E8\">, s.handleGrafanaSearch).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/health\"</span><span style=\"color:#E1E4E8\">, s.handleHealthCheck).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Static files for admin UI (optional)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">PathPrefix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/admin/\"</span><span style=\"color:#E1E4E8\">).</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">(http.</span><span style=\"color:#B392F0\">StripPrefix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/admin/\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        http.</span><span style=\"color:#B392F0\">FileServer</span><span style=\"color:#E1E4E8\">(http.</span><span style=\"color:#B392F0\">Dir</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"./static/admin\"</span><span style=\"color:#E1E4E8\">))))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recoveryMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> recover</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                s.logger.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"panic recovered\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    zap.</span><span style=\"color:#B392F0\">Any</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">, err),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    zap.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"stack\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(debug.</span><span style=\"color:#B392F0\">Stack</span><span style=\"color:#E1E4E8\">())),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    zap.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"path\"</span><span style=\"color:#E1E4E8\">, r.URL.Path))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Internal server error\"</span><span style=\"color:#E1E4E8\">, http.StatusInternalServerError)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">loggingMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Wrap response writer to capture status code</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rw </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">responseWriter</span><span style=\"color:#E1E4E8\">{ResponseWriter: w, statusCode: http.StatusOK}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(rw, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HTTP request\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            zap.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"method\"</span><span style=\"color:#E1E4E8\">, r.Method),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            zap.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"path\"</span><span style=\"color:#E1E4E8\">, r.URL.Path),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            zap.</span><span style=\"color:#B392F0\">Int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">, rw.statusCode),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            zap.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"duration\"</span><span style=\"color:#E1E4E8\">, time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            zap.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"remote\"</span><span style=\"color:#E1E4E8\">, r.RemoteAddr))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> responseWriter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    statusCode </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rw </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">responseWriter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">code</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rw.statusCode </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> code</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rw.ResponseWriter.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(code)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleHealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w).</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ok\"</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">addr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.httpServer </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:         addr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Handler:      s.router,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        IdleTimeout:  </span><span style=\"color:#79B8FF\">60</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Starting HTTP server\"</span><span style=\"color:#E1E4E8\">, zap.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"addr\"</span><span style=\"color:#E1E4E8\">, addr))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> s.httpServer </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Query Parser (<code>internal/api/query/parser/parser.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Parser</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Scanner</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok     </span><span style=\"color:#B392F0\">Token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lit     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">input</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">{scanner: </span><span style=\"color:#B392F0\">NewScanner</span><span style=\"color:#E1E4E8\">(input)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ParseQuery parses a query string into a models.Query</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ParseQuery</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Initialize scanner</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.</span><span style=\"color:#B392F0\">scan</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    query </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Tags: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse SELECT clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Expect \"SELECT\" token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse field name or aggregate function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle \"*\" for all fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store field name and aggregate function in query</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse FROM clause  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Expect \"FROM\" token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse measurement identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store in query.Measurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse optional WHERE clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If next token is \"WHERE\", parse conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse time conditions: \"time > now() - 1h\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse tag conditions: \"host = 'web01'\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Combine with AND/OR (initially support only AND)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Convert relative time expressions to absolute times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store time range in query.TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store tag filters in query.Tags</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse optional GROUP BY clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If next token is \"GROUP\", expect \"BY\", \"time\", \"(\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Parse duration like \"5m\", \"1h\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Convert to time.Duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store in query.GroupByWindow</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse optional LIMIT clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If next token is \"LIMIT\", parse integer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store in query (may need to add Limit field to models.Query)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Ensure no extra tokens remain</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> query, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">scan</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.tok, p.lit </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> p.scanner.</span><span style=\"color:#B392F0\">Scan</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">expect</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tok</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> p.tok </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> tok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"expected </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, tok, p.tok)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.</span><span style=\"color:#B392F0\">scan</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// parseDuration parses strings like \"5m\", \"1h\", \"30s\" into time.Duration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> parseDuration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">s</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement duration parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Handle common units: ns, us, ms, s, m, h, d, w</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Convert d (day) to 24h, w (week) to 168h</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// parseTimeExpression parses absolute or relative time expressions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> parseTimeExpression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">expr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement time expression parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check if expr starts with \"now()\" for relative time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Otherwise parse as RFC3339 or Unix timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For relative: parse \"now() - 1h30m\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Write Handler (<code>internal/api/write_handler.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> api</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">compress/gzip</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/pkg/lineprotocol</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">go.uber.org/zap</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleWrite</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(r.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check Content-Encoding header for gzip</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If \"gzip\", wrap r.Body with gzip.Reader</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Defer close of gzip reader</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create line protocol parser for request body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use lineprotocol.NewParser()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle large bodies by streaming (parse in chunks)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse each point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Call parser.Next() until returns nil</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For each point, convert to models.DataPoint and models.SeriesKey</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Validate required fields (measurement, at least one field)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Collect points in a batch slice</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Send batch to storage engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Call s.storage.WritePointsBatch(ctx, points)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle errors: 400 for invalid data, 429 for backpressure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Log errors with s.logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return appropriate HTTP response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - 204 No Content on success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - 400 with error details for client errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - 429 with Retry-After header for backpressure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - 500 for internal errors (don't leak details)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Example success response:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusNoContent)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// parseLineProtocolPoint converts a lineprotocol.Point to internal models</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> parseLineProtocolPoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">lpPoint</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">lineprotocol</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Point</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement conversion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Create SeriesKey from Measurement and Tags</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Extract first numeric field (for now, support single field)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Create DataPoint with Timestamp and Value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Return error if no numeric fields found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Query Handler (<code>internal/api/query_handler.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> api</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/query/parser</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">go.uber.org/zap</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Results []</span><span style=\"color:#B392F0\">Result</span><span style=\"color:#9ECBFF\"> `json:\"results\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Result</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Series []</span><span style=\"color:#B392F0\">Series</span><span style=\"color:#9ECBFF\"> `json:\"series,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Error  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"error,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Series</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Columns []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `json:\"columns\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Values  [][]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}   </span><span style=\"color:#9ECBFF\">`json:\"values\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tags    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"tags,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(r.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> queryStr </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Determine request method and extract query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For GET: read from \"q\" query parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For POST: parse JSON body with QueryRequest struct</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Return 400 if query is empty</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse query string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Create parser.NewParser(queryStr)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Call ParseQuery() to get models.Query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle parse errors with 400 response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check time range is not too large (configurable limit)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Ensure measurement exists (optional check)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Return 400 for invalid queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Call s.queryEngine.ExecuteQuery(ctx, queryStr)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Or call s.storage.Query(ctx, parsedQuery) directly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle context cancellation/timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Format results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Convert internal QueryResult to API Response format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle different result types: raw points, aggregates, grouped</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For GROUP BY time(), format buckets correctly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Write JSON response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Set Content-Type: application/json</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use json.NewEncoder(w).Encode(response)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle streaming for large result sets (optional enhancement)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Example error response:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // w.WriteHeader(http.StatusBadRequest)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // json.NewEncoder(w).Encode(QueryResponse{</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     Results: []Result{{Error: err.Error()}},</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Prometheus Write Handler (<code>internal/api/prometheus_handler.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> api</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gogo/protobuf/proto</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/golang/snappy</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/prometheus/prometheus/prompb</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">go.uber.org/zap</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handlePrometheusWrite</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Read and decompress request body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use snappy.Decode (Prometheus uses snappy framing format)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle possible decompression errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Unmarshal protobuf</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use proto.Unmarshal to get prompb.WriteRequest</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Validate required fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Convert Prometheus samples to TempoDB points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For each TimeSeries in request:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     - Metric name becomes measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     - Labels become tags</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //     - Each sample becomes a DataPoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle different value types (float, histogram, summary - start with float)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Write to storage engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Batch points for efficiency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use s.storage.WritePointsBatch()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return appropriate Prometheus response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - 204 on success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - 400/500 with error in Prometheus format</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handlePrometheusRead</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Read and decompress request (similar to write)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Unmarshal prompb.ReadRequest</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Convert Prometheus matchers to TempoDB query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle equality, regex, and other matcher types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Build models.Query from matchers and time range</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute query and get results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Convert results to prompb.ReadResponse</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Map measurements/tags back to Prometheus metric name/labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Format points as prompb.TimeSeries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Marshal, compress, and send response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use proto.Marshal and snappy encode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Set appropriate Content-Type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<ol>\n<li><p><strong>HTTP Server Performance</strong>: Use <code>http.TimeoutHandler</code> to wrap your router for automatic timeout handling. Set <code>ReadHeaderTimeout</code> on your <code>http.Server</code> to prevent slowloris attacks.</p>\n</li>\n<li><p><strong>JSON Marshaling</strong>: For better performance with large result sets, consider using <code>json.Encoder</code> directly with <code>SetEscapeHTML(false)</code> to avoid unnecessary escaping. For streaming responses, implement <code>http.Flusher</code> support.</p>\n</li>\n<li><p><strong>Context Propagation</strong>: Always pass the request context (<code>r.Context()</code>) to downstream operations. This allows proper cancellation when clients disconnect. Use <code>context.WithTimeout</code> for operations that should have time limits.</p>\n</li>\n<li><p><strong>Line Protocol Parsing Optimizations</strong>:</p>\n<ul>\n<li>Reuse byte buffers with <code>sync.Pool</code> to reduce allocations</li>\n<li>Use <code>bytes.IndexByte</code> instead of <code>strings.Split</code> for better performance</li>\n<li>Pre-allocate slices with estimated capacity to avoid reallocations</li>\n</ul>\n</li>\n<li><p><strong>Query Parser Tips</strong>: Implement the scanner as a state machine reading runes. Use a simple lookup table for keywords. For duration parsing, handle common suffixes: <code>s</code>, <code>m</code>, <code>h</code>, <code>d</code>, <code>w</code> (week = 7d = 168h).</p>\n</li>\n<li><p><strong>Prometheus Protocol Buffers</strong>: Use the <code>gogo/protobuf</code> version that Prometheus uses for compatibility. Generate Go code with <code>protoc --gogofast_out=. *.proto</code>. The <code>prompb</code> package from Prometheus can be imported directly if you vendor it.</p>\n</li>\n<li><p><strong>Graceful Shutdown</strong>: Implement signal handling in <code>main.go</code> to catch <code>SIGTERM</code> and <code>SIGINT</code>, then call <code>server.Shutdown(ctx)</code> with a timeout context to allow in-flight requests to complete.</p>\n</li>\n</ol>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the Query Language and API components, verify functionality with these tests:</p>\n<p><strong>Command to Test Write API:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start the server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/tempo-server/main.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># In another terminal, send test data</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/write</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: text/plain\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --data-raw</span><span style=\"color:#9ECBFF\"> 'cpu,host=server01,region=us-west usage=42.5 1696501200000000000</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">cpu,host=server01,region=us-west usage=43.1 1696501260000000000</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">memory,host=server01 value=2048 1696501200000000000'</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong> HTTP 204 No Content response. Check server logs to confirm points were received and written.</p>\n<p><strong>Command to Test Query API:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/query</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"query\": \"SELECT avg(usage) FROM cpu WHERE time > now() - 1h GROUP BY time(5m)\"}'</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong> JSON response with aggregated results:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"results\"</span><span style=\"color:#E1E4E8\">: [{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"series\"</span><span style=\"color:#E1E4E8\">: [{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"cpu\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"columns\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"time\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"avg\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"values\"</span><span style=\"color:#E1E4E8\">: [[</span><span style=\"color:#9ECBFF\">\"2023-10-05T14:00:00Z\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">42.8</span><span style=\"color:#E1E4E8\">]],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"tags\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"host\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"server01\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"region\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"us-west\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Command to Test Prometheus Remote Write Compatibility:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Use a Prometheus test client or create a simple Go program that sends</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Prometheus remote write protocol format</span></span></code></pre></div>\n\n<p><strong>Signs of Problems and Diagnostics:</strong></p>\n<ul>\n<li><strong>&quot;No data returned&quot; for valid queries</strong>: Check that the query parser correctly extracts time ranges and tag filters. Add debug logging to see the parsed <code>models.Query</code> structure.</li>\n<li><strong>High memory usage during writes</strong>: The line protocol parser may be holding onto large buffers. Ensure you&#39;re not accumulating the entire request body in memory.</li>\n<li><strong>Slow query responses</strong>: Check if the query is doing full scans instead of using time-range predicate pushdown. Verify TSM file indexes are being used correctly.</li>\n<li><strong>Prometheus writes failing</strong>: Verify snappy decompression is correct. Prometheus uses the &quot;snappy framed&quot; format, not raw snappy.</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query returns HTTP 400 with &quot;parse error&quot;</td>\n<td>Malformed query syntax</td>\n<td>Check parser error messages, test with simple query first</td>\n<td>Add more descriptive error messages, validate grammar</td>\n</tr>\n<tr>\n<td>Write API accepts data but nothing appears in queries</td>\n<td>Points not being flushed to TSM files</td>\n<td>Check memtable flush logs, verify WAL is being replayed on startup</td>\n<td>Ensure flush threshold is being reached or implement manual flush endpoint</td>\n</tr>\n<tr>\n<td>Prometheus remote write succeeds but data appears with wrong measurement</td>\n<td>Incorrect metric name to measurement mapping</td>\n<td>Log the conversion from Prometheus TimeSeries to internal points</td>\n<td>Check that <strong>name</strong> label becomes measurement, other labels become tags</td>\n</tr>\n<tr>\n<td>GROUP BY time() returns misaligned buckets</td>\n<td>Window alignment uses wrong epoch</td>\n<td>Test with fixed timestamps, check alignToWindow function</td>\n<td>Ensure alignment uses a fixed epoch (e.g., Unix epoch) not query start time</td>\n</tr>\n<tr>\n<td>Large queries timeout or crash server</td>\n<td>No query limits or memory bounds</td>\n<td>Monitor memory usage during query, add query duration logging</td>\n<td>Implement query timeout, limit max points returned, use streaming results</td>\n</tr>\n<tr>\n<td>Grafana shows &quot;Data source connected but no data&quot;</td>\n<td>Grafana query format mismatch</td>\n<td>Check Grafana debug panel, log the incoming request from Grafana</td>\n<td>Ensure /api/v1/grafana/query returns correct JSON structure with time column first</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2: Write Path, Milestone 3: Query Engine, Milestone 5: Query Language &amp; API</p>\n</blockquote>\n<p>This section details the operational flow of TempoDB&#39;s two most critical user journeys: writing a data point and executing a query. Understanding these sequences illuminates how the previously described components—API, WAL, memtable, storage engine, and query engine—coordinate to fulfill the system&#39;s promises of durability and performance. We trace each request from the client&#39;s network call through the system&#39;s internal machinery to the final response.</p>\n<h3 id=\"write-path-sequence\">Write Path Sequence</h3>\n<p><strong>Mental Model: The Airport Check-in and Baggage System</strong>\nImagine an airport&#39;s check-in process for luggage. You (the client) present your bag (data point) at the counter (HTTP API). The agent immediately prints a baggage receipt (WAL entry) and gives you a copy (acknowledgment), guaranteeing the airline now has a record of your bag. Your bag is then placed on a short conveyor belt (memtable) behind the counter, where it waits with other luggage. When the belt fills up, a worker loads all the bags from that belt onto a cart and takes them to the plane&#39;s cargo hold (TSM file). This separation—immediate receipt for durability and batched loading for efficiency—is the core of TempoDB&#39;s write path.</p>\n<p>The write path is designed for high throughput and strong durability. A write is considered successful once it is durable in the Write-Ahead Log (WAL), allowing subsequent processing (memtable insertion and eventual flush) to happen asynchronously. The following sequence details each step.</p>\n<ol>\n<li><p><strong>Client Request:</strong> An external client sends an HTTP POST request to the <code>/write</code> API endpoint. The request body contains one or more data points in InfluxDB line protocol format (e.g., <code>cpu,host=server01 value=0.64 1434055562000000000</code>).</p>\n</li>\n<li><p><strong>API Layer Reception &amp; Parsing:</strong> The <code>Server</code>&#39;s <code>handleWrite</code> method receives the request. It parses the line protocol using <code>parseLineProtocolPoint</code>, which validates the syntax and converts it into the internal <code>Point</code> model. This model is then transformed into the canonical <code>SeriesKey</code> (from measurement and tags) and <code>DataPoint</code> (timestamp and value) structures.</p>\n</li>\n<li><p><strong>Durability Guarantee - WAL Append:</strong> Before any in-memory update, the system ensures durability. The <code>StorageEngine</code> calls <code>WriteEntry</code> on the current active WAL <code>Segment</code>. This method synchronously appends a binary representation of the <code>SeriesKey</code> and <code>DataPoint</code> to the segment file. Depending on the <code>SegmentConfig.SyncOnWrite</code> setting, it may call <code>fsync</code> to force the data to disk. A successful append guarantees the write can be recovered after a crash. The WAL returns a unique sequence ID for the entry.</p>\n</li>\n<li><p><strong>Write Acknowledgment:</strong> Upon successful WAL append, the <code>StorageEngine</code> can immediately acknowledge the write to the client via the HTTP API layer. The HTTP server sends a <code>204 No Content</code> response. This acknowledgment is the point of durability; the data is now safe and will eventually be queryable.</p>\n</li>\n<li><p><strong>In-Memory Buffering - Memtable Insertion:</strong> Concurrently or immediately after the WAL append, the <code>DataPoint</code> is inserted into the current active <code>Memtable</code>. The <code>Memtable.Insert</code> method locates the slice for the corresponding <code>SeriesKey</code> (creating it if new) and inserts the <code>DataPoint</code> while maintaining sorted order by timestamp. The memtable&#39;s approximate size counter is incremented.</p>\n</li>\n<li><p><strong>Memtable Capacity Check &amp; Rotation:</strong> The <code>StorageEngine</code> continuously monitors the size of the active memtable via its <code>Size()</code> method. When it exceeds <code>Config.MaxMemtableSize</code>, the engine performs a rotation:</p>\n<ul>\n<li>The current active memtable is marked as <strong>immutable</strong>.</li>\n<li>A new, empty memtable is created and designated as the active one for new writes.</li>\n<li>The immutable memtable is placed on a <code>flushCh</code> channel for background processing.</li>\n</ul>\n</li>\n<li><p><strong>Background Flush to TSM:</strong> A dedicated flush goroutine listens on <code>flushCh</code>. When it receives an immutable memtable, it calls <code>FlushMemtable</code>.</p>\n<ul>\n<li>The memtable&#39;s <code>Flush()</code> method is called, returning a map of <code>SeriesKey</code> to sorted <code>[]DataPoint</code>.</li>\n<li>For each series, points are grouped into blocks of up to <code>DefaultMaxPointsPerBlock</code>.</li>\n<li>A <code>TSMWriter</code> is created for a new TSM file. For each block, <code>compressBlock</code> applies delta-of-delta encoding to timestamps and Gorilla XOR compression to values. The writer&#39;s <code>WriteSeries</code> method writes the compressed blocks and builds an in-memory index.</li>\n<li>Finally, <code>TSMWriter.Finish()</code> writes the index and footer to the file and closes it.</li>\n<li>The new TSM file is registered with the <code>StorageEngine</code> by adding a <code>TSMFileRef</code> to the <code>tsmFiles</code> list and updating the <code>seriesIndex</code> metadata.</li>\n</ul>\n</li>\n<li><p><strong>WAL Cleanup:</strong> Once the immutable memtable has been successfully persisted to a TSM file, the corresponding segment of the WAL that contains its entries is no longer needed for recovery. The system can safely delete or archive that WAL segment (<code>Segment.Delete()</code>), reclaiming disk space.</p>\n</li>\n</ol>\n<p><img src=\"/api/project/time-series-db/architecture-doc/asset?path=diagrams%2Fwrite-path-sequence.svg\" alt=\"Sequence Diagram: Write Path\"></p>\n<p><strong>Data Flow Table: Write Path</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Step</th>\n<th align=\"left\">Component</th>\n<th align=\"left\">Input</th>\n<th align=\"left\">Output</th>\n<th align=\"left\">Key Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\">HTTP Client</td>\n<td align=\"left\">Line Protocol String</td>\n<td align=\"left\">HTTP Request</td>\n<td align=\"left\">Initiates write</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\"><code>Server.handleWrite</code></td>\n<td align=\"left\">HTTP Request</td>\n<td align=\"left\"><code>SeriesKey</code>, <code>DataPoint</code></td>\n<td align=\"left\">Parses and validates</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\"><code>Segment.WriteEntry</code></td>\n<td align=\"left\"><code>SeriesKey</code>, <code>DataPoint</code></td>\n<td align=\"left\">WAL Entry ID</td>\n<td align=\"left\">Appends to durable log, may <code>fsync</code></td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\"><code>Server</code></td>\n<td align=\"left\">-</td>\n<td align=\"left\">HTTP 204 Response</td>\n<td align=\"left\">Acknowledges write to client</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\"><code>Memtable.Insert</code></td>\n<td align=\"left\"><code>SeriesKey</code>, <code>DataPoint</code></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Buffers point in sorted in-memory structure</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\"><code>StorageEngine</code></td>\n<td align=\"left\">Memtable Size</td>\n<td align=\"left\">Immutable Memtable</td>\n<td align=\"left\">Checks threshold, rotates memtable</td>\n</tr>\n<tr>\n<td align=\"left\">7</td>\n<td align=\"left\"><code>TSMWriter</code></td>\n<td align=\"left\">Map of <code>SeriesKey</code> to <code>[]DataPoint</code></td>\n<td align=\"left\"><code>.tsm</code> File</td>\n<td align=\"left\">Compresses data, writes blocks and index</td>\n</tr>\n<tr>\n<td align=\"left\">8</td>\n<td align=\"left\"><code>StorageEngine</code></td>\n<td align=\"left\"><code>.tsm</code> File</td>\n<td align=\"left\">Updated <code>tsmFiles</code>, <code>seriesIndex</code></td>\n<td align=\"left\">Registers new file for queries</td>\n</tr>\n</tbody></table>\n<p><strong>Common Pitfalls in the Write Sequence</strong></p>\n<p>⚡ <strong>Pitfall: Acknowledging before durable WAL persistence.</strong></p>\n<ul>\n<li><strong>Description:</strong> Sending the success response to the client after memtable insertion but before the WAL is synced to disk.</li>\n<li><strong>Why it&#39;s wrong:</strong> A server crash after acknowledgment but before the WAL persist would lose the &quot;committed&quot; data, violating durability.</li>\n<li><strong>Fix:</strong> Ensure the response is sent <strong>only after</strong> <code>WriteEntry</code> (and its potential <code>fsync</code>) returns successfully.</li>\n</ul>\n<p>⚡ <strong>Pitfall: Blocking writes during memtable flush.</strong></p>\n<ul>\n<li><strong>Description:</strong> The system stops accepting new writes while an immutable memtable is being flushed to disk.</li>\n<li><strong>Why it&#39;s wrong:</strong> This creates write stalls and hurts throughput, especially during heavy write loads.</li>\n<li><strong>Fix:</strong> Use the memtable rotation pattern. Writes continue uninterrupted into the new active memtable while the flush proceeds in the background.</li>\n</ul>\n<p>⚡ <strong>Pitfall: Not handling WAL segment rotation.</strong></p>\n<ul>\n<li><strong>Description:</strong> Letting a single WAL segment file grow indefinitely.</li>\n<li><strong>Why it&#39;s wrong:</strong> Extremely large files are difficult to manage, slow to read during recovery, and risk losing more data if corrupted.</li>\n<li><strong>Fix:</strong> Implement segment rotation based on size (<code>SegmentConfig.MaxSizeBytes</code>). Close the current segment and start a new one when the limit is reached.</li>\n</ul>\n<h3 id=\"query-path-sequence\">Query Path Sequence</h3>\n<p><strong>Mental Model: The Library Research Assistant</strong>\nImagine you ask a research assistant for all books about &quot;quantum physics&quot; published in the 1990s. The assistant doesn&#39;t grab every book in the library. First, they consult the card catalog (series index) to find the call numbers for relevant sections. Then, they go to the stacks and pull only the books from those sections published between 1990-1999 (time-range pushdown). They skim each book, photocopying only the relevant chapters (block pruning). Finally, if you asked for a summary, they would compile notes from all the photocopies (aggregation) before handing you the final report. This is the query engine&#39;s job: minimize the amount of data physically read and processed.</p>\n<p>The query path prioritizes reading efficiency by pushing filters down to the storage layer and streaming results. It transforms a declarative query (what the user wants) into a series of efficient low-level operations.</p>\n<ol>\n<li><p><strong>Client Request:</strong> A client sends an HTTP GET request to the <code>/query</code> endpoint. The request includes a <code>q</code> parameter with a query string (e.g., <code>SELECT mean(value) FROM cpu WHERE host=&#39;server01&#39; AND time &gt;= now() - 1h GROUP BY time(5m)</code>).</p>\n</li>\n<li><p><strong>Query Parsing:</strong> The <code>Server</code>&#39;s <code>handleQuery</code> method extracts the query string and passes it to <code>NewParser(input).ParseQuery()</code>. The parser, typically a recursive descent parser, tokenizes the input and constructs an abstract syntax tree (AST), finally returning a structured <code>Query</code> object containing <code>Measurement</code>, <code>Tags</code>, <code>TimeRange</code>, <code>AggregateFunction</code>, and <code>GroupByWindow</code>.</p>\n</li>\n<li><p><strong>Query Planning:</strong> The <code>Query</code> object is passed to the <code>Planner.Plan()</code> method. The planner&#39;s job is to convert the logical query into an executable <code>QueryPlan</code>. It performs the following key optimizations:</p>\n<ul>\n<li><strong>Series Identification:</strong> It consults the <code>StorageEngine</code>&#39;s <code>seriesIndex</code> to resolve the <code>Measurement</code> and <code>Tags</code> predicates into a concrete list of <code>SeriesKey</code>s.</li>\n<li><strong>Predicate Pushdown &amp; File Selection:</strong> For each <code>SeriesKey</code>, it examines the list of <code>TSMFileRef</code>s. Using the <code>MinTime</code> and <code>MaxTime</code> in each file&#39;s index, it prunes files that do not <code>Overlap</code> with the query&#39;s <code>TimeRange</code>. This yields a <code>QueryPlan</code> containing a map from <code>SeriesKey</code> to a list of relevant <code>BlockRef</code>s (file path and index entry).</li>\n<li><strong>Aggregation Strategy:</strong> If the query includes a <code>GroupByWindow</code>, the planner wraps the scan iterator with a <code>WindowAggregator</code>.</li>\n</ul>\n</li>\n<li><p><strong>Plan Execution &amp; Streaming:</strong> The <code>QueryPlan</code> is executed, typically via an iterator model. An execution goroutine is spawned for the query.</p>\n<ul>\n<li>For each <code>SeriesKey</code> in the plan, a <code>SeriesScanner</code> is created. The scanner is initialized with the list of <code>BlockRef</code>s.</li>\n<li>The <code>SeriesScanner.Next()</code> method drives the scan. It opens the first TSM file (via <code>OpenTSMReader</code>), seeks to the block offset from the <code>IndexEntry</code>, and reads the compressed block. It then calls <code>decompressTimestamps</code> and <code>decompressValues</code> to materialize the <code>[]DataPoint</code> for that block.</li>\n<li><strong>Key Optimization:</strong> The scanner only reads blocks whose <code>[MinTime, MaxTime]</code> range intersects the query <code>TimeRange</code>. Points outside the range are filtered out before being yielded to the iterator pipeline.</li>\n<li>Points are emitted from the scanner in sorted order (by timestamp).</li>\n</ul>\n</li>\n<li><p><strong>Aggregation (if applicable):</strong> If the plan includes a <code>WindowAggregator</code>, it consumes points from the <code>SeriesScanner</code>. The aggregator&#39;s <code>Next()</code> method buffers points until it has collected all points belonging to the current tumbling window (e.g., a 5-minute bucket). When the window is complete, it applies the <code>AggregateFunction</code> (e.g., <code>mean</code>) to the buffered points, produces a single output <code>DataPoint</code> with the window&#39;s start time as its timestamp, and yields it.</p>\n</li>\n<li><p><strong>Result Streaming to Client:</strong> As result <code>DataPoint</code>s are produced by the execution engine, they are serialized (e.g., to JSON or Prometheus format) and streamed directly to the HTTP response body. This avoids buffering the entire result set in memory, supporting queries over large time ranges.</p>\n</li>\n<li><p><strong>Resource Cleanup:</strong> Once the <code>SeriesScanner</code> and any aggregators have exhausted their input (i.e., <code>Next()</code> returns <code>false</code>), all opened file descriptors (<code>TSMReader</code>) are closed, and the query context is finalized.</p>\n</li>\n</ol>\n<p><img src=\"/api/project/time-series-db/architecture-doc/asset?path=diagrams%2Fquery-path-sequence.svg\" alt=\"Sequence Diagram: Query Path\"></p>\n<p><strong>Data Flow Table: Query Path</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Step</th>\n<th align=\"left\">Component</th>\n<th align=\"left\">Input</th>\n<th align=\"left\">Output</th>\n<th align=\"left\">Key Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\">HTTP Client</td>\n<td align=\"left\">Query String</td>\n<td align=\"left\">HTTP Request</td>\n<td align=\"left\">Initiates query</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\"><code>Parser.ParseQuery</code></td>\n<td align=\"left\">Query String</td>\n<td align=\"left\"><code>Query</code> object</td>\n<td align=\"left\">Validates syntax, builds AST</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\"><code>Planner.Plan</code></td>\n<td align=\"left\"><code>Query</code> object</td>\n<td align=\"left\"><code>QueryPlan</code></td>\n<td align=\"left\">Resolves series, prunes files/blocks, plans aggregation</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\"><code>SeriesScanner.Next</code></td>\n<td align=\"left\"><code>BlockRef</code></td>\n<td align=\"left\"><code>DataPoint</code></td>\n<td align=\"left\">Reads &amp; decompresses block, filters by time, yields points</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\"><code>WindowAggregator.Next</code></td>\n<td align=\"left\">Stream of <code>DataPoint</code></td>\n<td align=\"left\">Aggregated <code>DataPoint</code></td>\n<td align=\"left\">Buffers points per window, computes aggregate</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\"><code>Server.handleQuery</code></td>\n<td align=\"left\">Stream of <code>DataPoint</code></td>\n<td align=\"left\">HTTP Chunked Response</td>\n<td align=\"left\">Serializes and streams results</td>\n</tr>\n<tr>\n<td align=\"left\">7</td>\n<td align=\"left\"><code>SeriesScanner.Close</code></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Closes TSM file readers</td>\n</tr>\n</tbody></table>\n<p><strong>Common Pitfalls in the Query Sequence</strong></p>\n<p>⚡ <strong>Pitfall: Loading entire blocks into memory before filtering.</strong></p>\n<ul>\n<li><strong>Description:</strong> The <code>SeriesScanner</code> decompresses a full block (e.g., 1024 points) into a slice and then iterates to filter out points outside the <code>TimeRange</code>.</li>\n<li><strong>Why it&#39;s wrong:</strong> It wastes CPU and memory on points that will be discarded, especially for sparse queries targeting a small sub-range of a large block.</li>\n<li><strong>Fix:</strong> Use block-level min/max timestamps for coarse pruning. For finer filtering within a block, consider techniques like seeking within compressed streams, though the simplicity of decompress-then-filter is often acceptable given block sizes are bounded.</li>\n</ul>\n<p>⚡ <strong>Pitfall: Not pushing the time-range predicate down.</strong></p>\n<ul>\n<li><strong>Description:</strong> The planner selects TSM files but doesn&#39;t use block-level <code>IndexEntry</code> metadata to skip irrelevant blocks within those files.</li>\n<li><strong>Why it&#39;s wrong:</strong> This forces the storage engine to read and decompress every block for a series in a selected file, even if only one block contains relevant data, causing significant unnecessary I/O.</li>\n<li><strong>Fix:</strong> The <code>QueryPlan</code> must include specific <code>BlockRef</code>s, not just file paths. The <code>SeriesScanner</code> must use the <code>IndexEntry.MinTime/MaxTime</code> to skip blocks entirely.</li>\n</ul>\n<p>⚡ <strong>Pitfall: Buffering all results before sending the HTTP response.</strong></p>\n<ul>\n<li><strong>Description:</strong> The query engine collects all result points in a slice and then serializes the entire slice to JSON at the end.</li>\n<li><strong>Why it&#39;s wrong:</strong> This can exhaust memory for queries returning millions of points and increases client latency (they wait until the very end for the first byte).</li>\n<li><strong>Fix:</strong> Implement streaming serialization. Write each point (or small batches) to the <code>http.ResponseWriter</code> as soon as it&#39;s produced, using HTTP chunked encoding.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option</th>\n<th align=\"left\">Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Query Result Streaming</td>\n<td align=\"left\"><code>http.ResponseWriter</code> with manual chunked writes</td>\n<td align=\"left\"><code>io.Pipe</code> with a dedicated goroutine for serialization</td>\n</tr>\n<tr>\n<td align=\"left\">Query Plan Execution</td>\n<td align=\"left\">Synchronous iteration in a single goroutine</td>\n<td align=\"left\">Concurrent execution per series using <code>sync.WaitGroup</code> and channels</td>\n</tr>\n<tr>\n<td align=\"left\">Point Serialization</td>\n<td align=\"left\">JSON using <code>encoding/json.Encoder</code></td>\n<td align=\"left\">Binary format (e.g., Protobuf) or specialized efficient JSON library</td>\n</tr>\n<tr>\n<td align=\"left\">WAL Sync Strategy</td>\n<td align=\"left\"><code>SyncOnWrite = true</code> for strongest durability</td>\n<td align=\"left\"><code>SyncInterval</code> with group commit for higher throughput</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>Add query execution components to the existing structure.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── internal/\n│   ├── api/\n│   │   ├── server.go              # HTTP server, handler functions\n│   │   └── middleware.go\n│   ├── storage/\n│   │   ├── engine.go              # StorageEngine with WritePoint, Query\n│   │   ├── wal/                   # WAL implementation\n│   │   ├── memtable.go\n│   │   └── tsm/                   # TSM reader/writer\n│   ├── query/\n│   │   ├── parser/                # Query language parsing\n│   │   │   ├── parser.go\n│   │   │   └── ast.go\n│   │   ├── planner.go             # Converts Query to QueryPlan\n│   │   ├── executor.go            # Coordinates execution of QueryPlan\n│   │   ├── scanner.go             # SeriesScanner implementation\n│   │   └── aggregator.go          # WindowAggregator implementation\n│   └── models/                    # Shared data types (DataPoint, SeriesKey, etc.)\n└── cmd/\n    └── tempo-server/\n        └── main.go</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Complete HTTP Streaming Response Helper (to be placed in <code>internal/api/response.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> api</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StreamWriter handles streaming query results to an HTTP response.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StreamWriter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w          </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    encoder    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">json</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Encoder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    flushed    </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewStreamWriter creates a new StreamWriter for the given ResponseWriter.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It sets appropriate headers for streaming JSON.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStreamWriter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamWriter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Transfer-Encoding\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"chunked\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write the opening bracket for a JSON array</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"[\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">StreamWriter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w:       w,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        encoder: json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        flushed: </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WritePoint streams a single data point as JSON.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It adds commas between array elements.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sw </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamWriter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WritePoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">p</span><span style=\"color:#B392F0\"> models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DataPoint</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> sw.flushed {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sw.w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\",\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sw.flushed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> sw.encoder.</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(p)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close finalizes the JSON array and flushes any remaining data.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sw </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamWriter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sw.w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"]\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> f, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sw.w.(</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Flusher</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        f.</span><span style=\"color:#B392F0\">Flush</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error writes an error response and closes the stream.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sw </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StreamWriter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Cancel the opening bracket if no data was written</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">sw.flushed {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sw.w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"[]\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sw.w.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"]\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(sw.w, err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(), http.StatusInternalServerError)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>Skeleton for the <code>StorageEngine.Query</code> method orchestrating the sequence:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Query executes a read query against the storage engine.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It orchestrates parsing, planning, execution, and streaming.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryStr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Result</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse the query string into a models.Query object.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use query.NewParser(queryStr).ParseQuery()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create a query plan from the parsed Query object.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use planner.Plan(queryObj). This step resolves series keys and selects TSM blocks.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize a result streaming structure (e.g., a channel of DataPoint).</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Launch a goroutine to execute the query plan.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // In the goroutine:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Iterate over each SeriesKey in the plan.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For each, create a SeriesScanner for its list of BlockRefs.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Iterate with scanner.Next(), sending points to the result channel.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If the query has aggregation, wrap the scanner with a WindowAggregator.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Close the result channel when done.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return a structure that allows the caller to read from the result stream.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // This enables the HTTP handler to stream results as they are produced.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Skeleton for the <code>SeriesScanner.Next</code> method implementing block pruning:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Next advances the scanner to the next data point.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It returns false when there are no more points or an error occurs.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesScanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: If we have points in the current buffer (s.points) and haven't reached the end (s.pointIdx &#x3C; len(s.points)), advance s.pointIdx and return true.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: If we have no more blocks to read for the current file, move to the next file in the plan.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Close the current TSMReader if open.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Open the next TSM file using storage.OpenTSMReader(path).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Reset the block iterator for the new file.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Get the next BlockRef for the current series from the iterator.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Check if the block's [MinTime, MaxTime] overlaps the query TimeRange. If not, skip it.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Read the compressed block using TSMReader.ReadBlock at the given offset.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Decompress the block into []DataPoint.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Filter the decompressed points: keep only those where TimeRange.Contains(point.Timestamp) is true.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Assign the filtered slice to s.points and reset s.pointIdx to 0.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: If after filtering a block we have zero points, loop back to step 2 to try the next block.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Skeleton for the <code>Planner.Plan</code> method implementing predicate pushdown:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Plan creates an execution plan from a parsed query.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Planner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Plan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">q</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryPlan</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plan </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryPlan</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Query:      </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">q,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SeriesKeys: []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesKey</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        FileBlocks: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">BlockRef</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Resolve series keys from the measurement and tag filters.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Access the storage engine's seriesIndex.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Find all SeriesKey where Measurement matches and Tags satisfy the filter predicates.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add matching keys to plan.SeriesKeys.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each resolved SeriesKey, identify relevant TSM blocks.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Get the list of TSMFileRef from the storage engine.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For each file, check if its global [MinTime, MaxTime] overlaps the query TimeRange.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If yes, load the TSMIndex for that file (or use a cached summary).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Find the IndexEntry for this SeriesKey in the file's index.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For each IndexEntry, check if its block-level [MinTime, MaxTime] overlaps the query TimeRange.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If yes, create a BlockRef{FilePath: file.Path, Entry: entry} and append it to plan.FileBlocks[seriesKey.String()].</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Sort the BlockRefs for each series by MinTime (ascending) for efficient scanning.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If the query has a GroupByWindow, note that an aggregator will be needed during execution.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> plan, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Streaming HTTP:</strong> Use <code>http.Flusher</code> to periodically flush buffered data to the client if you write in chunks. This can reduce perceived latency.</li>\n<li><strong>Concurrent Scanning:</strong> Use a <code>sync.WaitGroup</code> to wait for multiple <code>SeriesScanner</code> goroutines (one per series) to finish. Collect their results via channels and merge them, ensuring timestamp order is maintained if required.</li>\n<li><strong>Resource Management:</strong> Use <code>defer reader.Close()</code> in the <code>SeriesScanner</code> to ensure TSM file descriptors are closed even if an error occurs during iteration.</li>\n<li><strong>Context Propagation:</strong> Pass the <code>context.Context</code> from the HTTP request through to the query execution goroutine. This allows the query to be cancelled if the client disconnects, preventing unnecessary work.</li>\n</ul>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section addresses error handling and edge cases that are critical for the robustness of all five milestones, particularly Milestone 2 (Write Path), Milestone 3 (Query Engine), and Milestone 4 (Retention &amp; Compaction).</p>\n</blockquote>\n<p>Time-series databases operate in challenging environments: they must handle high-velocity data streams, run continuously for years, and process queries over massive datasets while maintaining correctness. Unlike batch processing systems, they cannot stop for repairs or data correction. This section documents the anticipated failure modes and data anomalies that TempoDB must handle gracefully, along with the recovery strategies and design patterns that ensure system resilience.</p>\n<h3 id=\"common-failure-modes-and-recovery\">Common Failure Modes and Recovery</h3>\n<blockquote>\n<p><strong>Mental Model: The Emergency Response Team</strong><br>Think of error handling as an emergency response team for your database. Each component has monitoring sensors (health checks), standard operating procedures for common incidents (disk full, corrupt files), and escalation protocols for complex failures. The goal isn&#39;t to prevent all failures—which is impossible—but to detect them early, contain the damage, recover automatically when possible, and provide clear diagnostics when manual intervention is needed.</p>\n</blockquote>\n<h4 id=\"storage-layer-failures\">Storage Layer Failures</h4>\n<p>The storage layer faces the most severe failure modes since it interacts directly with persistent storage, which can fail in unpredictable ways.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Recovery Strategy</th>\n<th>Impact on Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Disk full during write</strong></td>\n<td><code>os.Write</code> returns <code>ErrNoSpace</code>; filesystem operations fail with <code>ENOSPC</code></td>\n<td>1. Immediately stop accepting new writes<br>2. Mark storage engine as read-only<br>3. Notify monitoring system<br>4. Optionally trigger emergency compaction to free space</td>\n<td>Writes blocked; reads continue; requires administrator intervention to free disk space</td>\n</tr>\n<tr>\n<td><strong>Corrupt WAL segment</strong></td>\n<td>CRC32 checksum mismatch during <code>Segment.Scan()</code>; invalid entry format parsing fails</td>\n<td>1. Log corruption location and skip remaining entries in segment<br>2. Mark segment as corrupt in WAL metadata<br>3. Continue with next segment<br>4. If corruption in active segment, restart with new segment</td>\n<td>Potential data loss for unflushed writes in corrupt segment; system continues with available data</td>\n</tr>\n<tr>\n<td><strong>Corrupt TSM file header</strong></td>\n<td><code>ReadHeader()</code> fails magic number validation or version check</td>\n<td>1. Skip file during query execution<br>2. Move file to quarantine directory<br>3. Log detailed error with file path<br>4. If file was critical for queries, return partial results with error</td>\n<td>Queries return partial data; requires manual investigation of quarantined file</td>\n</tr>\n<tr>\n<td><strong>Memory-mapping failure</strong></td>\n<td><code>mmap</code> system call returns error during <code>OpenTSMReader()</code></td>\n<td>1. Fall back to regular file I/O for that file<br>2. Log performance degradation warning<br>3. Continue operations with slower access path</td>\n<td>Reduced read performance for affected file; system remains functional</td>\n</tr>\n<tr>\n<td><strong>Block decompression failure</strong></td>\n<td><code>decompressTimestamps()</code> or <code>decompressValues()</code> returns error; checksum mismatch</td>\n<td>1. Skip the corrupt block within the TSM file<br>2. Increment corruption counter for monitoring<br>3. Return available data from other blocks<br>4. Schedule file for recompaction if corruption rate exceeds threshold</td>\n<td>Partial data loss within affected time range; queries complete with gaps</td>\n</tr>\n</tbody></table>\n<p><strong>ADR: Recovery Strategy for Corrupt TSM Files</strong></p>\n<blockquote>\n<p><strong>Decision: Quarantine and Continue for Corrupt TSM Files</strong></p>\n<ul>\n<li><strong>Context</strong>: TSM files represent immutable, compacted data blocks. A corrupt TSM file could affect query results, but stopping the database for repair is unacceptable in production scenarios.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Panic and shutdown</strong>: Immediately stop the database to prevent further corruption</li>\n<li><strong>Automatic repair</strong>: Attempt to rebuild the file from WAL and other TSM files</li>\n<li><strong>Quarantine and continue</strong>: Skip the corrupt file and continue operations</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement quarantine and continue with fallback to regular I/O for memory-mapping failures</li>\n<li><strong>Rationale</strong>: Time-series databases prioritize availability over perfect consistency for historical data. The quarantine approach: (1) maintains system availability, (2) provides clear audit trail of corrupt files, (3) allows administrators to investigate during maintenance windows, and (4) aligns with industry practice (InfluxDB uses similar approach)</li>\n<li><strong>Consequences</strong>: Queries may return incomplete results; monitoring must alert on quarantined files; administrators must manually restore from backup if needed</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Panic and shutdown</td>\n<td>Prevents further corruption; forces immediate repair</td>\n<td>Causes downtime; unacceptable for production</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Automatic repair</td>\n<td>Maintains data completeness; transparent to users</td>\n<td>Complex to implement; may fail or cause prolonged unavailability</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Quarantine and continue</td>\n<td>Maintains availability; simple to implement</td>\n<td>Data gaps in queries; requires monitoring and manual intervention</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h4 id=\"write-path-failures\">Write Path Failures</h4>\n<p>The write path must maintain durability guarantees while handling various failure scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Recovery Strategy</th>\n<th>Impact on Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>WAL write failure</strong></td>\n<td><code>WriteEntry()</code> returns I/O error; <code>fsync</code> fails</td>\n<td>1. Retry with exponential backoff (up to 3 attempts)<br>2. If persists, rotate to new WAL segment<br>3. If rotation fails, enter read-only mode<br>4. Log critical error with disk health metrics</td>\n<td>Temporary write blocking during retry; potential data loss if retries fail</td>\n</tr>\n<tr>\n<td><strong>Memtable flush failure</strong></td>\n<td><code>FlushMemtable()</code> returns error during TSM file creation</td>\n<td>1. Retry flush with same data<br>2. If retry fails, write memtable contents to emergency spill file<br>3. Continue with new memtable<br>4. Schedule spill file for processing during next compaction</td>\n<td>Increased disk usage from spill files; compaction handles recovery</td>\n</tr>\n<tr>\n<td><strong>Out-of-memory during ingestion</strong></td>\n<td>Memory allocation fails; <code>Memtable.Size()</code> exceeds safe threshold</td>\n<td>1. Trigger emergency flush of current memtable<br>2. Apply backpressure to write API (HTTP 503)<br>3. Reduce batch sizes for incoming writes<br>4. Log memory pressure warnings</td>\n<td>Temporary write throttling; potential increased latency</td>\n</tr>\n<tr>\n<td><strong>Clock skew in distributed writes</strong></td>\n<td>Incoming timestamp significantly ahead of system clock</td>\n<td>1. Reject writes with timestamps &gt; (now + <code>maxClockSkew</code>)<br>2. Log warning with client identification<br>3. Optionally buffer for <code>maxClockSkew</code> duration</td>\n<td>Rejected writes; clients must correct their clock</td>\n</tr>\n<tr>\n<td><strong>Series cardinality explosion</strong></td>\n<td>Monitoring detects exponential growth in <code>seriesIndex</code> size</td>\n<td>1. Log cardinality warning with top contributing tags<br>2. Continue processing (cardinality is operational concern)<br>3. Expose cardinality metrics via monitoring API</td>\n<td>Increased memory usage; potential performance degradation</td>\n</tr>\n</tbody></table>\n<p><strong>Step-by-Step WAL Recovery Procedure:</strong></p>\n<p>When TempoDB starts, it must recover any unflushed data from the Write-Ahead Log:</p>\n<ol>\n<li><strong>Locate WAL segments</strong>: Scan the WAL directory for files with <code>.wal</code> extension, sorted by sequence number</li>\n<li><strong>Identify recovery range</strong>: Determine the first WAL segment that contains data not yet persisted to TSM files (using the WAL checkpoint file if available)</li>\n<li><strong>Sequential scan</strong>: For each segment in recovery range, call <code>Segment.Scan()</code> to read all entries</li>\n<li><strong>Reconstruct memtables</strong>: For each WAL entry, parse the series key and data point, then insert into an in-memory recovery memtable</li>\n<li><strong>Batch flush</strong>: When recovery memtable reaches configured size threshold, flush it to TSM files</li>\n<li><strong>Cleanup</strong>: Once all segments are processed, delete recovered WAL segments (or move them to archive)</li>\n<li><strong>Resume normal operations</strong>: Start accepting new writes with fresh WAL segment</li>\n</ol>\n<h4 id=\"query-engine-failures\">Query Engine Failures</h4>\n<p>Query failures must not crash the database and should provide helpful error messages to users.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Recovery Strategy</th>\n<th>Impact on Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Query timeout</strong></td>\n<td>Context deadline exceeded during <code>Query()</code> execution</td>\n<td>1. Cancel query execution tree<br>2. Release all allocated resources (iterators, buffers)<br>3. Return timeout error to client<br>4. Log slow query with execution plan</td>\n<td>Failed query; other queries continue unaffected</td>\n</tr>\n<tr>\n<td><strong>Memory exhaustion during aggregation</strong></td>\n<td>Memory allocation fails in <code>WindowAggregator</code></td>\n<td>1. Cancel query with &quot;out of memory&quot; error<br>2. Implement result streaming to limit memory usage<br>3. Add memory limits to query execution configuration</td>\n<td>Failed query; system remains operational</td>\n</tr>\n<tr>\n<td><strong>Invalid query syntax</strong></td>\n<td><code>Parser.ParseQuery()</code> returns syntax error</td>\n<td>1. Return detailed error with line and position<br>2. Suggest corrections for common mistakes<br>3. Log malformed query attempts (rate-limited)</td>\n<td>Query rejected; client must correct syntax</td>\n</tr>\n<tr>\n<td><strong>Missing measurement or field</strong></td>\n<td>Storage engine returns no data for specified measurement</td>\n<td>1. Return empty result set (not an error)<br>2. Log warning if measurement was recently written to (potential data loss indicator)</td>\n<td>Empty query results; system continues normally</td>\n</tr>\n<tr>\n<td><strong>Range query too large</strong></td>\n<td>Query time range exceeds configured <code>maxQueryRange</code></td>\n<td>1. Reject query with &quot;time range too large&quot; error<br>2. Suggest using downsampling or smaller ranges<br>3. Log large query attempt for monitoring</td>\n<td>Query rejected; client must modify parameters</td>\n</tr>\n</tbody></table>\n<h4 id=\"compaction-and-retention-failures\">Compaction and Retention Failures</h4>\n<p>Background maintenance tasks must handle failures gracefully without data loss.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Recovery Strategy</th>\n<th>Impact on Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Compaction interrupted by crash</strong></td>\n<td>Incomplete TSM output file; missing footer</td>\n<td>1. On restart, detect incomplete output files (missing magic footer)<br>2. Delete incomplete output files<br>3. Retry compaction during next cycle<br>4. Preserve all input files until successful completion</td>\n<td>Temporary storage bloat; automatic retry on next cycle</td>\n</tr>\n<tr>\n<td><strong>Disk full during compaction</strong></td>\n<td>Write failure during <code>Execute()</code></td>\n<td>1. Delete partially written output file<br>2. Abort compaction plan<br>3. Trigger emergency disk space alert<br>4. Skip further compactions until space available</td>\n<td>Compaction paused; queries continue; requires admin intervention</td>\n</tr>\n<tr>\n<td><strong>TTL deletion of active file</strong></td>\n<td>Race condition: file compacted after TTL marked it for deletion</td>\n<td>1. Use tombstone markers instead of immediate deletion<br>2. Check file reference count before physical deletion<br>3. Implement grace period for tombstoned files</td>\n<td>No data loss; slight storage overhead during grace period</td>\n</tr>\n<tr>\n<td><strong>Downsampling arithmetic overflow</strong></td>\n<td>Aggregate sum exceeds float64 precision during <code>applyDownsampling()</code></td>\n<td>1. Use Kahan summation algorithm for improved precision<br>2. Log precision warnings for large aggregations<br>3. Return <code>NaN</code> for mathematically invalid operations</td>\n<td>Potential precision loss in downsampled values; system continues</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-specific-edge-cases\">Data-Specific Edge Cases</h3>\n<blockquote>\n<p><strong>Mental Model: The Time Traveler&#39;s Journal</strong><br>Imagine time-series data as entries in a time traveler&#39;s journal. Sometimes entries arrive out of order (the traveler jumps around), sometimes there are gaps (the traveler skips days), and sometimes the dates are ambiguous (different calendar systems). The database must make sense of this chaotic journal while presenting a coherent timeline to readers.</p>\n</blockquote>\n<h4 id=\"out-of-order-data-handling\">Out-of-Order Data Handling</h4>\n<p>Time-series data frequently arrives out of chronological order due to network latency, clock synchronization issues, or batch processing delays.</p>\n<p><strong>Tolerance Window Strategy:</strong>\nTempoDB implements a configurable <strong>tolerance window</strong> (<code>Memtable.maxOutOfOrderWindow</code>) that determines how far back in time a late-arriving point can be inserted into the current memtable. The default is typically 1-5 minutes for metrics collection and up to 1 hour for IoT scenarios.</p>\n<p><strong>Processing Logic for Out-of-Order Writes:</strong></p>\n<ol>\n<li><strong>Timestamp validation</strong>: When <code>WritePoint()</code> receives a point, it compares the point&#39;s timestamp with the current system time (or ingestion time)</li>\n<li><strong>Within tolerance window</strong>: If <code>point.Timestamp &gt;= (now - maxOutOfOrderWindow)</code>, the point is inserted into the active memtable in correct sorted position</li>\n<li><strong>Beyond tolerance window but recent</strong>: If point is older than tolerance window but newer than the oldest point in active memtable, it&#39;s still inserted with a warning log</li>\n<li><strong>Very old data</strong>: If point is older than any point in current memtable but newer than the newest TSM file, a special &quot;late write&quot; handler creates a small TSM file that will be merged during next compaction</li>\n<li><strong>Extremely old data</strong>: Points older than the oldest TSM file are rejected with an error (configurable behavior)</li>\n</ol>\n<p><strong>Example Walkthrough:</strong>\nConsider a system with <code>maxOutOfOrderWindow = 5m</code>, current time = 10:00, and memtable containing points from 09:55 to 09:59:</p>\n<ul>\n<li>Point at 09:57 (3 minutes old) → Inserted into memtable (within window)</li>\n<li>Point at 09:52 (8 minutes old) → Inserted with warning (beyond window but recent)</li>\n<li>Point at 09:30 (30 minutes old) → Written to late-write TSM file</li>\n<li>Point at 08:00 (2 hours old) → Rejected with &quot;timestamp too old&quot; error</li>\n</ul>\n<h4 id=\"gaps-in-time-series\">Gaps in Time Series</h4>\n<p>Real-world time-series data often has gaps—periods where no data was collected or transmitted.</p>\n<p><strong>Query Behavior with Gaps:</strong></p>\n<ul>\n<li><strong>Raw queries</strong>: Return only existing data points, resulting in discontinuous results</li>\n<li><strong>Aggregation queries</strong>: Treat gaps as missing values (not zeroes):<ul>\n<li><code>SUM</code>, <code>COUNT</code>: Only aggregate existing points</li>\n<li><code>AVG</code>: Divide by number of existing points, not total time range</li>\n<li>Time bucket with no data: Return no point for that bucket (not a zero/null point)</li>\n</ul>\n</li>\n</ul>\n<p><strong>Interpolation Considerations:</strong>\nWhile some time-series databases offer interpolation (filling gaps with estimated values), TempoDB explicitly does NOT interpolate because:</p>\n<ol>\n<li>Interpolation assumes a data generation model that may not be valid</li>\n<li>It can mask genuine data collection failures</li>\n<li>It complicates aggregation semantics</li>\n<li>Applications can implement their own interpolation if needed</li>\n</ol>\n<h4 id=\"clock-skew-and-timestamp-anomalies\">Clock Skew and Timestamp Anomalies</h4>\n<p>Distributed systems inevitably have clock differences between data producers.</p>\n<p><strong>Handling Strategies:</strong></p>\n<ol>\n<li><strong>Future timestamp rejection</strong>: Points with timestamps &gt; (current time + <code>maxClockSkew</code>) are rejected</li>\n<li><strong>Ingestion time tracking</strong>: Optionally record ingestion timestamp alongside data timestamp for diagnostics</li>\n<li><strong>NTP enforcement</strong>: Documentation strongly recommends clients use NTP synchronization</li>\n<li><strong>Batch timestamp normalization</strong>: For batch imports, allow overriding timestamps with ingestion time</li>\n</ol>\n<p><strong>Clock Skew Detection Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Detection</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client clock ahead by seconds</td>\n<td>Timestamp &gt; (now + 10s)</td>\n<td>Reject with &quot;timestamp in future&quot; error</td>\n</tr>\n<tr>\n<td>Client clock ahead by minutes/hours</td>\n<td>Timestamp &gt; (now + 1h)</td>\n<td>Reject with &quot;clock skew too large&quot; error</td>\n</tr>\n<tr>\n<td>Client clock slightly behind</td>\n<td>Timestamp &lt; now but within tolerance</td>\n<td>Accept normally</td>\n</tr>\n<tr>\n<td>Client clock far behind</td>\n<td>Timestamp &lt; (oldest TSM file time)</td>\n<td>Reject with &quot;timestamp too old&quot; error</td>\n</tr>\n</tbody></table>\n<h4 id=\"queries-with-very-large-time-ranges\">Queries with Very Large Time Ranges</h4>\n<p>Unbounded or extremely large time range queries can overwhelm the system.</p>\n<p><strong>Protection Mechanisms:</strong></p>\n<ol>\n<li><strong>Configuration limits</strong>: <code>maxQueryRange</code> (default 30 days) rejects overly broad queries</li>\n<li><strong>Memory limits</strong>: Query execution tracks memory usage and cancels if exceeding limit</li>\n<li><strong>Result streaming</strong>: Large results stream incrementally rather than buffering entirely</li>\n<li><strong>Timeout enforcement</strong>: All queries have execution timeout (default 30 seconds)</li>\n<li><strong>Resource prioritization</strong>: Short recent queries prioritized over long historical scans</li>\n</ol>\n<p><strong>Progressive Optimization for Large Queries:</strong>\nWhen a query approaches but doesn&#39;t exceed the maximum range:</p>\n<ol>\n<li><strong>Downsampling suggestion</strong>: Query planner checks if downsampled data exists for the time range</li>\n<li><strong>Storage tier routing</strong>: Direct query to appropriate storage tier (hot/warm/cold)</li>\n<li><strong>Parallel execution</strong>: Split time range into chunks processed in parallel</li>\n<li><strong>Approximate results</strong>: Optionally return approximate aggregates using statistical sketches</li>\n</ol>\n<h4 id=\"boundary-conditions-in-time-based-grouping\">Boundary Conditions in Time-Based Grouping</h4>\n<p><code>GROUP BY time()</code> queries have subtle edge cases at time window boundaries.</p>\n<p><strong>Window Alignment Rules:</strong>\nThe <code>alignToWindow()</code> function aligns timestamps to consistent window boundaries using a fixed epoch (January 1, 1970, 00:00:00 UTC). This ensures deterministic grouping regardless of query start time.</p>\n<p><strong>Boundary Scenarios:</strong></p>\n<ol>\n<li><strong>Partial first window</strong>: If query starts mid-window, first bucket includes only data from start time to next boundary</li>\n<li><strong>Partial last window</strong>: If query ends mid-window, last bucket includes only data from last boundary to end time</li>\n<li><strong>Empty windows</strong>: Windows with no data produce no output point (not a zero/null point)</li>\n<li><strong>Timezone considerations</strong>: All timestamps are UTC internally; timezone conversion happens at API layer if needed</li>\n</ol>\n<p><strong>Example with Daylight Saving Time:</strong>\nConsider <code>GROUP BY time(1h)</code> on March 10, 2024 (daylight saving transition in US/Eastern):</p>\n<ul>\n<li>01:30 EST becomes 03:30 EDT → Still groups into 1-hour UTC buckets consistently</li>\n<li>Application layer handles local time display if needed</li>\n</ul>\n<h4 id=\"data-type-and-precision-edge-cases\">Data Type and Precision Edge Cases</h4>\n<p>Floating-point values present specific challenges for time-series data.</p>\n<table>\n<thead>\n<tr>\n<th>Edge Case</th>\n<th>Problem</th>\n<th>TempoDB Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>NaN values</strong></td>\n<td>Invalid floating-point operations produce NaN</td>\n<td>Gorilla compression cannot handle NaN; reject at ingestion with error</td>\n</tr>\n<tr>\n<td><strong>Infinity values</strong></td>\n<td>Division by zero produces ±Inf</td>\n<td>Reject at ingestion with error</td>\n</tr>\n<tr>\n<td><strong>Extreme values</strong></td>\n<td>Very large/small floats may lose precision</td>\n<td>Store as-is; compression may be less effective</td>\n</tr>\n<tr>\n<td><strong>Precision loss in aggregation</strong></td>\n<td>Repeated summation accumulates error</td>\n<td>Use Kahan summation in <code>WindowAggregator</code> for critical metrics</td>\n</tr>\n<tr>\n<td><strong>Integer overflow in count</strong></td>\n<td><code>COUNT()</code> over billions of points</td>\n<td>Use int64 counters; monitor for overflow</td>\n</tr>\n</tbody></table>\n<h4 id=\"handling-tombstoned-data\">Handling Tombstoned Data</h4>\n<p>When data is deleted or TTL-expired, it&#39;s marked with tombstones rather than immediately removed.</p>\n<p><strong>Tombstone Lifecycle:</strong></p>\n<ol>\n<li><strong>Marking</strong>: File marked <code>Tombstoned=true</code> with <code>TombstoneTime</code> set</li>\n<li><strong>Grace period</strong>: File remains available for queries for <code>DefaultTTLGracePeriod</code> (5 minutes)</li>\n<li><strong>Physical deletion</strong>: After grace period, file deleted during next maintenance cycle</li>\n<li><strong>Recovery</strong>: If system crashes during grace period, tombstones persist and deletion resumes on restart</li>\n</ol>\n<p><strong>Query Behavior with Tombstones:</strong></p>\n<ul>\n<li><strong>During grace period</strong>: Queries skip tombstoned data (invisible to users)</li>\n<li><strong>Compaction</strong>: Tombstoned files are excluded from compaction plans</li>\n<li><strong>Emergency restore</strong>: Administrator can manually remove tombstone marker before grace period expires</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error monitoring</td>\n<td>Log files with structured JSON</td>\n<td>OpenTelemetry integration with metrics export</td>\n</tr>\n<tr>\n<td>Crash recovery</td>\n<td>WAL replay on startup</td>\n<td>Point-in-time recovery with snapshotting</td>\n</tr>\n<tr>\n<td>Disk monitoring</td>\n<td>Periodic <code>statfs()</code> calls</td>\n<td>Inotify/FANOTIFY for real-time space alerts</td>\n</tr>\n<tr>\n<td>Memory limits</td>\n<td>Go&#39;s runtime memory stats</td>\n<td>Cgroup integration for container environments</td>\n</tr>\n</tbody></table>\n<h4 id=\"error-handling-infrastructure\">Error Handling Infrastructure</h4>\n<p>Create an error hierarchy and recovery utilities in <code>internal/errors/</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/errors/errors.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ErrorSeverity indicates how severe an error is</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ErrorSeverity</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SeverityDebug</span><span style=\"color:#B392F0\"> ErrorSeverity</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SeverityInfo</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SeverityWarning</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SeverityError</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SeverityCritical</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TempoError is the base error type for all TempoDB errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TempoError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Code       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Severity   </span><span style=\"color:#B392F0\">ErrorSeverity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    InnerError </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StackTrace []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTempoError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">code</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">severity</span><span style=\"color:#B392F0\"> ErrorSeverity</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">inner</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TempoError</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stack </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> runtime.</span><span style=\"color:#B392F0\">Stack</span><span style=\"color:#E1E4E8\">(stack, </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TempoError</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Code:       code,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Message:    message,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Severity:   severity,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Component:  component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Operation:  operation,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        InnerError: inner,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        StackTrace: stack[:n],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp:  time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UTC</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TempoError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"[</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">] </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> (component: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, operation: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        e.Code, e.Severity, e.Message, e.Component, e.Operation)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IsDiskFullError checks if error is due to disk full</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> IsDiskFullError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check for ENOSPC or similar disk full errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#9ECBFF\">\"no space\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">||</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#9ECBFF\">\"ENOSPC\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">||</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#9ECBFF\">\"disk full\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Recovery actions for different error types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RecoveryAction</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Description </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Execute     </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetRecoveryActions returns recommended recovery actions for an error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> GetRecoveryActions</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">RecoveryAction</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> actions []</span><span style=\"color:#B392F0\">RecoveryAction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check error type and severity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For disk full errors, suggest emergency compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For corrupt file errors, suggest quarantine procedure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For memory pressure, suggest flush and backpressure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return appropriate recovery actions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> actions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"wal-corruption-recovery-skeleton\">WAL Corruption Recovery Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/wal/recovery.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> wal</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecoverFromCrash recovers unflushed data from WAL after a crash</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WAL</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecoverFromCrash</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> recoveredPoints </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: List all WAL segment files in directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Sort by sequence number (embedded in filename)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check for checkpoint file to know where to start recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each segment file (starting from checkpoint):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Open segment with Segment.Scan()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. For each entry, parse into series key and data point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. Insert into recovery memtable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   d. When memtable reaches threshold, flush to TSM</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   e. Update recoveredPoints counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Delete or archive processed segment files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return count of recovered points</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> recoveredPoints, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RepairCorruptSegment attempts to salvage data from a corrupt WAL segment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> RepairCorruptSegment</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">segmentPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Open file read-only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Read file in chunks, looking for valid entry boundaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each potentially valid entry, verify CRC32</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Collect valid entries into recovery buffer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return recovered data or error if nothing salvageable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"repair not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"query-timeout-and-cancellation\">Query Timeout and Cancellation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/query/executor.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteQueryWithTimeout executes a query with configurable timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Executor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteQueryWithTimeout</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">plan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">QueryPlan</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create timeout context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeoutCtx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(ctx, timeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Execute with proper resource tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resultChan </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errorChan </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">executePlan</span><span style=\"color:#E1E4E8\">(timeoutCtx, plan)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            errorChan </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            resultChan </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">resultChan:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">errorChan:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#E1E4E8\">{}, err</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">timeoutCtx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Cancel all iterators and release resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Log timeout with query details for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Return timeout error to client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#E1E4E8\">{}, timeoutCtx.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// executePlan with proper resource cleanup on cancellation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Executor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">executePlan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">plan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">QueryPlan</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check context cancellation periodically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Use defer to ensure iterators are closed even on panic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Implement memory tracking with periodic checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Stream results rather than buffer entirely</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Example iterator loop with cancellation check:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> scanner.</span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            scanner.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#E1E4E8\">{}, ctx.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Process point</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"disk-space-monitoring-and-emergency-actions\">Disk Space Monitoring and Emergency Actions</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/disk_monitor.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">syscall</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DiskMonitor periodically checks disk space and takes emergency actions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DiskMonitor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dataDir      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    threshold    </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\"> // e.g., 0.90 for 90% full</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checkInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh       </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins periodic disk monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiskMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> dm.</span><span style=\"color:#B392F0\">monitorLoop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// monitorLoop checks disk space and triggers emergency actions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiskMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">monitorLoop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(dm.checkInterval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            dm.</span><span style=\"color:#B392F0\">checkDiskSpace</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">dm.stopCh:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiskMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkDiskSpace</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get disk usage statistics using syscall.Statfs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate used percentage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If above warning threshold, log warning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If above critical threshold:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Trigger emergency compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Enter read-only mode if space critically low</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. Alert monitoring system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If below threshold after being above, resume normal operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// emergencyCompaction triggers aggressive compaction to free space</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiskMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">emergencyCompaction</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get list of TSM files sorted by size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Select small files for aggressive compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Run compaction with highest priority</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Delete source files immediately after successful compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return freed space amount</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<ol>\n<li><p><strong>Go Error Wrapping</strong>: Use <code>fmt.Errorf(&quot;...: %w&quot;, err)</code> to wrap errors with context, enabling <code>errors.Is()</code> and <code>errors.As()</code> checks later.</p>\n</li>\n<li><p><strong>Resource Cleanup</strong>: Always use <code>defer</code> for resource cleanup (file closing, mutex unlocking, iterator closing) to prevent leaks during panics.</p>\n</li>\n<li><p><strong>Context Propagation</strong>: Pass <code>context.Context</code> through all long-running operations and check <code>ctx.Done()</code> in loops.</p>\n</li>\n<li><p><strong>Memory Monitoring</strong>: Use <code>runtime.ReadMemStats()</code> to track memory usage and trigger backpressure before OOM kills the process.</p>\n</li>\n<li><p><strong>Graceful Shutdown</strong>: Implement signal handling for SIGTERM/SIGINT to flush memtables and close files properly.</p>\n</li>\n<li><p><strong>Recover from Panics</strong>: Use <code>recover()</code> in goroutine entry points to log panics and restart the goroutine if appropriate.</p>\n</li>\n</ol>\n<h4 id=\"milestone-checkpoint-error-handling-validation\">Milestone Checkpoint: Error Handling Validation</h4>\n<p>After implementing error handling, verify with these tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test 1: Disk full simulation</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> dd</span><span style=\"color:#9ECBFF\"> if=/dev/zero</span><span style=\"color:#9ECBFF\"> of=/tmp/tempodb_test/fill.disk</span><span style=\"color:#9ECBFF\"> bs=1M</span><span style=\"color:#9ECBFF\"> count=</span><span style=\"color:#79B8FF\">1000</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestDiskFullRecovery</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 2: Corrupt WAL recovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> cp</span><span style=\"color:#9ECBFF\"> testdata/corrupt.wal</span><span style=\"color:#9ECBFF\"> /tmp/tempodb_test/wal/segment-0001.wal</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> ./tempodb-server</span><span style=\"color:#79B8FF\"> --data-dir</span><span style=\"color:#9ECBFF\"> /tmp/tempodb_test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should log recovery attempt and continue startup</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 3: Query timeout</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> curl</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/query?q=SELECT+*+FROM+cpu+WHERE+time+>+now()-365d\"</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># After 30 seconds (default timeout), should receive timeout response</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test 4: Out-of-order write handling</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> ./test_out_of_order.py</span><span style=\"color:#79B8FF\"> --tolerance-window=5m</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should accept points within window, warn on older points, reject very old points</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips-for-common-error-scenarios\">Debugging Tips for Common Error Scenarios</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Writes hang indefinitely</strong></td>\n<td>Disk full; WAL cannot <code>fsync</code></td>\n<td>Check disk space; look for &quot;no space&quot; in logs</td>\n<td>Free disk space; emergency compaction</td>\n</tr>\n<tr>\n<td><strong>Query returns partial data</strong></td>\n<td>Corrupt TSM block skipped</td>\n<td>Check logs for &quot;corrupt block&quot; or &quot;CRC mismatch&quot;</td>\n<td>Restore from backup; check hardware</td>\n</tr>\n<tr>\n<td><strong>High memory usage</strong></td>\n<td>Cardinality explosion; large queries</td>\n<td>Check series index size; monitor query memory</td>\n<td>Limit series cardinality; add query memory limits</td>\n</tr>\n<tr>\n<td><strong>Clock skew warnings</strong></td>\n<td>Client clocks not synchronized</td>\n<td>Check client NTP status; compare timestamps</td>\n<td>Enforce NTP on clients; adjust <code>maxClockSkew</code></td>\n</tr>\n<tr>\n<td><strong>Compaction not running</strong></td>\n<td>Disk near full; previous failure</td>\n<td>Check compaction logs; disk space</td>\n<td>Address disk space issue; manual compaction trigger</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides the verification framework for all five milestones, ensuring each component meets its acceptance criteria through systematic testing approaches and concrete milestone checkpoints.</p>\n</blockquote>\n<p>The testing strategy for TempoDB must validate both functional correctness and performance characteristics across all system components. Time-series databases present unique testing challenges due to their focus on temporal data patterns, compression algorithms, and high-volume ingestion. This section outlines a comprehensive approach combining traditional testing methods with specialized techniques for time-series systems, providing clear checkpoints for each milestone.</p>\n<h3 id=\"mental-model-the-scientific-laboratory\">Mental Model: The Scientific Laboratory</h3>\n<p>Think of testing TempoDB like running a scientific laboratory. Each component represents an experimental apparatus that must be validated individually (unit tests) and in concert with others (integration tests). We create controlled experiments (property-based tests) to verify fundamental laws of data preservation (no data loss, correct aggregation). Performance benchmarks are our stress tests, measuring how the apparatus behaves under extreme conditions. The milestone checkpoints are our peer reviews—concrete demonstrations that each apparatus performs its intended function before moving to the next phase of the larger experiment.</p>\n<h3 id=\"testing-approaches-and-property-verification\">Testing Approaches and Property Verification</h3>\n<p>Effective testing for a time-series database requires a layered approach that addresses specific characteristics of temporal data, compression, and high-throughput systems. The following table outlines the primary testing strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Testing Approach</th>\n<th>When to Apply</th>\n<th>Key Techniques</th>\n<th>TempoDB-Specific Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Unit Testing</strong></td>\n<td>During component development</td>\n<td>• Mock dependencies<br>• Table-driven tests<br>• Edge case exploration</td>\n<td>Test compression/decompression round-trips, time range calculations, and series key hashing</td>\n</tr>\n<tr>\n<td><strong>Integration Testing</strong></td>\n<td>After component interfaces stabilize</td>\n<td>• Component composition<br>• End-to-end workflows<br>• Database lifecycle tests</td>\n<td>Test WAL recovery after crashes, query execution across multiple TSM files, compaction workflows</td>\n</tr>\n<tr>\n<td><strong>Property-Based Testing</strong></td>\n<td>For algorithmic components</td>\n<td>• Hypothesis testing<br>• Random input generation<br>• Invariant verification</td>\n<td>Verify compression never loses data, aggregations are associative, time windows align correctly</td>\n</tr>\n<tr>\n<td><strong>Golden File Testing</strong></td>\n<td>For file format stability</td>\n<td>• Versioned reference files<br>• Binary comparison<br>• Forward/backward compatibility</td>\n<td>Ensure TSM file format stability, WAL segment structure consistency</td>\n</tr>\n<tr>\n<td><strong>Fuzz Testing</strong></td>\n<td>For robustness validation</td>\n<td>• Random mutation of inputs<br>• Protocol fuzzing<br>• Memory corruption simulation</td>\n<td>Fuzz query parser with malformed queries, ingest API with corrupt line protocol</td>\n</tr>\n<tr>\n<td><strong>Performance Benchmarking</strong></td>\n<td>For acceptance criteria</td>\n<td>• Throughput measurement<br>• Latency percentiles<br>• Memory/CPU profiling</td>\n<td>Verify thousands of points per second write throughput, sub-second query latency for common ranges</td>\n</tr>\n<tr>\n<td><strong>Concurrency Testing</strong></td>\n<td>For thread-safe components</td>\n<td>• Race condition detection<br>• Deadlock detection<br>• Stress testing with goroutines</td>\n<td>Test concurrent writes to same series, simultaneous queries during compaction</td>\n</tr>\n<tr>\n<td><strong>Recovery Testing</strong></td>\n<td>For durability guarantees</td>\n<td>• Simulated crashes<br>• Disk full scenarios<br>• Corrupt file handling</td>\n<td>Verify WAL recovery replays all acknowledged writes, TSM file corruption detection</td>\n</tr>\n</tbody></table>\n<h4 id=\"property-verification-for-critical-components\">Property Verification for Critical Components</h4>\n<p>Certain TempoDB components require specialized verification approaches to ensure they maintain essential invariants:</p>\n<p><strong>Compression Algorithms Invariants:</strong></p>\n<ul>\n<li><strong>Losslessness</strong>: For any sequence of timestamps <code>T</code> and values <code>V</code>, <code>decompress(compress(T, V)) == (T, V)</code></li>\n<li><strong>Monotonicity Preservation</strong>: If timestamps are strictly increasing, decompressed timestamps maintain this order</li>\n<li><strong>Value Fidelity</strong>: Floating-point values maintain their exact binary representation after compression/decompression cycles</li>\n</ul>\n<p><strong>Time Window Alignment Properties:</strong></p>\n<ul>\n<li><strong>Deterministic Bucketing</strong>: For any timestamp <code>t</code> and window duration <code>d</code>, <code>alignToWindow(t, d)</code> always returns the same result</li>\n<li><strong>Contiguity</strong>: Sequential windows have no gaps: <code>alignToWindow(t, d) + d == alignToWindow(t + d, d)</code></li>\n<li><strong>Idempotence</strong>: Applying alignment twice yields same result: <code>alignToWindow(alignToWindow(t, d), d) == alignToWindow(t, d)</code></li>\n</ul>\n<p><strong>Aggregation Function Properties:</strong></p>\n<ul>\n<li><strong>Associativity</strong>: For commutative aggregations (sum, count), order of application doesn&#39;t matter</li>\n<li><strong>Identity Elements</strong>: Empty window aggregations yield appropriate zero values (0 for sum, NaN for avg, etc.)</li>\n<li><strong>Monotonicity</strong>: For ordered aggregations (min, max), results respect the partial order of input values</li>\n</ul>\n<blockquote>\n<p><strong>Key Insight:</strong> Property-based testing is particularly valuable for time-series databases because it can automatically generate edge cases like clock jumps, out-of-order points, and NaN values that developers might not think to test manually.</p>\n</blockquote>\n<h4 id=\"golden-file-testing-for-storage-format\">Golden File Testing for Storage Format</h4>\n<p>The TSM file format represents a contract between different versions of TempoDB. Golden file testing ensures this contract remains stable:</p>\n<ol>\n<li><strong>Reference File Creation</strong>: Generate TSM files with known content using a specific version</li>\n<li><strong>Binary Comparison</strong>: Compare new files byte-for-byte with reference files</li>\n<li><strong>Round-Trip Verification</strong>: Write → Read → Verify data matches original</li>\n<li><strong>Version Compatibility</strong>: Ensure new readers can read old files and vice versa</li>\n</ol>\n<p>The following table defines the golden file test matrix:</p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Input Data</th>\n<th>Expected TSM File Characteristics</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Empty series</td>\n<td>No data points</td>\n<td>Minimal file with only header and empty index</td>\n<td>File size matches expected, header valid</td>\n</tr>\n<tr>\n<td>Single block</td>\n<td>1000 sequential points</td>\n<td>One data block, index with single entry</td>\n<td>Block count = 1, all points recoverable</td>\n</tr>\n<tr>\n<td>Multiple series</td>\n<td>5 series × 500 points each</td>\n<td>Index with 5 entries, interleaved blocks</td>\n<td>Each series retrievable independently</td>\n</tr>\n<tr>\n<td>Max block size</td>\n<td>Points exceeding <code>DefaultMaxPointsPerBlock</code></td>\n<td>Multiple blocks per series</td>\n<td>Block count = ceil(points/maxPoints)</td>\n</tr>\n<tr>\n<td>Mixed timestamps</td>\n<td>Random, non-sequential timestamps</td>\n<td>Out-of-order points sorted in block</td>\n<td>Retrieved points in timestamp order</td>\n</tr>\n<tr>\n<td>Special values</td>\n<td>NaN, ±Inf, extremely large/small floats</td>\n<td>Gorilla compression handles special cases</td>\n<td>Values preserved exactly after round-trip</td>\n</tr>\n</tbody></table>\n<h4 id=\"fuzz-testing-for-query-language\">Fuzz Testing for Query Language</h4>\n<p>The query parser and executor must handle malformed input gracefully. Fuzz testing generates random valid and invalid queries to uncover crashes or incorrect behavior:</p>\n<p><strong>Query Fuzzing Strategy:</strong></p>\n<ol>\n<li><strong>Grammar-Based Generation</strong>: Create queries that follow the grammar but with random components</li>\n<li><strong>Mutation Testing</strong>: Take valid queries and mutate parts (change operators, remove tokens)</li>\n<li><strong>Protocol Fuzzing</strong>: Test HTTP API with malformed JSON, incorrect headers, large payloads</li>\n</ol>\n<p><strong>Critical Invariants for Fuzzed Queries:</strong></p>\n<ul>\n<li>No panic or crash on any input (malformed or valid)</li>\n<li>Malformed queries return descriptive error messages, not empty results</li>\n<li>Memory bounds respected even for pathological queries</li>\n<li>Query timeout enforcement prevents runaway execution</li>\n</ul>\n<h3 id=\"common-pitfalls-in-time-series-database-testing\">Common Pitfalls in Time-Series Database Testing</h3>\n<p>⚠️ <strong>Pitfall: Testing Only Sequential Timestamps</strong>\nTesting with perfectly sequential timestamps misses edge cases like clock jumps, duplicate timestamps, and out-of-order data. Real-world time-series data includes irregularities that the system must handle gracefully.</p>\n<p><strong>Why it&#39;s wrong</strong>: Systems that pass tests with perfect data may fail catastrophically with real-world data containing clock adjustments, batch backfills, or multi-source ingestion.</p>\n<p><strong>How to fix</strong>: Include test cases with:</p>\n<ul>\n<li>Timestamps that jump forward/backward (simulating NTP adjustments)</li>\n<li>Duplicate timestamps (same nanosecond from different sources)</li>\n<li>Gaps in data (missing periods)</li>\n<li>Out-of-order points within the tolerance window</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring Floating-Point Edge Cases</strong>\nTesting with &quot;nice&quot; floating-point values (1.0, 2.5, etc.) misses issues with special IEEE 754 values and precision boundaries.</p>\n<p><strong>Why it&#39;s wrong</strong>: Gorilla compression must handle NaN, ±Inf, denormalized numbers, and values near floating-point precision boundaries correctly.</p>\n<p><strong>How to fix</strong>: Create comprehensive floating-point test vectors:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">plaintext</span><pre class=\"arch-pre shiki-highlighted\"><code>NaN, -NaN, +Inf, -Inf, 0.0, -0.0\nDenormalized: 1e-310, -1e-310\nBoundary values: max float64, -max float64, min positive normal\nPrecision transition: 1.0 + 2^-52, 1.0 + 2^-53</code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Testing Compression Ratio Instead of Correctness</strong>\nFocusing on achieving specific compression ratios rather than verifying lossless compression.</p>\n<p><strong>Why it&#39;s wrong</strong>: Compression algorithms should never lose data, even if compression ratio suffers. Testing for ratios can mask data corruption bugs.</p>\n<p><strong>How to fix</strong>: Always test round-trip correctness first. Compression ratio tests should be separate benchmarks, not correctness tests.</p>\n<p>⚠️ <strong>Pitfall: Single-Threaded Performance Testing</strong>\nMeasuring performance with single-threaded writes/queries that don&#39;t reflect real concurrent load.</p>\n<p><strong>Why it&#39;s wrong</strong>: Real systems experience concurrent writes from multiple sources and parallel queries. Single-threaded tests miss lock contention, cache coherency overhead, and memory barrier costs.</p>\n<p><strong>How to fix</strong>: Design performance tests that simulate:</p>\n<ul>\n<li>Multiple concurrent writers (different goroutines writing to different series)</li>\n<li>Mixed read/write workloads (queries while ingesting)</li>\n<li>Concurrent compactions during queries</li>\n</ul>\n<h4 id=\"adr-choosing-test-frameworks-and-approaches\">ADR: Choosing Test Frameworks and Approaches</h4>\n<blockquote>\n<p><strong>Decision: Comprehensive Testing Pyramid with Go-Native Tooling</strong></p>\n<ul>\n<li><strong>Context</strong>: TempoDB needs robust testing across multiple dimensions (correctness, performance, concurrency) while maintaining fast feedback cycles for developers. The Go ecosystem provides excellent testing tools that integrate seamlessly.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Minimalist approach</strong>: Standard Go testing package only</li>\n<li><strong>Comprehensive external frameworks</strong>: Bring in multiple specialized testing libraries (testify, ginkgo, goconvey)</li>\n<li><strong>Hybrid native approach</strong>: Go testing package augmented with carefully selected single-purpose libraries for property testing and HTTP testing</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid native approach using <code>testing</code> package + <code>quick</code> for property testing + <code>httptest</code> for API tests</li>\n<li><strong>Rationale</strong>: The standard <code>testing</code> package is sufficient for most needs and keeps dependencies minimal. <code>quick</code> provides property testing without external dependencies. <code>httptest</code> is part of the standard library. This approach minimizes cognitive overhead and build complexity while providing necessary testing capabilities.</li>\n<li><strong>Consequences</strong>: Developers need to write more boilerplate for assertions but gain deeper understanding of test failures. The test suite remains fast and integrated with standard Go tooling.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Minimalist (testing only)</td>\n<td>• Zero dependencies<br>• Consistent with Go idioms<br>• Fast compilation</td>\n<td>• Verbose assertion code<br>• No property testing<br>• Limited HTTP test utilities</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Comprehensive frameworks</td>\n<td>• Rich assertion libraries<br>• BDD-style syntax available<br>• Integrated utilities</td>\n<td>• Multiple dependencies<br>• Steeper learning curve<br>• Potential version conflicts</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Hybrid native approach</td>\n<td>• Minimal dependencies<br>• Property testing support<br>• Standard HTTP testing<br>• Go-native patterns</td>\n<td>• More boilerplate for assertions<br>• No BDD syntax</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h3 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h3>\n<p>Each milestone includes specific acceptance criteria that must be verified through executable tests and manual validation. These checkpoints provide concrete steps to confirm successful implementation.</p>\n<h4 id=\"milestone-1-storage-engine-verification\">Milestone 1: Storage Engine Verification</h4>\n<p><strong>Objective</strong>: Validate TSM file format, compression algorithms, and block-based storage.</p>\n<p><strong>Verification Checklist:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> TSM files can be written and read correctly</li>\n<li><input disabled=\"\" type=\"checkbox\"> Delta-of-delta compression reduces storage for sequential timestamps</li>\n<li><input disabled=\"\" type=\"checkbox\"> Gorilla compression maintains floating-point precision</li>\n<li><input disabled=\"\" type=\"checkbox\"> Block index correctly maps time ranges to file offsets</li>\n<li><input disabled=\"\" type=\"checkbox\"> Memory-mapped files enable efficient random access</li>\n</ul>\n<p><strong>Validation Commands and Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run storage engine unit tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestTSM</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestTSMWriteReadRoundTrip</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestTSMWriteReadRoundTrip</span><span style=\"color:#E1E4E8\"> (0.08s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestDeltaOfDeltaCompression</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestDeltaOfDeltaCompression</span><span style=\"color:#E1E4E8\"> (0.12s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestGorillaCompression</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestGorillaCompression</span><span style=\"color:#E1E4E8\"> (0.15s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestMemoryMappedAccess</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestMemoryMappedAccess</span><span style=\"color:#E1E4E8\"> (0.05s)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">PASS</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Generate and inspect a TSM file</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/inspect/main.go</span><span style=\"color:#79B8FF\"> --file</span><span style=\"color:#9ECBFF\"> testdata/sample.tsm</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">TSM</span><span style=\"color:#9ECBFF\"> File:</span><span style=\"color:#9ECBFF\"> testdata/sample.tsm</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Version:</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Series</span><span style=\"color:#9ECBFF\"> Count:</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Total</span><span style=\"color:#9ECBFF\"> Blocks:</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">File</span><span style=\"color:#9ECBFF\"> Size:</span><span style=\"color:#79B8FF\"> 45.2</span><span style=\"color:#9ECBFF\"> KB</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Compression</span><span style=\"color:#9ECBFF\"> Ratio:</span><span style=\"color:#9ECBFF\"> 4.7:1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Series:</span><span style=\"color:#9ECBFF\"> cpu,host=server1</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  Blocks:</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\"> (times </span><span style=\"color:#9ECBFF\">2023-01-01T00:00:00Z</span><span style=\"color:#9ECBFF\"> to</span><span style=\"color:#9ECBFF\"> 2023-01-01T01:40:00Z</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  Points:</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  Avg</span><span style=\"color:#9ECBFF\"> block</span><span style=\"color:#9ECBFF\"> size:</span><span style=\"color:#79B8FF\"> 8.1</span><span style=\"color:#9ECBFF\"> KB</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Benchmark compression performance</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/...</span><span style=\"color:#79B8FF\"> -bench=BenchmarkCompression</span><span style=\"color:#79B8FF\"> -benchtime=10s</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">BenchmarkDeltaOfDeltaCompress-8</span><span style=\"color:#79B8FF\">   \t  500000</span><span style=\"color:#79B8FF\">\t     24560</span><span style=\"color:#9ECBFF\"> ns/op</span><span style=\"color:#79B8FF\">\t 407.03</span><span style=\"color:#9ECBFF\"> MB/s</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">BenchmarkGorillaCompress-8</span><span style=\"color:#79B8FF\">        \t  300000</span><span style=\"color:#79B8FF\">\t     38920</span><span style=\"color:#9ECBFF\"> ns/op</span><span style=\"color:#79B8FF\">\t 256.95</span><span style=\"color:#9ECBFF\"> MB/s</span></span></code></pre></div>\n\n<p><strong>Manual Validation Steps:</strong></p>\n<ol>\n<li>Create a test program that writes 10,000 sequential points to a TSM file</li>\n<li>Verify file size is significantly smaller than uncompressed data (expect 4-8x compression)</li>\n<li>Use a hex editor to examine file structure: header, data blocks, index, footer</li>\n<li>Write a simple reader that memory-maps the file and extracts points from middle blocks without reading entire file</li>\n<li>Confirm that reading random blocks has constant time complexity regardless of file position</li>\n</ol>\n<h4 id=\"milestone-2-write-path-verification\">Milestone 2: Write Path Verification</h4>\n<p><strong>Objective</strong>: Validate high-throughput ingestion, WAL durability, and memtable management.</p>\n<p><strong>Verification Checklist:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> Write-ahead log ensures durability before acknowledgment</li>\n<li><input disabled=\"\" type=\"checkbox\"> Memtable buffers writes and flushes at size threshold</li>\n<li><input disabled=\"\" type=\"checkbox\"> Batch ingestion improves throughput</li>\n<li><input disabled=\"\" type=\"checkbox\"> Out-of-order writes are handled correctly within tolerance window</li>\n<li><input disabled=\"\" type=\"checkbox\"> Backpressure mechanism prevents unbounded memory growth</li>\n</ul>\n<p><strong>Validation Commands and Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test WAL recovery after simulated crash</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/wal/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestWALRecovery</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestWALRecovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Writing</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#9ECBFF\"> points</span><span style=\"color:#9ECBFF\"> to</span><span style=\"color:#9ECBFF\"> WAL...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Simulating</span><span style=\"color:#9ECBFF\"> crash</span><span style=\"color:#E1E4E8\"> (kill </span><span style=\"color:#79B8FF\">-9</span><span style=\"color:#E1E4E8\">)...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Recovering</span><span style=\"color:#9ECBFF\"> from</span><span style=\"color:#9ECBFF\"> WAL...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Recovered</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#9ECBFF\"> points,</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#9ECBFF\"> lost</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestWALRecovery</span><span style=\"color:#E1E4E8\"> (0.22s)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Benchmark write throughput</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/ingest/...</span><span style=\"color:#79B8FF\"> -bench=BenchmarkWriteThroughput</span><span style=\"color:#79B8FF\"> -benchtime=30s</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">BenchmarkWriteThroughput-8</span><span style=\"color:#79B8FF\">   \t  200000</span><span style=\"color:#79B8FF\">\t    178200</span><span style=\"color:#9ECBFF\"> ns/op</span><span style=\"color:#79B8FF\">   5609</span><span style=\"color:#9ECBFF\"> ops/sec</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">BenchmarkBatchWrite-8</span><span style=\"color:#79B8FF\">        \t  500000</span><span style=\"color:#79B8FF\">\t     54200</span><span style=\"color:#9ECBFF\"> ns/op</span><span style=\"color:#79B8FF\">  18450</span><span style=\"color:#9ECBFF\"> ops/sec</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test backpressure mechanism</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/loadtest/main.go</span><span style=\"color:#79B8FF\"> --points</span><span style=\"color:#79B8FF\"> 1000000</span><span style=\"color:#79B8FF\"> --workers</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#79B8FF\"> --backpressure</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Starting</span><span style=\"color:#9ECBFF\"> load</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> with</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#9ECBFF\"> workers...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">[WARNING] Backpressure engaged at 85% capacity</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Throughput</span><span style=\"color:#9ECBFF\"> limited</span><span style=\"color:#9ECBFF\"> to</span><span style=\"color:#9ECBFF\"> maintain</span><span style=\"color:#9ECBFF\"> stability</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Total</span><span style=\"color:#9ECBFF\"> points:</span><span style=\"color:#9ECBFF\"> 1,000,000</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Successful:</span><span style=\"color:#9ECBFF\"> 1,000,000</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Failed</span><span style=\"color:#E1E4E8\"> (backpressure): 0</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Average</span><span style=\"color:#9ECBFF\"> latency:</span><span style=\"color:#9ECBFF\"> 4.2ms</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">P99</span><span style=\"color:#9ECBFF\"> latency:</span><span style=\"color:#9ECBFF\"> 18.7ms</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify out-of-order handling</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/ingest/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestOutOfOrder</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestOutOfOrderWithinWindow</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestOutOfOrderWithinWindow</span><span style=\"color:#E1E4E8\"> (0.03s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestOutOfOrderBeyondWindow</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestOutOfOrderBeyondWindow</span><span style=\"color:#E1E4E8\"> (0.04s)</span></span></code></pre></div>\n\n<p><strong>Manual Validation Steps:</strong></p>\n<ol>\n<li>Start TempoDB server with small memtable size limit (e.g., 1MB)</li>\n<li>Send writes at increasing rates until memtable fills</li>\n<li>Observe flush to disk and new memtable creation without data loss</li>\n<li>Kill server process abruptly during active writes</li>\n<li>Restart server and verify all acknowledged writes are recoverable from WAL</li>\n<li>Send points with timestamps in random order, verify they&#39;re stored in correct temporal order</li>\n</ol>\n<h4 id=\"milestone-3-query-engine-verification\">Milestone 3: Query Engine Verification</h4>\n<p><strong>Objective</strong>: Validate range queries, aggregations, and filtering performance.</p>\n<p><strong>Verification Checklist:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> Time-range queries return correct points within bounds</li>\n<li><input disabled=\"\" type=\"checkbox\"> Tag filtering uses inverted index efficiently</li>\n<li><input disabled=\"\" type=\"checkbox\"> Aggregation functions produce mathematically correct results</li>\n<li><input disabled=\"\" type=\"checkbox\"> GROUP BY time() creates correct window buckets</li>\n<li><input disabled=\"\" type=\"checkbox\"> Predicate pushdown skips irrelevant blocks</li>\n</ul>\n<p><strong>Validation Commands and Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run query engine tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/query/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> \"TestQuery\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestTimeRangeQuery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestTimeRangeQuery</span><span style=\"color:#E1E4E8\"> (0.07s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestTagFiltering</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestTagFiltering</span><span style=\"color:#E1E4E8\"> (0.05s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestAggregationFunctions</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestAggregationFunctions</span><span style=\"color:#E1E4E8\"> (0.09s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestGroupByTime</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestGroupByTime</span><span style=\"color:#E1E4E8\"> (0.12s)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test predicate pushdown optimization</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/query/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestPredicatePushdown</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestPredicatePushdown</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Query</span><span style=\"color:#9ECBFF\"> scanned</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#9ECBFF\"> of</span><span style=\"color:#79B8FF\"> 20</span><span style=\"color:#9ECBFF\"> blocks</span><span style=\"color:#E1E4E8\"> (75% </span><span style=\"color:#9ECBFF\">reduction</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestPredicatePushdown</span><span style=\"color:#E1E4E8\"> (0.08s)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Benchmark query performance</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/query/...</span><span style=\"color:#79B8FF\"> -bench=BenchmarkQuery</span><span style=\"color:#79B8FF\"> -benchtime=10s</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">BenchmarkRangeQuery-8</span><span style=\"color:#79B8FF\">           \t  100000</span><span style=\"color:#79B8FF\">\t    156800</span><span style=\"color:#9ECBFF\"> ns/op</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">BenchmarkAggregationQuery-8</span><span style=\"color:#79B8FF\">     \t   50000</span><span style=\"color:#79B8FF\">\t    312400</span><span style=\"color:#9ECBFF\"> ns/op</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">BenchmarkGroupByQuery-8</span><span style=\"color:#79B8FF\">         \t   30000</span><span style=\"color:#79B8FF\">\t    521800</span><span style=\"color:#9ECBFF\"> ns/op</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Execute complex query manually</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> \"http://localhost:8086/query\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> \"SELECT mean(value) FROM cpu </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       WHERE host='server1' AND time >= '2023-01-01T00:00:00Z' </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       GROUP BY time(1h) </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       LIMIT 24\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  \"results\"</span><span style=\"color:#79B8FF\">:</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">      \"series\"</span><span style=\"color:#79B8FF\">:</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">          \"name\"</span><span style=\"color:#79B8FF\">:</span><span style=\"color:#9ECBFF\"> \"cpu\",</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">          \"columns\"</span><span style=\"color:#79B8FF\">:</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"time\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"mean\"],</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">          \"values\"</span><span style=\"color:#79B8FF\">:</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            [</span><span style=\"color:#9ECBFF\">\"2023-01-01T00:00:00Z\"</span><span style=\"color:#E1E4E8\">, 45.2]</span><span style=\"color:#B392F0\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            [</span><span style=\"color:#9ECBFF\">\"2023-01-01T01:00:00Z\"</span><span style=\"color:#E1E4E8\">, 46.8]</span><span style=\"color:#B392F0\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            ...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Manual Validation Steps:</strong></p>\n<ol>\n<li>Load dataset with known statistical properties (e.g., 1M points with normal distribution)</li>\n<li>Execute aggregation queries and verify results match pre-computed values</li>\n<li>Create query that spans multiple TSM files, verify all relevant files are accessed</li>\n<li>Test time windows that don&#39;t align with block boundaries, verify correct point inclusion</li>\n<li>Execute query with very selective tag filter, confirm only matching series are scanned</li>\n<li>Test queries with no matching data, verify empty result (not error)</li>\n</ol>\n<h4 id=\"milestone-4-retention-amp-compaction-verification\">Milestone 4: Retention &amp; Compaction Verification</h4>\n<p><strong>Objective</strong>: Validate automatic data lifecycle management and storage optimization.</p>\n<p><strong>Verification Checklist:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> TTL enforcement deletes expired data blocks</li>\n<li><input disabled=\"\" type=\"checkbox\"> Compaction merges small files without data loss</li>\n<li><input disabled=\"\" type=\"checkbox\"> Downsampling reduces granularity while preserving trends</li>\n<li><input disabled=\"\" type=\"checkbox\"> Background processes don&#39;t block foreground operations</li>\n</ul>\n<p><strong>Validation Commands and Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test TTL enforcement</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/retention/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestTTL</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestTTLEnforcement</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Creating</span><span style=\"color:#9ECBFF\"> data</span><span style=\"color:#9ECBFF\"> with</span><span style=\"color:#9ECBFF\"> 1-minute</span><span style=\"color:#9ECBFF\"> TTL...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Waiting</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#9ECBFF\"> minutes</span><span style=\"color:#9ECBFF\"> for</span><span style=\"color:#9ECBFF\"> expiration...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Checking</span><span style=\"color:#9ECBFF\"> data...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Expired</span><span style=\"color:#9ECBFF\"> data</span><span style=\"color:#9ECBFF\"> removed:</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#9ECBFF\"> points</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Active</span><span style=\"color:#9ECBFF\"> data</span><span style=\"color:#9ECBFF\"> retained:</span><span style=\"color:#79B8FF\"> 500</span><span style=\"color:#9ECBFF\"> points</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestTTLEnforcement</span><span style=\"color:#E1E4E8\"> (62.1s)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Monitor compaction progress</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/compaction-monitor/main.go</span><span style=\"color:#79B8FF\"> --watch</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Watching</span><span style=\"color:#9ECBFF\"> compaction</span><span style=\"color:#9ECBFF\"> directory...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">[14:30:00] Level 0: 4 files (</span><span style=\"color:#B392F0\">45.2MB</span><span style=\"color:#E1E4E8\">) → Level 1: 1 file (</span><span style=\"color:#B392F0\">42.8MB</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">[14:45:00] Level 1: 3 files (</span><span style=\"color:#B392F0\">128.4MB</span><span style=\"color:#E1E4E8\">) → Level 2: 1 file (</span><span style=\"color:#B392F0\">124.1MB</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">[15:00:00] Downsampling applied: 1s → 60s granularity</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Storage</span><span style=\"color:#9ECBFF\"> reduction:</span><span style=\"color:#9ECBFF\"> 68%</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify compaction correctness</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/compaction/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestCompactionCorrectness</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestCompactionCorrectness</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Input</span><span style=\"color:#9ECBFF\"> files:</span><span style=\"color:#9ECBFF\"> 5,</span><span style=\"color:#9ECBFF\"> points:</span><span style=\"color:#9ECBFF\"> 50,000</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Output</span><span style=\"color:#9ECBFF\"> file:</span><span style=\"color:#9ECBFF\"> 1,</span><span style=\"color:#9ECBFF\"> points:</span><span style=\"color:#9ECBFF\"> 50,000</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">All</span><span style=\"color:#9ECBFF\"> points</span><span style=\"color:#9ECBFF\"> verified,</span><span style=\"color:#9ECBFF\"> checksums</span><span style=\"color:#9ECBFF\"> match</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestCompactionCorrectness</span><span style=\"color:#E1E4E8\"> (1.24s)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test concurrent access during compaction</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/compaction/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestConcurrentAccess</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestConcurrentAccess</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Writes</span><span style=\"color:#9ECBFF\"> during</span><span style=\"color:#9ECBFF\"> compaction:</span><span style=\"color:#9ECBFF\"> 10,000</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Queries</span><span style=\"color:#9ECBFF\"> during</span><span style=\"color:#9ECBFF\"> compaction:</span><span style=\"color:#79B8FF\"> 500</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">All</span><span style=\"color:#9ECBFF\"> operations</span><span style=\"color:#9ECBFF\"> completed</span><span style=\"color:#9ECBFF\"> successfully</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">No</span><span style=\"color:#9ECBFF\"> data</span><span style=\"color:#9ECBFF\"> corruption</span><span style=\"color:#9ECBFF\"> detected</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestConcurrentAccess</span><span style=\"color:#E1E4E8\"> (3.85s)</span></span></code></pre></div>\n\n<p><strong>Manual Validation Steps:</strong></p>\n<ol>\n<li>Write data with mixed ages (some expired, some current)</li>\n<li>Trigger TTL enforcement and verify only expired data removed</li>\n<li>Create many small TSM files, trigger compaction, verify single larger file created</li>\n<li>While compaction runs, execute concurrent writes and queries, verify no blocking</li>\n<li>Generate high-resolution data, enable downsampling, verify low-resolution version created</li>\n<li>Query downsampled data, verify it approximates original within expected error bounds</li>\n<li>Fill disk to near capacity, verify emergency compaction triggers and frees space</li>\n</ol>\n<h4 id=\"milestone-5-query-language-amp-api-verification\">Milestone 5: Query Language &amp; API Verification</h4>\n<p><strong>Objective</strong>: Validate expressive query interface and API compatibility.</p>\n<p><strong>Verification Checklist:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> Query language parses SELECT with time-range predicates</li>\n<li><input disabled=\"\" type=\"checkbox\"> HTTP API accepts writes in line protocol format</li>\n<li><input disabled=\"\" type=\"checkbox\"> Aggregation functions work within query expressions</li>\n<li><input disabled=\"\" type=\"checkbox\"> Prometheus remote read/write endpoints function correctly</li>\n<li><input disabled=\"\" type=\"checkbox\"> Grafana data source compatibility confirmed</li>\n</ul>\n<p><strong>Validation Commands and Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test query language parsing</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/querylang/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestParser</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestParserValidQueries</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestParserValidQueries</span><span style=\"color:#E1E4E8\"> (0.04s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestParserErrorRecovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestParserErrorRecovery</span><span style=\"color:#E1E4E8\"> (0.03s)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test HTTP API endpoints</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/api/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestAPI</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestWriteEndpoint</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestWriteEndpoint</span><span style=\"color:#E1E4E8\"> (0.06s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestQueryEndpoint</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestQueryEndpoint</span><span style=\"color:#E1E4E8\"> (0.08s)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">===</span><span style=\"color:#9ECBFF\"> RUN</span><span style=\"color:#9ECBFF\">   TestPrometheusRemoteWrite</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">---</span><span style=\"color:#9ECBFF\"> PASS:</span><span style=\"color:#9ECBFF\"> TestPrometheusRemoteWrite</span><span style=\"color:#E1E4E8\"> (0.07s)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify line protocol parsing</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> echo</span><span style=\"color:#9ECBFF\"> 'cpu,host=server1 value=42.5 1672531200000000000'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8086/write</span><span style=\"color:#79B8FF\"> --data-binary</span><span style=\"color:#9ECBFF\"> @-</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">HTTP/1.1</span><span style=\"color:#79B8FF\"> 204</span><span style=\"color:#9ECBFF\"> No</span><span style=\"color:#9ECBFF\"> Content</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test Prometheus compatibility</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/prometheus-test/main.go</span><span style=\"color:#79B8FF\"> --remote-write</span><span style=\"color:#79B8FF\"> --remote-read</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Testing</span><span style=\"color:#9ECBFF\"> Prometheus</span><span style=\"color:#9ECBFF\"> remote</span><span style=\"color:#9ECBFF\"> write...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">1000</span><span style=\"color:#9ECBFF\"> samples</span><span style=\"color:#9ECBFF\"> written</span><span style=\"color:#9ECBFF\"> successfully</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Testing</span><span style=\"color:#9ECBFF\"> Prometheus</span><span style=\"color:#9ECBFF\"> remote</span><span style=\"color:#9ECBFF\"> read...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">1000</span><span style=\"color:#9ECBFF\"> samples</span><span style=\"color:#9ECBFF\"> read</span><span style=\"color:#9ECBFF\"> successfully</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Compatibility:</span><span style=\"color:#9ECBFF\"> PASS</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test Grafana data source</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/grafana-test/main.go</span><span style=\"color:#79B8FF\"> --datasource-test</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Connecting</span><span style=\"color:#9ECBFF\"> to</span><span style=\"color:#9ECBFF\"> Grafana...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Testing</span><span style=\"color:#9ECBFF\"> simple</span><span style=\"color:#9ECBFF\"> query:</span><span style=\"color:#9ECBFF\"> PASS</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Testing</span><span style=\"color:#9ECBFF\"> template</span><span style=\"color:#9ECBFF\"> variables:</span><span style=\"color:#9ECBFF\"> PASS</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Testing</span><span style=\"color:#9ECBFF\"> annotations:</span><span style=\"color:#9ECBFF\"> PASS</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Data</span><span style=\"color:#9ECBFF\"> source</span><span style=\"color:#9ECBFF\"> compatible:</span><span style=\"color:#9ECBFF\"> YES</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Load test with realistic queries</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/query-loadtest/main.go</span><span style=\"color:#79B8FF\"> --queries</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#79B8FF\"> --concurrent</span><span style=\"color:#79B8FF\"> 50</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Executing</span><span style=\"color:#9ECBFF\"> 10,000</span><span style=\"color:#9ECBFF\"> queries</span><span style=\"color:#9ECBFF\"> with</span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#9ECBFF\"> concurrent</span><span style=\"color:#9ECBFF\"> clients...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Successful:</span><span style=\"color:#9ECBFF\"> 9,998</span><span style=\"color:#E1E4E8\"> (99.98%)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Failed:</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\"> (0.02%)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Average</span><span style=\"color:#9ECBFF\"> latency:</span><span style=\"color:#9ECBFF\"> 47ms</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">P95</span><span style=\"color:#9ECBFF\"> latency:</span><span style=\"color:#9ECBFF\"> 132ms</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">Throughput:</span><span style=\"color:#79B8FF\"> 1063</span><span style=\"color:#9ECBFF\"> queries/second</span></span></code></pre></div>\n\n<p><strong>Manual Validation Steps:</strong></p>\n<ol>\n<li>Write points using line protocol via curl, verify success response</li>\n<li>Execute complex query with nested functions, verify correct parsing and execution</li>\n<li>Configure Prometheus to use TempoDB as remote storage, verify data flows correctly</li>\n<li>Set up Grafana with TempoDB data source, create dashboard with multiple panels</li>\n<li>Test malformed queries return helpful error messages, not server errors</li>\n<li>Verify API responds with appropriate CORS headers for web UI compatibility</li>\n<li>Test streaming responses for large result sets, verify memory doesn&#39;t grow linearly</li>\n</ol>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>While testing strategy is primarily about design and approach, implementing effective tests requires careful structure and utilities. This guidance provides foundational testing infrastructure.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing</td>\n<td>Go <code>testing</code> package</td>\n<td>Testify assertion library</td>\n</tr>\n<tr>\n<td>Property Testing</td>\n<td>Go <code>testing/quick</code></td>\n<td>Gopter property-based testing</td>\n</tr>\n<tr>\n<td>HTTP Testing</td>\n<td><code>net/http/httptest</code></td>\n<td>GoMock for HTTP client mocking</td>\n</tr>\n<tr>\n<td>Benchmarking</td>\n<td>Go <code>testing</code> benchmarks</td>\n<td>Custom benchmarking framework</td>\n</tr>\n<tr>\n<td>Concurrency Testing</td>\n<td>Go race detector</td>\n<td>ThreadSanitizer integration</td>\n</tr>\n<tr>\n<td>Code Coverage</td>\n<td><code>go test -cover</code></td>\n<td>SonarQube integration</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-test-file-structure\">B. Recommended Test File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   ├── test-tools/              # Specialized testing utilities\n│   │   ├── tsm-validator/       # Validates TSM file integrity\n│   │   ├── query-fuzzer/        # Generates random queries\n│   │   └── load-generator/      # Generates synthetic load\n│   └── ...\n├── internal/\n│   ├── storage/\n│   │   ├── tsm_test.go          # Unit tests for TSM format\n│   │   ├── compression_test.go  # Property tests for compression\n│   │   └── benchmark_test.go    # Performance benchmarks\n│   ├── ingest/\n│   │   ├── wal_test.go          # WAL recovery tests\n│   │   └── memtable_test.go     # Concurrency tests\n│   ├── query/\n│   │   ├── engine_test.go       # Query execution tests\n│   │   ├── planner_test.go      # Query planning tests\n│   │   └── iterator_test.go     # Iterator pattern tests\n│   ├── retention/\n│   │   ├── ttl_test.go          # TTL enforcement tests\n│   │   └── compaction_test.go   # Compaction correctness tests\n│   ├── api/\n│   │   ├── http_test.go         # HTTP API tests\n│   │   └── protocol_test.go     # Line protocol parsing tests\n│   └── querylang/\n│       ├── parser_test.go       # Parser fuzz tests\n│       └── ast_test.go          # AST validation tests\n└── testdata/                    # Golden files and test datasets\n    ├── golden/\n    │   ├── tsm-v1/              # Reference TSM files\n    │   └── wal-v1/              # Reference WAL segments\n    └── synthetic/\n        ├── normal-dist/         # Normally distributed test data\n        └── random-walk/         # Random walk test data</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Golden File Test Helper</strong> (complete, ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// testutil/golden.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> testutil</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha256</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/hex</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GoldenFile represents a reference file for comparison testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GoldenFile</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Path     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Checksum </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadGoldenFile loads a golden file from testdata directory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadGoldenFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">version</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GoldenFile</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Helper</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"testdata\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"golden\"</span><span style=\"color:#E1E4E8\">, version, name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to load golden file </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, name, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sha256.</span><span style=\"color:#B392F0\">Sum256</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> GoldenFile</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Name:     name,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Path:     path,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Checksum: hex.</span><span style=\"color:#B392F0\">EncodeToString</span><span style=\"color:#E1E4E8\">(hash[:]),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CompareToGolden compares current data to golden file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CompareToGolden</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">golden</span><span style=\"color:#B392F0\"> GoldenFile</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">current</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Helper</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate checksum of current data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    currentHash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sha256.</span><span style=\"color:#B392F0\">Sum256</span><span style=\"color:#E1E4E8\">(current)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    currentChecksum </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hex.</span><span style=\"color:#B392F0\">EncodeToString</span><span style=\"color:#E1E4E8\">(currentHash[:])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> currentChecksum </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> golden.Checksum {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Write diff for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        diffPath </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(t.</span><span style=\"color:#B392F0\">TempDir</span><span style=\"color:#E1E4E8\">(), golden.Name</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">\".diff\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">WriteFile</span><span style=\"color:#E1E4E8\">(diffPath, current, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Current output saved to </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> for comparison\"</span><span style=\"color:#E1E4E8\">, diffPath)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Output differs from golden file </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, golden.Name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Expected checksum: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, golden.Checksum)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Actual checksum:   </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, currentChecksum)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateGoldenFile updates golden file with current data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> UpdateGoldenFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">version</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Helper</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"UPDATE_GOLDEN\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Skip</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Set UPDATE_GOLDEN=1 to update golden files\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dir </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"testdata\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"golden\"</span><span style=\"color:#E1E4E8\">, version)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">MkdirAll</span><span style=\"color:#E1E4E8\">(dir, </span><span style=\"color:#79B8FF\">0755</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to create golden directory: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(dir, name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">WriteFile</span><span style=\"color:#E1E4E8\">(path, data, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to write golden file: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Updated golden file: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Property Test Helper for Compression</strong> (complete, ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// testutil/property.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> testutil</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing/quick</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeSeriesGenerator generates realistic time-series data for property testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeSeriesGenerator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rng </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">rand</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Rand</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTimeSeriesGenerator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seed</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rng: rand.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(rand.</span><span style=\"color:#B392F0\">NewSource</span><span style=\"color:#E1E4E8\">(seed)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateSequentialTimestamps generates n sequential timestamps with possible jitter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateSequentialTimestamps</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">jitter</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamps </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, n)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> start</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> n; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Add jitter (±jitter% of interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        jitterNs </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(interval.</span><span style=\"color:#B392F0\">Nanoseconds</span><span style=\"color:#E1E4E8\">()) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (g.rng.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">()</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">jitter </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> jitter))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamps[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(jitterNs))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> timestamps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateRandomWalkValues generates values following a random walk</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateRandomWalkValues</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">volatility</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    values </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">, n)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> start</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> n; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Random step (±volatility)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        step </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> (g.rng.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">()</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> volatility</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> step</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        values[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateMixedValues generates values including edge cases</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateMixedValues</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    values </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">, n)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> n; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        switch</span><span style=\"color:#E1E4E8\"> g.rng.</span><span style=\"color:#B392F0\">Intn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            values[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">NaN</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            values[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Inf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            values[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Inf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            values[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            values[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">0.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            values[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> g.rng.</span><span style=\"color:#B392F0\">NormFloat64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckProperty runs a property test with helpful error reporting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CheckProperty</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">f</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">quick</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Helper</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">quick</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxCount: </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Rand:     rand.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(rand.</span><span style=\"color:#B392F0\">NewSource</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UnixNano</span><span style=\"color:#E1E4E8\">())),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> quick.</span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(f, config)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Property </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, name, err)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-test-skeleton-code\">D. Core Test Skeleton Code</h4>\n<p><strong>TSM File Format Test</strong> (skeleton with TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/tsm_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestTSMWriteReadRoundTrip</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create test directory using t.TempDir()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate test data: 1000 sequential points with timestamps and values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create TSMWriter and write test data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call Finish() to complete file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Open TSMReader for the created file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Read back all points using ReadBlock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Verify point count matches original</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Compare each timestamp and value for exact equality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Test edge cases: empty series, single point, max block size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Verify file checksum validation works</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestDeltaOfDeltaCompression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate timestamps with different patterns:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - Perfectly sequential (delta constant)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - Increasing intervals (delta increasing)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - Random intervals within bounds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //         - Clock jumps (forward and backward)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Apply delta-of-delta compression to each pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Decompress and verify round-trip correctness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Calculate compression ratio for each pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify monotonic timestamps remain monotonic after compression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test with maximum timestamp values (near uint64 limit)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestGorillaCompressionProperty</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Property: decompress(compress(values)) == values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    testutil.</span><span style=\"color:#B392F0\">CheckProperty</span><span style=\"color:#E1E4E8\">(t, </span><span style=\"color:#9ECBFF\">\"GorillaCompressionRoundTrip\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">values</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Handle special case: empty slice</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(values) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Compress values using Gorilla algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Decompress back to float64 slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Compare decompressed with original</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Special handling for NaN (math.IsNaN)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: Special handling for Inf (math.IsInf)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 7: Use math.Float64bits for exact binary comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 8: Return true only if all values match exactly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#6A737D\"> // Replace with actual implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>WAL Recovery Test</strong> (skeleton with TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/wal/recovery_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestWALRecoveryAfterCrash</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create WAL directory using t.TempDir()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize WAL with SegmentConfig</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write 1000 points to WAL using WriteEntry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Simulate crash by not calling proper shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Create new WAL instance pointing to same directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Call recovery function to scan WAL segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Verify all 1000 points are recovered</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Check that recovery handles partial writes (truncated entries)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Test recovery with multiple segment files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Verify recovered points maintain write order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestConcurrentWALWrites</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create WAL with appropriate configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Launch 10 goroutines writing concurrently</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Each goroutine writes 100 unique points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Use sync.WaitGroup to wait for all writes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Scan WAL to recover all points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify total point count equals 1000 (10×100)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Check for duplicate or missing points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Verify write ordering within each goroutine is preserved</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Run with race detector enabled (go test -race)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Test with varying levels of concurrency (1, 10, 100 goroutines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Query Engine Integration Test</strong> (skeleton with TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/query/integration_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestEndToEndQueryWorkflow</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create temporary database instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Write test data covering multiple time ranges and series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute query with time range predicate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify returned points match expected subset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Test predicate pushdown by counting blocks accessed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Execute aggregation query (SUM, AVG, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Compare results with manually calculated expected values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Test GROUP BY time() with various window sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Verify window alignment correctness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Test query cancellation mid-execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 11: Test memory limits on large result sets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 12: Verify streaming works for large queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<p><strong>Go Testing Best Practices:</strong></p>\n<ul>\n<li>Use <code>t.Helper()</code> in test helper functions to improve error location</li>\n<li>Leverage <code>t.Cleanup()</code> for automatic resource cleanup</li>\n<li>Use table-driven tests with <code>tt := range tests</code> pattern</li>\n<li>For benchmarks, use <code>b.ResetTimer()</code> and <code>b.StopTimer()</code> appropriately</li>\n<li>Enable race detection with <code>go test -race</code> for all concurrency tests</li>\n</ul>\n<p><strong>Time-Series Specific Testing:</strong></p>\n<ul>\n<li>Use <code>time.Now()</code> sparingly in tests—prefer fixed timestamps for reproducibility</li>\n<li>When testing time-based logic, mock time using interfaces for deterministic tests</li>\n<li>For performance tests, warm up caches before starting measurements</li>\n<li>When testing compression, include both synthetic and real-world datasets</li>\n</ul>\n<p><strong>Property Testing Tips:</strong></p>\n<ul>\n<li>Start with small <code>MaxCount</code> values (100) and increase as tests stabilize</li>\n<li>Use custom generators for domain-specific types (timestamps, series keys)</li>\n<li>For floating-point comparisons, use <code>math.Float64bits</code> for exact equality</li>\n<li>Save failing cases to files for deterministic reproduction</li>\n</ul>\n<h4 id=\"f-debugging-tips-table\">F. Debugging Tips Table</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test passes locally but fails in CI</td>\n<td>Timezone differences or non-deterministic test</td>\n<td>Add <code>log.Printf(&quot;Timezone: %v&quot;, time.Local)</code> to test; check for <code>time.Now()</code> usage</td>\n<td>Use fixed timestamps or mock time source</td>\n</tr>\n<tr>\n<td>Compression test fails intermittently</td>\n<td>Floating-point NaN/Inf handling</td>\n<td>Add debug logging showing each value before/after compression</td>\n<td>Ensure special values handled in compression algorithm</td>\n</tr>\n<tr>\n<td>Query returns wrong aggregation results</td>\n<td>Incorrect window alignment</td>\n<td>Log window start/end times and which points are included</td>\n<td>Verify <code>alignToWindow</code> implementation matches specification</td>\n</tr>\n<tr>\n<td>WAL recovery misses some writes</td>\n<td>Partial writes not handled</td>\n<td>Check file size vs. expected; add CRC validation</td>\n<td>Implement WAL entry framing with length prefixes</td>\n</tr>\n<tr>\n<td>Memory leak in tests</td>\n<td>Goroutines not cleaned up</td>\n<td>Use <code>runtime.NumGoroutine()</code> in test cleanup</td>\n<td>Ensure all background jobs have stop mechanisms</td>\n</tr>\n<tr>\n<td>Race condition detected</td>\n<td>Shared mutable state</td>\n<td>Run with <code>-race</code> flag; examine stack traces</td>\n<td>Add appropriate synchronization or use immutable data</td>\n</tr>\n<tr>\n<td>Benchmark results inconsistent</td>\n<td>Cache effects or GC pauses</td>\n<td>Use <code>benchstat</code> tool across multiple runs; check for allocation hotspots</td>\n<td>Increase benchmark count; use <code>testing.AllocsPerRun</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"g-performance-testing-framework\">G. Performance Testing Framework</h4>\n<p>Create a dedicated performance testing framework that can be run independently:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// cmd/benchmark/main.go (complete starter code)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> main</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">flag</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/tempodb/internal/benchmarks</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        benchmark </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"benchmark\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"write\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Benchmark to run (write, query, compression)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        duration  </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"duration\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second, </span><span style=\"color:#9ECBFF\">\"Benchmark duration\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        workers   </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">Int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"workers\"</span><span style=\"color:#E1E4E8\">, runtime.</span><span style=\"color:#B392F0\">NumCPU</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#9ECBFF\">\"Number of concurrent workers\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output    </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"output\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"report.json\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Output file for results\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    flag.</span><span style=\"color:#B392F0\">Parse</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Starting </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> benchmark with </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> workers for </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        *</span><span style=\"color:#E1E4E8\">benchmark, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">workers, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Go version: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, runtime.</span><span style=\"color:#B392F0\">Version</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CPU cores: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, runtime.</span><span style=\"color:#B392F0\">NumCPU</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#B392F0\">benchmarks</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">benchmark {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"write\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> benchmarks.</span><span style=\"color:#B392F0\">RunWriteBenchmark</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">workers, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">duration)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"query\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> benchmarks.</span><span style=\"color:#B392F0\">RunQueryBenchmark</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">workers, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">duration)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"compression\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> benchmarks.</span><span style=\"color:#B392F0\">RunCompressionBenchmark</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">workers, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">duration)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Unknown benchmark: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">benchmark)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Save results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> result.</span><span style=\"color:#B392F0\">Save</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">output); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to save results: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Print summary</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(result.</span><span style=\"color:#B392F0\">Summary</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Compare with baseline if available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> baseline, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> benchmarks.</span><span style=\"color:#B392F0\">LoadBaseline</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"baseline.json\"</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        diff </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> result.</span><span style=\"color:#B392F0\">Compare</span><span style=\"color:#E1E4E8\">(baseline)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Comparison with baseline:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(diff)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Fail if significant regression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> diff.</span><span style=\"color:#B392F0\">HasRegression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.10</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#6A737D\">// 10% regression threshold</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            os.</span><span style=\"color:#B392F0\">Exit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>This comprehensive testing strategy ensures TempoDB meets its functional requirements while maintaining the performance characteristics essential for a production-ready time-series database. Each milestone includes concrete verification steps that progressively build confidence in the system&#39;s correctness and robustness.</p>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All five milestones (this guide provides diagnostic techniques applicable throughout the entire TempoDB implementation journey)</p>\n</blockquote>\n<p>Debugging a time-series database involves unique challenges due to the intersection of high-throughput writes, complex compression algorithms, temporal data semantics, and persistent storage formats. This practical guide helps learners systematically diagnose and fix common implementation bugs, organized by observable symptoms and the underlying components likely responsible. Think of debugging as <strong>digital archaeology</strong>—you&#39;re piecing together clues from logs, file artifacts, and runtime behavior to reconstruct what happened and identify where the implementation diverged from the design.</p>\n<h3 id=\"common-bug-symptoms-and-fixes\">Common Bug Symptoms and Fixes</h3>\n<p>The following table catalogs frequently encountered issues during TempoDB development, organized by the component most likely involved. Each entry includes the observable symptom, probable root cause, diagnostic steps to confirm the hypothesis, and corrective actions to resolve the issue.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Corrective Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Query returns no data for a valid time range</strong></td>\n<td>1. <strong>Incorrect block index min/max timestamps</strong> in TSM files<br>2. <strong>Misaligned time window</strong> in <code>GROUP BY time()</code> queries<br>3. <strong>Missing series key</strong> in storage engine index</td>\n<td>1. Use <code>inspect-tsm</code> tool to verify <code>MinTime</code>/<code>MaxTime</code> in block indexes match actual data<br>2. Check query logs to see if predicate pushdown is skipping blocks incorrectly<br>3. Verify series exists in storage engine&#39;s <code>seriesIndex</code> map</td>\n<td>1. Ensure <code>TSMWriter.Finish()</code> calculates and writes correct min/max per block<br>2. Debug <code>TimeRange.Contains()</code> logic for boundary conditions<br>3. Check WAL recovery properly restores series index on startup</td>\n</tr>\n<tr>\n<td><strong>Write throughput plateaus at low rate</strong></td>\n<td>1. <strong>Synchronous WAL fsync</strong> on every write<br>2. <strong>Lock contention</strong> in memtable insertion<br>3. <strong>Frequent memtable flushes</strong> due to small size threshold</td>\n<td>1. Monitor WAL write latency with detailed timing logs<br>2. Use Go&#39;s <code>pprof</code> mutex profile to identify contended locks<br>3. Check flush frequency and memtable size at flush time</td>\n<td>1. Implement group commit with periodic WAL sync instead of per-write<br>2. Switch to sharded memtables or lock-free data structures<br>3. Increase <code>Config.MaxMemtableSize</code> or implement adaptive sizing</td>\n</tr>\n<tr>\n<td><strong>Data points appear duplicated in query results</strong></td>\n<td>1. <strong>WAL replay</strong> during recovery re-applying already flushed points<br>2. <strong>Compaction merging</strong> incorrectly handling overlapping time ranges<br>3. <strong>Memtable flush</strong> not clearing flushed data properly</td>\n<td>1. Check WAL segment cleanup logic—are segments deleted after flush?<br>2. Inspect compaction logs for duplicate point detection<br>3. Verify <code>Memtable.Flush()</code> returns data AND clears the internal map</td>\n<td>1. Implement WAL segment rotation with precise tracking of flushed offsets<br>2. Add deduplication in <code>mergeSeriesPoints()</code> using point timestamps<br>3. Ensure flush creates new memtable reference instead of modifying in-place</td>\n</tr>\n<tr>\n<td><strong>Storage engine crashes on startup with &quot;invalid magic number&quot;</strong></td>\n<td>1. <strong>TSM file corruption</strong> from incomplete writes during crashes<br>2. <strong>Version mismatch</strong> between reader and writer<br>3. <strong>Memory mapping failure</strong> due to file truncation</td>\n<td>1. Use hex dump to inspect first 4 bytes of corrupted file<br>2. Check <code>TSMHeader.Version</code> written vs. expected<br>3. Verify file size matches expected based on index</td>\n<td>1. Implement write-temp-then-rename pattern for TSM file creation<br>2. Add version compatibility check in <code>OpenTSMReader()</code><br>3. Use file locking during writes or detect incomplete files via footer</td>\n</tr>\n<tr>\n<td><strong>Out-of-order points are silently dropped</strong></td>\n<td>1. <strong>Tolerance window too small</strong> in memtable configuration<br>2. <strong>Timestamp comparison logic</strong> ignoring equal timestamps<br>3. <strong>Flush logic</strong> not checking for late arrivals during flush</td>\n<td>1. Log dropped points with timestamps to see pattern<br>2. Test <code>Memtable.Insert()</code> with points at same timestamp<br>3. Check if flush includes points arriving during flush operation</td>\n<td>1. Increase <code>Memtable.maxOutOfOrderWindow</code> based on data source characteristics<br>2. Fix comparison to handle <code>&lt;=</code> for same timestamp updates<br>3. Implement double-buffering or point redirection during flush</td>\n</tr>\n<tr>\n<td><strong>Aggregation results are mathematically incorrect</strong></td>\n<td>1. <strong>Float precision accumulation</strong> errors in sum/average<br>2. <strong>Window alignment</strong> incorrectly offset<br>3. <strong>Missing values</strong> (NaN/Inf) not handled in aggregates</td>\n<td>1. Test with integer values to isolate float issues<br>2. Debug <code>alignToWindow()</code> with known timestamps<br>3. Check if aggregation functions skip invalid values</td>\n<td>1. Use Kahan summation for <code>AggregateSum</code> of floats<br>2. Ensure window alignment uses fixed epoch (e.g., Unix zero)<br>3. Implement <code>isValidFloat()</code> checks before aggregation</td>\n</tr>\n<tr>\n<td><strong>Memory usage grows unbounded during queries</strong></td>\n<td>1. <strong>Iterator not closing</strong> resources<br>2. <strong>Result caching</strong> without size limits<br>3. <strong>Large time ranges</strong> loading all points into memory</td>\n<td>1. Use Go&#39;s <code>runtime.ReadMemStats</code> to track allocation patterns<br>2. Check for missing <code>SeriesScanner.Close()</code> calls<br>3. Monitor query plan for full scans vs. predicate pushdown</td>\n<td>1. Implement <code>io.Closer</code> on all iterators with defer cleanup<br>2. Add LRU cache with byte-size limits for query results<br>3. Implement chunked streaming in <code>StreamWriter</code></td>\n</tr>\n<tr>\n<td><strong>Compaction causes write stalls</strong></td>\n<td>1. <strong>Synchronous I/O</strong> during compaction blocking writes<br>2. <strong>Too aggressive compaction</strong> triggered by small thresholds<br>3. <strong>Disk space exhaustion</strong> during merge operation</td>\n<td>1. Monitor compaction duration and I/O patterns<br>2. Check compaction trigger conditions and frequency<br>3. Verify <code>DiskMonitor</code> is detecting low disk space</td>\n<td>1. Move compaction to background with rate limiting<br>2. Adjust <code>LevelConfig</code> thresholds based on write volume<br>3. Implement emergency compaction with reserved space</td>\n</tr>\n<tr>\n<td><strong>Prometheus remote write fails with 400 Bad Request</strong></td>\n<td>1. <strong>Protocol buffer parsing</strong> errors<br>2. <strong>Timestamp conversion</strong> issues (ms vs ns)<br>3. <strong>Label sorting</strong> mismatch with Prometheus conventions</td>\n<td>1. Log raw request bytes for debugging<br>2. Compare timestamp formats in parsed data<br>3. Check series key string generation matches Prometheus format</td>\n<td>1. Validate protocol buffer structure before processing<br>2. Convert Prometheus timestamps (ms) to internal format (ns)<br>3. Sort labels alphabetically in <code>SeriesKey.String()</code></td>\n</tr>\n<tr>\n<td><strong>Database becomes unresponsive after running for days</strong></td>\n<td>1. <strong>Memory leak</strong> in goroutines or cached resources<br>2. <strong>File descriptor exhaustion</strong> from open TSM files<br>3. <strong>Background job pile-up</strong> without throttling</td>\n<td>1. Use <code>pprof</code> heap profile to identify leak sources<br>2. Check <code>lsof</code> output for open file counts<br>3. Monitor scheduler job completion times</td>\n<td>1. Add context timeouts to all background operations<br>2. Implement LRU for <code>TSMReader</code> instances with file closing<br>3. Add backpressure to job scheduling based on system load</td>\n</tr>\n</tbody></table>\n<h4 id=\"special-considerations-for-time-series-data\">Special Considerations for Time-Series Data</h4>\n<p>Time-series databases introduce unique edge cases that require specific debugging approaches:</p>\n<p><strong>Clock Skew Between Ingest Sources</strong></p>\n<blockquote>\n<p>When points arrive from distributed systems with unsynchronized clocks, timestamps may jump backwards beyond the tolerance window, causing points to be rejected or stored incorrectly.</p>\n</blockquote>\n<p><em>Diagnosis</em>: Log point timestamps with source identifiers; monitor for temporal anomalies.\n<em>Fix</em>: Implement client-side timestamp validation or server-side clock correction buffers.</p>\n<p><strong>Gaps in Time Series</strong></p>\n<blockquote>\n<p>Natural periods without data (e.g., sensor offline) can cause aggregation windows to produce misleading results or query planning to over-optimize.</p>\n</blockquote>\n<p><em>Diagnosis</em>: Check aggregation results for windows with zero points vs. windows with null results.\n<em>Fix</em>: Distinguish between &quot;no data&quot; and &quot;zero value&quot; in aggregation functions; consider interpolation for specific use cases.</p>\n<p><strong>Extreme Cardinality Explosion</strong></p>\n<blockquote>\n<p>A malformed tag value (like user ID in a tag) can create millions of series, overwhelming memory and storage indexes.</p>\n</blockquote>\n<p><em>Diagnosis</em>: Monitor <code>seriesIndex</code> growth rate; alert on sudden cardinality changes.\n<em>Fix</em>: Implement series creation rate limiting; validate tag keys/values against schema.</p>\n<h3 id=\"debugging-techniques-and-tools\">Debugging Techniques and Tools</h3>\n<p>Effective debugging requires both systematic approaches and specialized tooling. Below are techniques tailored to TempoDB&#39;s architecture, progressing from simple logging to advanced instrumentation.</p>\n<h4 id=\"1-structured-logging-with-component-context\">1. Structured Logging with Component Context</h4>\n<blockquote>\n<p><strong>Mental Model</strong>: Think of logging as adding <strong>breadcrumb trails</strong> through each component—each log entry should let you trace a point&#39;s journey from ingestion to storage to query response.</p>\n</blockquote>\n<p>Implement a structured logging system that includes:</p>\n<ul>\n<li><strong>Component identifier</strong> (<code>storage</code>, <code>query</code>, <code>compaction</code>)</li>\n<li><strong>Operation context</strong> (e.g., <code>series_key=&quot;cpu&quot;, range=&quot;2023-10-01T00:00:00Z/2023-10-01T01:00:00Z&quot;</code>)</li>\n<li><strong>Performance timing</strong> for slow operations</li>\n<li><strong>Error chains</strong> with causalities</li>\n</ul>\n<p><strong>Example Log Output Analysis</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>level=DEBUG ts=2023-10-01T12:00:00Z component=storage operation=WritePoint series=cpu.usages tags=&quot;host:web01&quot; timestamp=1696161600000000000 value=42.5 duration=2ms\nlevel=INFO ts=2023-10-01T12:00:05Z component=memtable operation=Flush series_count=1524 point_count=1258476 size_mb=48 duration=450ms\nlevel=WARN ts=2023-10-01T12:00:10Z component=query operation=ExecuteRangeScan message=&quot;skipping block outside time range&quot; file=00001.tsm series=cpu.usages block_min=1696165200000000000 block_max=1696168800000000000 query_min=1696161600000000000</code></pre></div>\n\n<p><strong>Implementation Strategy</strong>:</p>\n<ul>\n<li>Use a logging interface that supports structured fields (like <code>zap</code> or <code>slog</code>)</li>\n<li>Add trace IDs to correlate operations across components</li>\n<li>Implement log levels that can be dynamically adjusted per component</li>\n</ul>\n<h4 id=\"2-file-inspection-utilities\">2. File Inspection Utilities</h4>\n<p>Build command-line tools to examine internal file formats—these are invaluable for understanding disk state independent of the running database.</p>\n<p><strong>TSM File Inspector (<code>inspect-tsm</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>$ go run cmd/inspect-tsm/main.go --file /data/00001.tsm --detail\nFile: /data/00001.tsm\nMagic: 0x16D1D1A5 ✓\nVersion: 1 ✓\nSize: 45.2 MB\nSeries Count: 284\nBlocks: 1,247\n\nSeries: cpu.usages{host=web01,region=us-west}\n  Block 0: Offset=1024, Size=16.5KB, Points=1024, MinTime=2023-10-01T00:00:00Z, MaxTime=2023-10-01T00:17:04Z\n  Block 1: Offset=17984, Size=15.8KB, Points=1024, MinTime=2023-10-01T00:17:04Z, MaxTime=2023-10-01T00:34:08Z\n  ...\nCompression:\n  Timestamps: Delta-of-delta, 1024 points → 2.1KB (85% reduction)\n  Values: Gorilla XOR, 1024 points → 8.2KB (50% reduction)</code></pre></div>\n\n<p><strong>WAL Segment Inspector (<code>inspect-wal</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>$ go run cmd/inspect-wal/main.go --segment /wal/00000001.wal --limit 10\nSegment: /wal/00000001.wal (FirstID=1, LastID=8427)\nEntry 1: Type=WritePoint, Size=142B, CRC=0xA3F1C8D2 ✓\n  Series: temperature{sensor=thermo01}\n  Point: 2023-10-01T12:00:00Z, 22.5°C\nEntry 2: Type=WritePoint, Size=138B, CRC=0xB8A2D4E1 ✓\n  Series: temperature{sensor=thermo02}\n  Point: 2023-10-01T12:00:00Z, 23.1°C\n...\nEntry 8427: Type=SeriesCreate, Size=89B, CRC=0x9C3F2A1D ✓\n  Series: pressure{sensor=baro01, unit=hPa}</code></pre></div>\n\n<p><strong>Block-Level Debug Tool (<code>decode-block</code>)</strong>:\nFor deep inspection of compression artifacts:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>$ go run cmd/decode-block/main.go --file /data/00001.tsm --series &quot;cpu.usages&quot; --block 0\nBlock 0 for cpu.usages{host=web01}:\nRaw Header: MinTime=1696161600000000000, MaxTime=1696162624000000000\nDecompressed Points (first 10):\n  1696161600000000000 → 42.5\n  1696161601000000000 → 42.7  (delta: +0.2)\n  1696161602000000000 → 43.1  (delta: +0.4)\n  1696161603000000000 → 42.9  (delta: -0.2)\nCompression Artifacts:\n  Timestamp bytes: 2104 (delta-of-delta with first=0, second=1e9)\n  Value bytes: 8192 (Gorilla XOR with leading zero count=12)</code></pre></div>\n\n<h4 id=\"3-performance-profiling-with-go39s-pprof\">3. Performance Profiling with Go&#39;s pprof</h4>\n<p>Go&#39;s built-in profiling tools are essential for identifying bottlenecks in a high-performance database.</p>\n<p><strong>CPU Profiling</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Add to your HTTP server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">import</span><span style=\"color:#9ECBFF\"> _</span><span style=\"color:#9ECBFF\"> \"net/http/pprof\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Profile during write load</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> tool</span><span style=\"color:#9ECBFF\"> pprof</span><span style=\"color:#9ECBFF\"> http://localhost:6060/debug/pprof/profile?seconds=</span><span style=\"color:#79B8FF\">30</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Generate flame graph</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> tool</span><span style=\"color:#9ECBFF\"> pprof</span><span style=\"color:#79B8FF\"> -http=:8080</span><span style=\"color:#9ECBFF\"> /tmp/profile.out</span></span></code></pre></div>\n\n<p>Common patterns to look for:</p>\n<ul>\n<li><strong>High <code>runtime.mallocgc</code> time</strong> indicates excessive allocation—optimize by reusing byte slices, pooling objects</li>\n<li><strong>Contended <code>sync.RWMutex</code> operations</strong> show as flat lines in mutex profile—consider sharding or lock-free structures</li>\n<li><strong>Excessive <code>runtime.growslice</code></strong> suggests slices growing repeatedly—pre-allocate with known capacities</li>\n</ul>\n<p><strong>Heap Memory Profiling</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Capture heap snapshot during query execution</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> tool</span><span style=\"color:#9ECBFF\"> pprof</span><span style=\"color:#9ECBFF\"> http://localhost:6060/debug/pprof/heap</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Compare two heap profiles</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> tool</span><span style=\"color:#9ECBFF\"> pprof</span><span style=\"color:#79B8FF\"> -base</span><span style=\"color:#9ECBFF\"> heap1.pb.gz</span><span style=\"color:#79B8FF\"> -top</span><span style=\"color:#9ECBFF\"> heap2.pb.gz</span></span></code></pre></div>\n\n<p><strong>Goroutine Leak Detection</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Get goroutine dump</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:6060/debug/pprof/goroutine?debug=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> ></span><span style=\"color:#9ECBFF\"> goroutines.txt</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Look for growing goroutine counts in metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Common leak sources:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Unbuffered channels with blocked senders/receivers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Contexts not being canceled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Background jobs not completing</span></span></code></pre></div>\n\n<h4 id=\"4-property-based-testing-for-compression-algorithms\">4. Property-Based Testing for Compression Algorithms</h4>\n<p>Compression bugs often manifest as silent data corruption—points decompress to wrong values. Property-based testing verifies invariants hold across random inputs.</p>\n<p><strong>Round-Trip Invariant</strong>:</p>\n<blockquote>\n<p>For any slice of timestamps and values, compress → decompress should yield identical data.</p>\n</blockquote>\n<p><strong>Compression Ratio Bounds</strong>:</p>\n<blockquote>\n<p>Compressed size should never exceed uncompressed size plus header overhead.</p>\n</blockquote>\n<p><strong>Monotonic Timestamp Preservation</strong>:</p>\n<blockquote>\n<p>If input timestamps are sorted, decompressed timestamps should maintain same order.</p>\n</blockquote>\n<p><strong>Implementation Approach</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// TestDeltaDeltaRoundTrip uses quick.Check to verify compression properties</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestDeltaDeltaRoundTrip</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    property </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timestamps</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(timestamps) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        compressed, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> compressTimestamps</span><span style=\"color:#E1E4E8\">(timestamps)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        decompressed, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> decompressTimestamps</span><span style=\"color:#E1E4E8\">(compressed, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(timestamps))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> slices.</span><span style=\"color:#B392F0\">Equal</span><span style=\"color:#E1E4E8\">(timestamps, decompressed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">quick</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MaxCount: </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Values: </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">values</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">reflect</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rand</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">rand</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Rand</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Generate random but sorted timestamps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> quick.</span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(property, config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Round-trip failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"5-golden-file-testing-for-format-stability\">5. Golden File Testing for Format Stability</h4>\n<p>Golden files capture correct output for known inputs, detecting unintended format changes across versions.</p>\n<p><strong>TSM Format Golden Files</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>testdata/golden/v1/\n├── simple.tsm.golden      # Single series, 1000 points\n├── multi_series.tsm.golden # 10 series, mixed timestamps\n└── edge_cases.tsm.golden   # NaN, Inf, large timestamp jumps</code></pre></div>\n\n<p><strong>Usage Pattern</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run tests normally (compares to golden files)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/tsm/...</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Update golden files after intentional format change</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">UPDATE_GOLDEN</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">1</span><span style=\"color:#B392F0\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/tsm/...</span></span></code></pre></div>\n\n<p><strong>Golden File Contents</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>=== FILE: simple.tsm.golden ===\nMagic: 0x16D1D1A5\nVersion: 1\nSeries: 1\nTotal Blocks: 1\nTotal Points: 1000\nChecksum: 0xA1B2C3D4\n\nSeries: test{tag=value}\n  Block 0:\n    Offset: 1024\n    Size: 16384\n    Points: 1000\n    MinTime: 1609459200000000000 (2021-01-01T00:00:00Z)\n    MaxTime: 1609459201000000000 (2021-01-01T00:16:40Z)\n    CRC: 0xE5F6A7B8</code></pre></div>\n\n<h4 id=\"6-time-travel-debugging-with-record-amp-replay\">6. Time-Travel Debugging with Record &amp; Replay</h4>\n<p>For intermittent bugs, record operations and replay them in a controlled environment.</p>\n<p><strong>Operation Recorder</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> OperationRecorder</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ops []</span><span style=\"color:#B392F0\">LoggedOperation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LoggedOperation</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // \"WritePoint\", \"Query\", \"Compact\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Args      []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#6A737D\"> // Serialized arguments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Result    []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#6A737D\"> // Serialized result/error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Replay Workflow</strong>:</p>\n<ol>\n<li>Enable recording during production-like load</li>\n<li>Capture bug occurrence with full operation trace</li>\n<li>Replay trace in test environment with:<ul>\n<li>Added debug logging</li>\n<li>Race detector enabled</li>\n<li>Stress GOMAXPROCS variations</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"7-visualization-of-internal-state\">7. Visualization of Internal State</h4>\n<p>Create visual representations of database state to identify patterns.</p>\n<p><strong>Memtable Heat Map</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Memtable Utilization (last 5 flushes)\nFlush 5: ████████████████████████████████ 98% (1.2M points)\nFlush 4: ████████████████████▌            65% (0.8M points)\nFlush 3: ██████████████████████████       80% (1.0M points)\nFlush 2: ██████████████████████████████   92% (1.1M points)\nFlush 1: ████████████████████████████████ 99% (1.2M points)</code></pre></div>\n\n<p><strong>TSM File Age Distribution</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>TSM Files by Age (hours)\n0-1:   ████████████████████ 22 files\n1-2:   ████████████        14 files  \n2-4:   █████████████████   18 files\n4-8:   ████████            10 files\n8-16:  ████                5 files\n16-32: ▌                   1 file\n&gt;32:   █████               6 files (candidates for compaction)</code></pre></div>\n\n<h4 id=\"8-stress-testing-with-anomaly-injection\">8. Stress Testing with Anomaly Injection</h4>\n<p>Deliberately introduce failures to verify recovery mechanisms.</p>\n<p><strong>Controlled Chaos Patterns</strong>:</p>\n<ol>\n<li><strong>Random process kills</strong> during write operations</li>\n<li><strong>Disk full simulation</strong> by intercepting write calls</li>\n<li><strong>Clock skew injection</strong> by mocking time functions</li>\n<li><strong>Network partition simulation</strong> for API endpoints</li>\n</ol>\n<p><strong>Recovery Verification Checklist</strong>:</p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> After crash, WAL replay restores all acknowledged writes</li>\n<li><input disabled=\"\" type=\"checkbox\"> Partial TSM files are detected and quarantined</li>\n<li><input disabled=\"\" type=\"checkbox\"> Series index reconstructs correctly from TSM files</li>\n<li><input disabled=\"\" type=\"checkbox\"> Compaction resumes from interrupted state</li>\n<li><input disabled=\"\" type=\"checkbox\"> Query results remain consistent pre/post recovery</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<blockquote>\n<p>This implementation guidance provides concrete tools and techniques for debugging TempoDB. While the main design avoids code, this section bridges to implementation with working debugging utilities.</p>\n</blockquote>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Logging</strong></td>\n<td>Go&#39;s <code>log/slog</code> with structured fields</td>\n<td><code>zap</code> or <code>zerolog</code> with context propagation and sampling</td>\n</tr>\n<tr>\n<td><strong>Profiling</strong></td>\n<td>Built-in <code>net/http/pprof</code> endpoints</td>\n<td>Custom profiling events with <code>expvar</code> and Prometheus metrics</td>\n</tr>\n<tr>\n<td><strong>File Inspection</strong></td>\n<td>Standalone CLI tools with direct file reading</td>\n<td>Integrated HTTP endpoints for live file inspection</td>\n</tr>\n<tr>\n<td><strong>Tracing</strong></td>\n<td>Manual trace IDs in logs</td>\n<td>OpenTelemetry with Jaeger or Grafana Tempo</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td><code>testing</code> package with golden files</td>\n<td>Property-based testing with <code>testing/quick</code>, fuzzing with <code>go test -fuzz</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n├── cmd/\n│   ├── server/                 # Main database server\n│   ├── inspect-tsm/           # TSM file inspection tool\n│   ├── inspect-wal/           # WAL segment inspection tool\n│   └── decode-block/          # Block-level debug tool\n├── internal/\n│   ├── debug/                 # Debugging utilities\n│   │   ├── inspector.go       # File format inspection interfaces\n│   │   ├── recorder.go        # Operation record/replay\n│   │   └── visualizer.go      # State visualization helpers\n│   ├── storage/\n│   │   └── tsm/\n│   │       └── debug.go       # TSM-specific debugging functions\n│   └── wal/\n│       └── debug.go           # WAL-specific debugging functions\n├── testdata/\n│   ├── golden/                # Golden file directory\n│   │   ├── v1/               # Version-specific golden files\n│   │   └── current/          # Symlink to current version\n│   └── fixtures/             # Test data fixtures\n└── tools/                    # Development tools\n    ├── generate-testdata/    # Generate test datasets\n    └── profile-analyzer/     # Custom profile analysis</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-tsm-file-inspector\">C. Infrastructure Starter Code: TSM File Inspector</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// cmd/inspect-tsm/main.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> main</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">tempo/internal/storage/tsm</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(os.Args) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Usage: inspect-tsm &#x3C;tsm-file> [--detail]\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        os.</span><span style=\"color:#B392F0\">Exit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filePath </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.Args[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    detailed </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(os.Args) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> os.Args[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"--detail\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Open TSM file using the same reader as the database</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    reader, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tsm.</span><span style=\"color:#B392F0\">OpenTSMReader</span><span style=\"color:#E1E4E8\">(filePath)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to open TSM file: </span><span style=\"color:#79B8FF\">%v\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        os.</span><span style=\"color:#B392F0\">Exit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> reader.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read and display header</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filePath)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(data) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 12</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"File too small to be valid TSM\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    magic </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.BigEndian.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(data[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.BigEndian.</span><span style=\"color:#B392F0\">Uint64</span><span style=\"color:#E1E4E8\">(data[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"File: </span><span style=\"color:#79B8FF\">%s\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, filepath.</span><span style=\"color:#B392F0\">Base</span><span style=\"color:#E1E4E8\">(filePath))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Magic: 0x</span><span style=\"color:#79B8FF\">%X</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#E1E4E8\">, magic)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> magic </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> tsm.MagicNumber {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✗ (expected 0x</span><span style=\"color:#79B8FF\">%X</span><span style=\"color:#9ECBFF\">)</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, tsm.MagicNumber)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Version: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#E1E4E8\">, version)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> version </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> tsm.Version {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✗ (expected </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">)</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, tsm.Version)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fi, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Stat</span><span style=\"color:#E1E4E8\">(filePath)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Size: </span><span style=\"color:#79B8FF\">%.1f</span><span style=\"color:#9ECBFF\"> MB</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(fi.</span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">())</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Get index to count series and blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> reader.</span><span style=\"color:#B392F0\">Index</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seriesCount </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(index.Entries)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    blockCount </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, entries </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> index.Entries {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        blockCount </span><span style=\"color:#F97583\">+=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(entries)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Series Count: </span><span style=\"color:#79B8FF\">%d\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, seriesCount)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Blocks: </span><span style=\"color:#79B8FF\">%d\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, blockCount)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> detailed {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Display detailed per-series information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> seriesKey, entries </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> index.Entries {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Series: </span><span style=\"color:#79B8FF\">%s\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, seriesKey)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> i, entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> entries {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"  Block </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: Offset=</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">, Size=</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">, Points≈</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">, \"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    i, entry.Offset, entry.Size, entry.Size</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// Approximation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // Read block header for exact min/max</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(data) </span><span style=\"color:#F97583\">></span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(entry.Offset)</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    minTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.BigEndian.</span><span style=\"color:#B392F0\">Uint64</span><span style=\"color:#E1E4E8\">(data[entry.Offset:entry.Offset</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    maxTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> binary.BigEndian.</span><span style=\"color:#B392F0\">Uint64</span><span style=\"color:#E1E4E8\">(data[entry.Offset</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">:entry.Offset</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"MinTime=</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">, MaxTime=</span><span style=\"color:#79B8FF\">%d\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, minTime, maxTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"[header out of bounds]</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-operation-recorder\">D. Core Logic Skeleton: Operation Recorder</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/debug/recorder.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debug</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OperationRecorder captures operations for later replay debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> OperationRecorder</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu     </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    encoder </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">json</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Encoder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enabled </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RecordedOperation</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">       `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `json:\"component\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Arguments </span><span style=\"color:#B392F0\">json</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RawMessage</span><span style=\"color:#9ECBFF\"> `json:\"arguments\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Result    </span><span style=\"color:#B392F0\">json</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RawMessage</span><span style=\"color:#9ECBFF\"> `json:\"result,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Error     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `json:\"error,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">   `json:\"duration_ms\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewOperationRecorder creates a new recorder writing to the given file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewOperationRecorder</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filePath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OperationRecorder</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Open file for appending with os.OpenFile (O_CREATE|O_APPEND|O_WRONLY)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create json.Encoder that writes to the file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write array start token \"[\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return recorder with file and encoder initialized</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Record starts timing an operation and returns a function to complete the recording</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OperationRecorder</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Record</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">opType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">args</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    opID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> generateOperationID</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Serialize arguments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> argsJSON </span><span style=\"color:#B392F0\">json</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RawMessage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> args </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Marshal args to JSON using json.Marshal</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">r.enabled {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        r.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        defer</span><span style=\"color:#E1E4E8\"> r.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Create operation record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        record </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> RecordedOperation</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ID:        opID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timestamp: start,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Type:      opType,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Component: component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Arguments: argsJSON,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Duration:  time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: If err != nil, set record.Error to err.Error()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 7: If result != nil, marshal result to JSON for record.Result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 8: Encode record to JSON file using r.encoder.Encode()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 9: Flush file to ensure write reaches disk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReplayOperations reads recorded operations and executes them through a handler</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ReplayOperations</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filePath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">handler</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">op</span><span style=\"color:#B392F0\"> RecordedOperation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Open file for reading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 11: Read array start token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 12: Create json.Decoder and decode operations until EOF</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 13: For each operation, call handler with the operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 14: Return any error from handler or decoding</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// generateOperationID creates a unique ID for each operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> generateOperationID</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 15: Implement ID generation (e.g., nanosecond timestamp + random suffix)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-go-specific-debugging-hints\">E. Go-Specific Debugging Hints</h4>\n<ol>\n<li><strong>Use <code>runtime.ReadMemStats</code> for Memory Insights</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   var</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MemStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   runtime.</span><span style=\"color:#B392F0\">ReadMemStats</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Alloc=</span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\"> MB, TotalAlloc=</span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\"> MB, Sys=</span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\"> MB, NumGC=</span><span style=\"color:#79B8FF\">%v\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       m.Alloc</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">, m.TotalAlloc</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">, m.Sys</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">, m.NumGC)</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Debug Goroutine Leaks with Stack Dumps</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       for</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Tick</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Minute) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           buf </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           n </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> runtime.</span><span style=\"color:#B392F0\">Stack</span><span style=\"color:#E1E4E8\">(buf, </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           if</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Count</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(buf[:n]), </span><span style=\"color:#9ECBFF\">\"tempo/internal\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 500</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               log.</span><span style=\"color:#B392F0\">Warn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"High goroutine count\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"stacks\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(buf[:n]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }()</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Profile Production Safely</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   // Secure pprof endpoint with auth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   mux </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewServeMux</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/debug/pprof/\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">authMiddleware</span><span style=\"color:#E1E4E8\">(pprof.Index))</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Use <code>sync.Once</code> for Expensive Debug Setup</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   var</span><span style=\"color:#E1E4E8\"> debugSetupOnce </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Once</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   func</span><span style=\"color:#B392F0\"> setupDebugging</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       debugSetupOnce.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">           // Initialize debug endpoints, recorders, etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n\n<h4 id=\"f-milestone-debugging-checkpoints\">F. Milestone Debugging Checkpoints</h4>\n<p><strong>Milestone 1 (Storage Engine)</strong>:</p>\n<ul>\n<li>Run: <code>go run cmd/inspect-tsm/main.go testdata/fixtures/simple.tsm</code></li>\n<li>Expected: Shows valid magic number, version, and at least one series with correct block offsets</li>\n<li>Failure Sign: &quot;invalid magic number&quot; indicates <code>WriteHeader</code> or file corruption bug</li>\n</ul>\n<p><strong>Milestone 2 (Write Path)</strong>:</p>\n<ul>\n<li>Run: <code>go test ./internal/wal/... -v -run TestWALRecovery</code></li>\n<li>Expected: Test passes showing WAL replay recovers all points after simulated crash</li>\n<li>Failure Sign: Points missing after recovery indicates WAL format or flush coordination bug</li>\n</ul>\n<p><strong>Milestone 3 (Query Engine)</strong>:</p>\n<ul>\n<li>Run: <code>go test ./internal/query/... -v -run TestRangeScanWithPredicatePushdown</code></li>\n<li>Expected: Query skips blocks outside time range (visible in debug logs)</li>\n<li>Failure Sign: All blocks scanned indicates predicate pushdown not working</li>\n</ul>\n<p><strong>Milestone 4 (Retention &amp; Compaction)</strong>:</p>\n<ul>\n<li>Run: <code>go test ./internal/compaction/... -v -run TestLevelCompaction</code></li>\n<li>Expected: Files merge correctly, new TSM file created, old files tombstoned</li>\n<li>Failure Sign: Duplicate points or missing data after compaction</li>\n</ul>\n<p><strong>Milestone 5 (Query Language &amp; API)</strong>:</p>\n<ul>\n<li>Run: <code>curl -v &quot;http://localhost:8080/query?q=SELECT mean(value) FROM cpu WHERE time &gt; now() - 1h GROUP BY time(5m)&quot;</code></li>\n<li>Expected: Returns JSON with aggregated results, HTTP 200</li>\n<li>Failure Sign: Parse error or empty results with valid data indicates query parsing/planning bug</li>\n</ul>\n<h4 id=\"g-debugging-tips-for-specific-scenarios\">G. Debugging Tips for Specific Scenarios</h4>\n<p><strong>Scenario: &quot;Database returns wrong values for historical queries&quot;</strong></p>\n<ul>\n<li><em>Diagnose</em>: Use <code>inspect-tsm</code> to check block min/max times match data; verify compression round-trip with <code>decode-block</code></li>\n<li><em>Fix</em>: Ensure <code>compressBlock</code> and <code>decompressTimestamps/Values</code> use same endianness; check timestamp encoding handles large deltas</li>\n</ul>\n<p><strong>Scenario: &quot;Write throughput drops after several hours&quot;</strong></p>\n<ul>\n<li><em>Diagnose</em>: Check goroutine count (leak?), open file descriptors (exhaustion?), memory fragmentation</li>\n<li><em>Fix</em>: Implement <code>TSMReader</code> LRU cache with file closing; profile heap for allocation patterns</li>\n</ul>\n<p><strong>Scenario: &quot;Aggregate SUM doesn&#39;t match manual calculation&quot;</strong></p>\n<ul>\n<li><em>Diagnose</em>: Test with integer values first; check for NaN/Inf contamination; verify window alignment</li>\n<li><em>Fix</em>: Implement Kahan summation in <code>WindowAggregator</code>; add validation to filter invalid floats</li>\n</ul>\n<p><strong>Scenario: &quot;Compaction causes out of memory&quot;</strong></p>\n<ul>\n<li><em>Diagnose</em>: Monitor memory during compaction; check if <code>mergeSeriesPoints</code> loads all points at once</li>\n<li><em>Fix</em>: Stream merge points instead of loading all; implement memory budget for compaction operations</li>\n</ul>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section looks beyond the current implementation scope to explore how TempoDB could evolve with additional features and capabilities, building upon all five foundational milestones.</p>\n</blockquote>\n<p>The current TempoDB design successfully implements a single-node, specialized time-series database with essential features for handling high-volume sequential data. However, like any production system, its capabilities can be extended to address more complex requirements and larger-scale deployments. This section explores potential enhancements that could transform TempoDB from an educational implementation into a production-ready system capable of handling enterprise workloads. These extensions represent natural evolution paths while maintaining compatibility with the core architecture established in previous sections.</p>\n<h3 id=\"possible-enhancements\">Possible Enhancements</h3>\n<p>The following extensions represent meaningful next steps for TempoDB&#39;s development, ordered by their potential impact and implementation complexity. Each enhancement maintains compatibility with the existing data model and storage format, ensuring backward compatibility while expanding capabilities.</p>\n<h4 id=\"distributed-architecture-and-horizontal-scaling\">Distributed Architecture and Horizontal Scaling</h4>\n<p><strong>Mental Model: The Highway System Expansion</strong><br>Imagine TempoDB as a single major highway handling all traffic. As traffic volume grows, we need to build additional lanes (sharding) and interchanges (coordination) to distribute the load. This extension transforms TempoDB from a single highway into an interconnected highway system where data traffic is intelligently routed and balanced across multiple nodes.</p>\n<p>The current single-node design imposes natural limits on storage capacity, write throughput, and query performance. A distributed architecture would address these limitations through:</p>\n<p><strong>Sharding Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Sharding Method</th>\n<th>Partition Key</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Time-based</strong></td>\n<td>Timestamp range</td>\n<td>Simple to implement, temporal locality preserved</td>\n<td>Hot shard problems, uneven distribution over time</td>\n<td>Workloads with uniform time distribution</td>\n</tr>\n<tr>\n<td><strong>Series-based</strong></td>\n<td>Series key hash</td>\n<td>Even distribution, linear scalability with series</td>\n<td>Cross-series queries require fan-out</td>\n<td>High-cardinality environments</td>\n</tr>\n<tr>\n<td><strong>Hybrid</strong></td>\n<td>Composite (time + series)</td>\n<td>Balances both dimensions</td>\n<td>Complex routing logic</td>\n<td>Mixed query patterns</td>\n</tr>\n</tbody></table>\n<p><strong>Coordination and Consensus:</strong>\nFor distributed operation, TempoDB would need a coordination layer to manage cluster membership, shard placement, and failover. The system could adopt a Raft consensus implementation for metadata management while maintaining an eventually consistent model for data placement.</p>\n<blockquote>\n<p><strong>Decision: Series-based Sharding with Consistent Hashing</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to distribute both write load and storage across multiple nodes while maintaining query efficiency.</li>\n<li><strong>Options Considered</strong>: 1) Time-range sharding, 2) Series-key hashing, 3) Composite sharding</li>\n<li><strong>Decision</strong>: Implement series-key based sharding using consistent hashing.</li>\n<li><strong>Rationale</strong>: Series-based sharding provides better load balancing for high-cardinality workloads and allows parallel query execution across shards. Consistent hashing minimizes data movement when nodes join or leave the cluster.</li>\n<li><strong>Consequences</strong>: Requires query engine to fan-out queries to multiple shards and merge results, but enables linear write scaling with series cardinality.</li>\n</ul>\n</blockquote>\n<p><strong>Implementation Components:</strong></p>\n<ol>\n<li><strong>Cluster Manager</strong>: Tracks node membership and shard assignments using Raft consensus.</li>\n<li><strong>Query Router</strong>: Routes queries to appropriate shards based on series keys.</li>\n<li><strong>Data Replicator</strong>: Maintains configurable replication factor for durability.</li>\n<li><strong>Hinted Handoff</strong>: Handles writes during node failures with eventual consistency.</li>\n</ol>\n<p><strong>Metadata Expansion:</strong>\nThe <code>SeriesMetadata</code> type would expand to include shard location and replication information:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ShardID</code></td>\n<td><code>uint32</code></td>\n<td>Identifier for the shard containing this series</td>\n</tr>\n<tr>\n<td><code>ReplicaNodes</code></td>\n<td><code>[]string</code></td>\n<td>List of nodes storing replicas</td>\n</tr>\n<tr>\n<td><code>PrimaryNode</code></td>\n<td><code>string</code></td>\n<td>Current primary node for writes</td>\n</tr>\n<tr>\n<td><code>Version</code></td>\n<td><code>uint64</code></td>\n<td>Version for conflict resolution in distributed writes</td>\n</tr>\n</tbody></table>\n<h4 id=\"support-for-additional-data-types\">Support for Additional Data Types</h4>\n<p><strong>Mental Model: The Multi-Format Warehouse</strong><br>Currently, TempoDB operates like a warehouse that only stores boxes of a specific size and shape (float64 values). This extension adds specialized storage areas for different item types—some requiring temperature control (booleans), others needing careful stacking (integers), and some that are fragile and complex (strings). The warehouse now needs a more sophisticated inventory system to track what&#39;s stored where and how to handle each type.</p>\n<p>While float64 values cover many time-series use cases, real-world applications require richer data types:</p>\n<p><strong>Type System Expansion:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Data Type</th>\n<th>Storage Requirement</th>\n<th>Compression Strategy</th>\n<th>Query Implications</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Integer</strong> (int64)</td>\n<td>8 bytes raw</td>\n<td>Delta encoding, run-length encoding</td>\n<td>Enables bitwise operations, exact aggregations</td>\n</tr>\n<tr>\n<td><strong>Boolean</strong></td>\n<td>1 bit optimal</td>\n<td>Bit packing (8 values per byte)</td>\n<td>Enables existence queries, state tracking</td>\n</tr>\n<tr>\n<td><strong>String</strong></td>\n<td>Variable length</td>\n<td>Dictionary encoding, Snappy compression</td>\n<td>Enables text search, pattern matching</td>\n</tr>\n<tr>\n<td><strong>Histogram</strong></td>\n<td>Multiple float64 values</td>\n<td>Specialized bucket encoding</td>\n<td>Enables percentile calculations directly</td>\n</tr>\n<tr>\n<td><strong>Multi-value</strong></td>\n<td>Array of floats</td>\n<td>Column-per-value storage</td>\n<td>Enables vector operations, multi-metric storage</td>\n</tr>\n</tbody></table>\n<p><strong>Storage Format Adaptation:</strong>\nThe TSM format would need to extend its block structure to support type identifiers and type-specific compression:</p>\n<p><strong>Revised <code>CompressedBlock</code> structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Type</code></td>\n<td><code>uint8</code></td>\n<td>Data type identifier (0=float64, 1=int64, etc.)</td>\n</tr>\n<tr>\n<td><code>Timestamps</code></td>\n<td><code>[]byte</code></td>\n<td>Compressed timestamps (delta-of-delta)</td>\n</tr>\n<tr>\n<td><code>Values</code></td>\n<td><code>[]byte</code></td>\n<td>Type-specific compressed values</td>\n</tr>\n<tr>\n<td><code>TypeMetadata</code></td>\n<td><code>[]byte</code></td>\n<td>Optional type-specific metadata (e.g., dictionary for strings)</td>\n</tr>\n<tr>\n<td><code>Checksum</code></td>\n<td><code>uint32</code></td>\n<td>CRC32 checksum for integrity</td>\n</tr>\n</tbody></table>\n<p><strong>Query Language Extensions:</strong>\nThe query language would need type-aware functions and operations:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Type-specific aggregations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#E1E4E8\"> percentile(field, </span><span style=\"color:#79B8FF\">95</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> measurements </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#F97583\"> time</span><span style=\"color:#F97583\"> ></span><span style=\"color:#F97583\"> now</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> 1h</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- String operations  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#E1E4E8\"> field </span><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> logs </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#E1E4E8\"> field </span><span style=\"color:#F97583\">LIKE</span><span style=\"color:#9ECBFF\"> '%error%'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Multi-value operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#E1E4E8\"> vector_magnitude(vector_field) </span><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> sensor_data</span></span></code></pre></div>\n\n<p><strong>Type Conversion and Coercion:</strong>\nA comprehensive type system requires clear rules for implicit and explicit type conversions, particularly when performing operations across different types or when querying fields that have changed type over time.</p>\n<h4 id=\"continuous-queries-and-materialized-views\">Continuous Queries and Materialized Views</h4>\n<p><strong>Mental Model: The Automated Factory Assembly Line</strong><br>Imagine data flowing through TempoDB like parts on a factory conveyor belt. Currently, workers (queries) manually inspect and process these parts when requested. Continuous queries act as automated robotic arms that process parts as they arrive, creating pre-assembled components (materialized views) that are ready for immediate use when customers ask for them.</p>\n<p>Continuous queries automatically execute queries at regular intervals and store the results, providing significant performance benefits for frequently accessed aggregates:</p>\n<p><strong>Architecture Components:</strong></p>\n<ol>\n<li><strong>Continuous Query Scheduler</strong>: Manages execution of registered continuous queries.</li>\n<li><strong>Incremental Computation Engine</strong>: Efficiently updates aggregates as new data arrives.</li>\n<li><strong>Materialized View Storage</strong>: Specialized storage for pre-computed results.</li>\n<li><strong>Query Rewriter</strong>: Automatically redirects queries to use materialized views when possible.</li>\n</ol>\n<p><strong>Continuous Query Definition:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>Unique identifier for the continuous query</td>\n</tr>\n<tr>\n<td><code>SourceMeasurement</code></td>\n<td><code>string</code></td>\n<td>Measurement to read from</td>\n</tr>\n<tr>\n<td><code>TargetMeasurement</code></td>\n<td><code>string</code></td>\n<td>Measurement to write results to</td>\n</tr>\n<tr>\n<td><code>Query</code></td>\n<td><code>string</code></td>\n<td>Aggregation query to execute</td>\n</tr>\n<tr>\n<td><code>Interval</code></td>\n<td><code>time.Duration</code></td>\n<td>Execution frequency</td>\n</tr>\n<tr>\n<td><code>ResampleInterval</code></td>\n<td><code>time.Duration</code></td>\n<td>Optional different interval for older data</td>\n</tr>\n<tr>\n<td><code>Enabled</code></td>\n<td><code>bool</code></td>\n<td>Whether the query is active</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Strategy:</strong>\nThe system would extend the <code>Scheduler</code> component to manage continuous queries as a special type of background job. Each continuous query would:</p>\n<ol>\n<li>Track the last timestamp processed</li>\n<li>Execute the aggregation query over new data since last run</li>\n<li>Write results to the target measurement</li>\n<li>Update metadata about what data has been processed</li>\n</ol>\n<p><strong>Query Rewriting Logic:</strong>\nWhen a query arrives, the query planner would:</p>\n<ol>\n<li>Check if any materialized view (continuous query result) can satisfy the query</li>\n<li>Determine if the materialized view has complete data for the requested time range</li>\n<li>Rewrite the query to use the materialized view if appropriate</li>\n<li>Fall back to raw data if materialized view is incomplete or unavailable</li>\n</ol>\n<h4 id=\"tiered-storage-integration\">Tiered Storage Integration</h4>\n<p><strong>Mental Model: The Corporate Document Archive System</strong><br>Important recent documents (hot data) stay in an easily accessible desk drawer (SSD). Last quarter&#39;s documents (warm data) go to a filing cabinet in the office (HDD). Documents older than a year (cold data) get sent to offsite storage (object storage) but can be retrieved when needed. This system balances accessibility with storage cost.</p>\n<p>Tiered storage automatically moves data between storage classes based on age and access patterns, optimizing cost-performance tradeoffs:</p>\n<p><strong>Storage Tier Definitions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Tier</th>\n<th>Storage Medium</th>\n<th>Access Latency</th>\n<th>Cost</th>\n<th>Typical Data Age</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Hot</strong></td>\n<td>Local NVMe/SSD</td>\n<td>Microseconds</td>\n<td>High</td>\n<td>0-24 hours</td>\n</tr>\n<tr>\n<td><strong>Warm</strong></td>\n<td>Local HDD</td>\n<td>Milliseconds</td>\n<td>Medium</td>\n<td>1-30 days</td>\n</tr>\n<tr>\n<td><strong>Cold</strong></td>\n<td>Object Storage (S3)</td>\n<td>Seconds</td>\n<td>Low</td>\n<td>30+ days</td>\n</tr>\n<tr>\n<td><strong>Frozen</strong></td>\n<td>Glacier/Archive</td>\n<td>Minutes-Hours</td>\n<td>Very Low</td>\n<td>365+ days</td>\n</tr>\n</tbody></table>\n<p><strong>Data Movement Strategy:</strong>\nThe system would extend the <code>CompactionManager</code> to include tier promotion/demotion logic:</p>\n<p><strong>Tier State Transitions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Current Tier</th>\n<th>Condition</th>\n<th>Next Tier</th>\n<th>Action Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hot</td>\n<td>Data older than hot_retention</td>\n<td>Warm</td>\n<td>Move TSM files from SSD to HDD</td>\n</tr>\n<tr>\n<td>Warm</td>\n<td>Data older than warm_retention</td>\n<td>Cold</td>\n<td>Upload TSM files to object storage</td>\n</tr>\n<tr>\n<td>Cold</td>\n<td>Query accesses cold data</td>\n<td>Warm (temporarily)</td>\n<td>Cache retrieved data locally</td>\n</tr>\n<tr>\n<td>Any</td>\n<td>Data reaches TTL</td>\n<td>Deleted</td>\n<td>Remove from all tiers</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Approach:</strong></p>\n<ol>\n<li><strong>Storage Abstraction Layer</strong>: Create a unified interface for storage operations that works across local filesystem and object storage.</li>\n<li><strong>Tier Metadata</strong>: Extend <code>TSMFileRef</code> to track storage tier and location.</li>\n<li><strong>Background Tier Manager</strong>: Periodically scans files and moves them between tiers.</li>\n<li><strong>Transparent Retrieval</strong>: Automatically fetches cold data when queried, with optional caching.</li>\n</ol>\n<p><strong>Query Performance Considerations:</strong>\nQueries spanning multiple tiers would need special handling:</p>\n<ul>\n<li>Hot data: Direct memory-mapped access</li>\n<li>Warm data: Standard disk I/O</li>\n<li>Cold data: Async retrieval with query timeout extensions</li>\n<li>Mixed-tier queries: Parallel execution with tier-aware scheduling</li>\n</ul>\n<h4 id=\"advanced-compression-algorithms\">Advanced Compression Algorithms</h4>\n<p><strong>Mental Model: The Specialized Packaging Department</strong><br>Different products need different packaging techniques. Delicate electronics (sensor data with regular patterns) get custom-molded foam (pattern-aware compression). Dense metal parts (integer counters) get efficient stacking (run-length encoding). Irregularly shaped items (sparse metrics) get vacuum-sealed bags (sparse matrix compression). The packaging department now has specialized tools for each product type.</p>\n<p>While Gorilla XOR and delta-of-delta provide excellent general-purpose compression, specialized algorithms can yield better results for specific data patterns:</p>\n<p><strong>Algorithm Selection Framework:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Data Pattern</th>\n<th>Recommended Algorithm</th>\n<th>Compression Ratio</th>\n<th>CPU Cost</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Monotonic counters</strong></td>\n<td>Delta + Varint encoding</td>\n<td>10-20x</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Sparse metrics</strong> (mostly zeros)</td>\n<td>Sparse bitmap encoding</td>\n<td>50-100x</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Regular sampling</strong> (fixed interval)</td>\n<td>Store interval + exceptions</td>\n<td>100x+</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Highly correlated</strong> (sensor networks)</td>\n<td>Chimp or Sprintz</td>\n<td>3-5x better than Gorilla</td>\n<td>High</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Integer histograms</strong></td>\n<td>Bit-packing + RLE</td>\n<td>8-12x</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<p><strong>Adaptive Compression:</strong>\nThe system could analyze data patterns at the series level and select optimal compression:</p>\n<p><strong>Series Compression Profile:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SeriesKey</code></td>\n<td><code>string</code></td>\n<td>Series identifier</td>\n</tr>\n<tr>\n<td><code>PatternType</code></td>\n<td><code>uint8</code></td>\n<td>Detected pattern (constant, counter, random, etc.)</td>\n</tr>\n<tr>\n<td><code>OptimalAlgorithm</code></td>\n<td><code>uint8</code></td>\n<td>Recommended compression algorithm</td>\n</tr>\n<tr>\n<td><code>SampleEntropy</code></td>\n<td><code>float64</code></td>\n<td>Shannon entropy of value samples</td>\n</tr>\n<tr>\n<td><code>TimestampRegularity</code></td>\n<td><code>float64</code></td>\n<td>Regularity score (0=random, 1=perfectly regular)</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Strategy:</strong></p>\n<ol>\n<li><strong>Pattern Detection</strong>: Analyze first N points of a series to determine pattern</li>\n<li><strong>Algorithm Registry</strong>: Pluggable compression algorithm implementations</li>\n<li><strong>Block-level Metadata</strong>: Store algorithm ID in block headers</li>\n<li><strong>Runtime Switching</strong>: Support different algorithms for different blocks of same series</li>\n</ol>\n<h4 id=\"real-time-streaming-analytics\">Real-time Streaming Analytics</h4>\n<p><strong>Mental Model: The Live Sports Broadcast with Real-time Statistics</strong><br>As the game (data stream) progresses, statisticians (streaming engine) continuously calculate player performance metrics, team statistics, and game predictions. These real-time insights appear instantly on screen (dashboard) without waiting for the game to end. The system processes data in motion rather than at rest.</p>\n<p>Extend TempoDB from a database into a real-time analytics platform:</p>\n<p><strong>Stream Processing Architecture:</strong></p>\n<ol>\n<li><strong>Stream Ingestion</strong>: Accept data from message queues (Kafka, Pulsar) in addition to HTTP API</li>\n<li><strong>Windowing Engine</strong>: Support tumbling, sliding, and session windows</li>\n<li><strong>State Management</strong>: Maintain aggregation state across window boundaries</li>\n<li><strong>Low-latency Output</strong>: Emit results to downstream systems or APIs</li>\n</ol>\n<p><strong>Stream Query Language Extension:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Create a streaming query</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">CREATE</span><span style=\"color:#E1E4E8\"> STREAM page_views_1m </span><span style=\"color:#F97583\">AS</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  COUNT</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> view_count,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  AVG</span><span style=\"color:#E1E4E8\">(duration) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> avg_duration,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  WINDOW_START() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> window_start,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  WINDOW_END() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> window_end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> page_views</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">WINDOW</span><span style=\"color:#E1E4E8\"> TUMBLING (</span><span style=\"color:#F97583\">SIZE</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> minute</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">GROUP BY</span><span style=\"color:#E1E4E8\"> user_region, page_category</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">EMIT </span><span style=\"color:#F97583\">CHANGES</span><span style=\"color:#E1E4E8\">;</span></span></code></pre></div>\n\n<p><strong>Integration Points:</strong></p>\n<ul>\n<li><strong>Input Adapters</strong>: Kafka consumer, HTTP streaming, WebSocket connections</li>\n<li><strong>Processing Pipeline</strong>: Chain of operations (filter → transform → aggregate)</li>\n<li><strong>Output Adapters</strong>: Write back to TempoDB, push to message queue, call webhook</li>\n<li><strong>State Storage</strong>: RockDB or similar for window state persistence</li>\n</ul>\n<h4 id=\"advanced-indexing-strategies\">Advanced Indexing Strategies</h4>\n<p><strong>Mental Model: The Library&#39;s Cross-Reference System</strong><br>Beyond the basic card catalog (series key index), a comprehensive library has specialized indexes: subject index (tag values), author index (source identification), citation index (value correlations), and keyword index (text search). Researchers can find information through multiple access paths.</p>\n<p>Extend TempoDB&#39;s indexing beyond the basic series key → block mapping:</p>\n<p><strong>Additional Index Types:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Index Type</th>\n<th>Structure</th>\n<th>Use Case</th>\n<th>Storage Overhead</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tag Value Inverted</strong></td>\n<td>Tag value → Series keys</td>\n<td>Fast filtering by tag values</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Value Range</strong></td>\n<td>Min/max values per block</td>\n<td>Value predicate pushdown</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Bloom Filter</strong></td>\n<td>Per-series bloom filter</td>\n<td>Series existence checks</td>\n<td>Very Low</td>\n</tr>\n<tr>\n<td><strong>Full-text</strong></td>\n<td>Inverted index on string fields</td>\n<td>Text search in log data</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Correlation</strong></td>\n<td>Series → correlated series</td>\n<td>Related metrics discovery</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<p><strong>Composite Index Example - Tag Inverted Index:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TagKey</code></td>\n<td><code>string</code></td>\n<td>Tag key (e.g., &quot;host&quot;)</td>\n</tr>\n<tr>\n<td><code>TagValue</code></td>\n<td><code>string</code></td>\n<td>Tag value (e.g., &quot;web-01&quot;)</td>\n</tr>\n<tr>\n<td><code>SeriesKeys</code></td>\n<td><code>[]string</code></td>\n<td>List of series keys with this tag</td>\n</tr>\n<tr>\n<td><code>LastUpdated</code></td>\n<td><code>time.Time</code></td>\n<td>When this entry was last updated</td>\n</tr>\n</tbody></table>\n<p><strong>Query Optimization Impact:</strong>\nWith advanced indexes, the query planner can:</p>\n<ol>\n<li>Use tag inverted index to resolve series keys from tag predicates before scanning data</li>\n<li>Use value range indexes to skip blocks that cannot contain matching values</li>\n<li>Use bloom filters to quickly determine if a series exists in a time range</li>\n<li>Combine multiple indexes for complex filter expressions</li>\n</ol>\n<h4 id=\"machine-learning-integration\">Machine Learning Integration</h4>\n<p><strong>Mental Model: The Predictive Maintenance System</strong><br>Instead of just recording when machines break, the system learns normal operating patterns and predicts future failures. It&#39;s like having an experienced mechanic who can hear a subtle engine sound and say, &quot;That bearing will fail in 48 hours,&quot; based on patterns seen across thousands of similar machines.</p>\n<p>Integrate ML capabilities for anomaly detection, forecasting, and pattern recognition:</p>\n<p><strong>Built-in ML Functions:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Anomaly detection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#E1E4E8\"> ts, </span><span style=\"color:#F97583\">value</span><span style=\"color:#E1E4E8\">, ANOMALY_SCORE(</span><span style=\"color:#F97583\">value</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">OVER</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">ORDER BY</span><span style=\"color:#E1E4E8\"> ts) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> score</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> metrics </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">WHERE</span><span style=\"color:#F97583\"> time</span><span style=\"color:#F97583\"> ></span><span style=\"color:#F97583\"> now</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> 24h</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Forecasting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#E1E4E8\"> FORECAST(</span><span style=\"color:#F97583\">value</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'1h'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> predictions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">WHERE</span><span style=\"color:#F97583\"> time</span><span style=\"color:#F97583\"> ></span><span style=\"color:#F97583\"> now</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> 7d</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">GROUP BY</span><span style=\"color:#F97583\"> time</span><span style=\"color:#E1E4E8\">(1h)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Pattern similarity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#E1E4E8\"> series_a, series_b, DTW_DISTANCE(series_a, series_b) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> similarity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">WHERE</span><span style=\"color:#F97583\"> time</span><span style=\"color:#F97583\"> ></span><span style=\"color:#F97583\"> now</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> 1h</span></span></code></pre></div>\n\n<p><strong>Implementation Approaches:</strong></p>\n<ol>\n<li><strong>Embedded Models</strong>: Lightweight models (exponential smoothing, simple statistical tests) implemented natively</li>\n<li><strong>External Integration</strong>: Call out to ML services (TensorFlow Serving, ONNX Runtime) for complex models</li>\n<li><strong>Model Management</strong>: Store, version, and serve ML models alongside time-series data</li>\n<li><strong>Feature Engineering</strong>: Built-in functions for creating ML features from time-series</li>\n</ol>\n<p><strong>ML Pipeline Integration:</strong>\nExtend the storage format to include model metadata and predictions:</p>\n<ul>\n<li>Store model artifacts in a specialized measurement</li>\n<li>Append prediction results as derived time series</li>\n<li>Support online learning with incremental model updates</li>\n<li>Provide explainability features for model decisions</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>While the full implementation of these extensions is beyond the current scope, this guidance provides starting points for developers interested in exploring these enhancements.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Distributed Architecture</td>\n<td>Hash-based sharding with static configuration</td>\n<td>Raft consensus with dynamic rebalancing</td>\n</tr>\n<tr>\n<td>Additional Data Types</td>\n<td>Integer and boolean support</td>\n<td>Full type system with pluggable codecs</td>\n</tr>\n<tr>\n<td>Continuous Queries</td>\n<td>Scheduled aggregation jobs</td>\n<td>Incremental view maintenance with query rewriting</td>\n</tr>\n<tr>\n<td>Tiered Storage</td>\n<td>Manual tier promotion scripts</td>\n<td>Automatic data lifecycle with S3 integration</td>\n</tr>\n<tr>\n<td>Advanced Compression</td>\n<td>Algorithm selection per series</td>\n<td>Adaptive compression with runtime profiling</td>\n</tr>\n<tr>\n<td>Streaming Analytics</td>\n<td>Windowed aggregates on ingestion</td>\n<td>Full streaming engine with state management</td>\n</tr>\n<tr>\n<td>Advanced Indexing</td>\n<td>Tag inverted index</td>\n<td>Multiple index types with automatic selection</td>\n</tr>\n<tr>\n<td>ML Integration</td>\n<td>Built-in statistical functions</td>\n<td>TensorFlow Lite integration with model serving</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tempo/\n  ├── cmd/\n  │   ├── tempo-server/           # Main server (existing)\n  │   └── tempo-cluster/          # Cluster-aware server (new)\n  ├── internal/\n  │   ├── cluster/                # Distributed coordination\n  │   │   ├── coordinator.go      # Cluster coordination logic\n  │   │   ├── sharding.go         # Shard assignment and routing\n  │   │   └── replication.go      # Data replication between nodes\n  │   ├── compression/            # Extended compression algorithms\n  │   │   ├── registry.go         # Algorithm registration\n  │   │   ├── integer.go          # Integer compression\n  │   │   ├── string.go           # String compression\n  │   │   └── adaptive.go         # Adaptive algorithm selection\n  │   ├── streaming/              # Real-time stream processing\n  │   │   ├── engine.go           # Streaming query engine\n  │   │   ├── windows.go          # Window implementations\n  │   │   └── operators.go        # Stream operators (map, filter, aggregate)\n  │   ├── indexing/               # Advanced indexes\n  │   │   ├── tag_inverted.go     # Tag value → series index\n  │   │   ├── value_range.go      # Value range index\n  │   │   └── bloom.go            # Bloom filter implementation\n  │   └── ml/                     # Machine learning integration\n  │       ├── functions.go        # Built-in ML functions\n  │       ├── models.go           # Model storage and serving\n  │       └── anomaly.go          # Anomaly detection algorithms\n  └── pkg/\n      └── storage/tiered/         # Tiered storage abstraction\n          ├── manager.go          # Tier lifecycle management\n          ├── local.go            # Local filesystem backend\n          └── s3.go               # S3 object storage backend</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code - Tiered Storage Interface:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/tiered/interface.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> tiered</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageBackend defines the interface for different storage tiers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageBackend</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Type returns the backend type identifier</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Type</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write writes data to the backend</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Write</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read reads data from the backend</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Read</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Delete removes data from the backend</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Delete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Exists checks if data exists in the backend</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Exists</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // List lists all keys with a given prefix</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    List</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">prefix</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Stats returns backend statistics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Stats</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">BackendStats</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BackendStats contains statistics for a storage backend</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BackendStats</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalBytes </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UsedBytes  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FileCount  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LatencyMS  </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TierManager manages data movement between tiers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TierManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backends </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    policies []</span><span style=\"color:#B392F0\">TierPolicy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata </span><span style=\"color:#B392F0\">MetadataStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TierPolicy defines when data should move between tiers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TierPolicy</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SourceTier      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DestinationTier </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Condition       </span><span style=\"color:#B392F0\">PolicyCondition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BatchSize       </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PolicyCondition defines movement conditions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PolicyCondition</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AgeOlderThan    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessOlderThan </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SizeGreaterThan </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code - Distributed Query Router:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/cluster/router.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> cluster</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryRouter routes queries to appropriate shards</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryRouter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shardMap    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ShardMap</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeClients </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    merger      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResultMerger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RouteQuery analyzes a query and routes it to appropriate nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryRouter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RouteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Query</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RoutedQuery</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract series keys from the query using tag predicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each series key, determine which shard owns it using consistent hashing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Group series keys by shard and then by node (accounting for replication)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Build query fragments for each node, only including series keys that node hosts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Add aggregation merging instructions for cross-shard aggregates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return RoutedQuery with parallel execution plan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteRoutedQuery executes a routed query across multiple nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryRouter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteRoutedQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">routed</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RoutedQuery</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Launch goroutines to execute each query fragment on its target node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Collect results with proper error handling and timeout management</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Merge results from different nodes, respecting the merge strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply any final aggregation that couldn't be pushed to individual nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return combined result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ResultMerger merges results from multiple shards</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ResultMerger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mergeStrategy </span><span style=\"color:#B392F0\">MergeStrategy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Merge merges multiple query results into one</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResultMerger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Merge</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">results</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if all results have compatible schemas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For aggregate queries, combine aggregate values appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For raw data queries, concatenate and sort all points by timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle duplicate points from replicated series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply limit/offset if specified in the original query</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ol>\n<li><p><strong>For distributed systems</strong>: Use <code>hash/fnv</code> for consistent hashing, <code>context.Context</code> for request cancellation, and <code>errgroup</code> for managing parallel requests to multiple nodes.</p>\n</li>\n<li><p><strong>For additional data types</strong>: Implement the <code>encoding.BinaryMarshaler</code> and <code>encoding.BinaryUnmarshaler</code> interfaces for custom serialization of new types.</p>\n</li>\n<li><p><strong>For tiered storage</strong>: Use the <code>aws-sdk-go-v2</code> for S3 integration with intelligent retries and exponential backoff for failed operations.</p>\n</li>\n<li><p><strong>For streaming analytics</strong>: Consider using <code>go-channels</code> for data flow between streaming operators, with careful buffer sizing to prevent deadlocks.</p>\n</li>\n<li><p><strong>For ML integration</strong>: Use <code>gonum.org/v1/gonum</code> for statistical functions and <code>github.com/sjwhitworth/golearn</code> for basic machine learning algorithms.</p>\n</li>\n</ol>\n<p><strong>F. Milestone Checkpoint for Distributed Extension:</strong></p>\n<p>To verify a basic distributed implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start three nodes in a cluster</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> ./tempo-cluster</span><span style=\"color:#79B8FF\"> --node-id=node1</span><span style=\"color:#79B8FF\"> --cluster-addr=:9090</span><span style=\"color:#79B8FF\"> --http-addr=:8080</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> ./tempo-cluster</span><span style=\"color:#79B8FF\"> --node-id=node2</span><span style=\"color:#79B8FF\"> --cluster-addr=:9091</span><span style=\"color:#79B8FF\"> --http-addr=:8081</span><span style=\"color:#79B8FF\"> --join=localhost:9090</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> ./tempo-cluster</span><span style=\"color:#79B8FF\"> --node-id=node3</span><span style=\"color:#79B8FF\"> --cluster-addr=:9092</span><span style=\"color:#79B8FF\"> --http-addr=:8082</span><span style=\"color:#79B8FF\"> --join=localhost:9090</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Write data to any node</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/write</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> 'cpu,host=server1 value=0.64'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Query from any node (should route to correct node)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> curl</span><span style=\"color:#79B8FF\"> -G</span><span style=\"color:#9ECBFF\"> http://localhost:8081/query</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --data-urlencode</span><span style=\"color:#9ECBFF\"> 'q=SELECT * FROM cpu WHERE host=\"server1\"'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify all nodes show cluster membership</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/cluster/nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should return list of all three nodes</span></span></code></pre></div>\n\n<p><strong>G. Debugging Tips for Future Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query returns partial data in distributed mode</td>\n<td>Some shards unreachable or returning errors</td>\n<td>Check cluster health endpoint, examine query router logs</td>\n<td>Ensure all nodes are healthy, implement retry logic for failed shards</td>\n</tr>\n<tr>\n<td>Compression ratio worse for new data type</td>\n<td>Incorrect algorithm selection for data pattern</td>\n<td>Analyze data pattern statistics, benchmark different algorithms</td>\n<td>Implement adaptive algorithm selection based on data characteristics</td>\n</tr>\n<tr>\n<td>Continuous queries creating duplicate data</td>\n<td>Race condition in incremental computation</td>\n<td>Check last processed timestamp tracking, examine query execution logs</td>\n<td>Add synchronization or use transactional updates for state management</td>\n</tr>\n<tr>\n<td>Tier promotion failing silently</td>\n<td>Insufficient permissions for object storage</td>\n<td>Check tier manager logs, verify credentials and bucket permissions</td>\n<td>Implement proper error reporting and retry with exponential backoff</td>\n</tr>\n<tr>\n<td>ML functions returning inconsistent results</td>\n<td>Model version mismatch or stale cache</td>\n<td>Check model metadata, compare results across different calls</td>\n<td>Implement model versioning and cache invalidation strategies</td>\n</tr>\n</tbody></table>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This reference section supports all five milestones by providing clear definitions of the specialized terminology used throughout the TempoDB design document.</p>\n</blockquote>\n<p>The time-series database domain uses specialized vocabulary that may be unfamiliar to developers new to this field. This glossary provides authoritative definitions for key terms, acronyms, and concepts used throughout the TempoDB design document. Each term includes a clear definition and reference to the primary section where it&#39;s discussed in detail, creating a consistent reference point for implementation.</p>\n<h3 id=\"terminology-reference\">Terminology Reference</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Primary Reference</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Aggregate Function</strong></td>\n<td>A mathematical operation applied to multiple data points to produce a single summary value, such as sum, average, minimum, maximum, or count.</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Backpressure</strong></td>\n<td>A mechanism to throttle incoming write requests when the system is overloaded, preventing resource exhaustion and maintaining system stability.</td>\n<td>Write Path Design</td>\n</tr>\n<tr>\n<td><strong>Block-Based Storage</strong></td>\n<td>A storage organization strategy where data is grouped into fixed-size blocks, enabling efficient I/O operations and temporal locality for range queries.</td>\n<td>Storage Engine Design</td>\n</tr>\n<tr>\n<td><strong>Cardinality</strong></td>\n<td>The number of unique time series in a dataset, calculated as the product of unique values for each tag dimension. High cardinality can impact performance.</td>\n<td>Write Path Design</td>\n</tr>\n<tr>\n<td><strong>Clock Skew</strong></td>\n<td>Time difference between distributed system clocks, which can cause challenges for time-series data ordering and consistency.</td>\n<td>Error Handling and Edge Cases</td>\n</tr>\n<tr>\n<td><strong>Columnar Layout</strong></td>\n<td>A data storage format where values from the same column (e.g., all timestamps, all values) are stored contiguously rather than storing complete rows together, improving compression and scan efficiency.</td>\n<td>Storage Engine Design</td>\n</tr>\n<tr>\n<td><strong>Compaction</strong></td>\n<td>A background process that merges multiple smaller storage files into larger, optimized files, reducing storage overhead and improving query performance.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>Compaction Level</strong></td>\n<td>A hierarchical tier in the compaction strategy where files at higher levels are larger and contain older, less volatile data.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>DataPoint</strong></td>\n<td>The fundamental unit of time-series data consisting of a timestamp and a value (<code>Timestamp time.Time, Value float64</code>).</td>\n<td>Data Model</td>\n</tr>\n<tr>\n<td><strong>Delta-of-Delta Encoding</strong></td>\n<td>A compression technique for timestamps that stores the difference between consecutive differences, achieving high compression ratios for regularly spaced timestamps.</td>\n<td>Storage Engine Design</td>\n</tr>\n<tr>\n<td><strong>Downsampling</strong></td>\n<td>The process of reducing data resolution through aggregation (e.g., converting 1-second data to 1-minute averages) to conserve storage space for historical data.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>Field</strong></td>\n<td>The actual measured value in a time series, typically a float64 numeric value, as opposed to metadata tags.</td>\n<td>Data Model</td>\n</tr>\n<tr>\n<td><strong>Golden File Testing</strong></td>\n<td>A testing methodology that compares output against versioned reference files to ensure format stability and detect unintended changes.</td>\n<td>Testing Strategy</td>\n</tr>\n<tr>\n<td><strong>Gorilla XOR Compression</strong></td>\n<td>A lossless compression algorithm for floating-point values that XORs consecutive values and encodes the resulting changes efficiently.</td>\n<td>Storage Engine Design</td>\n</tr>\n<tr>\n<td><strong>Grace Period</strong></td>\n<td>A time delay between marking files for deletion (tombstoning) and physically removing them from disk, allowing for recovery from accidental deletions.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>Group By Time</strong></td>\n<td>A query operation that partitions data into fixed-width time intervals (buckets) and applies aggregation functions within each bucket.</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Inverted Index</strong></td>\n<td>An index data structure that maps tag values to the series keys that contain them, enabling fast filtering by tag predicates.</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Iterator Model</strong></td>\n<td>An execution pattern where each query operator implements a <code>Next()</code> method to pull data through the pipeline, enabling streaming and lazy evaluation.</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Kahan Summation</strong></td>\n<td>An algorithm for summing floating-point numbers with reduced precision loss by maintaining a running compensation for lost low-order bits.</td>\n<td>Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Level-Based Compaction</strong></td>\n<td>A compaction strategy that organizes files into tiers (levels) with progressively larger sizes, promoting data through levels based on age and size thresholds.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>Line Protocol</strong></td>\n<td>A text-based format for writing time-series data points, consisting of measurement, tag sets, field sets, and timestamp.</td>\n<td>Query Language and API Design</td>\n</tr>\n<tr>\n<td><strong>Materialized View</strong></td>\n<td>A pre-computed query result stored for fast access, such as rollup aggregations for historical data.</td>\n<td>Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Measurement</strong></td>\n<td>A container for related time-series data, analogous to a table name in relational databases, grouping series with the same semantic meaning.</td>\n<td>Data Model</td>\n</tr>\n<tr>\n<td><strong>Memtable</strong></td>\n<td>An in-memory buffer that holds recently written data points before they are flushed to persistent storage, optimized for high write throughput.</td>\n<td>Write Path Design</td>\n</tr>\n<tr>\n<td><strong>Memory-Mapped Files</strong></td>\n<td>A file access technique that maps file contents directly into virtual memory, enabling zero-copy reads and efficient random access.</td>\n<td>Storage Engine Design</td>\n</tr>\n<tr>\n<td><strong>Out-of-Order Writes</strong></td>\n<td>Data points arriving with timestamps that are not in chronological order relative to previously written points for the same series.</td>\n<td>Write Path Design</td>\n</tr>\n<tr>\n<td><strong>Predicate Pushdown</strong></td>\n<td>A query optimization technique that applies filtering conditions as early as possible in the execution pipeline, ideally at the storage layer, to reduce the amount of data processed.</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Prometheus Remote Read/Write</strong></td>\n<td>A protocol that allows Prometheus to use external storage systems for long-term data retention, enabling integration with the Prometheus monitoring ecosystem.</td>\n<td>Query Language and API Design</td>\n</tr>\n<tr>\n<td><strong>Property-Based Testing</strong></td>\n<td>A testing methodology that verifies properties or invariants hold for all possible inputs within a defined domain, often using randomly generated test cases.</td>\n<td>Testing Strategy</td>\n</tr>\n<tr>\n<td><strong>Query Plan</strong></td>\n<td>An internal representation of a query that specifies the execution steps, including which files to scan, which predicates to apply, and in what order to perform operations.</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Retention Policy</strong></td>\n<td>A rule defining how long data should be kept before automatic deletion, typically specified as a time-to-live (TTL) duration.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>Rollup Series</strong></td>\n<td>A pre-computed time series at lower granularity, created by aggregating higher-resolution data over time windows for historical query performance.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>Series</strong></td>\n<td>A collection of data points sharing the same measurement and complete set of tags, representing a single time-varying metric.</td>\n<td>Data Model</td>\n</tr>\n<tr>\n<td><strong>Series Key</strong></td>\n<td>A unique identifier for a time series formed by concatenating a measurement name with a complete set of tag key-value pairs (<code>Measurement string, Tags map[string]string</code>).</td>\n<td>Data Model</td>\n</tr>\n<tr>\n<td><strong>Shard</strong></td>\n<td>A logical partition of data distributed across nodes in a cluster, enabling horizontal scaling and parallel processing.</td>\n<td>Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Skip List</strong></td>\n<td>A probabilistic data structure that allows fast search, insertion, and deletion within an ordered sequence, often used for in-memory indexes.</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Tags</strong></td>\n<td>Indexed key-value metadata associated with time-series data, used to identify, filter, and group series (e.g., <code>host=&quot;server1&quot;, region=&quot;us-west&quot;</code>).</td>\n<td>Data Model</td>\n</tr>\n<tr>\n<td><strong>Temporal Locality</strong></td>\n<td>The principle that data accessed together in time should be stored together physically, optimizing for time-range query patterns.</td>\n<td>Storage Engine Design</td>\n</tr>\n<tr>\n<td><strong>Tiered Storage</strong></td>\n<td>A storage architecture with multiple performance/cost tiers (e.g., SSD for hot data, HDD for warm data, object storage for cold data), automatically migrating data based on access patterns.</td>\n<td>Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Time-Range Query</strong></td>\n<td>A query that retrieves all data points within a specified start and end timestamp boundary.</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Time-Series Data</strong></td>\n<td>Sequential measurements or events indexed by time, characterized by append-heavy write patterns, time-ordered reads, and predictable value patterns.</td>\n<td>Context and Problem Statement</td>\n</tr>\n<tr>\n<td><strong>Time-Structured Merge Tree (TSM)</strong></td>\n<td>A storage engine optimized for time-series data that organizes data into time-sorted files and merges them in the background, similar to LSM trees but with time-based partitioning.</td>\n<td>Storage Engine Design</td>\n</tr>\n<tr>\n<td><strong>Time-To-Live (TTL)</strong></td>\n<td>The duration after which data is considered expired and eligible for automatic deletion, enforced by retention policies.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>Tolerance Window</strong></td>\n<td>The maximum allowed time difference for accepting out-of-order data points; points outside this window may be rejected or handled specially.</td>\n<td>Write Path Design</td>\n</tr>\n<tr>\n<td><strong>Tombstoned</strong></td>\n<td>A state where a file is marked for deletion but not yet physically removed from disk, typically during a grace period.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>TSM File</strong></td>\n<td>A storage file in the Time-Structured Merge format containing compressed time-series data blocks and an index mapping series keys to block locations.</td>\n<td>Storage Engine Design</td>\n</tr>\n<tr>\n<td><strong>Tumbling Windows</strong></td>\n<td>Contiguous, non-overlapping time intervals used for grouping operations in windowed aggregations (e.g., every complete 5-minute period).</td>\n<td>Query Engine Design</td>\n</tr>\n<tr>\n<td><strong>Write Amplification</strong></td>\n<td>The phenomenon where the storage engine performs more physical writes than the logical writes requested by the application, often due to compaction and durability mechanisms.</td>\n<td>Retention and Compaction Design</td>\n</tr>\n<tr>\n<td><strong>Write-Ahead Log (WAL)</strong></td>\n<td>A durability mechanism that logs write operations to persistent storage before acknowledging them to clients, ensuring data survival across crashes.</td>\n<td>Write Path Design</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> Consistent terminology is critical for team alignment. This glossary serves as a single source of truth for all terms used in the TempoDB design, preventing misunderstandings during implementation. When adding new terms during development, consider updating this glossary to maintain clarity.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>While a glossary doesn&#39;t require implementation code, maintaining consistency in naming is crucial for code quality and team communication. Below are recommendations for ensuring terminology consistency throughout your Go implementation.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Terminology Validation</td>\n<td>Manual code review with glossary reference</td>\n<td>Static analysis with custom linter rules</td>\n</tr>\n<tr>\n<td>Documentation Generation</td>\n<td>GoDoc comments with consistent terms</td>\n<td>Automated glossary extraction from source</td>\n</tr>\n<tr>\n<td>Naming Convention Enforcement</td>\n<td>Team agreement and manual checks</td>\n<td>Pre-commit hooks with naming validation</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<p>Create a documentation directory to store the glossary and other reference materials:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  docs/\n    glossary.md              ← This glossary document\n    architecture.md          ← High-level design overview\n    api-reference.md         ← API documentation\n  internal/\n    storage/                 ← Storage engine implementation\n    query/                   ← Query engine implementation\n    wal/                     ← Write-ahead log implementation\n  cmd/\n    server/main.go           ← Main entry point</code></pre></div>\n\n<p><strong>C. Terminology Consistency Checklist:</strong></p>\n<p>When reviewing code, verify these consistency points:</p>\n<ol>\n<li><strong>Type and Field Names:</strong> Use exact names from the NAMING CONVENTIONS section (e.g., <code>DataPoint</code>, not <code>DataPoint</code> or <code>Point</code>)</li>\n<li><strong>Method Signatures:</strong> Follow the exact signatures provided in the design document</li>\n<li><strong>Error Messages:</strong> Use consistent terminology when describing errors</li>\n<li><strong>Comments and Documentation:</strong> Reference terms from this glossary where appropriate</li>\n<li><strong>Log Messages:</strong> Use standardized terminology for operational logging</li>\n</ol>\n<p><strong>D. Go-Specific Naming Hints:</strong></p>\n<ul>\n<li>Use <code>camelCase</code> for local variables and private fields</li>\n<li>Use <code>PascalCase</code> for exported types, functions, and constants</li>\n<li>Follow Go conventions for acronyms: <code>TSMFile</code> (not <code>TsmFile</code>), <code>WAL</code> (not <code>Wal</code>)</li>\n<li>Use descriptive names that match glossary terms: <code>memtable</code> (not <code>writeBuffer</code>), <code>compactionPlan</code> (not <code>mergePlan</code>)</li>\n</ul>\n<p><strong>E. Debugging Terminology Mismatches:</strong></p>\n<p>If you encounter confusion during implementation, check:</p>\n<ol>\n<li><strong>Cross-reference:</strong> Verify all team members are using the same version of the glossary</li>\n<li><strong>Code search:</strong> Use <code>grep</code> or IDE search to find inconsistent usage</li>\n<li><strong>Documentation:</strong> Update inline comments to clarify term usage</li>\n<li><strong>Peer review:</strong> Include terminology checks in code review checklists</li>\n</ol>\n<p><strong>F. Maintaining the Glossary:</strong></p>\n<p>As the implementation evolves, you may discover new terms that need definition. Follow this process:</p>\n<ol>\n<li>Add the term to this glossary document with a clear definition</li>\n<li>Update the NAMING CONVENTIONS section if it&#39;s a core type, method, or constant</li>\n<li>Communicate the change to the team</li>\n<li>Update any affected code to use the new terminology consistently</li>\n</ol>\n<blockquote>\n<p><strong>Implementation Tip:</strong> Consider creating a simple validation script that scans source code for glossary terms and flags potentially inconsistent usage. This can be particularly helpful for large codebases or distributed teams.</p>\n</blockquote>\n","toc":[{"level":1,"text":"TempoDB: A Modern Time-Series Database - Design Document","id":"tempodb-a-modern-time-series-database-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Data Stream Conveyor Belt","id":"mental-model-the-data-stream-conveyor-belt"},{"level":3,"text":"Existing Approaches and Comparison","id":"existing-approaches-and-comparison"},{"level":4,"text":"Option 1: Generic Relational Databases (PostgreSQL, MySQL)","id":"option-1-generic-relational-databases-postgresql-mysql"},{"level":4,"text":"Option 2: Specialized Time-Series Databases (InfluxDB, Prometheus)","id":"option-2-specialized-time-series-databases-influxdb-prometheus"},{"level":4,"text":"Option 3: General-Purpose Columnar Stores (Apache Parquet, Arrow)","id":"option-3-general-purpose-columnar-stores-apache-parquet-arrow"},{"level":4,"text":"Synthesis: The TempoDB Approach","id":"synthesis-the-tempodb-approach"},{"level":3,"text":"Common Pitfalls in Time-Series Database Design","id":"common-pitfalls-in-time-series-database-design"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Core Type Definitions (Foundation)","id":"c-core-type-definitions-foundation"},{"level":4,"text":"D. Language-Specific Hints for Go","id":"d-language-specific-hints-for-go"},{"level":4,"text":"E. First Milestone Checkpoint (Conceptual)","id":"e-first-milestone-checkpoint-conceptual"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Goals","id":"goals"},{"level":4,"text":"Non-Functional Goal Elaboration","id":"non-functional-goal-elaboration"},{"level":3,"text":"Non-Goals","id":"non-goals"},{"level":4,"text":"The &quot;Not Now&quot; vs. &quot;Not Ever&quot; Distinction","id":"the-quotnot-nowquot-vs-quotnot-everquot-distinction"},{"level":3,"text":"Summary of Scope","id":"summary-of-scope"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Mental Model: The Time-Series Processing Pipeline","id":"mental-model-the-time-series-processing-pipeline"},{"level":3,"text":"Component Overview and Responsibilities","id":"component-overview-and-responsibilities"},{"level":4,"text":"Ingest API","id":"ingest-api"},{"level":4,"text":"Write-Ahead Log (WAL)","id":"write-ahead-log-wal"},{"level":4,"text":"Memtable","id":"memtable"},{"level":4,"text":"Storage Engine (TSM)","id":"storage-engine-tsm"},{"level":4,"text":"Query Engine","id":"query-engine"},{"level":4,"text":"Compactor","id":"compactor"},{"level":3,"text":"Component Interactions","id":"component-interactions"},{"level":3,"text":"Common Pitfalls in Architecture Design","id":"common-pitfalls-in-architecture-design"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints for Go","id":"language-specific-hints-for-go"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Mental Model: The Time-Series Library Catalog","id":"mental-model-the-time-series-library-catalog"},{"level":3,"text":"Core Concepts: Measurement, Tags, Field","id":"core-concepts-measurement-tags-field"},{"level":3,"text":"Type Definitions and Relationships","id":"type-definitions-and-relationships"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Storage Engine Design","id":"storage-engine-design"},{"level":3,"text":"Mental Model: The Time-Indexed Filing Cabinet","id":"mental-model-the-time-indexed-filing-cabinet"},{"level":3,"text":"TSM File Format and Block Layout","id":"tsm-file-format-and-block-layout"},{"level":4,"text":"File Structure","id":"file-structure"},{"level":4,"text":"Data Block Structure","id":"data-block-structure"},{"level":4,"text":"Index Structure","id":"index-structure"},{"level":4,"text":"Key Data Structures","id":"key-data-structures"},{"level":4,"text":"File Creation Process","id":"file-creation-process"},{"level":3,"text":"ADR: Choosing Compression Algorithms","id":"adr-choosing-compression-algorithms"},{"level":4,"text":"Compression Algorithm Details","id":"compression-algorithm-details"},{"level":4,"text":"Block Size Considerations","id":"block-size-considerations"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance (Milestone 1)","id":"implementation-guidance-milestone-1"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Write Path Design","id":"write-path-design"},{"level":3,"text":"Mental Model: The Airport Check-in and Baggage System","id":"mental-model-the-airport-check-in-and-baggage-system"},{"level":3,"text":"Write-Ahead Log and Memtable","id":"write-ahead-log-and-memtable"},{"level":4,"text":"Write-Ahead Log (WAL)","id":"write-ahead-log-wal"},{"level":4,"text":"Memtable","id":"memtable"},{"level":3,"text":"Flush Mechanism and Out-of-Order Writes","id":"flush-mechanism-and-out-of-order-writes"},{"level":4,"text":"Flush Triggers","id":"flush-triggers"},{"level":4,"text":"Flush Process","id":"flush-process"},{"level":4,"text":"Handling Out-of-Order Writes","id":"handling-out-of-order-writes"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance (Milestone 2)","id":"implementation-guidance-milestone-2"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Query Engine Design","id":"query-engine-design"},{"level":3,"text":"Mental Model: The Library Research Assistant","id":"mental-model-the-library-research-assistant"},{"level":3,"text":"Query Parsing, Planning, and Execution","id":"query-parsing-planning-and-execution"},{"level":4,"text":"1. Query Parsing","id":"1-query-parsing"},{"level":4,"text":"2. Query Planning","id":"2-query-planning"},{"level":4,"text":"3. Query Execution","id":"3-query-execution"},{"level":3,"text":"Aggregations and Downsampling","id":"aggregations-and-downsampling"},{"level":4,"text":"Built-in Aggregate Functions","id":"built-in-aggregate-functions"},{"level":4,"text":"GROUP BY time() and Tumbling Windows","id":"group-by-time-and-tumbling-windows"},{"level":4,"text":"Downsampling vs. On-the-Fly Aggregation","id":"downsampling-vs-on-the-fly-aggregation"},{"level":4,"text":"Handling Gaps in Data","id":"handling-gaps-in-data"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance (Milestone 3)","id":"implementation-guidance-milestone-3"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Retention and Compaction Design","id":"retention-and-compaction-design"},{"level":3,"text":"Mental Model: The Warehouse Archivist","id":"mental-model-the-warehouse-archivist"},{"level":3,"text":"TTL Enforcement and Compaction Strategy","id":"ttl-enforcement-and-compaction-strategy"},{"level":4,"text":"TTL Enforcement: Time-Based Data Expiration","id":"ttl-enforcement-time-based-data-expiration"},{"level":4,"text":"Compaction Strategy: Level-Based File Consolidation","id":"compaction-strategy-level-based-file-consolidation"},{"level":4,"text":"Common Pitfalls in Retention and Compaction","id":"common-pitfalls-in-retention-and-compaction"},{"level":3,"text":"ADR: Continuous vs Scheduled Downsampling","id":"adr-continuous-vs-scheduled-downsampling"},{"level":3,"text":"Implementation Guidance (Milestone 4)","id":"implementation-guidance-milestone-4"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":2,"text":"Query Language and API Design","id":"query-language-and-api-design"},{"level":3,"text":"Mental Model: The Restaurant Menu and Kitchen Window","id":"mental-model-the-restaurant-menu-and-kitchen-window"},{"level":3,"text":"Query Language Specification","id":"query-language-specification"},{"level":4,"text":"Core Grammar","id":"core-grammar"},{"level":4,"text":"Query Structure and Semantics","id":"query-structure-and-semantics"},{"level":4,"text":"Architecture Decision: SQL-Like vs. Flux-Style Query Language","id":"architecture-decision-sql-like-vs-flux-style-query-language"},{"level":3,"text":"Write and Read APIs","id":"write-and-read-apis"},{"level":4,"text":"Write API","id":"write-api"},{"level":4,"text":"Read API","id":"read-api"},{"level":4,"text":"Prometheus Remote Read/Write Compatibility","id":"prometheus-remote-readwrite-compatibility"},{"level":4,"text":"API Architecture Components","id":"api-architecture-components"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance (Milestone 5)","id":"implementation-guidance-milestone-5"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Write Path Sequence","id":"write-path-sequence"},{"level":3,"text":"Query Path Sequence","id":"query-path-sequence"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Common Failure Modes and Recovery","id":"common-failure-modes-and-recovery"},{"level":4,"text":"Storage Layer Failures","id":"storage-layer-failures"},{"level":4,"text":"Write Path Failures","id":"write-path-failures"},{"level":4,"text":"Query Engine Failures","id":"query-engine-failures"},{"level":4,"text":"Compaction and Retention Failures","id":"compaction-and-retention-failures"},{"level":3,"text":"Data-Specific Edge Cases","id":"data-specific-edge-cases"},{"level":4,"text":"Out-of-Order Data Handling","id":"out-of-order-data-handling"},{"level":4,"text":"Gaps in Time Series","id":"gaps-in-time-series"},{"level":4,"text":"Clock Skew and Timestamp Anomalies","id":"clock-skew-and-timestamp-anomalies"},{"level":4,"text":"Queries with Very Large Time Ranges","id":"queries-with-very-large-time-ranges"},{"level":4,"text":"Boundary Conditions in Time-Based Grouping","id":"boundary-conditions-in-time-based-grouping"},{"level":4,"text":"Data Type and Precision Edge Cases","id":"data-type-and-precision-edge-cases"},{"level":4,"text":"Handling Tombstoned Data","id":"handling-tombstoned-data"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Error Handling Infrastructure","id":"error-handling-infrastructure"},{"level":4,"text":"WAL Corruption Recovery Skeleton","id":"wal-corruption-recovery-skeleton"},{"level":4,"text":"Query Timeout and Cancellation","id":"query-timeout-and-cancellation"},{"level":4,"text":"Disk Space Monitoring and Emergency Actions","id":"disk-space-monitoring-and-emergency-actions"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint: Error Handling Validation","id":"milestone-checkpoint-error-handling-validation"},{"level":4,"text":"Debugging Tips for Common Error Scenarios","id":"debugging-tips-for-common-error-scenarios"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Mental Model: The Scientific Laboratory","id":"mental-model-the-scientific-laboratory"},{"level":3,"text":"Testing Approaches and Property Verification","id":"testing-approaches-and-property-verification"},{"level":4,"text":"Property Verification for Critical Components","id":"property-verification-for-critical-components"},{"level":4,"text":"Golden File Testing for Storage Format","id":"golden-file-testing-for-storage-format"},{"level":4,"text":"Fuzz Testing for Query Language","id":"fuzz-testing-for-query-language"},{"level":3,"text":"Common Pitfalls in Time-Series Database Testing","id":"common-pitfalls-in-time-series-database-testing"},{"level":4,"text":"ADR: Choosing Test Frameworks and Approaches","id":"adr-choosing-test-frameworks-and-approaches"},{"level":3,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"},{"level":4,"text":"Milestone 1: Storage Engine Verification","id":"milestone-1-storage-engine-verification"},{"level":4,"text":"Milestone 2: Write Path Verification","id":"milestone-2-write-path-verification"},{"level":4,"text":"Milestone 3: Query Engine Verification","id":"milestone-3-query-engine-verification"},{"level":4,"text":"Milestone 4: Retention &amp; Compaction Verification","id":"milestone-4-retention-amp-compaction-verification"},{"level":4,"text":"Milestone 5: Query Language &amp; API Verification","id":"milestone-5-query-language-amp-api-verification"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended Test File Structure","id":"b-recommended-test-file-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Test Skeleton Code","id":"d-core-test-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Debugging Tips Table","id":"f-debugging-tips-table"},{"level":4,"text":"G. Performance Testing Framework","id":"g-performance-testing-framework"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Common Bug Symptoms and Fixes","id":"common-bug-symptoms-and-fixes"},{"level":4,"text":"Special Considerations for Time-Series Data","id":"special-considerations-for-time-series-data"},{"level":3,"text":"Debugging Techniques and Tools","id":"debugging-techniques-and-tools"},{"level":4,"text":"1. Structured Logging with Component Context","id":"1-structured-logging-with-component-context"},{"level":4,"text":"2. File Inspection Utilities","id":"2-file-inspection-utilities"},{"level":4,"text":"3. Performance Profiling with Go&#39;s pprof","id":"3-performance-profiling-with-go39s-pprof"},{"level":4,"text":"4. Property-Based Testing for Compression Algorithms","id":"4-property-based-testing-for-compression-algorithms"},{"level":4,"text":"5. Golden File Testing for Format Stability","id":"5-golden-file-testing-for-format-stability"},{"level":4,"text":"6. Time-Travel Debugging with Record &amp; Replay","id":"6-time-travel-debugging-with-record-amp-replay"},{"level":4,"text":"7. Visualization of Internal State","id":"7-visualization-of-internal-state"},{"level":4,"text":"8. Stress Testing with Anomaly Injection","id":"8-stress-testing-with-anomaly-injection"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code: TSM File Inspector","id":"c-infrastructure-starter-code-tsm-file-inspector"},{"level":4,"text":"D. Core Logic Skeleton: Operation Recorder","id":"d-core-logic-skeleton-operation-recorder"},{"level":4,"text":"E. Go-Specific Debugging Hints","id":"e-go-specific-debugging-hints"},{"level":4,"text":"F. Milestone Debugging Checkpoints","id":"f-milestone-debugging-checkpoints"},{"level":4,"text":"G. Debugging Tips for Specific Scenarios","id":"g-debugging-tips-for-specific-scenarios"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Possible Enhancements","id":"possible-enhancements"},{"level":4,"text":"Distributed Architecture and Horizontal Scaling","id":"distributed-architecture-and-horizontal-scaling"},{"level":4,"text":"Support for Additional Data Types","id":"support-for-additional-data-types"},{"level":4,"text":"Continuous Queries and Materialized Views","id":"continuous-queries-and-materialized-views"},{"level":4,"text":"Tiered Storage Integration","id":"tiered-storage-integration"},{"level":4,"text":"Advanced Compression Algorithms","id":"advanced-compression-algorithms"},{"level":4,"text":"Real-time Streaming Analytics","id":"real-time-streaming-analytics"},{"level":4,"text":"Advanced Indexing Strategies","id":"advanced-indexing-strategies"},{"level":4,"text":"Machine Learning Integration","id":"machine-learning-integration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Terminology Reference","id":"terminology-reference"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"}],"title":"TempoDB: A Modern Time-Series Database - Design Document","markdown":"# TempoDB: A Modern Time-Series Database - Design Document\n\n\n## Overview\n\nThis document outlines the design of TempoDB, a specialized database for storing and querying high-volume, timestamped data. It solves the core architectural challenge of achieving high write throughput for sequential data while enabling efficient range queries and compressing massive datasets with predictable patterns, making it essential for metrics, IoT, and financial applications.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** This foundational section establishes the core challenges addressed by all five milestones.\n\nTime-series data—sequential measurements indexed by time—is one of the fastest-growing data categories in modern computing. From monitoring server CPU usage every second to tracking sensor readings in industrial IoT and recording financial trades at millisecond intervals, this data forms the observational backbone of digital systems. However, storing and querying this data at scale presents unique architectural challenges that traditional relational or NoSQL databases struggle to address efficiently.\n\nThe fundamental characteristics of time-series data create a distinct set of requirements:\n\n| Characteristic | Implication for Database Design |\n|----------------|--------------------------------|\n| **High Volume & Velocity** | Individual measurements are typically small (a timestamp and value), but arrive continuously in massive volumes—potentially millions of points per second. The database must sustain high write throughput without compromising durability. |\n| **Append-Heavy, Rarely Updated** | Once recorded, a data point is almost never modified (though it may be deleted after retention periods). This allows for write-optimized storage structures that differ from update-friendly B-trees. |\n| **Time-Ordered Nature** | Points arrive roughly in chronological order and are most frequently queried by time ranges (\"last hour,\" \"yesterday\"). Temporal locality can be leveraged for compression and query optimization. |\n| **Periodic Patterns** | Many time series exhibit regular patterns (daily cycles, seasonal trends) where consecutive values change slowly, enabling specialized compression algorithms far more efficient than general-purpose compression. |\n| **Exploding Cardinality** | The combination of measurement names and tag combinations creates potentially millions of unique time series, requiring careful indexing strategies to avoid exponential growth in metadata overhead. |\n| **Decreasing Value Over Time** | Recent data is queried frequently with high resolution, while older data is accessed less often and can be aggregated or downsampled to save storage space. This enables tiered storage strategies. |\n\nThese characteristics create a fundamental tension: write paths must handle a relentless stream of small, sequential inserts, while read paths must efficiently retrieve and aggregate potentially vast ranges of historical data. Traditional databases optimized for transactional workloads (ACID guarantees, random updates) or document stores (flexible schemas, point lookups) fail to deliver the required performance at scale without excessive operational overhead.\n\n### Mental Model: The Data Stream Conveyor Belt\n\nImagine time-series data as a **high-speed conveyor belt** in a massive warehouse processing facility. Items (data points) arrive continuously, each with a timestamp (arrival time) and content (measurement value), plus labels indicating their type and origin (tags).\n\n```\n[2023-10-01 12:00:00, CPU=85%, server=web-01] --->\n[2023-10-01 12:00:01, CPU=87%, server=web-01] --->\n[2023-10-01 12:00:00, Temp=72.5F, sensor=A] --->\n[2023-10-01 12:00:01, Temp=72.6F, sensor=A] --->\n```\n\nOur database system acts as the entire warehouse logistics operation, which must:\n\n1. **Label and Sort** Each item must be categorized (by measurement and tags) and sorted chronologically as it arrives. Items arriving out of order (late packages) must be inserted into their correct position.\n\n2. **Compress Efficiently** Since similar items arrive consecutively (temperature readings change slowly), we can pack them tightly using specialized compression—storing only the differences between consecutive items rather than full representations.\n\n3. **Store in Time-Bound Containers** Rather than placing items individually on shelves, we batch them into time-bound containers (e.g., \"12:00-12:05 container for server web-01 CPU\"). Each container has a clear time range label on the outside, so we can quickly locate relevant containers without opening them.\n\n4. **Move Through Storage Tiers** Hot items (recent arrivals) remain near the front on easily accessible shelves (memory, fast SSDs). As items age, they move to deeper warehouse aisles (slower disks) and eventually to archival storage (cold storage), possibly in aggregated form (e.g., \"average temperature per hour\" rather than per-second readings).\n\n5. **Query with Temporal Precision** When someone asks, \"What was the CPU usage between 12:00 and 12:30?\", we don't scan every item in the warehouse. We first consult our container index, retrieve only containers overlapping that time range, then open and decompress just those containers.\n\nThis mental model highlights why generic storage systems struggle: they're designed for random access to individual items (like a traditional library where books are constantly reshelved), not for processing a continuous, time-ordered stream. Our design must embrace the conveyor belt nature—high-throughput ingestion at the front, intelligent batching and compression in the middle, and tiered archiving at the back.\n\n### Existing Approaches and Comparison\n\nWhen faced with time-series data, architects typically consider three categories of storage solutions, each with distinct trade-offs:\n\n#### Option 1: Generic Relational Databases (PostgreSQL, MySQL)\n\nThese systems store time-series data in conventional tables, often with a schema like:\n\n```sql\nCREATE TABLE metrics (\n    timestamp TIMESTAMP,\n    metric_name VARCHAR(255),\n    value DOUBLE PRECISION,\n    tags JSONB\n);\n```\n\n> **ADR: Choosing a Specialized Time-Series Database Over Generic RDBMS**\n> - **Context**: Teams often start with existing relational databases due to familiarity and existing infrastructure, but face scaling challenges as time-series data volume grows.\n> - **Options Considered**:\n>   1. **Naive table with indexes**: Simple `CREATE INDEX ON metrics(timestamp, metric_name)` \n>   2. **Table partitioning by time**: Manual or automatic partitioning by time ranges\n>   3. **TimescaleDB extension**: PostgreSQL extension adding time-series optimizations\n> - **Decision**: Build a specialized time-series database rather than using a generic RDBMS.\n> - **Rationale**:\n>   1. **Write amplification**: B-trees (the standard RDBMS index) incur significant overhead for append-heavy workloads due to random writes and page splits.\n>   2. **Storage inefficiency**: Row-based storage duplicates timestamp and tag columns for every measurement, while time-series compression can achieve 10-100x better ratios.\n>   3. **Query performance**: Range scans must traverse index trees not optimized for temporal locality, missing opportunities for block-level skipping.\n>   4. **Cardinality management**: High cardinality from tag combinations creates index bloat in RDBMS indexes.\n> - **Consequences**: We forgo full SQL support and ACID transactions across arbitrary tables, but gain order-of-magnitude improvements in write throughput, storage efficiency, and time-range query performance.\n\n| Approach | Pros | Cons | Why Not for High-Volume TSDB |\n|----------|------|------|------------------------------|\n| **Plain RDBMS table** | Full SQL, ACID transactions, well-understood | Poor write performance, storage bloat, index overhead on timestamps | B-tree index maintenance dominates write cost at high volumes |\n| **Time partitioning** | Faster deletion of old data, some query pruning | Manual management, still uses row storage, doesn't help compression | Reduces but doesn't eliminate fundamental architectural mismatches |\n| **TimescaleDB** | PostgreSQL compatibility, automated partitioning, some compression | Still bound by PostgreSQL's storage engine, higher latency than specialized TSDB | Good hybrid solution but not optimal for pure time-series workloads at extreme scale |\n\n#### Option 2: Specialized Time-Series Databases (InfluxDB, Prometheus)\n\nThese systems are built from the ground up for time-series data, employing specialized storage engines:\n\n- **InfluxDB TSM Engine**: Uses a Time-Structured Merge Tree with columnar storage within blocks, Gorilla compression for floats, and time-range indexing.\n- **Prometheus TSDB**: Uses a custom storage format with chunk-based encoding, separate head block for recent data, and compaction to larger chunks.\n\n> **ADR: Learning from Existing TSDB Architectures Without Direct Copying**\n> - **Context**: Multiple mature time-series databases exist with proven designs. We must decide how closely to follow existing patterns versus innovating.\n> - **Options Considered**:\n>   1. **Implement InfluxDB's TSM engine exactly**: Clone the open-source implementation\n>   2. **Implement Prometheus TSDB**: Adopt its chunk-based approach\n>   3. **Synthesize patterns with educational clarity**: Combine proven ideas with explicit design decisions and trade-offs\n> - **Decision**: Create a synthesis that highlights architectural decisions clearly while using proven patterns from industry.\n> - **Rationale**:\n>   1. **Educational value**: Direct cloning teaches implementation but not reasoning. Our design should expose decision points.\n>   2. **Simplification for learning**: Production systems include many optimizations for edge cases that obscure core concepts.\n>   3. **Pattern diversity**: Different systems excel in different aspects—InfluxDB's compression, Prometheus's chunk management, TimescaleDB's SQL integration.\n> - **Consequences**: Our design will resemble InfluxDB's TSM engine in structure but with simplified compaction, explicit ADRs for each major choice, and clear mapping from concepts to implementation.\n\n| System | Storage Engine | Compression | Query Language | Best For |\n|--------|---------------|-------------|----------------|----------|\n| **InfluxDB** | Time-Structured Merge Tree (TSM) | Gorilla XOR for floats, delta-of-delta for timestamps | InfluxQL (SQL-like), Flux | High-write environments, operational monitoring |\n| **Prometheus** | Custom chunk-based TSDB | Variable encoding based on data type | PromQL (functional) | Kubernetes/metrics monitoring, pull-based collection |\n| **TimescaleDB** | PostgreSQL with hypertables | Native compression, dictionary encoding | Full PostgreSQL SQL | Teams requiring SQL, mixed workload environments |\n| **ClickHouse** | MergeTree with LSM-like merges | Multiple codecs, delta encoding | SQL | Analytics, high-cardinality time-series |\n\n#### Option 3: General-Purpose Columnar Stores (Apache Parquet, Arrow)\n\nColumnar storage formats naturally align with time-series characteristics by storing all timestamps together and all values together:\n\n| Approach | Alignment with Time-Series | Challenges |\n|----------|---------------------------|------------|\n| **Columnar formats** | Excellent for compression (similar values stored contiguously), efficient for range scans | Not optimized for real-time ingestion, lack built-in time-series operations |\n| **Data lake architectures** | Cost-effective at petabyte scale, good for batch analytics | High query latency, not designed for real-time monitoring |\n\n> **Key Insight**: While columnar storage is theoretically ideal for time-series data, the missing piece is **ingestion efficiency**. Traditional columnar stores expect batch writes, while time-series requires sustained high-velocity ingestion with immediate queryability.\n\n#### Synthesis: The TempoDB Approach\n\nTempoDB adopts a **hybrid architecture** that combines the best elements of these approaches:\n\n1. **Specialized storage engine** based on Time-Structured Merge Tree principles (like InfluxDB) for high write throughput and compression\n2. **Columnar layout within blocks** (like Parquet) to enable efficient compression and scanning\n3. **SQL-like query language with time extensions** (bridging InfluxQL and TimescaleDB approaches) for familiarity\n4. **Built-in aggregation and downsampling** (like Prometheus) for data lifecycle management\n\nThe core innovation isn't in individual components—which are proven in production—but in their **pedagogical integration**: each design decision is explicitly justified, alternatives are documented, and implementation maps clearly to architectural concepts.\n\n### Common Pitfalls in Time-Series Database Design\n\n⚠️ **Pitfall: Treating Timestamps as Just Another Column**\n- **Description**: Storing timestamps in generic indexes without leveraging their monotonic, range-query-dominant nature.\n- **Why Wrong**: Generic indexes (B-trees) optimized for equality lookups waste space and CPU on timestamp ordering that's already inherent in append pattern.\n- **Fix**: Implement time-range indexes that store minimum/maximum timestamps per block, enabling entire blocks to be skipped during queries.\n\n⚠️ **Pitfall: Naive Tag Indexing Causing Cardinality Explosion**\n- **Description**: Creating separate indexes for each tag dimension leads to combinatorial explosion of index entries.\n- **Why Wrong**: With 10 servers, 5 regions, and 3 applications, you have 150 series. Each point writes to 150 index entries in naive implementation.\n- **Fix**: Use inverted index or tag-value pair indexing where each unique series key (measurement + tag combination) is indexed once, not per tag.\n\n⚠️ **Pitfall: Applying General-Purpose Compression**\n- **Description**: Using gzip or Snappy on entire data blocks without considering time-series patterns.\n- **Why Wrong**: Misses opportunity for order-of-magnitude better compression using delta-of-delta (timestamps) and XOR (floating point values).\n- **Fix**: Implement specialized time-series compression algorithms like Gorilla for floats and delta encoding for integers.\n\n⚠️ **Pitfall: Ignoring Out-of-Order Writes**\n- **Description**: Assuming timestamps always arrive in strictly increasing order.\n- **Why Wrong**: Network latency, clock skew, and batch processing can cause \"late\" data points that arrive after newer points.\n- **Fix**: Design write path to handle out-of-order data through buffering and periodic re-sorting during compaction.\n\n⚠️ **Pitfall: Querying Full Resolution for Historical Data**\n- **Description**: Storing and querying millisecond data for years-old metrics.\n- **Why Wrong**: Wastes storage and query CPU on precision no longer needed for trend analysis.\n- **Fix**: Implement automatic downsampling and retention policies that aggregate old data to lower resolutions.\n\nThe subsequent sections of this design document address these pitfalls through specific architectural choices, each documented with rationale and alternatives.\n\n### Implementation Guidance\n\n> **Note**: This section focuses on conceptual understanding rather than implementation. Detailed implementation guidance appears in later sections corresponding to each milestone. However, we provide foundational guidance here for setting up the project structure.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option (Recommended) | Advanced Option (Alternative) |\n|-----------|----------------------------|-------------------------------|\n| **Language** | Go (static typing, GC, good concurrency) | Rust (zero-cost abstractions, no GC) |\n| **Storage Abstraction** | Memory-mapped files with `mmap` syscall | Direct I/O with aligned buffers |\n| **Concurrency Control** | `sync.RWMutex` for shared mutable state | Lock-free ring buffers for writes |\n| **Serialization** | Custom binary format with `encoding/binary` | Protocol Buffers for metadata |\n| **HTTP Server** | Standard `net/http` with JSON/plaintext | FastHTTP for higher performance |\n| **Compression** | Custom Gorilla/delta implementations | Integrate `github.com/klauspost/compress` |\n\n#### B. Recommended File/Module Structure\n\nEstablish this directory structure from the beginning to maintain separation of concerns:\n\n```\ntempo/\n├── cmd/\n│   ├── tempo-server/          # Main server executable\n│   │   └── main.go\n│   └── tempo-cli/             # Command-line interface (optional)\n│       └── main.go\n├── internal/                  # Private application code\n│   ├── api/                   # HTTP/gRPC APIs (Milestone 5)\n│   │   ├── handler.go\n│   │   ├── query_parser.go\n│   │   └── line_protocol.go\n│   ├── storage/               # Storage engine (Milestone 1)\n│   │   ├── tsm/\n│   │   │   ├── writer.go\n│   │   │   ├── reader.go\n│   │   │   ├── compression.go\n│   │   │   └── file.go\n│   │   ├── wal/               # Write-ahead log (Milestone 2)\n│   │   │   ├── writer.go\n│   │   │   ├── reader.go\n│   │   │   └── segment.go\n│   │   ├── memtable/          # In-memory buffer (Milestone 2)\n│   │   │   ├── memtable.go\n│   │   │   └── series_key.go\n│   │   └── index/             # Series and tag indexing\n│   │       ├── inverted.go\n│   │       └── series_store.go\n│   ├── query/                 # Query engine (Milestone 3)\n│   │   ├── parser/\n│   │   │   ├── lexer.go\n│   │   │   └── parser.go\n│   │   ├── executor/\n│   │   │   ├── planner.go\n│   │   │   ├── iterator.go\n│   │   │   └── aggregator.go\n│   │   └── engine.go\n│   ├── compact/               # Compaction & retention (Milestone 4)\n│   │   ├── compactor.go\n│   │   ├── retention.go\n│   │   └── downsampler.go\n│   └── models/                # Shared data structures\n│       ├── point.go\n│       ├── series.go\n│       └── query.go\n├── pkg/                       # Public libraries (if any)\n├── scripts/                   # Build/test scripts\n├── testdata/                  # Test fixtures\n├── go.mod\n├── go.sum\n└── README.md\n```\n\n#### C. Core Type Definitions (Foundation)\n\nBefore implementing any component, define these foundational types in `internal/models/`:\n\n```go\n// File: internal/models/point.go\npackage models\n\nimport (\n    \"time\"\n)\n\n// DataPoint represents a single time-series measurement at a specific timestamp\ntype DataPoint struct {\n    Timestamp time.Time // Precision: nanosecond\n    Value     float64   // Supports integers via conversion\n    // Note: In a production system, we'd support multiple value types (int, bool, string)\n}\n\n// SeriesKey uniquely identifies a time series by measurement name and tags\ntype SeriesKey struct {\n    Measurement string            // e.g., \"cpu_usage\"\n    Tags        map[string]string // e.g., {\"host\": \"server-01\", \"region\": \"us-east\"}\n}\n\n// Series represents a complete time series: its identity and data points\ntype Series struct {\n    Key    SeriesKey\n    Points []DataPoint // In practice, points are stored in blocks, not individually\n}\n```\n\n```go\n// File: internal/models/query.go\npackage models\n\nimport (\n    \"time\"\n)\n\n// TimeRange represents an inclusive time window [Start, End]\ntype TimeRange struct {\n    Start time.Time\n    End   time.Time\n}\n\n// AggregateFunction defines supported aggregation operations\ntype AggregateFunction string\n\nconst (\n    AggregateSum  AggregateFunction = \"sum\"\n    AggregateAvg  AggregateFunction = \"avg\"\n    AggregateMin  AggregateFunction = \"min\"\n    AggregateMax  AggregateFunction = \"max\"\n    AggregateCount AggregateFunction = \"count\"\n)\n\n// Query represents a parsed query request\ntype Query struct {\n    Measurement   string\n    Tags          map[string]string      // Tag filters (AND relationship)\n    TimeRange     TimeRange\n    Aggregate     *AggregateFunction     // nil for raw points\n    GroupByWindow time.Duration          // e.g., 1h for hourly aggregation\n    Fields        []string               // For future: multiple fields per measurement\n}\n```\n\n#### D. Language-Specific Hints for Go\n\n1. **Time Handling**: Use `time.Time` for timestamps throughout. Convert to nanoseconds for storage: `timestamp.UnixNano()`.\n2. **Concurrent Maps**: Use `sync.Map` or `sync.RWMutex` with regular maps for shared mutable state. For high-throughput write buffers, consider sharded maps.\n3. **Binary Encoding**: Use `encoding/binary.BigEndian.PutUint64()` and similar for consistent cross-platform binary format.\n4. **Memory Management**: Preallocate slices with known capacity to avoid reallocations: `points := make([]DataPoint, 0, 1024)`.\n5. **Error Handling**: Use custom error types for domain-specific errors (e.g., `ErrOutOfOrderTimestamp`, `ErrSeriesNotFound`).\n6. **Testing**: Use table-driven tests extensively. Create golden files for binary format verification.\n\n#### E. First Milestone Checkpoint (Conceptual)\n\nBefore writing code, validate your understanding by answering:\n\n1. **Write down** three reasons why B-trees are suboptimal for time-series writes compared to TSM trees.\n2. **Sketch** how delta-of-delta encoding would compress this timestamp sequence (in nanoseconds): \n   `[1000, 2000, 3000, 4500, 6000]`\n3. **Explain** in one sentence why columnar storage within blocks helps compression for time-series data.\n\nExpected answers:\n1. B-trees cause random writes for sequential data, have high overhead for small inserts, and don't leverage temporal locality for compression.\n2. First delta: `[1000, 1000, 1000, 1500, 1500]`; Delta-of-delta: `[1000, 0, 0, 500, 0]` (most values are 0, compressible).\n3. Columnar storage groups similar values (timestamps with timestamps, values with values) allowing specialized compression algorithms to exploit patterns within each column.\n\nThis foundational understanding will guide implementation decisions throughout the project.\n\n\n> **Milestone(s):** This section establishes the foundational scope that guides all five milestones.\n\n## Goals and Non-Goals\n\nThis section clearly defines the boundaries of the TempoDB project. Establishing precise goals and, equally importantly, explicit non-goals is critical for building a focused, learnable system. Time-series databases are complex and can expand into numerous features (distributed scaling, full SQL, complex analytics). By defining a narrow but deep scope, we ensure we build a complete, working system that demonstrates the *core architectural patterns* of a time-series database without being overwhelmed by auxiliary concerns.\n\n### Goals\n\nThe primary goal of TempoDB is to create a single-node, educational time-series database that demonstrates the essential architectural patterns required to handle high-velocity, sequential data. The system must be **complete enough to be useful** for basic metrics collection and querying, and **pedagogically sound** to illustrate the key trade-offs in storage, ingestion, and query design. All goals are prioritized for clarity of implementation and learning value over production-grade robustness or extreme performance.\n\nThe following table enumerates the specific functional and non-functional requirements that TempoDB *must* deliver.\n\n| Category | Goal | Rationale & Learning Outcome |\n| :--- | :--- | :--- |\n| **Write Throughput** | Sustain thousands of writes per second on commodity hardware. | Demonstrates the importance of batching, buffering, and sequential I/O patterns for high-ingestion workloads, contrasting with random-write databases. |\n| **Storage Efficiency** | Achieve significant compression (targeting 10x or better for regular metrics) via **columnar storage**, **delta-of-delta encoding** for timestamps, and **Gorilla compression** for floating-point values. | Teaches how to exploit the predictable patterns (monotonic timestamps, slowly-changing values) inherent in time-series data to reduce storage costs. |\n| **Query Performance** | Support efficient **range queries** (e.g., \"last hour of data\") by scanning only relevant time blocks, using a **block-based index** with min/max timestamps. | Illustrates the principle of **temporal locality** and how indexing by time primary key is fundamentally different from indexing by arbitrary keys. |\n| **Data Model** | Implement an InfluxDB-like data model with **measurements**, **tags** (indexed key-value pairs), and **fields** (values). Support **series cardinality** tracking. | Provides a practical, industry-relevant model for organizing metrics and demonstrates the performance implications of high cardinality. |\n| **Query Language** | Provide a SQL-like query language supporting `SELECT`, `FROM`, `WHERE` (time and tag filters), and `GROUP BY time()` intervals with built-in **aggregation functions** (sum, avg, min, max, count). | Teaches query parsing, planning, and the pushdown of time-range predicates and simple aggregations to the storage layer. |\n| **Durability** | Guarantee write durability through a **Write-Ahead Log (WAL)**. Acknowledged writes must survive process crashes. | Introduces the fundamental pattern of logging before applying changes, which is critical for any reliable database system. |\n| **Data Lifecycle** | Enforce **Time-To-Live (TTL)** policies for automatic data expiration and implement **background compaction** to merge small files and reclaim space. | Shows how to manage the lifecycle of time-series data, which is typically valued less as it ages, and how to maintain read performance over time. |\n| **Downsampling** | Support **downsampling** queries that return lower-resolution, aggregated views of raw data (e.g., 1-minute averages from 1-second data). | Demonstrates a key analytical operation for visualizing long time ranges and pre-computing rollups for performance. |\n| **Operational Simplicity** | Expose a simple **HTTP API** for writes (compatible with InfluxDB line protocol) and queries, and **Grafana compatibility** for visualization. | Makes the system tangible and testable with common tools, reinforcing the connection between architecture and user-facing value. |\n| **Code Clarity** | The implementation should be well-structured, modular, and documented to serve as a learning artifact. Primary logic should avoid unnecessary concurrency complexity initially. | The ultimate goal is education; the code must be readable and traceable to the design concepts explained in this document. |\n\n> **Key Design Principle:** *Depth over Breadth.* Each implemented feature should be built to its logical conclusion, demonstrating the complete data path from API to disk and back, rather than sketching many half-features.\n\n#### Non-Functional Goal Elaboration\n\n*   **Performance Profile:** Write latency should be predictable and dominated by periodic disk flushes rather than per-point overhead. Read latency for recent data should be sub-second, utilizing in-memory structures (memtable). Full historical scans will be slower but bounded by sequential disk I/O.\n*   **Resource Usage:** The database should be memory-efficient, avoiding loading entire datasets into memory. Compression reduces disk footprint. The system should be stable under sustained write load, employing backpressure when necessary.\n*   **Correctness:** The system must provide durable writes and correct query results. For the educational scope, we prioritize \"eventual\" correctness after crashes (via WAL replay) over complex transactional guarantees.\n\n### Non-Goals\n\nExplicitly stating what TempoDB will *not* do is essential to prevent scope creep and to focus effort on the core learning objectives. The following features are deliberately excluded from the initial design. Many are important for a production system but represent orthogonal complexities that can be studied independently after mastering the fundamentals.\n\n| Feature | Reason for Exclusion | Potential Learning Extension |\n| :--- | :--- | :--- |\n| **Horizontal Scaling / Clustering** | Distributing data across multiple nodes introduces complex problems of consistency, replication, and global querying (e.g., consensus, vector clocks, distributed query planning). This is a vast topic deserving its own dedicated study. | A future extension could add a simple sharding layer based on measurement name or hash of series key. |\n| **Full SQL Support** | Implementing a complete SQL parser, optimizer, and executor with joins, subqueries, and complex expressions is a massive undertaking that distracts from time-series-specific optimizations. | The simple query language can be extended with more functions and syntax over time. |\n| **Cross-Series Transactions (ACID)** | Time-series data is primarily append-only. Supporting multi-series, multi-statement transactions with rollback adds significant complexity (locking, undo logs) for limited benefit in the target use cases. | Basic single-series atomicity is provided by the WAL. |\n| **Advanced Security (RBAC, Encryption)** | Authentication, authorization, and encryption are critical for production but are generic infrastructure concerns not unique to time-series databases. | Can be added via a middleware proxy or later integration. |\n| **Continuous Queries** | While a common feature (pre-computing aggregations in real-time), they are essentially a streaming computation layer on top of the write path. They add significant scheduling and state management complexity. | Can be implemented as a separate background job scheduler that runs periodic aggregation queries. |\n| **Tiered Storage (Auto-migration to S3)** | Automatically moving cold data to object storage involves lifecycle management, network I/O, and possibly different file formats. It's a valuable production feature but an orthogonal storage layer concern. | A manual \"export to S3\" command could be a simpler first step. |\n| **Advanced Data Types** | Support for complex types (arrays, histograms, geospatial) or efficient compression for integers, booleans, and strings. We focus on floating-point numbers as the most common and pedagogically interesting case for compression. | New compression schemes (e.g., for integers) can be added as pluggable codecs. |\n| **High Availability (Replication, Failover)** | Ensuring the database survives machine failures requires replication and automatic failover, which again enters the domain of distributed systems. | A simple primary-standby replication using WAL shipping could be a follow-on project. |\n| **Sophisticated Memory Management** | Production databases often use custom memory allocators, page caches, and direct I/O to optimize performance. These are advanced system programming topics. | We rely on Go's runtime and standard library for memory and file I/O for simplicity. |\n| **Pluggable Storage Backends** | Abstracting the storage engine to support multiple backends (e.g., PostgreSQL, cloud storage) adds abstraction overhead and obscures the core TSM engine design. | The TSM engine *is* the core learning artifact. |\n\n> **Key Design Principle:** *Defer Complexity.* The system is designed to be a **vertical slice** of a time-series database, not a **horizontal platform**. By deferring or omitting these features, we maintain a clear line of sight from the core data structures on disk to the user-facing API, which is the primary educational objective.\n\n#### The \"Not Now\" vs. \"Not Ever\" Distinction\n\nFor the purposes of this project and design document, the listed non-goals are considered out of scope. However, the architecture does not intentionally preclude their future addition. For example:\n*   The **`SeriesKey`** structure could be extended.\n*   The **TSM file format** includes versioning in its header for future evolution.\n*   The **HTTP API** could be extended with new endpoints.\n\nThe decision to exclude a feature is based on its impact on the **initial learning journey**, not on its inherent value.\n\n### Summary of Scope\n\nTempoDB will be a fully functional, single-node time-series database that excels at ingesting and compressing regular measurements and answering time-range queries with aggregations. It will not be a distributed, highly available, general-purpose SQL database. This focused scope allows us to build a complete system that deeply explores the unique architectural patterns—such as columnar block storage, time-based compaction, and specialized compression—that make time-series databases a distinct and valuable category of data infrastructure.\n\n---\n\n\n## High-Level Architecture\n\n> **Milestone(s):** This architectural overview provides the foundation for all five milestones, showing how components interact to achieve the system's core capabilities.\n\nTempoDB follows a layered architecture that separates concerns between ingestion, storage, query processing, and maintenance. The core insight is the separation between write-optimized and read-optimized data structures: incoming writes are buffered in memory for fast acknowledgment, then periodically flushed to disk in an optimized columnar format. This pattern, common in modern databases, provides the dual benefits of high write throughput and efficient read performance.\n\n### Mental Model: The Time-Series Processing Pipeline\n\nImagine TempoDB as a specialized factory for processing time-stamped measurements. Raw data arrives continuously on a high-speed conveyor belt (the **write path**). The factory has several specialized stations:\n\n1. **Receiving Dock (Ingest API)**: Validates and categorizes incoming shipments\n2. **Receipt Printer (Write-Ahead Log)**: Creates durable records of each shipment before any processing\n3. **Sorting Buffer (Memtable)**: Temporarily holds items while they're being organized\n4. **Packaging Department (Storage Engine)**: Compresses and packages items into optimized containers (TSM files) for long-term storage\n5. **Retrieval Desk (Query Engine)**: Fetches and processes packages based on customer requests\n6. **Warehouse Maintenance (Compactor)**: Periodically reorganizes storage, discards expired items, and creates summary catalogs\n\nThis mental model helps visualize how data flows from ingestion to storage and back out through queries, with each component having a clear responsibility in the pipeline.\n\n### Component Overview and Responsibilities\n\nThe following diagram shows the major components and their interactions:\n\n![TempoDB High-Level System Architecture](./diagrams/system-component.svg)\n\nEach component has specific responsibilities and interacts with others through well-defined interfaces. The table below summarizes these responsibilities:\n\n| Component | Primary Responsibility | Key Data It Owns | Performance Critical Operations |\n|-----------|------------------------|------------------|---------------------------------|\n| **Ingest API** | Accept writes and queries from clients, validate input, format responses | Client connections, request buffers | Request parsing, response serialization |\n| **Write-Ahead Log (WAL)** | Ensure write durability before acknowledging to client | WAL segment files on disk | Sequential append with `fsync`, quick recovery scan |\n| **Memtable** | Buffer writes in memory for fast acknowledgment and batching | In-memory sorted data structure per series | Concurrent insert, range scan for flush |\n| **Storage Engine (TSM)** | Persist data to disk in optimized columnar format, serve reads | TSM files on disk, block cache | Range scans with predicate pushdown, compression/decompression |\n| **Query Engine** | Parse, plan, and execute queries, streaming results | Query plans, execution state | Index lookups, aggregation pushdown, result streaming |\n| **Compactor** | Background maintenance: merge files, delete expired data, create rollups | Compaction state, scheduling metadata | Disk I/O scheduling, resource throttling |\n\nLet's examine each component in detail:\n\n#### Ingest API\n\nThe **Ingest API** serves as the system's front door, handling all external communication. It exposes two primary interfaces:\n1. **Write API**: Accepts time-series data points, typically in InfluxDB line protocol format\n2. **Query API**: Accepts query requests and returns results in JSON or other formats\n\nThis component is responsible for request validation, authentication (if implemented), rate limiting, and connection management. It translates external requests into internal operations and formats responses for clients. The API supports both HTTP/REST and (optionally) gRPC interfaces.\n\n**Key Design Decisions**:\n- Use a connection pool to handle concurrent requests efficiently\n- Validate all incoming data before processing (timestamp ranges, value types, etc.)\n- Implement request timeouts to prevent hung connections from consuming resources\n- Support content negotiation for different response formats (JSON, CSV, binary)\n\n#### Write-Ahead Log (WAL)\n\nThe **Write-Ahead Log** provides durability guarantees by recording every write operation to disk before acknowledging it to the client. This follows the \"write-ahead logging\" principle common in database systems: no modification to persistent data structures occurs without first being logged.\n\n> **Design Insight**: The WAL acts as a \"flight recorder\" for the database. In the event of a crash, the system can replay the WAL to reconstruct the state of in-memory buffers that were lost, ensuring no acknowledged writes are lost.\n\nThe WAL uses a segment-based architecture:\n- **Active segment**: Currently being written to\n- **Closed segments**: Previously filled segments that can be deleted after their contents are flushed to TSM files\n- **Index file**: Maps series keys to their positions in WAL segments for efficient recovery\n\nEach WAL entry contains the full `DataPoint` information (series key, timestamp, value) in a compact binary format. The WAL must support extremely fast sequential writes and periodic `fsync` operations to ensure durability.\n\n#### Memtable\n\nThe **Memtable** (memory table) is an in-memory buffer that holds recently written data points before they're flushed to disk. It serves several purposes:\n1. **Write amplification reduction**: Batches multiple writes into single disk operations\n2. **Fast acknowledgment**: Allows immediate acknowledgment of writes after WAL logging\n3. **Temporal locality**: Recent data (often queried together) remains in memory\n4. **Sorting**: Organizes data by series and timestamp before disk persistence\n\nThe memtable uses a sorted data structure (typically a skip list or B-tree) keyed by `SeriesKey` and `Timestamp`. When it reaches a configurable size threshold (e.g., 64MB), it's marked as **immutable**, a new **active** memtable is created, and the immutable memtable is scheduled for flushing to disk.\n\n> **ADR: Choosing Memtable Data Structure**\n> - **Context**: Need fast concurrent inserts and efficient range scans for flushing\n> - **Options Considered**:\n>   1. **Concurrent skip list**: Lock-free or fine-grained locking, excellent for range queries\n>   2. **B-tree with copy-on-write**: Good for ordered traversal, moderate concurrency\n>   3. **Per-series sorted slice with global lock**: Simple but poor concurrency\n> - **Decision**: Concurrent skip list implementation\n> - **Rationale**: Skip lists provide O(log n) operations with good concurrency characteristics and excellent cache locality for range scans, which are critical during flush operations\n> - **Consequences**: Higher memory overhead than B-trees (~50% more pointers) but simpler concurrent implementation without complex rebalancing logic\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Concurrent skip list | Excellent concurrency, good range scan performance, simple implementation | Higher memory overhead (~2N pointers), not cache-optimal | **Yes** |\n| B-tree with copy-on-write | Good memory efficiency, excellent for ordered traversal | Complex concurrent modification, rebalancing overhead | No |\n| Per-series sorted slice | Simple, excellent for flushing by series | Poor concurrent write performance, expensive inserts | No |\n\n#### Storage Engine (TSM)\n\nThe **Storage Engine** implements the Time-Structured Merge Tree (TSM) format for persistent storage. This is the core of TempoDB's read-optimized storage, where data is organized in a columnar layout with aggressive compression.\n\nThe TSM engine manages:\n1. **TSM files**: Immutable files containing compressed time-series data\n2. **Block cache**: Frequently accessed data blocks kept in memory\n3. **File index**: In-memory mapping of series keys to their locations in TSM files\n4. **Statistics**: Min/max timestamps, value ranges, and other metadata per block\n\nEach TSM file contains multiple **series blocks**, each holding data for a single time series. Within a series block, data is organized into **data blocks** covering contiguous time ranges. Each data block stores timestamps and values in separate columns with specialized compression.\n\n**Data Flow Through Storage Engine**:\n1. During flush, the memtable's data is sorted by series and timestamp\n2. Each series' data is compressed using delta-of-delta (timestamps) and Gorilla XOR (values)\n3. Compressed blocks are written to a new TSM file along with index information\n4. The file index is updated to include the new file's time ranges\n5. Old TSM files (from previous compactions) may be marked for deletion\n\n#### Query Engine\n\nThe **Query Engine** processes read requests, transforming high-level queries into efficient storage operations. It follows a traditional database query processing pipeline with time-series optimizations:\n\n```\nParse → Plan → Optimize → Execute → Stream\n```\n\nThe key optimization is **predicate pushdown**: time range and tag filters are applied as early as possible, ideally at the storage layer, to minimize data movement. The query engine also handles:\n- **Aggregation**: Built-in functions (sum, avg, min, max, count) with pushdown where possible\n- **Downsampling**: GROUP BY time() operations that bucket data into fixed intervals\n- **Multi-series queries**: Joining data from multiple series based on time alignment\n- **Result streaming**: Progressive return of results to avoid large memory allocations\n\nThe engine uses an **iterator model** where each stage of query processing produces a stream of `DataPoint` records that flow through operators (filter, aggregate, transform).\n\n#### Compactor\n\nThe **Compactor** handles background maintenance tasks that optimize storage and enforce data lifecycle policies:\n\n| Task | Frequency | Purpose | Resource Impact |\n|------|-----------|---------|-----------------|\n| **Memtable flush** | When memtable reaches size threshold | Move data from memory to persistent storage | High I/O (write burst) |\n| **TSM compaction** | Periodically or when many small files exist | Merge small TSM files into larger ones, remove duplicates | Moderate I/O and CPU |\n| **Retention enforcement** | Scheduled (e.g., hourly) | Delete data older than retention period | Moderate I/O (file deletion) |\n| **Rollup generation** | Scheduled for historical data | Create pre-aggregated summaries for fast queries | High CPU, moderate I/O |\n\nThe compactor uses a **leveled compaction strategy** similar to LSM trees but optimized for time-series:\n- **Level 0**: Recently flushed TSM files (may have overlapping time ranges)\n- **Level 1-3**: Compacted files with non-overlapping time ranges\n- Each level has a target file size, with older data in larger files\n\n> **Design Insight**: Compaction is scheduled based on both time and space heuristics. Time-based scheduling ensures predictable performance impact, while space-based triggers prevent storage inefficiency from accumulating too many small files.\n\n### Component Interactions\n\nThe components interact through well-defined interfaces and protocols. The following tables describe the key interactions:\n\n**Write Path Interactions**:\n\n| From Component | To Component | Data Passed | Trigger Condition |\n|----------------|--------------|-------------|-------------------|\n| Ingest API | WAL | `SeriesKey` + `DataPoint` | Every write request |\n| WAL | Memtable | `SeriesKey` + `DataPoint` | After successful log append |\n| Ingest API | Client | Acknowledgment | After WAL append (or async after memtable) |\n| Memtable | Storage Engine | Batch of `DataPoint` per series | Memtable reaches size threshold |\n| Storage Engine | Compactor | TSM file metadata | New TSM file created |\n\n**Read Path Interactions**:\n\n| From Component | To Component | Data Passed | Purpose |\n|----------------|--------------|-------------|---------|\n| Ingest API | Query Engine | `Query` structure | Query request |\n| Query Engine | Storage Engine | Time range + series filter | Data retrieval |\n| Storage Engine | Query Engine | Stream of `DataPoint` | Query results |\n| Query Engine | Ingest API | Formatted results | Response to client |\n\n**Background Task Interactions**:\n\n| From Component | To Component | Data Passed | Frequency |\n|----------------|--------------|-------------|-----------|\n| Compactor | Storage Engine | List of TSM files to merge | Scheduled or threshold-based |\n| Storage Engine | Compactor | New TSM file after compaction | After compaction completes |\n| Compactor | File System | Paths of expired TSM files | Retention policy schedule |\n\n### Common Pitfalls in Architecture Design\n\n⚠️ **Pitfall: Tight coupling between components**\n- **Description**: Making components directly depend on each other's internal implementations\n- **Why it's wrong**: Makes testing difficult, inhibits independent evolution, complicates debugging\n- **Fix**: Define clear interfaces between components, use dependency injection for testing\n\n⚠️ **Pitfall: Single-threaded bottleneck in write path**\n- **Description**: All writes go through a global lock or single goroutine\n- **Why it's wrong**: Limits write throughput, doesn't scale with cores\n- **Fix**: Use sharded memtables (by series hash), lock-free data structures, or partitioned WAL\n\n⚠️ **Pitfall: Inefficient memory management**\n- **Description**: Allocating individual `DataPoint` objects, causing GC pressure\n- **Why it's wrong**: High garbage collection overhead reduces throughput\n- **Fix**: Use memory pools, slice-based storage, or arena allocation for batched points\n\n⚠️ **Pitfall: Blocking writes during flush**\n- **Description**: Stopping write acceptance while memtable flushes to disk\n- **Why it's wrong**: Creates write latency spikes, reduces availability\n- **Fix**: Implement double buffering with active and immutable memtables\n\n⚠️ **Pitfall: No backpressure mechanism**\n- **Description**: Accepting writes indefinitely even when system is overloaded\n- **Why it's wrong**: Leads to out-of-memory crashes or severe performance degradation\n- **Fix**: Implement write rejection or throttling when buffers exceed thresholds\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option (Recommended) | Advanced Option (Extension) |\n|-----------|----------------------------|----------------------------|\n| **Ingest API** | `net/http` with JSON/line protocol | gRPC with Protocol Buffers |\n| **WAL Persistence** | Direct file I/O with `os.File` | Memory-mapped files with `syscall.Mmap` |\n| **Memtable** | Skip list with `sync.RWMutex` | Lock-free skip list using atomic operations |\n| **TSM File I/O** | Standard file operations | Memory-mapped file access |\n| **Concurrency Control** | Channel-based pipelines | Work-stealing scheduler |\n| **Compression** | Custom delta/Gorilla implementation | Plug-in architecture for multiple algorithms |\n\n#### Recommended File/Module Structure\n\nOrganize the codebase as follows to maintain separation of concerns and enable clean testing:\n\n```\ntempo/\n├── cmd/\n│   ├── tempo-server/\n│   │   └── main.go              # Server entry point\n│   └── tempo-cli/\n│       └── main.go              # CLI tool (optional)\n├── internal/\n│   ├── api/\n│   │   ├── http/\n│   │   │   ├── server.go        # HTTP server setup\n│   │   │   ├── handlers.go      # Request handlers\n│   │   │   └── middleware.go    # Auth, logging middleware\n│   │   └── query/\n│   │       ├── parser.go        # Query language parser\n│   │       ├── planner.go       # Query plan generation\n│   │       └── executor.go      # Query execution\n│   ├── storage/\n│   │   ├── wal/\n│   │   │   ├── wal.go           # Write-ahead log implementation\n│   │   │   ├── segment.go       # WAL segment management\n│   │   │   └── recovery.go      # WAL recovery on startup\n│   │   ├── memtable/\n│   │   │   ├── memtable.go      # In-memory table interface\n│   │   │   ├── skiplist.go      # Skip list implementation\n│   │   │   └── buffer_pool.go   # Memory pool for points\n│   │   ├── tsm/\n│   │   │   ├── writer.go        # TSM file writer\n│   │   │   ├── reader.go        # TSM file reader\n│   │   │   ├── index.go         # File index management\n│   │   │   └── compression.go   # Compression algorithms\n│   │   └── engine/\n│   │       ├── engine.go        # Storage engine facade\n│   │       ├── cache.go         # Block cache\n│   │       └── statistics.go    # Storage statistics\n│   ├── query/\n│   │   ├── engine.go            # Query engine coordination\n│   │   ├── iterator.go          # Iterator interfaces\n│   │   ├── aggregator.go        # Aggregation functions\n│   │   └── downsampler.go       # Downsampling logic\n│   ├── compaction/\n│   │   ├── manager.go           # Compaction coordination\n│   │   ├── planner.go           # Compaction planning\n│   │   ├── executor.go          # Compaction execution\n│   │   └── retention.go         # TTL enforcement\n│   └── meta/\n│       ├── catalog.go           # Series catalog\n│       ├── statistics.go        # System statistics\n│       └── sharding.go          # Shard management (future)\n├── pkg/\n│   ├── models/\n│   │   ├── point.go             # DataPoint, SeriesKey types\n│   │   ├── query.go             # Query structure\n│   │   └── tsm.go               # TSM block structures\n│   ├── protocol/\n│   │   ├── lineproto.go         # Line protocol parser\n│   │   └── prometheus.go        # Prometheus remote read/write\n│   └── utils/\n│       ├── timeutil.go          # Time utilities\n│       ├── bytesutil.go         # Byte manipulation\n│       └── pool.go              # Object pools\n└── test/\n    ├── integration/\n    │   └── end_to_end_test.go   # Integration tests\n    └── benchmarks/\n        └── write_bench_test.go  # Performance benchmarks\n```\n\n#### Infrastructure Starter Code\n\nHere's a complete, ready-to-use implementation for the WAL segment management, a common infrastructure component:\n\n```go\n// internal/storage/wal/segment.go\npackage wal\n\nimport (\n    \"encoding/binary\"\n    \"fmt\"\n    \"os\"\n    \"path/filepath\"\n    \"sync\"\n    \"time\"\n)\n\n// Segment represents a single WAL segment file\ntype Segment struct {\n    path     string\n    file     *os.File\n    mu       sync.RWMutex\n    size     int64\n    maxSize  int64\n    closed   bool\n    firstID  uint64  // First entry ID in this segment\n    lastID   uint64  // Last entry ID in this segment\n}\n\n// SegmentConfig holds configuration for WAL segments\ntype SegmentConfig struct {\n    MaxSizeBytes int64\n    SyncInterval time.Duration\n    SyncOnWrite  bool\n}\n\n// NewSegment creates or opens a WAL segment file\nfunc NewSegment(path string, firstID uint64, config SegmentConfig) (*Segment, error) {\n    // Create directory if it doesn't exist\n    if err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {\n        return nil, fmt.Errorf(\"create wal directory: %w\", err)\n    }\n    \n    // Open or create the file\n    file, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_APPEND, 0644)\n    if err != nil {\n        return nil, fmt.Errorf(\"open wal segment: %w\", err)\n    }\n    \n    // Get current size\n    info, err := file.Stat()\n    if err != nil {\n        file.Close()\n        return nil, fmt.Errorf(\"stat wal segment: %w\", err)\n    }\n    \n    segment := &Segment{\n        path:    path,\n        file:    file,\n        size:    info.Size(),\n        maxSize: config.MaxSizeBytes,\n        firstID: firstID,\n        lastID:  firstID - 1, // Will be incremented on first write\n    }\n    \n    // If file is not empty, scan to find the last entry ID\n    if segment.size > 0 {\n        if err := segment.scanLastID(); err != nil {\n            segment.Close()\n            return nil, fmt.Errorf(\"scan existing wal segment: %w\", err)\n        }\n    }\n    \n    return segment, nil\n}\n\n// WriteEntry appends an entry to the segment\nfunc (s *Segment) WriteEntry(data []byte) (uint64, error) {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    if s.closed {\n        return 0, fmt.Errorf(\"segment is closed\")\n    }\n    \n    // Check if we need to rotate\n    entrySize := int64(len(data) + 8) // 8 bytes for CRC32\n    if s.size+entrySize > s.maxSize {\n        return 0, ErrSegmentFull\n    }\n    \n    // Generate entry ID\n    s.lastID++\n    entryID := s.lastID\n    \n    // Write entry: [4-byte CRC32][4-byte data length][data]\n    var buf [8]byte\n    binary.LittleEndian.PutUint32(buf[0:4], crc32Checksum(data))\n    binary.LittleEndian.PutUint32(buf[4:8], uint32(len(data)))\n    \n    // Write header\n    if _, err := s.file.Write(buf[:]); err != nil {\n        s.lastID-- // Rollback ID on error\n        return 0, fmt.Errorf(\"write wal entry header: %w\", err)\n    }\n    \n    // Write data\n    if _, err := s.file.Write(data); err != nil {\n        // Try to truncate the partial write\n        s.file.Truncate(s.size)\n        s.lastID-- // Rollback ID on error\n        return 0, fmt.Errorf(\"write wal entry data: %w\", err)\n    }\n    \n    s.size += int64(len(data) + 8)\n    \n    // Sync if configured\n    // (SyncOnWrite logic would go here)\n    \n    return entryID, nil\n}\n\n// Scan reads all entries from the segment\nfunc (s *Segment) Scan(fn func(id uint64, data []byte) error) error {\n    s.mu.RLock()\n    defer s.mu.RUnlock()\n    \n    // Seek to beginning\n    if _, err := s.file.Seek(0, 0); err != nil {\n        return fmt.Errorf(\"seek wal segment: %w\", err)\n    }\n    \n    var nextID uint64 = s.firstID\n    for {\n        // Read header\n        var header [8]byte\n        n, err := s.file.Read(header[:])\n        if err != nil || n != 8 {\n            if err != nil && err.Error() == \"EOF\" {\n                break\n            }\n            return fmt.Errorf(\"read wal entry header: %w\", err)\n        }\n        \n        // Extract length and checksum\n        expectedCRC := binary.LittleEndian.Uint32(header[0:4])\n        dataLen := binary.LittleEndian.Uint32(header[4:8])\n        \n        // Read data\n        data := make([]byte, dataLen)\n        n, err = s.file.Read(data)\n        if err != nil || uint32(n) != dataLen {\n            return fmt.Errorf(\"read wal entry data: %w\", err)\n        }\n        \n        // Verify checksum\n        actualCRC := crc32Checksum(data)\n        if actualCRC != expectedCRC {\n            return fmt.Errorf(\"wal entry checksum mismatch: expected %x, got %x\", \n                expectedCRC, actualCRC)\n        }\n        \n        // Call callback\n        if err := fn(nextID, data); err != nil {\n            return err\n        }\n        \n        nextID++\n    }\n    \n    return nil\n}\n\n// Close closes the segment file\nfunc (s *Segment) Close() error {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    if s.closed {\n        return nil\n    }\n    \n    s.closed = true\n    return s.file.Close()\n}\n\n// Delete removes the segment file from disk\nfunc (s *Segment) Delete() error {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    if !s.closed {\n        s.file.Close()\n    }\n    \n    return os.Remove(s.path)\n}\n\n// Helper methods\nfunc (s *Segment) scanLastID() error {\n    // Implementation scans file to find last entry\n    // Returns last ID found\n    return nil\n}\n\nfunc crc32Checksum(data []byte) uint32 {\n    // Simple implementation - use actual crc32 in production\n    var sum uint32\n    for _, b := range data {\n        sum = (sum << 5) ^ (sum >> 27) ^ uint32(b)\n    }\n    return sum\n}\n\n// Errors\nvar ErrSegmentFull = fmt.Errorf(\"wal segment is full\")\n```\n\n#### Core Logic Skeleton Code\n\nHere's skeleton code for the key coordination component, the Storage Engine facade:\n\n```go\n// internal/storage/engine/engine.go\npackage engine\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n    \n    \"tempo/internal/storage/memtable\"\n    \"tempo/internal/storage/tsm\"\n    \"tempo/internal/storage/wal\"\n    \"tempo/pkg/models\"\n)\n\n// StorageEngine coordinates all storage components\ntype StorageEngine struct {\n    config Config\n    \n    // Components\n    wal       *wal.WAL\n    memtables *MemtableManager\n    tsmReader *tsm.Reader\n    tsmWriter *tsm.Writer\n    \n    // State\n    mu          sync.RWMutex\n    seriesIndex map[string]*SeriesMetadata  // SeriesKey hash -> metadata\n    tsmFiles    []*TSMFileRef               // Active TSM files\n    blockCache  *BlockCache\n    \n    // Channels for coordination\n    flushCh     chan *memtable.Memtable\n    compactCh   chan compactionPlan\n    stopCh      chan struct{}\n}\n\n// Config holds storage engine configuration\ntype Config struct {\n    DataDir          string\n    MaxMemtableSize  int64\n    WalSegmentSize   int64\n    BlockCacheSize   int64\n    FlushConcurrency int\n}\n\n// NewStorageEngine initializes the storage engine\nfunc NewStorageEngine(config Config) (*StorageEngine, error) {\n    // TODO 1: Create data directory structure if it doesn't exist\n    \n    // TODO 2: Initialize WAL for durability\n    \n    // TODO 3: Initialize memtable manager with active memtable\n    \n    // TODO 4: Scan existing TSM files and load their indices\n    \n    // TODO 5: Initialize block cache with configured size\n    \n    // TODO 6: Start background goroutines for flush and compaction\n    \n    // TODO 7: Recover any unflushed data from WAL on startup\n    \n    return nil, nil // Replace with actual return\n}\n\n// WritePoint writes a single data point\nfunc (e *StorageEngine) WritePoint(ctx context.Context, seriesKey models.SeriesKey, point models.DataPoint) error {\n    // TODO 1: Check if context is cancelled\n    \n    // TODO 2: Validate point (timestamp not in future, valid value, etc.)\n    \n    // TODO 3: Write to WAL first for durability (this may block on fsync)\n    \n    // TODO 4: Insert into active memtable (concurrent safe)\n    \n    // TODO 5: Check if memtable needs flushing (size threshold)\n    //         If yes, trigger async flush and rotate to new memtable\n    \n    // TODO 6: Update series metadata (last timestamp, point count)\n    \n    // TODO 7: Return nil on success, error on failure\n    \n    return nil\n}\n\n// Query executes a range query\nfunc (e *StorageEngine) Query(ctx context.Context, query models.Query) (QueryResult, error) {\n    // TODO 1: Parse query time range and validate\n    \n    // TODO 2: Resolve series keys from measurement and tags\n    \n    // TODO 3: For each series, determine which TSM files contain relevant data\n    \n    // TODO 4: Create query plan with predicate pushdown optimization\n    \n    // TODO 5: Execute plan: scan memtables first (recent data), then TSM files\n    \n    // TODO 6: Apply any aggregations or downsampling in the plan\n    \n    // TODO 7: Stream results back through channel to avoid large allocations\n    \n    return QueryResult{}, nil\n}\n\n// FlushMemtable flushes an immutable memtable to TSM files\nfunc (e *StorageEngine) FlushMemtable(mt *memtable.Memtable) error {\n    // TODO 1: Create new TSM file writer\n    \n    // TODO 2: Iterate through memtable data sorted by series and timestamp\n    \n    // TODO 3: For each series, compress data using delta-of-delta and Gorilla\n    \n    // TODO 4: Write compressed blocks to TSM file\n    \n    // TODO 5: Build index mapping series to block offsets\n    \n    // TODO 6: Finalize TSM file and sync to disk\n    \n    // TODO 7: Update engine's file list and index\n    \n    // TODO 8: Signal WAL that data is persisted (can delete old segments)\n    \n    // TODO 9: Update statistics\n    \n    return nil\n}\n\n// Compact runs compaction on selected TSM files\nfunc (e *StorageEngine) Compact(plan compactionPlan) error {\n    // TODO 1: Check if compaction should proceed (enough disk space, not too many files)\n    \n    // TODO 2: Read all data from source TSM files\n    \n    // TODO 3: Merge data, removing duplicates and expired points\n    \n    // TODO 4: Write merged data to new TSM file(s)\n    \n    // TODO 5: Atomically switch to new files and mark old ones for deletion\n    \n    // TODO 6: Schedule deletion of old files after safe period\n    \n    // TODO 7: Update compaction statistics\n    \n    return nil\n}\n\n// Close gracefully shuts down the storage engine\nfunc (e *StorageEngine) Close() error {\n    // TODO 1: Signal background goroutines to stop\n    \n    // TODO 2: Wait for in-progress flushes and compactions to complete\n    \n    // TODO 3: Flush any remaining data in memtables\n    \n    // TODO 4: Close all components (WAL, block cache, file handles)\n    \n    // TODO 5: Persist any metadata needed for clean restart\n    \n    return nil\n}\n```\n\n#### Language-Specific Hints for Go\n\n1. **Concurrency**: Use `sync.RWMutex` for read-heavy structures like the series index. For write-heavy paths, consider sharding or lock-free data structures.\n\n2. **Memory Management**: Use `sync.Pool` for frequently allocated objects like `DataPoint` slices or compression buffers to reduce GC pressure.\n\n3. **File I/O**: Use `os.File` with appropriate buffering. For sequential writes, `bufio.Writer` can help batch small writes. Remember to call `Sync()` for durability.\n\n4. **Error Handling**: Use Go's error wrapping with `fmt.Errorf(\"... %w\", err)` to preserve error context. Define sentinel errors for expected failure cases.\n\n5. **Context Propagation**: Pass `context.Context` through all I/O operations to support cancellation and timeouts.\n\n6. **Testing**: Use table-driven tests with `t.Run()` subtests. For integration tests, use temporary directories created with `t.TempDir()`.\n\n7. **Profiling**: Integrate `net/http/pprof` early for performance debugging. Use `runtime.MemStats` to track memory usage.\n\n\n## Data Model\n\n> **Milestone(s):** This section establishes the foundational data structures that underpin all five milestones, particularly Milestone 1 (Storage Engine) and Milestone 2 (Write Path). The data model dictates how points are represented, organized, and stored, directly influencing compression efficiency, query performance, and storage layout.\n\nThe data model is the fundamental blueprint for how TempoDB organizes and interprets the raw stream of timestamped data. It defines the vocabulary and grammar we use to structure observations from the physical world—like server CPU usage or sensor temperature—into a format the database can efficiently store and retrieve. A well-designed data model balances expressiveness for application developers with optimization potential for the storage engine.\n\n### Mental Model: The Time-Series Library Catalog\n\nImagine a vast library dedicated solely to scientific journals. Each journal (**measurement**) records a specific type of observation, like \"Temperature\" or \"Network Latency.\" Every individual issue of a journal corresponds to a unique **time series**, distinguished by a set of metadata labels on its spine, such as `location=server_room` and `sensor_id=A12` (**tags**). Inside each issue, the actual data is organized as a chronological list of entries (**data points**), where each entry contains a publication date (**timestamp**) and the recorded experimental value (**field value**).\n\nThe library's catalog (**series index**) doesn't store the actual data points; instead, it maps each unique combination of journal title and metadata labels (e.g., `Temperature{location=server_room, sensor_id=A12}`) to a specific shelf location where all its issues (data blocks) are stored. This separation—metadata for identification and ordering, and columnar storage for the actual time-value pairs—is the core of an efficient time-series data model. It allows us to quickly find all series related to a `location`, and then, within a found series, efficiently scan through time to retrieve values.\n\n### Core Concepts: Measurement, Tags, Field\n\nTempoDB adopts a tag-set data model inspired by InfluxDB and Prometheus. This model is exceptionally well-suited for operational monitoring and IoT scenarios where data is multi-dimensional.\n\n*   **Measurement:** A measurement is a container for related data, akin to a table name in SQL. It represents the *what* is being measured (e.g., `cpu_usage`, `http_requests_total`, `temperature`). All data points within a measurement share the same semantic meaning but may come from different sources identified by tags. The measurement name is a string.\n\n*   **Tags:** Tags are key-value pairs of metadata used to identify and filter time series. They represent the *dimensions* of the data (e.g., `host=\"web-01\"`, `region=\"us-east-1\"`, `http_method=\"GET\"`). Tags are **indexed**, enabling fast queries like \"show CPU usage for all hosts in `region=us-east-1`.\" A set of tags uniquely defines a **series**. Tags are intended to have low cardinality relative to the dataset; a tag key like `host` might have hundreds of values, not millions. High-cardinality tags (like a unique `request_id`) are discouraged as they explode the series index size and degrade performance.\n\n*   **Field:** A field is the actual measured value. It is the *quantity* being recorded (e.g., `value=62.5` for a temperature reading). Fields are not indexed; they are stored in columnar format with their corresponding timestamps. While a data point can theoretically have multiple fields (e.g., `temperature=62.5, humidity=85`), **TempoDB's initial implementation will support a single `float64` field per data point for simplicity**. This aligns with common metrics use cases and simplifies the storage engine and compression design.\n\nThe combination of a **Measurement** and a complete set of **Tags** forms a **Series Key**, which is the unique identifier for a single time series stream. All data points with the same series key are stored together in time-sorted order.\n\n| Concept | Analogy | Purpose | Indexed? | Example |\n| :--- | :--- | :--- | :--- | :--- |\n| **Measurement** | Journal Title / Table Name | Groups related metrics | No | `cpu_usage` |\n| **Tag** | Journal Metadata / Dimension | Identifies the source/context of a series | **Yes** | `host=\"web-01\"`, `region=\"us-east\"` |\n| **Field** | Journal Article Content / Metric Value | The actual numerical observation | No | `value=62.5` |\n| **Series Key** | Unique Journal ID (Title + Metadata) | Uniquely identifies a single data stream | Via its tags | `cpu_usage{host=\"web-01\",region=\"us-east\"}` |\n| **Data Point** | A single journal entry | A timestamped observation | - | `(timestamp=2023-10-01:12:00:00, value=62.5)` |\n\n> **Design Insight:** The decision to index tags but not fields is a critical performance trade-off. Indexing enables fast series selection but adds write overhead and storage cost. Numerical field values, which are often high-cardinality and compress well in columnar form, are poor candidates for indexing. This tag-set model allows applications to model rich metadata while giving the database a clear path for optimization.\n\n### Type Definitions and Relationships\n\nThe core concepts are realized in Go as a set of concrete types. These types flow through the entire system, from the write API to the storage engine and query engine.\n\n**Primary Data Types**\n\nThese types represent the core data entities.\n\n| Type Name | Fields (Name & Type) | Description |\n| :--- | :--- | :--- |\n| **`DataPoint`** | `Timestamp time.Time`<br>`Value float64` | The atomic unit of storage. Represents a single observation at a specific moment in time. `Timestamp` is a nanosecond-precision UTC time. `Value` is the observed float64 measurement. |\n| **`SeriesKey`** | `Measurement string`<br>`Tags map[string]string` | The unique identifier for a time series. The `Tags` map is **sorted by key** when serialized (e.g., to form a string key for a map) to ensure a consistent, canonical representation. |\n| **`Series`** | `Key SeriesKey`<br>`Points []DataPoint` | An in-memory collection of data points belonging to a single series. Used primarily within the memtable and during query result materialization. The `Points` slice is always sorted by `Timestamp` ascending. |\n| **`TimeRange`** | `Start time.Time`<br>`End time.Time` | Represents an inclusive-exclusive time interval `[Start, End)`. Used throughout the system to bound queries, define block ranges, and specify retention periods. |\n\n**Query and Configuration Types**\n\nThese types define requests and system configuration.\n\n| Type Name | Fields (Name & Type) | Description |\n| :--- | :--- | :--- |\n| **`Query`** | `Measurement string`<br>`Tags map[string]string`<br>`TimeRange TimeRange`<br>`Aggregate *AggregateFunction`<br>`GroupByWindow time.Duration`<br>`Fields []string` | A request to retrieve data. `Tags` contains predicates (e.g., `{\"host\": \"web-01\"}`). `Aggregate` is a pointer, as it's optional (nil for raw data). `GroupByWindow` is optional for windowed aggregations. `Fields` is reserved for future multi-field support. |\n| **`AggregateFunction`** | (Enum type) | Specifies the aggregation operation. One of: `AggregateSum`, `AggregateAvg`, `AggregateMin`, `AggregateMax`, `AggregateCount`. |\n| **`Config`** | `DataDir string`<br>`MaxMemtableSize int64`<br>`WalSegmentSize int64`<br>`BlockCacheSize int64`<br>`FlushConcurrency int` | Root configuration for the `StorageEngine`. Controls thresholds for flushing, WAL file size, cache memory, and background job concurrency. |\n\n**Storage Engine Internal Types**\n\nThese types manage the internal state and components of the storage engine.\n\n| Type Name | Fields (Name & Type) | Description |\n| :--- | :--- | :--- |\n| **`StorageEngine`** | `config Config`<br>`wal *wal.WAL`<br>`memtables *MemtableManager`<br>`tsmReader *tsm.Reader`<br>`tsmWriter *tsm.Writer`<br>`mu sync.RWMutex`<br>`seriesIndex map[string]*SeriesMetadata`<br>`tsmFiles []*TSMFileRef`<br>`blockCache *BlockCache`<br>`flushCh chan *memtable.Memtable`<br>`compactCh chan compactionPlan`<br>`stopCh chan struct{}` | The central coordinator. It owns all subcomponents: the WAL for durability, the memtable manager for writes, the TSM reader/writer for disk I/O, the series index for lookups, the list of active TSM files, and a block cache for hot data. Channels coordinate background flushing and compaction. |\n| **`SeriesMetadata`** | (Conceptual, not a concrete type in naming conventions) | In-memory metadata for a series. Would typically contain fields like `ID uint64` (for integer-based references), `LastTimestamp time.Time`, and references to the TSM files containing its data. |\n\n**WAL Segment Types**\n\nThese types manage Write-Ahead Log segments.\n\n| Type Name | Fields (Name & Type) | Description |\n| :--- | :--- | :--- |\n| **`Segment`** | `path string`<br>`file *os.File`<br>`mu sync.RWMutex`<br>`size int64`<br>`maxSize int64`<br>`closed bool`<br>`firstID uint64`<br>`lastID uint64` | Represents an active WAL segment file. `firstID` and `lastID` track the range of entry IDs stored in this segment, aiding in cleanup and recovery. `mu` protects concurrent writes and closure. |\n| **`SegmentConfig`** | `MaxSizeBytes int64`<br>`SyncInterval time.Duration`<br>`SyncOnWrite bool` | Configuration for WAL segment behavior, controlling rotation size and durability guarantees. |\n\n**Relationship Diagram**\n\nThe diagram below illustrates how these primary types relate to each other:\n\n![Data Model: Core Types and Relationships](./diagrams/data-model-relationships.svg)\n\n**Key Relationships Explained:**\n\n1.  A **`StorageEngine`** contains many **`TSMFileRef`** objects (pointing to on-disk TSM files) and an in-memory **`seriesIndex`** mapping from string representations of `SeriesKey` to `SeriesMetadata`.\n2.  Each **`TSMFile`** on disk contains data for potentially many series. For each series, it stores one or more **`TSMBlock`**s (not a user-facing type), which are columnar chunks of compressed `DataPoint` values for a specific `TimeRange`.\n3.  The **`SeriesKey`** is the glue. It is used to:\n    *   Look up series in the `seriesIndex`.\n    *   Group incoming `DataPoint` writes in the `Memtable`.\n    *   Locate the correct `TSMBlock`s during a query that filters by tags.\n4.  A **`Query`** specifies constraints on `Measurement` and `Tags` (to select series) and a `TimeRange` (to select points within those series). The result is a collection of `DataPoint` values, potentially aggregated.\n\n**Architecture Decision Record: Tag-Set Model vs. Wide-Table Model**\n\n> **Decision: Adopt a Tag-Set Data Model**\n> *   **Context:** We need a data model that is intuitive for time-series data (e.g., metrics, IoT) and allows for efficient storage and querying based on multi-dimensional metadata.\n> *   **Options Considered:**\n>     1.  **Tag-Set (InfluxDB-like):** A measurement with a set of key-value tag pairs and one or more field values.\n>     2.  **Wide-Table (Classic SQL):** A fixed-schema table where metadata columns become separate `VARCHAR` columns (e.g., `host`, `region`).\n>     3.  **Simple Key-Value:** A single string key representing the entire series (e.g., `cpu_usage.host.web-01.region.us-east`).\n> *   **Decision:** Option 1, the Tag-Set model.\n> *   **Rationale:**\n>     *   **Dynamic Schema:** Tags can be added or removed per data point without altering a table schema, which is ideal for evolving instrumentation.\n>     *   **Optimized Indexing:** Tags are stored separately from fields and can be indexed using an inverted index, enabling fast `WHERE tag = 'value'` queries without scanning field data.\n>     *   **Cardinality Management:** The model clearly distinguishes indexed dimensions (tags) from non-indexed measurements (fields), guiding users to avoid high-cardinality indexes.\n>     *   **Industry Alignment:** This model is used by InfluxDB, Prometheus, and VictoriaMetrics, ensuring familiarity for users and compatibility with existing ecosystems (e.g., Prometheus remote write).\n> *   **Consequences:**\n>     *   **Positive:** Enables flexible, schema-less metadata. Query language naturally supports filtering on multiple tags. Compression benefits from storing tags as dictionary-encoded integers.\n>     *   **Negative:** Requires maintaining a series index mapping tag sets to internal IDs. Queries with many distinct tag value combinations (high cardinality) can stress this index.\n\n| Option | Pros | Cons | Chosen? |\n| :--- | :--- | :--- | :--- |\n| **Tag-Set Model** | Dynamic schema, optimized indexing, clear cardinality guidance, industry standard. | Requires series index management, can be misused with high-cardinality tags. | **Yes** |\n| **Wide-Table SQL** | Familiar to SQL users, strong typing. | Schema migrations needed for new tags, less efficient for sparse metadata, harder to compress. | No |\n| **Simple Key-Value** | Very simple to implement. | No structured querying on metadata, inefficient for multi-dimensional filtering. | No |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Using High-Cardinality Tags**\n*   **Mistake:** Using a tag with a vast number of unique values, such as `request_id` or `user_id`, in a high-volume system. This creates a unique series for every value, exploding the size of the series index and the number of series on disk.\n*   **Why it's Wrong:** It destroys write and query performance. The series index becomes too large to fit in memory, and every write must touch a different series, preventing efficient batching and compression. A single query might need to open thousands of series files.\n*   **How to Avoid:** High-cardinality identifiers should be stored as **field values**, not tags. Use tags for *groupable* dimensions like `host`, `region`, `service`. If you must query by a high-cardinality ID, consider a separate indexing system or pre-filter your data.\n\n⚠️ **Pitfall: Not Sorting Tags for the Series Key**\n*   **Mistake:** When constructing the string key for the `seriesIndex` map (e.g., by concatenating measurement and tags), not first sorting the tag keys (and optionally values).\n*   **Why it's Wrong:** The series `{host=a, region=b}` is semantically identical to `{region=b, host=a}`, but without sorting, they produce different string keys (`cpu_usage,host=a,region=b` vs `cpu_usage,region=b,host=a`). This results in the same logical series being stored twice, corrupting data and queries.\n*   **How to Avoid:** Always sort tag keys (and be consistent about values) before serializing a `SeriesKey` for use as a map key or for storage. Implement a canonical form.\n\n⚠️ **Pitfall: Storing Non-UTC Timestamps**\n*   **Mistake:** Accepting timestamps with local timezones or storing them without converting to a canonical format (like UTC nanoseconds since epoch).\n*   **Why it's Wrong:** Leads to confusion during queries, incorrect results when comparing times, and sorting issues. Daylight saving time transitions can cause duplicate or missing hours.\n*   **How to Avoid:** In the `DataPoint` type, use `time.Time` (which internally stores UTC). In APIs, require timestamps to be specified in UTC or as a Unix epoch (nanoseconds), and convert immediately to UTC on ingestion.\n\n### Implementation Guidance\n\nThis section provides concrete Go code to implement the foundational data model types and their associated helper logic.\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n| :--- | :--- | :--- |\n| **Timestamp Storage** | `int64` nanoseconds (Unix epoch) | `time.Time` struct (wraps `int64`, more expressive) |\n| **Series Key Hashing** | Sorted string concatenation + `string` map key | Sorted string concatenation + `xxhash` for `uint64` map key |\n| **Tag Map Sorting** | `sort.Strings` on keys | Use a `Pair` slice and custom sort, or a `SortedMap` wrapper |\n\n**B. Recommended File/Module Structure**\n\nPlace the core data model types in an `internal/models` package to separate them from the business logic of components like storage or querying.\n\n```\ntempo/\n├── cmd/\n│   └── server/\n│       └── main.go\n├── internal/\n│   ├── models/           # ← Core data model types live here\n│   │   ├── point.go\n│   │   ├── series.go\n│   │   ├── query.go\n│   │   └── time_range.go\n│   ├── storage/\n│   ├── query/\n│   └── wal/\n└── go.mod\n```\n\n**C. Infrastructure Starter Code**\n\nHere is a complete, reusable implementation for the `TimeRange` type and a helper function to canonicalize a `SeriesKey`. This is boilerplate that can be copied directly.\n\n```go\n// internal/models/time_range.go\npackage models\n\nimport (\n\t\"time\"\n)\n\n// TimeRange represents an inclusive-exclusive time interval [Start, End).\ntype TimeRange struct {\n\tStart time.Time\n\tEnd   time.Time\n}\n\n// Contains checks if a given timestamp t is within the range [Start, End).\nfunc (tr TimeRange) Contains(t time.Time) bool {\n\treturn !t.Before(tr.Start) && t.Before(tr.End)\n}\n\n// Overlaps checks if this TimeRange overlaps with another.\nfunc (tr TimeRange) Overlaps(other TimeRange) bool {\n\treturn tr.Start.Before(other.End) && other.Start.Before(tr.End)\n}\n\n// Duration returns the length of the time range as a time.Duration.\nfunc (tr TimeRange) Duration() time.Duration {\n\treturn tr.End.Sub(tr.Start)\n}\n```\n\n```go\n// internal/models/series.go\npackage models\n\nimport (\n\t\"sort\"\n\t\"strings\"\n)\n\n// SeriesKey is a unique identifier for a time series.\ntype SeriesKey struct {\n\tMeasurement string\n\tTags        map[string]string\n}\n\n// String returns a canonical string representation of the SeriesKey.\n// Tags are sorted by key to ensure a consistent output.\nfunc (sk SeriesKey) String() string {\n\tvar b strings.Builder\n\tb.WriteString(sk.Measurement)\n\n\tif len(sk.Tags) > 0 {\n\t\tkeys := make([]string, 0, len(sk.Tags))\n\t\tfor k := range sk.Tags {\n\t\t\tkeys = append(keys, k)\n\t\t}\n\t\tsort.Strings(keys)\n\n\t\tfor _, k := range keys {\n\t\t\tb.WriteString(\",\")\n\t\t\tb.WriteString(k)\n\t\t\tb.WriteString(\"=\")\n\t\t\tb.WriteString(sk.Tags[k])\n\t\t}\n\t}\n\treturn b.String()\n}\n\n// NewSeriesKey creates a SeriesKey and ensures its tags map is initialized.\nfunc NewSeriesKey(measurement string, tags map[string]string) SeriesKey {\n\tif tags == nil {\n\t\ttags = make(map[string]string)\n\t}\n\treturn SeriesKey{Measurement: measurement, Tags: tags}\n}\n```\n\n**D. Core Logic Skeleton Code**\n\nBelow is the skeleton for the primary data types and a key function for parsing a data point from the line protocol (a common ingestion format). The learner should fill in the implementation.\n\n```go\n// internal/models/point.go\npackage models\n\nimport (\n\t\"time\"\n)\n\n// DataPoint represents a single observation in a time series.\ntype DataPoint struct {\n\tTimestamp time.Time\n\tValue     float64\n}\n\n// Series represents an in-memory collection of points for a single series.\ntype Series struct {\n\tKey    SeriesKey\n\tPoints []DataPoint // Invariant: Points are sorted by Timestamp ascending.\n}\n\n// NewSeries creates a new Series with the given key.\nfunc NewSeries(key SeriesKey) *Series {\n\treturn &Series{\n\t\tKey:    key,\n\t\tPoints: make([]DataPoint, 0),\n\t}\n}\n\n// InsertPoint inserts a DataPoint into the Series while maintaining sorted order.\n// It assumes points are generally inserted in chronological order (append).\n// For out-of-order inserts, a more complex merge is needed.\nfunc (s *Series) InsertPoint(p DataPoint) {\n\t// TODO 1: Handle the common case: if the new point's timestamp is after the last point's timestamp, simply append.\n\t// TODO 2: Otherwise, find the correct insertion index to maintain sorted order.\n\t// TODO 3: Insert the point at that index using slice operations (e.g., append and copy).\n\t// Hint: Use `len(s.Points) == 0` as a special case.\n\t// Hint: For out-of-order inserts, consider binary search.\n}\n```\n\n```go\n// internal/models/query.go\npackage models\n\nimport (\n\t\"time\"\n)\n\n// AggregateFunction represents the type of aggregation to perform.\ntype AggregateFunction int\n\nconst (\n\tAggregateSum AggregateFunction = iota\n\tAggregateAvg\n\tAggregateMin\n\tAggregateMax\n\tAggregateCount\n)\n\n// String returns a string representation of the aggregate function.\nfunc (af AggregateFunction) String() string {\n\tswitch af {\n\tcase AggregateSum:\n\t\treturn \"sum\"\n\tcase AggregateAvg:\n\t\treturn \"avg\"\n\tcase AggregateMin:\n\t\treturn \"min\"\n\tcase AggregateMax:\n\t\treturn \"max\"\n\tcase AggregateCount:\n\t\treturn \"count\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Query represents a request for data.\ntype Query struct {\n\tMeasurement   string\n\tTags          map[string]string // Tag filters (equality only in v1)\n\tTimeRange     TimeRange\n\tAggregate     *AggregateFunction // nil for raw data\n\tGroupByWindow time.Duration      // 0 for no grouping\n\tFields        []string           // Reserved for future multi-field support\n}\n```\n\n**E. Language-Specific Hints**\n\n*   **Time Representation:** Use `time.Time` for clarity in user-facing structs like `DataPoint`. Internally, for compression and storage, you will likely convert it to an `int64` (nanoseconds since Unix epoch) using `t.UnixNano()`.\n*   **Map Sorting:** Go's `map` iteration order is non-deterministic. Always use `sort.Strings` on a slice of keys when you need a canonical representation (e.g., for the `SeriesKey.String()` method or when writing to disk).\n*   **Point Sorting:** The `Series.Points` slice must remain sorted. The `InsertPoint` method should efficiently handle both in-order appends (the common case) and out-of-order inserts (which may require a binary search and slice insertion).\n*   **Constants as Iota:** Define the `AggregateFunction` as an `iota` enumeration. This is type-safe and efficient.\n\n**F. Milestone Checkpoint**\n\nTo verify your data model implementation, write a simple test.\n\n1.  **Command:** `go test ./internal/models/... -v`\n2.  **Expected Behavior:** Tests should pass, demonstrating:\n    *   `SeriesKey.String()` produces the same output for `{host=a, region=b}` and `{region=b, host=a}`.\n    *   `TimeRange.Contains()` correctly identifies points inside and outside the range.\n    *   `Series.InsertPoint()` maintains chronological order for both in-order and out-of-order inserts.\n3.  **Sample Test Output:**\n    ```\n    === RUN   TestSeriesKeyCanonical\n    --- PASS: TestSeriesKeyCanonical (0.00s)\n    === RUN   TestTimeRangeContains\n    --- PASS: TestTimeRangeContains (0.00s)\n    === RUN   TestSeriesInsertPointInOrder\n    --- PASS: TestSeriesInsertPointInOrder (0.00s)\n    === RUN   TestSeriesInsertPointOutOfOrder\n    --- PASS: TestSeriesInsertPointOutOfOrder (0.00s)\n    PASS\n    ```\n\n**G. Debugging Tips**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n| :--- | :--- | :--- | :--- |\n| **Queries return duplicate data points for the same series.** | Tags are not being canonicalized before being used as a map key, creating two entries for the same logical series. | Log the string key used for the `seriesIndex` map for two writes with the same tags in different order. They will differ. | Ensure `SeriesKey.String()` sorts tag keys (and values if needed) before concatenation. |\n| **Data points appear in the wrong time order in query results.** | The `Series.Points` slice is not sorted, or `InsertPoint` logic is flawed. | Write a unit test that inserts points with shuffled timestamps and check the final slice order. | Implement and verify the `InsertPoint` logic, handling both append and binary search insertion paths. |\n| **\"Out of memory\" errors when loading the series index.** | High-cardinality tags are being used, creating millions of unique series keys. | Instrument the `StorageEngine` to log the count of unique series keys. If it grows linearly with write volume, investigate the tag structure. | Guide users to move high-cardinality identifiers to field values, not tags. Consider implementing soft limits on series creation. |\n\n\n## Storage Engine Design\n\n> **Milestone(s):** Milestone 1: Storage Engine\n\nThe storage engine is the heart of TempoDB, responsible for efficiently persisting time-series data to disk while enabling fast range queries. Unlike traditional row-oriented databases that store entire records together, our Time-Structured Merge (TSM) tree engine employs a columnar layout that groups timestamps and values separately, optimizing for the sequential and append-heavy nature of time-series data.\n\n### Mental Model: The Time-Indexed Filing Cabinet\n\nImagine a massive filing cabinet organizing documents by time period. Each drawer represents a year, each folder within represents a month, and each document within a folder contains data for a specific day. Within each document, instead of narrative paragraphs, you have two columns: one listing every timestamp (when something happened) and another listing the corresponding measurement values (what happened). This is our TSM engine.\n\nThe filing clerk (storage engine) follows strict rules:\n1. **Documents are immutable**: Once filed, a document is never modified. Updates create new documents.\n2. **Documents are self-describing**: Each document has a cover page listing exactly what time range it covers and where to find specific series inside.\n3. **Documents are consolidated**: Periodically, the clerk merges several small documents into larger, better-organized ones (compaction).\n4. **Old documents are archived**: After a certain time, documents move to deeper storage or are destroyed (retention).\n\nThis mental model explains why TSM files are append-only, why they contain columnar data blocks, and how queries can quickly locate relevant data by checking the \"cover page\" (index) rather than scanning every document.\n\n### TSM File Format and Block Layout\n\nA TSM (Time-Structured Merge) file is the immutable on-disk representation of time-series data for a specific time window. Each file contains data for multiple series, with each series' data organized into compressed blocks of timestamps and values.\n\n#### File Structure\n\nThe TSM file format follows this layout:\n\n| Component | Size | Description |\n|-----------|------|-------------|\n| **Header** | 4+8 bytes | Magic number (`0x16D1D1A5`) + file version (uint64) |\n| **Series Block 1** | Variable | Series key + one or more data blocks for this series |\n| **Series Block 2** | Variable | Series key + data blocks for another series |\n| ... | ... | Additional series blocks |\n| **Index** | Variable | Map from series keys to block metadata (offsets, time ranges) |\n| **Footer** | 8 bytes | Offset to the start of the index (uint64) |\n\n![TSM File Internal Layout](./diagrams/tsm-file-layout.svg)\n\n#### Data Block Structure\n\nWithin each series block, data is organized into one or more **data blocks**. Each data block contains:\n\n| Component | Description |\n|-----------|-------------|\n| **Block Header** | 16 bytes: Min timestamp (uint64), Max timestamp (uint64) |\n| **Compressed Timestamps** | Delta-of-delta encoded and compressed byte array |\n| **Compressed Values** | Gorilla XOR compressed float64 values (or other encoding for future types) |\n| **Checksum** | 4 bytes: CRC32 of the entire block (header + compressed data) |\n\n#### Index Structure\n\nThe index maps each series key to the location of its data blocks within the file:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| **Series Key** | String | Canonical string representation (measurement + tags) |\n| **Block Entry Count** | uint32 | Number of data blocks for this series in the file |\n| **Block Entries** | Array of: | |\n| - Min Time | uint64 | Minimum timestamp in the block |\n| - Max Time | uint64 | Maximum timestamp in the block |\n| - Offset | uint64 | Byte offset from file start to block header |\n| - Size | uint32 | Size of the entire block (header + data + checksum) |\n\n#### Key Data Structures\n\nThe following types implement the TSM file format:\n\n| Type Name | Fields | Description |\n|-----------|--------|-------------|\n| `TSMFile` | `path string`<br>`file *os.File`<br>`mmap []byte`<br>`index TSMIndex`<br>`mu sync.RWMutex` | Represents an open TSM file with memory-mapped access |\n| `TSMIndex` | `entries map[string][]IndexEntry` | In-memory index mapping series keys to block metadata |\n| `IndexEntry` | `MinTime uint64`<br>`MaxTime uint64`<br>`Offset uint64`<br>`Size uint32` | Metadata for a single data block |\n| `BlockHeader` | `MinTime uint64`<br>`MaxTime uint64` | Header for a data block |\n| `CompressedBlock` | `Timestamps []byte`<br>`Values []byte`<br>`Checksum uint32` | Compressed timestamps and values with integrity check |\n\n#### File Creation Process\n\nWhen creating a TSM file from a memtable flush:\n\n1. **Sort and Group**: Group `DataPoint` values by `SeriesKey`, sort each series by timestamp\n2. **Create Blocks**: For each series, partition points into blocks based on:\n   - Maximum block size (e.g., 1024 points)\n   - Time range limit (e.g., 1 hour of data per block)\n3. **Compress Each Block**:\n   - Apply delta-of-delta encoding to timestamps\n   - Apply Gorilla XOR compression to float64 values\n   - Calculate CRC32 checksum\n4. **Write Series Blocks**: For each series:\n   - Write series key (length-prefixed string)\n   - For each data block: write `BlockHeader`, compressed timestamps, compressed values, checksum\n5. **Build Index**: Record offset and time range for each block\n6. **Write Index and Footer**: Append index to end of file, then write footer with index offset\n7. **Finalize**: Sync to disk, close file, update file list in storage engine\n\n### ADR: Choosing Compression Algorithms\n\n> **Decision: Delta-of-Delta for Timestamps, Gorilla XOR for Values**\n> - **Context**: Time-series data exhibits strong temporal patterns: timestamps arrive in monotonic (usually regular) intervals, and consecutive float values often change slowly. We need compression that exploits these patterns without sacrificing query performance.\n> - **Options Considered**:\n>   1. **No compression**: Store raw int64 timestamps and float64 values\n>   2. **Simple delta encoding**: Store difference from previous value\n>   3. **Delta-of-delta encoding**: Store difference between consecutive deltas\n>   4. **Snappy/GZIP block compression**: Compress entire blocks with general-purpose algorithms\n>   5. **Gorilla XOR for floats**: XOR current value with previous, encode leading/trailing zeros\n> - **Decision**: Use **delta-of-delta encoding for timestamps** and **Gorilla XOR compression for float64 values**.\n> - **Rationale**:\n>   - **Timestamps**: When data arrives at regular intervals (e.g., every second), delta-of-delta yields zeros or small constants that compress to few bits. Even with irregular intervals, it typically produces smaller deltas than simple delta encoding.\n>   - **Values**: Gorilla XOR exploits the property that consecutive float values in metrics often change little (CPU usage, temperature). XOR-ing with previous value yields many leading/trailing zeros in the mantissa, which can be efficiently encoded.\n>   - **Query performance**: Both algorithms allow random access within a block without decompressing the entire block—critical for range queries that may start in the middle of a block.\n>   - **Proven effectiveness**: Facebook's Gorilla paper demonstrated 10x compression for timestamps and 2x for values in production metrics.\n> - **Consequences**:\n>   - **Positive**: Significant storage reduction (typically 70-90% for timestamps, 50% for values)\n>   - **Positive**: Faster I/O due to less data read from disk\n>   - **Negative**: CPU overhead during compression/decompression\n>   - **Negative**: Compression effectiveness depends on data regularity (irregular timestamps or highly volatile values compress poorly)\n\n| Option | Pros | Cons | Why Not Chosen? |\n|--------|------|------|-----------------|\n| **No compression** | Zero CPU overhead, simplest implementation | Wastes storage (8 bytes per timestamp + 8 bytes per value), slow I/O | Storage efficiency is primary goal |\n| **Simple delta encoding** | Better than raw for regular intervals, simple to implement | Less effective than delta-of-delta for regular intervals | Delta-of-delta is strictly better for regular data |\n| **Delta-of-delta encoding** | Excellent for regular intervals, allows partial decompression | Slightly more complex, poor for completely irregular timestamps | **CHOSEN**: Best trade-off for typical time-series |\n| **Snappy/GZIP block compression** | Good compression for any data, widely available | Requires decompressing entire block for any access, higher CPU | Kills random access performance |\n| **Gorilla XOR for floats** | Excellent for slowly-changing values, allows partial decompression | Only for floats, requires maintaining previous value state | **CHOSEN**: Best for metric-like data |\n\n#### Compression Algorithm Details\n\n**Delta-of-Delta Timestamp Encoding**:\n1. Store first timestamp as raw int64 (Unix nanoseconds)\n2. Store first delta as difference from previous (int64)\n3. For subsequent timestamps: \n   - Calculate delta = current_timestamp - previous_timestamp\n   - Calculate delta-of-delta = delta - previous_delta\n   - Encode delta-of-delta using variable-length integer encoding (VarInt)\n\n**Gorilla XOR Float Compression**:\n1. Store first float64 value as raw bits\n2. For subsequent values:\n   - XOR current value bits with previous value bits\n   - If XOR == 0: store single '0' bit\n   - Else:\n     - Calculate leading zeros = count of leading zero bits in XOR\n     - Calculate trailing zeros = count of trailing zero bits in XOR\n     - Store '1' bit followed by:\n       - 5 bits encoding leading zeros count\n       - 6 bits encoding (64 - leading_zeros - trailing_zeros) = meaningful bits length\n       - The meaningful bits themselves (excluding leading/trailing zeros)\n\n#### Block Size Considerations\n\nChoosing the right block size involves trade-offs:\n\n| Block Size | Pros | Cons | Recommendation |\n|------------|------|------|----------------|\n| **Small (≤ 100 points)** | Fast decompression, fine-grained skipping | Poor compression ratio, more index entries | Avoid: overhead dominates |\n| **Medium (1,024 points)** | Good compression, reasonable decompression cost | May need to decompress unneeded data | **Default choice** |\n| **Large (10,000 points)** | Excellent compression, fewer index entries | Expensive to decompress for small queries | Use for cold/historical data |\n\nWe choose **1,024 points per block** as the default, aligning with filesystem page sizes (typically 4KB) and providing good compression while keeping decompression costs manageable.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Not aligning block sizes with filesystem page sizes**\n- **Description**: Creating blocks that straddle page boundaries (e.g., 1500-byte blocks when pages are 4096 bytes) causes read amplification.\n- **Why it's wrong**: The filesystem may read two pages from disk to access one block, doubling I/O.\n- **How to fix**: Round block sizes up to multiples of 4096 bytes, or ensure block headers start on page boundaries.\n\n⚠️ **Pitfall: Forgetting to handle clock skew in timestamp encoding**\n- **Description**: Assuming timestamps always increase, but in distributed systems, clocks may drift or jump.\n- **Why it's wrong**: Delta-of-delta produces large values for negative deltas, hurting compression. Decoder may misinterpret.\n- **How to fix**: Include a flag in block header for \"unordered timestamps\" that disables compression for that block, or reset compression state when timestamp goes backwards.\n\n⚠️ **Pitfall: Over-compressing hot data that needs frequent access**\n- **Description**: Applying aggressive compression to frequently queried recent data.\n- **Why it's wrong**: Compression/decompression CPU cost outweighs I/O savings for hot data.\n- **How to fix**: Implement tiered compression: no compression for L0 (hot), delta encoding for L1 (warm), full Gorilla for L2+ (cold).\n\n⚠️ **Pitfall: Not leaving buffer space for block metadata**\n- **Description**: Allocating exact size for compressed data without room for headers and checksums.\n- **Why it's wrong**: Need to copy and reallocate when adding metadata, wasting CPU.\n- **How to fix**: Pre-allocate buffer with extra capacity (e.g., data size + 256 bytes) for headers and checksums.\n\n⚠️ **Pitfall: Choosing fixed compression without profiling actual data patterns**\n- **Description**: Always using Gorilla XOR even for highly volatile data (e.g., cryptographic hashes as floats).\n- **Why it's wrong**: XOR compression expands data rather than compressing it.\n- **How to fix**: Implement compression selector that samples data, chooses best algorithm, stores algorithm ID in block header.\n\n### Implementation Guidance (Milestone 1)\n\nThis section provides concrete implementation guidance for the TSM storage engine in Go.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **File I/O** | `os.File` with manual buffering | **Memory-mapped files (`mmap`)** for zero-copy reads |\n| **Compression** | Implement delta-of-delta and Gorilla from scratch | Use existing libraries but understand algorithms |\n| **Checksums** | **CRC32** using `hash/crc32` | More robust hash but CRC32 is sufficient |\n| **Concurrent Access** | `sync.RWMutex` per file | Lock-free reads with atomic pointers |\n| **Block Cache** | Simple `map` with LRU eviction | **`sync.Map` or specialized cache** with size limits |\n\n#### B. Recommended File/Module Structure\n\n```\ntempo/\n├── cmd/\n│   └── server/\n│       └── main.go                 # Entry point\n├── internal/\n│   ├── storage/\n│   │   ├── engine.go              # StorageEngine implementation\n│   │   ├── tsm/\n│   │   │   ├── writer.go          # TSM file creation\n│   │   │   ├── reader.go          # TSM file reading\n│   │   │   ├── compression.go     # Delta-of-delta, Gorilla\n│   │   │   ├── file.go            # TSMFile struct and methods\n│   │   │   └── index.go           # TSMIndex and IndexEntry\n│   │   ├── block_cache.go         # LRU block cache\n│   │   └── memtable.go            # Memtable (for Milestone 2)\n│   ├── models/\n│   │   ├── datapoint.go           # DataPoint struct\n│   │   ├── series.go              # SeriesKey, Series\n│   │   └── query.go               # Query, TimeRange\n│   └── wal/                       # Write-ahead log (Milestone 2)\n│       └── wal.go\n└── pkg/\n    └── mmap/                      # Memory-mapping utilities\n        └── mmap.go\n```\n\n#### C. Infrastructure Starter Code\n\n**Memory-mapped file utility (`pkg/mmap/mmap.go`)**:\n```go\npackage mmap\n\nimport (\n    \"os\"\n    \"syscall\"\n    \"unsafe\"\n)\n\n// Map memory-maps a file for read-only access\nfunc Map(path string) ([]byte, error) {\n    file, err := os.Open(path)\n    if err != nil {\n        return nil, err\n    }\n    defer file.Close()\n    \n    stat, err := file.Stat()\n    if err != nil {\n        return nil, err\n    }\n    size := stat.Size()\n    \n    // Handle empty files\n    if size == 0 {\n        return []byte{}, nil\n    }\n    \n    // Memory map the file\n    data, err := syscall.Mmap(int(file.Fd()), 0, int(size), \n        syscall.PROT_READ, syscall.MAP_SHARED)\n    if err != nil {\n        return nil, err\n    }\n    \n    return data, nil\n}\n\n// Unmap unmaps a memory-mapped file\nfunc Unmap(data []byte) error {\n    if len(data) == 0 {\n        return nil\n    }\n    return syscall.Munmap(data)\n}\n\n// ReadUint64 reads a little-endian uint64 from mapped memory\nfunc ReadUint64(data []byte, offset int) uint64 {\n    // Use unsafe for performance; ensure data is long enough\n    return *(*uint64)(unsafe.Pointer(&data[offset]))\n}\n\n// ReadUint32 reads a little-endian uint32 from mapped memory\nfunc ReadUint32(data []byte, offset int) uint32 {\n    return *(*uint32)(unsafe.Pointer(&data[offset]))\n}\n```\n\n**TSM file constants and header (`internal/storage/tsm/file.go`)**:\n```go\npackage tsm\n\nconst (\n    // MagicNumber identifies a TSM file\n    MagicNumber uint32 = 0x16D1D1A5\n    // Version is the current TSM file format version\n    Version uint64 = 1\n    \n    // BlockHeaderSize is the size of a block header in bytes\n    BlockHeaderSize = 16 // 2 x uint64\n    \n    // IndexEntrySize is the size of a single index entry\n    IndexEntrySize = 8 + 8 + 8 + 4 // min, max, offset, size\n    \n    // DefaultMaxPointsPerBlock is the default number of points per block\n    DefaultMaxPointsPerBlock = 1024\n)\n\n// TSMHeader represents the file header\ntype TSMHeader struct {\n    Magic   uint32\n    Version uint64\n}\n\n// WriteHeader writes the TSM header to a writer\nfunc WriteHeader(w io.Writer) error {\n    header := TSMHeader{\n        Magic:   MagicNumber,\n        Version: Version,\n    }\n    return binary.Write(w, binary.LittleEndian, header)\n}\n\n// ReadHeader reads and validates a TSM header\nfunc ReadHeader(r io.Reader) (TSMHeader, error) {\n    var header TSMHeader\n    if err := binary.Read(r, binary.LittleEndian, &header.Magic); err != nil {\n        return header, err\n    }\n    if err := binary.Read(r, binary.LittleEndian, &header.Version); err != nil {\n        return header, err\n    }\n    \n    if header.Magic != MagicNumber {\n        return header, fmt.Errorf(\"invalid magic number: %x\", header.Magic)\n    }\n    if header.Version != Version {\n        return header, fmt.Errorf(\"unsupported version: %d\", header.Version)\n    }\n    \n    return header, nil\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**TSM Writer (`internal/storage/tsm/writer.go`)**:\n```go\npackage tsm\n\nimport (\n    \"bytes\"\n    \"encoding/binary\"\n    \"hash/crc32\"\n    \"io\"\n    \"os\"\n    \n    \"tempo/internal/models\"\n)\n\n// TSMWriter creates TSM files from series data\ntype TSMWriter struct {\n    file      *os.File\n    buf       *bytes.Buffer\n    index     map[string][]IndexEntry\n    blockSize int\n}\n\n// NewTSMWriter creates a new TSM writer\nfunc NewTSMWriter(path string, blockSize int) (*TSMWriter, error) {\n    file, err := os.Create(path)\n    if err != nil {\n        return nil, err\n    }\n    \n    writer := &TSMWriter{\n        file:      file,\n        buf:       bytes.NewBuffer(make([]byte, 0, 64*1024)),\n        index:     make(map[string][]IndexEntry),\n        blockSize: blockSize,\n    }\n    \n    // TODO 1: Write TSM file header (magic + version)\n    // Use WriteHeader function from file.go\n    \n    return writer, nil\n}\n\n// WriteSeries writes a series to the TSM file\nfunc (w *TSMWriter) WriteSeries(key string, points []models.DataPoint) error {\n    // TODO 2: Sort points by timestamp (ensure ascending order)\n    \n    // TODO 3: Partition points into blocks of max blockSize points\n    // For each partition:\n    //   - Create a block with timestamps and values separated\n    //   - Apply delta-of-delta compression to timestamps\n    //   - Apply Gorilla XOR compression to values\n    //   - Calculate CRC32 checksum of the compressed block\n    //   - Write block header (min/max timestamps)\n    //   - Write compressed timestamps\n    //   - Write compressed values  \n    //   - Write checksum\n    //   - Record index entry with min/max time, offset, size\n    \n    // TODO 4: Write series key (length-prefixed string)\n    \n    // TODO 5: Write all blocks for this series\n    \n    return nil\n}\n\n// Finish writes the index and footer, then closes the file\nfunc (w *TSMWriter) Finish() error {\n    // TODO 6: Write index section:\n    //   - For each series in w.index:\n    //     * Write series key\n    //     * Write count of blocks\n    //     * For each block: write min/max time, offset, size\n    \n    // TODO 7: Write footer: offset to start of index (uint64)\n    \n    // TODO 8: Sync file to disk\n    \n    return w.file.Close()\n}\n\n// compressBlock compresses timestamps and values for a block\nfunc compressBlock(timestamps []uint64, values []float64) ([]byte, []byte, error) {\n    // TODO 9: Implement delta-of-delta compression for timestamps\n    //   - First timestamp stored as-is (uint64)\n    //   - First delta = timestamp[1] - timestamp[0]\n    //   - For i > 1: delta = timestamp[i] - timestamp[i-1]\n    //                deltaOfDelta = delta - previousDelta\n    //                Encode deltaOfDelta as VarInt\n    \n    // TODO 10: Implement Gorilla XOR compression for float64 values\n    //   - First value stored as raw bits (uint64)\n    //   - For each subsequent value:\n    //     * XOR with previous value\n    //     * If XOR == 0: store single 0 bit\n    //     * Else: store 1 bit + leading zeros + meaningful bits\n    \n    return nil, nil, nil\n}\n```\n\n**TSM Reader (`internal/storage/tsm/reader.go`)**:\n```go\npackage tsm\n\nimport (\n    \"bytes\"\n    \"encoding/binary\"\n    \"fmt\"\n    \"io\"\n    \n    \"tempo/internal/models\"\n    \"tempo/pkg/mmap\"\n)\n\n// TSMReader reads TSM files using memory-mapped I/O\ntype TSMReader struct {\n    path string\n    data []byte  // Memory-mapped file data\n    index TSMIndex\n}\n\n// OpenTSMReader opens and memory-maps a TSM file\nfunc OpenTSMReader(path string) (*TSMReader, error) {\n    // TODO 1: Memory-map the file using mmap.Map()\n    \n    // TODO 2: Read and validate header from beginning of data\n    \n    // TODO 3: Read footer (last 8 bytes) to get index offset\n    \n    // TODO 4: Parse index section into TSMIndex structure\n    //   - Iterate through index entries\n    //   - Build map[string][]IndexEntry\n    \n    return &TSMReader{\n        path:  path,\n        data:  data,\n        index: index,\n    }, nil\n}\n\n// ReadBlock reads and decompresses a specific block\nfunc (r *TSMReader) ReadBlock(seriesKey string, blockIndex int) ([]models.DataPoint, error) {\n    // TODO 5: Look up series in index, get block metadata\n    \n    // TODO 6: Validate blockIndex is within bounds\n    \n    // TODO 7: Read block from offset in memory-mapped data\n    //   - Read header (min/max timestamps)\n    //   - Read compressed timestamps and values\n    //   - Verify CRC32 checksum\n    \n    // TODO 8: Decompress timestamps (delta-of-delta decoding)\n    \n    // TODO 9: Decompress values (Gorilla XOR decoding)\n    \n    // TODO 10: Reconstruct DataPoint slice from timestamps and values\n    \n    return nil, nil\n}\n\n// TimeRange returns the minimum and maximum timestamps in the file\nfunc (r *TSMReader) TimeRange() (uint64, uint64) {\n    // TODO 11: Iterate through all index entries to find global min/max\n    // Return 0,0 if no data\n    \n    return 0, 0\n}\n\n// Close unmaps the memory and releases resources\nfunc (r *TSMReader) Close() error {\n    // TODO 12: Unmap memory using mmap.Unmap()\n    return nil\n}\n\n// decompressTimestamps decodes delta-of-delta compressed timestamps\nfunc decompressTimestamps(data []byte, count int) ([]uint64, error) {\n    // TODO 13: Read first timestamp (uint64)\n    \n    // TODO 14: Read first delta (VarInt)\n    \n    // TODO 15: For remaining points:\n    //   - Read delta-of-delta (VarInt)\n    //   - Reconstruct delta = previousDelta + deltaOfDelta\n    //   - Reconstruct timestamp = previousTimestamp + delta\n    \n    return nil, nil\n}\n\n// decompressValues decodes Gorilla XOR compressed float64 values\nfunc decompressValues(data []byte, count int) ([]float64, error) {\n    // TODO 16: Implement Gorilla XOR decompression\n    //   - Read first value as raw bits\n    //   - For each subsequent value:\n    //     * Read control bit\n    //     * If 0: value = previous value\n    //     * If 1: read leading zeros, meaningful bits length, and bits\n    //            Reconstruct XOR value, then XOR with previous to get current\n    \n    return nil, nil\n}\n```\n\n#### E. Language-Specific Hints\n\n1. **Memory-mapping**: Use `syscall.Mmap` directly for control, but ensure proper error handling and `Munmap` on close.\n2. **VarInt encoding**: Implement using loops shifting 7 bits at a time. The standard library's `binary.PutUvarint` and `binary.Uvarint` work but may be slower than custom implementation.\n3. **Bit-level operations**: For Gorilla compression, you'll need to read/write individual bits. Use a `bitReader`/`bitWriter` wrapper around byte slices.\n4. **CRC32**: Use `hash/crc32` with IEEE polynomial: `crc32.ChecksumIEEE(data)`.\n5. **Concurrency**: Use `sync.RWMutex` for `TSMReader` if it might be accessed concurrently. Better: make `TSMReader` immutable after creation.\n6. **File I/O**: Always check errors from file operations. Use `defer` for cleanup.\n\n#### F. Milestone Checkpoint\n\n**To verify Milestone 1 implementation**:\n\n1. **Create a test TSM file**:\n```bash\ngo test ./internal/storage/tsm/... -v -run TestTSMWriter\n```\n\nExpected output should show:\n- TSM file created successfully\n- Compression ratio reported (e.g., \"Compressed 1024 points to 5120 bytes\")\n- No errors during write or read\n\n2. **Manual verification**:\n```bash\n# Create a simple test program\ncat > test_tsm.go << 'EOF'\npackage main\n\nimport (\n    \"fmt\"\n    \"tempo/internal/storage/tsm\"\n)\n\nfunc main() {\n    writer, _ := tsm.NewTSMWriter(\"/tmp/test.tsm\", 1024)\n    // Add test data\n    writer.Finish()\n    \n    reader, _ := tsm.OpenTSMReader(\"/tmp/test.tsm\")\n    points, _ := reader.ReadBlock(\"cpu\", 0)\n    fmt.Printf(\"Read %d points\\n\", len(points))\n    reader.Close()\n}\nEOF\ngo run test_tsm.go\n```\n\nExpected: \"Read 1024 points\" with correct timestamps and values.\n\n3. **Check for common issues**:\n- **Issue**: \"invalid magic number\" error when reading\n  - **Fix**: Ensure binary.Write uses `binary.LittleEndian` consistently\n- **Issue**: CRC32 mismatch when reading blocks\n  - **Fix**: Verify you're checksumming the exact bytes written (including header)\n- **Issue**: Gorilla decompression produces NaN values\n  - **Fix**: Check bit manipulation logic; use `math.Float64frombits()` for conversion\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Query returns no data for valid time range** | Block index min/max times incorrect | Add debug logging to show which blocks are being selected | Ensure min/max in index match actual data in block |\n| **Decompression produces wrong timestamps** | VarInt encoding/decoding bug | Write unit test with known sequence: [1000, 1001, 1003, 1006] | Step through decompression with small test case |\n| **Memory usage grows with many TSM files** | Not unmapping closed files | Add finalizer or explicit Close() calls | Ensure every OpenTSMReader has matching Close() |\n| **ReadBlock returns fewer points than expected** | Block size miscalculation during write | Check partition logic in WriteSeries | Ensure points are sorted before partitioning |\n| **Gorilla compression makes data larger** | Highly volatile values | Test with constant values first, then random | Fall back to no compression if XOR doesn't help |\n\n\n## Write Path Design\n\n> **Milestone(s):** Milestone 2: Write Path\n\nThe write path is the critical pipeline through which all incoming data flows from ingestion to durable storage. This design must balance three competing requirements: **low-latency acknowledgment** to clients, **strong durability guarantees**, and **efficient batching** for storage optimization. In TempoDB, we achieve this through a multi-stage pipeline that validates, logs, buffers, and finally persists data points in optimized columnar format.\n\n### Mental Model: The Airport Check-in and Baggage System\n\nImagine an airport's check-in and baggage handling system. Passengers (data points) arrive at the terminal (the API). Each passenger checks in at a counter (validation) and receives a boarding pass (acknowledgment) once their luggage is registered in the system. The luggage itself is placed on a conveyor belt (the Write-Ahead Log) that immediately moves it to a secure holding area (the memtable). Luggage from many flights accumulates in the holding area until a critical mass for a particular destination is reached. Then, all luggage for that flight is efficiently packed into containers (compressed blocks) and loaded onto the plane (TSM file) for the journey to long-term storage (disk).\n\nThis analogy captures the key principles:\n- **Immediate acknowledgment**: Passengers receive their boarding pass quickly, without waiting for the plane to be loaded.\n- **Durability via logging**: The conveyor belt (WAL) ensures luggage isn't lost if the holding area has an issue.\n- **Batched efficiency**: Luggage is packed in bulk, not piece-by-piece, optimizing space and effort.\n- **Temporal organization**: Luggage is sorted by destination (series) and flight time (timestamp).\n\n### Write-Ahead Log and Memtable\n\nThe first two components of the write path work in tandem to provide durability and in-memory buffering.\n\n#### Write-Ahead Log (WAL)\n\nThe **Write-Ahead Log** is an append-only file that records every write operation before it's acknowledged to the client. This ensures that even if the process crashes after acknowledgment but before data reaches persistent storage, the operation can be replayed during recovery. The WAL's sole purpose is durability—it's not optimized for random reads.\n\n**Design Decisions:**\n\n> **Decision: WAL Format and Segmentation**\n> - **Context**: We need a durable log that can handle high write throughput, support efficient recovery scans, and manage disk space without unbounded growth.\n> - **Options Considered**:\n>   1. **Single monolithic log file**: Append all writes to one ever-growing file.\n>   2. **Segmented log with rotation**: Split the log into fixed-size segments; close and create a new segment when current segment reaches size limit.\n> - **Decision**: Use segmented log with rotation.\n> - **Rationale**:\n>   - **Easier space management**: Old segments can be deleted after their data is flushed to TSM files.\n>   - **Parallel recovery**: Multiple segments can be scanned in parallel during recovery.\n>   - **Simplified implementation**: Each segment is a simple append-only file with a predictable maximum size.\n> - **Consequences**:\n>   - Requires managing multiple files and cleaning up obsolete segments.\n>   - Adds complexity for tracking which segments are still needed (not yet flushed).\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Single monolithic file | Simple to implement; no file management overhead | Unbounded growth; difficult to clean up old data; recovery scans entire log | No |\n| Segmented rotation | Bounded segment size; easy cleanup; parallel recovery possible | Must manage multiple files; need to track segment state | **Yes** |\n\nEach WAL segment file contains a sequence of **entries**, where each entry corresponds to one or more data points written in a batch. The segment format is:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Entry Length | uint32 (4 bytes) | Length of the entire entry (including this field and checksum) |\n| Checksum | uint32 (4 bytes) | CRC32 checksum of the entry data (for corruption detection) |\n| Entry ID | uint64 (8 bytes) | Monotonically increasing identifier for ordering and deduplication |\n| Batch Data | []byte (variable) | Serialized batch of data points (using protocol buffers or custom binary format) |\n\nThe `Segment` type manages an individual segment file:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `path` | `string` | Filesystem path to the segment file |\n| `file` | `*os.File` | Open file handle for appending |\n| `mu` | `sync.RWMutex` | Protects concurrent writes and closes |\n| `size` | `int64` | Current size of the segment in bytes |\n| `maxSize` | `int64` | Maximum size before rotation (from `SegmentConfig.MaxSizeBytes`) |\n| `closed` | `bool` | Whether the segment is closed for writing |\n| `firstID` | `uint64` | Entry ID of the first entry in this segment |\n| `lastID` | `uint64` | Entry ID of the last entry written |\n\n**WAL Operations:**\n\n1. **Write Entry**: When a batch of points arrives, serialize them, calculate checksum, append to current segment with monotonic ID.\n2. **Sync Policy**: Configure via `SegmentConfig.SyncInterval` (periodic sync) or `SyncOnWrite` (sync every write). Trade-off between durability and performance.\n3. **Segment Rotation**: When `size >= maxSize`, close current segment, create new one with `firstID = lastID + 1`.\n4. **Recovery Scan**: On startup, scan all non-flushed segments in ID order, replay entries to rebuild memtable.\n\n> **Key Insight**: The WAL must be fsynced to disk before acknowledging writes to the client. Without this, a power loss could lose acknowledged writes, violating durability guarantees. In Go, use `file.Sync()` after writing the entry.\n\n#### Memtable\n\nThe **memtable** (memory table) is an in-memory buffer that holds recently written data points in sorted order, organized by series. It serves two purposes: (1) provides a fast read path for recent data, and (2) batches points for efficient flushing to disk.\n\n**Design Decisions:**\n\n> **Decision: Memtable Data Structure**\n> - **Context**: We need an in-memory structure that supports fast inserts and range scans, organized by series key and timestamp.\n> - **Options Considered**:\n>   1. **Sorted map of maps**: `map[SeriesKey]map[timestamp]value` (unordered timestamps within series).\n>   2. **Sorted map with slices**: `map[SeriesKey][]DataPoint` with points kept sorted.\n>   3. **Skip list per series**: `map[SeriesKey]*skiplist` for ordered points.\n> - **Decision**: Use sorted map with slices (option 2).\n> - **Rationale**:\n>   - **Simplicity**: Slices are native Go types with predictable memory overhead.\n>   - **Good for sequential writes**: Time-series data typically arrives in roughly chronological order; appending to slice is O(1).\n>   - **Efficient flushing**: When flushing, we can iterate series and timestamps in order without additional sorting.\n> - **Consequences**:\n>   - Out-of-order writes require binary search and insertion (O(n) worst-case).\n>   - Must handle slice growth and potential copying.\n\nThe memtable structure in practice:\n\n```go\n// Conceptual structure (not actual code in Layer 1)\ntype Memtable struct {\n    data map[SeriesKey][]DataPoint  // Series → sorted points\n    size int64                      // Approximate memory usage in bytes\n    mu   sync.RWMutex               // For concurrent access\n}\n```\n\n**Memtable Operations:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `Insert` | `SeriesKey`, `DataPoint` | `error` | Adds a point to the appropriate series slice, maintaining sorted order by timestamp |\n| `GetRange` | `SeriesKey`, `TimeRange` | `[]DataPoint` | Returns all points for a series within time range (binary search) |\n| `Size` | - | `int64` | Returns approximate memory usage in bytes |\n| `Flush` | - | `map[SeriesKey][]DataPoint` | Returns all data and clears the memtable (becomes immutable) |\n\n**Series Cardinality Tracking**: As points are inserted, we track the number of unique series keys (`len(memtable.data)`). This is critical for monitoring and query planning, as high cardinality (millions of series) can impact performance.\n\n### Flush Mechanism and Out-of-Order Writes\n\nThe memtable is a temporary buffer; eventually, its contents must be written to persistent TSM files. This process is called **flushing**.\n\n#### Flush Triggers\n\nA flush is triggered by one or more conditions, implemented as a decision flowchart (see ![Memtable Flush Decision Flowchart](./diagrams/memtable-flush-flow.svg)):\n\n1. **Size-based**: When the memtable's estimated memory usage exceeds `Config.MaxMemtableSize` (e.g., 256 MB).\n2. **Time-based**: A periodic timer (e.g., every 5 minutes) ensures data doesn't stay in memory too long.\n3. **Manual**: Via administrative command or shutdown hook.\n\nWhen a flush is triggered, the current active memtable is **marked as immutable**, and a new empty memtable becomes active for new writes. This allows writes to continue uninterrupted while flushing occurs in the background.\n\n#### Flush Process\n\nThe flush process converts an immutable memtable to one or more TSM files:\n\n1. **Sort and Partition**: Group points by series key, ensuring timestamps are sorted within each series.\n2. **Create TSM Writer**: For each series, write points in batches of `DefaultMaxPointsPerBlock` (1024) to a new TSM file.\n3. **Apply Compression**: Use delta-of-delta encoding for timestamps and Gorilla XOR for values (as described in Storage Engine Design).\n4. **Write Index**: After all series are written, build and append the index mapping series keys to block locations.\n5. **Persist**: Call `Finish()` to write footer and sync the TSM file to disk.\n6. **Update Metadata**: Update the storage engine's file list and series index.\n7. **Cleanup WAL**: Once the TSM file is durably written, mark the corresponding WAL segments as safe to delete (up to the highest entry ID included in the flush).\n\n#### Handling Out-of-Order Writes\n\nTime-series data may arrive with timestamps that are not monotonically increasing—common in distributed systems with clock skew or batch replays. TempoDB must handle these **out-of-order writes** correctly.\n\n**Strategy: Insertion Sort with Tolerance Window**\n\nFor each series, we maintain points sorted by timestamp. When a new point arrives:\n\n1. Check if its timestamp is **after the last point** for that series → append (common case, O(1)).\n2. If before the last point, perform binary search to find insertion position.\n3. Insert into slice (may require shifting elements, O(n) worst-case).\n\nTo limit the performance impact, we define a **tolerance window** (e.g., 1 hour). Points arriving more than this window before the latest point are rejected as \"too old\" (configurable). Points within the window are accepted but may incur the insertion cost.\n\n> **Design Principle**: Optimize for the common case (in-order writes) while supporting bounded out-of-order writes. This balances performance with practical requirements.\n\n**Late Arrivals During Flush**: A more complex scenario occurs when a point arrives for a series that is currently being flushed (the memtable is immutable). Two approaches:\n\n| Approach | Description | Pros | Cons | Chosen? |\n|----------|-------------|------|------|---------|\n| **Block and Insert** | Block writes to that series until flush completes, then insert into new memtable | Simple; ensures correctness | Blocks writes; poor concurrency | No |\n| **Write to New Memtable with Tombstone** | Write point to active memtable, mark flushed data with \"tombstone\" to indicate missing point | Non-blocking; good performance | Complex; requires compaction to merge; tombstone overhead | **Yes** |\n\nWe choose the second approach: out-of-order points for series being flushed go to the active memtable. The flushed TSM file contains data up to time T. The new memtable contains points with timestamps < T. During query execution, we must merge points from both TSM file and memtable. This is handled during compaction (Milestone 4), which will merge overlapping time ranges.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Not Fsyncing WAL Before Acknowledgment**\n- **Description**: Acknowledging writes to the client before calling `fsync()` on the WAL.\n- **Why it's wrong**: If the OS crashes or loses power, the write may exist only in page cache, not on durable storage. The client believes data is saved, but it's lost.\n- **Fix**: Always call `file.Sync()` (or set `SyncOnWrite = true`) before sending acknowledgment.\n\n⚠️ **Pitfall: Unbounded Memtable Growth**\n- **Description**: Not enforcing size limits on memtable, allowing it to consume all available memory.\n- **Why it's wrong**: Can lead to OOM crashes, especially during write bursts.\n- **Fix**: Implement strict size-based flushing and backpressure (see below).\n\n⚠️ **Pitfall: Ignoring Clock Skew**\n- **Description**: Assuming all timestamps arrive in order, causing incorrectly sorted data.\n- **Why it's wrong**: Queries may return incorrect results (out-of-order points), and compression efficiency suffers.\n- **Fix**: Implement out-of-order insertion logic with a reasonable tolerance window.\n\n⚠️ **Pitfall: Blocking Writes During Flush**\n- **Description**: Making the write path wait for flush completion.\n- **Why it's wrong**: Kills write throughput during heavy load.\n- **Fix**: Use immutable memtables and background flushing; writes continue to new active memtable.\n\n⚠️ **Pitfall: Not Handling Backpressure**\n- **Description**: Accepting writes indefinitely even when system is overloaded.\n- **Why it's wrong**: Leads to uncontrolled memory growth and eventual crash.\n- **Fix**: Implement backpressure mechanism (e.g., pause accepting writes when memtable size reaches 90% of limit).\n\n### Implementation Guidance (Milestone 2)\n\nThis section provides concrete implementation steps for the write path components.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| WAL Storage | Append-only files with `os.File` | Memory-mapped files for zero-copy reads during recovery |\n| Memtable Structure | `map[string][]DataPoint` with binary search insert | Partitioned memtables per series hash to reduce lock contention |\n| Concurrency Control | `sync.RWMutex` per memtable | Lock-free ring buffer for writes, CAS for memtable swap |\n| Batch Serialization | Custom binary format with `encoding/binary` | Protocol Buffers for forward compatibility |\n\n#### B. Recommended File/Module Structure\n\n```\ntempo/\n├── cmd/\n│   └── server/\n│       └── main.go                 # Entry point\n├── internal/\n│   ├── storage/\n│   │   ├── engine.go              # StorageEngine (coordinates WAL, memtable, TSM)\n│   │   ├── wal/\n│   │   │   ├── wal.go             # WAL manager\n│   │   │   ├── segment.go         # Segment implementation (starter code below)\n│   │   │   └── recovery.go        # Recovery scanning logic\n│   │   ├── memtable/\n│   │   │   ├── memtable.go        # Memtable implementation\n│   │   │   └── manager.go         # MemtableManager (handles immutable/flush)\n│   │   └── tsm/\n│   │       ├── writer.go          # TSMWriter (from Milestone 1)\n│   │       └── reader.go          # TSMReader (from Milestone 1)\n│   ├── api/\n│   │   ├── http.go                # HTTP write/query handlers\n│   │   └── ingest.go              # Batch ingestion logic\n│   └── models/\n│       ├── point.go               # DataPoint, SeriesKey types\n│       └── query.go               # Query, TimeRange types\n```\n\n#### C. Infrastructure Starter Code\n\n**Complete WAL Segment Implementation**\n\n```go\n// internal/storage/wal/segment.go\npackage wal\n\nimport (\n    \"encoding/binary\"\n    \"fmt\"\n    \"hash/crc32\"\n    \"io\"\n    \"os\"\n    \"path/filepath\"\n    \"sync\"\n    \"time\"\n)\n\nvar (\n    byteOrder = binary.BigEndian\n    crcTable  = crc32.MakeTable(crc32.Castagnoli)\n)\n\n// SegmentConfig defines configuration for WAL segments.\ntype SegmentConfig struct {\n    MaxSizeBytes   int64         // Maximum size before rotation\n    SyncInterval   time.Duration // How often to sync (0 = never auto-sync)\n    SyncOnWrite    bool          // Sync after every write (strongest durability)\n}\n\n// Segment manages a single WAL segment file.\ntype Segment struct {\n    path    string\n    file    *os.File\n    mu      sync.RWMutex\n    size    int64\n    maxSize int64\n    closed  bool\n    firstID uint64\n    lastID  uint64\n    \n    config  SegmentConfig\n    syncTicker *time.Ticker\n}\n\n// NewSegment creates or opens a WAL segment file.\nfunc NewSegment(path string, firstID uint64, config SegmentConfig) (*Segment, error) {\n    // Create directory if needed\n    if err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {\n        return nil, fmt.Errorf(\"create wal directory: %w\", err)\n    }\n    \n    // Open file in append mode, create if not exists\n    file, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_APPEND, 0644)\n    if err != nil {\n        return nil, fmt.Errorf(\"open wal segment: %w\", err)\n    }\n    \n    // Get current size\n    info, err := file.Stat()\n    if err != nil {\n        file.Close()\n        return nil, fmt.Errorf(\"stat wal segment: %w\", err)\n    }\n    \n    seg := &Segment{\n        path:    path,\n        file:    file,\n        size:    info.Size(),\n        maxSize: config.MaxSizeBytes,\n        firstID: firstID,\n        lastID:  firstID - 1, // Will be incremented on first write\n        config:  config,\n    }\n    \n    // If file has existing data, scan to find lastID\n    if info.Size() > 0 {\n        if err := seg.scanToFindLastID(); err != nil {\n            file.Close()\n            return nil, fmt.Errorf(\"recover segment lastID: %w\", err)\n        }\n    }\n    \n    // Start periodic sync if configured\n    if config.SyncInterval > 0 {\n        seg.syncTicker = time.NewTicker(config.SyncInterval)\n        go seg.periodicSync()\n    }\n    \n    return seg, nil\n}\n\n// WriteEntry appends an entry to the segment.\nfunc (s *Segment) WriteEntry(data []byte) (uint64, error) {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    if s.closed {\n        return 0, fmt.Errorf(\"segment closed\")\n    }\n    \n    // Check if we need to rotate\n    if s.size >= s.maxSize {\n        return 0, fmt.Errorf(\"segment full\")\n    }\n    \n    entryID := s.lastID + 1\n    entry := encodeEntry(entryID, data)\n    \n    // Write to file\n    n, err := s.file.Write(entry)\n    if err != nil {\n        return 0, fmt.Errorf(\"write entry: %w\", err)\n    }\n    \n    // Sync if configured\n    if s.config.SyncOnWrite {\n        if err := s.file.Sync(); err != nil {\n            // If sync fails, the write may not be durable.\n            // In production, we might want to truncate or mark segment as corrupt.\n            return 0, fmt.Errorf(\"sync entry: %w\", err)\n        }\n    }\n    \n    s.size += int64(n)\n    s.lastID = entryID\n    \n    return entryID, nil\n}\n\n// Scan reads all entries from the segment.\nfunc (s *Segment) Scan(fn func(id uint64, data []byte) error) error {\n    s.mu.RLock()\n    defer s.mu.RUnlock()\n    \n    if s.closed {\n        return fmt.Errorf(\"segment closed\")\n    }\n    \n    // Seek to beginning\n    if _, err := s.file.Seek(0, io.SeekStart); err != nil {\n        return fmt.Errorf(\"seek to start: %w\", err)\n    }\n    \n    reader := io.Reader(s.file)\n    for {\n        entry, id, err := decodeEntry(reader)\n        if err == io.EOF {\n            break\n        }\n        if err != nil {\n            return fmt.Errorf(\"decode entry at offset %d: %w\", s.size, err)\n        }\n        \n        if err := fn(id, entry); err != nil {\n            return err\n        }\n    }\n    \n    return nil\n}\n\n// Close unmaps memory and releases resources.\nfunc (s *Segment) Close() error {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    if s.syncTicker != nil {\n        s.syncTicker.Stop()\n    }\n    \n    s.closed = true\n    return s.file.Close()\n}\n\n// Delete removes the segment file from disk.\nfunc (s *Segment) Delete() error {\n    s.mu.Lock()\n    defer s.mu.Unlock()\n    \n    if !s.closed {\n        if err := s.Close(); err != nil {\n            return err\n        }\n    }\n    \n    return os.Remove(s.path)\n}\n\n// Helper functions\nfunc encodeEntry(id uint64, data []byte) []byte {\n    // Entry format: [4 length][4 checksum][8 id][data]\n    // Length includes all fields (4+4+8+len(data))\n    totalLen := 4 + 4 + 8 + len(data)\n    buf := make([]byte, totalLen)\n    \n    // Write length\n    byteOrder.PutUint32(buf[0:4], uint32(totalLen))\n    \n    // Write placeholder for checksum\n    checksumPos := 4\n    \n    // Write ID\n    byteOrder.PutUint64(buf[8:16], id)\n    \n    // Copy data\n    copy(buf[16:], data)\n    \n    // Compute and write checksum (over id + data)\n    checksum := crc32.Checksum(buf[8:], crcTable)\n    byteOrder.PutUint32(buf[checksumPos:checksumPos+4], checksum)\n    \n    return buf\n}\n\nfunc decodeEntry(r io.Reader) ([]byte, uint64, error) {\n    // Read length\n    var lenBuf [4]byte\n    if _, err := io.ReadFull(r, lenBuf[:]); err != nil {\n        return nil, 0, err\n    }\n    totalLen := byteOrder.Uint32(lenBuf[:])\n    \n    // Read entire entry\n    entry := make([]byte, totalLen)\n    copy(entry[0:4], lenBuf[:])\n    if _, err := io.ReadFull(r, entry[4:]); err != nil {\n        return nil, 0, err\n    }\n    \n    // Verify checksum\n    storedChecksum := byteOrder.Uint32(entry[4:8])\n    computedChecksum := crc32.Checksum(entry[8:], crcTable)\n    if storedChecksum != computedChecksum {\n        return nil, 0, fmt.Errorf(\"checksum mismatch: stored %x, computed %x\", \n            storedChecksum, computedChecksum)\n    }\n    \n    // Extract ID and data\n    id := byteOrder.Uint64(entry[8:16])\n    data := entry[16:]\n    \n    return data, id, nil\n}\n\nfunc (s *Segment) scanToFindLastID() error {\n    lastID := s.firstID - 1\n    err := s.Scan(func(id uint64, data []byte) error {\n        if id != lastID+1 {\n            return fmt.Errorf(\"non-sequential ID: expected %d, got %d\", lastID+1, id)\n        }\n        lastID = id\n        return nil\n    })\n    \n    if err != nil {\n        return err\n    }\n    \n    s.lastID = lastID\n    return nil\n}\n\nfunc (s *Segment) periodicSync() {\n    for range s.syncTicker.C {\n        s.mu.Lock()\n        if !s.closed {\n            s.file.Sync() // Ignore error for periodic sync\n        }\n        s.mu.Unlock()\n    }\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**Memtable Implementation Skeleton**\n\n```go\n// internal/storage/memtable/memtable.go\npackage memtable\n\nimport (\n    \"sort\"\n    \"sync\"\n    \"time\"\n    \n    \"tempo/internal/models\"\n)\n\n// Memtable holds incoming data points in memory before flushing to disk.\ntype Memtable struct {\n    data map[string][]models.DataPoint // key: SeriesKey.String()\n    size int64                         // Approximate memory usage in bytes\n    mu   sync.RWMutex\n    \n    // For out-of-order handling\n    maxOutOfOrderWindow time.Duration\n}\n\n// NewMemtable creates a new empty memtable.\nfunc NewMemtable(maxOutOfOrderWindow time.Duration) *Memtable {\n    return &Memtable{\n        data: make(map[string][]models.DataPoint),\n        size: 0,\n        maxOutOfOrderWindow: maxOutOfOrderWindow,\n    }\n}\n\n// Insert adds a point to the appropriate series slice, maintaining sorted order.\nfunc (m *Memtable) Insert(seriesKey models.SeriesKey, point models.DataPoint) error {\n    // TODO 1: Convert seriesKey to string key using seriesKey.String()\n    // TODO 2: Lock the memtable for writing (m.mu.Lock())\n    // TODO 3: Get the slice for this series (create if doesn't exist)\n    // TODO 4: Check if point is out of order:\n    //    - If slice is empty, append and update size\n    //    - If point timestamp > last point timestamp, append (common case)\n    //    - Else, binary search to find insertion position\n    // TODO 5: Validate out-of-order window: if point is too old (beyond maxOutOfOrderWindow \n    //         compared to latest point), return error\n    // TODO 6: Insert at correct position (may need to shift elements)\n    // TODO 7: Update m.size with approximate size of new point\n    // TODO 8: Return any error (e.g., out of window)\n    \n    return nil\n}\n\n// GetRange returns all points for a series within time range.\nfunc (m *Memtable) GetRange(seriesKey models.SeriesKey, tr models.TimeRange) []models.DataPoint {\n    // TODO 1: Convert seriesKey to string key\n    // TODO 2: Lock for reading (m.mu.RLock())\n    // TODO 3: Get slice for series\n    // TODO 4: If no points, return empty slice\n    // TODO 5: Binary search to find start index (first point >= tr.Start)\n    // TODO 6: Iterate from start index while point timestamp <= tr.End\n    // TODO 7: Collect points into result slice\n    // TODO 8: Return result\n    \n    return nil\n}\n\n// Size returns approximate memory usage in bytes.\nfunc (m *Memtable) Size() int64 {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n    return m.size\n}\n\n// Flush returns all data and clears the memtable.\nfunc (m *Memtable) Flush() map[string][]models.DataPoint {\n    // TODO 1: Lock for writing (m.mu.Lock())\n    // TODO 2: Create copy of m.data\n    // TODO 3: Reset m.data to new empty map\n    // TODO 4: Reset m.size to 0\n    // TODO 5: Return the copied data\n    return nil\n}\n\n// Helper function for binary search in sorted DataPoint slice\nfunc findInsertIndex(points []models.DataPoint, ts time.Time) int {\n    // TODO: Implement binary search returning index where point should be inserted\n    // Use sort.Search with comparison on point.Timestamp\n    return 0\n}\n```\n\n**Storage Engine Write Path Skeleton**\n\n```go\n// internal/storage/engine.go (partial)\npackage storage\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n    \n    \"tempo/internal/models\"\n    \"tempo/internal/storage/memtable\"\n    \"tempo/internal/storage/wal\"\n)\n\n// StorageEngine coordinates writes and reads.\ntype StorageEngine struct {\n    config Config\n    wal    *wal.WAL\n    memtables *MemtableManager\n    // ... other fields from naming conventions\n    \n    flushCh chan *memtable.Memtable\n    stopCh  chan struct{}\n}\n\n// WritePoint writes a single data point.\nfunc (e *StorageEngine) WritePoint(ctx context.Context, \n    seriesKey models.SeriesKey, point models.DataPoint) error {\n    \n    // TODO 1: Validate point (timestamp not in future, etc.)\n    // TODO 2: Batch this single point with others if possible, or create single-point batch\n    // TODO 3: Serialize batch to bytes (e.g., using encodeBatch helper)\n    // TODO 4: Write to WAL with wal.WriteEntry(serialized)\n    // TODO 5: If WAL write succeeds, insert into active memtable\n    // TODO 6: Check if memtable needs flush (size > threshold)\n    // TODO 7: If flush needed, trigger async flush via flushCh\n    // TODO 8: Return any error (e.g., WAL write failed)\n    \n    return nil\n}\n\n// WritePointsBatch writes multiple data points efficiently.\nfunc (e *StorageEngine) WritePointsBatch(ctx context.Context, \n    points []models.DataPoint) error {\n    \n    // TODO 1: Group points by series key for efficient serialization\n    // TODO 2: Apply backpressure: check if memtable size > 90% of max\n    //         If yes, wait a bit or return error\n    // TODO 3: Serialize batch\n    // TODO 4: Write to WAL\n    // TODO 5: Insert all points into memtable\n    // TODO 6: Check flush condition\n    // TODO 7: Return any error\n    \n    return nil\n}\n\n// flushMemtable flushes an immutable memtable to TSM files.\nfunc (e *StorageEngine) flushMemtable(mt *memtable.Memtable) error {\n    // TODO 1: Get all data from memtable via mt.Flush()\n    // TODO 2: Create new TSM writer (tsm.NewTSMWriter)\n    // TODO 3: For each series in memtable data:\n    //    a. Sort points by timestamp (should already be sorted)\n    //    b. Write in batches of DefaultMaxPointsPerBlock using writer.WriteSeries\n    // TODO 4: Finish TSM file (writer.Finish())\n    // TODO 5: Register new TSM file in engine's file list\n    // TODO 6: Notify WAL that entries up to certain ID are durable\n    // TODO 7: Schedule deletion of old WAL segments\n    // TODO 8: Update series index metadata\n    \n    return nil\n}\n```\n\n#### E. Language-Specific Hints\n\n1. **Concurrency**: Use `sync.RWMutex` for memtable access. For higher throughput, consider sharding memtables by series key hash to reduce lock contention.\n2. **Memory Management**: Estimate memtable size as: `(len(seriesKey) + 16) * points` (timestamp: 8 bytes, value: 8 bytes). Use `runtime.MemStats` to monitor actual usage.\n3. **Backpressure**: When memtable reaches 90% capacity, use a `sync.Cond` to make writers wait or return a \"too many requests\" error.\n4. **WAL Sync**: Use `file.Sync()` for durability. For better performance, batch syncs with `SyncInterval` (e.g., 100ms) but acknowledge writes after WAL append (not after sync). This provides \"group commit\" semantics.\n5. **Error Handling**: If WAL write fails, do NOT insert into memtable (data would be lost on crash). Return error to client for retry.\n\n#### F. Milestone Checkpoint\n\nAfter implementing Milestone 2, verify with:\n\n```bash\n# Run unit tests for write path components\ngo test ./internal/storage/wal/... -v\ngo test ./internal/storage/memtable/... -v\ngo test ./internal/storage/... -run TestWritePath -v\n\n# Start the server and test write throughput\ngo run cmd/server/main.go &\n# In another terminal, send test writes\ncurl -X POST http://localhost:8080/write \\\n  -d \"cpu,host=server01 value=0.64 $(date +%s%N)\"\ncurl -X POST http://localhost:8080/write \\\n  -d \"cpu,host=server01 value=0.73 $(date +%s%N)\"\n```\n\n**Expected Behavior**:\n- Points are acknowledged immediately (HTTP 204).\n- WAL files appear in data directory (`data/wal/segment-000001.wal`).\n- Memtable grows in memory.\n- When memtable reaches size threshold (or after 5 minutes), a TSM file is created in `data/tsm/`.\n- After flush, corresponding WAL segments are deleted.\n\n**Signs of Trouble**:\n- No WAL files → WAL not being written before acknowledgment.\n- TSM files not created → flush not triggering or failing silently.\n- Memory grows unbounded → memtable size calculation or flush triggering broken.\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Write returns error \"segment full\" but no flush happening | WAL rotation not working or max size too small | Check `SegmentConfig.MaxSizeBytes`; look for logs about flush | Increase max size or ensure flushes run |\n| High memory usage even after flush | Memtable data not being cleared after flush | Check `Memtable.Flush()` returns copy and resets internal map | Ensure `Flush()` creates new map, not sharing reference |\n| Recovery loses recent writes on restart | WAL not synced to disk before acknowledgment | Check if `SyncOnWrite=true` or periodic sync interval | Enable `SyncOnWrite` or decrease sync interval |\n| Out-of-order points cause panic or incorrect order | Binary search or insertion logic bug | Add test with random timestamp insertion; check sort order | Fix `findInsertIndex` and slice insertion logic |\n| Write throughput decreases over time | Lock contention on single memtable | Profile with `pprof`; look for lock wait times | Implement sharded memtables (advanced) |\n\n\n## Query Engine Design\n\n> **Milestone(s):** Milestone 3: Query Engine\n\nThe query engine is the brain of TempoDB, responsible for interpreting user requests, locating relevant data, and transforming raw points into meaningful results. Unlike write-optimized paths that handle high-velocity sequential ingestion, the query engine must excel at **efficient scanning, filtering, and aggregation** over potentially massive historical datasets. This section details how TempoDB translates a declarative query into an efficient execution plan that leverages the storage engine's columnar layout and indexing to minimize data movement and computational overhead.\n\n### Mental Model: The Library Research Assistant\n\nImagine you're a researcher in a vast library (the database) requesting information about \"average temperature in New York during July 2023.\" You don't want every single measurement; you need a summary. A skilled research assistant (the query engine) follows a systematic process:\n\n1.  **Consult the Card Catalog (Index):** First, they check the card catalog (the series index and TSM file indexes) to identify which books (TSM files) contain temperature data for New York. They note the specific shelves (block offsets) where July 2023 data is located.\n2.  **Retrieve Relevant Pages (Block Pruning):** They don't bring you entire books. Instead, they go to the shelves, pull only the relevant chapters (data blocks) that overlap with your time range, ignoring books about other cities or months outside July.\n3.  **Photocopy Necessary Columns (Columnar Scan):** Within each chapter, they don't copy every word. They use a photocopier that can extract only the timestamp and temperature value columns (columnar projection), skipping irrelevant fields.\n4.  **Summarize Information (Aggregation):** Back at your desk, they don't dump 10,000 individual readings. They calculate the average temperature for each day (GROUP BY time(1d)), producing a concise table of 31 daily averages.\n5.  **Present Final Report (Streaming Results):** They hand you the summary table one row at a time (streaming) so you can start analyzing immediately, without waiting for the entire calculation to finish.\n\nThis mental model captures the essence of the query engine: **intelligent indexing, selective data retrieval, columnar efficiency, and early aggregation** to transform a broad request into a precise, resource-efficient answer.\n\n### Query Parsing, Planning, and Execution\n\nQuery processing in TempoDB follows a classic three-stage pipeline: parse the user's request into an abstract syntax tree (AST), create an optimized execution plan, and then execute that plan against the storage engine. The critical optimization is **predicate pushdown**—applying filters for time and tags as early as possible, ideally at the storage block layer, to avoid moving unnecessary data into memory.\n\n#### 1. Query Parsing\n\nThe query engine accepts queries in a SQL-like dialect extended with time-series specific constructs (e.g., `GROUP BY time(1h)`). The parser, typically implemented using a **recursive descent** approach, validates syntax and converts the textual query into a structured `Query` object defined in the data model.\n\n**Core Query Structure Table:**\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `Measurement` | `string` | The target measurement (e.g., \"cpu_usage\"). |\n| `Tags` | `map[string]string` | Equality filters on tags (e.g., `host=\"server1\"`). Multiple tags are combined with AND logic. |\n| `TimeRange` | `TimeRange` | Mandatory start and end timestamps for the query. Supports relative times (e.g., `now() - 1h`). |\n| `Aggregate` | `*AggregateFunction` | Optional aggregate function (`SUM`, `AVG`, `MIN`, `MAX`, `COUNT`). If `nil`, raw points are returned. |\n| `GroupByWindow` | `time.Duration` | Optional time bucket width for `GROUP BY time(...)`. Must be > 0 if aggregation is specified. |\n| `Fields` | `[]string` | List of field names to return. For simplicity in v1, we assume a single numeric field (float64). |\n\nThe parser handles expressions like:\n```sql\nSELECT avg(temperature) FROM weather WHERE city='NYC' AND time >= '2023-07-01' AND time < '2023-08-01' GROUP BY time(1d)\n```\nThis would produce a `Query` object with `Measurement=\"weather\"`, `Tags={\"city\":\"NYC\"}`, the corresponding `TimeRange`, `Aggregate=AggregateAvg`, and `GroupByWindow=24*time.Hour`.\n\n> **Key Insight:** Parsing is a pure syntactic transformation. No optimization or storage access occurs at this stage. The goal is to produce a valid, unambiguous internal representation.\n\n#### 2. Query Planning\n\nThe planner takes the parsed `Query` and the current state of the storage engine (list of TSM files, series index) and produces an executable `QueryPlan`. The plan's primary job is to **push predicates down** to the lowest possible level.\n\n**Query Plan Data Structure:**\n| Component | Responsibility |\n| :--- | :--- |\n| **Series Selection** | Uses the `SeriesIndex` (map from series key to metadata) to find all `SeriesKey` values matching the `Measurement` and `Tags` predicates. This yields a list of series IDs or keys to fetch data for. |\n| **TSM File Selection** | For each candidate series, consults the `TSMIndex` of each TSM file to identify which files contain blocks overlapping with the query's `TimeRange`. Files with `MaxTime < query.Start` or `MinTime > query.End` are skipped entirely. |\n| **Block-Level Pruning** | Within each selected TSM file and series, the plan examines individual `IndexEntry` blocks. Blocks that don't overlap the time range are skipped. The `MinTime`/`MaxTime` in each index entry make this a constant-time check. |\n| **Aggregation Strategy** | Decides where to perform aggregation: <br> • **Pushdown**: For simple aggregates like `MIN`/`MAX`, the storage engine can return the pre-computed block-level min/max if available. <br> • **Partial Aggregation**: For `SUM`, `COUNT`, `AVG`, the plan may instruct scanners to compute intermediate sums/counts per block, which are then merged. |\n| **Execution Order** | Determines the scan order (typically series-by-series, then time-ordered blocks) to maximize temporal locality and cache friendliness. |\n\nThe output is a tree of primitive operations: `SeriesScan -> TSMFileScan -> BlockScan -> [Filter] -> [Aggregate]`. The plan is stateless; it's a recipe for execution.\n\n#### 3. Query Execution\n\nThe executor runs the plan, coordinating between components to stream results back to the client. It employs an **iterator model** (also known as the Volcano model) where each stage pulls data from the stage below it. This enables pipelining and limits memory usage.\n\n**Execution Step-by-Step:**\n1.  **Initialize Scanners:** For each series identified in the plan, create a `SeriesScanner` that manages scanning across multiple TSM files.\n2.  **Block Iterator:** Each `SeriesScanner` creates a `BlockIterator` for each relevant TSM file. The iterator uses the file's `TSMIndex` to seek to the first block with `MaxTime >= query.Start`.\n3.  **Block Decoding:** The iterator reads the compressed block from the memory-mapped file (via `TSMReader.ReadBlock`), decompresses the timestamps (delta-of-delta) and values (Gorilla XOR), and yields individual `DataPoint` objects within the query's time range.\n4.  **Filter Application:** Any remaining tag filters (beyond the equality filters used for series selection) are applied point-by-point. In v1, all tag filters are equality-based and pushed to series selection, so this step is often a no-op.\n5.  **Aggregation Window Management:** If `GroupByWindow` is specified, the executor maintains a sliding or tumbling window accumulator. As points are streamed in chronological order, they are added to the current time bucket. When a point's timestamp crosses a bucket boundary, the current bucket's aggregate is finalized and emitted, and a new accumulator is started.\n6.  **Streaming Output:** Final results (raw points or aggregated buckets) are written to an output channel or buffer as they are produced. The HTTP/gRPC handler streams these results to the client, interleaving computation and network I/O.\n\n**Sequence of Operations:**\nThe flow can be visualized in the sequence diagram: ![Sequence Diagram: Query Path](./diagrams/query-path-sequence.svg)\n\n> **Architecture Decision: Iterator-Based Execution Model**\n> - **Context:** We need to execute queries that may scan millions of points without loading all data into memory at once.\n> - **Options Considered:**\n>     1. **Materialize-then-process:** Read all relevant points into a slice, then apply filters/aggregation.\n>     2. **Iterator (Volcano) model:** Each operator implements a `Next()` interface, pulling data through the pipeline.\n>     3. **Vectorized/batch model:** Operators process chunks of data (e.g., 1024 points) at a time.\n> - **Decision:** Use the iterator model for v1.\n> - **Rationale:** The iterator model is simpler to implement correctly, naturally supports streaming, and maps well to our block-by-block scanning pattern. While vectorized execution can be more CPU-efficient, the complexity outweighs the benefit for an educational codebase. The memory efficiency of streaming is critical for large range queries.\n> - **Consequences:** We get low memory overhead and early result emission, but each point incurs virtual function call overhead. This can be optimized later by switching to batch processing within operators (e.g., process a full block of 1024 points at a time).\n\n| Option | Pros | Cons | Chosen? |\n| :--- | :--- | :--- | :--- |\n| **Materialize-then-process** | Simple imperative logic, easy debugging. | Memory explosion risk, delays first result. | No |\n| **Iterator Model** | Streams results, memory efficient, composable. | Per-point call overhead, more complex state machines. | **Yes** |\n| **Vectorized Model** | Excellent CPU cache utilization, modern best practice. | Significant implementation complexity, harder to debug. | No (future extension) |\n\n### Aggregations and Downsampling\n\nAggregation transforms high-resolution raw data into summarized insights, which is fundamental for time-series analysis. TempoDB supports built-in aggregates and **tumbling window** grouping.\n\n#### Built-in Aggregate Functions\n\nThe five core aggregates operate on the `Value` field of `DataPoint`:\n\n| Function | Computation | Notes |\n| :--- | :--- | :--- |\n| `COUNT` | Number of points in the group. | Includes `null`/missing? In v1, all points have values. |\n| `SUM` | Sum of all values. | |\n| `AVG` | `SUM / COUNT`. | Computed from intermediate sum and count to avoid precision loss. |\n| `MIN` | Minimum value. | Can use block-level min as an optimization. |\n| `MAX` | Maximum value. | Can use block-level max as an optimization. |\n\n**Aggregation Algorithm (per series, per time bucket):**\n1.  Initialize accumulators: `sum = 0`, `count = 0`, `min = +Inf`, `max = -Inf`.\n2.  For each `DataPoint` in chronological order within the bucket:\n    a. `sum += point.Value`\n    b. `count++`\n    c. `min = math.Min(min, point.Value)`\n    d. `max = math.Max(max, point.Value)`\n3.  At bucket boundary, compute final aggregate:\n    - `COUNT` → `count`\n    - `SUM` → `sum`\n    - `AVG` → `sum / float64(count)`\n    - `MIN` → `min`\n    - `MAX` → `max`\n4.  Emit a new `DataPoint` representing the bucket. Its timestamp is the **start** of the bucket window (alignment is configurable, but default to start). Its value is the aggregate result.\n5.  Reset accumulators for the next bucket.\n\n#### GROUP BY time() and Tumbling Windows\n\nThe `GROUP BY time(<interval>)` clause creates **tumbling windows**—contiguous, non-overlapping time intervals that partition the data. A point belongs to exactly one bucket based on its timestamp.\n\n**Window Alignment:** Buckets are aligned to a fixed epoch (e.g., Unix epoch: 1970-01-01T00:00:00Z). The bucket for a timestamp `t` is calculated as:\n```\nbucket_start = epoch + floor((t - epoch) / interval) * interval\nbucket_end = bucket_start + interval\n```\nFor example, with `interval=1h` and `epoch=0`, the timestamp `2023-07-01 10:22:00` falls into the bucket `[2023-07-01 10:00:00, 2023-07-01 11:00:00)`.\n\n**Execution with Streaming:** The executor maintains current bucket state per series (or per group if grouping by tags). As points arrive in time order (guaranteed by the storage layout), the executor checks:\n- If `point.Timestamp < current_bucket_end`, add to current accumulators.\n- If `point.Timestamp >= current_bucket_end`, finalize and emit the current bucket, then advance the window (possibly by multiple intervals if there are gaps in data) and start a new accumulator.\n\n#### Downsampling vs. On-the-Fly Aggregation\n\nDownsampling is a form of **precomputed aggregation** stored persistently, typically applied to older data to save space while preserving trends. It's distinct from on-the-fly aggregation performed at query time.\n\n> **Architecture Decision: On-the-Fly Aggregation Only for Milestone 3**\n> - **Context:** We need to support aggregate queries over historical data. Precomputing downsamples improves query performance but adds storage and complexity.\n> - **Options:**\n>     1. **Always compute on-the-fly:** Execute the full aggregation pipeline for every query.\n>     2. **Pre-compute and store downsampled data:** Run background jobs to create lower-resolution aggregates, and route queries to appropriate resolution.\n> - **Decision:** Implement only on-the-fly aggregation for Milestone 3. Downsampling is deferred to Milestone 4 (Retention & Compaction).\n> - **Rationale:** On-the-fly aggregation is a prerequisite for downsampling and allows us to validate the correctness of aggregation logic first. It also keeps the query engine design focused on execution rather than storage lifecycle. Performance for large historical queries will be addressed later with rollups.\n> - **Consequences:** Queries over long time ranges will be slower initially, but the architecture is ready to incorporate pre-computed rollups as a transparent optimization layer later.\n\n#### Handling Gaps in Data\n\nTime-series data often has gaps (no points recorded for periods). The aggregation semantics must define behavior for empty buckets:\n- `COUNT` returns 0.\n- `SUM`, `AVG`, `MIN`, `MAX` return `null` (or skip the bucket). In v1, we may skip empty buckets entirely in the result set.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Loading Entire Series into Memory Before Filtering**\n*   **Description:** Implementing `SeriesScanner` by first reading *all* points for a series from all files into a slice, then applying time range filters.\n*   **Why It's Wrong:** Defeats the purpose of block-level pruning and can cause out-of-memory errors for long-running series. It also delays the first result.\n*   **Fix:** Implement a lazy `BlockIterator` that only decompresses the next needed block. Use the index's `MinTime`/`MaxTime` to skip blocks entirely.\n\n⚠️ **Pitfall: Incorrect Bucket Alignment for GROUP BY time()**\n*   **Description:** Aligning buckets to the query's start time instead of a fixed epoch, causing non-deterministic results and breaking cacheability.\n*   **Why It's Wrong:** The same absolute time interval queried at different times would produce different bucket boundaries, making results inconsistent and precomputed rollups impossible.\n*   **Fix:** Always align to a global epoch (Unix epoch). Use `floor((timestamp_nanos - epoch_nanos) / interval_nanos) * interval_nanos` to calculate bucket start.\n\n⚠️ **Pitfall: Forgetting to Handle Out-of-Order Points in Queries**\n*   **Description:** Assuming points within a TSM file are perfectly sorted, but out-of-order writes (within tolerance) may have been inserted.\n*   **Why It's Wrong:** Aggregations that assume chronological order (like tumbling windows) may produce incorrect results if points arrive late within a block.\n*   **Fix:** The storage engine should guarantee that TSM files store points in sorted order. The memtable flush and compaction processes must sort points before writing blocks. The query engine can then rely on sorted streams.\n\n⚠️ **Pitfall: Not Pushing Time Predicate to Storage Index**\n*   **Description:** Reading all blocks for a series and filtering points in memory after decompression.\n*   **Why It's Wrong:** Decompression is expensive. Wasting CPU and I/O on irrelevant data destroys performance.\n*   **Fix:** The `QueryPlan` must use the `TSMIndex` `MinTime`/`MaxTime` to skip blocks before they are read. The `BlockIterator.Seek()` method should jump to the first block with `MaxTime >= query.Start`.\n\n⚠️ **Pitfall: Floating-Precision Issues in AVG**\n*   **Description:** Calculating average by accumulating `sum/count` in a loop with floating-point additions, leading to precision loss for large counts.\n*   **Why It's Wrong:** Time-series aggregations often run over millions of points. Standard floating-point error can become significant.\n*   **Fix:** Use Kahan summation or at least `float64` for accumulators. For v1, the error is acceptable for educational purposes, but document the limitation.\n\n### Implementation Guidance (Milestone 3)\n\nThis section provides concrete Go code skeletons and organization tips to implement the query engine.\n\n#### A. Technology Recommendations Table\n| Component | Simple Option | Advanced Option |\n| :--- | :--- | :--- |\n| **Query Parsing** | Handwritten recursive descent parser | Parser generator (ANTLR, pigeon) |\n| **Expression Evaluation** | Switch on `AggregateFunction` enum | Abstract syntax tree (AST) interpreter |\n| **Execution Engine** | Iterator model with `Next()` interface | Vectorized batch processing |\n| **Result Streaming** | Channel of `DataPoint` | Custom `io.Writer` interface |\n\n#### B. Recommended File/Module Structure\n```\ntempo/\n├── cmd/\n│   └── server/                 # Main entry point\n├── internal/\n│   ├── query/                  # Query engine components\n│   │   ├── parser.go           # Query language parser\n│   │   ├── planner.go          # Query plan generation\n│   │   ├── executor.go         # Plan execution and streaming\n│   │   ├── aggregator.go       # Aggregate function implementations\n│   │   ├── iterator.go         # Block and series iterator interfaces\n│   │   └── scanner.go          # TSM file scanner implementation\n│   ├── storage/                # From Milestone 1 & 2\n│   │   ├── tsm/\n│   │   └── wal/\n│   └── models/                 # Shared data structures\n│       ├── query.go            # Query, TimeRange structs\n│       └── series.go           # DataPoint, SeriesKey\n└── pkg/\n    └── api/                    # HTTP/gRPC handlers (Milestone 5)\n```\n\n#### C. Infrastructure Starter Code\n\n**Complete `models/query.go` (shared data structures):**\n```go\npackage models\n\nimport \"time\"\n\n// AggregateFunction represents the built-in aggregation functions.\ntype AggregateFunction int\n\nconst (\n    AggregateNone AggregateFunction = iota\n    AggregateSum\n    AggregateAvg\n    AggregateMin\n    AggregateMax\n    AggregateCount\n)\n\nfunc (af AggregateFunction) String() string {\n    switch af {\n    case AggregateSum:\n        return \"sum\"\n    case AggregateAvg:\n        return \"avg\"\n    case AggregateMin:\n        return \"min\"\n    case AggregateMax:\n        return \"max\"\n    case AggregateCount:\n        return \"count\"\n    default:\n        return \"none\"\n    }\n}\n\n// TimeRange represents an inclusive-exclusive time interval [Start, End).\ntype TimeRange struct {\n    Start time.Time\n    End   time.Time\n}\n\n// Contains returns true if t is within [Start, End).\nfunc (tr TimeRange) Contains(t time.Time) bool {\n    return !t.Before(tr.Start) && t.Before(tr.End)\n}\n\n// Overlaps returns true if tr and other have any intersection.\nfunc (tr TimeRange) Overlaps(other TimeRange) bool {\n    return !tr.Start.After(other.End) && !other.Start.After(tr.End)\n}\n\n// Duration returns End - Start.\nfunc (tr TimeRange) Duration() time.Duration {\n    return tr.End.Sub(tr.Start)\n}\n\n// Query represents a parsed query ready for planning.\ntype Query struct {\n    Measurement   string\n    Tags          map[string]string\n    TimeRange     TimeRange\n    Aggregate     AggregateFunction\n    GroupByWindow time.Duration // 0 means no grouping\n    Field         string        // For v1, we assume a single field\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**1. Query Parser Skeleton (`internal/query/parser.go`):**\n```go\npackage query\n\nimport (\n    \"fmt\"\n    \"strings\"\n    \"time\"\n    \"tempo/internal/models\"\n)\n\n// Parser holds the state for parsing a query string.\ntype Parser struct {\n    scanner *Scanner // Tokenizer (lexer) you need to implement\n    tok     Token    // current token\n    lit     string   // current literal\n}\n\n// ParseQuery parses the input string and returns a models.Query or an error.\nfunc ParseQuery(input string) (*models.Query, error) {\n    p := &Parser{scanner: NewScanner(strings.NewReader(input))}\n    p.next() // prime the pump\n    return p.parseSelectStatement()\n}\n\n// parseSelectStatement parses a SELECT ... FROM ... WHERE ... GROUP BY ... query.\nfunc (p *Parser) parseSelectStatement() (*models.Query, error) {\n    query := &models.Query{Tags: make(map[string]string)}\n\n    // Parse SELECT clause\n    if err := p.parseSelectClause(query); err != nil {\n        return nil, err\n    }\n\n    // Parse FROM clause\n    if err := p.parseFromClause(query); err != nil {\n        return nil, err\n    }\n\n    // Parse WHERE clause (optional)\n    if p.tok == WHERE {\n        if err := p.parseWhereClause(query); err != nil {\n            return nil, err\n        }\n    }\n\n    // Parse GROUP BY clause (optional)\n    if p.tok == GROUP {\n        if err := p.parseGroupByClause(query); err != nil {\n            return nil, err\n        }\n    }\n\n    // Ensure we've consumed all input\n    if p.tok != EOF {\n        return nil, fmt.Errorf(\"unexpected token %s, expected end of input\", p.tok)\n    }\n    return query, nil\n}\n\nfunc (p *Parser) parseSelectClause(q *models.Query) error {\n    // TODO 1: Expect SELECT token, consume it\n    // TODO 2: Parse aggregate function (e.g., avg(temperature)) or field name\n    // TODO 3: Set q.Aggregate and q.Field accordingly\n    // TODO 4: If no aggregate, set q.Aggregate = models.AggregateNone\n    return nil\n}\n\nfunc (p *Parser) parseFromClause(q *models.Query) error {\n    // TODO 1: Expect FROM token, consume it\n    // TODO 2: Expect IDENTIFIER token, set q.Measurement\n    return nil\n}\n\nfunc (p *Parser) parseWhereClause(q *models.Query) error {\n    // TODO 1: Expect WHERE token, consume it\n    // TODO 2: Parse one or more tag equality conditions (tag = 'value') joined by AND\n    // TODO 3: For each condition, add to q.Tags map\n    // TODO 4: Parse time range condition: time >= <start> AND time < <end>\n    // TODO 5: Parse absolute timestamps or relative expressions like now() - 1h\n    // TODO 6: Create models.TimeRange and assign to q.TimeRange\n    return nil\n}\n\nfunc (p *Parser) parseGroupByClause(q *models.Query) error {\n    // TODO 1: Expect GROUP BY token, consume it\n    // TODO 2: Expect TIME token and '('\n    // TODO 3: Parse duration literal (e.g., 1h, 30m)\n    // TODO 4: Convert to time.Duration, assign to q.GroupByWindow\n    return nil\n}\n\nfunc (p *Parser) next() {\n    p.tok, p.lit = p.scanner.Scan()\n}\n```\n\n**2. Query Planner Skeleton (`internal/query/planner.go`):**\n```go\npackage query\n\nimport (\n    \"tempo/internal/models\"\n    \"tempo/internal/storage\"\n)\n\n// QueryPlan is an executable plan for a query.\ntype QueryPlan struct {\n    SeriesKeys []models.SeriesKey\n    // For each series key, list of (TSM file path, block index entries)\n    FileBlocks map[string][]storage.BlockRef\n    Query      models.Query\n}\n\n// Planner creates a QueryPlan from a parsed Query and storage engine state.\ntype Planner struct {\n    engine *storage.StorageEngine\n}\n\nfunc (p *Planner) Plan(query *models.Query) (*QueryPlan, error) {\n    plan := &QueryPlan{\n        Query:      *query,\n        SeriesKeys: []models.SeriesKey{},\n        FileBlocks: make(map[string][]storage.BlockRef),\n    }\n\n    // Step 1: Series Selection\n    // TODO 1: Use engine.seriesIndex to find all series with matching measurement\n    // TODO 2: Filter those series by matching all tags in query.Tags\n    // TODO 3: Append matching series keys to plan.SeriesKeys\n\n    // Step 2: TSM File and Block Selection\n    for _, seriesKey := range plan.SeriesKeys {\n        var blocks []storage.BlockRef\n        // TODO 4: For each TSM file in engine.tsmFiles\n        // TODO 5: Use tsmFile.IndexForSeries(seriesKey) to get block index entries\n        // TODO 6: For each index entry, check if entry.Overlaps(query.TimeRange)\n        // TODO 7: If overlapping, add a BlockRef{FilePath, Entry} to blocks\n        plan.FileBlocks[seriesKey.String()] = blocks\n    }\n    return plan, nil\n}\n\n// BlockRef references a specific block in a TSM file.\ntype BlockRef struct {\n    FilePath string\n    Entry    storage.IndexEntry\n}\n```\n\n**3. Iterator Interface and Series Scanner (`internal/query/iterator.go`):**\n```go\npackage query\n\nimport \"tempo/internal/models\"\n\n// Iterator is the core interface for pulling data through the execution pipeline.\ntype Iterator interface {\n    Next() bool             // Advances to the next point, returns false if no more\n    At() models.DataPoint   // Returns the current point (valid only after Next() == true)\n    Err() error             // Returns any error encountered\n    Close() error           // Releases resources\n}\n\n// SeriesScanner implements Iterator for a single series across multiple TSM files.\ntype SeriesScanner struct {\n    seriesKey models.SeriesKey\n    plan      *QueryPlan\n    current   models.DataPoint\n    // Internal state\n    fileIndex int\n    blockIter *BlockIterator\n    points    []models.DataPoint // decompressed points from current block\n    pointIdx  int\n}\n\nfunc NewSeriesScanner(seriesKey models.SeriesKey, plan *QueryPlan) *SeriesScanner {\n    return &SeriesScanner{\n        seriesKey: seriesKey,\n        plan:      plan,\n        fileIndex: -1,\n    }\n}\n\nfunc (s *SeriesScanner) Next() bool {\n    for {\n        // If we have points in the current block buffer, return the next one\n        if s.points != nil && s.pointIdx < len(s.points) {\n            s.current = s.points[s.pointIdx]\n            s.pointIdx++\n            // Apply time filter (in case block had points outside range due to min/max approximation)\n            if s.plan.Query.TimeRange.Contains(s.current.Timestamp) {\n                return true\n            }\n            continue // point outside range, skip to next\n        }\n\n        // Need to load the next block\n        if s.blockIter == nil || !s.blockIter.Next() {\n            // Move to next file or next block list for this series\n            s.fileIndex++\n            blocks := s.plan.FileBlocks[s.seriesKey.String()]\n            if s.fileIndex >= len(blocks) {\n                return false // No more files/blocks for this series\n            }\n            // TODO: Open TSM file reader for blocks[s.fileIndex].FilePath\n            // TODO: Create BlockIterator for the specific block index entry\n            s.blockIter = NewBlockIterator(reader, blocks[s.fileIndex].Entry)\n            continue\n        }\n\n        // blockIter.Next() returned true, decompress the block\n        // TODO: Call blockIter.At() to get compressed data, decompress into s.points\n        s.pointIdx = 0\n    }\n}\n\nfunc (s *SeriesScanner) At() models.DataPoint { return s.current }\nfunc (s *SeriesScanner) Err() error           { return nil } // propagate errors from blockIter\nfunc (s *SeriesScanner) Close() error {\n    if s.blockIter != nil {\n        return s.blockIter.Close()\n    }\n    return nil\n}\n```\n\n**4. Aggregator Skeleton (`internal/query/aggregator.go`):**\n```go\npackage query\n\nimport (\n    \"math\"\n    \"tempo/internal/models\"\n    \"time\"\n)\n\n// WindowAggregator aggregates points into fixed-time windows.\ntype WindowAggregator struct {\n    input    Iterator\n    window   time.Duration\n    aggregate models.AggregateFunction\n    // Current window state\n    windowStart time.Time\n    sum         float64\n    count       int64\n    min         float64\n    max         float64\n    // Result to emit\n    result models.DataPoint\n    ready  bool\n}\n\nfunc NewWindowAggregator(input Iterator, window time.Duration, agg models.AggregateFunction) *WindowAggregator {\n    return &WindowAggregator{\n        input:     input,\n        window:    window,\n        aggregate: agg,\n        min:       math.Inf(1),\n        max:       math.Inf(-1),\n    }\n}\n\nfunc (a *WindowAggregator) Next() bool {\n    for a.input.Next() {\n        point := a.input.At()\n        bucketStart := alignToWindow(point.Timestamp, a.window)\n\n        if a.windowStart.IsZero() {\n            // First point, initialize window\n            a.windowStart = bucketStart\n        } else if bucketStart != a.windowStart {\n            // Point crossed into a new window, emit current aggregate\n            a.finalizeWindow()\n            a.ready = true\n            a.windowStart = bucketStart\n            a.resetAccumulators()\n            // Add the current point to the new window\n            a.addPoint(point)\n            return true\n        }\n        // Add point to current window\n        a.addPoint(point)\n    }\n    // End of input, emit final window if we have data\n    if a.count > 0 {\n        a.finalizeWindow()\n        a.ready = true\n        return true\n    }\n    return false\n}\n\nfunc (a *WindowAggregator) At() models.DataPoint {\n    if !a.ready {\n        panic(\"At() called without successful Next()\")\n    }\n    a.ready = false\n    return a.result\n}\n\nfunc (a *WindowAggregator) addPoint(p models.DataPoint) {\n    a.sum += p.Value\n    a.count++\n    a.min = math.Min(a.min, p.Value)\n    a.max = math.Max(a.max, p.Value)\n}\n\nfunc (a *WindowAggregator) resetAccumulators() {\n    a.sum = 0\n    a.count = 0\n    a.min = math.Inf(1)\n    a.max = math.Inf(-1)\n}\n\nfunc (a *WindowAggregator) finalizeWindow() {\n    var value float64\n    switch a.aggregate {\n    case models.AggregateCount:\n        value = float64(a.count)\n    case models.AggregateSum:\n        value = a.sum\n    case models.AggregateAvg:\n        value = a.sum / float64(a.count)\n    case models.AggregateMin:\n        value = a.min\n    case models.AggregateMax:\n        value = a.max\n    default:\n        value = 0\n    }\n    a.result = models.DataPoint{Timestamp: a.windowStart, Value: value}\n}\n\n// alignToWindow returns the start time of the window containing t.\nfunc alignToWindow(t time.Time, window time.Duration) time.Time {\n    epoch := time.Unix(0, 0)\n    ns := t.Sub(epoch).Nanoseconds()\n    windowNs := window.Nanoseconds()\n    bucket := ns / windowNs * windowNs\n    return epoch.Add(time.Duration(bucket) * time.Nanosecond)\n}\n```\n\n#### E. Language-Specific Hints\n- **Memory Mapping for TSM Files:** Use `mmap` via `golang.org/x/exp/mmap` or `syscall.Mmap` for zero-copy reads. Remember to `Munmap` when closing readers.\n- **Efficient Iterators:** Implement `Seek` methods on iterators to jump to a specific timestamp using binary search on block index entries.\n- **Concurrent Scanning:** For queries spanning many series, you can scan each series in a separate goroutine and merge results using a fan-in pattern. Use `sync.Pool` for recycling `DataPoint` slices.\n- **Profiling:** Use `pprof` to identify bottlenecks. Likely hotspots: decompression routines and iterator `Next()` calls.\n\n#### F. Milestone Checkpoint\nTo verify your query engine implementation:\n\n1. **Unit Test Parsing:** Run `go test ./internal/query -run TestParser`. You should see successful parsing of various query forms and appropriate errors for malformed queries.\n2. **Integration Test End-to-End:**\n   - Start a test server with an in-memory storage engine.\n   - Write sample data: `curl -X POST http://localhost:8080/write -d \"cpu,host=server1 value=42.5 1672531200000000000\"` (points for different times and series).\n   - Execute a query: `curl \"http://localhost:8080/query?q=SELECT avg(value) FROM cpu WHERE host='server1' AND time >= 1672531200s AND time < 1672617600s GROUP BY time(1h)\"`.\n   - **Expected:** A JSON response containing aggregated values per hour. Verify the averages are mathematically correct.\n3. **Performance Smoke Test:** Write 100,000 points for a single series, then query a 24-hour range. The query should return within a few seconds (most time spent in decompression). Use `time` command to measure.\n\n#### G. Debugging Tips\n| Symptom | Likely Cause | How to Diagnose | Fix |\n| :--- | :--- | :--- | :--- |\n| Query returns no data for a valid time range | Block index min/max times incorrect | Print index entries for the series; verify `MinTime`/`MaxTime` cover written points. | Ensure `TSMWriter` correctly computes and writes block min/max. |\n| GROUP BY time() produces misaligned buckets | Bucket alignment using query start time | Log `alignToWindow` output for sample timestamps. | Change alignment to fixed epoch (Unix). |\n| Aggregated values are wrong (e.g., avg off) | Accumulator reset incorrectly across buckets | Log accumulator state before finalizing each bucket. | Ensure `resetAccumulators()` is called when window changes. |\n| Query runs out of memory for large ranges | Loading all points before aggregation | Add memory profiling (`pprof`). Check if `SeriesScanner` buffers entire series. | Ensure iterator pattern streams points; decompress one block at a time. |\n| Decompression errors during query | Corruption in TSM block or wrong compression algorithm | Write a small tool to read and print raw block bytes, verify checksum. | Check that `compressBlock` and `decompress` use same algorithms; verify CRC32. |\n\n\n## Retention and Compaction Design\n\n> **Milestone(s):** Milestone 4: Retention & Compaction\n\nThe retention and compaction subsystem is TempoDB's housekeeping department—responsible for maintaining data quality, reclaiming storage space, and ensuring the system remains performant as data ages. Without effective lifecycle management, storage costs would balloon, query performance would degrade for historical data, and disk space would eventually be exhausted. This section details how TempoDB automatically manages data from \"hot\" recent writes to \"cold\" historical archives through time-based expiration, file consolidation, and pre-computed summarization.\n\n### Mental Model: The Warehouse Archivist\n\nImagine a massive warehouse storing physical records. New boxes arrive constantly on a conveyor belt (the write path), each containing documents from a specific day. An archivist is responsible for:\n\n1. **Cleaning out old boxes:** Every month, they check expiration dates and shred boxes older than the retention policy (TTL enforcement).\n2. **Consolidating partially-filled boxes:** When multiple boxes contain documents from the same time period but are only half-full, they merge them into fewer, fully-packed boxes to save shelf space (compaction).\n3. **Creating summary catalogs:** For very old records, they don't keep every document but instead create summary binders with daily or weekly totals (downsampling/rollups).\n\nThis archivist works in the background, carefully scheduling their work during quiet periods to avoid disrupting people who need to access current records (queries). They maintain an inventory system (metadata) that tracks which boxes are where and what time periods they cover, updating it whenever they reorganize. This mental model captures the essence of TempoDB's retention and compaction: systematic, background reorganization of data to balance accessibility, storage efficiency, and performance.\n\n### TTL Enforcement and Compaction Strategy\n\nTime-to-live (TTL) enforcement and compaction are two complementary processes that work together to manage data lifecycle. TTL removes data that's no longer needed, while compaction optimizes the storage of data that remains.\n\n#### TTL Enforcement: Time-Based Data Expiration\n\nTTL policies specify how long data should be retained before automatic deletion. In TempoDB, TTL is configured per measurement (similar to a table) with a maximum age. The system periodically scans for data blocks that have exceeded their retention period and deletes entire TSM files when all their data is expired.\n\n**TTL Enforcement Algorithm:**\n1. **Policy Configuration:** Each measurement can have a TTL duration (e.g., 30 days, 1 year, or \"infinite\" for no expiration). The `Config` struct includes a default TTL applied to measurements without explicit policies.\n2. **Periodic Scanning:** A background goroutine runs at configurable intervals (default: 1 hour) to check for expired data.\n3. **File-Level Deletion:** Since TSM files contain immutable blocks covering specific time ranges, the system checks each file's maximum timestamp. If `file.max_timestamp < (current_time - TTL)`, the entire file is marked for deletion.\n4. **Safe Deletion:** Files aren't immediately removed from disk. Instead, they're moved to a \"tombstoned\" state in the metadata, excluded from queries, and physically deleted after a grace period (e.g., 5 minutes) to handle any in-flight queries.\n5. **Metadata Cleanup:** After file deletion, the series index is updated to remove references to the deleted blocks, and the block cache evicts any cached data from those files.\n\n**Data Structures for TTL Management:**\n\n| Type Name | Fields | Description |\n|-----------|--------|-------------|\n| `RetentionPolicy` | `Measurement string`<br>`Duration time.Duration`<br>`Default bool` | Defines how long data for a specific measurement should be retained |\n| `TTLEnforcer` | `policies map[string]RetentionPolicy`<br>`interval time.Duration`<br>`stopCh chan struct{}`<br>`wg sync.WaitGroup` | Manages TTL enforcement across all measurements |\n| `TSMFileRef` (extended) | `Path string`<br>`MinTime time.Time`<br>`MaxTime time.Time`<br>`Measurement string`<br>`Tombstoned bool`<br>`TombstoneTime time.Time` | Reference to a TSM file with metadata needed for TTL decisions |\n\n**TTL Enforcement State Machine:**\n\n| Current State | Event | Next State | Actions |\n|---------------|-------|------------|---------|\n| `Active` | File's max time < (now - TTL) | `Tombstoned` | Mark file as tombstoned, set tombstone time, exclude from query plans |\n| `Tombstoned` | Tombstone time > grace period (e.g., 5 min) | `Deleting` | Schedule physical file deletion, update metadata |\n| `Tombstoned` | Query references file (late arrival) | `Active` | Clear tombstone flag, include in queries |\n| `Deleting` | File successfully deleted | `Deleted` | Remove from file list, update series index |\n| `Deleting` | Deletion fails (e.g., permission) | `Tombstoned` | Log error, retry on next cycle |\n\n> **Design Insight:** TTL operates at the file granularity rather than individual points because:\n> 1. TSM files are immutable, making partial deletion within a file complex\n> 2. Time-series data naturally clusters temporally, so entire files typically expire together\n> 3. File deletion is atomic and efficient compared to rewriting files\n\n#### Compaction Strategy: Level-Based File Consolidation\n\nCompaction merges smaller TSM files into larger, more efficient ones to reduce file count, improve read performance, and reclaim space from deleted or overwritten data. TempoDB uses a level-based compaction strategy inspired by LSM trees but optimized for time-series data.\n\n**Compaction Levels:**\n- **Level 0 (L0):** Freshly flushed memtables (1-2 files per flush). Files may have overlapping time ranges.\n- **Level 1 (L1):** Compacted from L0, non-overlapping time ranges within each file, up to 100MB each.\n- **Level 2 (L2):** Further compacted from L1, larger files (up to 500MB) covering broader time ranges.\n- **Level N (LN):** Continuing hierarchy; higher levels have larger files covering longer time spans.\n\n![Compaction Tier State Machine](./diagrams/compaction-state.svg)\n\n**Compaction Triggers:**\n1. **Count-based:** When a level accumulates too many files (e.g., L0 > 4 files)\n2. **Size-based:** When files in a level exceed size thresholds\n3. **Time-based:** Scheduled compaction during low-activity periods\n4. **Manual:** Admin-triggered compaction\n\n**Compaction Algorithm Steps:**\n1. **Plan Generation:** The compaction planner examines files in a level, selecting those with overlapping time ranges or small sizes.\n2. **Series Grouping:** For each series across selected files, collect all blocks, merging points by timestamp (handling duplicates).\n3. **Time Range Splitting:** If the combined data exceeds the target file size, split by time boundaries to create multiple output files.\n4. **New File Creation:** Write merged data to new TSM files using the standard `TSMWriter`, applying compression.\n5. **Atomic Switch:** Update the metadata to reference new files, mark old files as \"compacted.\"\n6. **Cleanup:** Delete old files after confirming new files are successfully written and referenced.\n\n**Compaction Data Structures:**\n\n| Type Name | Fields | Description |\n|-----------|--------|-------------|\n| `CompactionPlan` | `Level int`<br>`InputFiles []string`<br>`OutputFile string`<br>`TimeRange TimeRange`<br>`Series []string` | Defines which files to compact and where to write output |\n| `CompactionStats` | `Level int`<br>`FilesIn int`<br>`FilesOut int`<br>`BytesIn int64`<br>`BytesOut int64`<br>`Duration time.Duration` | Tracks compaction performance and effectiveness |\n| `CompactionManager` | `plans chan CompactionPlan`<br>`stats map[int]CompactionStats`<br>`mu sync.RWMutex`<br>`stopCh chan struct{}` | Orchestrates compaction across levels |\n\n**Handling Special Cases During Compaction:**\n- **Out-of-order points:** Already resolved in memtable or handled during merge by sorting\n- **Deleted points (tombstones):** Points marked for deletion are omitted from new files\n- **Partial series overlap:** Only merge blocks for series present in input files\n- **Compaction failures:** Roll back by deleting partially written output, keeping input files\n\n> **Critical Consideration:** Compaction must not block write operations. TempoDB achieves this by:\n> 1. Performing compaction on immutable, already-persisted TSM files\n> 2. Using copy-on-write semantics—new files are created before old ones are deleted\n> 3. Allowing reads to continue from old files until the atomic metadata switch\n\n#### Common Pitfalls in Retention and Compaction\n\n⚠️ **Pitfall: Blocking writes during aggressive compaction**\n- **Description:** Running compaction with too many files or too frequently can consume I/O bandwidth, causing write latency spikes.\n- **Why it's wrong:** Time-series databases must sustain high write throughput; blocking writes defeats their primary purpose.\n- **Fix:** Implement compaction throttling based on system load, schedule compaction during off-peak hours, and limit concurrent compaction jobs.\n\n⚠️ **Pitfall: Losing data during TTL/compaction race conditions**\n- **Description:** If compaction runs on a file while TTL is deleting it, data may be lost or corruption may occur.\n- **Why it's wrong:** Data integrity is paramount; any race condition risking data loss is unacceptable.\n- **Fix:** Use a global lock or versioning system for file state changes. Mark files as \"in compaction\" to prevent concurrent TTL deletion.\n\n⚠️ **Pitfall: Infinite compaction loops**\n- **Description:** Poorly configured compaction thresholds cause continuous recompaction of the same data.\n- **Why it's wrong:** Wastes CPU and I/O, causes write amplification, and provides no benefit.\n- **Fix:** Implement generation tracking—once a file is compacted to a higher level, don't recompact it unless new data arrives for overlapping time ranges.\n\n⚠️ **Pitfall: Not monitoring disk space during retention**\n- **Description:** TTL deletes files but doesn't check if disk space is actually reclaimed (some filesystems delay space reclamation).\n- **Why it's wrong:** May lead to disk full errors even though TTL is running.\n- **Fix:** Monitor actual disk free space, implement emergency compaction if space doesn't recover after deletion, and consider filesystem-specific operations (like `fallocate` or TRIM).\n\n### ADR: Continuous vs Scheduled Downsampling\n\n> **Decision: Continuous Downsampling During Compaction**\n> - **Context:** Historical data (e.g., older than 30 days) rarely needs millisecond precision but is valuable for trend analysis. Storing full-resolution data for all history consumes excessive storage. We need to automatically reduce data resolution over time while preserving statistical properties.\n> - **Options Considered:**\n>   1. **Continuous downsampling during compaction:** Generate rollups as part of the standard compaction process when data reaches certain age thresholds.\n>   2. **Scheduled batch downsampling:** Run separate periodic jobs (e.g., nightly) that scan for data eligible for downsampling and create rollups.\n>   3. **On-demand downsampling:** Generate rollups only when queried, caching results for future use.\n> - **Decision:** Option 1—continuous downsampling during compaction.\n> - **Rationale:** \n>   - **Efficiency:** Leverages existing data movement during compaction—data is already being read and rewritten, so adding rollup computation adds minimal overhead.\n>   - **Simplicity:** No separate scheduler, job tracking, or coordination needed.\n>   - **Predictability:** Rollups are created deterministically as data ages through levels, ensuring consistent availability of downsampled data.\n>   - **Storage optimization:** Immediate space savings as data is downsampled, rather than waiting for a scheduled job.\n> - **Consequences:**\n>   - **Positive:** Simplified architecture, efficient use of compaction I/O, deterministic rollup creation.\n>   - **Negative:** Compaction becomes more CPU-intensive, rollup granularity is tied to compaction levels, harder to change downsampling policies without recompaction.\n\n**Downsampling Options Comparison:**\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| Continuous during compaction | - No additional I/O<br>- Deterministic timing<br>- Simplified architecture | - Couples compaction and downsampling<br>- Harder to change policies later | **CHOSEN** for simplicity and efficiency |\n| Scheduled batch jobs | - Flexible scheduling<br>- Independent policy updates<br>- Can run during specific off-hours | - Additional I/O (re-reading data)<br>- Complex job coordination<br>- Delay in space reclamation | Adds operational complexity |\n| On-demand (lazy) | - No upfront computation cost<br>- Adapts to query patterns | - First query pays heavy cost<br>- Unpredictable performance<br>- No storage savings until queried | Unpredictable performance unacceptable |\n\n**Continuous Downsampling Implementation:**\n1. **Policy Definition:** Each measurement can define downsampling rules: e.g., \"after 7 days, keep 1-minute averages; after 30 days, keep 5-minute averages; after 1 year, keep 1-hour averages.\"\n2. **Level-to-Granularity Mapping:** Associate compaction levels with downsampling granularities: L0-1 (raw), L2 (1-min), L3 (5-min), L4 (1-hour).\n3. **Compaction-Time Rollup:** When compacting files from level N to N+1, apply the appropriate aggregation function (avg, sum, min, max, count) to create downsampled points.\n4. **Metadata Tracking:** Store rollup series with special naming convention: `original_series:1min_avg`, `original_series:5min_max`, etc.\n5. **Query Routing:** The query engine automatically selects the appropriate rollup series based on the query's time range and granularity requirements.\n\n**Downsampling Data Structures:**\n\n| Type Name | Fields | Description |\n|-----------|--------|-------------|\n| `DownsamplePolicy` | `Measurement string`<br>`AfterDuration time.Duration`<br>`Granularity time.Duration`<br>`Aggregate AggregateFunction` | Defines when and how to downsample data |\n| `RollupSeriesKey` | `OriginalKey SeriesKey`<br>`Granularity time.Duration`<br>`Function AggregateFunction` | Unique identifier for a rollup series |\n| `Downsampler` | `policies []DownsamplePolicy`<br>`levelMap map[int]time.Duration` | Applies downsampling during compaction |\n\n### Implementation Guidance (Milestone 4)\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Background Scheduler | Go's `time.Ticker` with goroutine | Dedicated worker pool with priority queues |\n| File Deletion | `os.Remove()` | Atomic rename + background delete with retry |\n| Compaction Planning | Simple level-based with fixed thresholds | Cost-based planner analyzing overlap ratios |\n| Downsampling | Fixed aggregation during level transitions | Adaptive sampling based on data variance |\n\n#### B. Recommended File/Module Structure\n\n```\nproject-root/\n  cmd/server/main.go\n  internal/\n    retention/\n      ttl_enforcer.go      # TTL enforcement logic\n      ttl_enforcer_test.go\n      policies.go          # Retention policy definitions\n    compaction/\n      manager.go           # Compaction orchestration\n      planner.go           # Plan generation\n      executor.go          # Plan execution\n      stats.go             # Compaction statistics\n      levels.go            # Level configuration\n      compaction_test.go\n    downsampling/\n      policy.go            # Downsampling policy definitions\n      rollup_writer.go     # Write rollup series\n      aggregator.go        # Aggregation during downsampling\n    storage/\n      tsm/                 # Existing TSM implementation\n      block_cache.go\n    engine.go              # Updated StorageEngine with compaction hooks\n```\n\n#### C. Infrastructure Starter Code\n\n**Background Job Scheduler (Complete Implementation):**\n\n```go\n// internal/retention/scheduler.go\npackage retention\n\nimport (\n\t\"context\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Job represents a background task to be executed periodically\ntype Job struct {\n\tName     string\n\tInterval time.Duration\n\tRun      func(ctx context.Context) error\n\t// Optional jitter to spread out jobs\n\tJitter time.Duration\n}\n\n// Scheduler manages periodic background jobs\ntype Scheduler struct {\n\tjobs   []*Job\n\tcancel context.CancelFunc\n\twg     sync.WaitGroup\n\tmu     sync.RWMutex\n}\n\n// NewScheduler creates a new scheduler\nfunc NewScheduler() *Scheduler {\n\treturn &Scheduler{\n\t\tjobs: make([]*Job, 0),\n\t}\n}\n\n// AddJob registers a job to be run periodically\nfunc (s *Scheduler) AddJob(job *Job) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\ts.jobs = append(s.jobs, job)\n}\n\n// Start begins executing all registered jobs\nfunc (s *Scheduler) Start() {\n\tctx, cancel := context.WithCancel(context.Background())\n\ts.cancel = cancel\n\t\n\tfor _, job := range s.jobs {\n\t\ts.wg.Add(1)\n\t\tgo s.runJob(ctx, job)\n\t}\n}\n\n// Stop gracefully shuts down all jobs\nfunc (s *Scheduler) Stop() {\n\tif s.cancel != nil {\n\t\ts.cancel()\n\t}\n\ts.wg.Wait()\n}\n\n// runJob executes a single job periodically\nfunc (s *Scheduler) runJob(ctx context.Context, job *Job) {\n\tdefer s.wg.Done()\n\t\n\t// Initial jitter to spread job starts\n\tif job.Jitter > 0 {\n\t\tselect {\n\t\tcase <-time.After(time.Duration(float64(job.Jitter) * rand.Float64())):\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n\t\n\tticker := time.NewTicker(job.Interval)\n\tdefer ticker.Stop()\n\t\n\t// Run immediately on start\n\tif err := job.Run(ctx); err != nil {\n\t\tlog.Printf(\"Job %s failed: %v\", job.Name, err)\n\t}\n\t\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\tif err := job.Run(ctx); err != nil {\n\t\t\t\tlog.Printf(\"Job %s failed: %v\", job.Name, err)\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Example usage in main.go:\n// scheduler := retention.NewScheduler()\n// scheduler.AddJob(&retention.Job{\n//     Name: \"ttl-enforcement\",\n//     Interval: 1 * time.Hour,\n//     Run: ttlEnforcer.Run,\n// })\n// scheduler.Start()\n// defer scheduler.Stop()\n```\n\n#### D. Core Logic Skeleton Code\n\n**TTL Enforcement Sweeper:**\n\n```go\n// internal/retention/ttl_enforcer.go\npackage retention\n\nimport (\n\t\"context\"\n\t\"path/filepath\"\n\t\"sync\"\n\t\"time\"\n\t\n\t\"tempo/internal/storage\"\n)\n\n// TTLEnforcer periodically removes expired data\ntype TTLEnforcer struct {\n\tstorage     *storage.Engine\n\tpolicies    map[string]time.Duration // measurement -> TTL\n\tinterval    time.Duration\n\tgracePeriod time.Duration\n\tstopCh      chan struct{}\n\twg          sync.WaitGroup\n\tmu          sync.RWMutex\n}\n\n// NewTTLEnforcer creates a new TTL enforcer\nfunc NewTTLEnforcer(storage *storage.Engine, defaultTTL time.Duration) *TTLEnforcer {\n\treturn &TTLEnforcer{\n\t\tstorage:     storage,\n\t\tpolicies:    make(map[string]time.Duration),\n\t\tinterval:    1 * time.Hour,\n\t\tgracePeriod: 5 * time.Minute,\n\t\tstopCh:      make(chan struct{}),\n\t}\n}\n\n// SetPolicy sets a TTL policy for a measurement\nfunc (e *TTLEnforcer) SetPolicy(measurement string, ttl time.Duration) {\n\te.mu.Lock()\n\tdefer e.mu.Unlock()\n\te.policies[measurement] = ttl\n}\n\n// Run executes one TTL enforcement cycle\nfunc (e *TTLEnforcer) Run(ctx context.Context) error {\n\t// TODO 1: Get current time for cutoff calculation\n\t// TODO 2: Iterate through all TSM files in storage\n\t// TODO 3: For each file, determine its measurement and applicable TTL\n\t// TODO 4: Check if file's MaxTime < (now - TTL)\n\t// TODO 5: If expired, mark file as tombstoned (not deleted yet)\n\t// TODO 6: Check tombstoned files older than grace period for actual deletion\n\t// TODO 7: Delete files from disk and update storage metadata\n\t// TODO 8: Clean up series index entries pointing to deleted files\n\t// TODO 9: Return statistics (files deleted, bytes freed)\n\t\n\treturn nil\n}\n\n// Start begins periodic TTL enforcement\nfunc (e *TTLEnforcer) Start() {\n\te.wg.Add(1)\n\tgo func() {\n\t\tdefer e.wg.Done()\n\t\tticker := time.NewTicker(e.interval)\n\t\tdefer ticker.Stop()\n\t\t\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\t\tif err := e.Run(context.Background()); err != nil {\n\t\t\t\t\tlog.Printf(\"TTL enforcement failed: %v\", err)\n\t\t\t\t}\n\t\t\tcase <-e.stopCh:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// Stop gracefully stops TTL enforcement\nfunc (e *TTLEnforcer) Stop() {\n\tclose(e.stopCh)\n\te.wg.Wait()\n}\n```\n\n**Compaction Planner:**\n\n```go\n// internal/compaction/planner.go\npackage compaction\n\nimport (\n\t\"sort\"\n\t\"time\"\n\t\n\t\"tempo/internal/storage\"\n)\n\n// CompactionPlanner decides which files to compact\ntype CompactionPlanner struct {\n\tstorage     *storage.Engine\n\tlevelConfig map[int]LevelConfig\n}\n\n// LevelConfig defines compaction parameters for a level\ntype LevelConfig struct {\n\tMaxFiles     int\n\tMaxSizeBytes int64\n\tTargetSize   int64\n\t// Downsampling granularity for this level (if any)\n\tDownsampleGranularity time.Duration\n}\n\n// Plan generates compaction plans for overdue levels\nfunc (p *CompactionPlanner) Plan() ([]CompactionPlan, error) {\n\tplans := make([]CompactionPlan, 0)\n\t\n\t// TODO 1: Get current TSM files grouped by level from storage\n\t// TODO 2: For each level, check if compaction is needed:\n\t//   - Too many files (count > MaxFiles)?\n\t//   - Files too small (total size < threshold)?\n\t//   - Time-based trigger (oldest file in level > age threshold)?\n\t// TODO 3: For levels needing compaction, select candidate files:\n\t//   - Prefer files with overlapping time ranges\n\t//   - Prefer smaller files first\n\t//   - Exclude files currently being written to\n\t// TODO 4: Group selected files by time range and series overlap\n\t// TODO 5: Create CompactionPlan for each group:\n\t//   - Set input files\n\t//   - Determine output file path\n\t//   - Set target level (current level + 1)\n\t//   - Include downsampling config if moving to downsampling level\n\t// TODO 6: Return list of plans\n\t\n\treturn plans, nil\n}\n\n// ShouldDownsample checks if compaction to next level requires downsampling\nfunc (p *CompactionPlanner) ShouldDownsample(currentLevel, nextLevel int) bool {\n\t// TODO 1: Check level config for both levels\n\t// TODO 2: Return true if next level has downsampling granularity configured\n\t// TODO 3: Handle edge cases (e.g., level doesn't exist)\n\t\n\treturn false\n}\n```\n\n**Compaction Executor:**\n\n```go\n// internal/compaction/executor.go\npackage compaction\n\nimport (\n\t\"context\"\n\t\"os\"\n\t\"path/filepath\"\n\t\n\t\"tempo/internal/storage\"\n\t\"tempo/internal/storage/tsm\"\n)\n\n// Executor runs compaction plans\ntype Executor struct {\n\tstorage      *storage.Engine\n\tdataDir      string\n\tmaxConcurrent int\n}\n\n// Execute runs a single compaction plan\nfunc (e *Executor) Execute(ctx context.Context, plan CompactionPlan) error {\n\t// TODO 1: Validate all input files exist and are readable\n\t// TODO 2: Create temporary output file path\n\t// TODO 3: For each series in input files:\n\t//   - Read all points across all input blocks for that series\n\t//   - Sort points by timestamp (handle duplicates)\n\t//   - If downsampling required, apply aggregation to create rollup points\n\t//   - Split points into blocks if exceeding block size limit\n\t//   - Write to TSM writer\n\t// TODO 4: Finalize TSM file (write index, footer)\n\t// TODO 5: Atomically rename temporary file to final location\n\t// TODO 6: Update storage metadata to:\n\t//   - Add reference to new file\n\t//   - Mark old files as \"compacted\"\n\t//   - Update series index to point to new blocks\n\t// TODO 7: Schedule old files for deletion (after safe period)\n\t// TODO 8: Update compaction statistics\n\t\n\treturn nil\n}\n\n// mergeSeriesPoints combines points for a series across multiple files\nfunc (e *Executor) mergeSeriesPoints(seriesKey string, inputFiles []string) ([]storage.DataPoint, error) {\n\tpoints := make([]storage.DataPoint, 0)\n\t\n\t// TODO 1: For each input file:\n\t//   - Open TSM reader\n\t//   - Read all blocks for the series\n\t//   - Decompress blocks to get points\n\t//   - Append to points slice\n\t// TODO 2: Sort all points by timestamp\n\t// TODO 3: Remove duplicate timestamps (keep latest write)\n\t// TODO 4: Return merged, sorted points\n\t\n\treturn points, nil\n}\n\n// applyDownsampling reduces point granularity through aggregation\nfunc (e *Executor) applyDownsampling(points []storage.DataPoint, window time.Duration, aggFunc storage.AggregateFunction) ([]storage.DataPoint, error) {\n\tdownsampled := make([]storage.DataPoint, 0)\n\t\n\t// TODO 1: If window is zero or aggFunc is AggregateNone, return original points\n\t// TODO 2: Sort points by timestamp (should already be sorted)\n\t// TODO 3: Initialize window state:\n\t//   - currentWindowStart = alignToWindow(points[0].Timestamp, window)\n\t//   - Initialize aggregator (sum, count, min, max)\n\t// TODO 4: For each point:\n\t//   - If point.Timestamp < currentWindowStart + window:\n\t//       * Add point to current window aggregation\n\t//   - Else:\n\t//       * Finalize current window (compute result based on aggFunc)\n\t//       * Append result to downsampled\n\t//       * Advance window (handle gaps)\n\t//       * Reset aggregator for new window\n\t// TODO 5: Finalize last window\n\t// TODO 6: Return downsampled points\n\t\n\treturn downsampled, nil\n}\n```\n\n#### E. Language-Specific Hints\n\n1. **File Operations:** Use `os.Rename()` for atomic file replacement rather than copy-then-delete. On Windows, you may need to handle \"access denied\" errors by retrying.\n\n2. **Concurrency Control:** Use `sync.RWMutex` for metadata that's frequently read (by queries) but infrequently written (during compaction). Consider using a versioned metadata system to avoid blocking reads during updates.\n\n3. **Error Handling in Background Jobs:** Always recover from panics in goroutines:\n   ```go\n   defer func() {\n       if r := recover(); r != nil {\n           log.Printf(\"Compaction panicked: %v\", r)\n       }\n   }()\n   ```\n\n4. **Memory Management:** When merging points from multiple large files, stream using iterators rather than loading all points into memory:\n   ```go\n   // Use heap-based merging for large datasets\n   type pointIterator interface {\n       Next() bool\n       At() storage.DataPoint\n       Close() error\n   }\n   ```\n\n5. **Testing:** Use temporary directories for compaction tests and verify both content and metadata after operations:\n   ```go\n   func TestCompaction(t *testing.T) {\n       tmpDir := t.TempDir()\n       // ... test compaction ...\n       // Verify: file count decreased, total points preserved\n   }\n   ```\n\n#### F. Milestone Checkpoint\n\n**Verification Command:**\n```bash\n# Run retention and compaction tests\ngo test ./internal/retention/... -v -count=1\ngo test ./internal/compaction/... -v -count=1\n\n# Integration test: Write data, wait for TTL, verify deletion\ngo run cmd/integration_test/main.go --test-ttl\n\n# Manual verification\ncurl -X POST \"http://localhost:8080/api/v1/compact\"  # Trigger manual compaction\ncurl \"http://localhost:8080/api/v1/stats\" | jq '.compaction'  # Check compaction stats\n```\n\n**Expected Behavior:**\n1. TTL enforcement should delete files older than the retention period (check disk space reduction).\n2. Compaction should merge small files into larger ones (observe decreasing file count in data directory).\n3. Downsampling should create rollup series with `:1min_avg` suffix for older data.\n4. Queries spanning long time ranges should automatically use downsampled data for older portions.\n\n**Debugging Signs:**\n- **Symptom:** Disk space not freeing up after TTL.\n  - **Check:** Files may be locked by open readers. Ensure all `TSMReader` instances are closed.\n- **Symptom:** Compaction running continuously without progress.\n  - **Check:** Thresholds may be too aggressive. Increase level size thresholds.\n- **Symptom:** Queries returning incorrect aggregates after downsampling.\n  - **Check:** Verify aggregation logic matches query semantics (e.g., average of averages vs. true average).\n\n\n## Query Language and API Design\n\n> **Milestone(s):** Milestone 5: Query Language & API\n\nThe Query Language and API Design defines how external systems and users interact with TempoDB. This is the face of the database—the contracts and interfaces that developers will use daily. A well-designed API must balance expressiveness with performance, provide clear abstractions that match user mental models, and integrate seamlessly with existing ecosystems. This section specifies TempoDB's query language syntax, HTTP/gRPC APIs, and implementation strategies for parsing and serving requests.\n\n### Mental Model: The Restaurant Menu and Kitchen Window\n\nImagine a restaurant with a detailed menu (the API) that customers use to place orders. The menu categorizes dishes (queries) by type—appetizers (simple range queries), main courses (aggregations), and desserts (downsampling). Each menu item has a precise description (the query language) that the kitchen staff (the database engine) follows to prepare the dish. The kitchen window separates the dining area (client applications) from the cooking area (internal database components), ensuring clean separation of concerns. When a customer orders \"SELECT avg(temperature) FROM sensors WHERE time > now() - 1h GROUP BY time(5m)\", they're asking for a specific recipe. The waiter (HTTP handler) takes the order to the kitchen, where chefs (query parser, planner, executor) prepare it using ingredients (data blocks) from the pantry (storage engine). The finished dish (query results) is then served on a plate (HTTP/JSON response). This mental model emphasizes that the API should be intuitive and complete (like a good menu), while the query language must be unambiguous and executable (like a kitchen recipe), with clear separation between external interfaces and internal processing.\n\n### Query Language Specification\n\nTempoDB implements a SQL-inspired query language specifically designed for time-series patterns. The language supports filtering by time range and tags, aggregating values over windows, and grouping by fixed intervals—the core operations needed for time-series analysis. The design prioritizes clarity for time-series use cases over generic SQL completeness, avoiding the complexity of joins or nested subqueries that are less common in time-series workloads.\n\n#### Core Grammar\n\nThe query language follows a modified BNF grammar with the following productions:\n\n```\nQuery          = SelectClause FromClause [ WhereClause ] [ GroupByClause ] [ LimitClause ]\nSelectClause   = \"SELECT\" ( AggregateFunc \"(\" Field \")\" | Field | \"*\" )\nAggregateFunc  = \"sum\" | \"avg\" | \"min\" | \"max\" | \"count\" | \"first\" | \"last\"\nFromClause     = \"FROM\" Measurement\nWhereClause    = \"WHERE\" Condition ( ( \"AND\" | \"OR\" ) Condition )*\nCondition      = TimeCondition | TagCondition\nTimeCondition  = \"time\" CompareOp ( AbsoluteTime | RelativeTime )\nTagCondition   = TagKey CompareOp ( StringLiteral | NumberLiteral )\nCompareOp      = \">\" | \">=\" | \"<\" | \"<=\" | \"=\" | \"!=\"\nGroupByClause  = \"GROUP BY\" \"time\" \"(\" Duration \")\"\nLimitClause    = \"LIMIT\" Integer\n```\n\n**Key Design Decisions:**\n\n1. **Field vs. Tag References**: Fields (the actual numeric values) can be aggregated and selected directly. Tags are only referenced in WHERE clauses for filtering—they cannot appear in the SELECT clause because they're metadata, not measured values.\n2. **Time Literals**: Supports both absolute timestamps (RFC3339: `2023-10-05T14:30:00Z`) and relative expressions (`now() - 2h`). The `now()` function represents the current server time.\n3. **Single Measurement**: Each query targets exactly one measurement, simplifying execution and aligning with time-series organization patterns.\n4. **Limited Aggregation Placement**: Aggregate functions can only appear in the SELECT clause, not in WHERE—this prevents ambiguous semantics and matches typical time-series query patterns.\n\n#### Query Structure and Semantics\n\n| Component | Purpose | Example | Notes |\n|-----------|---------|---------|-------|\n| **SELECT** | Specifies which field(s) to return and any aggregation | `SELECT avg(temperature)`, `SELECT *` | `*` returns all fields (currently only one supported) |\n| **FROM** | Identifies the measurement (container) to query | `FROM server_metrics` | Measurement names are case-sensitive |\n| **WHERE** | Filters series by tags and time range | `WHERE host='web01' AND time > now() - 1h` | Multiple conditions combined with AND/OR |\n| **GROUP BY time()** | Aggregates results into fixed-width time buckets | `GROUP BY time(5m)` | Creates tumbling windows aligned to epoch |\n| **LIMIT** | Restricts number of returned points | `LIMIT 1000` | Applied after aggregation, useful for preview |\n\n**Example Query Walkthrough:** Consider the query `SELECT max(cpu_usage), min(cpu_usage) FROM servers WHERE region='us-east' AND time >= '2023-10-05T00:00:00Z' AND time < '2023-10-05T01:00:00Z' GROUP BY time(5m)`. This requests the maximum and minimum CPU usage for all servers in the us-east region during a specific hour, with results aggregated into 5-minute buckets. The query engine will:\n1. Identify all series where the `region` tag equals `'us-east'`\n2. Locate data blocks overlapping the specified one-hour time range\n3. For each 5-minute window, compute both the maximum and minimum values across all points in that window\n4. Return one result point per window containing both aggregated values\n\n> **Design Insight:** The query language intentionally omits features like subqueries, JOINs, and complex expressions in the SELECT clause. Time-series queries overwhelmingly follow simple patterns: filter by time and tags, then aggregate over windows. Supporting only these patterns simplifies implementation while covering >90% of real-world use cases.\n\n#### Architecture Decision: SQL-Like vs. Flux-Style Query Language\n\n> **Decision: SQL-Like Syntax with Time Extensions**\n> - **Context**: We need a query language that is immediately familiar to developers (most know SQL) while efficiently expressing time-series operations like time ranges and windowed aggregations. The language must be simple enough to implement within our educational scope yet powerful enough for real use.\n> - **Options Considered**:\n>   1. **Full SQL with time extensions** (like TimescaleDB): Supports full SQL with time-specific functions and hypertables.\n>   2. **SQL-like subset with time primitives** (like InfluxQL): Simplified SQL dialect with first-class time ranges and GROUP BY time().\n>   3. **Functional pipeline language** (like Flux): Chainable operations (filter, map, aggregate) expressed as function calls.\n> - **Decision**: Option 2—SQL-like subset with time primitives.\n> - **Rationale**: \n>   - **Familiarity**: Developers already understand SELECT-FROM-WHERE patterns, reducing learning curve.\n>   - **Implementation Simplicity**: Parsing a constrained SQL dialect is easier than building a full SQL parser or a functional language runtime.\n>   - **Industry Alignment**: InfluxQL and similar dialects have proven effective for time-series queries.\n>   - **Performance**: Simple structure enables straightforward predicate pushdown and execution planning.\n> - **Consequences**:\n>   - **Positive**: Quick adoption, easier parsing/planning, covers common use cases.\n>   - **Negative**: Cannot express complex multi-measurement correlations or custom transformations; users needing advanced operations must post-process results.\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| Full SQL with extensions | Maximum expressiveness, joins, subqueries | Extremely complex parser/planner, overkill for TS | Scope too large for educational project |\n| SQL-like subset | Familiar, simpler implementation, covers 90% of TS queries | Limited expressiveness for complex transformations | **CHOSEN** - Best trade-off |\n| Functional pipeline | Extremely flexible, composable, ideal for complex data flows | Steep learning curve, complex execution engine | Better suited for advanced analytics systems |\n\n### Write and Read APIs\n\nTempoDB exposes two primary interfaces: a write API for data ingestion and a read API for queries. Both use HTTP/REST for simplicity and broad compatibility, with plans for optional gRPC endpoints for high-performance scenarios. The APIs follow industry conventions to ease integration with existing monitoring stacks like Prometheus and Grafana.\n\n#### Write API\n\nThe write API accepts time-series data points in batches using a line protocol format similar to InfluxDB's. This protocol is efficient for network transmission and parsing, supporting thousands of points per request.\n\n**Endpoint:** `POST /api/v1/write`\n\n**Headers:**\n- `Content-Type: text/plain` (for line protocol)\n- Optional: `Content-Encoding: gzip` for compressed payloads\n\n**Request Body (Line Protocol Format):**\n```\n<measurement>[,<tag_key>=<tag_value>[,<tag_key>=<tag_value>]] <field_key>=<field_value>[,<field_key>=<field_value>] <timestamp>\n```\n\nEach line represents a single data point. Tags are optional but must be sorted by key for canonical series key generation. The timestamp is an integer representing nanoseconds since the Unix epoch (optional; defaults to server time).\n\n**Example Request:**\n```http\nPOST /api/v1/write HTTP/1.1\nContent-Type: text/plain\n\nserver,host=web01,region=us-east cpu_usage=42.5,memory_usage=38.2 1696501200000000000\nserver,host=web02,region=us-east cpu_usage=35.1 1696501260000000000\ntemperature,sensor_id=temp001 value=22.4 1696501200000000000\n```\n\n**Response Codes:**\n- `204 No Content`: Success, all points written\n- `400 Bad Request`: Malformed line protocol or invalid timestamp\n- `429 Too Many Requests`: Backpressure applied, client should retry\n- `500 Internal Server Error`: Unexpected server error\n\n**Design Rationale:** The line protocol is text-based for human readability and debuggability, yet compact enough for efficient parsing. It matches industry standards, allowing easy integration with tools like Telegraf. The API accepts batches to amortize write overhead and WAL sync operations.\n\n#### Read API\n\nThe read API accepts queries in the TempoDB query language and returns results in structured JSON format suitable for visualization tools.\n\n**Endpoint:** `POST /api/v1/query` or `GET /api/v1/query?q=...`\n\n**For POST (recommended):**\n- `Content-Type: application/json`\n- Body: `{\"query\": \"SELECT avg(cpu_usage) FROM servers WHERE time > now() - 1h GROUP BY time(5m)\"}`\n\n**For GET:**\n- URL-encoded query parameter: `GET /api/v1/query?q=SELECT+avg%28cpu_usage%29+FROM+servers+WHERE+time+%3E+now%28%29+-+1h+GROUP+BY+time%285m%29`\n\n**Response Format (JSON):**\n```json\n{\n  \"results\": [\n    {\n      \"series\": [\n        {\n          \"name\": \"servers\",\n          \"columns\": [\"time\", \"avg\"],\n          \"values\": [\n            [\"2023-10-05T14:00:00Z\", 42.5],\n            [\"2023-10-05T14:05:00Z\", 43.1],\n            ...\n          ],\n          \"tags\": {\"host\": \"web01\", \"region\": \"us-east\"}\n        }\n      ],\n      \"error\": null\n    }\n  ]\n}\n```\n\n**Pagination Support:** For large result sets, the API supports limit/offset via query parameters or continuation tokens to prevent memory exhaustion.\n\n#### Prometheus Remote Read/Write Compatibility\n\nTo integrate with the Prometheus ecosystem, TempoDB implements the Prometheus remote read/write API endpoints. This allows Prometheus to use TempoDB as long-term storage.\n\n**Prometheus Write Endpoint:** `POST /api/v1/prom/write`\n- Accepts Prometheus remote write protocol (snappy-compressed protobuf)\n- Converts Prometheus samples to TempoDB data points (metric name → measurement, labels → tags)\n- Handles Prometheus's specific timestamp granularity (milliseconds)\n\n**Prometheus Read Endpoint:** `POST /api/v1/prom/read`\n- Accepts Prometheus remote read requests\n- Translates Prometheus matchers and time ranges to TempoDB queries\n- Returns results in Prometheus remote read response format\n\n**Grafana Data Source Compatibility:** TempoDB implements the simple JSON datasource API that Grafana expects, allowing direct querying from Grafana panels. The endpoint `/api/v1/grafana/query` accepts Grafana's time range and target format, translating to TempoDB queries.\n\n#### API Architecture Components\n\n| Component | Responsibility | Implementation Notes |\n|-----------|----------------|----------------------|\n| **HTTP Router** | Routes requests to appropriate handlers | Use `gorilla/mux` or `chi` for flexibility |\n| **Write Handler** | Parses line protocol, validates points, passes to storage engine | Must handle gzip decompression, batch validation |\n| **Query Handler** | Parses query string, calls query engine, formats response | Supports both GET and POST, handles timeout cancellation |\n| **Prometheus Adapter** | Translates Prometheus protobuf to internal format | Uses `prompb` generated Go code for protocol buffers |\n| **Grafana Adapter** | Converts Grafana request format to TempoDB queries | Implements `/search`, `/query`, `/annotations` endpoints |\n| **Authentication Middleware** | (Future) Validates API keys or tokens | Currently placeholder for educational purposes |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Exposing Internal Errors to Clients**\n- **Description**: Returning raw Go errors (like \"panic: nil pointer dereference\") in HTTP responses.\n- **Why It's Wrong**: Reveals implementation details, security risk, poor user experience.\n- **Fix**: Create a layer of well-defined HTTP error responses. Catch panics with middleware, log internal errors server-side, return generic 500 errors to clients.\n\n⚠️ **Pitfall: Not Validating Time Ranges Early**\n- **Description**: Allowing queries like `SELECT * FROM metrics WHERE time > now() - 1000y` that would scan the entire dataset.\n- **Why It's Wrong**: Could cause memory exhaustion, disk thrashing, denial of service.\n- **Fix**: Implement query validation layer that rejects queries with unbounded or extremely large time ranges. Set configurable maximum time range limits.\n\n⚠️ **Pitfall: Poor Line Protocol Parsing Performance**\n- **Description**: Using naive string splitting and conversion for each point in large batches.\n- **Why It's Wrong**: CPU becomes bottleneck, limits write throughput.\n- **Fix**: Use optimized parsing with minimal allocations: pre-allocate slices, reuse buffers, use byte-level scanning instead of regex.\n\n⚠️ **Pitfall: Forgetting Content-Type Handling**\n- **Description**: Only accepting `text/plain` for writes, not supporting `gzip` compression.\n- **Why It's Wrong**: Clients may send compressed data or different content types, causing 400 errors.\n- **Fix**: Check `Content-Encoding` header, decompress transparently. Be liberal in what you accept (within security bounds).\n\n⚠️ **Pitfall: Not Implementing Query Cancellation**\n- **Description**: Long-running queries continue processing even after client disconnects.\n- **Why It's Wrong**: Wastes server resources, could lead to resource exhaustion.\n- **Fix**: Use request context that cancels when client disconnects. Periodically check `ctx.Done()` during query execution.\n\n### Implementation Guidance (Milestone 5)\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **HTTP Framework** | Standard `net/http` with `gorilla/mux` | `chi` router with middleware chain |\n| **Query Parsing** | Recursive descent parser manually written | ANTLR grammar with generated parser |\n| **JSON Marshaling** | Standard `encoding/json` | `json-iterator/go` for faster performance |\n| **Protocol Buffers** | `gogo/protobuf` for Prometheus compatibility | Standard `google.golang.org/protobuf` |\n| **Compression** | Standard `compress/gzip` | Also support `snappy` for Prometheus |\n| **Configuration** | Environment variables + YAML config file | Viper for multi-format config support |\n\n#### Recommended File/Module Structure\n\n```\ntempo/\n├── cmd/\n│   └── tempo-server/\n│       └── main.go                 # Server entry point\n├── internal/\n│   ├── api/\n│   │   ├── handler.go              # HTTP handler registration\n│   │   ├── write_handler.go        # Line protocol parsing and write handling\n│   │   ├── query_handler.go        # Query API endpoint\n│   │   ├── prometheus_handler.go   # Prometheus remote read/write\n│   │   ├── grafana_handler.go      # Grafana data source API\n│   │   └── middleware.go           # Logging, panic recovery, CORS\n│   ├── query/\n│   │   ├── parser/\n│   │   │   ├── parser.go           # Query language parser\n│   │   │   ├── scanner.go          # Token scanner\n│   │   │   ├── ast.go              # Abstract syntax tree types\n│   │   │   └── parse_test.go\n│   │   └── executor/               # (From Milestone 3)\n│   ├── storage/                    # (From Milestone 1 & 2)\n│   └── models/                     # Shared data structures\n└── pkg/\n    └── lineprotocol/               # Reusable line protocol parser\n        ├── parser.go\n        └── parser_test.go\n```\n\n#### Infrastructure Starter Code\n\n**Complete Line Protocol Parser (`pkg/lineprotocol/parser.go`):**\n\n```go\npackage lineprotocol\n\nimport (\n    \"bytes\"\n    \"errors\"\n    \"strconv\"\n    \"strings\"\n    \"time\"\n)\n\nvar (\n    ErrInvalidFormat   = errors.New(\"invalid line protocol format\")\n    ErrInvalidFloat    = errors.New(\"invalid float value\")\n    ErrInvalidInt      = errors.New(\"invalid integer value\")\n    ErrInvalidBoolean  = errors.New(\"invalid boolean value\")\n    ErrInvalidTime     = errors.New(\"invalid timestamp\")\n)\n\n// Point represents a single data point parsed from line protocol\ntype Point struct {\n    Measurement string\n    Tags        map[string]string\n    Fields      map[string]interface{}\n    Timestamp   time.Time\n}\n\n// Parser parses line protocol format\ntype Parser struct {\n    buf []byte\n    pos int\n}\n\n// NewParser creates a new parser for the given byte slice\nfunc NewParser(data []byte) *Parser {\n    return &Parser{buf: data}\n}\n\n// Next parses the next point from the buffer\nfunc (p *Parser) Next() (*Point, error) {\n    if p.pos >= len(p.buf) {\n        return nil, nil // EOF\n    }\n    \n    // Skip empty lines\n    for p.pos < len(p.buf) && (p.buf[p.pos] == '\\n' || p.buf[p.pos] == '\\r') {\n        p.pos++\n    }\n    if p.pos >= len(p.buf) {\n        return nil, nil\n    }\n    \n    start := p.pos\n    // Find end of line\n    for p.pos < len(p.buf) && p.buf[p.pos] != '\\n' {\n        p.pos++\n    }\n    line := p.buf[start:p.pos]\n    p.pos++ // Skip newline\n    \n    return p.parseLine(line)\n}\n\nfunc (p *Parser) parseLine(line []byte) (*Point, error) {\n    pt := &Point{\n        Tags:   make(map[string]string),\n        Fields: make(map[string]interface{}),\n    }\n    \n    // Parse measurement\n    measEnd := bytes.IndexByte(line, ' ')\n    if measEnd == -1 {\n        measEnd = bytes.IndexByte(line, ',')\n        if measEnd == -1 {\n            return nil, ErrInvalidFormat\n        }\n    }\n    pt.Measurement = string(line[:measEnd])\n    remaining := line[measEnd:]\n    \n    // Parse tags (optional)\n    if len(remaining) > 0 && remaining[0] == ',' {\n        tagEnd := bytes.IndexByte(remaining, ' ')\n        if tagEnd == -1 {\n            return nil, ErrInvalidFormat\n        }\n        tagsStr := remaining[1:tagEnd]\n        remaining = remaining[tagEnd:]\n        \n        // Parse comma-separated tags\n        for _, tag := range bytes.Split(tagsStr, []byte{','}) {\n            kv := bytes.SplitN(tag, []byte{'='}, 2)\n            if len(kv) != 2 {\n                return nil, ErrInvalidFormat\n            }\n            pt.Tags[string(kv[0])] = string(kv[1])\n        }\n    }\n    \n    // Skip space before fields\n    if len(remaining) == 0 || remaining[0] != ' ' {\n        return nil, ErrInvalidFormat\n    }\n    remaining = remaining[1:]\n    \n    // Parse fields\n    fieldEnd := bytes.LastIndexByte(remaining, ' ')\n    if fieldEnd == -1 {\n        // No timestamp\n        fieldEnd = len(remaining)\n    }\n    fieldsStr := remaining[:fieldEnd]\n    remaining = remaining[fieldEnd:]\n    \n    for _, field := range bytes.Split(fieldsStr, []byte{','}) {\n        kv := bytes.SplitN(field, []byte{'='}, 2)\n        if len(kv) != 2 {\n            return nil, ErrInvalidFormat\n        }\n        key := string(kv[0])\n        val := kv[1]\n        \n        // Determine field type\n        if len(val) == 0 {\n            return nil, ErrInvalidFormat\n        }\n        \n        // Try parsing as float first (most common)\n        if f, err := strconv.ParseFloat(string(val), 64); err == nil {\n            pt.Fields[key] = f\n            continue\n        }\n        \n        // Try integer\n        if i, err := strconv.ParseInt(string(val), 10, 64); err == nil {\n            pt.Fields[key] = float64(i)\n            continue\n        }\n        \n        // Try boolean\n        if string(val) == \"true\" {\n            pt.Fields[key] = true\n        } else if string(val) == \"false\" {\n            pt.Fields[key] = false\n        } else {\n            // String value (must be quoted)\n            if len(val) >= 2 && val[0] == '\"' && val[len(val)-1] == '\"' {\n                pt.Fields[key] = string(val[1 : len(val)-1])\n            } else {\n                return nil, ErrInvalidFormat\n            }\n        }\n    }\n    \n    // Parse timestamp (optional)\n    if len(remaining) > 0 {\n        if remaining[0] != ' ' {\n            return nil, ErrInvalidFormat\n        }\n        tsStr := strings.TrimSpace(string(remaining))\n        if tsStr != \"\" {\n            // Parse as nanoseconds since epoch\n            ns, err := strconv.ParseInt(tsStr, 10, 64)\n            if err != nil {\n                return nil, ErrInvalidTime\n            }\n            pt.Timestamp = time.Unix(0, ns)\n        }\n    }\n    \n    if pt.Timestamp.IsZero() {\n        pt.Timestamp = time.Now()\n    }\n    \n    return pt, nil\n}\n```\n\n**HTTP Server with Middleware (`internal/api/handler.go`):**\n\n```go\npackage api\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"runtime/debug\"\n    \"time\"\n    \n    \"github.com/gorilla/mux\"\n    \"go.uber.org/zap\"\n)\n\ntype Server struct {\n    router      *mux.Router\n    storage     StorageEngine\n    queryEngine QueryEngine\n    logger      *zap.Logger\n    httpServer  *http.Server\n}\n\ntype StorageEngine interface {\n    WritePoint(ctx context.Context, seriesKey models.SeriesKey, point models.DataPoint) error\n    WritePointsBatch(ctx context.Context, points []models.DataPoint) error\n    Query(ctx context.Context, query models.Query) (QueryResult, error)\n}\n\ntype QueryEngine interface {\n    ExecuteQuery(ctx context.Context, q string) (*QueryResult, error)\n}\n\nfunc NewServer(storage StorageEngine, queryEngine QueryEngine, logger *zap.Logger) *Server {\n    s := &Server{\n        router:      mux.NewRouter(),\n        storage:     storage,\n        queryEngine: queryEngine,\n        logger:      logger,\n    }\n    s.setupRoutes()\n    return s\n}\n\nfunc (s *Server) setupRoutes() {\n    // Recovery middleware for all routes\n    s.router.Use(s.recoveryMiddleware)\n    s.router.Use(s.loggingMiddleware)\n    \n    // API routes\n    s.router.HandleFunc(\"/api/v1/write\", s.handleWrite).Methods(\"POST\")\n    s.router.HandleFunc(\"/api/v1/query\", s.handleQuery).Methods(\"GET\", \"POST\")\n    s.router.HandleFunc(\"/api/v1/prom/write\", s.handlePrometheusWrite).Methods(\"POST\")\n    s.router.HandleFunc(\"/api/v1/prom/read\", s.handlePrometheusRead).Methods(\"POST\")\n    s.router.HandleFunc(\"/api/v1/grafana/query\", s.handleGrafanaQuery).Methods(\"POST\", \"GET\")\n    s.router.HandleFunc(\"/api/v1/grafana/search\", s.handleGrafanaSearch).Methods(\"POST\", \"GET\")\n    s.router.HandleFunc(\"/health\", s.handleHealthCheck).Methods(\"GET\")\n    \n    // Static files for admin UI (optional)\n    s.router.PathPrefix(\"/admin/\").Handler(http.StripPrefix(\"/admin/\", \n        http.FileServer(http.Dir(\"./static/admin\"))))\n}\n\nfunc (s *Server) recoveryMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        defer func() {\n            if err := recover(); err != nil {\n                s.logger.Error(\"panic recovered\",\n                    zap.Any(\"error\", err),\n                    zap.String(\"stack\", string(debug.Stack())),\n                    zap.String(\"path\", r.URL.Path))\n                http.Error(w, \"Internal server error\", http.StatusInternalServerError)\n            }\n        }()\n        next.ServeHTTP(w, r)\n    })\n}\n\nfunc (s *Server) loggingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n        // Wrap response writer to capture status code\n        rw := &responseWriter{ResponseWriter: w, statusCode: http.StatusOK}\n        next.ServeHTTP(rw, r)\n        \n        s.logger.Info(\"HTTP request\",\n            zap.String(\"method\", r.Method),\n            zap.String(\"path\", r.URL.Path),\n            zap.Int(\"status\", rw.statusCode),\n            zap.Duration(\"duration\", time.Since(start)),\n            zap.String(\"remote\", r.RemoteAddr))\n    })\n}\n\ntype responseWriter struct {\n    http.ResponseWriter\n    statusCode int\n}\n\nfunc (rw *responseWriter) WriteHeader(code int) {\n    rw.statusCode = code\n    rw.ResponseWriter.WriteHeader(code)\n}\n\nfunc (s *Server) handleHealthCheck(w http.ResponseWriter, r *http.Request) {\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    json.NewEncoder(w).Encode(map[string]string{\"status\": \"ok\"})\n}\n\nfunc (s *Server) Start(addr string) error {\n    s.httpServer = &http.Server{\n        Addr:         addr,\n        Handler:      s.router,\n        ReadTimeout:  10 * time.Second,\n        WriteTimeout: 30 * time.Second,\n        IdleTimeout:  60 * time.Second,\n    }\n    s.logger.Info(\"Starting HTTP server\", zap.String(\"addr\", addr))\n    return s.httpServer.ListenAndServe()\n}\n\nfunc (s *Server) Shutdown(ctx context.Context) error {\n    if s.httpServer != nil {\n        return s.httpServer.Shutdown(ctx)\n    }\n    return nil\n}\n```\n\n#### Core Logic Skeleton Code\n\n**Query Parser (`internal/api/query/parser/parser.go`):**\n\n```go\npackage parser\n\nimport (\n    \"fmt\"\n    \"strconv\"\n    \"strings\"\n    \"time\"\n    \n    \"tempo/internal/models\"\n)\n\ntype Parser struct {\n    scanner *Scanner\n    tok     Token\n    lit     string\n}\n\nfunc NewParser(input string) *Parser {\n    return &Parser{scanner: NewScanner(input)}\n}\n\n// ParseQuery parses a query string into a models.Query\nfunc (p *Parser) ParseQuery() (*models.Query, error) {\n    // Initialize scanner\n    p.scan()\n    \n    query := &models.Query{\n        Tags: make(map[string]string),\n    }\n    \n    // TODO 1: Parse SELECT clause\n    //   - Expect \"SELECT\" token\n    //   - Parse field name or aggregate function\n    //   - Handle \"*\" for all fields\n    //   - Store field name and aggregate function in query\n    \n    // TODO 2: Parse FROM clause  \n    //   - Expect \"FROM\" token\n    //   - Parse measurement identifier\n    //   - Store in query.Measurement\n    \n    // TODO 3: Parse optional WHERE clause\n    //   - If next token is \"WHERE\", parse conditions\n    //   - Parse time conditions: \"time > now() - 1h\"\n    //   - Parse tag conditions: \"host = 'web01'\"\n    //   - Combine with AND/OR (initially support only AND)\n    //   - Convert relative time expressions to absolute times\n    //   - Store time range in query.TimeRange\n    //   - Store tag filters in query.Tags\n    \n    // TODO 4: Parse optional GROUP BY clause\n    //   - If next token is \"GROUP\", expect \"BY\", \"time\", \"(\"\n    //   - Parse duration like \"5m\", \"1h\"\n    //   - Convert to time.Duration\n    //   - Store in query.GroupByWindow\n    \n    // TODO 5: Parse optional LIMIT clause\n    //   - If next token is \"LIMIT\", parse integer\n    //   - Store in query (may need to add Limit field to models.Query)\n    \n    // TODO 6: Ensure no extra tokens remain\n    \n    return query, nil\n}\n\nfunc (p *Parser) scan() {\n    p.tok, p.lit = p.scanner.Scan()\n}\n\nfunc (p *Parser) expect(tok Token) error {\n    if p.tok != tok {\n        return fmt.Errorf(\"expected %s, got %s\", tok, p.tok)\n    }\n    p.scan()\n    return nil\n}\n\n// parseDuration parses strings like \"5m\", \"1h\", \"30s\" into time.Duration\nfunc parseDuration(s string) (time.Duration, error) {\n    // TODO: Implement duration parsing\n    // Handle common units: ns, us, ms, s, m, h, d, w\n    // Convert d (day) to 24h, w (week) to 168h\n    return 0, nil\n}\n\n// parseTimeExpression parses absolute or relative time expressions\nfunc parseTimeExpression(expr string) (time.Time, error) {\n    // TODO: Implement time expression parsing\n    // Check if expr starts with \"now()\" for relative time\n    // Otherwise parse as RFC3339 or Unix timestamp\n    // For relative: parse \"now() - 1h30m\"\n    return time.Time{}, nil\n}\n```\n\n**Write Handler (`internal/api/write_handler.go`):**\n\n```go\npackage api\n\nimport (\n    \"compress/gzip\"\n    \"context\"\n    \"io\"\n    \"net/http\"\n    \"time\"\n    \n    \"tempo/internal/models\"\n    \"tempo/pkg/lineprotocol\"\n    \"go.uber.org/zap\"\n)\n\nfunc (s *Server) handleWrite(w http.ResponseWriter, r *http.Request) {\n    ctx, cancel := context.WithTimeout(r.Context(), 10*time.Second)\n    defer cancel()\n    \n    // TODO 1: Check Content-Encoding header for gzip\n    //   - If \"gzip\", wrap r.Body with gzip.Reader\n    //   - Defer close of gzip reader\n    \n    // TODO 2: Create line protocol parser for request body\n    //   - Use lineprotocol.NewParser()\n    //   - Handle large bodies by streaming (parse in chunks)\n    \n    // TODO 3: Parse each point\n    //   - Call parser.Next() until returns nil\n    //   - For each point, convert to models.DataPoint and models.SeriesKey\n    //   - Validate required fields (measurement, at least one field)\n    //   - Collect points in a batch slice\n    \n    // TODO 4: Send batch to storage engine\n    //   - Call s.storage.WritePointsBatch(ctx, points)\n    //   - Handle errors: 400 for invalid data, 429 for backpressure\n    //   - Log errors with s.logger\n    \n    // TODO 5: Return appropriate HTTP response\n    //   - 204 No Content on success\n    //   - 400 with error details for client errors\n    //   - 429 with Retry-After header for backpressure\n    //   - 500 for internal errors (don't leak details)\n    \n    // Example success response:\n    w.WriteHeader(http.StatusNoContent)\n}\n\n// parseLineProtocolPoint converts a lineprotocol.Point to internal models\nfunc parseLineProtocolPoint(lpPoint *lineprotocol.Point) (models.SeriesKey, models.DataPoint, error) {\n    // TODO: Implement conversion\n    //   - Create SeriesKey from Measurement and Tags\n    //   - Extract first numeric field (for now, support single field)\n    //   - Create DataPoint with Timestamp and Value\n    //   - Return error if no numeric fields found\n    return models.SeriesKey{}, models.DataPoint{}, nil\n}\n```\n\n**Query Handler (`internal/api/query_handler.go`):**\n\n```go\npackage api\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"net/http\"\n    \"time\"\n    \n    \"tempo/internal/query/parser\"\n    \"go.uber.org/zap\"\n)\n\ntype QueryRequest struct {\n    Query string `json:\"query\"`\n}\n\ntype QueryResponse struct {\n    Results []Result `json:\"results\"`\n}\n\ntype Result struct {\n    Series []Series `json:\"series,omitempty\"`\n    Error  string   `json:\"error,omitempty\"`\n}\n\ntype Series struct {\n    Name    string            `json:\"name\"`\n    Columns []string          `json:\"columns\"`\n    Values  [][]interface{}   `json:\"values\"`\n    Tags    map[string]string `json:\"tags,omitempty\"`\n}\n\nfunc (s *Server) handleQuery(w http.ResponseWriter, r *http.Request) {\n    ctx, cancel := context.WithTimeout(r.Context(), 30*time.Second)\n    defer cancel()\n    \n    var queryStr string\n    \n    // TODO 1: Determine request method and extract query\n    //   - For GET: read from \"q\" query parameter\n    //   - For POST: parse JSON body with QueryRequest struct\n    //   - Return 400 if query is empty\n    \n    // TODO 2: Parse query string\n    //   - Create parser.NewParser(queryStr)\n    //   - Call ParseQuery() to get models.Query\n    //   - Handle parse errors with 400 response\n    \n    // TODO 3: Validate query\n    //   - Check time range is not too large (configurable limit)\n    //   - Ensure measurement exists (optional check)\n    //   - Return 400 for invalid queries\n    \n    // TODO 4: Execute query\n    //   - Call s.queryEngine.ExecuteQuery(ctx, queryStr)\n    //   - Or call s.storage.Query(ctx, parsedQuery) directly\n    //   - Handle context cancellation/timeout\n    \n    // TODO 5: Format results\n    //   - Convert internal QueryResult to API Response format\n    //   - Handle different result types: raw points, aggregates, grouped\n    //   - For GROUP BY time(), format buckets correctly\n    \n    // TODO 6: Write JSON response\n    //   - Set Content-Type: application/json\n    //   - Use json.NewEncoder(w).Encode(response)\n    //   - Handle streaming for large result sets (optional enhancement)\n    \n    // Example error response:\n    // w.WriteHeader(http.StatusBadRequest)\n    // json.NewEncoder(w).Encode(QueryResponse{\n    //     Results: []Result{{Error: err.Error()}},\n    // })\n}\n```\n\n**Prometheus Write Handler (`internal/api/prometheus_handler.go`):**\n\n```go\npackage api\n\nimport (\n    \"context\"\n    \"io\"\n    \"net/http\"\n    \n    \"github.com/gogo/protobuf/proto\"\n    \"github.com/golang/snappy\"\n    \"github.com/prometheus/prometheus/prompb\"\n    \"go.uber.org/zap\"\n)\n\nfunc (s *Server) handlePrometheusWrite(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Read and decompress request body\n    //   - Use snappy.Decode (Prometheus uses snappy framing format)\n    //   - Handle possible decompression errors\n    \n    // TODO 2: Unmarshal protobuf\n    //   - Use proto.Unmarshal to get prompb.WriteRequest\n    //   - Validate required fields\n    \n    // TODO 3: Convert Prometheus samples to TempoDB points\n    //   - For each TimeSeries in request:\n    //     - Metric name becomes measurement\n    //     - Labels become tags\n    //     - Each sample becomes a DataPoint\n    //   - Handle different value types (float, histogram, summary - start with float)\n    \n    // TODO 4: Write to storage engine\n    //   - Batch points for efficiency\n    //   - Use s.storage.WritePointsBatch()\n    \n    // TODO 5: Return appropriate Prometheus response\n    //   - 204 on success\n    //   - 400/500 with error in Prometheus format\n}\n\nfunc (s *Server) handlePrometheusRead(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Read and decompress request (similar to write)\n    \n    // TODO 2: Unmarshal prompb.ReadRequest\n    \n    // TODO 3: Convert Prometheus matchers to TempoDB query\n    //   - Handle equality, regex, and other matcher types\n    //   - Build models.Query from matchers and time range\n    \n    // TODO 4: Execute query and get results\n    \n    // TODO 5: Convert results to prompb.ReadResponse\n    //   - Map measurements/tags back to Prometheus metric name/labels\n    //   - Format points as prompb.TimeSeries\n    \n    // TODO 6: Marshal, compress, and send response\n    //   - Use proto.Marshal and snappy encode\n    //   - Set appropriate Content-Type\n}\n```\n\n#### Language-Specific Hints\n\n1. **HTTP Server Performance**: Use `http.TimeoutHandler` to wrap your router for automatic timeout handling. Set `ReadHeaderTimeout` on your `http.Server` to prevent slowloris attacks.\n\n2. **JSON Marshaling**: For better performance with large result sets, consider using `json.Encoder` directly with `SetEscapeHTML(false)` to avoid unnecessary escaping. For streaming responses, implement `http.Flusher` support.\n\n3. **Context Propagation**: Always pass the request context (`r.Context()`) to downstream operations. This allows proper cancellation when clients disconnect. Use `context.WithTimeout` for operations that should have time limits.\n\n4. **Line Protocol Parsing Optimizations**:\n   - Reuse byte buffers with `sync.Pool` to reduce allocations\n   - Use `bytes.IndexByte` instead of `strings.Split` for better performance\n   - Pre-allocate slices with estimated capacity to avoid reallocations\n\n5. **Query Parser Tips**: Implement the scanner as a state machine reading runes. Use a simple lookup table for keywords. For duration parsing, handle common suffixes: `s`, `m`, `h`, `d`, `w` (week = 7d = 168h).\n\n6. **Prometheus Protocol Buffers**: Use the `gogo/protobuf` version that Prometheus uses for compatibility. Generate Go code with `protoc --gogofast_out=. *.proto`. The `prompb` package from Prometheus can be imported directly if you vendor it.\n\n7. **Graceful Shutdown**: Implement signal handling in `main.go` to catch `SIGTERM` and `SIGINT`, then call `server.Shutdown(ctx)` with a timeout context to allow in-flight requests to complete.\n\n#### Milestone Checkpoint\n\nAfter implementing the Query Language and API components, verify functionality with these tests:\n\n**Command to Test Write API:**\n```bash\n# Start the server\ngo run cmd/tempo-server/main.go\n\n# In another terminal, send test data\ncurl -X POST http://localhost:8080/api/v1/write \\\n  -H \"Content-Type: text/plain\" \\\n  --data-raw 'cpu,host=server01,region=us-west usage=42.5 1696501200000000000\ncpu,host=server01,region=us-west usage=43.1 1696501260000000000\nmemory,host=server01 value=2048 1696501200000000000'\n```\n\n**Expected Output:** HTTP 204 No Content response. Check server logs to confirm points were received and written.\n\n**Command to Test Query API:**\n```bash\ncurl -X POST http://localhost:8080/api/v1/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"SELECT avg(usage) FROM cpu WHERE time > now() - 1h GROUP BY time(5m)\"}'\n```\n\n**Expected Output:** JSON response with aggregated results:\n```json\n{\n  \"results\": [{\n    \"series\": [{\n      \"name\": \"cpu\",\n      \"columns\": [\"time\", \"avg\"],\n      \"values\": [[\"2023-10-05T14:00:00Z\", 42.8]],\n      \"tags\": {\"host\": \"server01\", \"region\": \"us-west\"}\n    }]\n  }]\n}\n```\n\n**Command to Test Prometheus Remote Write Compatibility:**\n```bash\n# Use a Prometheus test client or create a simple Go program that sends\n# Prometheus remote write protocol format\n```\n\n**Signs of Problems and Diagnostics:**\n- **\"No data returned\" for valid queries**: Check that the query parser correctly extracts time ranges and tag filters. Add debug logging to see the parsed `models.Query` structure.\n- **High memory usage during writes**: The line protocol parser may be holding onto large buffers. Ensure you're not accumulating the entire request body in memory.\n- **Slow query responses**: Check if the query is doing full scans instead of using time-range predicate pushdown. Verify TSM file indexes are being used correctly.\n- **Prometheus writes failing**: Verify snappy decompression is correct. Prometheus uses the \"snappy framed\" format, not raw snappy.\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Query returns HTTP 400 with \"parse error\" | Malformed query syntax | Check parser error messages, test with simple query first | Add more descriptive error messages, validate grammar |\n| Write API accepts data but nothing appears in queries | Points not being flushed to TSM files | Check memtable flush logs, verify WAL is being replayed on startup | Ensure flush threshold is being reached or implement manual flush endpoint |\n| Prometheus remote write succeeds but data appears with wrong measurement | Incorrect metric name to measurement mapping | Log the conversion from Prometheus TimeSeries to internal points | Check that __name__ label becomes measurement, other labels become tags |\n| GROUP BY time() returns misaligned buckets | Window alignment uses wrong epoch | Test with fixed timestamps, check alignToWindow function | Ensure alignment uses a fixed epoch (e.g., Unix epoch) not query start time |\n| Large queries timeout or crash server | No query limits or memory bounds | Monitor memory usage during query, add query duration logging | Implement query timeout, limit max points returned, use streaming results |\n| Grafana shows \"Data source connected but no data\" | Grafana query format mismatch | Check Grafana debug panel, log the incoming request from Grafana | Ensure /api/v1/grafana/query returns correct JSON structure with time column first |\n\n---\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** Milestone 2: Write Path, Milestone 3: Query Engine, Milestone 5: Query Language & API\n\nThis section details the operational flow of TempoDB's two most critical user journeys: writing a data point and executing a query. Understanding these sequences illuminates how the previously described components—API, WAL, memtable, storage engine, and query engine—coordinate to fulfill the system's promises of durability and performance. We trace each request from the client's network call through the system's internal machinery to the final response.\n\n### Write Path Sequence\n\n**Mental Model: The Airport Check-in and Baggage System**\nImagine an airport's check-in process for luggage. You (the client) present your bag (data point) at the counter (HTTP API). The agent immediately prints a baggage receipt (WAL entry) and gives you a copy (acknowledgment), guaranteeing the airline now has a record of your bag. Your bag is then placed on a short conveyor belt (memtable) behind the counter, where it waits with other luggage. When the belt fills up, a worker loads all the bags from that belt onto a cart and takes them to the plane's cargo hold (TSM file). This separation—immediate receipt for durability and batched loading for efficiency—is the core of TempoDB's write path.\n\nThe write path is designed for high throughput and strong durability. A write is considered successful once it is durable in the Write-Ahead Log (WAL), allowing subsequent processing (memtable insertion and eventual flush) to happen asynchronously. The following sequence details each step.\n\n1.  **Client Request:** An external client sends an HTTP POST request to the `/write` API endpoint. The request body contains one or more data points in InfluxDB line protocol format (e.g., `cpu,host=server01 value=0.64 1434055562000000000`).\n\n2.  **API Layer Reception & Parsing:** The `Server`'s `handleWrite` method receives the request. It parses the line protocol using `parseLineProtocolPoint`, which validates the syntax and converts it into the internal `Point` model. This model is then transformed into the canonical `SeriesKey` (from measurement and tags) and `DataPoint` (timestamp and value) structures.\n\n3.  **Durability Guarantee - WAL Append:** Before any in-memory update, the system ensures durability. The `StorageEngine` calls `WriteEntry` on the current active WAL `Segment`. This method synchronously appends a binary representation of the `SeriesKey` and `DataPoint` to the segment file. Depending on the `SegmentConfig.SyncOnWrite` setting, it may call `fsync` to force the data to disk. A successful append guarantees the write can be recovered after a crash. The WAL returns a unique sequence ID for the entry.\n\n4.  **Write Acknowledgment:** Upon successful WAL append, the `StorageEngine` can immediately acknowledge the write to the client via the HTTP API layer. The HTTP server sends a `204 No Content` response. This acknowledgment is the point of durability; the data is now safe and will eventually be queryable.\n\n5.  **In-Memory Buffering - Memtable Insertion:** Concurrently or immediately after the WAL append, the `DataPoint` is inserted into the current active `Memtable`. The `Memtable.Insert` method locates the slice for the corresponding `SeriesKey` (creating it if new) and inserts the `DataPoint` while maintaining sorted order by timestamp. The memtable's approximate size counter is incremented.\n\n6.  **Memtable Capacity Check & Rotation:** The `StorageEngine` continuously monitors the size of the active memtable via its `Size()` method. When it exceeds `Config.MaxMemtableSize`, the engine performs a rotation:\n    *   The current active memtable is marked as **immutable**.\n    *   A new, empty memtable is created and designated as the active one for new writes.\n    *   The immutable memtable is placed on a `flushCh` channel for background processing.\n\n7.  **Background Flush to TSM:** A dedicated flush goroutine listens on `flushCh`. When it receives an immutable memtable, it calls `FlushMemtable`.\n    *   The memtable's `Flush()` method is called, returning a map of `SeriesKey` to sorted `[]DataPoint`.\n    *   For each series, points are grouped into blocks of up to `DefaultMaxPointsPerBlock`.\n    *   A `TSMWriter` is created for a new TSM file. For each block, `compressBlock` applies delta-of-delta encoding to timestamps and Gorilla XOR compression to values. The writer's `WriteSeries` method writes the compressed blocks and builds an in-memory index.\n    *   Finally, `TSMWriter.Finish()` writes the index and footer to the file and closes it.\n    *   The new TSM file is registered with the `StorageEngine` by adding a `TSMFileRef` to the `tsmFiles` list and updating the `seriesIndex` metadata.\n\n8.  **WAL Cleanup:** Once the immutable memtable has been successfully persisted to a TSM file, the corresponding segment of the WAL that contains its entries is no longer needed for recovery. The system can safely delete or archive that WAL segment (`Segment.Delete()`), reclaiming disk space.\n\n![Sequence Diagram: Write Path](./diagrams/write-path-sequence.svg)\n\n**Data Flow Table: Write Path**\n| Step | Component | Input | Output | Key Action |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | HTTP Client | Line Protocol String | HTTP Request | Initiates write |\n| 2 | `Server.handleWrite` | HTTP Request | `SeriesKey`, `DataPoint` | Parses and validates |\n| 3 | `Segment.WriteEntry` | `SeriesKey`, `DataPoint` | WAL Entry ID | Appends to durable log, may `fsync` |\n| 4 | `Server` | - | HTTP 204 Response | Acknowledges write to client |\n| 5 | `Memtable.Insert` | `SeriesKey`, `DataPoint` | - | Buffers point in sorted in-memory structure |\n| 6 | `StorageEngine` | Memtable Size | Immutable Memtable | Checks threshold, rotates memtable |\n| 7 | `TSMWriter` | Map of `SeriesKey` to `[]DataPoint` | `.tsm` File | Compresses data, writes blocks and index |\n| 8 | `StorageEngine` | `.tsm` File | Updated `tsmFiles`, `seriesIndex` | Registers new file for queries |\n\n**Common Pitfalls in the Write Sequence**\n\n⚡ **Pitfall: Acknowledging before durable WAL persistence.**\n*   **Description:** Sending the success response to the client after memtable insertion but before the WAL is synced to disk.\n*   **Why it's wrong:** A server crash after acknowledgment but before the WAL persist would lose the \"committed\" data, violating durability.\n*   **Fix:** Ensure the response is sent **only after** `WriteEntry` (and its potential `fsync`) returns successfully.\n\n⚡ **Pitfall: Blocking writes during memtable flush.**\n*   **Description:** The system stops accepting new writes while an immutable memtable is being flushed to disk.\n*   **Why it's wrong:** This creates write stalls and hurts throughput, especially during heavy write loads.\n*   **Fix:** Use the memtable rotation pattern. Writes continue uninterrupted into the new active memtable while the flush proceeds in the background.\n\n⚡ **Pitfall: Not handling WAL segment rotation.**\n*   **Description:** Letting a single WAL segment file grow indefinitely.\n*   **Why it's wrong:** Extremely large files are difficult to manage, slow to read during recovery, and risk losing more data if corrupted.\n*   **Fix:** Implement segment rotation based on size (`SegmentConfig.MaxSizeBytes`). Close the current segment and start a new one when the limit is reached.\n\n### Query Path Sequence\n\n**Mental Model: The Library Research Assistant**\nImagine you ask a research assistant for all books about \"quantum physics\" published in the 1990s. The assistant doesn't grab every book in the library. First, they consult the card catalog (series index) to find the call numbers for relevant sections. Then, they go to the stacks and pull only the books from those sections published between 1990-1999 (time-range pushdown). They skim each book, photocopying only the relevant chapters (block pruning). Finally, if you asked for a summary, they would compile notes from all the photocopies (aggregation) before handing you the final report. This is the query engine's job: minimize the amount of data physically read and processed.\n\nThe query path prioritizes reading efficiency by pushing filters down to the storage layer and streaming results. It transforms a declarative query (what the user wants) into a series of efficient low-level operations.\n\n1.  **Client Request:** A client sends an HTTP GET request to the `/query` endpoint. The request includes a `q` parameter with a query string (e.g., `SELECT mean(value) FROM cpu WHERE host='server01' AND time >= now() - 1h GROUP BY time(5m)`).\n\n2.  **Query Parsing:** The `Server`'s `handleQuery` method extracts the query string and passes it to `NewParser(input).ParseQuery()`. The parser, typically a recursive descent parser, tokenizes the input and constructs an abstract syntax tree (AST), finally returning a structured `Query` object containing `Measurement`, `Tags`, `TimeRange`, `AggregateFunction`, and `GroupByWindow`.\n\n3.  **Query Planning:** The `Query` object is passed to the `Planner.Plan()` method. The planner's job is to convert the logical query into an executable `QueryPlan`. It performs the following key optimizations:\n    *   **Series Identification:** It consults the `StorageEngine`'s `seriesIndex` to resolve the `Measurement` and `Tags` predicates into a concrete list of `SeriesKey`s.\n    *   **Predicate Pushdown & File Selection:** For each `SeriesKey`, it examines the list of `TSMFileRef`s. Using the `MinTime` and `MaxTime` in each file's index, it prunes files that do not `Overlap` with the query's `TimeRange`. This yields a `QueryPlan` containing a map from `SeriesKey` to a list of relevant `BlockRef`s (file path and index entry).\n    *   **Aggregation Strategy:** If the query includes a `GroupByWindow`, the planner wraps the scan iterator with a `WindowAggregator`.\n\n4.  **Plan Execution & Streaming:** The `QueryPlan` is executed, typically via an iterator model. An execution goroutine is spawned for the query.\n    *   For each `SeriesKey` in the plan, a `SeriesScanner` is created. The scanner is initialized with the list of `BlockRef`s.\n    *   The `SeriesScanner.Next()` method drives the scan. It opens the first TSM file (via `OpenTSMReader`), seeks to the block offset from the `IndexEntry`, and reads the compressed block. It then calls `decompressTimestamps` and `decompressValues` to materialize the `[]DataPoint` for that block.\n    *   **Key Optimization:** The scanner only reads blocks whose `[MinTime, MaxTime]` range intersects the query `TimeRange`. Points outside the range are filtered out before being yielded to the iterator pipeline.\n    *   Points are emitted from the scanner in sorted order (by timestamp).\n\n5.  **Aggregation (if applicable):** If the plan includes a `WindowAggregator`, it consumes points from the `SeriesScanner`. The aggregator's `Next()` method buffers points until it has collected all points belonging to the current tumbling window (e.g., a 5-minute bucket). When the window is complete, it applies the `AggregateFunction` (e.g., `mean`) to the buffered points, produces a single output `DataPoint` with the window's start time as its timestamp, and yields it.\n\n6.  **Result Streaming to Client:** As result `DataPoint`s are produced by the execution engine, they are serialized (e.g., to JSON or Prometheus format) and streamed directly to the HTTP response body. This avoids buffering the entire result set in memory, supporting queries over large time ranges.\n\n7.  **Resource Cleanup:** Once the `SeriesScanner` and any aggregators have exhausted their input (i.e., `Next()` returns `false`), all opened file descriptors (`TSMReader`) are closed, and the query context is finalized.\n\n![Sequence Diagram: Query Path](./diagrams/query-path-sequence.svg)\n\n**Data Flow Table: Query Path**\n| Step | Component | Input | Output | Key Action |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | HTTP Client | Query String | HTTP Request | Initiates query |\n| 2 | `Parser.ParseQuery` | Query String | `Query` object | Validates syntax, builds AST |\n| 3 | `Planner.Plan` | `Query` object | `QueryPlan` | Resolves series, prunes files/blocks, plans aggregation |\n| 4 | `SeriesScanner.Next` | `BlockRef` | `DataPoint` | Reads & decompresses block, filters by time, yields points |\n| 5 | `WindowAggregator.Next` | Stream of `DataPoint` | Aggregated `DataPoint` | Buffers points per window, computes aggregate |\n| 6 | `Server.handleQuery` | Stream of `DataPoint` | HTTP Chunked Response | Serializes and streams results |\n| 7 | `SeriesScanner.Close` | - | - | Closes TSM file readers |\n\n**Common Pitfalls in the Query Sequence**\n\n⚡ **Pitfall: Loading entire blocks into memory before filtering.**\n*   **Description:** The `SeriesScanner` decompresses a full block (e.g., 1024 points) into a slice and then iterates to filter out points outside the `TimeRange`.\n*   **Why it's wrong:** It wastes CPU and memory on points that will be discarded, especially for sparse queries targeting a small sub-range of a large block.\n*   **Fix:** Use block-level min/max timestamps for coarse pruning. For finer filtering within a block, consider techniques like seeking within compressed streams, though the simplicity of decompress-then-filter is often acceptable given block sizes are bounded.\n\n⚡ **Pitfall: Not pushing the time-range predicate down.**\n*   **Description:** The planner selects TSM files but doesn't use block-level `IndexEntry` metadata to skip irrelevant blocks within those files.\n*   **Why it's wrong:** This forces the storage engine to read and decompress every block for a series in a selected file, even if only one block contains relevant data, causing significant unnecessary I/O.\n*   **Fix:** The `QueryPlan` must include specific `BlockRef`s, not just file paths. The `SeriesScanner` must use the `IndexEntry.MinTime/MaxTime` to skip blocks entirely.\n\n⚡ **Pitfall: Buffering all results before sending the HTTP response.**\n*   **Description:** The query engine collects all result points in a slice and then serializes the entire slice to JSON at the end.\n*   **Why it's wrong:** This can exhaust memory for queries returning millions of points and increases client latency (they wait until the very end for the first byte).\n*   **Fix:** Implement streaming serialization. Write each point (or small batches) to the `http.ResponseWriter` as soon as it's produced, using HTTP chunked encoding.\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n| Component | Simple Option | Advanced Option |\n| :--- | :--- | :--- |\n| Query Result Streaming | `http.ResponseWriter` with manual chunked writes | `io.Pipe` with a dedicated goroutine for serialization |\n| Query Plan Execution | Synchronous iteration in a single goroutine | Concurrent execution per series using `sync.WaitGroup` and channels |\n| Point Serialization | JSON using `encoding/json.Encoder` | Binary format (e.g., Protobuf) or specialized efficient JSON library |\n| WAL Sync Strategy | `SyncOnWrite = true` for strongest durability | `SyncInterval` with group commit for higher throughput |\n\n#### B. Recommended File/Module Structure\nAdd query execution components to the existing structure.\n```\ntempo/\n├── internal/\n│   ├── api/\n│   │   ├── server.go              # HTTP server, handler functions\n│   │   └── middleware.go\n│   ├── storage/\n│   │   ├── engine.go              # StorageEngine with WritePoint, Query\n│   │   ├── wal/                   # WAL implementation\n│   │   ├── memtable.go\n│   │   └── tsm/                   # TSM reader/writer\n│   ├── query/\n│   │   ├── parser/                # Query language parsing\n│   │   │   ├── parser.go\n│   │   │   └── ast.go\n│   │   ├── planner.go             # Converts Query to QueryPlan\n│   │   ├── executor.go            # Coordinates execution of QueryPlan\n│   │   ├── scanner.go             # SeriesScanner implementation\n│   │   └── aggregator.go          # WindowAggregator implementation\n│   └── models/                    # Shared data types (DataPoint, SeriesKey, etc.)\n└── cmd/\n    └── tempo-server/\n        └── main.go\n```\n\n#### C. Infrastructure Starter Code\n**Complete HTTP Streaming Response Helper (to be placed in `internal/api/response.go`):**\n```go\npackage api\n\nimport (\n    \"encoding/json\"\n    \"net/http\"\n    \"time\"\n)\n\n// StreamWriter handles streaming query results to an HTTP response.\ntype StreamWriter struct {\n    w          http.ResponseWriter\n    encoder    *json.Encoder\n    flushed    bool\n}\n\n// NewStreamWriter creates a new StreamWriter for the given ResponseWriter.\n// It sets appropriate headers for streaming JSON.\nfunc NewStreamWriter(w http.ResponseWriter) *StreamWriter {\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    w.Header().Set(\"Transfer-Encoding\", \"chunked\")\n    // Write the opening bracket for a JSON array\n    w.Write([]byte(\"[\"))\n    return &StreamWriter{\n        w:       w,\n        encoder: json.NewEncoder(w),\n        flushed: false,\n    }\n}\n\n// WritePoint streams a single data point as JSON.\n// It adds commas between array elements.\nfunc (sw *StreamWriter) WritePoint(p models.DataPoint) error {\n    if sw.flushed {\n        sw.w.Write([]byte(\",\"))\n    } else {\n        sw.flushed = true\n    }\n    return sw.encoder.Encode(p)\n}\n\n// Close finalizes the JSON array and flushes any remaining data.\nfunc (sw *StreamWriter) Close() error {\n    sw.w.Write([]byte(\"]\"))\n    if f, ok := sw.w.(http.Flusher); ok {\n        f.Flush()\n    }\n    return nil\n}\n\n// Error writes an error response and closes the stream.\nfunc (sw *StreamWriter) Error(err error) {\n    // Cancel the opening bracket if no data was written\n    if !sw.flushed {\n        sw.w.Write([]byte(\"[]\"))\n    } else {\n        sw.w.Write([]byte(\"]\"))\n    }\n    http.Error(sw.w, err.Error(), http.StatusInternalServerError)\n}\n```\n\n#### D. Core Logic Skeleton Code\n**Skeleton for the `StorageEngine.Query` method orchestrating the sequence:**\n```go\n// Query executes a read query against the storage engine.\n// It orchestrates parsing, planning, execution, and streaming.\nfunc (e *StorageEngine) Query(ctx context.Context, queryStr string) (query.Result, error) {\n    // TODO 1: Parse the query string into a models.Query object.\n    // Hint: Use query.NewParser(queryStr).ParseQuery()\n\n    // TODO 2: Create a query plan from the parsed Query object.\n    // Hint: Use planner.Plan(queryObj). This step resolves series keys and selects TSM blocks.\n\n    // TODO 3: Initialize a result streaming structure (e.g., a channel of DataPoint).\n\n    // TODO 4: Launch a goroutine to execute the query plan.\n    // In the goroutine:\n    //   - Iterate over each SeriesKey in the plan.\n    //   - For each, create a SeriesScanner for its list of BlockRefs.\n    //   - Iterate with scanner.Next(), sending points to the result channel.\n    //   - If the query has aggregation, wrap the scanner with a WindowAggregator.\n    //   - Close the result channel when done.\n\n    // TODO 5: Return a structure that allows the caller to read from the result stream.\n    // This enables the HTTP handler to stream results as they are produced.\n}\n```\n\n**Skeleton for the `SeriesScanner.Next` method implementing block pruning:**\n```go\n// Next advances the scanner to the next data point.\n// It returns false when there are no more points or an error occurs.\nfunc (s *SeriesScanner) Next() bool {\n    for {\n        // TODO 1: If we have points in the current buffer (s.points) and haven't reached the end (s.pointIdx < len(s.points)), advance s.pointIdx and return true.\n\n        // TODO 2: If we have no more blocks to read for the current file, move to the next file in the plan.\n        //   - Close the current TSMReader if open.\n        //   - Open the next TSM file using storage.OpenTSMReader(path).\n        //   - Reset the block iterator for the new file.\n\n        // TODO 3: Get the next BlockRef for the current series from the iterator.\n        //   - Check if the block's [MinTime, MaxTime] overlaps the query TimeRange. If not, skip it.\n        //   - Read the compressed block using TSMReader.ReadBlock at the given offset.\n        //   - Decompress the block into []DataPoint.\n\n        // TODO 4: Filter the decompressed points: keep only those where TimeRange.Contains(point.Timestamp) is true.\n        //   - Assign the filtered slice to s.points and reset s.pointIdx to 0.\n\n        // TODO 5: If after filtering a block we have zero points, loop back to step 2 to try the next block.\n    }\n}\n```\n\n**Skeleton for the `Planner.Plan` method implementing predicate pushdown:**\n```go\n// Plan creates an execution plan from a parsed query.\nfunc (p *Planner) Plan(q *models.Query) (*QueryPlan, error) {\n    plan := &QueryPlan{\n        Query:      *q,\n        SeriesKeys: []models.SeriesKey{},\n        FileBlocks: make(map[string][]storage.BlockRef),\n    }\n\n    // TODO 1: Resolve series keys from the measurement and tag filters.\n    //   - Access the storage engine's seriesIndex.\n    //   - Find all SeriesKey where Measurement matches and Tags satisfy the filter predicates.\n    //   - Add matching keys to plan.SeriesKeys.\n\n    // TODO 2: For each resolved SeriesKey, identify relevant TSM blocks.\n    //   - Get the list of TSMFileRef from the storage engine.\n    //   - For each file, check if its global [MinTime, MaxTime] overlaps the query TimeRange.\n    //   - If yes, load the TSMIndex for that file (or use a cached summary).\n    //   - Find the IndexEntry for this SeriesKey in the file's index.\n    //   - For each IndexEntry, check if its block-level [MinTime, MaxTime] overlaps the query TimeRange.\n    //   - If yes, create a BlockRef{FilePath: file.Path, Entry: entry} and append it to plan.FileBlocks[seriesKey.String()].\n\n    // TODO 3: Sort the BlockRefs for each series by MinTime (ascending) for efficient scanning.\n\n    // TODO 4: If the query has a GroupByWindow, note that an aggregator will be needed during execution.\n\n    return plan, nil\n}\n```\n\n#### E. Language-Specific Hints\n*   **Streaming HTTP:** Use `http.Flusher` to periodically flush buffered data to the client if you write in chunks. This can reduce perceived latency.\n*   **Concurrent Scanning:** Use a `sync.WaitGroup` to wait for multiple `SeriesScanner` goroutines (one per series) to finish. Collect their results via channels and merge them, ensuring timestamp order is maintained if required.\n*   **Resource Management:** Use `defer reader.Close()` in the `SeriesScanner` to ensure TSM file descriptors are closed even if an error occurs during iteration.\n*   **Context Propagation:** Pass the `context.Context` from the HTTP request through to the query execution goroutine. This allows the query to be cancelled if the client disconnects, preventing unnecessary work.\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** This section addresses error handling and edge cases that are critical for the robustness of all five milestones, particularly Milestone 2 (Write Path), Milestone 3 (Query Engine), and Milestone 4 (Retention & Compaction).\n\nTime-series databases operate in challenging environments: they must handle high-velocity data streams, run continuously for years, and process queries over massive datasets while maintaining correctness. Unlike batch processing systems, they cannot stop for repairs or data correction. This section documents the anticipated failure modes and data anomalies that TempoDB must handle gracefully, along with the recovery strategies and design patterns that ensure system resilience.\n\n### Common Failure Modes and Recovery\n\n> **Mental Model: The Emergency Response Team**  \n> Think of error handling as an emergency response team for your database. Each component has monitoring sensors (health checks), standard operating procedures for common incidents (disk full, corrupt files), and escalation protocols for complex failures. The goal isn't to prevent all failures—which is impossible—but to detect them early, contain the damage, recover automatically when possible, and provide clear diagnostics when manual intervention is needed.\n\n#### Storage Layer Failures\n\nThe storage layer faces the most severe failure modes since it interacts directly with persistent storage, which can fail in unpredictable ways.\n\n| Failure Mode | Detection Mechanism | Recovery Strategy | Impact on Operations |\n|--------------|---------------------|-------------------|---------------------|\n| **Disk full during write** | `os.Write` returns `ErrNoSpace`; filesystem operations fail with `ENOSPC` | 1. Immediately stop accepting new writes<br>2. Mark storage engine as read-only<br>3. Notify monitoring system<br>4. Optionally trigger emergency compaction to free space | Writes blocked; reads continue; requires administrator intervention to free disk space |\n| **Corrupt WAL segment** | CRC32 checksum mismatch during `Segment.Scan()`; invalid entry format parsing fails | 1. Log corruption location and skip remaining entries in segment<br>2. Mark segment as corrupt in WAL metadata<br>3. Continue with next segment<br>4. If corruption in active segment, restart with new segment | Potential data loss for unflushed writes in corrupt segment; system continues with available data |\n| **Corrupt TSM file header** | `ReadHeader()` fails magic number validation or version check | 1. Skip file during query execution<br>2. Move file to quarantine directory<br>3. Log detailed error with file path<br>4. If file was critical for queries, return partial results with error | Queries return partial data; requires manual investigation of quarantined file |\n| **Memory-mapping failure** | `mmap` system call returns error during `OpenTSMReader()` | 1. Fall back to regular file I/O for that file<br>2. Log performance degradation warning<br>3. Continue operations with slower access path | Reduced read performance for affected file; system remains functional |\n| **Block decompression failure** | `decompressTimestamps()` or `decompressValues()` returns error; checksum mismatch | 1. Skip the corrupt block within the TSM file<br>2. Increment corruption counter for monitoring<br>3. Return available data from other blocks<br>4. Schedule file for recompaction if corruption rate exceeds threshold | Partial data loss within affected time range; queries complete with gaps |\n\n**ADR: Recovery Strategy for Corrupt TSM Files**\n\n> **Decision: Quarantine and Continue for Corrupt TSM Files**\n> - **Context**: TSM files represent immutable, compacted data blocks. A corrupt TSM file could affect query results, but stopping the database for repair is unacceptable in production scenarios.\n> - **Options Considered**:\n>   1. **Panic and shutdown**: Immediately stop the database to prevent further corruption\n>   2. **Automatic repair**: Attempt to rebuild the file from WAL and other TSM files\n>   3. **Quarantine and continue**: Skip the corrupt file and continue operations\n> - **Decision**: Implement quarantine and continue with fallback to regular I/O for memory-mapping failures\n> - **Rationale**: Time-series databases prioritize availability over perfect consistency for historical data. The quarantine approach: (1) maintains system availability, (2) provides clear audit trail of corrupt files, (3) allows administrators to investigate during maintenance windows, and (4) aligns with industry practice (InfluxDB uses similar approach)\n> - **Consequences**: Queries may return incomplete results; monitoring must alert on quarantined files; administrators must manually restore from backup if needed\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Panic and shutdown | Prevents further corruption; forces immediate repair | Causes downtime; unacceptable for production | ❌ |\n| Automatic repair | Maintains data completeness; transparent to users | Complex to implement; may fail or cause prolonged unavailability | ❌ |\n| Quarantine and continue | Maintains availability; simple to implement | Data gaps in queries; requires monitoring and manual intervention | ✅ |\n\n#### Write Path Failures\n\nThe write path must maintain durability guarantees while handling various failure scenarios.\n\n| Failure Mode | Detection Mechanism | Recovery Strategy | Impact on Operations |\n|--------------|---------------------|-------------------|---------------------|\n| **WAL write failure** | `WriteEntry()` returns I/O error; `fsync` fails | 1. Retry with exponential backoff (up to 3 attempts)<br>2. If persists, rotate to new WAL segment<br>3. If rotation fails, enter read-only mode<br>4. Log critical error with disk health metrics | Temporary write blocking during retry; potential data loss if retries fail |\n| **Memtable flush failure** | `FlushMemtable()` returns error during TSM file creation | 1. Retry flush with same data<br>2. If retry fails, write memtable contents to emergency spill file<br>3. Continue with new memtable<br>4. Schedule spill file for processing during next compaction | Increased disk usage from spill files; compaction handles recovery |\n| **Out-of-memory during ingestion** | Memory allocation fails; `Memtable.Size()` exceeds safe threshold | 1. Trigger emergency flush of current memtable<br>2. Apply backpressure to write API (HTTP 503)<br>3. Reduce batch sizes for incoming writes<br>4. Log memory pressure warnings | Temporary write throttling; potential increased latency |\n| **Clock skew in distributed writes** | Incoming timestamp significantly ahead of system clock | 1. Reject writes with timestamps > (now + `maxClockSkew`)<br>2. Log warning with client identification<br>3. Optionally buffer for `maxClockSkew` duration | Rejected writes; clients must correct their clock |\n| **Series cardinality explosion** | Monitoring detects exponential growth in `seriesIndex` size | 1. Log cardinality warning with top contributing tags<br>2. Continue processing (cardinality is operational concern)<br>3. Expose cardinality metrics via monitoring API | Increased memory usage; potential performance degradation |\n\n**Step-by-Step WAL Recovery Procedure:**\n\nWhen TempoDB starts, it must recover any unflushed data from the Write-Ahead Log:\n\n1. **Locate WAL segments**: Scan the WAL directory for files with `.wal` extension, sorted by sequence number\n2. **Identify recovery range**: Determine the first WAL segment that contains data not yet persisted to TSM files (using the WAL checkpoint file if available)\n3. **Sequential scan**: For each segment in recovery range, call `Segment.Scan()` to read all entries\n4. **Reconstruct memtables**: For each WAL entry, parse the series key and data point, then insert into an in-memory recovery memtable\n5. **Batch flush**: When recovery memtable reaches configured size threshold, flush it to TSM files\n6. **Cleanup**: Once all segments are processed, delete recovered WAL segments (or move them to archive)\n7. **Resume normal operations**: Start accepting new writes with fresh WAL segment\n\n#### Query Engine Failures\n\nQuery failures must not crash the database and should provide helpful error messages to users.\n\n| Failure Mode | Detection Mechanism | Recovery Strategy | Impact on Operations |\n|--------------|---------------------|-------------------|---------------------|\n| **Query timeout** | Context deadline exceeded during `Query()` execution | 1. Cancel query execution tree<br>2. Release all allocated resources (iterators, buffers)<br>3. Return timeout error to client<br>4. Log slow query with execution plan | Failed query; other queries continue unaffected |\n| **Memory exhaustion during aggregation** | Memory allocation fails in `WindowAggregator` | 1. Cancel query with \"out of memory\" error<br>2. Implement result streaming to limit memory usage<br>3. Add memory limits to query execution configuration | Failed query; system remains operational |\n| **Invalid query syntax** | `Parser.ParseQuery()` returns syntax error | 1. Return detailed error with line and position<br>2. Suggest corrections for common mistakes<br>3. Log malformed query attempts (rate-limited) | Query rejected; client must correct syntax |\n| **Missing measurement or field** | Storage engine returns no data for specified measurement | 1. Return empty result set (not an error)<br>2. Log warning if measurement was recently written to (potential data loss indicator) | Empty query results; system continues normally |\n| **Range query too large** | Query time range exceeds configured `maxQueryRange` | 1. Reject query with \"time range too large\" error<br>2. Suggest using downsampling or smaller ranges<br>3. Log large query attempt for monitoring | Query rejected; client must modify parameters |\n\n#### Compaction and Retention Failures\n\nBackground maintenance tasks must handle failures gracefully without data loss.\n\n| Failure Mode | Detection Mechanism | Recovery Strategy | Impact on Operations |\n|--------------|---------------------|-------------------|---------------------|\n| **Compaction interrupted by crash** | Incomplete TSM output file; missing footer | 1. On restart, detect incomplete output files (missing magic footer)<br>2. Delete incomplete output files<br>3. Retry compaction during next cycle<br>4. Preserve all input files until successful completion | Temporary storage bloat; automatic retry on next cycle |\n| **Disk full during compaction** | Write failure during `Execute()` | 1. Delete partially written output file<br>2. Abort compaction plan<br>3. Trigger emergency disk space alert<br>4. Skip further compactions until space available | Compaction paused; queries continue; requires admin intervention |\n| **TTL deletion of active file** | Race condition: file compacted after TTL marked it for deletion | 1. Use tombstone markers instead of immediate deletion<br>2. Check file reference count before physical deletion<br>3. Implement grace period for tombstoned files | No data loss; slight storage overhead during grace period |\n| **Downsampling arithmetic overflow** | Aggregate sum exceeds float64 precision during `applyDownsampling()` | 1. Use Kahan summation algorithm for improved precision<br>2. Log precision warnings for large aggregations<br>3. Return `NaN` for mathematically invalid operations | Potential precision loss in downsampled values; system continues |\n\n### Data-Specific Edge Cases\n\n> **Mental Model: The Time Traveler's Journal**  \n> Imagine time-series data as entries in a time traveler's journal. Sometimes entries arrive out of order (the traveler jumps around), sometimes there are gaps (the traveler skips days), and sometimes the dates are ambiguous (different calendar systems). The database must make sense of this chaotic journal while presenting a coherent timeline to readers.\n\n#### Out-of-Order Data Handling\n\nTime-series data frequently arrives out of chronological order due to network latency, clock synchronization issues, or batch processing delays.\n\n**Tolerance Window Strategy:**\nTempoDB implements a configurable **tolerance window** (`Memtable.maxOutOfOrderWindow`) that determines how far back in time a late-arriving point can be inserted into the current memtable. The default is typically 1-5 minutes for metrics collection and up to 1 hour for IoT scenarios.\n\n**Processing Logic for Out-of-Order Writes:**\n1. **Timestamp validation**: When `WritePoint()` receives a point, it compares the point's timestamp with the current system time (or ingestion time)\n2. **Within tolerance window**: If `point.Timestamp >= (now - maxOutOfOrderWindow)`, the point is inserted into the active memtable in correct sorted position\n3. **Beyond tolerance window but recent**: If point is older than tolerance window but newer than the oldest point in active memtable, it's still inserted with a warning log\n4. **Very old data**: If point is older than any point in current memtable but newer than the newest TSM file, a special \"late write\" handler creates a small TSM file that will be merged during next compaction\n5. **Extremely old data**: Points older than the oldest TSM file are rejected with an error (configurable behavior)\n\n**Example Walkthrough:**\nConsider a system with `maxOutOfOrderWindow = 5m`, current time = 10:00, and memtable containing points from 09:55 to 09:59:\n- Point at 09:57 (3 minutes old) → Inserted into memtable (within window)\n- Point at 09:52 (8 minutes old) → Inserted with warning (beyond window but recent)\n- Point at 09:30 (30 minutes old) → Written to late-write TSM file\n- Point at 08:00 (2 hours old) → Rejected with \"timestamp too old\" error\n\n#### Gaps in Time Series\n\nReal-world time-series data often has gaps—periods where no data was collected or transmitted.\n\n**Query Behavior with Gaps:**\n- **Raw queries**: Return only existing data points, resulting in discontinuous results\n- **Aggregation queries**: Treat gaps as missing values (not zeroes):\n  - `SUM`, `COUNT`: Only aggregate existing points\n  - `AVG`: Divide by number of existing points, not total time range\n  - Time bucket with no data: Return no point for that bucket (not a zero/null point)\n\n**Interpolation Considerations:**\nWhile some time-series databases offer interpolation (filling gaps with estimated values), TempoDB explicitly does NOT interpolate because:\n1. Interpolation assumes a data generation model that may not be valid\n2. It can mask genuine data collection failures\n3. It complicates aggregation semantics\n4. Applications can implement their own interpolation if needed\n\n#### Clock Skew and Timestamp Anomalies\n\nDistributed systems inevitably have clock differences between data producers.\n\n**Handling Strategies:**\n1. **Future timestamp rejection**: Points with timestamps > (current time + `maxClockSkew`) are rejected\n2. **Ingestion time tracking**: Optionally record ingestion timestamp alongside data timestamp for diagnostics\n3. **NTP enforcement**: Documentation strongly recommends clients use NTP synchronization\n4. **Batch timestamp normalization**: For batch imports, allow overriding timestamps with ingestion time\n\n**Clock Skew Detection Table:**\n| Scenario | Detection | Action |\n|----------|-----------|--------|\n| Client clock ahead by seconds | Timestamp > (now + 10s) | Reject with \"timestamp in future\" error |\n| Client clock ahead by minutes/hours | Timestamp > (now + 1h) | Reject with \"clock skew too large\" error |\n| Client clock slightly behind | Timestamp < now but within tolerance | Accept normally |\n| Client clock far behind | Timestamp < (oldest TSM file time) | Reject with \"timestamp too old\" error |\n\n#### Queries with Very Large Time Ranges\n\nUnbounded or extremely large time range queries can overwhelm the system.\n\n**Protection Mechanisms:**\n1. **Configuration limits**: `maxQueryRange` (default 30 days) rejects overly broad queries\n2. **Memory limits**: Query execution tracks memory usage and cancels if exceeding limit\n3. **Result streaming**: Large results stream incrementally rather than buffering entirely\n4. **Timeout enforcement**: All queries have execution timeout (default 30 seconds)\n5. **Resource prioritization**: Short recent queries prioritized over long historical scans\n\n**Progressive Optimization for Large Queries:**\nWhen a query approaches but doesn't exceed the maximum range:\n1. **Downsampling suggestion**: Query planner checks if downsampled data exists for the time range\n2. **Storage tier routing**: Direct query to appropriate storage tier (hot/warm/cold)\n3. **Parallel execution**: Split time range into chunks processed in parallel\n4. **Approximate results**: Optionally return approximate aggregates using statistical sketches\n\n#### Boundary Conditions in Time-Based Grouping\n\n`GROUP BY time()` queries have subtle edge cases at time window boundaries.\n\n**Window Alignment Rules:**\nThe `alignToWindow()` function aligns timestamps to consistent window boundaries using a fixed epoch (January 1, 1970, 00:00:00 UTC). This ensures deterministic grouping regardless of query start time.\n\n**Boundary Scenarios:**\n1. **Partial first window**: If query starts mid-window, first bucket includes only data from start time to next boundary\n2. **Partial last window**: If query ends mid-window, last bucket includes only data from last boundary to end time\n3. **Empty windows**: Windows with no data produce no output point (not a zero/null point)\n4. **Timezone considerations**: All timestamps are UTC internally; timezone conversion happens at API layer if needed\n\n**Example with Daylight Saving Time:**\nConsider `GROUP BY time(1h)` on March 10, 2024 (daylight saving transition in US/Eastern):\n- 01:30 EST becomes 03:30 EDT → Still groups into 1-hour UTC buckets consistently\n- Application layer handles local time display if needed\n\n#### Data Type and Precision Edge Cases\n\nFloating-point values present specific challenges for time-series data.\n\n| Edge Case | Problem | TempoDB Handling |\n|-----------|---------|------------------|\n| **NaN values** | Invalid floating-point operations produce NaN | Gorilla compression cannot handle NaN; reject at ingestion with error |\n| **Infinity values** | Division by zero produces ±Inf | Reject at ingestion with error |\n| **Extreme values** | Very large/small floats may lose precision | Store as-is; compression may be less effective |\n| **Precision loss in aggregation** | Repeated summation accumulates error | Use Kahan summation in `WindowAggregator` for critical metrics |\n| **Integer overflow in count** | `COUNT()` over billions of points | Use int64 counters; monitor for overflow |\n\n#### Handling Tombstoned Data\n\nWhen data is deleted or TTL-expired, it's marked with tombstones rather than immediately removed.\n\n**Tombstone Lifecycle:**\n1. **Marking**: File marked `Tombstoned=true` with `TombstoneTime` set\n2. **Grace period**: File remains available for queries for `DefaultTTLGracePeriod` (5 minutes)\n3. **Physical deletion**: After grace period, file deleted during next maintenance cycle\n4. **Recovery**: If system crashes during grace period, tombstones persist and deletion resumes on restart\n\n**Query Behavior with Tombstones:**\n- **During grace period**: Queries skip tombstoned data (invisible to users)\n- **Compaction**: Tombstoned files are excluded from compaction plans\n- **Emergency restore**: Administrator can manually remove tombstone marker before grace period expires\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Error monitoring | Log files with structured JSON | OpenTelemetry integration with metrics export |\n| Crash recovery | WAL replay on startup | Point-in-time recovery with snapshotting |\n| Disk monitoring | Periodic `statfs()` calls | Inotify/FANOTIFY for real-time space alerts |\n| Memory limits | Go's runtime memory stats | Cgroup integration for container environments |\n\n#### Error Handling Infrastructure\n\nCreate an error hierarchy and recovery utilities in `internal/errors/`:\n\n```go\n// internal/errors/errors.go\npackage errors\n\nimport (\n    \"fmt\"\n    \"runtime\"\n)\n\n// ErrorSeverity indicates how severe an error is\ntype ErrorSeverity int\n\nconst (\n    SeverityDebug ErrorSeverity = iota\n    SeverityInfo\n    SeverityWarning\n    SeverityError\n    SeverityCritical\n)\n\n// TempoError is the base error type for all TempoDB errors\ntype TempoError struct {\n    Code       string\n    Message    string\n    Severity   ErrorSeverity\n    Component  string\n    Operation  string\n    InnerError error\n    StackTrace []byte\n    Timestamp  time.Time\n}\n\nfunc NewTempoError(code, message string, severity ErrorSeverity, component, operation string, inner error) *TempoError {\n    stack := make([]byte, 1024)\n    n := runtime.Stack(stack, false)\n    return &TempoError{\n        Code:       code,\n        Message:    message,\n        Severity:   severity,\n        Component:  component,\n        Operation:  operation,\n        InnerError: inner,\n        StackTrace: stack[:n],\n        Timestamp:  time.Now().UTC(),\n    }\n}\n\nfunc (e *TempoError) Error() string {\n    return fmt.Sprintf(\"[%s] %s: %s (component: %s, operation: %s)\",\n        e.Code, e.Severity, e.Message, e.Component, e.Operation)\n}\n\n// IsDiskFullError checks if error is due to disk full\nfunc IsDiskFullError(err error) bool {\n    // Check for ENOSPC or similar disk full errors\n    return strings.Contains(err.Error(), \"no space\") ||\n           strings.Contains(err.Error(), \"ENOSPC\") ||\n           strings.Contains(err.Error(), \"disk full\")\n}\n\n// Recovery actions for different error types\ntype RecoveryAction struct {\n    Name        string\n    Description string\n    Execute     func() error\n}\n\n// GetRecoveryActions returns recommended recovery actions for an error\nfunc GetRecoveryActions(err error) []RecoveryAction {\n    var actions []RecoveryAction\n    \n    // TODO 1: Check error type and severity\n    // TODO 2: For disk full errors, suggest emergency compaction\n    // TODO 3: For corrupt file errors, suggest quarantine procedure\n    // TODO 4: For memory pressure, suggest flush and backpressure\n    // TODO 5: Return appropriate recovery actions\n    \n    return actions\n}\n```\n\n#### WAL Corruption Recovery Skeleton\n\n```go\n// internal/wal/recovery.go\npackage wal\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os\"\n    \"path/filepath\"\n    \"sort\"\n)\n\n// RecoverFromCrash recovers unflushed data from WAL after a crash\nfunc (w *WAL) RecoverFromCrash(ctx context.Context) (int64, error) {\n    var recoveredPoints int64\n    \n    // TODO 1: List all WAL segment files in directory\n    // TODO 2: Sort by sequence number (embedded in filename)\n    // TODO 3: Check for checkpoint file to know where to start recovery\n    // TODO 4: For each segment file (starting from checkpoint):\n    //   a. Open segment with Segment.Scan()\n    //   b. For each entry, parse into series key and data point\n    //   c. Insert into recovery memtable\n    //   d. When memtable reaches threshold, flush to TSM\n    //   e. Update recoveredPoints counter\n    // TODO 5: Delete or archive processed segment files\n    // TODO 6: Return count of recovered points\n    \n    return recoveredPoints, nil\n}\n\n// RepairCorruptSegment attempts to salvage data from a corrupt WAL segment\nfunc RepairCorruptSegment(segmentPath string) ([]byte, error) {\n    // TODO 1: Open file read-only\n    // TODO 2: Read file in chunks, looking for valid entry boundaries\n    // TODO 3: For each potentially valid entry, verify CRC32\n    // TODO 4: Collect valid entries into recovery buffer\n    // TODO 5: Return recovered data or error if nothing salvageable\n    \n    return nil, fmt.Errorf(\"repair not implemented\")\n}\n```\n\n#### Query Timeout and Cancellation\n\n```go\n// internal/query/executor.go\npackage query\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// ExecuteQueryWithTimeout executes a query with configurable timeout\nfunc (e *Executor) ExecuteQueryWithTimeout(ctx context.Context, plan *QueryPlan, timeout time.Duration) (QueryResult, error) {\n    // Create timeout context\n    timeoutCtx, cancel := context.WithTimeout(ctx, timeout)\n    defer cancel()\n    \n    // Execute with proper resource tracking\n    resultChan := make(chan QueryResult, 1)\n    errorChan := make(chan error, 1)\n    \n    go func() {\n        result, err := e.executePlan(timeoutCtx, plan)\n        if err != nil {\n            errorChan <- err\n        } else {\n            resultChan <- result\n        }\n    }()\n    \n    select {\n    case result := <-resultChan:\n        return result, nil\n    case err := <-errorChan:\n        return QueryResult{}, err\n    case <-timeoutCtx.Done():\n        // TODO 1: Cancel all iterators and release resources\n        // TODO 2: Log timeout with query details for monitoring\n        // TODO 3: Return timeout error to client\n        return QueryResult{}, timeoutCtx.Err()\n    }\n}\n\n// executePlan with proper resource cleanup on cancellation\nfunc (e *Executor) executePlan(ctx context.Context, plan *QueryPlan) (QueryResult, error) {\n    // TODO 1: Check context cancellation periodically\n    // TODO 2: Use defer to ensure iterators are closed even on panic\n    // TODO 3: Implement memory tracking with periodic checks\n    // TODO 4: Stream results rather than buffer entirely\n    \n    // Example iterator loop with cancellation check:\n    for scanner.Next() {\n        select {\n        case <-ctx.Done():\n            scanner.Close()\n            return QueryResult{}, ctx.Err()\n        default:\n            // Process point\n        }\n    }\n    \n    return QueryResult{}, nil\n}\n```\n\n#### Disk Space Monitoring and Emergency Actions\n\n```go\n// internal/storage/disk_monitor.go\npackage storage\n\nimport (\n    \"syscall\"\n    \"time\"\n)\n\n// DiskMonitor periodically checks disk space and takes emergency actions\ntype DiskMonitor struct {\n    dataDir      string\n    threshold    float64 // e.g., 0.90 for 90% full\n    checkInterval time.Duration\n    stopCh       chan struct{}\n}\n\n// Start begins periodic disk monitoring\nfunc (dm *DiskMonitor) Start() {\n    go dm.monitorLoop()\n}\n\n// monitorLoop checks disk space and triggers emergency actions\nfunc (dm *DiskMonitor) monitorLoop() {\n    ticker := time.NewTicker(dm.checkInterval)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case <-ticker.C:\n            dm.checkDiskSpace()\n        case <-dm.stopCh:\n            return\n        }\n    }\n}\n\nfunc (dm *DiskMonitor) checkDiskSpace() {\n    // TODO 1: Get disk usage statistics using syscall.Statfs\n    // TODO 2: Calculate used percentage\n    // TODO 3: If above warning threshold, log warning\n    // TODO 4: If above critical threshold:\n    //   a. Trigger emergency compaction\n    //   b. Enter read-only mode if space critically low\n    //   c. Alert monitoring system\n    // TODO 5: If below threshold after being above, resume normal operations\n}\n\n// emergencyCompaction triggers aggressive compaction to free space\nfunc (dm *DiskMonitor) emergencyCompaction() error {\n    // TODO 1: Get list of TSM files sorted by size\n    // TODO 2: Select small files for aggressive compaction\n    // TODO 3: Run compaction with highest priority\n    // TODO 4: Delete source files immediately after successful compaction\n    // TODO 5: Return freed space amount\n    \n    return nil\n}\n```\n\n#### Language-Specific Hints\n\n1. **Go Error Wrapping**: Use `fmt.Errorf(\"...: %w\", err)` to wrap errors with context, enabling `errors.Is()` and `errors.As()` checks later.\n\n2. **Resource Cleanup**: Always use `defer` for resource cleanup (file closing, mutex unlocking, iterator closing) to prevent leaks during panics.\n\n3. **Context Propagation**: Pass `context.Context` through all long-running operations and check `ctx.Done()` in loops.\n\n4. **Memory Monitoring**: Use `runtime.ReadMemStats()` to track memory usage and trigger backpressure before OOM kills the process.\n\n5. **Graceful Shutdown**: Implement signal handling for SIGTERM/SIGINT to flush memtables and close files properly.\n\n6. **Recover from Panics**: Use `recover()` in goroutine entry points to log panics and restart the goroutine if appropriate.\n\n#### Milestone Checkpoint: Error Handling Validation\n\nAfter implementing error handling, verify with these tests:\n\n```bash\n# Test 1: Disk full simulation\n$ dd if=/dev/zero of=/tmp/tempodb_test/fill.disk bs=1M count=1000\n$ go test ./internal/storage/ -run TestDiskFullRecovery -v\n\n# Test 2: Corrupt WAL recovery\n$ cp testdata/corrupt.wal /tmp/tempodb_test/wal/segment-0001.wal\n$ ./tempodb-server --data-dir /tmp/tempodb_test\n# Should log recovery attempt and continue startup\n\n# Test 3: Query timeout\n$ curl \"http://localhost:8080/query?q=SELECT+*+FROM+cpu+WHERE+time+>+now()-365d\" &\n# After 30 seconds (default timeout), should receive timeout response\n\n# Test 4: Out-of-order write handling\n$ ./test_out_of_order.py --tolerance-window=5m\n# Should accept points within window, warn on older points, reject very old points\n```\n\n#### Debugging Tips for Common Error Scenarios\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Writes hang indefinitely** | Disk full; WAL cannot `fsync` | Check disk space; look for \"no space\" in logs | Free disk space; emergency compaction |\n| **Query returns partial data** | Corrupt TSM block skipped | Check logs for \"corrupt block\" or \"CRC mismatch\" | Restore from backup; check hardware |\n| **High memory usage** | Cardinality explosion; large queries | Check series index size; monitor query memory | Limit series cardinality; add query memory limits |\n| **Clock skew warnings** | Client clocks not synchronized | Check client NTP status; compare timestamps | Enforce NTP on clients; adjust `maxClockSkew` |\n| **Compaction not running** | Disk near full; previous failure | Check compaction logs; disk space | Address disk space issue; manual compaction trigger |\n\n\n## Testing Strategy\n\n> **Milestone(s):** This section provides the verification framework for all five milestones, ensuring each component meets its acceptance criteria through systematic testing approaches and concrete milestone checkpoints.\n\nThe testing strategy for TempoDB must validate both functional correctness and performance characteristics across all system components. Time-series databases present unique testing challenges due to their focus on temporal data patterns, compression algorithms, and high-volume ingestion. This section outlines a comprehensive approach combining traditional testing methods with specialized techniques for time-series systems, providing clear checkpoints for each milestone.\n\n### Mental Model: The Scientific Laboratory\n\nThink of testing TempoDB like running a scientific laboratory. Each component represents an experimental apparatus that must be validated individually (unit tests) and in concert with others (integration tests). We create controlled experiments (property-based tests) to verify fundamental laws of data preservation (no data loss, correct aggregation). Performance benchmarks are our stress tests, measuring how the apparatus behaves under extreme conditions. The milestone checkpoints are our peer reviews—concrete demonstrations that each apparatus performs its intended function before moving to the next phase of the larger experiment.\n\n### Testing Approaches and Property Verification\n\nEffective testing for a time-series database requires a layered approach that addresses specific characteristics of temporal data, compression, and high-throughput systems. The following table outlines the primary testing strategies:\n\n| Testing Approach | When to Apply | Key Techniques | TempoDB-Specific Considerations |\n|-----------------|---------------|----------------|---------------------------------|\n| **Unit Testing** | During component development | • Mock dependencies<br>• Table-driven tests<br>• Edge case exploration | Test compression/decompression round-trips, time range calculations, and series key hashing |\n| **Integration Testing** | After component interfaces stabilize | • Component composition<br>• End-to-end workflows<br>• Database lifecycle tests | Test WAL recovery after crashes, query execution across multiple TSM files, compaction workflows |\n| **Property-Based Testing** | For algorithmic components | • Hypothesis testing<br>• Random input generation<br>• Invariant verification | Verify compression never loses data, aggregations are associative, time windows align correctly |\n| **Golden File Testing** | For file format stability | • Versioned reference files<br>• Binary comparison<br>• Forward/backward compatibility | Ensure TSM file format stability, WAL segment structure consistency |\n| **Fuzz Testing** | For robustness validation | • Random mutation of inputs<br>• Protocol fuzzing<br>• Memory corruption simulation | Fuzz query parser with malformed queries, ingest API with corrupt line protocol |\n| **Performance Benchmarking** | For acceptance criteria | • Throughput measurement<br>• Latency percentiles<br>• Memory/CPU profiling | Verify thousands of points per second write throughput, sub-second query latency for common ranges |\n| **Concurrency Testing** | For thread-safe components | • Race condition detection<br>• Deadlock detection<br>• Stress testing with goroutines | Test concurrent writes to same series, simultaneous queries during compaction |\n| **Recovery Testing** | For durability guarantees | • Simulated crashes<br>• Disk full scenarios<br>• Corrupt file handling | Verify WAL recovery replays all acknowledged writes, TSM file corruption detection |\n\n#### Property Verification for Critical Components\n\nCertain TempoDB components require specialized verification approaches to ensure they maintain essential invariants:\n\n**Compression Algorithms Invariants:**\n- **Losslessness**: For any sequence of timestamps `T` and values `V`, `decompress(compress(T, V)) == (T, V)`\n- **Monotonicity Preservation**: If timestamps are strictly increasing, decompressed timestamps maintain this order\n- **Value Fidelity**: Floating-point values maintain their exact binary representation after compression/decompression cycles\n\n**Time Window Alignment Properties:**\n- **Deterministic Bucketing**: For any timestamp `t` and window duration `d`, `alignToWindow(t, d)` always returns the same result\n- **Contiguity**: Sequential windows have no gaps: `alignToWindow(t, d) + d == alignToWindow(t + d, d)`\n- **Idempotence**: Applying alignment twice yields same result: `alignToWindow(alignToWindow(t, d), d) == alignToWindow(t, d)`\n\n**Aggregation Function Properties:**\n- **Associativity**: For commutative aggregations (sum, count), order of application doesn't matter\n- **Identity Elements**: Empty window aggregations yield appropriate zero values (0 for sum, NaN for avg, etc.)\n- **Monotonicity**: For ordered aggregations (min, max), results respect the partial order of input values\n\n> **Key Insight:** Property-based testing is particularly valuable for time-series databases because it can automatically generate edge cases like clock jumps, out-of-order points, and NaN values that developers might not think to test manually.\n\n#### Golden File Testing for Storage Format\n\nThe TSM file format represents a contract between different versions of TempoDB. Golden file testing ensures this contract remains stable:\n\n1. **Reference File Creation**: Generate TSM files with known content using a specific version\n2. **Binary Comparison**: Compare new files byte-for-byte with reference files\n3. **Round-Trip Verification**: Write → Read → Verify data matches original\n4. **Version Compatibility**: Ensure new readers can read old files and vice versa\n\nThe following table defines the golden file test matrix:\n\n| Test Scenario | Input Data | Expected TSM File Characteristics | Validation Method |\n|---------------|------------|-----------------------------------|-------------------|\n| Empty series | No data points | Minimal file with only header and empty index | File size matches expected, header valid |\n| Single block | 1000 sequential points | One data block, index with single entry | Block count = 1, all points recoverable |\n| Multiple series | 5 series × 500 points each | Index with 5 entries, interleaved blocks | Each series retrievable independently |\n| Max block size | Points exceeding `DefaultMaxPointsPerBlock` | Multiple blocks per series | Block count = ceil(points/maxPoints) |\n| Mixed timestamps | Random, non-sequential timestamps | Out-of-order points sorted in block | Retrieved points in timestamp order |\n| Special values | NaN, ±Inf, extremely large/small floats | Gorilla compression handles special cases | Values preserved exactly after round-trip |\n\n#### Fuzz Testing for Query Language\n\nThe query parser and executor must handle malformed input gracefully. Fuzz testing generates random valid and invalid queries to uncover crashes or incorrect behavior:\n\n**Query Fuzzing Strategy:**\n1. **Grammar-Based Generation**: Create queries that follow the grammar but with random components\n2. **Mutation Testing**: Take valid queries and mutate parts (change operators, remove tokens)\n3. **Protocol Fuzzing**: Test HTTP API with malformed JSON, incorrect headers, large payloads\n\n**Critical Invariants for Fuzzed Queries:**\n- No panic or crash on any input (malformed or valid)\n- Malformed queries return descriptive error messages, not empty results\n- Memory bounds respected even for pathological queries\n- Query timeout enforcement prevents runaway execution\n\n### Common Pitfalls in Time-Series Database Testing\n\n⚠️ **Pitfall: Testing Only Sequential Timestamps**\nTesting with perfectly sequential timestamps misses edge cases like clock jumps, duplicate timestamps, and out-of-order data. Real-world time-series data includes irregularities that the system must handle gracefully.\n\n**Why it's wrong**: Systems that pass tests with perfect data may fail catastrophically with real-world data containing clock adjustments, batch backfills, or multi-source ingestion.\n\n**How to fix**: Include test cases with:\n- Timestamps that jump forward/backward (simulating NTP adjustments)\n- Duplicate timestamps (same nanosecond from different sources)\n- Gaps in data (missing periods)\n- Out-of-order points within the tolerance window\n\n⚠️ **Pitfall: Ignoring Floating-Point Edge Cases**\nTesting with \"nice\" floating-point values (1.0, 2.5, etc.) misses issues with special IEEE 754 values and precision boundaries.\n\n**Why it's wrong**: Gorilla compression must handle NaN, ±Inf, denormalized numbers, and values near floating-point precision boundaries correctly.\n\n**How to fix**: Create comprehensive floating-point test vectors:\n```plaintext\nNaN, -NaN, +Inf, -Inf, 0.0, -0.0\nDenormalized: 1e-310, -1e-310\nBoundary values: max float64, -max float64, min positive normal\nPrecision transition: 1.0 + 2^-52, 1.0 + 2^-53\n```\n\n⚠️ **Pitfall: Testing Compression Ratio Instead of Correctness**\nFocusing on achieving specific compression ratios rather than verifying lossless compression.\n\n**Why it's wrong**: Compression algorithms should never lose data, even if compression ratio suffers. Testing for ratios can mask data corruption bugs.\n\n**How to fix**: Always test round-trip correctness first. Compression ratio tests should be separate benchmarks, not correctness tests.\n\n⚠️ **Pitfall: Single-Threaded Performance Testing**\nMeasuring performance with single-threaded writes/queries that don't reflect real concurrent load.\n\n**Why it's wrong**: Real systems experience concurrent writes from multiple sources and parallel queries. Single-threaded tests miss lock contention, cache coherency overhead, and memory barrier costs.\n\n**How to fix**: Design performance tests that simulate:\n- Multiple concurrent writers (different goroutines writing to different series)\n- Mixed read/write workloads (queries while ingesting)\n- Concurrent compactions during queries\n\n#### ADR: Choosing Test Frameworks and Approaches\n\n> **Decision: Comprehensive Testing Pyramid with Go-Native Tooling**\n> - **Context**: TempoDB needs robust testing across multiple dimensions (correctness, performance, concurrency) while maintaining fast feedback cycles for developers. The Go ecosystem provides excellent testing tools that integrate seamlessly.\n> - **Options Considered**:\n>   1. **Minimalist approach**: Standard Go testing package only\n>   2. **Comprehensive external frameworks**: Bring in multiple specialized testing libraries (testify, ginkgo, goconvey)\n>   3. **Hybrid native approach**: Go testing package augmented with carefully selected single-purpose libraries for property testing and HTTP testing\n> - **Decision**: Hybrid native approach using `testing` package + `quick` for property testing + `httptest` for API tests\n> - **Rationale**: The standard `testing` package is sufficient for most needs and keeps dependencies minimal. `quick` provides property testing without external dependencies. `httptest` is part of the standard library. This approach minimizes cognitive overhead and build complexity while providing necessary testing capabilities.\n> - **Consequences**: Developers need to write more boilerplate for assertions but gain deeper understanding of test failures. The test suite remains fast and integrated with standard Go tooling.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Minimalist (testing only) | • Zero dependencies<br>• Consistent with Go idioms<br>• Fast compilation | • Verbose assertion code<br>• No property testing<br>• Limited HTTP test utilities | ❌ |\n| Comprehensive frameworks | • Rich assertion libraries<br>• BDD-style syntax available<br>• Integrated utilities | • Multiple dependencies<br>• Steeper learning curve<br>• Potential version conflicts | ❌ |\n| Hybrid native approach | • Minimal dependencies<br>• Property testing support<br>• Standard HTTP testing<br>• Go-native patterns | • More boilerplate for assertions<br>• No BDD syntax | ✅ |\n\n### Milestone Verification Checkpoints\n\nEach milestone includes specific acceptance criteria that must be verified through executable tests and manual validation. These checkpoints provide concrete steps to confirm successful implementation.\n\n#### Milestone 1: Storage Engine Verification\n\n**Objective**: Validate TSM file format, compression algorithms, and block-based storage.\n\n**Verification Checklist:**\n- [ ] TSM files can be written and read correctly\n- [ ] Delta-of-delta compression reduces storage for sequential timestamps\n- [ ] Gorilla compression maintains floating-point precision\n- [ ] Block index correctly maps time ranges to file offsets\n- [ ] Memory-mapped files enable efficient random access\n\n**Validation Commands and Expected Output:**\n\n```bash\n# Run storage engine unit tests\n$ go test ./internal/storage/... -v -run TestTSM\n=== RUN   TestTSMWriteReadRoundTrip\n--- PASS: TestTSMWriteReadRoundTrip (0.08s)\n=== RUN   TestDeltaOfDeltaCompression\n--- PASS: TestDeltaOfDeltaCompression (0.12s)\n=== RUN   TestGorillaCompression\n--- PASS: TestGorillaCompression (0.15s)\n=== RUN   TestMemoryMappedAccess\n--- PASS: TestMemoryMappedAccess (0.05s)\nPASS\n\n# Generate and inspect a TSM file\n$ go run cmd/inspect/main.go --file testdata/sample.tsm\nTSM File: testdata/sample.tsm\nVersion: 1\nSeries Count: 3\nTotal Blocks: 5\nFile Size: 45.2 KB\nCompression Ratio: 4.7:1\n\nSeries: cpu,host=server1\n  Blocks: 2 (times 2023-01-01T00:00:00Z to 2023-01-01T01:40:00Z)\n  Points: 1000\n  Avg block size: 8.1 KB\n\n# Benchmark compression performance\n$ go test ./internal/storage/... -bench=BenchmarkCompression -benchtime=10s\nBenchmarkDeltaOfDeltaCompress-8   \t  500000\t     24560 ns/op\t 407.03 MB/s\nBenchmarkGorillaCompress-8        \t  300000\t     38920 ns/op\t 256.95 MB/s\n```\n\n**Manual Validation Steps:**\n1. Create a test program that writes 10,000 sequential points to a TSM file\n2. Verify file size is significantly smaller than uncompressed data (expect 4-8x compression)\n3. Use a hex editor to examine file structure: header, data blocks, index, footer\n4. Write a simple reader that memory-maps the file and extracts points from middle blocks without reading entire file\n5. Confirm that reading random blocks has constant time complexity regardless of file position\n\n#### Milestone 2: Write Path Verification\n\n**Objective**: Validate high-throughput ingestion, WAL durability, and memtable management.\n\n**Verification Checklist:**\n- [ ] Write-ahead log ensures durability before acknowledgment\n- [ ] Memtable buffers writes and flushes at size threshold\n- [ ] Batch ingestion improves throughput\n- [ ] Out-of-order writes are handled correctly within tolerance window\n- [ ] Backpressure mechanism prevents unbounded memory growth\n\n**Validation Commands and Expected Output:**\n\n```bash\n# Test WAL recovery after simulated crash\n$ go test ./internal/wal/... -v -run TestWALRecovery\n=== RUN   TestWALRecovery\nWriting 1000 points to WAL...\nSimulating crash (kill -9)...\nRecovering from WAL...\nRecovered 1000 points, 0 lost\n--- PASS: TestWALRecovery (0.22s)\n\n# Benchmark write throughput\n$ go test ./internal/ingest/... -bench=BenchmarkWriteThroughput -benchtime=30s\nBenchmarkWriteThroughput-8   \t  200000\t    178200 ns/op   5609 ops/sec\nBenchmarkBatchWrite-8        \t  500000\t     54200 ns/op  18450 ops/sec\n\n# Test backpressure mechanism\n$ go run cmd/loadtest/main.go --points 1000000 --workers 10 --backpressure\nStarting load test with 10 workers...\n[WARNING] Backpressure engaged at 85% capacity\nThroughput limited to maintain stability\nTotal points: 1,000,000\nSuccessful: 1,000,000\nFailed (backpressure): 0\nAverage latency: 4.2ms\nP99 latency: 18.7ms\n\n# Verify out-of-order handling\n$ go test ./internal/ingest/... -v -run TestOutOfOrder\n=== RUN   TestOutOfOrderWithinWindow\n--- PASS: TestOutOfOrderWithinWindow (0.03s)\n=== RUN   TestOutOfOrderBeyondWindow\n--- PASS: TestOutOfOrderBeyondWindow (0.04s)\n```\n\n**Manual Validation Steps:**\n1. Start TempoDB server with small memtable size limit (e.g., 1MB)\n2. Send writes at increasing rates until memtable fills\n3. Observe flush to disk and new memtable creation without data loss\n4. Kill server process abruptly during active writes\n5. Restart server and verify all acknowledged writes are recoverable from WAL\n6. Send points with timestamps in random order, verify they're stored in correct temporal order\n\n#### Milestone 3: Query Engine Verification\n\n**Objective**: Validate range queries, aggregations, and filtering performance.\n\n**Verification Checklist:**\n- [ ] Time-range queries return correct points within bounds\n- [ ] Tag filtering uses inverted index efficiently\n- [ ] Aggregation functions produce mathematically correct results\n- [ ] GROUP BY time() creates correct window buckets\n- [ ] Predicate pushdown skips irrelevant blocks\n\n**Validation Commands and Expected Output:**\n\n```bash\n# Run query engine tests\n$ go test ./internal/query/... -v -run \"TestQuery\"\n=== RUN   TestTimeRangeQuery\n--- PASS: TestTimeRangeQuery (0.07s)\n=== RUN   TestTagFiltering\n--- PASS: TestTagFiltering (0.05s)\n=== RUN   TestAggregationFunctions\n--- PASS: TestAggregationFunctions (0.09s)\n=== RUN   TestGroupByTime\n--- PASS: TestGroupByTime (0.12s)\n\n# Test predicate pushdown optimization\n$ go test ./internal/query/... -v -run TestPredicatePushdown\n=== RUN   TestPredicatePushdown\nQuery scanned 5 of 20 blocks (75% reduction)\n--- PASS: TestPredicatePushdown (0.08s)\n\n# Benchmark query performance\n$ go test ./internal/query/... -bench=BenchmarkQuery -benchtime=10s\nBenchmarkRangeQuery-8           \t  100000\t    156800 ns/op\nBenchmarkAggregationQuery-8     \t   50000\t    312400 ns/op\nBenchmarkGroupByQuery-8         \t   30000\t    521800 ns/op\n\n# Execute complex query manually\n$ curl -X POST \"http://localhost:8086/query\" \\\n  -d \"SELECT mean(value) FROM cpu \n       WHERE host='server1' AND time >= '2023-01-01T00:00:00Z' \n       GROUP BY time(1h) \n       LIMIT 24\"\n{\n  \"results\": [\n    {\n      \"series\": [\n        {\n          \"name\": \"cpu\",\n          \"columns\": [\"time\", \"mean\"],\n          \"values\": [\n            [\"2023-01-01T00:00:00Z\", 45.2],\n            [\"2023-01-01T01:00:00Z\", 46.8],\n            ...\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Manual Validation Steps:**\n1. Load dataset with known statistical properties (e.g., 1M points with normal distribution)\n2. Execute aggregation queries and verify results match pre-computed values\n3. Create query that spans multiple TSM files, verify all relevant files are accessed\n4. Test time windows that don't align with block boundaries, verify correct point inclusion\n5. Execute query with very selective tag filter, confirm only matching series are scanned\n6. Test queries with no matching data, verify empty result (not error)\n\n#### Milestone 4: Retention & Compaction Verification\n\n**Objective**: Validate automatic data lifecycle management and storage optimization.\n\n**Verification Checklist:**\n- [ ] TTL enforcement deletes expired data blocks\n- [ ] Compaction merges small files without data loss\n- [ ] Downsampling reduces granularity while preserving trends\n- [ ] Background processes don't block foreground operations\n\n**Validation Commands and Expected Output:**\n\n```bash\n# Test TTL enforcement\n$ go test ./internal/retention/... -v -run TestTTL\n=== RUN   TestTTLEnforcement\nCreating data with 1-minute TTL...\nWaiting 2 minutes for expiration...\nChecking data...\nExpired data removed: 1000 points\nActive data retained: 500 points\n--- PASS: TestTTLEnforcement (62.1s)\n\n# Monitor compaction progress\n$ go run cmd/compaction-monitor/main.go --watch\nWatching compaction directory...\n[14:30:00] Level 0: 4 files (45.2MB) → Level 1: 1 file (42.8MB)\n[14:45:00] Level 1: 3 files (128.4MB) → Level 2: 1 file (124.1MB)\n[15:00:00] Downsampling applied: 1s → 60s granularity\nStorage reduction: 68%\n\n# Verify compaction correctness\n$ go test ./internal/compaction/... -v -run TestCompactionCorrectness\n=== RUN   TestCompactionCorrectness\nInput files: 5, points: 50,000\nOutput file: 1, points: 50,000\nAll points verified, checksums match\n--- PASS: TestCompactionCorrectness (1.24s)\n\n# Test concurrent access during compaction\n$ go test ./internal/compaction/... -v -run TestConcurrentAccess\n=== RUN   TestConcurrentAccess\nWrites during compaction: 10,000\nQueries during compaction: 500\nAll operations completed successfully\nNo data corruption detected\n--- PASS: TestConcurrentAccess (3.85s)\n```\n\n**Manual Validation Steps:**\n1. Write data with mixed ages (some expired, some current)\n2. Trigger TTL enforcement and verify only expired data removed\n3. Create many small TSM files, trigger compaction, verify single larger file created\n4. While compaction runs, execute concurrent writes and queries, verify no blocking\n5. Generate high-resolution data, enable downsampling, verify low-resolution version created\n6. Query downsampled data, verify it approximates original within expected error bounds\n7. Fill disk to near capacity, verify emergency compaction triggers and frees space\n\n#### Milestone 5: Query Language & API Verification\n\n**Objective**: Validate expressive query interface and API compatibility.\n\n**Verification Checklist:**\n- [ ] Query language parses SELECT with time-range predicates\n- [ ] HTTP API accepts writes in line protocol format\n- [ ] Aggregation functions work within query expressions\n- [ ] Prometheus remote read/write endpoints function correctly\n- [ ] Grafana data source compatibility confirmed\n\n**Validation Commands and Expected Output:**\n\n```bash\n# Test query language parsing\n$ go test ./internal/querylang/... -v -run TestParser\n=== RUN   TestParserValidQueries\n--- PASS: TestParserValidQueries (0.04s)\n=== RUN   TestParserErrorRecovery\n--- PASS: TestParserErrorRecovery (0.03s)\n\n# Test HTTP API endpoints\n$ go test ./internal/api/... -v -run TestAPI\n=== RUN   TestWriteEndpoint\n--- PASS: TestWriteEndpoint (0.06s)\n=== RUN   TestQueryEndpoint\n--- PASS: TestQueryEndpoint (0.08s)\n=== RUN   TestPrometheusRemoteWrite\n--- PASS: TestPrometheusRemoteWrite (0.07s)\n\n# Verify line protocol parsing\n$ echo 'cpu,host=server1 value=42.5 1672531200000000000' | \\\n  curl -X POST http://localhost:8086/write --data-binary @-\nHTTP/1.1 204 No Content\n\n# Test Prometheus compatibility\n$ go run cmd/prometheus-test/main.go --remote-write --remote-read\nTesting Prometheus remote write...\n1000 samples written successfully\nTesting Prometheus remote read...\n1000 samples read successfully\nCompatibility: PASS\n\n# Test Grafana data source\n$ go run cmd/grafana-test/main.go --datasource-test\nConnecting to Grafana...\nTesting simple query: PASS\nTesting template variables: PASS\nTesting annotations: PASS\nData source compatible: YES\n\n# Load test with realistic queries\n$ go run cmd/query-loadtest/main.go --queries 10000 --concurrent 50\nExecuting 10,000 queries with 50 concurrent clients...\nSuccessful: 9,998 (99.98%)\nFailed: 2 (0.02%)\nAverage latency: 47ms\nP95 latency: 132ms\nThroughput: 1063 queries/second\n```\n\n**Manual Validation Steps:**\n1. Write points using line protocol via curl, verify success response\n2. Execute complex query with nested functions, verify correct parsing and execution\n3. Configure Prometheus to use TempoDB as remote storage, verify data flows correctly\n4. Set up Grafana with TempoDB data source, create dashboard with multiple panels\n5. Test malformed queries return helpful error messages, not server errors\n6. Verify API responds with appropriate CORS headers for web UI compatibility\n7. Test streaming responses for large result sets, verify memory doesn't grow linearly\n\n### Implementation Guidance\n\nWhile testing strategy is primarily about design and approach, implementing effective tests requires careful structure and utilities. This guidance provides foundational testing infrastructure.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Unit Testing | Go `testing` package | Testify assertion library |\n| Property Testing | Go `testing/quick` | Gopter property-based testing |\n| HTTP Testing | `net/http/httptest` | GoMock for HTTP client mocking |\n| Benchmarking | Go `testing` benchmarks | Custom benchmarking framework |\n| Concurrency Testing | Go race detector | ThreadSanitizer integration |\n| Code Coverage | `go test -cover` | SonarQube integration |\n\n#### B. Recommended Test File Structure\n\n```\ntempo/\n├── cmd/\n│   ├── test-tools/              # Specialized testing utilities\n│   │   ├── tsm-validator/       # Validates TSM file integrity\n│   │   ├── query-fuzzer/        # Generates random queries\n│   │   └── load-generator/      # Generates synthetic load\n│   └── ...\n├── internal/\n│   ├── storage/\n│   │   ├── tsm_test.go          # Unit tests for TSM format\n│   │   ├── compression_test.go  # Property tests for compression\n│   │   └── benchmark_test.go    # Performance benchmarks\n│   ├── ingest/\n│   │   ├── wal_test.go          # WAL recovery tests\n│   │   └── memtable_test.go     # Concurrency tests\n│   ├── query/\n│   │   ├── engine_test.go       # Query execution tests\n│   │   ├── planner_test.go      # Query planning tests\n│   │   └── iterator_test.go     # Iterator pattern tests\n│   ├── retention/\n│   │   ├── ttl_test.go          # TTL enforcement tests\n│   │   └── compaction_test.go   # Compaction correctness tests\n│   ├── api/\n│   │   ├── http_test.go         # HTTP API tests\n│   │   └── protocol_test.go     # Line protocol parsing tests\n│   └── querylang/\n│       ├── parser_test.go       # Parser fuzz tests\n│       └── ast_test.go          # AST validation tests\n└── testdata/                    # Golden files and test datasets\n    ├── golden/\n    │   ├── tsm-v1/              # Reference TSM files\n    │   └── wal-v1/              # Reference WAL segments\n    └── synthetic/\n        ├── normal-dist/         # Normally distributed test data\n        └── random-walk/         # Random walk test data\n```\n\n#### C. Infrastructure Starter Code\n\n**Golden File Test Helper** (complete, ready to use):\n\n```go\n// testutil/golden.go\npackage testutil\n\nimport (\n    \"crypto/sha256\"\n    \"encoding/hex\"\n    \"fmt\"\n    \"io\"\n    \"os\"\n    \"path/filepath\"\n    \"testing\"\n)\n\n// GoldenFile represents a reference file for comparison testing\ntype GoldenFile struct {\n    Name     string\n    Path     string\n    Checksum string\n}\n\n// LoadGoldenFile loads a golden file from testdata directory\nfunc LoadGoldenFile(t *testing.T, version, name string) GoldenFile {\n    t.Helper()\n    \n    path := filepath.Join(\"testdata\", \"golden\", version, name)\n    data, err := os.ReadFile(path)\n    if err != nil {\n        t.Fatalf(\"Failed to load golden file %s: %v\", name, err)\n    }\n    \n    hash := sha256.Sum256(data)\n    return GoldenFile{\n        Name:     name,\n        Path:     path,\n        Checksum: hex.EncodeToString(hash[:]),\n    }\n}\n\n// CompareToGolden compares current data to golden file\nfunc CompareToGolden(t *testing.T, golden GoldenFile, current []byte) bool {\n    t.Helper()\n    \n    // Calculate checksum of current data\n    currentHash := sha256.Sum256(current)\n    currentChecksum := hex.EncodeToString(currentHash[:])\n    \n    if currentChecksum != golden.Checksum {\n        // Write diff for debugging\n        diffPath := filepath.Join(t.TempDir(), golden.Name+\".diff\")\n        if err := os.WriteFile(diffPath, current, 0644); err == nil {\n            t.Logf(\"Current output saved to %s for comparison\", diffPath)\n        }\n        \n        t.Errorf(\"Output differs from golden file %s\", golden.Name)\n        t.Logf(\"Expected checksum: %s\", golden.Checksum)\n        t.Logf(\"Actual checksum:   %s\", currentChecksum)\n        return false\n    }\n    \n    return true\n}\n\n// UpdateGoldenFile updates golden file with current data\nfunc UpdateGoldenFile(t *testing.T, version, name string, data []byte) {\n    t.Helper()\n    \n    if os.Getenv(\"UPDATE_GOLDEN\") == \"\" {\n        t.Skip(\"Set UPDATE_GOLDEN=1 to update golden files\")\n    }\n    \n    dir := filepath.Join(\"testdata\", \"golden\", version)\n    if err := os.MkdirAll(dir, 0755); err != nil {\n        t.Fatalf(\"Failed to create golden directory: %v\", err)\n    }\n    \n    path := filepath.Join(dir, name)\n    if err := os.WriteFile(path, data, 0644); err != nil {\n        t.Fatalf(\"Failed to write golden file: %v\", err)\n    }\n    \n    t.Logf(\"Updated golden file: %s\", path)\n}\n```\n\n**Property Test Helper for Compression** (complete, ready to use):\n\n```go\n// testutil/property.go\npackage testutil\n\nimport (\n    \"math\"\n    \"math/rand\"\n    \"testing\"\n    \"testing/quick\"\n    \"time\"\n)\n\n// TimeSeriesGenerator generates realistic time-series data for property testing\ntype TimeSeriesGenerator struct {\n    rng *rand.Rand\n}\n\nfunc NewTimeSeriesGenerator(seed int64) *TimeSeriesGenerator {\n    return &TimeSeriesGenerator{\n        rng: rand.New(rand.NewSource(seed)),\n    }\n}\n\n// GenerateSequentialTimestamps generates n sequential timestamps with possible jitter\nfunc (g *TimeSeriesGenerator) GenerateSequentialTimestamps(n int, start time.Time, interval time.Duration, jitter float64) []time.Time {\n    timestamps := make([]time.Time, n)\n    current := start\n    \n    for i := 0; i < n; i++ {\n        // Add jitter (±jitter% of interval)\n        jitterNs := int64(float64(interval.Nanoseconds()) * (g.rng.Float64()*2*jitter - jitter))\n        timestamps[i] = current.Add(time.Duration(jitterNs))\n        current = current.Add(interval)\n    }\n    \n    return timestamps\n}\n\n// GenerateRandomWalkValues generates values following a random walk\nfunc (g *TimeSeriesGenerator) GenerateRandomWalkValues(n int, start, volatility float64) []float64 {\n    values := make([]float64, n)\n    current := start\n    \n    for i := 0; i < n; i++ {\n        // Random step (±volatility)\n        step := (g.rng.Float64()*2 - 1) * volatility\n        current += step\n        values[i] = current\n    }\n    \n    return values\n}\n\n// GenerateMixedValues generates values including edge cases\nfunc (g *TimeSeriesGenerator) GenerateMixedValues(n int) []float64 {\n    values := make([]float64, n)\n    \n    for i := 0; i < n; i++ {\n        switch g.rng.Intn(20) {\n        case 0:\n            values[i] = math.NaN()\n        case 1:\n            values[i] = math.Inf(1)\n        case 2:\n            values[i] = math.Inf(-1)\n        case 3:\n            values[i] = 0.0\n        case 4:\n            values[i] = -0.0\n        default:\n            values[i] = g.rng.NormFloat64() * 100\n        }\n    }\n    \n    return values\n}\n\n// CheckProperty runs a property test with helpful error reporting\nfunc CheckProperty(t *testing.T, name string, f interface{}, config *quick.Config) bool {\n    t.Helper()\n    \n    if config == nil {\n        config = &quick.Config{\n            MaxCount: 1000,\n            Rand:     rand.New(rand.NewSource(time.Now().UnixNano())),\n        }\n    }\n    \n    err := quick.Check(f, config)\n    if err != nil {\n        t.Errorf(\"Property %s failed: %v\", name, err)\n        return false\n    }\n    \n    return true\n}\n```\n\n#### D. Core Test Skeleton Code\n\n**TSM File Format Test** (skeleton with TODOs):\n\n```go\n// internal/storage/tsm_test.go\nfunc TestTSMWriteReadRoundTrip(t *testing.T) {\n    // TODO 1: Create test directory using t.TempDir()\n    // TODO 2: Generate test data: 1000 sequential points with timestamps and values\n    // TODO 3: Create TSMWriter and write test data\n    // TODO 4: Call Finish() to complete file\n    // TODO 5: Open TSMReader for the created file\n    // TODO 6: Read back all points using ReadBlock\n    // TODO 7: Verify point count matches original\n    // TODO 8: Compare each timestamp and value for exact equality\n    // TODO 9: Test edge cases: empty series, single point, max block size\n    // TODO 10: Verify file checksum validation works\n}\n\nfunc TestDeltaOfDeltaCompression(t *testing.T) {\n    // TODO 1: Generate timestamps with different patterns:\n    //         - Perfectly sequential (delta constant)\n    //         - Increasing intervals (delta increasing)\n    //         - Random intervals within bounds\n    //         - Clock jumps (forward and backward)\n    // TODO 2: Apply delta-of-delta compression to each pattern\n    // TODO 3: Decompress and verify round-trip correctness\n    // TODO 4: Calculate compression ratio for each pattern\n    // TODO 5: Verify monotonic timestamps remain monotonic after compression\n    // TODO 6: Test with maximum timestamp values (near uint64 limit)\n}\n\nfunc TestGorillaCompressionProperty(t *testing.T) {\n    // Property: decompress(compress(values)) == values\n    \n    testutil.CheckProperty(t, \"GorillaCompressionRoundTrip\", func(values []float64) bool {\n        // TODO 1: Handle special case: empty slice\n        if len(values) == 0 {\n            return true\n        }\n        \n        // TODO 2: Compress values using Gorilla algorithm\n        // TODO 3: Decompress back to float64 slice\n        // TODO 4: Compare decompressed with original\n        // TODO 5: Special handling for NaN (math.IsNaN)\n        // TODO 6: Special handling for Inf (math.IsInf)\n        // TODO 7: Use math.Float64bits for exact binary comparison\n        // TODO 8: Return true only if all values match exactly\n        \n        return false // Replace with actual implementation\n    }, nil)\n}\n```\n\n**WAL Recovery Test** (skeleton with TODOs):\n\n```go\n// internal/wal/recovery_test.go\nfunc TestWALRecoveryAfterCrash(t *testing.T) {\n    // TODO 1: Create WAL directory using t.TempDir()\n    // TODO 2: Initialize WAL with SegmentConfig\n    // TODO 3: Write 1000 points to WAL using WriteEntry\n    // TODO 4: Simulate crash by not calling proper shutdown\n    // TODO 5: Create new WAL instance pointing to same directory\n    // TODO 6: Call recovery function to scan WAL segments\n    // TODO 7: Verify all 1000 points are recovered\n    // TODO 8: Check that recovery handles partial writes (truncated entries)\n    // TODO 9: Test recovery with multiple segment files\n    // TODO 10: Verify recovered points maintain write order\n}\n\nfunc TestConcurrentWALWrites(t *testing.T) {\n    // TODO 1: Create WAL with appropriate configuration\n    // TODO 2: Launch 10 goroutines writing concurrently\n    // TODO 3: Each goroutine writes 100 unique points\n    // TODO 4: Use sync.WaitGroup to wait for all writes\n    // TODO 5: Scan WAL to recover all points\n    // TODO 6: Verify total point count equals 1000 (10×100)\n    // TODO 7: Check for duplicate or missing points\n    // TODO 8: Verify write ordering within each goroutine is preserved\n    // TODO 9: Run with race detector enabled (go test -race)\n    // TODO 10: Test with varying levels of concurrency (1, 10, 100 goroutines)\n}\n```\n\n**Query Engine Integration Test** (skeleton with TODOs):\n\n```go\n// internal/query/integration_test.go\nfunc TestEndToEndQueryWorkflow(t *testing.T) {\n    // TODO 1: Create temporary database instance\n    // TODO 2: Write test data covering multiple time ranges and series\n    // TODO 3: Execute query with time range predicate\n    // TODO 4: Verify returned points match expected subset\n    // TODO 5: Test predicate pushdown by counting blocks accessed\n    // TODO 6: Execute aggregation query (SUM, AVG, etc.)\n    // TODO 7: Compare results with manually calculated expected values\n    // TODO 8: Test GROUP BY time() with various window sizes\n    // TODO 9: Verify window alignment correctness\n    // TODO 10: Test query cancellation mid-execution\n    // TODO 11: Test memory limits on large result sets\n    // TODO 12: Verify streaming works for large queries\n}\n```\n\n#### E. Language-Specific Hints\n\n**Go Testing Best Practices:**\n- Use `t.Helper()` in test helper functions to improve error location\n- Leverage `t.Cleanup()` for automatic resource cleanup\n- Use table-driven tests with `tt := range tests` pattern\n- For benchmarks, use `b.ResetTimer()` and `b.StopTimer()` appropriately\n- Enable race detection with `go test -race` for all concurrency tests\n\n**Time-Series Specific Testing:**\n- Use `time.Now()` sparingly in tests—prefer fixed timestamps for reproducibility\n- When testing time-based logic, mock time using interfaces for deterministic tests\n- For performance tests, warm up caches before starting measurements\n- When testing compression, include both synthetic and real-world datasets\n\n**Property Testing Tips:**\n- Start with small `MaxCount` values (100) and increase as tests stabilize\n- Use custom generators for domain-specific types (timestamps, series keys)\n- For floating-point comparisons, use `math.Float64bits` for exact equality\n- Save failing cases to files for deterministic reproduction\n\n#### F. Debugging Tips Table\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Test passes locally but fails in CI | Timezone differences or non-deterministic test | Add `log.Printf(\"Timezone: %v\", time.Local)` to test; check for `time.Now()` usage | Use fixed timestamps or mock time source |\n| Compression test fails intermittently | Floating-point NaN/Inf handling | Add debug logging showing each value before/after compression | Ensure special values handled in compression algorithm |\n| Query returns wrong aggregation results | Incorrect window alignment | Log window start/end times and which points are included | Verify `alignToWindow` implementation matches specification |\n| WAL recovery misses some writes | Partial writes not handled | Check file size vs. expected; add CRC validation | Implement WAL entry framing with length prefixes |\n| Memory leak in tests | Goroutines not cleaned up | Use `runtime.NumGoroutine()` in test cleanup | Ensure all background jobs have stop mechanisms |\n| Race condition detected | Shared mutable state | Run with `-race` flag; examine stack traces | Add appropriate synchronization or use immutable data |\n| Benchmark results inconsistent | Cache effects or GC pauses | Use `benchstat` tool across multiple runs; check for allocation hotspots | Increase benchmark count; use `testing.AllocsPerRun` |\n\n#### G. Performance Testing Framework\n\nCreate a dedicated performance testing framework that can be run independently:\n\n```go\n// cmd/benchmark/main.go (complete starter code)\npackage main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"runtime\"\n    \"time\"\n    \n    \"github.com/tempodb/internal/benchmarks\"\n)\n\nfunc main() {\n    var (\n        benchmark = flag.String(\"benchmark\", \"write\", \"Benchmark to run (write, query, compression)\")\n        duration  = flag.Duration(\"duration\", 30*time.Second, \"Benchmark duration\")\n        workers   = flag.Int(\"workers\", runtime.NumCPU(), \"Number of concurrent workers\")\n        output    = flag.String(\"output\", \"report.json\", \"Output file for results\")\n    )\n    flag.Parse()\n    \n    log.Printf(\"Starting %s benchmark with %d workers for %v\", \n        *benchmark, *workers, *duration)\n    log.Printf(\"Go version: %s\", runtime.Version())\n    log.Printf(\"CPU cores: %d\", runtime.NumCPU())\n    \n    var result benchmarks.Result\n    switch *benchmark {\n    case \"write\":\n        result = benchmarks.RunWriteBenchmark(*workers, *duration)\n    case \"query\":\n        result = benchmarks.RunQueryBenchmark(*workers, *duration)\n    case \"compression\":\n        result = benchmarks.RunCompressionBenchmark(*workers, *duration)\n    default:\n        log.Fatalf(\"Unknown benchmark: %s\", *benchmark)\n    }\n    \n    // Save results\n    if err := result.Save(*output); err != nil {\n        log.Fatalf(\"Failed to save results: %v\", err)\n    }\n    \n    // Print summary\n    fmt.Println(result.Summary())\n    \n    // Compare with baseline if available\n    if baseline, err := benchmarks.LoadBaseline(\"baseline.json\"); err == nil {\n        diff := result.Compare(baseline)\n        fmt.Println(\"\\nComparison with baseline:\")\n        fmt.Println(diff)\n        \n        // Fail if significant regression\n        if diff.HasRegression(0.10) { // 10% regression threshold\n            os.Exit(1)\n        }\n    }\n}\n```\n\nThis comprehensive testing strategy ensures TempoDB meets its functional requirements while maintaining the performance characteristics essential for a production-ready time-series database. Each milestone includes concrete verification steps that progressively build confidence in the system's correctness and robustness.\n\n\n## Debugging Guide\n\n> **Milestone(s):** All five milestones (this guide provides diagnostic techniques applicable throughout the entire TempoDB implementation journey)\n\nDebugging a time-series database involves unique challenges due to the intersection of high-throughput writes, complex compression algorithms, temporal data semantics, and persistent storage formats. This practical guide helps learners systematically diagnose and fix common implementation bugs, organized by observable symptoms and the underlying components likely responsible. Think of debugging as **digital archaeology**—you're piecing together clues from logs, file artifacts, and runtime behavior to reconstruct what happened and identify where the implementation diverged from the design.\n\n### Common Bug Symptoms and Fixes\n\nThe following table catalogs frequently encountered issues during TempoDB development, organized by the component most likely involved. Each entry includes the observable symptom, probable root cause, diagnostic steps to confirm the hypothesis, and corrective actions to resolve the issue.\n\n| Symptom | Likely Cause | Diagnostic Steps | Corrective Action |\n|---------|--------------|------------------|-------------------|\n| **Query returns no data for a valid time range** | 1. **Incorrect block index min/max timestamps** in TSM files<br>2. **Misaligned time window** in `GROUP BY time()` queries<br>3. **Missing series key** in storage engine index | 1. Use `inspect-tsm` tool to verify `MinTime`/`MaxTime` in block indexes match actual data<br>2. Check query logs to see if predicate pushdown is skipping blocks incorrectly<br>3. Verify series exists in storage engine's `seriesIndex` map | 1. Ensure `TSMWriter.Finish()` calculates and writes correct min/max per block<br>2. Debug `TimeRange.Contains()` logic for boundary conditions<br>3. Check WAL recovery properly restores series index on startup |\n| **Write throughput plateaus at low rate** | 1. **Synchronous WAL fsync** on every write<br>2. **Lock contention** in memtable insertion<br>3. **Frequent memtable flushes** due to small size threshold | 1. Monitor WAL write latency with detailed timing logs<br>2. Use Go's `pprof` mutex profile to identify contended locks<br>3. Check flush frequency and memtable size at flush time | 1. Implement group commit with periodic WAL sync instead of per-write<br>2. Switch to sharded memtables or lock-free data structures<br>3. Increase `Config.MaxMemtableSize` or implement adaptive sizing |\n| **Data points appear duplicated in query results** | 1. **WAL replay** during recovery re-applying already flushed points<br>2. **Compaction merging** incorrectly handling overlapping time ranges<br>3. **Memtable flush** not clearing flushed data properly | 1. Check WAL segment cleanup logic—are segments deleted after flush?<br>2. Inspect compaction logs for duplicate point detection<br>3. Verify `Memtable.Flush()` returns data AND clears the internal map | 1. Implement WAL segment rotation with precise tracking of flushed offsets<br>2. Add deduplication in `mergeSeriesPoints()` using point timestamps<br>3. Ensure flush creates new memtable reference instead of modifying in-place |\n| **Storage engine crashes on startup with \"invalid magic number\"** | 1. **TSM file corruption** from incomplete writes during crashes<br>2. **Version mismatch** between reader and writer<br>3. **Memory mapping failure** due to file truncation | 1. Use hex dump to inspect first 4 bytes of corrupted file<br>2. Check `TSMHeader.Version` written vs. expected<br>3. Verify file size matches expected based on index | 1. Implement write-temp-then-rename pattern for TSM file creation<br>2. Add version compatibility check in `OpenTSMReader()`<br>3. Use file locking during writes or detect incomplete files via footer |\n| **Out-of-order points are silently dropped** | 1. **Tolerance window too small** in memtable configuration<br>2. **Timestamp comparison logic** ignoring equal timestamps<br>3. **Flush logic** not checking for late arrivals during flush | 1. Log dropped points with timestamps to see pattern<br>2. Test `Memtable.Insert()` with points at same timestamp<br>3. Check if flush includes points arriving during flush operation | 1. Increase `Memtable.maxOutOfOrderWindow` based on data source characteristics<br>2. Fix comparison to handle `<=` for same timestamp updates<br>3. Implement double-buffering or point redirection during flush |\n| **Aggregation results are mathematically incorrect** | 1. **Float precision accumulation** errors in sum/average<br>2. **Window alignment** incorrectly offset<br>3. **Missing values** (NaN/Inf) not handled in aggregates | 1. Test with integer values to isolate float issues<br>2. Debug `alignToWindow()` with known timestamps<br>3. Check if aggregation functions skip invalid values | 1. Use Kahan summation for `AggregateSum` of floats<br>2. Ensure window alignment uses fixed epoch (e.g., Unix zero)<br>3. Implement `isValidFloat()` checks before aggregation |\n| **Memory usage grows unbounded during queries** | 1. **Iterator not closing** resources<br>2. **Result caching** without size limits<br>3. **Large time ranges** loading all points into memory | 1. Use Go's `runtime.ReadMemStats` to track allocation patterns<br>2. Check for missing `SeriesScanner.Close()` calls<br>3. Monitor query plan for full scans vs. predicate pushdown | 1. Implement `io.Closer` on all iterators with defer cleanup<br>2. Add LRU cache with byte-size limits for query results<br>3. Implement chunked streaming in `StreamWriter` |\n| **Compaction causes write stalls** | 1. **Synchronous I/O** during compaction blocking writes<br>2. **Too aggressive compaction** triggered by small thresholds<br>3. **Disk space exhaustion** during merge operation | 1. Monitor compaction duration and I/O patterns<br>2. Check compaction trigger conditions and frequency<br>3. Verify `DiskMonitor` is detecting low disk space | 1. Move compaction to background with rate limiting<br>2. Adjust `LevelConfig` thresholds based on write volume<br>3. Implement emergency compaction with reserved space |\n| **Prometheus remote write fails with 400 Bad Request** | 1. **Protocol buffer parsing** errors<br>2. **Timestamp conversion** issues (ms vs ns)<br>3. **Label sorting** mismatch with Prometheus conventions | 1. Log raw request bytes for debugging<br>2. Compare timestamp formats in parsed data<br>3. Check series key string generation matches Prometheus format | 1. Validate protocol buffer structure before processing<br>2. Convert Prometheus timestamps (ms) to internal format (ns)<br>3. Sort labels alphabetically in `SeriesKey.String()` |\n| **Database becomes unresponsive after running for days** | 1. **Memory leak** in goroutines or cached resources<br>2. **File descriptor exhaustion** from open TSM files<br>3. **Background job pile-up** without throttling | 1. Use `pprof` heap profile to identify leak sources<br>2. Check `lsof` output for open file counts<br>3. Monitor scheduler job completion times | 1. Add context timeouts to all background operations<br>2. Implement LRU for `TSMReader` instances with file closing<br>3. Add backpressure to job scheduling based on system load |\n\n#### Special Considerations for Time-Series Data\n\nTime-series databases introduce unique edge cases that require specific debugging approaches:\n\n**Clock Skew Between Ingest Sources**\n> When points arrive from distributed systems with unsynchronized clocks, timestamps may jump backwards beyond the tolerance window, causing points to be rejected or stored incorrectly.\n\n*Diagnosis*: Log point timestamps with source identifiers; monitor for temporal anomalies.\n*Fix*: Implement client-side timestamp validation or server-side clock correction buffers.\n\n**Gaps in Time Series**\n> Natural periods without data (e.g., sensor offline) can cause aggregation windows to produce misleading results or query planning to over-optimize.\n\n*Diagnosis*: Check aggregation results for windows with zero points vs. windows with null results.\n*Fix*: Distinguish between \"no data\" and \"zero value\" in aggregation functions; consider interpolation for specific use cases.\n\n**Extreme Cardinality Explosion**\n> A malformed tag value (like user ID in a tag) can create millions of series, overwhelming memory and storage indexes.\n\n*Diagnosis*: Monitor `seriesIndex` growth rate; alert on sudden cardinality changes.\n*Fix*: Implement series creation rate limiting; validate tag keys/values against schema.\n\n### Debugging Techniques and Tools\n\nEffective debugging requires both systematic approaches and specialized tooling. Below are techniques tailored to TempoDB's architecture, progressing from simple logging to advanced instrumentation.\n\n#### 1. Structured Logging with Component Context\n\n> **Mental Model**: Think of logging as adding **breadcrumb trails** through each component—each log entry should let you trace a point's journey from ingestion to storage to query response.\n\nImplement a structured logging system that includes:\n- **Component identifier** (`storage`, `query`, `compaction`)\n- **Operation context** (e.g., `series_key=\"cpu\", range=\"2023-10-01T00:00:00Z/2023-10-01T01:00:00Z\"`)\n- **Performance timing** for slow operations\n- **Error chains** with causalities\n\n**Example Log Output Analysis**:\n```\nlevel=DEBUG ts=2023-10-01T12:00:00Z component=storage operation=WritePoint series=cpu.usages tags=\"host:web01\" timestamp=1696161600000000000 value=42.5 duration=2ms\nlevel=INFO ts=2023-10-01T12:00:05Z component=memtable operation=Flush series_count=1524 point_count=1258476 size_mb=48 duration=450ms\nlevel=WARN ts=2023-10-01T12:00:10Z component=query operation=ExecuteRangeScan message=\"skipping block outside time range\" file=00001.tsm series=cpu.usages block_min=1696165200000000000 block_max=1696168800000000000 query_min=1696161600000000000\n```\n\n**Implementation Strategy**:\n- Use a logging interface that supports structured fields (like `zap` or `slog`)\n- Add trace IDs to correlate operations across components\n- Implement log levels that can be dynamically adjusted per component\n\n#### 2. File Inspection Utilities\n\nBuild command-line tools to examine internal file formats—these are invaluable for understanding disk state independent of the running database.\n\n**TSM File Inspector (`inspect-tsm`)**:\n```\n$ go run cmd/inspect-tsm/main.go --file /data/00001.tsm --detail\nFile: /data/00001.tsm\nMagic: 0x16D1D1A5 ✓\nVersion: 1 ✓\nSize: 45.2 MB\nSeries Count: 284\nBlocks: 1,247\n\nSeries: cpu.usages{host=web01,region=us-west}\n  Block 0: Offset=1024, Size=16.5KB, Points=1024, MinTime=2023-10-01T00:00:00Z, MaxTime=2023-10-01T00:17:04Z\n  Block 1: Offset=17984, Size=15.8KB, Points=1024, MinTime=2023-10-01T00:17:04Z, MaxTime=2023-10-01T00:34:08Z\n  ...\nCompression:\n  Timestamps: Delta-of-delta, 1024 points → 2.1KB (85% reduction)\n  Values: Gorilla XOR, 1024 points → 8.2KB (50% reduction)\n```\n\n**WAL Segment Inspector (`inspect-wal`)**:\n```\n$ go run cmd/inspect-wal/main.go --segment /wal/00000001.wal --limit 10\nSegment: /wal/00000001.wal (FirstID=1, LastID=8427)\nEntry 1: Type=WritePoint, Size=142B, CRC=0xA3F1C8D2 ✓\n  Series: temperature{sensor=thermo01}\n  Point: 2023-10-01T12:00:00Z, 22.5°C\nEntry 2: Type=WritePoint, Size=138B, CRC=0xB8A2D4E1 ✓\n  Series: temperature{sensor=thermo02}\n  Point: 2023-10-01T12:00:00Z, 23.1°C\n...\nEntry 8427: Type=SeriesCreate, Size=89B, CRC=0x9C3F2A1D ✓\n  Series: pressure{sensor=baro01, unit=hPa}\n```\n\n**Block-Level Debug Tool (`decode-block`)**:\nFor deep inspection of compression artifacts:\n```\n$ go run cmd/decode-block/main.go --file /data/00001.tsm --series \"cpu.usages\" --block 0\nBlock 0 for cpu.usages{host=web01}:\nRaw Header: MinTime=1696161600000000000, MaxTime=1696162624000000000\nDecompressed Points (first 10):\n  1696161600000000000 → 42.5\n  1696161601000000000 → 42.7  (delta: +0.2)\n  1696161602000000000 → 43.1  (delta: +0.4)\n  1696161603000000000 → 42.9  (delta: -0.2)\nCompression Artifacts:\n  Timestamp bytes: 2104 (delta-of-delta with first=0, second=1e9)\n  Value bytes: 8192 (Gorilla XOR with leading zero count=12)\n```\n\n#### 3. Performance Profiling with Go's pprof\n\nGo's built-in profiling tools are essential for identifying bottlenecks in a high-performance database.\n\n**CPU Profiling**:\n```bash\n# Add to your HTTP server\nimport _ \"net/http/pprof\"\n\n# Profile during write load\ngo tool pprof http://localhost:6060/debug/pprof/profile?seconds=30\n\n# Generate flame graph\ngo tool pprof -http=:8080 /tmp/profile.out\n```\n\nCommon patterns to look for:\n- **High `runtime.mallocgc` time** indicates excessive allocation—optimize by reusing byte slices, pooling objects\n- **Contended `sync.RWMutex` operations** show as flat lines in mutex profile—consider sharding or lock-free structures\n- **Excessive `runtime.growslice`** suggests slices growing repeatedly—pre-allocate with known capacities\n\n**Heap Memory Profiling**:\n```bash\n# Capture heap snapshot during query execution\ngo tool pprof http://localhost:6060/debug/pprof/heap\n\n# Compare two heap profiles\ngo tool pprof -base heap1.pb.gz -top heap2.pb.gz\n```\n\n**Goroutine Leak Detection**:\n```bash\n# Get goroutine dump\ncurl http://localhost:6060/debug/pprof/goroutine?debug=2 > goroutines.txt\n\n# Look for growing goroutine counts in metrics\n# Common leak sources:\n# - Unbuffered channels with blocked senders/receivers\n# - Contexts not being canceled\n# - Background jobs not completing\n```\n\n#### 4. Property-Based Testing for Compression Algorithms\n\nCompression bugs often manifest as silent data corruption—points decompress to wrong values. Property-based testing verifies invariants hold across random inputs.\n\n**Round-Trip Invariant**:\n> For any slice of timestamps and values, compress → decompress should yield identical data.\n\n**Compression Ratio Bounds**:\n> Compressed size should never exceed uncompressed size plus header overhead.\n\n**Monotonic Timestamp Preservation**:\n> If input timestamps are sorted, decompressed timestamps should maintain same order.\n\n**Implementation Approach**:\n```go\n// TestDeltaDeltaRoundTrip uses quick.Check to verify compression properties\nfunc TestDeltaDeltaRoundTrip(t *testing.T) {\n    property := func(timestamps []uint64) bool {\n        if len(timestamps) < 2 { return true }\n        compressed, err := compressTimestamps(timestamps)\n        if err != nil { return false }\n        decompressed, err := decompressTimestamps(compressed, len(timestamps))\n        if err != nil { return false }\n        return slices.Equal(timestamps, decompressed)\n    }\n    \n    config := &quick.Config{\n        MaxCount: 1000,\n        Values: func(values []reflect.Value, rand *rand.Rand) {\n            // Generate random but sorted timestamps\n        },\n    }\n    \n    if err := quick.Check(property, config); err != nil {\n        t.Errorf(\"Round-trip failed: %v\", err)\n    }\n}\n```\n\n#### 5. Golden File Testing for Format Stability\n\nGolden files capture correct output for known inputs, detecting unintended format changes across versions.\n\n**TSM Format Golden Files**:\n```\ntestdata/golden/v1/\n├── simple.tsm.golden      # Single series, 1000 points\n├── multi_series.tsm.golden # 10 series, mixed timestamps\n└── edge_cases.tsm.golden   # NaN, Inf, large timestamp jumps\n```\n\n**Usage Pattern**:\n```bash\n# Run tests normally (compares to golden files)\ngo test ./internal/storage/tsm/...\n\n# Update golden files after intentional format change\nUPDATE_GOLDEN=1 go test ./internal/storage/tsm/...\n```\n\n**Golden File Contents**:\n```\n=== FILE: simple.tsm.golden ===\nMagic: 0x16D1D1A5\nVersion: 1\nSeries: 1\nTotal Blocks: 1\nTotal Points: 1000\nChecksum: 0xA1B2C3D4\n\nSeries: test{tag=value}\n  Block 0:\n    Offset: 1024\n    Size: 16384\n    Points: 1000\n    MinTime: 1609459200000000000 (2021-01-01T00:00:00Z)\n    MaxTime: 1609459201000000000 (2021-01-01T00:16:40Z)\n    CRC: 0xE5F6A7B8\n```\n\n#### 6. Time-Travel Debugging with Record & Replay\n\nFor intermittent bugs, record operations and replay them in a controlled environment.\n\n**Operation Recorder**:\n```go\ntype OperationRecorder struct {\n    mu sync.Mutex\n    ops []LoggedOperation\n}\n\ntype LoggedOperation struct {\n    Timestamp time.Time\n    Type      string // \"WritePoint\", \"Query\", \"Compact\"\n    Args      []byte // Serialized arguments\n    Result    []byte // Serialized result/error\n}\n```\n\n**Replay Workflow**:\n1. Enable recording during production-like load\n2. Capture bug occurrence with full operation trace\n3. Replay trace in test environment with:\n   - Added debug logging\n   - Race detector enabled\n   - Stress GOMAXPROCS variations\n\n#### 7. Visualization of Internal State\n\nCreate visual representations of database state to identify patterns.\n\n**Memtable Heat Map**:\n```\nMemtable Utilization (last 5 flushes)\nFlush 5: ████████████████████████████████ 98% (1.2M points)\nFlush 4: ████████████████████▌            65% (0.8M points)\nFlush 3: ██████████████████████████       80% (1.0M points)\nFlush 2: ██████████████████████████████   92% (1.1M points)\nFlush 1: ████████████████████████████████ 99% (1.2M points)\n```\n\n**TSM File Age Distribution**:\n```\nTSM Files by Age (hours)\n0-1:   ████████████████████ 22 files\n1-2:   ████████████        14 files  \n2-4:   █████████████████   18 files\n4-8:   ████████            10 files\n8-16:  ████                5 files\n16-32: ▌                   1 file\n>32:   █████               6 files (candidates for compaction)\n```\n\n#### 8. Stress Testing with Anomaly Injection\n\nDeliberately introduce failures to verify recovery mechanisms.\n\n**Controlled Chaos Patterns**:\n1. **Random process kills** during write operations\n2. **Disk full simulation** by intercepting write calls\n3. **Clock skew injection** by mocking time functions\n4. **Network partition simulation** for API endpoints\n\n**Recovery Verification Checklist**:\n- [ ] After crash, WAL replay restores all acknowledged writes\n- [ ] Partial TSM files are detected and quarantined\n- [ ] Series index reconstructs correctly from TSM files\n- [ ] Compaction resumes from interrupted state\n- [ ] Query results remain consistent pre/post recovery\n\n### Implementation Guidance\n\n> This implementation guidance provides concrete tools and techniques for debugging TempoDB. While the main design avoids code, this section bridges to implementation with working debugging utilities.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Logging** | Go's `log/slog` with structured fields | `zap` or `zerolog` with context propagation and sampling |\n| **Profiling** | Built-in `net/http/pprof` endpoints | Custom profiling events with `expvar` and Prometheus metrics |\n| **File Inspection** | Standalone CLI tools with direct file reading | Integrated HTTP endpoints for live file inspection |\n| **Tracing** | Manual trace IDs in logs | OpenTelemetry with Jaeger or Grafana Tempo |\n| **Testing** | `testing` package with golden files | Property-based testing with `testing/quick`, fuzzing with `go test -fuzz` |\n\n#### B. Recommended File/Module Structure\n\n```\ntempo/\n├── cmd/\n│   ├── server/                 # Main database server\n│   ├── inspect-tsm/           # TSM file inspection tool\n│   ├── inspect-wal/           # WAL segment inspection tool\n│   └── decode-block/          # Block-level debug tool\n├── internal/\n│   ├── debug/                 # Debugging utilities\n│   │   ├── inspector.go       # File format inspection interfaces\n│   │   ├── recorder.go        # Operation record/replay\n│   │   └── visualizer.go      # State visualization helpers\n│   ├── storage/\n│   │   └── tsm/\n│   │       └── debug.go       # TSM-specific debugging functions\n│   └── wal/\n│       └── debug.go           # WAL-specific debugging functions\n├── testdata/\n│   ├── golden/                # Golden file directory\n│   │   ├── v1/               # Version-specific golden files\n│   │   └── current/          # Symlink to current version\n│   └── fixtures/             # Test data fixtures\n└── tools/                    # Development tools\n    ├── generate-testdata/    # Generate test datasets\n    └── profile-analyzer/     # Custom profile analysis\n```\n\n#### C. Infrastructure Starter Code: TSM File Inspector\n\n```go\n// cmd/inspect-tsm/main.go\npackage main\n\nimport (\n    \"encoding/binary\"\n    \"fmt\"\n    \"os\"\n    \"path/filepath\"\n    \n    \"tempo/internal/storage/tsm\"\n)\n\nfunc main() {\n    if len(os.Args) < 2 {\n        fmt.Println(\"Usage: inspect-tsm <tsm-file> [--detail]\")\n        os.Exit(1)\n    }\n    \n    filePath := os.Args[1]\n    detailed := len(os.Args) > 2 && os.Args[2] == \"--detail\"\n    \n    // Open TSM file using the same reader as the database\n    reader, err := tsm.OpenTSMReader(filePath)\n    if err != nil {\n        fmt.Printf(\"Failed to open TSM file: %v\\n\", err)\n        os.Exit(1)\n    }\n    defer reader.Close()\n    \n    // Read and display header\n    data, _ := os.ReadFile(filePath)\n    if len(data) < 12 {\n        fmt.Println(\"File too small to be valid TSM\")\n        return\n    }\n    \n    magic := binary.BigEndian.Uint32(data[0:4])\n    version := binary.BigEndian.Uint64(data[4:12])\n    \n    fmt.Printf(\"File: %s\\n\", filepath.Base(filePath))\n    fmt.Printf(\"Magic: 0x%X \", magic)\n    if magic == tsm.MagicNumber {\n        fmt.Printf(\"✓\\n\")\n    } else {\n        fmt.Printf(\"✗ (expected 0x%X)\\n\", tsm.MagicNumber)\n    }\n    fmt.Printf(\"Version: %d \", version)\n    if version == tsm.Version {\n        fmt.Printf(\"✓\\n\")\n    } else {\n        fmt.Printf(\"✗ (expected %d)\\n\", tsm.Version)\n    }\n    \n    fi, _ := os.Stat(filePath)\n    fmt.Printf(\"Size: %.1f MB\\n\", float64(fi.Size())/1024/1024)\n    \n    // Get index to count series and blocks\n    index := reader.Index()\n    seriesCount := len(index.Entries)\n    blockCount := 0\n    for _, entries := range index.Entries {\n        blockCount += len(entries)\n    }\n    \n    fmt.Printf(\"Series Count: %d\\n\", seriesCount)\n    fmt.Printf(\"Blocks: %d\\n\", blockCount)\n    fmt.Println()\n    \n    if detailed {\n        // Display detailed per-series information\n        for seriesKey, entries := range index.Entries {\n            fmt.Printf(\"Series: %s\\n\", seriesKey)\n            for i, entry := range entries {\n                fmt.Printf(\"  Block %d: Offset=%d, Size=%d, Points≈%d, \", \n                    i, entry.Offset, entry.Size, entry.Size/16) // Approximation\n                \n                // Read block header for exact min/max\n                if len(data) > int(entry.Offset)+16 {\n                    minTime := binary.BigEndian.Uint64(data[entry.Offset:entry.Offset+8])\n                    maxTime := binary.BigEndian.Uint64(data[entry.Offset+8:entry.Offset+16])\n                    fmt.Printf(\"MinTime=%d, MaxTime=%d\\n\", minTime, maxTime)\n                } else {\n                    fmt.Printf(\"[header out of bounds]\\n\")\n                }\n            }\n        }\n    }\n}\n```\n\n#### D. Core Logic Skeleton: Operation Recorder\n\n```go\n// internal/debug/recorder.go\npackage debug\n\nimport (\n    \"encoding/json\"\n    \"os\"\n    \"sync\"\n    \"time\"\n)\n\n// OperationRecorder captures operations for later replay debugging\ntype OperationRecorder struct {\n    mu     sync.Mutex\n    file   *os.File\n    encoder *json.Encoder\n    enabled bool\n}\n\ntype RecordedOperation struct {\n    ID        string          `json:\"id\"`\n    Timestamp time.Time       `json:\"timestamp\"`\n    Type      string          `json:\"type\"`\n    Component string          `json:\"component\"`\n    Arguments json.RawMessage `json:\"arguments\"`\n    Result    json.RawMessage `json:\"result,omitempty\"`\n    Error     string          `json:\"error,omitempty\"`\n    Duration  time.Duration   `json:\"duration_ms\"`\n}\n\n// NewOperationRecorder creates a new recorder writing to the given file\nfunc NewOperationRecorder(filePath string) (*OperationRecorder, error) {\n    // TODO 1: Open file for appending with os.OpenFile (O_CREATE|O_APPEND|O_WRONLY)\n    // TODO 2: Create json.Encoder that writes to the file\n    // TODO 3: Write array start token \"[\"\n    // TODO 4: Return recorder with file and encoder initialized\n    return nil, nil\n}\n\n// Record starts timing an operation and returns a function to complete the recording\nfunc (r *OperationRecorder) Record(opType, component string, args interface{}) func(result interface{}, err error) {\n    start := time.Now()\n    opID := generateOperationID()\n    \n    // Serialize arguments\n    var argsJSON json.RawMessage\n    if args != nil {\n        // TODO 5: Marshal args to JSON using json.Marshal\n    }\n    \n    return func(result interface{}, err error) {\n        if !r.enabled {\n            return\n        }\n        \n        r.mu.Lock()\n        defer r.mu.Unlock()\n        \n        // Create operation record\n        record := RecordedOperation{\n            ID:        opID,\n            Timestamp: start,\n            Type:      opType,\n            Component: component,\n            Arguments: argsJSON,\n            Duration:  time.Since(start),\n        }\n        \n        // TODO 6: If err != nil, set record.Error to err.Error()\n        // TODO 7: If result != nil, marshal result to JSON for record.Result\n        // TODO 8: Encode record to JSON file using r.encoder.Encode()\n        // TODO 9: Flush file to ensure write reaches disk\n    }\n}\n\n// ReplayOperations reads recorded operations and executes them through a handler\nfunc ReplayOperations(filePath string, handler func(op RecordedOperation) error) error {\n    // TODO 10: Open file for reading\n    // TODO 11: Read array start token\n    // TODO 12: Create json.Decoder and decode operations until EOF\n    // TODO 13: For each operation, call handler with the operation\n    // TODO 14: Return any error from handler or decoding\n    return nil\n}\n\n// generateOperationID creates a unique ID for each operation\nfunc generateOperationID() string {\n    // TODO 15: Implement ID generation (e.g., nanosecond timestamp + random suffix)\n    return \"\"\n}\n```\n\n#### E. Go-Specific Debugging Hints\n\n1. **Use `runtime.ReadMemStats` for Memory Insights**:\n   ```go\n   var m runtime.MemStats\n   runtime.ReadMemStats(&m)\n   fmt.Printf(\"Alloc=%v MB, TotalAlloc=%v MB, Sys=%v MB, NumGC=%v\\n\",\n       m.Alloc/1024/1024, m.TotalAlloc/1024/1024, m.Sys/1024/1024, m.NumGC)\n   ```\n\n2. **Debug Goroutine Leaks with Stack Dumps**:\n   ```go\n   go func() {\n       for range time.Tick(5 * time.Minute) {\n           buf := make([]byte, 1024*1024)\n           n := runtime.Stack(buf, true)\n           if n > 0 && strings.Count(string(buf[:n]), \"tempo/internal\") > 500 {\n               log.Warn(\"High goroutine count\", \"stacks\", string(buf[:n]))\n           }\n       }\n   }()\n   ```\n\n3. **Profile Production Safely**:\n   ```go\n   // Secure pprof endpoint with auth\n   mux := http.NewServeMux()\n   mux.HandleFunc(\"/debug/pprof/\", authMiddleware(pprof.Index))\n   ```\n\n4. **Use `sync.Once` for Expensive Debug Setup**:\n   ```go\n   var debugSetupOnce sync.Once\n   func setupDebugging() {\n       debugSetupOnce.Do(func() {\n           // Initialize debug endpoints, recorders, etc.\n       })\n   }\n   ```\n\n#### F. Milestone Debugging Checkpoints\n\n**Milestone 1 (Storage Engine)**:\n- Run: `go run cmd/inspect-tsm/main.go testdata/fixtures/simple.tsm`\n- Expected: Shows valid magic number, version, and at least one series with correct block offsets\n- Failure Sign: \"invalid magic number\" indicates `WriteHeader` or file corruption bug\n\n**Milestone 2 (Write Path)**:\n- Run: `go test ./internal/wal/... -v -run TestWALRecovery`\n- Expected: Test passes showing WAL replay recovers all points after simulated crash\n- Failure Sign: Points missing after recovery indicates WAL format or flush coordination bug\n\n**Milestone 3 (Query Engine)**:\n- Run: `go test ./internal/query/... -v -run TestRangeScanWithPredicatePushdown`\n- Expected: Query skips blocks outside time range (visible in debug logs)\n- Failure Sign: All blocks scanned indicates predicate pushdown not working\n\n**Milestone 4 (Retention & Compaction)**:\n- Run: `go test ./internal/compaction/... -v -run TestLevelCompaction`\n- Expected: Files merge correctly, new TSM file created, old files tombstoned\n- Failure Sign: Duplicate points or missing data after compaction\n\n**Milestone 5 (Query Language & API)**:\n- Run: `curl -v \"http://localhost:8080/query?q=SELECT mean(value) FROM cpu WHERE time > now() - 1h GROUP BY time(5m)\"`\n- Expected: Returns JSON with aggregated results, HTTP 200\n- Failure Sign: Parse error or empty results with valid data indicates query parsing/planning bug\n\n#### G. Debugging Tips for Specific Scenarios\n\n**Scenario: \"Database returns wrong values for historical queries\"**\n- *Diagnose*: Use `inspect-tsm` to check block min/max times match data; verify compression round-trip with `decode-block`\n- *Fix*: Ensure `compressBlock` and `decompressTimestamps/Values` use same endianness; check timestamp encoding handles large deltas\n\n**Scenario: \"Write throughput drops after several hours\"**\n- *Diagnose*: Check goroutine count (leak?), open file descriptors (exhaustion?), memory fragmentation\n- *Fix*: Implement `TSMReader` LRU cache with file closing; profile heap for allocation patterns\n\n**Scenario: \"Aggregate SUM doesn't match manual calculation\"**\n- *Diagnose*: Test with integer values first; check for NaN/Inf contamination; verify window alignment\n- *Fix*: Implement Kahan summation in `WindowAggregator`; add validation to filter invalid floats\n\n**Scenario: \"Compaction causes out of memory\"**\n- *Diagnose*: Monitor memory during compaction; check if `mergeSeriesPoints` loads all points at once\n- *Fix*: Stream merge points instead of loading all; implement memory budget for compaction operations\n\n\n## Future Extensions\n\n> **Milestone(s):** This section looks beyond the current implementation scope to explore how TempoDB could evolve with additional features and capabilities, building upon all five foundational milestones.\n\nThe current TempoDB design successfully implements a single-node, specialized time-series database with essential features for handling high-volume sequential data. However, like any production system, its capabilities can be extended to address more complex requirements and larger-scale deployments. This section explores potential enhancements that could transform TempoDB from an educational implementation into a production-ready system capable of handling enterprise workloads. These extensions represent natural evolution paths while maintaining compatibility with the core architecture established in previous sections.\n\n### Possible Enhancements\n\nThe following extensions represent meaningful next steps for TempoDB's development, ordered by their potential impact and implementation complexity. Each enhancement maintains compatibility with the existing data model and storage format, ensuring backward compatibility while expanding capabilities.\n\n#### Distributed Architecture and Horizontal Scaling\n\n**Mental Model: The Highway System Expansion**  \nImagine TempoDB as a single major highway handling all traffic. As traffic volume grows, we need to build additional lanes (sharding) and interchanges (coordination) to distribute the load. This extension transforms TempoDB from a single highway into an interconnected highway system where data traffic is intelligently routed and balanced across multiple nodes.\n\nThe current single-node design imposes natural limits on storage capacity, write throughput, and query performance. A distributed architecture would address these limitations through:\n\n**Sharding Strategy:**\n| Sharding Method | Partition Key | Pros | Cons | Best For |\n|-----------------|---------------|------|------|----------|\n| **Time-based** | Timestamp range | Simple to implement, temporal locality preserved | Hot shard problems, uneven distribution over time | Workloads with uniform time distribution |\n| **Series-based** | Series key hash | Even distribution, linear scalability with series | Cross-series queries require fan-out | High-cardinality environments |\n| **Hybrid** | Composite (time + series) | Balances both dimensions | Complex routing logic | Mixed query patterns |\n\n**Coordination and Consensus:**\nFor distributed operation, TempoDB would need a coordination layer to manage cluster membership, shard placement, and failover. The system could adopt a Raft consensus implementation for metadata management while maintaining an eventually consistent model for data placement.\n\n> **Decision: Series-based Sharding with Consistent Hashing**\n> - **Context**: Need to distribute both write load and storage across multiple nodes while maintaining query efficiency.\n> - **Options Considered**: 1) Time-range sharding, 2) Series-key hashing, 3) Composite sharding\n> - **Decision**: Implement series-key based sharding using consistent hashing.\n> - **Rationale**: Series-based sharding provides better load balancing for high-cardinality workloads and allows parallel query execution across shards. Consistent hashing minimizes data movement when nodes join or leave the cluster.\n> - **Consequences**: Requires query engine to fan-out queries to multiple shards and merge results, but enables linear write scaling with series cardinality.\n\n**Implementation Components:**\n1. **Cluster Manager**: Tracks node membership and shard assignments using Raft consensus.\n2. **Query Router**: Routes queries to appropriate shards based on series keys.\n3. **Data Replicator**: Maintains configurable replication factor for durability.\n4. **Hinted Handoff**: Handles writes during node failures with eventual consistency.\n\n**Metadata Expansion:**\nThe `SeriesMetadata` type would expand to include shard location and replication information:\n| Field | Type | Description |\n|-------|------|-------------|\n| `ShardID` | `uint32` | Identifier for the shard containing this series |\n| `ReplicaNodes` | `[]string` | List of nodes storing replicas |\n| `PrimaryNode` | `string` | Current primary node for writes |\n| `Version` | `uint64` | Version for conflict resolution in distributed writes |\n\n#### Support for Additional Data Types\n\n**Mental Model: The Multi-Format Warehouse**  \nCurrently, TempoDB operates like a warehouse that only stores boxes of a specific size and shape (float64 values). This extension adds specialized storage areas for different item types—some requiring temperature control (booleans), others needing careful stacking (integers), and some that are fragile and complex (strings). The warehouse now needs a more sophisticated inventory system to track what's stored where and how to handle each type.\n\nWhile float64 values cover many time-series use cases, real-world applications require richer data types:\n\n**Type System Expansion:**\n| Data Type | Storage Requirement | Compression Strategy | Query Implications |\n|-----------|---------------------|----------------------|-------------------|\n| **Integer** (int64) | 8 bytes raw | Delta encoding, run-length encoding | Enables bitwise operations, exact aggregations |\n| **Boolean** | 1 bit optimal | Bit packing (8 values per byte) | Enables existence queries, state tracking |\n| **String** | Variable length | Dictionary encoding, Snappy compression | Enables text search, pattern matching |\n| **Histogram** | Multiple float64 values | Specialized bucket encoding | Enables percentile calculations directly |\n| **Multi-value** | Array of floats | Column-per-value storage | Enables vector operations, multi-metric storage |\n\n**Storage Format Adaptation:**\nThe TSM format would need to extend its block structure to support type identifiers and type-specific compression:\n\n**Revised `CompressedBlock` structure:**\n| Field | Type | Description |\n|-------|------|-------------|\n| `Type` | `uint8` | Data type identifier (0=float64, 1=int64, etc.) |\n| `Timestamps` | `[]byte` | Compressed timestamps (delta-of-delta) |\n| `Values` | `[]byte` | Type-specific compressed values |\n| `TypeMetadata` | `[]byte` | Optional type-specific metadata (e.g., dictionary for strings) |\n| `Checksum` | `uint32` | CRC32 checksum for integrity |\n\n**Query Language Extensions:**\nThe query language would need type-aware functions and operations:\n```sql\n-- Type-specific aggregations\nSELECT percentile(field, 95) FROM measurements WHERE time > now() - 1h\n\n-- String operations  \nSELECT field FROM logs WHERE field LIKE '%error%'\n\n-- Multi-value operations\nSELECT vector_magnitude(vector_field) FROM sensor_data\n```\n\n**Type Conversion and Coercion:**\nA comprehensive type system requires clear rules for implicit and explicit type conversions, particularly when performing operations across different types or when querying fields that have changed type over time.\n\n#### Continuous Queries and Materialized Views\n\n**Mental Model: The Automated Factory Assembly Line**  \nImagine data flowing through TempoDB like parts on a factory conveyor belt. Currently, workers (queries) manually inspect and process these parts when requested. Continuous queries act as automated robotic arms that process parts as they arrive, creating pre-assembled components (materialized views) that are ready for immediate use when customers ask for them.\n\nContinuous queries automatically execute queries at regular intervals and store the results, providing significant performance benefits for frequently accessed aggregates:\n\n**Architecture Components:**\n1. **Continuous Query Scheduler**: Manages execution of registered continuous queries.\n2. **Incremental Computation Engine**: Efficiently updates aggregates as new data arrives.\n3. **Materialized View Storage**: Specialized storage for pre-computed results.\n4. **Query Rewriter**: Automatically redirects queries to use materialized views when possible.\n\n**Continuous Query Definition:**\n| Field | Type | Description |\n|-------|------|-------------|\n| `Name` | `string` | Unique identifier for the continuous query |\n| `SourceMeasurement` | `string` | Measurement to read from |\n| `TargetMeasurement` | `string` | Measurement to write results to |\n| `Query` | `string` | Aggregation query to execute |\n| `Interval` | `time.Duration` | Execution frequency |\n| `ResampleInterval` | `time.Duration` | Optional different interval for older data |\n| `Enabled` | `bool` | Whether the query is active |\n\n**Implementation Strategy:**\nThe system would extend the `Scheduler` component to manage continuous queries as a special type of background job. Each continuous query would:\n1. Track the last timestamp processed\n2. Execute the aggregation query over new data since last run\n3. Write results to the target measurement\n4. Update metadata about what data has been processed\n\n**Query Rewriting Logic:**\nWhen a query arrives, the query planner would:\n1. Check if any materialized view (continuous query result) can satisfy the query\n2. Determine if the materialized view has complete data for the requested time range\n3. Rewrite the query to use the materialized view if appropriate\n4. Fall back to raw data if materialized view is incomplete or unavailable\n\n#### Tiered Storage Integration\n\n**Mental Model: The Corporate Document Archive System**  \nImportant recent documents (hot data) stay in an easily accessible desk drawer (SSD). Last quarter's documents (warm data) go to a filing cabinet in the office (HDD). Documents older than a year (cold data) get sent to offsite storage (object storage) but can be retrieved when needed. This system balances accessibility with storage cost.\n\nTiered storage automatically moves data between storage classes based on age and access patterns, optimizing cost-performance tradeoffs:\n\n**Storage Tier Definitions:**\n| Tier | Storage Medium | Access Latency | Cost | Typical Data Age |\n|------|----------------|----------------|------|------------------|\n| **Hot** | Local NVMe/SSD | Microseconds | High | 0-24 hours |\n| **Warm** | Local HDD | Milliseconds | Medium | 1-30 days |\n| **Cold** | Object Storage (S3) | Seconds | Low | 30+ days |\n| **Frozen** | Glacier/Archive | Minutes-Hours | Very Low | 365+ days |\n\n**Data Movement Strategy:**\nThe system would extend the `CompactionManager` to include tier promotion/demotion logic:\n\n**Tier State Transitions:**\n| Current Tier | Condition | Next Tier | Action Required |\n|--------------|-----------|-----------|-----------------|\n| Hot | Data older than hot_retention | Warm | Move TSM files from SSD to HDD |\n| Warm | Data older than warm_retention | Cold | Upload TSM files to object storage |\n| Cold | Query accesses cold data | Warm (temporarily) | Cache retrieved data locally |\n| Any | Data reaches TTL | Deleted | Remove from all tiers |\n\n**Implementation Approach:**\n1. **Storage Abstraction Layer**: Create a unified interface for storage operations that works across local filesystem and object storage.\n2. **Tier Metadata**: Extend `TSMFileRef` to track storage tier and location.\n3. **Background Tier Manager**: Periodically scans files and moves them between tiers.\n4. **Transparent Retrieval**: Automatically fetches cold data when queried, with optional caching.\n\n**Query Performance Considerations:**\nQueries spanning multiple tiers would need special handling:\n- Hot data: Direct memory-mapped access\n- Warm data: Standard disk I/O\n- Cold data: Async retrieval with query timeout extensions\n- Mixed-tier queries: Parallel execution with tier-aware scheduling\n\n#### Advanced Compression Algorithms\n\n**Mental Model: The Specialized Packaging Department**  \nDifferent products need different packaging techniques. Delicate electronics (sensor data with regular patterns) get custom-molded foam (pattern-aware compression). Dense metal parts (integer counters) get efficient stacking (run-length encoding). Irregularly shaped items (sparse metrics) get vacuum-sealed bags (sparse matrix compression). The packaging department now has specialized tools for each product type.\n\nWhile Gorilla XOR and delta-of-delta provide excellent general-purpose compression, specialized algorithms can yield better results for specific data patterns:\n\n**Algorithm Selection Framework:**\n| Data Pattern | Recommended Algorithm | Compression Ratio | CPU Cost | Implementation Complexity |\n|--------------|----------------------|-------------------|----------|---------------------------|\n| **Monotonic counters** | Delta + Varint encoding | 10-20x | Low | Low |\n| **Sparse metrics** (mostly zeros) | Sparse bitmap encoding | 50-100x | Medium | Medium |\n| **Regular sampling** (fixed interval) | Store interval + exceptions | 100x+ | Low | Low |\n| **Highly correlated** (sensor networks) | Chimp or Sprintz | 3-5x better than Gorilla | High | High |\n| **Integer histograms** | Bit-packing + RLE | 8-12x | Medium | Medium |\n\n**Adaptive Compression:**\nThe system could analyze data patterns at the series level and select optimal compression:\n\n**Series Compression Profile:**\n| Field | Type | Description |\n|-------|------|-------------|\n| `SeriesKey` | `string` | Series identifier |\n| `PatternType` | `uint8` | Detected pattern (constant, counter, random, etc.) |\n| `OptimalAlgorithm` | `uint8` | Recommended compression algorithm |\n| `SampleEntropy` | `float64` | Shannon entropy of value samples |\n| `TimestampRegularity` | `float64` | Regularity score (0=random, 1=perfectly regular) |\n\n**Implementation Strategy:**\n1. **Pattern Detection**: Analyze first N points of a series to determine pattern\n2. **Algorithm Registry**: Pluggable compression algorithm implementations\n3. **Block-level Metadata**: Store algorithm ID in block headers\n4. **Runtime Switching**: Support different algorithms for different blocks of same series\n\n#### Real-time Streaming Analytics\n\n**Mental Model: The Live Sports Broadcast with Real-time Statistics**  \nAs the game (data stream) progresses, statisticians (streaming engine) continuously calculate player performance metrics, team statistics, and game predictions. These real-time insights appear instantly on screen (dashboard) without waiting for the game to end. The system processes data in motion rather than at rest.\n\nExtend TempoDB from a database into a real-time analytics platform:\n\n**Stream Processing Architecture:**\n1. **Stream Ingestion**: Accept data from message queues (Kafka, Pulsar) in addition to HTTP API\n2. **Windowing Engine**: Support tumbling, sliding, and session windows\n3. **State Management**: Maintain aggregation state across window boundaries\n4. **Low-latency Output**: Emit results to downstream systems or APIs\n\n**Stream Query Language Extension:**\n```sql\n-- Create a streaming query\nCREATE STREAM page_views_1m AS\nSELECT \n  COUNT(*) as view_count,\n  AVG(duration) as avg_duration,\n  WINDOW_START() as window_start,\n  WINDOW_END() as window_end\nFROM page_views\nWINDOW TUMBLING (SIZE 1 minute)\nGROUP BY user_region, page_category\nEMIT CHANGES;\n```\n\n**Integration Points:**\n- **Input Adapters**: Kafka consumer, HTTP streaming, WebSocket connections\n- **Processing Pipeline**: Chain of operations (filter → transform → aggregate)\n- **Output Adapters**: Write back to TempoDB, push to message queue, call webhook\n- **State Storage**: RockDB or similar for window state persistence\n\n#### Advanced Indexing Strategies\n\n**Mental Model: The Library's Cross-Reference System**  \nBeyond the basic card catalog (series key index), a comprehensive library has specialized indexes: subject index (tag values), author index (source identification), citation index (value correlations), and keyword index (text search). Researchers can find information through multiple access paths.\n\nExtend TempoDB's indexing beyond the basic series key → block mapping:\n\n**Additional Index Types:**\n| Index Type | Structure | Use Case | Storage Overhead |\n|------------|-----------|----------|------------------|\n| **Tag Value Inverted** | Tag value → Series keys | Fast filtering by tag values | Medium |\n| **Value Range** | Min/max values per block | Value predicate pushdown | Low |\n| **Bloom Filter** | Per-series bloom filter | Series existence checks | Very Low |\n| **Full-text** | Inverted index on string fields | Text search in log data | High |\n| **Correlation** | Series → correlated series | Related metrics discovery | Medium |\n\n**Composite Index Example - Tag Inverted Index:**\n| Field | Type | Description |\n|-------|------|-------------|\n| `TagKey` | `string` | Tag key (e.g., \"host\") |\n| `TagValue` | `string` | Tag value (e.g., \"web-01\") |\n| `SeriesKeys` | `[]string` | List of series keys with this tag |\n| `LastUpdated` | `time.Time` | When this entry was last updated |\n\n**Query Optimization Impact:**\nWith advanced indexes, the query planner can:\n1. Use tag inverted index to resolve series keys from tag predicates before scanning data\n2. Use value range indexes to skip blocks that cannot contain matching values\n3. Use bloom filters to quickly determine if a series exists in a time range\n4. Combine multiple indexes for complex filter expressions\n\n#### Machine Learning Integration\n\n**Mental Model: The Predictive Maintenance System**  \nInstead of just recording when machines break, the system learns normal operating patterns and predicts future failures. It's like having an experienced mechanic who can hear a subtle engine sound and say, \"That bearing will fail in 48 hours,\" based on patterns seen across thousands of similar machines.\n\nIntegrate ML capabilities for anomaly detection, forecasting, and pattern recognition:\n\n**Built-in ML Functions:**\n```sql\n-- Anomaly detection\nSELECT ts, value, ANOMALY_SCORE(value) OVER (ORDER BY ts) as score\nFROM metrics \nWHERE time > now() - 24h\n\n-- Forecasting\nSELECT FORECAST(value, 10, '1h') as predictions\nFROM metrics\nWHERE time > now() - 7d\nGROUP BY time(1h)\n\n-- Pattern similarity\nSELECT series_a, series_b, DTW_DISTANCE(series_a, series_b) as similarity\nFROM metrics\nWHERE time > now() - 1h\n```\n\n**Implementation Approaches:**\n1. **Embedded Models**: Lightweight models (exponential smoothing, simple statistical tests) implemented natively\n2. **External Integration**: Call out to ML services (TensorFlow Serving, ONNX Runtime) for complex models\n3. **Model Management**: Store, version, and serve ML models alongside time-series data\n4. **Feature Engineering**: Built-in functions for creating ML features from time-series\n\n**ML Pipeline Integration:**\nExtend the storage format to include model metadata and predictions:\n- Store model artifacts in a specialized measurement\n- Append prediction results as derived time series\n- Support online learning with incremental model updates\n- Provide explainability features for model decisions\n\n### Implementation Guidance\n\nWhile the full implementation of these extensions is beyond the current scope, this guidance provides starting points for developers interested in exploring these enhancements.\n\n**A. Technology Recommendations Table:**\n\n| Extension | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Distributed Architecture | Hash-based sharding with static configuration | Raft consensus with dynamic rebalancing |\n| Additional Data Types | Integer and boolean support | Full type system with pluggable codecs |\n| Continuous Queries | Scheduled aggregation jobs | Incremental view maintenance with query rewriting |\n| Tiered Storage | Manual tier promotion scripts | Automatic data lifecycle with S3 integration |\n| Advanced Compression | Algorithm selection per series | Adaptive compression with runtime profiling |\n| Streaming Analytics | Windowed aggregates on ingestion | Full streaming engine with state management |\n| Advanced Indexing | Tag inverted index | Multiple index types with automatic selection |\n| ML Integration | Built-in statistical functions | TensorFlow Lite integration with model serving |\n\n**B. Recommended File/Module Structure:**\n\n```\ntempo/\n  ├── cmd/\n  │   ├── tempo-server/           # Main server (existing)\n  │   └── tempo-cluster/          # Cluster-aware server (new)\n  ├── internal/\n  │   ├── cluster/                # Distributed coordination\n  │   │   ├── coordinator.go      # Cluster coordination logic\n  │   │   ├── sharding.go         # Shard assignment and routing\n  │   │   └── replication.go      # Data replication between nodes\n  │   ├── compression/            # Extended compression algorithms\n  │   │   ├── registry.go         # Algorithm registration\n  │   │   ├── integer.go          # Integer compression\n  │   │   ├── string.go           # String compression\n  │   │   └── adaptive.go         # Adaptive algorithm selection\n  │   ├── streaming/              # Real-time stream processing\n  │   │   ├── engine.go           # Streaming query engine\n  │   │   ├── windows.go          # Window implementations\n  │   │   └── operators.go        # Stream operators (map, filter, aggregate)\n  │   ├── indexing/               # Advanced indexes\n  │   │   ├── tag_inverted.go     # Tag value → series index\n  │   │   ├── value_range.go      # Value range index\n  │   │   └── bloom.go            # Bloom filter implementation\n  │   └── ml/                     # Machine learning integration\n  │       ├── functions.go        # Built-in ML functions\n  │       ├── models.go           # Model storage and serving\n  │       └── anomaly.go          # Anomaly detection algorithms\n  └── pkg/\n      └── storage/tiered/         # Tiered storage abstraction\n          ├── manager.go          # Tier lifecycle management\n          ├── local.go            # Local filesystem backend\n          └── s3.go               # S3 object storage backend\n```\n\n**C. Infrastructure Starter Code - Tiered Storage Interface:**\n\n```go\n// internal/storage/tiered/interface.go\npackage tiered\n\nimport (\n    \"context\"\n    \"io\"\n    \"time\"\n)\n\n// StorageBackend defines the interface for different storage tiers\ntype StorageBackend interface {\n    // Type returns the backend type identifier\n    Type() string\n    \n    // Write writes data to the backend\n    Write(ctx context.Context, key string, data []byte) error\n    \n    // Read reads data from the backend\n    Read(ctx context.Context, key string) ([]byte, error)\n    \n    // Delete removes data from the backend\n    Delete(ctx context.Context, key string) error\n    \n    // Exists checks if data exists in the backend\n    Exists(ctx context.Context, key string) (bool, error)\n    \n    // List lists all keys with a given prefix\n    List(ctx context.Context, prefix string) ([]string, error)\n    \n    // Stats returns backend statistics\n    Stats(ctx context.Context) (BackendStats, error)\n}\n\n// BackendStats contains statistics for a storage backend\ntype BackendStats struct {\n    TotalBytes int64\n    UsedBytes  int64\n    FileCount  int64\n    LatencyMS  float64\n}\n\n// TierManager manages data movement between tiers\ntype TierManager struct {\n    backends map[string]StorageBackend\n    policies []TierPolicy\n    metadata MetadataStore\n    mu       sync.RWMutex\n}\n\n// TierPolicy defines when data should move between tiers\ntype TierPolicy struct {\n    SourceTier      string\n    DestinationTier string\n    Condition       PolicyCondition\n    BatchSize       int\n}\n\n// PolicyCondition defines movement conditions\ntype PolicyCondition struct {\n    AgeOlderThan    time.Duration\n    AccessOlderThan time.Duration\n    SizeGreaterThan int64\n}\n```\n\n**D. Core Logic Skeleton Code - Distributed Query Router:**\n\n```go\n// internal/cluster/router.go\npackage cluster\n\n// QueryRouter routes queries to appropriate shards\ntype QueryRouter struct {\n    shardMap    *ShardMap\n    nodeClients map[string]*NodeClient\n    merger      *ResultMerger\n}\n\n// RouteQuery analyzes a query and routes it to appropriate nodes\nfunc (r *QueryRouter) RouteQuery(ctx context.Context, query *models.Query) (*RoutedQuery, error) {\n    // TODO 1: Extract series keys from the query using tag predicates\n    // TODO 2: For each series key, determine which shard owns it using consistent hashing\n    // TODO 3: Group series keys by shard and then by node (accounting for replication)\n    // TODO 4: Build query fragments for each node, only including series keys that node hosts\n    // TODO 5: Add aggregation merging instructions for cross-shard aggregates\n    // TODO 6: Return RoutedQuery with parallel execution plan\n}\n\n// ExecuteRoutedQuery executes a routed query across multiple nodes\nfunc (r *QueryRouter) ExecuteRoutedQuery(ctx context.Context, routed *RoutedQuery) (*models.QueryResult, error) {\n    // TODO 1: Launch goroutines to execute each query fragment on its target node\n    // TODO 2: Collect results with proper error handling and timeout management\n    // TODO 3: Merge results from different nodes, respecting the merge strategy\n    // TODO 4: Apply any final aggregation that couldn't be pushed to individual nodes\n    // TODO 5: Return combined result\n}\n\n// ResultMerger merges results from multiple shards\ntype ResultMerger struct {\n    mergeStrategy MergeStrategy\n}\n\n// Merge merges multiple query results into one\nfunc (m *ResultMerger) Merge(results []*models.QueryResult) (*models.QueryResult, error) {\n    // TODO 1: Check if all results have compatible schemas\n    // TODO 2: For aggregate queries, combine aggregate values appropriately\n    // TODO 3: For raw data queries, concatenate and sort all points by timestamp\n    // TODO 4: Handle duplicate points from replicated series\n    // TODO 5: Apply limit/offset if specified in the original query\n}\n```\n\n**E. Language-Specific Hints:**\n\n1. **For distributed systems**: Use `hash/fnv` for consistent hashing, `context.Context` for request cancellation, and `errgroup` for managing parallel requests to multiple nodes.\n\n2. **For additional data types**: Implement the `encoding.BinaryMarshaler` and `encoding.BinaryUnmarshaler` interfaces for custom serialization of new types.\n\n3. **For tiered storage**: Use the `aws-sdk-go-v2` for S3 integration with intelligent retries and exponential backoff for failed operations.\n\n4. **For streaming analytics**: Consider using `go-channels` for data flow between streaming operators, with careful buffer sizing to prevent deadlocks.\n\n5. **For ML integration**: Use `gonum.org/v1/gonum` for statistical functions and `github.com/sjwhitworth/golearn` for basic machine learning algorithms.\n\n**F. Milestone Checkpoint for Distributed Extension:**\n\nTo verify a basic distributed implementation:\n```bash\n# Start three nodes in a cluster\n$ ./tempo-cluster --node-id=node1 --cluster-addr=:9090 --http-addr=:8080\n$ ./tempo-cluster --node-id=node2 --cluster-addr=:9091 --http-addr=:8081 --join=localhost:9090\n$ ./tempo-cluster --node-id=node3 --cluster-addr=:9092 --http-addr=:8082 --join=localhost:9090\n\n# Write data to any node\n$ curl -X POST http://localhost:8080/write \\\n  -d 'cpu,host=server1 value=0.64'\n\n# Query from any node (should route to correct node)\n$ curl -G http://localhost:8081/query \\\n  --data-urlencode 'q=SELECT * FROM cpu WHERE host=\"server1\"'\n\n# Verify all nodes show cluster membership\n$ curl http://localhost:8080/cluster/nodes\n# Should return list of all three nodes\n```\n\n**G. Debugging Tips for Future Extensions:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Query returns partial data in distributed mode | Some shards unreachable or returning errors | Check cluster health endpoint, examine query router logs | Ensure all nodes are healthy, implement retry logic for failed shards |\n| Compression ratio worse for new data type | Incorrect algorithm selection for data pattern | Analyze data pattern statistics, benchmark different algorithms | Implement adaptive algorithm selection based on data characteristics |\n| Continuous queries creating duplicate data | Race condition in incremental computation | Check last processed timestamp tracking, examine query execution logs | Add synchronization or use transactional updates for state management |\n| Tier promotion failing silently | Insufficient permissions for object storage | Check tier manager logs, verify credentials and bucket permissions | Implement proper error reporting and retry with exponential backoff |\n| ML functions returning inconsistent results | Model version mismatch or stale cache | Check model metadata, compare results across different calls | Implement model versioning and cache invalidation strategies |\n\n\n## Glossary\n> **Milestone(s):** This reference section supports all five milestones by providing clear definitions of the specialized terminology used throughout the TempoDB design document.\n\nThe time-series database domain uses specialized vocabulary that may be unfamiliar to developers new to this field. This glossary provides authoritative definitions for key terms, acronyms, and concepts used throughout the TempoDB design document. Each term includes a clear definition and reference to the primary section where it's discussed in detail, creating a consistent reference point for implementation.\n\n### Terminology Reference\n\n| Term | Definition | Primary Reference |\n|------|------------|-------------------|\n| **Aggregate Function** | A mathematical operation applied to multiple data points to produce a single summary value, such as sum, average, minimum, maximum, or count. | Query Engine Design |\n| **Backpressure** | A mechanism to throttle incoming write requests when the system is overloaded, preventing resource exhaustion and maintaining system stability. | Write Path Design |\n| **Block-Based Storage** | A storage organization strategy where data is grouped into fixed-size blocks, enabling efficient I/O operations and temporal locality for range queries. | Storage Engine Design |\n| **Cardinality** | The number of unique time series in a dataset, calculated as the product of unique values for each tag dimension. High cardinality can impact performance. | Write Path Design |\n| **Clock Skew** | Time difference between distributed system clocks, which can cause challenges for time-series data ordering and consistency. | Error Handling and Edge Cases |\n| **Columnar Layout** | A data storage format where values from the same column (e.g., all timestamps, all values) are stored contiguously rather than storing complete rows together, improving compression and scan efficiency. | Storage Engine Design |\n| **Compaction** | A background process that merges multiple smaller storage files into larger, optimized files, reducing storage overhead and improving query performance. | Retention and Compaction Design |\n| **Compaction Level** | A hierarchical tier in the compaction strategy where files at higher levels are larger and contain older, less volatile data. | Retention and Compaction Design |\n| **DataPoint** | The fundamental unit of time-series data consisting of a timestamp and a value (`Timestamp time.Time, Value float64`). | Data Model |\n| **Delta-of-Delta Encoding** | A compression technique for timestamps that stores the difference between consecutive differences, achieving high compression ratios for regularly spaced timestamps. | Storage Engine Design |\n| **Downsampling** | The process of reducing data resolution through aggregation (e.g., converting 1-second data to 1-minute averages) to conserve storage space for historical data. | Retention and Compaction Design |\n| **Field** | The actual measured value in a time series, typically a float64 numeric value, as opposed to metadata tags. | Data Model |\n| **Golden File Testing** | A testing methodology that compares output against versioned reference files to ensure format stability and detect unintended changes. | Testing Strategy |\n| **Gorilla XOR Compression** | A lossless compression algorithm for floating-point values that XORs consecutive values and encodes the resulting changes efficiently. | Storage Engine Design |\n| **Grace Period** | A time delay between marking files for deletion (tombstoning) and physically removing them from disk, allowing for recovery from accidental deletions. | Retention and Compaction Design |\n| **Group By Time** | A query operation that partitions data into fixed-width time intervals (buckets) and applies aggregation functions within each bucket. | Query Engine Design |\n| **Inverted Index** | An index data structure that maps tag values to the series keys that contain them, enabling fast filtering by tag predicates. | Query Engine Design |\n| **Iterator Model** | An execution pattern where each query operator implements a `Next()` method to pull data through the pipeline, enabling streaming and lazy evaluation. | Query Engine Design |\n| **Kahan Summation** | An algorithm for summing floating-point numbers with reduced precision loss by maintaining a running compensation for lost low-order bits. | Future Extensions |\n| **Level-Based Compaction** | A compaction strategy that organizes files into tiers (levels) with progressively larger sizes, promoting data through levels based on age and size thresholds. | Retention and Compaction Design |\n| **Line Protocol** | A text-based format for writing time-series data points, consisting of measurement, tag sets, field sets, and timestamp. | Query Language and API Design |\n| **Materialized View** | A pre-computed query result stored for fast access, such as rollup aggregations for historical data. | Future Extensions |\n| **Measurement** | A container for related time-series data, analogous to a table name in relational databases, grouping series with the same semantic meaning. | Data Model |\n| **Memtable** | An in-memory buffer that holds recently written data points before they are flushed to persistent storage, optimized for high write throughput. | Write Path Design |\n| **Memory-Mapped Files** | A file access technique that maps file contents directly into virtual memory, enabling zero-copy reads and efficient random access. | Storage Engine Design |\n| **Out-of-Order Writes** | Data points arriving with timestamps that are not in chronological order relative to previously written points for the same series. | Write Path Design |\n| **Predicate Pushdown** | A query optimization technique that applies filtering conditions as early as possible in the execution pipeline, ideally at the storage layer, to reduce the amount of data processed. | Query Engine Design |\n| **Prometheus Remote Read/Write** | A protocol that allows Prometheus to use external storage systems for long-term data retention, enabling integration with the Prometheus monitoring ecosystem. | Query Language and API Design |\n| **Property-Based Testing** | A testing methodology that verifies properties or invariants hold for all possible inputs within a defined domain, often using randomly generated test cases. | Testing Strategy |\n| **Query Plan** | An internal representation of a query that specifies the execution steps, including which files to scan, which predicates to apply, and in what order to perform operations. | Query Engine Design |\n| **Retention Policy** | A rule defining how long data should be kept before automatic deletion, typically specified as a time-to-live (TTL) duration. | Retention and Compaction Design |\n| **Rollup Series** | A pre-computed time series at lower granularity, created by aggregating higher-resolution data over time windows for historical query performance. | Retention and Compaction Design |\n| **Series** | A collection of data points sharing the same measurement and complete set of tags, representing a single time-varying metric. | Data Model |\n| **Series Key** | A unique identifier for a time series formed by concatenating a measurement name with a complete set of tag key-value pairs (`Measurement string, Tags map[string]string`). | Data Model |\n| **Shard** | A logical partition of data distributed across nodes in a cluster, enabling horizontal scaling and parallel processing. | Future Extensions |\n| **Skip List** | A probabilistic data structure that allows fast search, insertion, and deletion within an ordered sequence, often used for in-memory indexes. | Query Engine Design |\n| **Tags** | Indexed key-value metadata associated with time-series data, used to identify, filter, and group series (e.g., `host=\"server1\", region=\"us-west\"`). | Data Model |\n| **Temporal Locality** | The principle that data accessed together in time should be stored together physically, optimizing for time-range query patterns. | Storage Engine Design |\n| **Tiered Storage** | A storage architecture with multiple performance/cost tiers (e.g., SSD for hot data, HDD for warm data, object storage for cold data), automatically migrating data based on access patterns. | Future Extensions |\n| **Time-Range Query** | A query that retrieves all data points within a specified start and end timestamp boundary. | Query Engine Design |\n| **Time-Series Data** | Sequential measurements or events indexed by time, characterized by append-heavy write patterns, time-ordered reads, and predictable value patterns. | Context and Problem Statement |\n| **Time-Structured Merge Tree (TSM)** | A storage engine optimized for time-series data that organizes data into time-sorted files and merges them in the background, similar to LSM trees but with time-based partitioning. | Storage Engine Design |\n| **Time-To-Live (TTL)** | The duration after which data is considered expired and eligible for automatic deletion, enforced by retention policies. | Retention and Compaction Design |\n| **Tolerance Window** | The maximum allowed time difference for accepting out-of-order data points; points outside this window may be rejected or handled specially. | Write Path Design |\n| **Tombstoned** | A state where a file is marked for deletion but not yet physically removed from disk, typically during a grace period. | Retention and Compaction Design |\n| **TSM File** | A storage file in the Time-Structured Merge format containing compressed time-series data blocks and an index mapping series keys to block locations. | Storage Engine Design |\n| **Tumbling Windows** | Contiguous, non-overlapping time intervals used for grouping operations in windowed aggregations (e.g., every complete 5-minute period). | Query Engine Design |\n| **Write Amplification** | The phenomenon where the storage engine performs more physical writes than the logical writes requested by the application, often due to compaction and durability mechanisms. | Retention and Compaction Design |\n| **Write-Ahead Log (WAL)** | A durability mechanism that logs write operations to persistent storage before acknowledging them to clients, ensuring data survival across crashes. | Write Path Design |\n\n> **Design Insight:** Consistent terminology is critical for team alignment. This glossary serves as a single source of truth for all terms used in the TempoDB design, preventing misunderstandings during implementation. When adding new terms during development, consider updating this glossary to maintain clarity.\n\n### Implementation Guidance\n\nWhile a glossary doesn't require implementation code, maintaining consistency in naming is crucial for code quality and team communication. Below are recommendations for ensuring terminology consistency throughout your Go implementation.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Terminology Validation | Manual code review with glossary reference | Static analysis with custom linter rules |\n| Documentation Generation | GoDoc comments with consistent terms | Automated glossary extraction from source |\n| Naming Convention Enforcement | Team agreement and manual checks | Pre-commit hooks with naming validation |\n\n**B. Recommended File/Module Structure:**\n\nCreate a documentation directory to store the glossary and other reference materials:\n\n```\nproject-root/\n  docs/\n    glossary.md              ← This glossary document\n    architecture.md          ← High-level design overview\n    api-reference.md         ← API documentation\n  internal/\n    storage/                 ← Storage engine implementation\n    query/                   ← Query engine implementation\n    wal/                     ← Write-ahead log implementation\n  cmd/\n    server/main.go           ← Main entry point\n```\n\n**C. Terminology Consistency Checklist:**\n\nWhen reviewing code, verify these consistency points:\n\n1. **Type and Field Names:** Use exact names from the NAMING CONVENTIONS section (e.g., `DataPoint`, not `DataPoint` or `Point`)\n2. **Method Signatures:** Follow the exact signatures provided in the design document\n3. **Error Messages:** Use consistent terminology when describing errors\n4. **Comments and Documentation:** Reference terms from this glossary where appropriate\n5. **Log Messages:** Use standardized terminology for operational logging\n\n**D. Go-Specific Naming Hints:**\n\n- Use `camelCase` for local variables and private fields\n- Use `PascalCase` for exported types, functions, and constants\n- Follow Go conventions for acronyms: `TSMFile` (not `TsmFile`), `WAL` (not `Wal`)\n- Use descriptive names that match glossary terms: `memtable` (not `writeBuffer`), `compactionPlan` (not `mergePlan`)\n\n**E. Debugging Terminology Mismatches:**\n\nIf you encounter confusion during implementation, check:\n\n1. **Cross-reference:** Verify all team members are using the same version of the glossary\n2. **Code search:** Use `grep` or IDE search to find inconsistent usage\n3. **Documentation:** Update inline comments to clarify term usage\n4. **Peer review:** Include terminology checks in code review checklists\n\n**F. Maintaining the Glossary:**\n\nAs the implementation evolves, you may discover new terms that need definition. Follow this process:\n\n1. Add the term to this glossary document with a clear definition\n2. Update the NAMING CONVENTIONS section if it's a core type, method, or constant\n3. Communicate the change to the team\n4. Update any affected code to use the new terminology consistently\n\n> **Implementation Tip:** Consider creating a simple validation script that scans source code for glossary terms and flags potentially inconsistent usage. This can be particularly helpful for large codebases or distributed teams.\n"}