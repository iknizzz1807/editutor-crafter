{"html":"<h1 id=\"-project-charter-c-like-language-tokenizer\">ðŸŽ¯ Project Charter: C-Like Language Tokenizer</h1>\n<h2 id=\"what-you-are-building\">What You Are Building</h2>\n<p>You are building a character-level lexical scanner (lexer) that transforms raw source text into a structured stream of categorized tokens. This is a single-pass, high-performance engine that implements a Deterministic Finite Automaton (DFA) to recognize identifiers, keywords, multi-character operators, number literals, and strings, while precisely tracking source positions for error reporting.</p>\n<h2 id=\"why-this-project-exists\">Why This Project Exists</h2>\n<p>Most developers use language tools daily but treat the translation of text into meaning as a &quot;black box.&quot; Building a tokenizer from scratch is the only way to truly understand the Maximal Munch principle, the mechanics of lookahead, and how compilers maintain $O(n)$ performance while handling complex context-sensitive features like multi-line comments and escape sequences.</p>\n<h2 id=\"what-you-will-be-able-to-do-when-done\">What You Will Be Able to Do When Done</h2>\n<ul>\n<li><strong>Implement an FSM:</strong> Build a character-by-character scanner using a state-based cursor model.</li>\n<li><strong>Apply Maximal Munch:</strong> Resolve lexical ambiguity in operators (e.g., distinguishing <code>=</code> from <code>==</code>) using greedy consumption logic.</li>\n<li><strong>Handle Context-Sensitivity:</strong> Switch scanning &quot;modes&quot; to handle string literals and multi-line comments correctly.</li>\n<li><strong>Implement Error Recovery:</strong> Design a scanner that doesn&#39;t crash on typos but instead emits diagnostic tokens and resumes operation.</li>\n<li><strong>Track Source Metadata:</strong> Generate precise line and column data required for IDE squiggles and compiler error messages.</li>\n</ul>\n<h2 id=\"final-deliverable\">Final Deliverable</h2>\n<p>A production-ready <code>Scanner</code> class (approx. 400-600 lines of code) that accepts a source string and returns a list of <code>Token</code> objects. It will include a comprehensive integration test suite and a performance benchmark capable of processing a 10,000-line C-like program in under one second.</p>\n<h2 id=\"is-this-project-for-you\">Is This Project For You?</h2>\n<p><strong>You should start this if you:</strong></p>\n<ul>\n<li>Have a solid handle on basic string manipulation (indexing, slicing, concatenation).</li>\n<li>Are comfortable with control flow structures (while-loops, if-else chains).</li>\n<li>Want to understand the &quot;front-end&quot; of compilers and interpreters.</li>\n</ul>\n<p><strong>Come back after you&#39;ve learned:</strong></p>\n<ul>\n<li>Basic Object-Oriented Programming (defining classes and methods).</li>\n<li>How to use Enumerations (Enums) and Dictionaries (Hash Maps) in your chosen language.</li>\n</ul>\n<h2 id=\"estimated-effort\">Estimated Effort</h2>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Types &amp; Scanner Foundation</td>\n<td>~2.5 hours</td>\n</tr>\n<tr>\n<td>Multi-Character Tokens &amp; Maximal Munch</td>\n<td>~3.5 hours</td>\n</tr>\n<tr>\n<td>Strings &amp; Comments Sub-States</td>\n<td>~2.5 hours</td>\n</tr>\n<tr>\n<td>Integration Testing &amp; Error Recovery</td>\n<td>~2.5 hours</td>\n</tr>\n<tr>\n<td><strong>Total</strong></td>\n<td><strong>~11 hours</strong></td>\n</tr>\n</tbody></table>\n<h2 id=\"definition-of-done\">Definition of Done</h2>\n<p>The project is complete when:</p>\n<ul>\n<li>The <code>scan_tokens()</code> method produces a deterministic list of tokens ending in a sentinel <code>EOF</code> for any valid input.</li>\n<li>The &quot;Canonical Statement&quot; test (<code>if (x &gt;= 42) { return &quot;ok&quot;; }</code>) produces 12 tokens with exact line/column matches.</li>\n<li>Multi-line block comments (<code>/* ... */</code>) are stripped entirely while maintaining correct line counts for subsequent tokens.</li>\n<li>The scanner successfully processes a 10,000-line generated source file in less than 1.0 second on standard hardware.</li>\n<li>Every unrecognized character in a file produces a <code>TokenType.ERROR</code> without halting the scanning of subsequent valid tokens.</li>\n</ul>\n<hr>\n<h1 id=\"tokenizer-lexer-building-a-character-level-scanner-for-a-c-like-language\">Tokenizer / Lexer: Building a Character-Level Scanner for a C-like Language</h1>\n<p>This project builds a complete lexer from scratch â€” the first phase of any compiler or interpreter pipeline. You will implement a character-by-character finite state machine that transforms raw source text into a structured stream of categorized tokens. The tokenizer handles single and multi-character operators, number and string literals, identifiers, keywords, comments, escape sequences, and error recovery â€” all while tracking precise source positions for downstream error reporting.</p>\n<p>By the end, you will have internalized how programming languages are read at the lowest level: not as words or lines, but as individual characters consumed one at a time through a state machine that makes greedy decisions (maximal munch) with minimal lookahead. This understanding cascades into parsers, compilers, language servers, syntax highlighters, and any tool that must understand source code structurally.</p>\n<p>The project is structured as four milestones that progressively layer complexity: foundation (tokens, scanner, single-char), multi-character recognition (operators, numbers, identifiers, keywords), strings and comments (escape sequences, nested state), and finally integration testing with error recovery.</p>\n<!-- MS_ID: tokenizer-m1 -->\n<h1 id=\"milestone-1-token-types-amp-scanner-foundation\">Milestone 1: Token Types &amp; Scanner Foundation</h1>\n<h2 id=\"where-you-are-in-the-pipeline\">Where You Are in the Pipeline</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-satellite-map.svg\" alt=\"Tokenizer System Satellite Map â€” The Complete Pipeline\"></p>\n<p>Before a compiler can parse expressions, type-check variables, or generate machine code, it needs to answer one deceptively simple question: <em>what is this source file made of?</em> Raw source text is just a sequence of characters â€” bytes with no inherent meaning. Your tokenizer is the component that transforms that flat stream of characters into a structured sequence of meaningful units called <strong>tokens</strong>.\nThink of it like reading a sentence in a human language. When you read &quot;the cat sat&quot;, your brain does not process individual letters â€” it automatically groups letters into words and assigns each word a grammatical role (article, noun, verb). The tokenizer does exactly this for source code: it groups characters into lexemes and assigns each lexeme a type.\nIn this milestone, you will build the foundation that all subsequent work rests on:</p>\n<ol>\n<li>A <strong>token type enumeration</strong> â€” the vocabulary of your language</li>\n<li>A <strong>Token data structure</strong> â€” the carrier of information about each recognized unit</li>\n<li>A <strong>Scanner class</strong> â€” the character-consuming engine with its two primitive operations: <code>advance()</code> and <code>peek()</code>\nBy the end of this milestone, your scanner will successfully process single-character tokens, swallow whitespace silently, and emit sentinel tokens at the boundaries of valid and invalid input.</li>\n</ol>\n<hr>\n<h2 id=\"the-revelation-tokenizers-are-not-split\">The Revelation: Tokenizers Are Not Split()</h2>\n<p>Here is what most developers assume when they first think about tokenizers:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># What people think tokenizers do (DON'T do this)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> tokenize_naive</span><span style=\"color:#E1E4E8\">(source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> source.split()</span></span></code></pre></div>\n<p>Maybe with some regex sprinkled on top:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Still wrong</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.findall(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\w</span><span style=\"color:#F97583\">+|</span><span style=\"color:#79B8FF\">[</span><span style=\"color:#F97583\">^</span><span style=\"color:#79B8FF\">\\w\\s]</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, source)</span></span></code></pre></div>\n<p>This feels reasonable. After all, Python&#39;s <code>split()</code> does break text into words, and regex can match patterns. The misconception is understandable.\n<strong>Here is where this model breaks.</strong> Consider the input:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>x &gt;= 42</code></pre></div>\n<p>A split-based approach gives you <code>[&#39;x&#39;, &#39;&gt;=&#39;, &#39;42&#39;]</code>. That looks fine. Now consider:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>x&gt;=42</code></pre></div>\n<p>Same semantics â€” no spaces. <code>split()</code> gives you <code>[&#39;x&gt;=42&#39;]</code>. One token. That is completely wrong.\nNow consider escape sequences inside strings:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>&quot;hello\\nworld&quot;</code></pre></div>\n<p>No regex operating on a substring can correctly distinguish whether the <code>\\n</code> is an escape sequence inside a string or the literal characters <code>\\</code> and <code>n</code>. To know that, you need to know <em>where you are</em> â€” are you inside a string literal right now? Regex has no memory of what it scanned two characters ago.\n<strong>The real model</strong>: A tokenizer is a <strong>finite state machine</strong> (FSM) that reads one character at a time, maintains a current <em>state</em>, and makes local decisions â€” emit a token, change state, advance the cursor â€” based only on the current character and the immediately next character (one character of lookahead). It never looks at the &quot;whole string.&quot; The cursor moves forward, one character at a time, and never goes back.\nThis is why tokenizers are O(n) with constant memory: every character is visited exactly once. It is also why they are fast enough to process millions of lines per second in production compilers.</p>\n<blockquote>\n<p><strong>ðŸ”‘ Foundation: Finite State Machines as applied to tokenization: states</strong></p>\n<h3 id=\"1-what-it-is\">1. What it IS</h3>\n<p>A <strong>Finite State Machine (FSM)</strong> is a mathematical model used to design logic that moves through a sequence of &quot;states&quot; based on input. In tokenization, an FSM treats a stream of characters as input and determines where one &quot;token&quot; (like a keyword, variable name, or operator) ends and the next begins.</p>\n</blockquote>\n<p>It consists of three core components:</p>\n<ul>\n<li><strong>States</strong>: The &quot;modes&quot; the tokenizer can be in (e.g., <code>START</code>, <code>READING_NUMBER</code>, <code>READING_STRING</code>).</li>\n<li><strong>Transitions</strong>: Rules that move the machine from one state to another based on the current character (e.g., &quot;If in <code>START</code> and see a digit, move to <code>READING_NUMBER</code>&quot;).</li>\n<li><strong>Accepting (Final) States</strong>: Specific states that signify a valid token has been successfully identified (e.g., &quot;We hit a space after some digits; the current buffer is a valid <code>INTEGER</code>&quot;).</li>\n</ul>\n<h3 id=\"2-why-you-need-it-right-now\">2. WHY you need it right now</h3>\n<p>As you move from simple string splitting to building a robust lexer, manual <code>if-else</code> or <code>switch</code> statements become &quot;spaghetti code&quot; that is difficult to debug. For example, distinguishing between a decimal <code>10.5</code>, an integer <code>10</code>, and a range <code>10..20</code> requires looking ahead or keeping track of what you&#39;ve already seen. </p>\n<p>An FSM provides a formal structure to manage this complexity. It ensures that your tokenizer is <strong>predictable</strong> and <strong>exhaustive</strong>, meaning it handles every possible character sequence without falling into ambiguous logic traps.</p>\n<h3 id=\"3-key-insight-quotthe-state-is-your-memoryquot\">3. Key Insight: &quot;The State is your Memory&quot;</h3>\n<p>The most important thing to remember is that <strong>the current state represents everything the tokenizer knows about the past.</strong> </p>\n<p>You donâ€™t need to look back at the last five characters to know if you are inside a comment; the fact that you are currently in the <code>IN_COMMENT</code> state already tells you everything you need to know. When you receive a new character, you only need two pieces of information to decide what to do next: <strong>&quot;What state am I in?&quot;</strong> and <strong>&quot;What is this character?&quot;</strong></p>\n<hr>\n<h2 id=\"designing-the-token-type-enumeration\">Designing the Token Type Enumeration</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m1-token-enum.svg\" alt=\"Token Type Enumeration â€” Complete Category Map\"></p>\n<p>The first thing you need is a vocabulary â€” a fixed set of categories into which every lexeme in your C-like language will be classified. In Python, this is a natural fit for <code>enum.Enum</code>.\nEvery token in your language falls into one of these categories:</p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>What it represents</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>NUMBER</code></td>\n<td>Integer or floating-point literals</td>\n<td><code>42</code>, <code>3.14</code>, <code>0</code></td>\n</tr>\n<tr>\n<td><code>STRING</code></td>\n<td>String literals (quoted)</td>\n<td><code>&quot;hello&quot;</code>, <code>&quot;x\\n&quot;</code></td>\n</tr>\n<tr>\n<td><code>IDENTIFIER</code></td>\n<td>Variable/function names</td>\n<td><code>x</code>, <code>myVar</code>, <code>_count</code></td>\n</tr>\n<tr>\n<td><code>KEYWORD</code></td>\n<td>Reserved words</td>\n<td><code>if</code>, <code>while</code>, <code>return</code></td>\n</tr>\n<tr>\n<td><code>OPERATOR</code></td>\n<td>Arithmetic and comparison operators</td>\n<td><code>+</code>, <code>-</code>, <code>&gt;=</code>, <code>==</code></td>\n</tr>\n<tr>\n<td><code>PUNCTUATION</code></td>\n<td>Structural delimiters</td>\n<td><code>(</code>, <code>)</code>, <code>{</code>, <code>;</code>, <code>,</code></td>\n</tr>\n<tr>\n<td><code>EOF</code></td>\n<td>End of input â€” the sentinel</td>\n<td>(no lexeme)</td>\n</tr>\n<tr>\n<td><code>ERROR</code></td>\n<td>Unrecognized character</td>\n<td><code>@</code>, <code>#</code>, <code>$</code></td>\n</tr>\n<tr>\n<td>Notice that <code>OPERATOR</code> and <code>PUNCTUATION</code> are both &quot;symbols.&quot; You might wonder: why separate them? Convention and downstream utility. An operator participates in expressions and has associativity and precedence. A punctuation mark is structural glue â€” a semicolon terminates a statement, a comma separates arguments. When your parser eventually consumes this token stream, it will care about this distinction. Design your token types for the consumer (the parser), not just for the producer (the scanner).</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Here is the complete enumeration:</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Symbols â€” operators (participate in expressions)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PLUS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># +</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MINUS</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># -</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STAR</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># *</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SLASH</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># /</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUAL</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ==</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUAL</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQ</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQ</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># >=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BANG</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Symbols â€” punctuation (structural)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LPAREN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RPAREN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RBRACE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACKET</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># [</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RBRACKET</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEMICOLON</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sentinel / diagnostic</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span></code></pre></div>\n<blockquote>\n<p><strong>Design note:</strong> <code>auto()</code> assigns sequential integer values automatically. You never need to remember which integer corresponds to which token â€” you always compare by name (<code>TokenType.PLUS</code>, not <code>5</code>). This is the correct way to use enums in Python: names over values.\nYou will notice that operators are <em>individually named</em> (<code>PLUS</code>, <code>MINUS</code>, <code>SLASH</code>) rather than using a generic <code>OPERATOR</code> type with a value string. This is intentional. A parser matching a <code>+</code> node does not want to check <code>token.type == TokenType.OPERATOR and token.lexeme == &quot;+&quot;</code>. It wants to check <code>token.type == TokenType.PLUS</code>. Be specific. Your parser will thank you.</p>\n</blockquote>\n<hr>\n<h2 id=\"the-token-data-structure\">The Token Data Structure</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m1-token-struct.svg\" alt=\"Token Data Structure Layout\"></p>\n<p>A token is not just a type. It carries four pieces of information:</p>\n<ol>\n<li><strong>type</strong> â€” which category this token belongs to (<code>TokenType.PLUS</code>, <code>TokenType.KEYWORD</code>, etc.)</li>\n<li><strong>lexeme</strong> â€” the exact raw text from the source that this token was scanned from (<code>&quot;+&quot;</code>, <code>&quot;while&quot;</code>, <code>&quot;3.14&quot;</code>)</li>\n<li><strong>line</strong> â€” the line number in the source file where this token starts (1-indexed)</li>\n<li><strong>column</strong> â€” the column number in the source file where this token starts (1-indexed)\nThe lexeme is the raw text â€” for a string literal <code>&quot;hello&quot;</code>, the lexeme includes the quote characters: <code>&#39;&quot;hello&quot;&#39;</code>. For an integer <code>42</code>, the lexeme is <code>&quot;42&quot;</code>. This matters because when you report an error, you want to show the user exactly what they wrote.\nLine and column are metadata that the tokenizer produces and the parser (and error reporter) consume. Without them, an error message can only say &quot;invalid syntax&quot; â€” with them, it says &quot;invalid syntax at line 12, column 8.&quot; This is the difference between a usable and an unusable language tool.</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span></code></pre></div>\n<p><code>@dataclass</code> gives you <code>__init__</code> and <code>__eq__</code> for free â€” you can construct a token with <code>Token(TokenType.PLUS, &quot;+&quot;, 1, 5)</code> and compare tokens in tests with <code>==</code>. The custom <code>__repr__</code> makes debugging output readable.\nA few token constructions you will use often enough to make helper functions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> make_eof</span><span style=\"color:#E1E4E8\">(line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, line, column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> make_error</span><span style=\"color:#E1E4E8\">(char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, line, column)</span></span></code></pre></div>\n<h2 id=\"the-eof-token-has-an-empty-lexeme-there-is-no-source-text-that-corresponds-to-end-of-file-the-error-token39s-lexeme-is-the-offending-character\">The EOF token has an empty lexeme â€” there is no source text that corresponds to end-of-file. The Error token&#39;s lexeme is the offending character.</h2>\n<h2 id=\"the-scanner-class-architecture-first\">The Scanner Class: Architecture First</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m1-scanner-anatomy.svg\" alt=\"Scanner Anatomy: The Character Cursor Machine\"></p>\n<p>The scanner holds:</p>\n<ul>\n<li>The <strong>source string</strong> â€” the complete source text, indexed as a Python string</li>\n<li>A <strong>current position</strong> cursor â€” an integer index into the source string</li>\n<li><strong>Line</strong> and <strong>column</strong> counters â€” updated as the cursor advances\nHere is the skeleton:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">   # index of next character to consume</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">      # current line number (1-indexed)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">    # current column number (1-indexed)</span></span></code></pre></div>\n<p>Notice <code>current</code> is the index of the <em>next</em> character to consume â€” it has not been consumed yet. When the scanner starts, <code>current = 0</code> points at the first character in the source.</p>\n<h3 id=\"is_at_end\">is_at_end()</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source)</span></span></code></pre></div>\n<p>This is the simplest guard. Every loop and conditional in the scanner begins by checking <code>is_at_end()</code> before touching <code>self.source[self.current]</code> â€” accessing past the end of a Python string raises <code>IndexError</code>.</p>\n<h3 id=\"peek\">peek()</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#6A737D\">   # null character as \"no character\" sentinel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span></code></pre></div>\n<p><code>peek()</code> reads the character at <code>self.current</code> without moving the cursor. Call it ten times in a row â€” you get the same character. This is your window into the future: before you decide what to do, you can look at the next character without committing to consuming it.\nThe return value <code>&quot;\\0&quot;</code> (the null character, ASCII 0) when at end is a convention borrowed from C tokenizers. It is a value that will never appear in real source code, so any comparison like <code>peek() == &#39;=&#39;</code> will safely return <code>False</code> at end-of-input without needing a separate <code>if is_at_end()</code> guard in every caller. This is the <strong>sentinel value pattern</strong> â€” returning a special out-of-band value to signal a boundary condition.</p>\n<h3 id=\"advance\">advance()</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> char</span></span></code></pre></div>\n<p><code>advance()</code> is the consuming operation. It reads the current character, moves the cursor forward by one, updates the position counters, and returns the character. After <code>advance()</code>, the cursor points at the character <em>after</em> the one just consumed.</p>\n<blockquote>\n<p><strong>Critical detail about <code>\\r\\n</code>:</strong> Windows line endings are the two-character sequence <code>\\r</code> followed by <code>\\n</code>. If you naively count every <code>\\n</code> as a new line, you will double-count Windows newlines. The standard approach: treat <code>\\r</code> as whitespace that does NOT increment the line counter (only <code>\\n</code> does). The <code>\\r\\n</code> pair will then correctly increment the line counter exactly once (when <code>\\n</code> is consumed). This is already handled by the code above â€” <code>\\r</code> falls through to the <code>else</code> branch and increments <code>column</code>, which is harmless.\n<strong>About tab width:</strong> The code above increments <code>column</code> by 1 for every character, including tab (<code>\\t</code>). This is the standard approach in most modern compilers (including LLVM&#39;s Clang). An alternative is to advance to the next tab stop (column rounded up to the nearest multiple of 8 or 4), but this makes position reporting dependent on editor settings and creates confusing mismatch between what the user sees and what the compiler reports. Consistent 1-per-character is simpler and more predictable.</p>\n</blockquote>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m1-advance-peek-trace.svg\" alt=\"Trace: advance() and peek() in Action\"></p>\n<h3 id=\"a-trace-through-advance-and-peek\">A Trace Through advance() and peek()</h3>\n<p>Suppose your source is <code>&quot;x+1&quot;</code>. Here is the scanner state at each step:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Initial:  current=0, line=1, col=1\nsource:   x + 1\n          ^\n          current\npeek()    â†’ 'x'      (current stays at 0)\nadvance() â†’ 'x'      current=1, col=2\npeek()    â†’ '+'      (current stays at 1)\nadvance() â†’ '+'      current=2, col=3\npeek()    â†’ '1'      (current stays at 2)\nadvance() â†’ '1'      current=3, col=4\nis_at_end() â†’ True\npeek()    â†’ '\\0'</code></pre></div>\n<h2 id=\"every-character-is-visited-exactly-once-peek-is-free-no-movement-advance-is-permanent-no-going-back\">Every character is visited exactly once. <code>peek()</code> is free â€” no movement. <code>advance()</code> is permanent â€” no going back.</h2>\n<h2 id=\"position-tracking-in-depth\">Position Tracking in Depth</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m1-position-tracking.svg\" alt=\"Line and Column Tracking Through Newlines and Tabs\"></p>\n<p>Position tracking has one subtle complication: the <code>line</code> and <code>column</code> you record in a token should be the position where the token <em>starts</em>, not where it ends. By the time you finish scanning a multi-character token like <code>while</code> or <code>42.5</code>, your cursor is past the end of the lexeme. So you need to capture the start position <em>before</em> you begin scanning.\nThe pattern you will use throughout this project:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> next_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Capture position BEFORE advancing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ... dispatch on char ...</span></span></code></pre></div>\n<h2 id=\"by-recording-tok_line-and-tok_col-from-selfline-selfcolumn-before-calling-advance-you-capture-the-position-of-the-first-character-of-the-token-all-subsequent-advance-calls-within-the-same-token-scan-will-update-selfline-and-selfcolumn-but-the-token-you-emit-will-carry-the-start-position-this-pattern-will-be-critical-in-milestone-3-when-you-scan-string-literals-that-span-multiple-lines-the-token39s-reported-position-should-be-the-opening-quote-not-the-closing-one\">By recording <code>tok_line</code> and <code>tok_col</code> from <code>self.line</code> / <code>self.column</code> <em>before</em> calling <code>advance()</code>, you capture the position of the first character of the token. All subsequent <code>advance()</code> calls within the same token scan will update <code>self.line</code> and <code>self.column</code>, but the token you emit will carry the <em>start</em> position.\nThis pattern will be critical in Milestone 3 when you scan string literals that span multiple lines â€” the token&#39;s reported position should be the opening quote, not the closing one.</h2>\n<h2 id=\"the-finite-state-machine-view\">The Finite State Machine View</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m1-fsm-basic.svg\" alt=\"Scanner as Finite State Machine â€” Basic States\"></p>\n<p>Your scanner&#39;s <code>scan_tokens()</code> method is a finite state machine. The <em>states</em> correspond to what the scanner is currently &quot;inside&quot;:</p>\n<ul>\n<li><strong>START</strong> â€” at the beginning of a new token, deciding what to scan</li>\n<li><strong>IN_NUMBER</strong> â€” currently consuming digit characters</li>\n<li><strong>IN_IDENTIFIER</strong> â€” currently consuming alphanumeric characters</li>\n<li><strong>IN_STRING</strong> â€” currently consuming characters inside double quotes</li>\n<li><strong>IN_COMMENT</strong> â€” skipping characters until comment ends\nIn this milestone, you only implement the START state (dispatching on the first character of each token) and the simple case where a single character completely determines the token. Multi-character states will come in Milestones 2 and 3.\nThe dispatch logic is a match (or if/elif chain) on the character returned by <code>advance()</code>:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Single-character tokens â€” one character, one decision, one token</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"+\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"-\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"*\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">STAR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"/\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SLASH</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># '/' alone is division; '//' and '/*' handled in M3</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"(\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \")\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"{\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"}\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"[\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"]\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \";\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \",\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">COMMA</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n<h2 id=\"using-a-dictionary-for-single-character-dispatch-is-both-idiomatic-python-and-faster-than-a-long-ifelif-chain-a-dictionary-lookup-is-o1-hash-table-access\">Using a dictionary for single-character dispatch is both idiomatic Python and faster than a long <code>if/elif</code> chain â€” a dictionary lookup is O(1) hash table access.</h2>\n<h2 id=\"putting-it-together-scan_tokens-and-next_token\">Putting It Together: scan_tokens() and next_token()</h2>\n<p>The top-level API of your scanner produces a complete list of tokens from the source:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens: list[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tokens</span></span></code></pre></div>\n<p>The loop is driven by <code>next_token()</code>, which produces one token per call. It terminates when <code>next_token()</code> returns an EOF token. Importantly, the EOF token is appended to the list before the loop exits â€” the parser downstream will expect to find <code>EOF</code> as the last element and will use it as its own termination signal. Forgetting the EOF token is a classic first-timer mistake that causes downstream parsers to crash with an <code>IndexError</code>.\nNow the complete <code>next_token()</code> for this milestone:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> next_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Skip whitespace â€” consume without emitting</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">._skip_whitespace()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Capture the start position of this token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Single-character tokens via lookup table</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(</span><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char], char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Unrecognized character â€” emit Error token, continue scanning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span></code></pre></div>\n<p>And the whitespace consumer:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> peek() </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\" \"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span></code></pre></div>\n<p>Wait â€” there is a subtle issue above. Inside <code>_skip_whitespace</code>, you should call <code>self.peek()</code>, not a standalone <code>peek()</code>. Let us write the complete, correct version:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\" \"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span></code></pre></div>\n<p>Why does <code>_skip_whitespace</code> call <code>advance()</code> rather than simply incrementing <code>self.current</code>? Because <code>advance()</code> handles position tracking. If you increment <code>self.current</code> directly, you bypass the newline detection and your line counter falls out of sync.</p>\n<blockquote>\n<p><strong>Position capture timing:</strong> Notice that <code>_skip_whitespace()</code> is called <em>before</em> capturing <code>tok_line</code> and <code>tok_col</code>. This is intentional â€” you skip past any leading whitespace, <em>then</em> record where the actual token starts. If you captured position first, then skipped whitespace, the token&#39;s reported position would point at the space rather than the token itself. Always skip whitespace before snapping the position.</p>\n</blockquote>\n<hr>\n<h2 id=\"the-full-scanner-milestone-1-complete-implementation\">The Full Scanner: Milestone 1 Complete Implementation</h2>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PLUS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MINUS</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STAR</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SLASH</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUAL</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUAL</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQ</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQ</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BANG</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LPAREN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RPAREN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RBRACE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACKET</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RBRACKET</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEMICOLON</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sentinel / diagnostic</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"+\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"-\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"*\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">STAR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"/\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SLASH</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"(\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \")\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"{\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"}\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"[\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"]\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \";\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \",\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">COMMA</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">WHITESPACE</span><span style=\"color:#E1E4E8\">: frozenset[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> frozenset</span><span style=\"color:#E1E4E8\">({</span><span style=\"color:#9ECBFF\">\" \"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Primitive Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"True when the cursor has reached or passed the end of source.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return the current character without advancing. Returns '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">0' at end.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Consume and return the current character.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Updates line and column counters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume whitespace characters without emitting tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> WHITESPACE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Token Production â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> next_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Scan and return the next token from the source.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._skip_whitespace()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tok_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tok_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(</span><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char], char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Unrecognized character: emit Error, continue (error recovery)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Scan the entire source and return a list of tokens.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Always ends with an EOF token.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens: list[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tokens</span></span></code></pre></div>\n<hr>\n<h2 id=\"testing-your-foundation\">Testing Your Foundation</h2>\n<p>Good tests for a tokenizer are not &quot;does it return <em>something</em>?&quot; â€” they are &quot;does it return the <em>exact</em> token stream, with correct types, lexemes, lines, and columns?&quot; Every test should pin down the complete token, not just its type.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_char_tokens</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"+ - * /\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"-\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">STAR</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">SLASH</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_whitespace_is_not_emitted</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"   </span><span style=\"color:#79B8FF\">\\t\\t</span><span style=\"color:#9ECBFF\">  +\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Only PLUS and EOF â€” whitespace produces nothing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">PLUS</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#6A737D\">   # 7 whitespace chars before, so col 8</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_newline_resets_column</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"{</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">}\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#9ECBFF\">\"{\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#9ECBFF\">\"}\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)   </span><span style=\"color:#6A737D\"># line 2, col 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_eof_on_empty_input</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_token_for_invalid_char</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_recovery_continues</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # After an error token, scanning continues normally</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#6A737D\">   # '@' is invalid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#6A737D\">    # '+' is still recognized</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_all_punctuation</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"()</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">[];,\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">LBRACKET</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">RBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">COMMA</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> [t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multiline_position_tracking</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"+</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">+</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">+\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_windows_line_endings</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # \\r\\n should count as ONE newline, not two</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"+</span><span style=\"color:#79B8FF\">\\r\\n</span><span style=\"color:#9ECBFF\">+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # one newline, not two</span></span></code></pre></div>\n<h2 id=\"run-all-tests-with-python-m-pytest-test_scannerpy-v-every-test-should-pass-green-before-you-proceed-to-milestone-2\">Run all tests with <code>python -m pytest test_scanner.py -v</code>. Every test should pass green before you proceed to Milestone 2.</h2>\n<h2 id=\"design-decisions-why-this-architecture\">Design Decisions: Why This Architecture?</h2>\n<h3 id=\"why-a-class-rather-than-a-function-with-a-loop\">Why a class rather than a function with a loop?</h3>\n<p>The scanner maintains mutable state (<code>current</code>, <code>line</code>, <code>column</code>) that must persist across calls to <code>next_token()</code>. A class is the natural Python idiom for encapsulating mutable state that evolves over time. An alternative would be to pass a mutable dictionary around, but that is less readable and less type-safe.</p>\n<h3 id=\"why-store-the-entire-source-string-rather-than-reading-character-by-character\">Why store the entire source string rather than reading character by character?</h3>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Used by</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Store full source in memory</strong> âœ“</td>\n<td>Simple indexing, peek is trivial, restartable</td>\n<td>Source must fit in RAM</td>\n<td>CPython, Go compiler, LLVM Clang</td>\n</tr>\n<tr>\n<td>Stream (file handle, generator)</td>\n<td>Handles huge files, lower memory</td>\n<td>Peek requires buffering; no restart</td>\n<td>Streaming compilers for very large codebases</td>\n</tr>\n<tr>\n<td>For source files up to tens of megabytes (which is essentially all real-world source code), storing the full string in memory is correct. The performance requirement for this project (10,000-line file in under 1 second) is trivially met by either approach in Python.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<h3 id=\"why-quot0quot-as-the-sentinel-from-peek\">Why <code>&quot;\\0&quot;</code> as the sentinel from peek()?</h3>\n<h2 id=\"alternatives-include-none-pythonic-but-requires-optionalstr-type-annotation-and-none-checks-everywhere-raising-an-exception-verbose-or-returning-quotquot-empty-string-comparisons-work-but-are-confusing-the-quot0quot-convention-comes-from-c-tokenizers-crafting-interpreters-gcc-clang-and-has-one-decisive-advantage-all-character-comparisons-like-peek-quotquot-and-peek-in-quotabcquot-work-correctly-and-safely-at-end-of-input-quot0quot-will-never-match-any-valid-source-character\">Alternatives include <code>None</code> (Pythonic but requires <code>Optional[str]</code> type annotation and <code>None</code> checks everywhere), raising an exception (verbose), or returning <code>&quot;&quot;</code> (empty string comparisons work but are confusing). The <code>&quot;\\0&quot;</code> convention comes from C tokenizers (Crafting Interpreters, GCC, Clang) and has one decisive advantage: all character comparisons like <code>peek() == &quot;=&quot;</code> and <code>peek() in &quot;abc&quot;</code> work correctly and safely at end-of-input â€” <code>&quot;\\0&quot;</code> will never match any valid source character.</h2>\n<h2 id=\"the-three-level-view-of-your-scanner\">The Three-Level View of Your Scanner</h2>\n<p>Let us look at what your scanner does from three levels:\n<strong>Level 1 â€” Source Language (programmer&#39;s view):</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>if (x &gt;= 42) { return true; }</code></pre></div>\n<p>A human sees keywords, variable names, operators, and braces.\n<strong>Level 2 â€” Scanner Internal (your scanner&#39;s view):</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cursor=0, line=1, col=1\nread 'i' â†’ start identifier scan...\nread 'f' â†’ continue identifier...\nend of alphanumerics â†’ lexeme = &quot;if&quot; â†’ lookup table â†’ KEYWORD\nemit Token(KEYWORD, &quot;if&quot;, 1, 1)\nread ' ' â†’ whitespace, skip\nread '(' â†’ LPAREN\nemit Token(LPAREN, &quot;(&quot;, 1, 4)\n...</code></pre></div>\n<p>The scanner sees a sequence of characters, a cursor, and dispatch decisions.\n<strong>Level 3 â€” Python Runtime (what actually executes):</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>String indexing: source[self.current]  â†’  O(1) pointer arithmetic\ndict lookup: char in SINGLE_CHAR_TOKENS  â†’  O(1) hash table probe\ndataclass __init__: Token(...)  â†’  __new__ + field assignment\nlist.append(token)  â†’  amortized O(1)</code></pre></div>\n<h2 id=\"python-strings-are-immutable-sequences-stored-in-contiguous-memory-sourcei-is-a-direct-offset-calculation-not-iteration-this-is-why-character-level-scanning-in-python-is-fast-enough-to-meet-the-10000-line-benchmark\">Python strings are immutable sequences stored in contiguous memory. <code>source[i]</code> is a direct offset calculation â€” not iteration. This is why character-level scanning in Python is fast enough to meet the 10,000-line benchmark.</h2>\n<h2 id=\"common-pitfalls\">Common Pitfalls</h2>\n<p><strong>1. Capturing position after advance instead of before</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># WRONG: tok_col is already 1 ahead of where the token started</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tok_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column   </span><span style=\"color:#6A737D\"># off by one!</span></span></code></pre></div>\n<p>Always capture <code>tok_line</code> and <code>tok_col</code> before calling <code>advance()</code>.\n<strong>2. Forgetting the EOF token</strong>\nIf <code>scan_tokens()</code> does not append the EOF token, the parser will eventually call <code>tokens[i]</code> on an out-of-bounds index and crash with <code>IndexError</code>. The EOF token is both a termination signal and a guard against the parser running off the end of the token list.\n<strong>3. Skipping whitespace but forgetting to update column</strong>\nIf you manually increment <code>self.current</code> to skip whitespace (bypassing <code>advance()</code>), newlines will not be detected and <code>self.line</code> will stay at 1 forever. Always use <code>self.advance()</code> â€” even for whitespace.\n<strong>4. Windows line endings counting as two newlines</strong>\nThe sequence <code>\\r\\n</code> should produce exactly one line increment. Since only <code>\\n</code> increments <code>self.line</code> in the code above, and <code>\\r</code> is consumed as whitespace (column increments by 1, then resets when <code>\\n</code> follows), this is handled correctly â€” but only if <code>\\r</code> is in your whitespace set. Verify this case with a test.\n<strong>5. Column not resetting to 1 â€” resetting to 0</strong>\nAfter a newline, the next character is at column 1 (one-indexed). Many implementations accidentally set <code>self.column = 0</code> in the newline branch of <code>advance()</code>, then increment to 1 when the next character is consumed. This works but requires careful reasoning. Setting <code>self.column = 1</code> directly in the newline branch and <em>not</em> incrementing in the same <code>advance()</code> call (since the <code>\\n</code> itself is not on the next line) is cleaner. The code above does this correctly by setting <code>self.column = 1</code> when <code>char == &quot;\\n&quot;</code> â€” this means the <em>next</em> character consumed will see <code>self.column = 1</code> as a starting point, then the <code>else: self.column += 1</code> branch will increment it to 2... \nWait â€” re-examine the code carefully. When <code>advance()</code> processes <code>\\n</code>, it sets <code>self.column = 1</code>. The <code>\\n</code> is consumed. The <em>next</em> call to <code>advance()</code> will process the first character of the new line. In that call, <code>char != &quot;\\n&quot;</code>, so it takes the <code>else: self.column += 1</code> branch, making column = 2. But the first character of the new line should be at column 1!\nThis is a real off-by-one bug. The fix: <code>self.column</code> should represent the column of the <em>next character to be consumed</em>, and <code>tok_col = self.column</code> captures the column of the current character <em>before</em> advancing. So when <code>\\n</code> is processed, we want the next character&#39;s column to be 1. Setting <code>self.column = 1</code> achieves this correctly â€” the next <code>advance()</code> call will read column 1, then increment to 2. But <code>tok_col</code> is captured <em>before</em> <code>advance()</code> is called, so the next token will correctly record column = 1.\nLet us trace it precisely:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>State after newline processed:  line=2, column=1\n                                                 â†‘ &quot;next char is at col 1&quot;\nnext call to next_token():\n  _skip_whitespace() â€” suppose no whitespace\n  tok_col = self.column  â†’  tok_col = 1         âœ“ correct!\n  char = self.advance()\n    â†’ char = 'x' (first char on line 2)\n    â†’ column becomes 2                           âœ“ (column now points PAST 'x')\n  â†’ token emitted with column = 1               âœ“ correct!</code></pre></div>\n<h2 id=\"the-invariant-is-selfcolumn-is-always-the-column-of-the-character-that-advance-will-consume-next-not-the-one-just-consumed-this-is-consistent-with-selfcurrent-being-the-index-of-the-next-character-to-consume-both-are-quotone-aheadquot-pointers\">The invariant is: <strong><code>self.column</code> is always the column of the character that <code>advance()</code> will consume next, not the one just consumed.</strong> This is consistent with <code>self.current</code> being the index of the next character to consume. Both are &quot;one ahead&quot; pointers.</h2>\n<h2 id=\"knowledge-cascade-one-concept-ten-unlocks\">Knowledge Cascade: One Concept, Ten Unlocks</h2>\n<p>You just built a finite state machine that reads one character at a time and emits structured output. Here is what this unlocks:\n<strong>1. Regex engines are scanners in disguise.</strong> Every regular expression engine implements the same FSM model you just built, but auto-generates the state machine from a pattern description. Now that you understand the underlying model, regex is no longer magic â€” it is a compiled FSM where the scanner&#39;s <code>next_token()</code> loop is the FSM execution loop. Non-deterministic finite automata (NFAs) are the theoretical model; your scanner is a hand-coded deterministic finite automaton (DFA).\n<strong>2. Stream processors work identically.</strong> Apache Kafka consumers, Python generators, Unix pipe filters â€” all of these process data one element at a time, maintain local state, and emit output. Your scanner is O(n) in time and O(1) in auxiliary memory (ignoring the output list). This is the same property that makes stream processors able to handle infinite data: they never buffer the whole input.\n<strong>3. Position metadata enables IDE tooling.</strong> Every feature in your IDE that involves source positions â€” error underlines (squiggles), &quot;go to definition,&quot; hover documentation, inline type hints â€” is built on exactly the <code>(line, column)</code> pairs you are generating right now. The Language Server Protocol (LSP), which powers VS Code&#39;s language support, defines its entire API in terms of <code>{line, character}</code> positions. You are generating the raw material.\n<strong>4. The EOF sentinel appears everywhere.</strong> The pattern of using a sentinel value to signal &quot;no more data&quot; appears in: SQL <code>NULL</code>, TCP&#39;s <code>FIN</code> flag (connection termination), Unix&#39;s <code>EOF</code> byte (<code>Ctrl+D</code>), C&#39;s <code>null</code> terminator in strings, and Python&#39;s <code>StopIteration</code> exception in generators. The common insight is: give the consumer a guaranteed terminal signal rather than making it check a boolean &quot;are we done?&quot; flag separately. Your EOF token does exactly this for the parser.\n<strong>5. Error recovery as a design choice.</strong> Your scanner emits an <code>ERROR</code> token and <em>continues</em> rather than raising an exception and halting. This is called <strong>panic mode recovery</strong> in compiler design â€” it is a deliberate choice to let the scanner (and later the parser) report as many errors as possible in one compilation run, rather than stopping at the first problem. This trade-off (more errors reported vs. possibility of cascading false positives) is one of the fundamental design decisions in compiler error handling. You just made it, probably without realizing it was a choice.</p>\n<blockquote>\n<p>ðŸ”­ <strong>Deep Dive</strong>: If you want to understand FSMs formally â€” including how NFA-to-DFA conversion works and how this relates to regex compilation â€” see <em>Introduction to the Theory of Computation</em> by Michael Sipser, Chapter 1 (Finite Automata). It is the clearest mathematical treatment of the theory underlying everything you are building.</p>\n</blockquote>\n<hr>\n<h2 id=\"summary-what-you-have-built\">Summary: What You Have Built</h2>\n<p>By completing this milestone, you have built a working character-level scanning foundation. Specifically:</p>\n<ul>\n<li>A <code>TokenType</code> enumeration covering every token category in your C-like language, with individually named operator and punctuation variants</li>\n<li>A <code>Token</code> dataclass that carries type, lexeme (raw source text), and precise source position (line and column, 1-indexed)</li>\n<li>A <code>Scanner</code> class with <code>peek()</code> (non-consuming look-ahead), <code>advance()</code> (consuming with position tracking), and <code>is_at_end()</code> (boundary guard)</li>\n<li>Position tracking that correctly handles newlines (incrementing line, resetting column) and Windows line endings (<code>\\r\\n</code> counts as one newline)</li>\n<li>Single-character token recognition via a dictionary dispatch table â€” O(1) per character</li>\n<li>Silent whitespace consumption â€” spaces, tabs, carriage returns, and newlines are consumed without producing tokens</li>\n<li>EOF sentinel emission â€” the final token in every scan is always <code>EOF</code></li>\n<li>Error token emission for unrecognized characters â€” scanning continues after the error (error recovery)\nIn Milestone 2, you will extend <code>next_token()</code> to handle multi-character tokens: two-character operators like <code>==</code> and <code>&gt;=</code>, number literals, and identifiers â€” applying the <strong>maximal munch</strong> principle to always prefer the longest matching token.</li>\n</ul>\n<hr>\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m2 -->\n<!-- MS_ID: tokenizer-m2 -->\n<h1 id=\"milestone-2-multi-character-tokens-amp-maximal-munch\">Milestone 2: Multi-Character Tokens &amp; Maximal Munch</h1>\n<h2 id=\"where-you-are-in-the-pipeline\">Where You Are in the Pipeline</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-satellite-map.svg\" alt=\"Tokenizer System Satellite Map â€” The Complete Pipeline\"></p>\n<h2 id=\"in-milestone-1-you-built-the-engine-a-character-consuming-scanner-that-can-recognize-single-characters-and-track-its-position-through-source-text-every-character-your-scanner-touches-is-visited-exactly-once-left-to-right-with-no-going-back-now-you-face-the-first-real-challenge-of-language-design-multi-character-tokens-the-characters-gt-and-each-mean-something-on-their-own-greater-assign-but-together-in-that-exact-order-they-mean-something-different-greater_eq-how-does-your-scanner-which-reads-one-character-at-a-time-decide-which-interpretation-is-correct-this-milestone-answers-that-question-with-a-principle-called-maximal-munch-always-consume-as-many-characters-as-possible-for-the-current-token-you-will-apply-it-to-two-character-operators-number-literals-and-identifiers-by-the-end-your-scanner-will-handle-the-full-vocabulary-of-your-c-like-language-except-for-strings-and-comments-those-come-in-milestone-3\">In Milestone 1, you built the engine: a character-consuming scanner that can recognize single characters and track its position through source text. Every character your scanner touches is visited exactly once, left to right, with no going back.\nNow you face the first real challenge of language design: <strong>multi-character tokens</strong>. The characters <code>&gt;</code> and <code>=</code> each mean something on their own (<code>GREATER</code>, <code>ASSIGN</code>). But together, in that exact order, they mean something different (<code>GREATER_EQ</code>). How does your scanner â€” which reads one character at a time â€” decide which interpretation is correct?\nThis milestone answers that question with a principle called <strong>maximal munch</strong>: <em>always consume as many characters as possible for the current token.</em> You will apply it to two-character operators, number literals, and identifiers. By the end, your scanner will handle the full vocabulary of your C-like language except for strings and comments (those come in Milestone 3).</h2>\n<h2 id=\"the-revelation-tokenization-is-not-pattern-matching\">The Revelation: Tokenization Is Not Pattern Matching</h2>\n<p>Here is the mental model that most developers arrive with when they first think about multi-character tokens:</p>\n<blockquote>\n<p><em>&quot;My scanner should look at the source text, try all the patterns it knows (like <code>==</code>, <code>&gt;=</code>, <code>!=</code>), and pick the one that matches best at the current position.&quot;</em>\nThis search-and-match framing is natural â€” it is how you would think about the problem if you were searching a document for a word. And it sounds reasonable enough. So let us follow it to its logical consequence and see where it breaks.\nSuppose your language had a token <code>&lt;==</code> (less-than-or-equal followed by assignment, hypothetically). And you are at position where the source reads <code>&lt;==</code>. The search-and-match approach says: try <code>&lt;==</code> first (length 3), then <code>&lt;=</code> (length 2), then <code>&lt;</code> (length 1). Pick the longest match.\nNow imagine doing this for every character. You need to try every pattern, every time, at every position. If you have 30 token types, you try 30 patterns at each character. For a 10,000-line source file with ~300,000 characters, that is potentially 9 million pattern trials. And you would need regex or string matching for each trial.\nBut the real problem is deeper: <strong>this framing is architecturally wrong</strong>. It treats tokenization as a repeated search â€” &quot;find the next token starting here&quot; â€” which implies that you could look arbitrarily far ahead to determine what the next token is. Some tokens could require looking 10 characters ahead, or 100. The search cost grows with the length of the longest pattern.\n<strong>The actual model is different.</strong> Your scanner does not search. It <em>consumes</em>. The moment <code>advance()</code> is called, a decision has been made: this character is part of the current token. The question is only ever: &quot;given what I have consumed so far, should I consume one more character, or stop?&quot;\nThis is the greedy consumption model. At each step, the scanner asks a single, local question: <em>&quot;Should I consume the next character too?&quot;</em> The answer is always one character of lookahead â€” <code>peek()</code>. If the answer is yes, consume and ask again. If no, stop and emit the token.\n<strong>This is why tokenizers need zero backtracking for well-designed languages.</strong> The decision to consume a character is never reversed. There is no &quot;oops, I should not have taken that character.&quot; The language is designed so that greedy consumption always produces the correct result.\nðŸ”‘ <strong>The Insight: &quot;Maximal Munch Is Greedy&quot;</strong></p>\n<p><strong>Maximal munch</strong> is a specific greedy algorithm: among all valid tokens that could start at the current position, always take the longest one. It is implemented not by trying multiple patterns, but by consuming characters one at a time and stopping only when the next character would not extend the current token. Single character of lookahead. No search. No backtracking.</p>\n<p><strong>Why &quot;greedy&quot;?</strong> In algorithm design, a greedy algorithm makes the locally optimal choice at each step without reconsidering previous decisions. Maximal munch makes the locally longest choice: &quot;can I take one more character?&quot; If yes, take it. This is the same principle as the greedy interval scheduling algorithm (take the earliest-ending interval), Huffman coding (always merge the two smallest trees), or TCP&#39;s Nagle algorithm (send when buffer is full, not before). Greedy works here because the language is designed to make it work â€” no valid tokenization requires the scanner to prefer a shorter match over a longer one.</p>\n</blockquote>\n<hr>\n<h2 id=\"two-character-operators-lookahead-in-practice\">Two-Character Operators: Lookahead in Practice</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m2-maximal-munch-decision.svg\" alt=\"Maximal Munch Decision Tree for Operators\"></p>\n<p>Your C-like language has these paired operators, where one character alone is a valid token and two characters together form a different valid token:</p>\n<table>\n<thead>\n<tr>\n<th>First Char</th>\n<th>Alone â†’ Token</th>\n<th>With <code>=</code> â†’ Token</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>=</code></td>\n<td><code>ASSIGN</code></td>\n<td><code>EQUAL</code> (<code>==</code>)</td>\n</tr>\n<tr>\n<td><code>!</code></td>\n<td><code>BANG</code></td>\n<td><code>NOT_EQUAL</code> (<code>!=</code>)</td>\n</tr>\n<tr>\n<td><code>&lt;</code></td>\n<td><code>LESS</code></td>\n<td><code>LESS_EQ</code> (<code>&lt;=</code>)</td>\n</tr>\n<tr>\n<td><code>&gt;</code></td>\n<td><code>GREATER</code></td>\n<td><code>GREATER_EQ</code> (<code>&gt;=</code>)</td>\n</tr>\n<tr>\n<td>In Milestone 1, your <code>next_token()</code> used a dictionary lookup for single-character tokens. That approach breaks here because by the time you know you have seen <code>&gt;</code>, you do not yet know if it should be <code>GREATER</code> or <code>GREATER_EQ</code>. You must look at the character <em>after</em> <code>&gt;</code> without consuming it.</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>This is exactly what <code>peek()</code> is for.</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>The pattern is always the same:</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<ol>\n<li><code>advance()</code> consumes the first character (e.g., <code>&gt;</code>).</li>\n<li><code>peek()</code> inspects the <em>next</em> character without consuming it.</li>\n<li>If <code>peek()</code> returns <code>=</code>, call <code>advance()</code> again (consuming <code>=</code>) and emit the two-character token.</li>\n<li>Otherwise, emit the one-character token â€” <code>peek()</code> is not consumed, and the next call to <code>next_token()</code> will see it fresh.\nHere is a helper method that encodes this pattern:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _match</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    If the next character equals `expected`, consume it and return True.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Otherwise, leave it unconsumed and return False.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Consume the character â€” update position tracking</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n<p><code>_match()</code> is the workhorse of two-character operator recognition. It is peek-and-consume in one operation: it only consumes if the next character is what you expected. If it returns <code>True</code>, the two-character token is complete. If <code>False</code>, the single-character token stands alone.\nNow the operator scanning logic becomes almost readable as English:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_operator</span><span style=\"color:#E1E4E8\">(self, char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Scan an operator character, applying maximal munch for two-char variants.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    `char` is already consumed. `tok_line` and `tok_col` are start position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"=\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"==\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"!\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">BANG</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"&#x3C;\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LESS_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LESS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Should not reach here if called correctly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span></code></pre></div>\n<p>Notice the lexeme argument. When you emit <code>Token(TokenType.EQUAL, &quot;==&quot;, ...)</code>, you construct the lexeme <code>&quot;==&quot;</code> explicitly â€” you are not going back to re-read the source. You know the lexeme because you know exactly which characters you consumed. This is another consequence of the consume-as-you-go model: the lexeme is always exactly the characters you <code>advance()</code>-d past.</p>\n<h3 id=\"the-gt-trace-maximal-munch-in-slow-motion\">The <code>&gt;==</code> Trace: Maximal Munch in Slow Motion</h3>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m2-munch-trace.svg\" alt=\"Trace: Maximal Munch on '>==' Input\"></p>\n<p>Let us trace through what happens when your scanner encounters the input <code>&gt;==</code>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Source:   &gt; = =\n          ^\n          current=0, line=1, col=1\nnext_token() call #1:\n  _skip_whitespace()  â†’ nothing to skip\n  tok_line=1, tok_col=1\n  char = advance()    â†’ '&gt;', current=1, col=2\n  char == '&gt;'  â†’  call _match('=')\n    peek() â†’ '='  (source[1] == '=')  â†’ matches!\n    advance()  â†’ '=', current=2, col=3\n    return True\n  emit Token(GREATER_EQ, &quot;&gt;=&quot;, 1, 1)\nnext_token() call #2:\n  _skip_whitespace()  â†’ nothing to skip\n  tok_line=1, tok_col=3\n  char = advance()    â†’ '=', current=3, col=4\n  char == '='  â†’  call _match('=')\n    is_at_end()  â†’ True  (current=3 &gt;= len(&quot;&gt;===&quot;)==3)\n    return False\n  emit Token(ASSIGN, &quot;=&quot;, 1, 3)\nnext_token() call #3:\n  is_at_end()  â†’ True\n  emit Token(EOF, &quot;&quot;, 1, 4)</code></pre></div>\n<h2 id=\"result-greater_eqquotgtquot-11-assignquotquot-13-eofquotquot-14-this-is-maximal-munch-in-action-when-the-scanner-sees-gt-it-greedily-consumed-the-that-followed-it-did-not-quotaskquot-whether-gt-should-be-followed-by-another-to-form-gt-which-is-not-a-token-in-this-language-it-simply-asked-quotshould-i-take-one-more-characterquot-at-each-step-and-stopped-as-soon-as-the-answer-was-no-the-second-is-left-for-the-next-call-to-next_token-which-correctly-scans-it-as-assign\">Result: <code>[GREATER_EQ(&quot;&gt;=&quot;, 1:1), ASSIGN(&quot;=&quot;, 1:3), EOF(&quot;&quot;, 1:4)]</code>\nThis is maximal munch in action. When the scanner sees <code>&gt;</code>, it greedily consumed the <code>=</code> that followed. It did not &quot;ask&quot; whether <code>&gt;=</code> should be followed by another <code>=</code> to form <code>&gt;==</code> (which is not a token in this language). It simply asked &quot;should I take one more character?&quot; at each step, and stopped as soon as the answer was no. The second <code>=</code> is left for the next call to <code>next_token()</code>, which correctly scans it as <code>ASSIGN</code>.</h2>\n<h2 id=\"integrating-operator-scanning-into-next_token\">Integrating Operator Scanning into next_token()</h2>\n<p>Your <code>next_token()</code> from Milestone 1 handled single-character tokens via a dictionary. The two-character operators need special handling â€” they cannot be in the dictionary because the decision depends on the <em>next</em> character, which is not yet consumed.\nThe cleanest approach: remove the ambiguous characters from <code>SINGLE_CHAR_TOKENS</code> and handle them explicitly in <code>next_token()</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Updated SINGLE_CHAR_TOKENS â€” unambiguous single-char tokens only</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"+\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"-\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"*\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">STAR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '/' removed â€” will be handled for comments in Milestone 3</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '=' removed â€” could be ASSIGN or part of EQUAL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '!' removed â€” could be BANG or part of NOT_EQUAL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '&#x3C;' removed â€” could be LESS or part of LESS_EQ</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '>' removed â€” could be GREATER or part of GREATER_EQ</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"(\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \")\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"{\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"}\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"[\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"]\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \";\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \",\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">COMMA</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Characters that start two-character operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">OPERATOR_CHARS</span><span style=\"color:#E1E4E8\">: frozenset[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> frozenset</span><span style=\"color:#E1E4E8\">({</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">\"</span><span style=\"color:#E1E4E8\">})</span></span></code></pre></div>\n<p>And <code>next_token()</code> extended:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> next_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">._skip_whitespace()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Single-character tokens (unambiguous)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(</span><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char], char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Two-character operators (maximal munch with peek)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> OPERATOR_CHARS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_operator(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Number literals</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Identifiers and keywords</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Unrecognized â€” error token, continue scanning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span></code></pre></div>\n<h2 id=\"notice-the-dispatch-order-matters-digits-before-identifiers-so-4abc-is-scanned-as-a-number-then-an-identifier-not-a-single-broken-token-and-the-operator-check-before-the-fallthrough-error-case-also-note-that-is-not-yet-in-single_char_tokens-in-milestone-3-a-might-start-a-comment-or-so-it-needs-the-same-two-character-lookahead-treatment-as-and-gt-for-now-if-you-want-to-test-division-you-can-temporarily-add-it-back-the-milestone-3-implementation-will-replace-it\">Notice the dispatch order matters: digits before identifiers (so <code>4abc</code> is scanned as a number then an identifier, not a single broken token), and the operator check before the fallthrough error case.\nAlso note that <code>/</code> is not yet in <code>SINGLE_CHAR_TOKENS</code>. In Milestone 3, a <code>/</code> might start a comment (<code>//</code> or <code>/*</code>) â€” so it needs the same two-character lookahead treatment as <code>=</code> and <code>&gt;</code>. For now, if you want to test division, you can temporarily add it back. The milestone 3 implementation will replace it.</h2>\n<h2 id=\"number-literals-the-state-machine-inside-_scan_number\">Number Literals: The State Machine Inside _scan_number()</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m2-number-scanning-fsm.svg\" alt=\"Number Literal Scanner â€” State Machine\"></p>\n<p>Number literals in your C-like language come in two forms:</p>\n<ul>\n<li><strong>Integer</strong>: one or more digits â€” <code>0</code>, <code>42</code>, <code>1000</code></li>\n<li><strong>Float</strong>: digits, a single decimal point, digits â€” <code>3.14</code>, <code>0.5</code>, <code>100.0</code>\nBoth begin with a digit. Your scanner has already consumed the first digit (it was used to recognize this as a number in <code>next_token()</code>). Now <code>_scan_number()</code> receives that first character and must consume the rest.\nThe scanning logic follows a simple two-state machine:\n<strong>State 1 â€” INTEGER</strong>: Consume digits until you see something that is not a digit. If that non-digit is <code>.</code> and the character <em>after</em> the dot is also a digit, transition to the float state. Otherwise, emit an integer token.\n<strong>State 2 â€” FLOAT</strong>: After consuming the <code>.</code>, continue consuming digits until you see something that is not a digit. Emit a float token.</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_number</span><span style=\"color:#E1E4E8\">(self, first_digit: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Scan a number literal. `first_digit` is already consumed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Handles both integers (42) and floats (3.14).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> first_digit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Consume remaining integer digits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for decimal point followed by digits (float)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \".\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._peek_next().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the '.'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Consume fractional digits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, lexeme, tok_line, tok_col)</span></span></code></pre></div>\n<p>This requires a second lookahead method, <code>_peek_next()</code>, which looks <em>two</em> characters ahead:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _peek_next</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Look two characters ahead without consuming. Returns '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">0' if not available.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> >=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n<blockquote>\n<p><strong>Why do we need two-character lookahead here?</strong> When the scanner is in the INTEGER state and sees a <code>.</code>, it needs to decide: is this a decimal point in a float literal, or a <code>.</code> that belongs to something else (like a method call, or a range operator in another language)? With only one character of lookahead (peek), you see <code>.</code> â€” but you don&#39;t know if it&#39;s <code>3.14</code> (float) or <code>3.toString()</code> (integer followed by method access). Looking one more character ahead to check if the character <em>after</em> the dot is a digit resolves the ambiguity without consuming anything. If the next-next character is not a digit, the <code>.</code> is left alone for the next <code>next_token()</code> call.</p>\n<p>This is a deliberate design point: <strong>most languages need no more than one or two characters of lookahead in the lexer</strong>. If your language required five characters of lookahead to tokenize, that would signal a design problem â€” it means tokens are ambiguous from their first few characters and the grammar is harder to reason about.</p>\n</blockquote>\n<h3 id=\"edge-cases-to-decide-explicitly\">Edge Cases to Decide Explicitly</h3>\n<p>Number scanning has a few cases where the spec is silent and you must make a choice:</p>\n<table>\n<thead>\n<tr>\n<th>Input</th>\n<th>Question</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>3.</code></td>\n<td>Trailing dot â€” is this <code>3.</code> or <code>3</code> + <code>.</code>?</td>\n<td>Scan as integer <code>3</code>, leave <code>.</code> for next token. The <code>_peek_next().isdigit()</code> check handles this â€” if nothing follows the dot, don&#39;t consume it.</td>\n</tr>\n<tr>\n<td><code>.5</code></td>\n<td>Leading dot â€” is this <code>.5</code> or <code>.</code> + <code>5</code>?</td>\n<td>Not a number: <code>.</code> is not a digit, so <code>_scan_number</code> is never triggered. <code>.</code> will likely be an error token. Document this decision.</td>\n</tr>\n<tr>\n<td><code>3.14.15</code></td>\n<td>Two dots â€” invalid float?</td>\n<td>Scan <code>3.14</code> as a float, then <code>.</code> as an error or punctuation, then <code>15</code> as an integer. Your scanner naturally produces this: after consuming <code>3.14</code>, the next <code>peek()</code> is <code>.</code>, which is not a digit, so the float scan stops.</td>\n</tr>\n<tr>\n<td><code>42abc</code></td>\n<td>Digit followed by letter</td>\n<td>Scan <code>42</code> as NUMBER, then <code>abc</code> as IDENTIFIER. The identifier check in <code>next_token()</code> will pick up <code>a</code> on the next call.</td>\n</tr>\n<tr>\n<td>The key rule: <strong>make a choice, document it in a comment, and write a test for it</strong>. Undefined behavior is the enemy.</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"identifiers-the-scan-then-lookup-pattern\">Identifiers: The Scan-Then-Lookup Pattern</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m2-keyword-vs-identifier.svg\" alt=\"Identifier Scanning + Keyword Table Lookup\"></p>\n<p>Identifiers in your language follow a simple rule: start with a letter or underscore, followed by zero or more letters, digits, or underscores. Examples: <code>x</code>, <code>myVar</code>, <code>_count</code>, <code>item2</code>.\nKeywords â€” <code>if</code>, <code>else</code>, <code>while</code>, <code>return</code>, <code>true</code>, <code>false</code>, <code>null</code> â€” look exactly like identifiers when you first encounter them character-by-character. The character sequence <code>i</code>, <code>f</code> is indistinguishable from the beginning of <code>iffy</code> or <code>if_x</code>. You cannot know whether you are scanning a keyword until you have consumed all its characters.\n<strong>The wrong approach:</strong> try to match keywords character-by-character in the scanner&#39;s state machine. This means adding states like <code>IN_KEYWORD_IF_SEEN_I</code>, <code>IN_KEYWORD_IF_SEEN_IF</code>. With 7 keywords, you&#39;d add dozens of states. The state machine becomes unmanageable. Production compilers like GCC&#39;s original scanner tried this â€” it was abandoned.\n<strong>The right approach:</strong> scan the entire identifier first, then look up the result in a table. This is the <strong>scan-then-lookup</strong> pattern. It has two steps:</p>\n<ol>\n<li>Consume all characters that are valid identifier characters (letter, digit, underscore). This produces the full lexeme.</li>\n<li>Look up the lexeme in a <code>KEYWORDS</code> dictionary. If found, emit a <code>KEYWORD</code> token. If not found, emit an <code>IDENTIFIER</code> token.</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">KEYWORDS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"if\"</span><span style=\"color:#E1E4E8\">:     TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"else\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"while\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"return\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"true\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"false\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"null\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_identifier</span><span style=\"color:#E1E4E8\">(self, first_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Scan an identifier or keyword. `first_char` is already consumed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns KEYWORD if the lexeme matches a reserved word, IDENTIFIER otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> first_char</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Consume all valid identifier characters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Keyword lookup â€” O(1) hash table probe</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> KEYWORDS</span><span style=\"color:#E1E4E8\">.get(lexeme, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(token_type, lexeme, tok_line, tok_col)</span></span></code></pre></div>\n<p>The lookup <code>KEYWORDS.get(lexeme, TokenType.IDENTIFIER)</code> is clean Python: it returns <code>TokenType.IDENTIFIER</code> as the default if the key is not found. One line, O(1), no if/else needed.</p>\n<h3 id=\"why-the-lookup-table-avoids-a-classic-bug\">Why the Lookup Table Avoids a Classic Bug</h3>\n<p>Consider the identifier <code>iffy</code>. If you tried to match the keyword <code>if</code> character-by-character, your scanner might see <code>i</code>, <code>f</code> and think &quot;keyword <code>if</code> found!&quot; â€” before noticing there are more characters. This is called a <strong>prefix match bug</strong>. The scan-then-lookup approach is immune: you consume <code>i</code>, <code>f</code>, <code>f</code>, <code>y</code> as a complete identifier, then look up <code>&quot;iffy&quot;</code> in the keyword table â€” which returns nothing â€” and correctly emit <code>IDENTIFIER(&quot;iffy&quot;)</code>.\nThe rule: <strong>keyword recognition must operate on the complete lexeme, never on a prefix.</strong> The lookup table enforces this naturally.</p>\n<h3 id=\"keyword-table-design-two-variants\">Keyword Table Design: Two Variants</h3>\n<p>Your implementation above stores all keywords with the same value (<code>TokenType.KEYWORD</code>). This means the keyword&#39;s identity lives in the lexeme: <code>token.type == KEYWORD</code> and <code>token.lexeme == &quot;if&quot;</code>. The parser will check both.\nAn alternative: give each keyword its own token type (<code>IF</code>, <code>ELSE</code>, <code>WHILE</code>, <code>RETURN</code>, <code>TRUE</code>, <code>FALSE</code>, <code>NULL</code>). Then the parser checks <code>token.type == TokenType.IF</code> without examining the lexeme.</p>\n<table>\n<thead>\n<tr>\n<th>Design</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Used By</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Single KEYWORD type + lexeme</strong> âœ“</td>\n<td>Fewer enum variants, easy to add new keywords</td>\n<td>Parser must check lexeme string</td>\n<td>CPython tokenizer, many small language runtimes</td>\n</tr>\n<tr>\n<td>Per-keyword token types</td>\n<td>Parser logic cleaner, no string comparison</td>\n<td>Enum grows with language, more boilerplate</td>\n<td>Go compiler, Clang, LLVM tools</td>\n</tr>\n<tr>\n<td>For this project, the single <code>KEYWORD</code> type is fine. The acceptance criteria specify it explicitly. If you were building a production compiler with a full parser, per-keyword types would be the right call â€” parsers matching on <code>TokenType.IF</code> are faster and clearer than parsers matching on <code>token.type == KEYWORD and token.lexeme == &quot;if&quot;</code>.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>ðŸ”­ <strong>Deep Dive</strong>: Go&#39;s scanner uses the scan-then-lookup pattern with per-keyword token types. You can read the exact implementation in <code>go/src/go/scanner/scanner.go</code> in the Go standard library â€” search for the <code>Lookup</code> function. It&#39;s under 50 lines and maps string â†’ token type via a plain array (since Go&#39;s keywords are few and densely packed). The design philosophy is described in <em>The Go Programming Language</em> (Donovan &amp; Kernighan), Chapter 1.</p>\n</blockquote>\n<hr>\n<h2 id=\"extended-fsm-what-your-scanner-now-looks-like\">Extended FSM: What Your Scanner Now Looks Like</h2>\n<p>{{DIAGRAM:diag-m2-fsm-extended}}\nAfter Milestone 1, your scanner had one effective &quot;state&quot; â€” <code>START</code> â€” from which it dispatched on single characters. Now it has four meaningful scanning states:</p>\n<ul>\n<li><strong>START</strong> â€” between tokens, deciding what the next token is based on the first character</li>\n<li><strong>IN_NUMBER</strong> â€” consuming digit characters (or a decimal point + more digits)</li>\n<li><strong>IN_IDENTIFIER</strong> â€” consuming alphanumeric/underscore characters</li>\n<li><strong>IN_OPERATOR</strong> â€” peeking at a second character to apply maximal munch\nThese states are not explicit enum values in your code â€” they are implicit in which scanning function is currently executing. <code>_scan_number()</code> represents the IN_NUMBER state. <code>_scan_identifier()</code> represents the IN_IDENTIFIER state. The state machine&#39;s &quot;state&quot; is the call stack.\nThis is called a <strong>recursive descent</strong> approach to scanning: instead of an explicit state enum with a transition table, you use function calls to represent state transitions. It is easier to write and debug, and it is exactly how production scanners in compilers like GCC, Clang, and Go work.</li>\n</ul>\n<hr>\n<h2 id=\"the-complete-milestone-2-scanner\">The Complete Milestone 2 Scanner</h2>\n<p>Here is <code>scanner.py</code> with all Milestone 2 additions integrated:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PLUS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MINUS</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STAR</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SLASH</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUAL</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ==</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUAL</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQ</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQ</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># >=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BANG</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LPAREN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RPAREN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RBRACE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACKET</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RBRACKET</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEMICOLON</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sentinels</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"+\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"-\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"*\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">STAR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"(\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \")\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"{\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"}\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"[\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"]\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \";\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \",\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">COMMA</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">OPERATOR_CHARS</span><span style=\"color:#E1E4E8\">: frozenset[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> frozenset</span><span style=\"color:#E1E4E8\">({</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">\"</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">WHITESPACE</span><span style=\"color:#E1E4E8\">: frozenset[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> frozenset</span><span style=\"color:#E1E4E8\">({</span><span style=\"color:#9ECBFF\">\" \"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">KEYWORDS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"if\"</span><span style=\"color:#E1E4E8\">:     TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"else\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"while\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"return\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"true\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"false\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"null\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Primitive Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _peek_next</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look two characters ahead without consuming.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> >=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _match</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume next char if it equals `expected`. Return True if consumed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> WHITESPACE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Scanning Methods â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_operator</span><span style=\"color:#E1E4E8\">(self, char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"=\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"==\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"!\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">BANG</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"&#x3C;\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LESS_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LESS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_number</span><span style=\"color:#E1E4E8\">(self, first_digit: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> first_digit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Integer part</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Optional fractional part: dot followed by at least one digit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \".\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._peek_next().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()   </span><span style=\"color:#6A737D\"># consume '.'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, lexeme, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_identifier</span><span style=\"color:#E1E4E8\">(self, first_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> first_char</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        token_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> KEYWORDS</span><span style=\"color:#E1E4E8\">.get(lexeme, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(token_type, lexeme, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Token Production â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> next_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._skip_whitespace()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tok_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tok_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(</span><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char], char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> OPERATOR_CHARS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_operator(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens: list[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tokens</span></span></code></pre></div>\n<hr>\n<h2 id=\"lookahead-as-a-language-design-constraint\">Lookahead as a Language Design Constraint</h2>\n<p>You have now used two different amounts of lookahead in this milestone:</p>\n<ul>\n<li><strong>One character</strong> (<code>peek()</code>) â€” for two-character operators</li>\n<li><strong>Two characters</strong> (<code>peek()</code> + <code>_peek_next()</code>) â€” for deciding whether a <code>.</code> after an integer starts a float\nThis is not accidental. The amount of lookahead a scanner needs is a property of the <em>language being tokenized</em>, not of the scanner implementation. And that amount has a direct connection to parser theory.\nA scanner that needs at most <em>k</em> characters of lookahead is called an <strong>LL(k) scanner</strong> (left-to-right, leftmost derivation, k tokens ahead). Most well-designed languages are LL(1) at the lexical level â€” one character of lookahead suffices. Your two-character lookahead for floats is technically LL(2) for that one case, but it is so localized that it has no practical impact.\nWhy does this matter? Because the number of lookahead characters determines how predictably the scanner can operate. An LL(1) scanner never has to &quot;wait&quot; to decide â€” it always knows the current token&#39;s type from at most the next character. A scanner requiring unbounded lookahead would need to buffer arbitrarily much input before emitting any token â€” not compatible with streaming or incremental parsing.<blockquote>\n<p><strong>The design principle</strong>: When you design a language (or a DSL), keep the lexical grammar LL(1). If you find yourself needing more than one or two characters of lookahead at the scanner level, reconsider the token design. The cost of getting this wrong is not just implementation complexity â€” it is that your scanner can no longer be a clean, stateless, streaming component.\nThis principle directly constrains language design. C++ famously requires complex, context-sensitive tokenization because <code>&gt;&gt;</code> means both &quot;right shift&quot; and &quot;end of two nested templates.&quot; The lexer cannot know which without parser context. This is called <strong>maximal munch ambiguity</strong> â€” and C++ compilers have to special-case it. A well-designed language avoids this by choosing operator syntax that is unambiguous from the first character.</p>\n</blockquote>\n</li>\n</ul>\n<hr>\n<h2 id=\"testing-milestone-2\">Testing Milestone 2</h2>\n<p>Tests in this milestone must verify not just token types but also the exact token stream ordering and positions. Every test should be a complete, pinned scenario.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_equal_operator</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"==\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"==\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_assign_not_equal</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Single '=' must NOT consume the next non-'=' character</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"=+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_greater_equal</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_maximal_munch_gee_assign</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '>== ' must tokenize as GREATER_EQ, ASSIGN (not GREATER, EQUAL)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\">==\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_not_equal</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"!=\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_bang_alone</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '!' not followed by '=' is BANG</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"! \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">BANG</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_less_and_less_eq</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"&#x3C; &#x3C;=\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LESS</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LESS_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_integer_literal</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_integer_zero</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_float_literal</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"3.14\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"3.14\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_float_with_leading_zero</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"0.5\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"0.5\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_integer_no_trailing_dot</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '3.' should scan as INTEGER '3', leaving '.' for next token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"3.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"3\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '.' is not a digit start â€” it becomes an error or punctuation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \".\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_number_followed_by_identifier</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '42abc' â†’ NUMBER(42), IDENTIFIER(abc)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"42abc\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"abc\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_identifier_simple</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"myVar\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"myVar\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_identifier_with_underscore</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"_count\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"_count\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_identifier_with_digits</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"item2\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"item2\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_keyword_if</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_keyword_return</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_keyword_not_a_prefix</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'iffy' must NOT match keyword 'if' â€” full lexeme lookup required</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"iffy\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"iffy\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_keyword_inside_identifier</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'if_x' contains 'if' as a prefix but is an identifier</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"if_x\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"if_x\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_all_keywords</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"if else while return true false null\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keyword_lexemes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.lexeme </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> keyword_lexemes </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"else\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"while\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"false\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_statement</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # The canonical acceptance test from the spec</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"if (x >= 42) { return true; }\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"(\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\")\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"{\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">14</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">23</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#9ECBFF\">\";\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">27</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"}\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">29</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected</span></span></code></pre></div>\n<p>The last test â€” <code>test_complete_statement</code> â€” is the canonical acceptance test for Milestone 2. It exercises every feature: keywords, identifiers, a two-character operator, a number literal, punctuation, and correct column positions. Run it last. If it passes, every other test will pass too.</p>\n<blockquote>\n<p><strong>Testing philosophy for scanners</strong>: The canonical test is more valuable than ten isolated tests. It catches interaction bugs â€” for example, a column tracking error that only shows up after processing whitespace after a two-character operator. Always include at least one full-sentence integration test.</p>\n</blockquote>\n<hr>\n<h2 id=\"design-decisions-the-string-accumulation-question\">Design Decisions: The String Accumulation Question</h2>\n<p>In <code>_scan_number()</code> and <code>_scan_identifier()</code>, you accumulate the lexeme character-by-character with string concatenation (<code>lexeme += self.advance()</code>). In Python, strings are immutable â€” each <code>+=</code> creates a new string object, copies the old content, and appends the new character. For a 30-character identifier, this means 30 string allocations.\nIs this a problem? Let&#39;s be quantitative.</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Implementation</th>\n<th>Cost per token</th>\n<th>Used by</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>String concat</strong> âœ“</td>\n<td><code>lexeme += char</code></td>\n<td>O(nÂ²) in worst case, small constant</td>\n<td>CPython&#39;s tokenizer for short tokens</td>\n</tr>\n<tr>\n<td>StringBuilder (<code>io.StringIO</code>)</td>\n<td><code>buf.write(char)</code> then <code>buf.getvalue()</code></td>\n<td>O(n) total</td>\n<td>Java scanners (Java&#39;s StringBuilder)</td>\n</tr>\n<tr>\n<td>Slice the source</td>\n<td><code>source[start:current]</code></td>\n<td>O(n) copy, but only one allocation</td>\n<td>Go&#39;s scanner, Clang&#39;s lexer</td>\n</tr>\n<tr>\n<td>For identifiers and numbers in real source code, typical token length is 1â€“20 characters. The O(nÂ²) behavior of string concatenation is O(400) operations in the worst case â€” completely negligible. The <code>+=</code> approach is readable, correct, and fast enough.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>The <strong>slice approach</strong> is worth knowing: instead of accumulating characters one by one, record <code>start = self.current</code> before the scan loop, then emit <code>Token(..., self.source[start:self.current], ...)</code> after the loop. This produces the lexeme in one O(n) allocation. Go&#39;s scanner uses this exact technique â€” it never accumulates characters into a buffer, it always produces lexemes by slicing the source string directly. For this project, the readability difference is small, so choose whichever you prefer.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"common-pitfalls\">Common Pitfalls</h2>\n<h2 id=\"1-consuming-the-character-that-terminates-a-token-the-most-common-bug-in-_scan_number-the-loop-condition-is-while-selfpeekisdigit-and-inside-the-loop-you-call-selfadvance-if-you-accidentally-write-while-selfadvanceisdigit-you-consume-the-terminating-character-eg-a-space-or-and-the-next-token-starts-in-the-wrong-place-always-peek-to-check-advance-to-consume-2-_match-skipping-position-tracking-if-you-implement-_match-by-directly-accessing-selfsourceselfcurrent-and-incrementing-selfcurrent-without-calling-selfadvance-you-bypass-newline-detection-this-is-safe-for-operators-which-are-never-newlines-but-it-is-a-trap-for-future-code-always-call-selfadvance-inside-_match-when-consuming-3-keyword-matching-39return39-inside-39returnvalue39-the-scan-then-lookup-pattern-avoids-this-_scan_identifier-consumes-until-a-non-identifier-character-is-found-producing-the-full-lexeme-quotreturnvaluequot-the-keyword-table-does-not-contain-quotreturnvaluequot-so-it-is-correctly-emitted-as-identifier-the-bug-only-appears-if-you-try-to-match-keywords-character-by-character-4-float-3-consuming-the-dot-when-it-shouldn39t-the-condition-if-selfpeek-quotquot-and-self_peek_nextisdigit-prevents-this-if-the-character-after-is-not-a-digit-the-dot-is-not-consumed-without-_peek_next-you-might-consume-the-dot-and-then-find-no-digits-leaving-you-with-the-malformed-lexeme-quot3quot-and-having-quotstolenquot-the-dot-from-whatever-follows-5-operator-position-captured-after-the-first-advance-remember-from-milestone-1-tok_line-and-tok_col-are-captured-before-advance-is-called-in-next_token-by-the-time-_scan_operator-is-called-the-first-character-is-already-consumed-and-the-position-is-already-captured-do-not-capture-position-inside-_scan_operator-the-positions-come-from-next_token-as-parameters-6-at-end-of-file-when-scanning-at-the-very-end-of-the-source-the-first-is-consumed-by-advance-then-_matchquotquot-is-called-_match-calls-is_at_end-which-returns-false-because-there-is-still-one-character-left-then-it-checks-selfsourceselfcurrent-quotquot-which-is-false-it-is-so-it-consumes-it-this-works-correctly-the-sentinel-quot0quot-from-peek-is-not-used-in-_match-_match-uses-is_at_end-and-direct-source-access-make-sure-_match-checks-is_at_end-first\"><strong>1. Consuming the character that terminates a token</strong>\nThe most common bug in <code>_scan_number()</code>: the loop condition is <code>while self.peek().isdigit()</code>, and inside the loop you call <code>self.advance()</code>. If you accidentally write <code>while self.advance().isdigit()</code>, you consume the terminating character (e.g., a space or <code>)</code>), and the next token starts in the wrong place. Always <code>peek()</code> to check, <code>advance()</code> to consume.\n<strong>2. <code>_match()</code> skipping position tracking</strong>\nIf you implement <code>_match()</code> by directly accessing <code>self.source[self.current]</code> and incrementing <code>self.current</code> without calling <code>self.advance()</code>, you bypass newline detection. This is safe for operators (which are never newlines), but it is a trap for future code. Always call <code>self.advance()</code> inside <code>_match()</code> when consuming.\n<strong>3. Keyword matching &#39;return&#39; inside &#39;returnValue&#39;</strong>\nThe scan-then-lookup pattern avoids this: <code>_scan_identifier()</code> consumes until a non-identifier character is found, producing the full lexeme <code>&quot;returnValue&quot;</code>. The keyword table does not contain <code>&quot;returnValue&quot;</code>, so it is correctly emitted as <code>IDENTIFIER</code>. The bug only appears if you try to match keywords character by character.\n<strong>4. Float <code>3.</code> consuming the dot when it shouldn&#39;t</strong>\nThe condition <code>if self.peek() == &quot;.&quot; and self._peek_next().isdigit()</code> prevents this. If the character after <code>.</code> is not a digit, the dot is not consumed. Without <code>_peek_next()</code>, you might consume the dot and then find no digits â€” leaving you with the malformed lexeme <code>&quot;3.&quot;</code> and having &quot;stolen&quot; the dot from whatever follows.\n<strong>5. Operator position captured after the first advance()</strong>\nRemember from Milestone 1: <code>tok_line</code> and <code>tok_col</code> are captured <em>before</em> <code>advance()</code> is called in <code>next_token()</code>. By the time <code>_scan_operator()</code> is called, the first character is already consumed and the position is already captured. Do not capture position inside <code>_scan_operator()</code> â€” the positions come from <code>next_token()</code> as parameters.\n<strong>6. <code>==</code> at end of file</strong>\nWhen scanning <code>==</code> at the very end of the source, the first <code>=</code> is consumed by <code>advance()</code>, then <code>_match(&quot;=&quot;)</code> is called. <code>_match</code> calls <code>is_at_end()</code> â€” which returns <code>False</code> because there is still one character left. Then it checks <code>self.source[self.current] != &quot;=&quot;</code> â€” which is <code>False</code> (it is <code>=</code>), so it consumes it. This works correctly. The sentinel <code>&quot;\\0&quot;</code> from <code>peek()</code> is not used in <code>_match()</code> â€” <code>_match()</code> uses <code>is_at_end()</code> and direct source access. Make sure <code>_match()</code> checks <code>is_at_end()</code> first.</h2>\n<h2 id=\"knowledge-cascade-maximal-munch-connects-to-everything\">Knowledge Cascade: Maximal Munch Connects to Everything</h2>\n<p>You have built greedy, one-directional token scanning. Here is where this knowledge radiates outward:\n<strong>1. Maximal munch is a greedy algorithm â€” and greedy algorithms appear everywhere.</strong>\nThe same &quot;take the longest valid choice now without reconsidering&quot; principle drives Huffman coding (always merge the smallest trees), the Unix <code>find -name &#39;*.c&#39;</code> glob matching algorithm, interval scheduling (always take the earliest-finishing task), and TCP&#39;s Nagle algorithm (buffer until maximum segment, then send). Greedy works when you can prove that local optimality implies global optimality. For tokenization, the proof is simple: well-designed languages guarantee that maximal munch produces the unique correct tokenization. If your language has a token ambiguity that maximal munch cannot resolve, you have a language design bug, not a scanner bug.\n<strong>2. Lookahead depth determines grammar class â€” and connects to parser design.</strong>\nYour scanner uses LL(1) lookahead for operators and LL(2) for float detection. When you build a parser in a future project, you will encounter the same concept: LL(1) parsers can be built without backtracking because one token of lookahead is always enough to decide which grammar rule to apply. LL(k) parsers need k tokens. The connection is direct: the lookahead discipline you are practicing in the scanner recurs in every subsequent layer of the compiler pipeline.\n<strong>3. The keyword table is a hash map â€” and hash maps power language implementations.</strong>\nThe <code>KEYWORDS</code> dictionary is a Python <code>dict</code>, which is a hash table. The O(1) keyword lookup is what makes scanners fast. This same pattern appears in symbol tables (looking up variable names in the compiler&#39;s environment), runtime method dispatch tables (finding which method to call for a given class), and CPU branch prediction tables (mapping instruction addresses to predicted branch directions). Whenever a language implementation needs to map names to meanings in constant time, a hash table is the answer.\n<strong>4. Scan-then-lookup vs. keyword states is a language implementer&#39;s design decision.</strong>\nGo&#39;s scanner, Clang&#39;s scanner, CPython&#39;s tokenizer, and V8&#39;s JavaScript scanner all use scan-then-lookup. Hand-coded state machines for individual keywords exist in some older compilers and in auto-generated scanners from tools like Flex â€” but modern practice heavily favors the lookup table approach because it is trivially extensible (add a keyword by adding a line to the table) and correct by construction (no prefix-match bugs). This is a real decision you have just made the right call on.\n<strong>5. Ambiguity in <code>&gt;&gt;=</code> and C++&#39;s template problem is a famous real-world consequence.</strong>\nIn C++, <code>vector&lt;vector&lt;int&gt;&gt;</code> was illegal before C++11 because <code>&gt;&gt;</code> was tokenized as the right-shift operator before the parser could tell the scanner it was inside a template parameter list. This required a workaround: <code>vector&lt;vector&lt;int&gt; &gt;</code> (space before the closing <code>&gt;</code>). C++11 fixed this by adding context-sensitive tokenization â€” the lexer now cooperates with the parser to decide whether <code>&gt;&gt;</code> is two template-closing <code>&gt;</code>s or a right-shift. This is a famous example of what happens when maximal munch produces the wrong result, and the language has to add complexity to compensate. Keep your language&#39;s token syntax unambiguous at the lexical level, and you will never face this problem.</p>\n<blockquote>\n<p>ðŸ”­ <strong>Deep Dive</strong>: The formal theory behind maximal munch and regular language tokenization â€” including proofs of correctness and the relationship to DFAs â€” is covered in <em>Compilers: Principles, Techniques, and Tools</em> (Aho, Lam, Sethi, Ullman â€” the &quot;Dragon Book&quot;), Section 3.3: &quot;Recognition of Tokens.&quot; If you want to understand why your scanner is guaranteed to work correctly (not just in practice but in theory), this section provides the rigorous foundation.</p>\n</blockquote>\n<hr>\n<h2 id=\"summary-what-you-have-built\">Summary: What You Have Built</h2>\n<p>By completing this milestone, your scanner now handles the full operator and literal vocabulary of your C-like language:</p>\n<ul>\n<li><strong>Two-character operator recognition</strong> via the <code>_match()</code> peek-and-consume helper. <code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&gt;=</code> are scanned as single tokens. Single <code>=</code>, <code>!</code>, <code>&lt;</code>, <code>&gt;</code> are correctly emitted alone when not followed by <code>=</code>.</li>\n<li><strong>Maximal munch applied universally</strong>: <code>&gt;==</code> tokenizes as <code>GREATER_EQ</code> + <code>ASSIGN</code>, never as <code>GREATER</code> + <code>EQUAL</code>. The scanner is greedy and never backtracks.</li>\n<li><strong>Integer and float literal scanning</strong> with two-character lookahead to handle the decimal point edge case. Trailing dots (<code>3.</code>) are not consumed; the dot is left for the next token.</li>\n<li><strong>Identifier scanning</strong> consuming letters, digits, and underscores in sequence.</li>\n<li><strong>Keyword table lookup</strong> distinguishing <code>if</code>, <code>else</code>, <code>while</code>, <code>return</code>, <code>true</code>, <code>false</code>, <code>null</code> from identifiers after the full lexeme is scanned. Prefix matches (<code>iffy</code>, <code>returnValue</code>) are correctly treated as identifiers.</li>\n<li><strong>Position accuracy</strong> maintained throughout: all tokens carry correct line and column corresponding to their first character.\nIn Milestone 3, you will add the two most complex scanning states: string literals (with escape sequences) and comments (both single-line <code>//</code> and multi-line <code>/* */</code>). These introduce a challenge you have not faced yet â€” the scanner must track what kind of thing it is <em>inside</em> (a string? a comment?) across many characters, and the meaning of <code>//</code> inside a string is completely different from <code>//</code> outside one.</li>\n</ul>\n<hr>\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m3 -->\n<!-- MS_ID: tokenizer-m3 -->\n<h1 id=\"milestone-3-strings-amp-comments\">Milestone 3: Strings &amp; Comments</h1>\n<h2 id=\"where-you-are-in-the-pipeline\">Where You Are in the Pipeline</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-satellite-map.svg\" alt=\"Tokenizer System Satellite Map â€” The Complete Pipeline\"></p>\n<h2 id=\"your-scanner-can-now-handle-every-token-in-your-c-like-language-except-two-string-literals-and-comments-at-first-glance-these-seem-simple-find-the-closing-quot-find-the-closing-done-but-that-intuition-is-the-misconception-this-milestone-exists-to-correct-string-literals-and-comments-are-not-just-quotlonger-tokensquot-they-are-the-first-point-in-your-scanner-where-the-rules-of-scanning-change-depending-on-context-inside-a-string-a-is-just-the-division-character-3939-not-the-start-of-a-comment-inside-a-comment-a-quot-is-just-a-character-not-the-start-of-a-string-the-same-character-in-two-different-positions-means-two-entirely-different-things-your-scanner-needs-to-know-where-it-is-to-interpret-what-it-sees-this-is-the-scanner39s-first-encounter-with-context-sensitivity-and-handling-it-correctly-requires-extending-your-finite-state-machine-with-new-states-mini-state-machines-that-take-over-while-you-are-inside-a-string-or-comment-apply-different-rules-and-eventually-hand-control-back-to-the-main-scanner-by-the-end-of-this-milestone-your-scanner-will-handle-every-character-in-a-well-formed-source-file-plus-gracefully-diagnose-malformed-ones-unterminated-strings-and-comments-the-gap-between-milestone-2-and-a-production-ready-lexer-is-almost-entirely-closed-here\">Your scanner can now handle every token in your C-like language <em>except</em> two: string literals and comments. At first glance, these seem simple â€” find the closing <code>&quot;</code>, find the closing <code>*/</code>, done. But that intuition is the misconception this milestone exists to correct.\nString literals and comments are not just &quot;longer tokens.&quot; They are the first point in your scanner where the <em>rules of scanning change depending on context</em>. Inside a string, a <code>/</code> is just the division character <code>&#39;/&#39;</code> â€” not the start of a comment. Inside a comment, a <code>&quot;</code> is just a character â€” not the start of a string. The same character, in two different positions, means two entirely different things. Your scanner needs to know <em>where it is</em> to interpret what it sees.\nThis is the scanner&#39;s first encounter with <strong>context-sensitivity</strong>, and handling it correctly requires extending your finite state machine with new states â€” mini state machines that take over while you are inside a string or comment, apply different rules, and eventually hand control back to the main scanner.\nBy the end of this milestone, your scanner will handle every character in a well-formed source file, plus gracefully diagnose malformed ones (unterminated strings and comments). The gap between Milestone 2 and a production-ready lexer is almost entirely closed here.</h2>\n<h2 id=\"the-revelation-it39s-not-a-search-problem\">The Revelation: It&#39;s Not a Search Problem</h2>\n<p>Here is the mental model most developers start with:</p>\n<blockquote>\n<p><em>&quot;Scanning a string literal is easy â€” find the opening <code>&quot;</code>, scan forward until you find the closing <code>&quot;</code>, and return everything in between as the token. Comments are the same: find <code>/*</code>, scan until <code>*/</code>.&quot;</em>\nThis sounds right. Python itself has something like this:</p>\n</blockquote>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># \"What people think string scanning looks like\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> scan_string_naive</span><span style=\"color:#E1E4E8\">(source, start):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.index(</span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">, start </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># find closing quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> source[start:end </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n<p>Follow this model to its consequences and see where it breaks.\n<strong>Scenario 1:</strong> The source contains <code>&quot;hello\\&quot;world&quot;</code>. There is a <code>\\&quot;</code> â€” a backslash followed by a quote. Your naive scanner calls <code>source.index(&#39;&quot;&#39;, 1)</code> and finds the <code>\\&quot;</code> â€” the escaped quote â€” and stops. But that quote is <em>inside</em> the string. It is not the terminator. The naive search returns <code>&quot;hello\\&quot;</code>, which is incomplete. The real string is <code>&quot;hello\\&quot;world&quot;</code> â€” the backslash &quot;escapes&quot; the quote, turning it from a string terminator into a literal quote character.\n<strong>Scenario 2:</strong> The source contains <code>&quot;hello // world&quot;</code>. Your naive scanner will eventually reach this string. But if your comment-detection logic runs before string scanning, it will see <code>//</code> and strip out everything after it â€” including the closing <code>&quot;</code>. Now the string is unterminated. The comment detector has eaten part of the string because it did not know it was inside one.\n<strong>Scenario 3:</strong> A multi-line comment <code>/* starts here\\n... and ends here */</code>. Your naive <code>source.index(&quot;*/&quot;)</code> approach would work â€” but while you are scanning past those newlines, you need to update <code>self.line</code>. A simple index-based search skips character-by-character traversal and therefore cannot update position counters.\n<strong>The real model:</strong> String and comment scanning are not search operations. They are <strong>sub-state machines</strong>. When your scanner enters a <code>&quot;</code>, it transitions into a new state â€” call it <code>IN_STRING</code> â€” where the rules change completely. In this state:</p>\n<ul>\n<li>Every character is a potential part of the string content.</li>\n<li><code>&quot;</code> terminates the string (exit state).</li>\n<li><code>\\</code> initiates an escape sequence (enter another sub-state: <code>IN_ESCAPE</code>).</li>\n<li><code>//</code> and <code>/*</code> are just characters, not comment starters.</li>\n<li>Newline or EOF terminates the string with an <em>error</em> (unterminated).\nOnly when the closing <code>&quot;</code> is found does the scanner leave <code>IN_STRING</code> and return to the normal <code>START</code> state.\nThis is the key insight: <strong>the same character means different things depending on the current state.</strong> And the state is determined by what the scanner has consumed so far â€” not what pattern the current character matches in isolation.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m3-context-sensitivity.svg\" alt=\"Context-Sensitivity: Same Character, Different Meaning\"></p>\n<hr>\n<h2 id=\"context-sensitivity-a-formal-aside-worth-two-minutes\">Context-Sensitivity: A Formal Aside (Worth Two Minutes)</h2>\n<h2 id=\"you-have-been-building-a-regular-language-tokenizer-one-that-a-finite-state-machine-can-handle-regular-languages-are-characterized-by-the-fact-that-their-recognition-depends-only-on-the-current-state-not-on-unbounded-memory-of-what-came-before-here-is-the-formal-tension-string-escaping-technically-pushes-beyond-regular-languages-a-scanner-must-track-whether-the-preceding-character-was-to-know-if-the-current-quot-terminates-the-string-this-quotmemoryquot-of-one-preceding-character-might-seem-to-require-more-than-a-finite-state-machine-but-it-does-not-you-simply-add-states-the-state-in_escape-captures-the-fact-that-a-was-just-seen-this-is-a-finite-amount-of-memory-one-bit-of-information-quotwas-the-previous-character-a-backslashquot-a-finite-state-machine-can-represent-any-finite-amount-of-memory-by-encoding-it-into-states-the-deeper-point-the-escape-mechanism-is-context-sensitive-at-the-character-level-the-character-after-is-interpreted-differently-from-any-other-character-when-you-implement-_scan_string-you-are-implementing-this-context-sensitivity-explicitly-by-changing-what-you-do-with-each-character-based-on-the-current-scanning-state-normal-vs-after-backslash-this-is-also-why-tokenizers-are-not-quotpurely-regularquot-in-practice-the-lexical-structure-of-real-languages-string-escape-sequences-here-documents-raw-strings-backtick-quoted-identifiers-in-sql-all-require-some-context-tracking-beyond-a-pure-dfa-production-scanners-handle-this-with-flags-and-sub-states-exactly-as-you-are-about-to-do\">You have been building a <strong>regular language</strong> tokenizer â€” one that a finite state machine can handle. Regular languages are characterized by the fact that their recognition depends only on the current state, not on unbounded memory of what came before.\nHere is the formal tension: <strong>string escaping technically pushes beyond regular languages</strong>. A scanner must track whether the preceding character was <code>\\</code> to know if the current <code>&quot;</code> terminates the string. This &quot;memory&quot; of one preceding character might seem to require more than a finite state machine â€” but it does not. You simply add states. The state <code>IN_ESCAPE</code> captures the fact that a <code>\\</code> was just seen. This is a finite amount of memory (one bit of information: &quot;was the previous character a backslash?&quot;). A finite state machine can represent any finite amount of memory by encoding it into states.\nThe deeper point: the <code>\\</code> escape mechanism is **context-sensitive at the character level**. The character after <code>\\</code> is interpreted differently from any other character. When you implement <code>_scan_string()</code>, you are implementing this context-sensitivity explicitly â€” by changing what you do with each character based on the current scanning state (normal vs. after-backslash).\nThis is also why tokenizers are not &quot;purely regular&quot; in practice. The <strong>lexical structure of real languages</strong> â€” string escape sequences, here-documents, raw strings, backtick-quoted identifiers in SQL â€” all require some context tracking beyond a pure DFA. Production scanners handle this with flags and sub-states, exactly as you are about to do.</h2>\n<h2 id=\"the-character-the-critical-branch\">The <code>/</code> Character: The Critical Branch</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m3-comment-scanning.svg\" alt=\"Comment Detection: The '/' Ambiguity\"></p>\n<p>In Milestone 2, you deliberately left <code>/</code> out of <code>SINGLE_CHAR_TOKENS</code> with a note that it would be handled in Milestone 3. Now you understand why: when the scanner sees <code>/</code>, it cannot immediately emit a <code>SLASH</code> (division) token because the next character might be <code>/</code> or <code>*</code> â€” starting a comment.\nThis is the same maximal munch pattern you used for <code>&gt;=</code> in Milestone 2, extended to a three-way decision:</p>\n<table>\n<thead>\n<tr>\n<th>Seen so far</th>\n<th>Next character</th>\n<th>Decision</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>/</code></td>\n<td><code>/</code></td>\n<td>Single-line comment: skip to end of line</td>\n</tr>\n<tr>\n<td><code>/</code></td>\n<td><code>*</code></td>\n<td>Multi-line comment: skip to <code>*/</code></td>\n</tr>\n<tr>\n<td><code>/</code></td>\n<td>anything else</td>\n<td>Division operator: emit <code>SLASH</code></td>\n</tr>\n<tr>\n<td>In code, this is a natural extension of <code>next_token()</code>:</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"/\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._skip_line_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()   </span><span style=\"color:#6A737D\"># comments produce no token; recurse for next</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tok_line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tok_col</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        err </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._skip_block_comment(start_line, start_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> err             </span><span style=\"color:#6A737D\"># unterminated comment â†’ Error token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()   </span><span style=\"color:#6A737D\"># comment consumed; get next token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">SLASH</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span></code></pre></div>\n<p>Notice the recursive call <code>return self.next_token()</code> after consuming a comment. Comments produce no tokens â€” they are discarded. After skipping a comment, the scanner needs to find the next <em>real</em> token. Calling <code>self.next_token()</code> again achieves this cleanly. The recursion depth is bounded: each call consumes at least one comment, so the recursion terminates when the source is exhausted or a non-comment token is found.</p>\n<blockquote>\n<p><strong>Alternative to recursion</strong>: some scanners use a loop inside <code>_skip_whitespace()</code> to also skip comments, so that both whitespace and comments are consumed before capturing <code>tok_line</code>/<code>tok_col</code>. Both approaches are correct. The recursive approach is more explicit about what is happening: a comment is &quot;transparent&quot; to the token stream.</p>\n</blockquote>\n<hr>\n<h2 id=\"single-line-comments-skipping-to-end-of-line\">Single-Line Comments: Skipping to End of Line</h2>\n<p>Single-line comments begin with <code>//</code> (both <code>/</code> characters already consumed by the time we enter this logic) and extend to the end of the current line. &quot;End of line&quot; means the character <code>\\n</code> or end of file â€” the newline itself is <em>not</em> consumed by the comment skip (it will be consumed as whitespace by <code>_skip_whitespace()</code> on the next call).</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _skip_line_comment</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Skip characters until end of line or end of file.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    The '//' has already been consumed. Newline is NOT consumed here.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span></code></pre></div>\n<h2 id=\"this-is-the-simplest-scanning-method-in-the-project-advance-as-long-as-you-are-not-at-a-newline-or-end-of-file-the-newline-will-be-consumed-by-_skip_whitespace-on-the-next-next_token-call-which-will-then-correctly-increment-selfline-why-not-consume-the-newline-here-because-the-newline-belongs-to-the-whitespace-layer-not-the-comment-layer-separating-concerns-quotcomments-skip-content-whitespace-handles-line-endingsquot-keeps-each-method-focused-it-also-means-_skip_line_comment-never-touches-selfline-avoiding-any-risk-of-double-counting-newlines-the-invisible-case-what-if-a-comment-appears-at-the-very-end-of-the-file-with-no-trailing-newline-selfpeek-quotnquot-will-never-be-true-but-selfis_at_end-will-become-true-and-exit-the-loop-this-is-handled-correctly-no-special-case-needed\">This is the simplest scanning method in the project: advance as long as you are not at a newline or end-of-file. The newline will be consumed by <code>_skip_whitespace()</code> on the next <code>next_token()</code> call, which will then correctly increment <code>self.line</code>.\nWhy not consume the newline here? Because the newline belongs to the whitespace layer, not the comment layer. Separating concerns â€” &quot;comments skip content, whitespace handles line endings&quot; â€” keeps each method focused. It also means <code>_skip_line_comment()</code> never touches <code>self.line</code>, avoiding any risk of double-counting newlines.\n<strong>The invisible case:</strong> What if a <code>//</code> comment appears at the very end of the file with no trailing newline? <code>self.peek() != &quot;\\n&quot;</code> will never be true, but <code>self.is_at_end()</code> will become true and exit the loop. This is handled correctly â€” no special case needed.</h2>\n<h2 id=\"multi-line-comments-the-state-machine-within-a-state-machine\">Multi-Line Comments: The State Machine Within a State Machine</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m3-multiline-comment-trace.svg\" alt=\"Trace: Multi-line Comment with Line Tracking\"></p>\n<p>Multi-line comments are more complex for three reasons:</p>\n<ol>\n<li><strong>Two-character delimiter</strong>: The comment ends with <code>*/</code>, not just <code>*</code> or <code>/</code>. You need to track &quot;did I just see <code>*</code>?&quot; to know when a subsequent <code>/</code> closes the comment.</li>\n<li><strong>Line tracking</strong>: The comment can span multiple lines. Every <code>\\n</code> inside the comment must update <code>self.line</code> â€” even though you are not emitting any tokens.</li>\n<li><strong>Unterminated detection</strong>: If the file ends before <code>*/</code> is found, you must emit an Error token at the position of the opening <code>/*</code> â€” not at the end of the file.\nThe implementation uses a small two-state machine: either you are reading normally inside the comment, or you just saw a <code>*</code> and are waiting to see if the next character is <code>/</code>.</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _skip_block_comment</span><span style=\"color:#E1E4E8\">(self, start_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, start_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">\"Token | None\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Skip a block comment. '/*' has already been consumed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns None on success, Error token on unterminated comment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Line/column tracking: every character is consumed via advance(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    so newlines inside the comment correctly update self.line.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"*\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"/\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()   </span><span style=\"color:#6A737D\"># consume the closing '/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">      # comment closed successfully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Reached end of file without finding '*/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"/*\"</span><span style=\"color:#E1E4E8\">, start_line, start_col)</span></span></code></pre></div>\n<p>Walk through the logic:</p>\n<ul>\n<li><code>advance()</code> consumes each character â€” this is why line tracking works. <code>advance()</code> handles <code>\\n</code> detection and updates <code>self.line</code>.</li>\n<li>After consuming <code>*</code>, <code>peek()</code> checks if the next character is <code>/</code> without consuming it.</li>\n<li>If yes, <code>advance()</code> consumes the <code>/</code> and returns <code>None</code> (success).</li>\n<li>If no, the loop continues â€” the <code>*</code> was part of the comment content, not the closing delimiter.</li>\n<li>If <code>is_at_end()</code> becomes true without having returned, the comment was unterminated. Return an Error token with the position of the opening <code>/*</code>.\n<strong>The non-nesting rule:</strong> Multi-line comments in your language do <em>not</em> nest. <code>/* outer /* inner */ still in comment? */</code> â€” the comment ends at the <em>first</em> <code>*/</code>, leaving <code>still in comment? */</code> as source code (likely generating errors). Your implementation above naturally achieves this: it stops at the first <code>*/</code> it encounters, regardless of how many <code>/*</code> are inside. There is no counter. There are no nested states.<blockquote>\n<p><strong>Why no nesting?</strong> Allowing nested comments (<code>/* /* */ */</code>) is attractive but requires a counter (<code>depth++</code> on <code>/*</code>, <code>depth--</code> on <code>*/</code>, stop when depth reaches 0). This counter is theoretically unbounded â€” which means a nested-comment scanner is no longer a finite state machine; it requires a pushdown automaton (PDA). This is the difference between regular languages (FSM, O(1) memory) and context-free languages (PDA, stack memory). Languages like Haskell and D support nested block comments, but they do so at a cost: the lexer is no longer purely regular. For your C-like language, following C&#39;s convention of non-nesting keeps the lexer in the regular language class. Document this in your code.</p>\n</blockquote>\n</li>\n</ul>\n<hr>\n<h2 id=\"escape-sequences-the-two-character-dance\">Escape Sequences: The Two-Character Dance</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m3-string-fsm.svg\" alt=\"String Literal Scanner â€” State Machine with Escape Handling\"></p>\n<p>String literal scanning introduces a new sub-state: the <strong>escape state</strong>. When the scanner is inside a string and encounters a <code>\\</code>, the next character does not have its normal meaning â€” it is the second half of an escape sequence. The pair <code>\\n</code> does not mean backslash then n; it means the newline character (ASCII 10).\nYour language supports these escape sequences:</p>\n<table>\n<thead>\n<tr>\n<th>Escape sequence</th>\n<th>Meaning</th>\n<th>Character produced</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>\\n</code></td>\n<td>Newline</td>\n<td><code>\\n</code> (ASCII 10)</td>\n</tr>\n<tr>\n<td><code>\\t</code></td>\n<td>Tab</td>\n<td><code>\\t</code> (ASCII 9)</td>\n</tr>\n<tr>\n<td><code>\\r</code></td>\n<td>Carriage return</td>\n<td><code>\\r</code> (ASCII 13)</td>\n</tr>\n<tr>\n<td><code>\\&quot;</code></td>\n<td>Literal double quote</td>\n<td><code>&quot;</code></td>\n</tr>\n<tr>\n<td><code>\\\\</code></td>\n<td>Literal backslash</td>\n<td><code>\\</code></td>\n</tr>\n<tr>\n<td>The implementation requires a decision about what the <code>Token.lexeme</code> field stores. There are two choices:</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Option A â€” Raw lexeme</strong>: <code>lexeme</code> stores the exact source characters, including escape sequences. For the source <code>&quot;hello\\nworld&quot;</code>, the lexeme is the string <code>&#39;&quot;hello\\\\nworld&quot;&#39;</code> (14 characters including the backslash, the n, and the quotes). The downstream parser or interpreter is responsible for interpreting escape sequences later.</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Option B â€” Processed value</strong>: <code>lexeme</code> stores the interpreted string content. For <code>&quot;hello\\nworld&quot;</code>, the lexeme is <code>&#39;&quot;hello\\nworld&quot;&#39;</code> where the <code>\\n</code> is the actual newline character (13 characters plus the quotes).</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>The tradeoff:</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Option</td>\n<td>Pros</td>\n<td>Cons</td>\n</tr>\n<tr>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n</tr>\n<tr>\n<td><strong>Raw lexeme</strong></td>\n<td>Faithful to source; error messages show what user typed</td>\n<td>Parser must do escape processing; two places that understand escapes</td>\n</tr>\n<tr>\n<td><strong>Processed value</strong></td>\n<td>Interpreter can use lexeme directly</td>\n<td>Error positions inside strings are harder; loss of original source text</td>\n</tr>\n<tr>\n<td>For this project, use <strong>raw lexeme</strong> â€” store exactly what appeared in the source. The lexeme of <code>&quot;hello\\nworld&quot;</code> is the literal source text <code>&quot;hello\\nworld&quot;</code> including the backslash and <code>n</code>. Escape interpretation happens at a higher level (the interpreter or constant-folding pass). This is the approach used by CPython&#39;s tokenizer, Go&#39;s scanner, and LLVM&#39;s Clang.</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>What to validate at scan time?</strong> Even with raw lexemes, the scanner must validate that escape sequences are <em>legal</em>. If the source contains <code>&quot;\\q&quot;</code> â€” backslash followed by <code>q</code>, which is not a valid escape â€” the scanner should emit an Error token. Unknown escapes are not silently passed through; they are caught here.</p>\n</blockquote>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m3-escape-trace.svg\" alt=\"Trace: Scanning 'hello\\nworld' with Escape Sequence\"></p>\n<p>Here is the complete string scanning method:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_string</span><span style=\"color:#E1E4E8\">(self, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Scan a string literal. The opening '\"' has already been consumed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    tok_line and tok_col point to the opening quote position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns a STRING token with the full lexeme including quotes,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    or an ERROR token if the string is unterminated or contains an invalid escape.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme_chars </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">]   </span><span style=\"color:#6A737D\"># start with the opening quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # EOF before closing quote â€” unterminated string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Closing quote found â€” string complete</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme_chars.append(</span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Newline before closing quote â€” unterminated string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Note: self.line has already been updated by advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # The Error token reports the OPENING quote position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Escape sequence â€” consume and validate the next character</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme_chars.append(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Backslash at EOF â€” unterminated</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            escape_char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> escape_char </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">'n'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'t'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Invalid escape sequence â€” emit error at opening quote position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Could alternatively emit at the backslash position; document your choice</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                lexeme_chars.append(escape_char)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme_chars.append(escape_char)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Normal character inside string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme_chars.append(char)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Unreachable â€” loop exits via return in all cases</span></span></code></pre></div>\n<blockquote>\n<p><strong>Why <code>list</code> then <code>&quot;&quot;.join()</code> instead of <code>lexeme += char</code>?</strong> In <code>_scan_number()</code> and <code>_scan_identifier()</code>, you used <code>lexeme += char</code>. For short tokens (numbers, identifiers), this is fine. For strings, which can be arbitrarily long, string concatenation inside a loop creates O(nÂ²) work â€” each <code>+=</code> allocates a new string and copies the old one. Using a <code>list</code> as a character buffer and joining at the end is O(n). In Python, <code>&quot;&quot;.join(list_of_chars)</code> is the idiomatic efficient string-building pattern for loops. For a 10,000-character string, the difference is measurable.</p>\n</blockquote>\n<h3 id=\"the-unterminated-string-blame-the-cause-not-the-symptom\">The Unterminated String: Blame the Cause, Not the Symptom</h3>\n<p>Notice that when an unterminated string is detected â€” whether because of EOF or a newline â€” the Error token&#39;s position (<code>tok_line</code>, <code>tok_col</code>) points to the <strong>opening quote</strong>, not to where the scanner stopped. This is deliberate.\nThink about the user experience. If a programmer writes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>if (x &gt; 0) {\n    message = &quot;error occurred\n    return false;\n}</code></pre></div>\n<h2 id=\"the-unterminated-string-starts-on-line-3-the-eof-or-the-next-line-is-where-the-scanner-discovers-the-problem-but-the-cause-is-on-line-3-the-unclosed-quot-an-error-message-saying-quotunterminated-string-at-line-5-column-1quot-pointing-at-return-would-be-useless-quotunterminated-string-at-line-3-column-15quot-pointing-at-the-opening-quot-is-actionable-this-is-a-general-principle-in-error-reporting-report-the-position-of-the-cause-not-the-position-where-the-consequence-was-detected-the-scanner-detects-the-problem-when-it-runs-out-of-valid-string-content-it-reports-the-problem-where-the-string-began-the-same-principle-applies-to-multi-line-comments-the-error-token-for-an-unterminated-reports-the-position-of-the-opening-even-if-the-scanner-consumed-thousands-of-characters-before-realizing-no-was-coming\">The unterminated string starts on line 3. The EOF (or the next line) is where the scanner <em>discovers</em> the problem, but the <em>cause</em> is on line 3 â€” the unclosed <code>&quot;</code>. An error message saying &quot;unterminated string at line 5, column 1&quot; (pointing at <code>return</code>) would be useless. &quot;Unterminated string at line 3, column 15&quot; (pointing at the opening <code>&quot;</code>) is actionable.\nThis is a general principle in error reporting: <strong>report the position of the cause, not the position where the consequence was detected.</strong> The scanner detects the problem when it runs out of valid string content. It reports the problem where the string began.\nThe same principle applies to multi-line comments: the Error token for an unterminated <code>/* ... */</code> reports the position of the opening <code>/*</code>, even if the scanner consumed thousands of characters before realizing no <code>*/</code> was coming.</h2>\n<h2 id=\"integrating-string-and-comment-scanning-into-next_token\">Integrating String and Comment Scanning into next_token()</h2>\n<p>Here is the updated <code>next_token()</code> incorporating all Milestone 3 additions. The changes are concentrated in two places: the <code>/</code> handling and the <code>&quot;</code> handling.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> next_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">._skip_whitespace()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tok_col  </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Single-character unambiguous tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(</span><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char], char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Two-character operators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> OPERATOR_CHARS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_operator(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Division operator or comment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"/\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Single-line comment: skip to end of line, get next token</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._skip_line_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Multi-line comment: skip to */, get next token (or error)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            err </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._skip_block_comment(tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Just a division operator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">SLASH</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ String literal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_string(tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Number literals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Identifiers and keywords â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Unrecognized character â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span></code></pre></div>\n<h2 id=\"the-order-of-checks-matters-the-quotquot-check-must-come-after-the-single-character-and-operator-checks-since-are-not-ambiguous-and-before-the-number-and-identifier-checks-since-is-neither-a-digit-nor-a-letter\">The order of checks matters. The <code>&quot;/&quot;</code> check must come after the single-character and operator checks (since <code>+</code>, <code>-</code>, <code>*</code> are not ambiguous), and before the number and identifier checks (since <code>/</code> is neither a digit nor a letter).</h2>\n<h2 id=\"line-number-tracking-inside-strings-and-comments\">Line Number Tracking Inside Strings and Comments</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m3-multiline-comment-trace.svg\" alt=\"Trace: Multi-line Comment with Line Tracking\"></p>\n<p>Both <code>_scan_string()</code> and <code>_skip_block_comment()</code> consume characters using <code>self.advance()</code>. Because <code>advance()</code> handles newline detection (incrementing <code>self.line</code> and resetting <code>self.column</code>), line tracking works automatically inside both constructs â€” you do not need any special handling.\nHere is a concrete trace to verify your intuition. Suppose the source is:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>x = /* first\n  second */ y</code></pre></div>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Scanning starts: line=1, col=1\nadvance() 'x'  â†’ line=1, col=2\nadvance() ' '  â†’ line=1, col=3   (whitespace, skipped)\nadvance() '='  â†’ line=1, col=4   â†’ Token(ASSIGN, &quot;=&quot;, 1, 3)\nadvance() ' '  â†’ line=1, col=5   (whitespace, skipped)\n-- next_token() sees '/' at line=1, col=5 --\ntok_line=1, tok_col=5\nadvance() '/'  â†’ line=1, col=6\n_match('*')    â†’ True, advance() '*' â†’ line=1, col=7\n_skip_block_comment(start_line=1, start_col=5):\n  advance() ' '   â†’ line=1, col=8\n  advance() 'f'   â†’ line=1, col=9\n  advance() 'i'   â†’ line=1, col=10\n  advance() 'r'   â†’ line=1, col=11\n  advance() 's'   â†’ line=1, col=12\n  advance() 't'   â†’ line=1, col=13\n  advance() '\\n'  â†’ line=2, col=1    â† LINE INCREMENTED\n  advance() ' '   â†’ line=2, col=2\n  advance() ' '   â†’ line=2, col=3\n  advance() 's'   â†’ line=2, col=4\n  ...\n  advance() '*'   â†’ line=2, col=10\n  peek() == '/'   â†’ True\n  advance() '/'   â†’ line=2, col=11\n  return None   â† comment closed\nnext_token() recurses:\nadvance() ' '   â†’ line=2, col=12  (whitespace)\ntok_line=2, tok_col=12\nadvance() 'y'   â†’ line=2, col=13\nâ†’ Token(IDENTIFIER, &quot;y&quot;, 2, 12)</code></pre></div>\n<h2 id=\"the-token-y-correctly-reports-line-2-column-12-even-though-it-was-scanned-long-after-the-comment-started-on-line-1-every-advance-call-inside-the-comment-handled-newline-detection-you-did-not-write-a-single-special-case-for-this-this-is-the-payoff-of-the-advance-abstraction-all-callers-benefit-from-correct-position-tracking-without-having-to-think-about-it\">The token <code>y</code> correctly reports line 2, column 12 â€” even though it was scanned long after the comment started on line 1. Every <code>advance()</code> call inside the comment handled newline detection. You did not write a single special case for this.\nThis is the payoff of the <code>advance()</code> abstraction: all callers benefit from correct position tracking without having to think about it.</h2>\n<h2 id=\"the-complete-milestone-3-scanner\">The Complete Milestone 3 Scanner</h2>\n<p>Here is the full <code>scanner.py</code> with all three milestones integrated:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PLUS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MINUS</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STAR</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SLASH</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUAL</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ==</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUAL</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQ</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQ</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># >=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BANG</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LPAREN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RPAREN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RBRACE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACKET</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RBRACKET</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEMICOLON</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sentinels</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Module-level constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"+\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"-\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"*\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">STAR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"(\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \")\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"{\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"}\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"[\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">LBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"]\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">RBRACKET</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \";\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \",\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">COMMA</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">OPERATOR_CHARS</span><span style=\"color:#E1E4E8\">: frozenset[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> frozenset</span><span style=\"color:#E1E4E8\">({</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">\"</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">WHITESPACE</span><span style=\"color:#E1E4E8\">: frozenset[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> frozenset</span><span style=\"color:#E1E4E8\">({</span><span style=\"color:#9ECBFF\">\" \"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">KEYWORDS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"if\"</span><span style=\"color:#E1E4E8\">:     TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"else\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"while\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"return\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"true\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"false\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"null\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">VALID_ESCAPES</span><span style=\"color:#E1E4E8\">: frozenset[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> frozenset</span><span style=\"color:#E1E4E8\">({</span><span style=\"color:#9ECBFF\">\"n\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"t\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"r\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Primitive operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _peek_next</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> >=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _match</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> WHITESPACE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Comment skipping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_line_comment</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Skip to end of line. '//' already consumed. Newline NOT consumed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_block_comment</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, start_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, start_col: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> Optional[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Skip block comment body. '/*' already consumed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns None on success, Error token at opening '/*' if unterminated.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Non-nesting: stops at the FIRST '*/' encountered.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"*\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"/\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.advance()   </span><span style=\"color:#6A737D\"># consume closing '/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Unterminated â€” report at opening '/*'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"/*\"</span><span style=\"color:#E1E4E8\">, start_line, start_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Scanning methods â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_string</span><span style=\"color:#E1E4E8\">(self, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Scan string literal. Opening '\"' already consumed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Lexeme includes surrounding quotes and raw escape sequences.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Unterminated (EOF or bare newline) â†’ Error token at opening quote.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Invalid escape sequence â†’ Error token at opening quote.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme_chars: list[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                lexeme_chars.append(</span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Bare newline terminates string as an error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                lexeme_chars.append(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                escape_char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                lexeme_chars.append(escape_char)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> escape_char </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> VALID_ESCAPES</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Invalid escape â€” report error at opening quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">.join(lexeme_chars), tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme_chars.append(char)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_number</span><span style=\"color:#E1E4E8\">(self, first_digit: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> first_digit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \".\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._peek_next().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()   </span><span style=\"color:#6A737D\"># consume '.'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, lexeme, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_identifier</span><span style=\"color:#E1E4E8\">(self, first_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> first_char</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        token_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> KEYWORDS</span><span style=\"color:#E1E4E8\">.get(lexeme, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(token_type, lexeme, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_operator</span><span style=\"color:#E1E4E8\">(self, char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tok_line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tok_col: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"=\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"==\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"=\"</span><span style=\"color:#F97583\"> and</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> and</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                        self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"=\"</span><span style=\"color:#F97583\"> else</span><span style=\"color:#9ECBFF\"> \"=\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tok_line, tok_col,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Simpler explicit version:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"=\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"==\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"!\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">BANG</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"&#x3C;\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LESS_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">LESS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Token production â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> next_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._skip_whitespace()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tok_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tok_col  </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(</span><span style=\"color:#79B8FF\">SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char], char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> OPERATOR_CHARS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_operator(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"/\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._skip_line_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                err </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._skip_block_comment(tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">SLASH</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_string(tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier(char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens: list[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next_token()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tokens</span></span></code></pre></div>\n<blockquote>\n<p><strong>Note on the <code>_scan_operator</code> method above:</strong> The first <code>if char == &quot;=&quot;</code> block contains a demonstration-then-replacement comment pattern to show the right approach. In your actual code, use the clean explicit version (the one with the plain <code>if self._match(&quot;=&quot;)</code> pattern from Milestone 2). The tangled first version is shown only to highlight why the explicit form is clearer.</p>\n</blockquote>\n<hr>\n<h2 id=\"the-complete-fsm-view\">The Complete FSM View</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m3-full-fsm.svg\" alt=\"Complete Scanner FSM â€” All States\"></p>\n<p>After Milestone 3, your scanner&#39;s state machine has the following explicit states:</p>\n<table>\n<thead>\n<tr>\n<th>State</th>\n<th>Entered when</th>\n<th>Exit condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>START</strong></td>\n<td>Between tokens</td>\n<td>Always â€” dispatch on first char</td>\n</tr>\n<tr>\n<td><strong>IN_STRING</strong></td>\n<td><code>&quot;</code> consumed</td>\n<td>Closing <code>&quot;</code>, newline (error), or EOF (error)</td>\n</tr>\n<tr>\n<td><strong>IN_ESCAPE</strong></td>\n<td><code>\\</code> inside string</td>\n<td>One character consumed (valid or invalid)</td>\n</tr>\n<tr>\n<td><strong>IN_NUMBER</strong></td>\n<td>Digit consumed</td>\n<td>Non-digit, non-dot, or dot without following digit</td>\n</tr>\n<tr>\n<td><strong>IN_FLOAT</strong></td>\n<td><code>.</code> after digits consumed</td>\n<td>Non-digit</td>\n</tr>\n<tr>\n<td><strong>IN_IDENTIFIER</strong></td>\n<td>Letter or <code>_</code> consumed</td>\n<td>Non-alphanumeric, non-<code>_</code></td>\n</tr>\n<tr>\n<td><strong>IN_OPERATOR</strong></td>\n<td><code>=</code>, <code>!</code>, <code>&lt;</code>, <code>&gt;</code> consumed</td>\n<td>One character peeked/consumed</td>\n</tr>\n<tr>\n<td><strong>IN_LINE_COMMENT</strong></td>\n<td><code>//</code> consumed</td>\n<td><code>\\n</code> or EOF</td>\n</tr>\n<tr>\n<td><strong>IN_BLOCK_COMMENT</strong></td>\n<td><code>/*</code> consumed</td>\n<td><code>*/</code> or EOF (error)</td>\n</tr>\n<tr>\n<td>In your code, these states are not explicit enum values â€” they are represented by which method is currently executing. The call stack <em>is</em> the state. <code>_scan_string()</code> executing means you are in <code>IN_STRING</code>. <code>_skip_block_comment()</code> executing means you are in <code>IN_BLOCK_COMMENT</code>. This is the recursive descent encoding of a state machine: states become functions, transitions become calls.</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"testing-milestone-3\">Testing Milestone 3</h2>\n<p>Tests for strings and comments require verifying not just that the right token type is emitted, but that the lexeme is correct, positions are accurate, and unterminated constructs produce errors at the right position.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ String literal tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_simple_string</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'\"hello\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_empty_string</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'\"\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_with_escape_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">nworld\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Raw lexeme: the source characters '\"hello\\nworld\"' (with literal backslash-n)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"hello</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">nworld\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_with_escaped_quote</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"say </span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"hi</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"say </span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"hi</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_with_escaped_backslash</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"path</span><span style=\"color:#79B8FF\">\\\\\\\\</span><span style=\"color:#9ECBFF\">file\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"path</span><span style=\"color:#79B8FF\">\\\\\\\\</span><span style=\"color:#9ECBFF\">file\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_all_valid_escapes</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">n</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">t</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">r</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\\\\\\\</span><span style=\"color:#9ECBFF\">\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_string_eof</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"EOF before closing quote â†’ Error token at opening quote position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # opening quote position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_string_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Bare newline inside string â†’ Error token at opening quote position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">world\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # opening quote on line 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_invalid_escape</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">q' is not a valid escape â€” Error token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">q\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_backslash_at_eof</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"String with trailing backslash at EOF â€” unterminated.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_comment_inside_string_not_a_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"// inside a string is part of the string content, not a comment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello // world\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"hello // world\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_markers_inside_string</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"/* and */ inside a string are string content.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"/* not a comment */\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"/* not a comment */\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_position_accuracy</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Position of string token is the opening quote.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'x = \"hello\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    string_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> string_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> string_token.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">   # the '\"' is at column 5</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Comment tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_comment_produces_no_token</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"// comment is completely invisible to the token stream.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x // this is a comment</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">y\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> types </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"x\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"y\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_comment_at_eof</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Line comment with no trailing newline â€” scanner reaches EOF cleanly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"// comment with no newline\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_comment_does_not_consume_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token after comment starts on a new line, not the comment's line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"// comment</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">x\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # 'x' is on line 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_slash_alone_is_division</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'/' not followed by '/' or '*' is a SLASH token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"a / b\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">SLASH</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_produces_no_token</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"/* comment */ is invisible to the token stream.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x /* ignored */ y\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> types </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_updates_line_numbers</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Line numbers are tracked inside multi-line block comments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x /* line1</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">line2</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">line3 */ y\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> y_token.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"y\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> y_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">   # 'y' is on line 3 after two newlines in comment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_non_nesting</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"/* /* */ ends at the FIRST */ â€” second /* is not matched.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"/* /* */ x\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Comment ends at first */; 'x' is scanned normally</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_block_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"/* with no closing */ â†’ Error token at opening /* position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x /* unterminated\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # First token is 'x', second is Error for unterminated comment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"/*\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">   # '/*' starts at column 3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_comment_reports_opening_position</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-line unterminated comment: error reports at '/*', not at EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"a</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">b</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">/* starts here</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">and never ends\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(error_tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">    # line where /* appears</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # column where /* appears</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_adjacent_line_comments</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multiple line comments on consecutive lines, all ignored.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"// line 1</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">// line 2</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">x\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # x + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"x\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_after_block_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token positions after a multi-line block comment are correct.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'/* comment</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\"> */ \"hello\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # string is on line 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_comment_between_tokens</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comments between tokens do not affect surrounding token positions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x /* ignored */ + y\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">17</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"y\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">19</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<h2 id=\"run-python-m-pytest-test_scannerpy-v\">Run: <code>python -m pytest test_scanner.py -v</code></h2>\n<h2 id=\"common-pitfalls\">Common Pitfalls</h2>\n<h2 id=\"1-consuming-the-closing-quote-as-part-of-the-next-token-the-string-scanner39s-closing-quote-branch-does-lexeme_charsappend39quot39-and-then-returns-after-this-return-selfcurrent-points-at-the-character-after-the-quot-the-next-call-to-next_token-correctly-starts-from-there-the-bug-occurs-if-you-forget-to-consume-the-closing-quote-if-the-loop39s-exit-condition-is-peek-39quot39-rather-than-advance-returning-39quot39-then-selfcurrent-still-points-at-quot-and-the-next-token-is-an-empty-string-or-a-new-string-starting-from-the-same-quote-2-the-escape-backslash-at-end-of-string-bug-the-source-quothelloquot-a-string-that-ends-with-a-backslash-and-a-quote-the-scanner-sees-enters-escape-handling-consumes-quot-it-just-consumed-the-closing-quote-as-an-escape-character-the-loop-continues-finds-eof-and-returns-an-error-token-this-is-correct-behavior-quothelloquot-is-unterminated-the-quot-is-an-escaped-quote-not-the-string-terminator-your-implementation-handles-this-naturally-because-the-branch-consumes-the-next-character-unconditionally-via-advance-not-via-the-main-loop-3-multi-line-comment-starting-with-whereadvancesselfcurrent-if-_skip_block_commentchecksif-char-quotquotand-peeks-at-the-next-character-it-must-then-consume-thewithselfadvance-if-it-returns-without-consuming-the-the-next-call-to-next_tokensees-emits-slash-and-your-token-stream-is-corrupted-the-implementation-above-calls-selfadvanceexplicitly-on-the-closing-4-column-counting-inside-strings-with-escape-sequences-the-lexeme-quothellonquotin-source-text-is-9-charactersquot-h-e-l-l-o-n-quot-the-scanner-advances-through-9-source-characters-incrementing-selfcolumn-for-each-the-n-in-the-source-is-a-backslash-followed-by-n-not-an-actual-newline-so-selfline-is-not-incremented-if-you-mistakenly-process-the-backslash-as-producing-an-actual-newline-character-and-feed-it-to-advance39s-newline-detection-you-would-incorrectly-increment-the-line-counter-the-fix-in-_scan_string-you-call-selfadvance-to-consume-the-raw-source-characters-the-escape-interpretation-turning-n-into-chr10-happens-elsewhere-in-the-interpreter-not-here-5-inside-a-string-triggering-comment-detection-the-string-quothttpexamplecomquot-contains-if-your-scanner-checks-for-before-it-checks-whether-it-is-inside-a-string-it-would-incorrectly-start-a-line-comment-your-next_token-only-reaches-the-handling-code-for-characters-that-are-not-inside-strings-the-inside-quothttpexamplecomquot-is-consumed-by-_scan_string-as-part-of-the-string-content-long-before-next_token-has-a-chance-to-see-it-the-state-machine-architecture-prevents-this-bug-by-design-_scan_string-has-taken-over-next_token-is-not-involved-until-the-closing-quot-is-found-6-block-comment-ending-with-the-source-body-two-asterisks-before-closing-slash-your-scanner-sees-in-the-loop-peeks-match-consumes-closes-comment-the-extra-before-the-that-matched-was-just-consumed-as-comment-content-the-final-in-is-the-one-that-matched-this-is-correct-per-the-spec-ends-the-comment-regardless-of-how-many-precede-it-no-special-handling-needed\"><strong>1. Consuming the closing quote as part of the next token</strong>\nThe string scanner&#39;s closing-quote branch does <code>lexeme_chars.append(&#39;&quot;&#39;)</code> and then returns. After this return, <code>self.current</code> points at the character <em>after</em> the <code>&quot;</code>. The next call to <code>next_token()</code> correctly starts from there. The bug occurs if you forget to consume the closing quote â€” if the loop&#39;s exit condition is <code>peek() == &#39;&quot;&#39;</code> rather than <code>advance()</code> returning <code>&#39;&quot;&#39;</code>. Then <code>self.current</code> still points at <code>&quot;</code> and the next token is an empty string or a new string starting from the same quote.\n<strong>2. The escape-backslash-at-end-of-string bug</strong>\nThe source <code>&quot;hello\\&quot;</code> â€” a string that ends with a backslash <em>and</em> a quote. The scanner sees <code>\\</code>, enters escape handling, consumes <code>&quot;</code>. It just consumed the closing quote as an escape character. The loop continues, finds EOF, and returns an Error token. This is correct behavior: <code>&quot;hello\\&quot;</code> is unterminated (the <code>\\&quot;</code> is an escaped quote, not the string terminator). Your implementation handles this naturally because the <code>\\\\</code> branch consumes the next character unconditionally via <code>advance()</code>, not via the main loop.\n*<em>3. Multi-line comment starting with `/</em><code>where</code><em><code>advances</code>self.current<code>** If </code>_skip_block_comment()<code>checks</code>if char == &quot;</em>&quot;<code>and peeks at the next character, it must then *consume* the</code>/<code>with</code>self.advance()<code>. If it returns without consuming the </code>/<code>, the next call to </code>next_token()<code>sees</code>/<code>, emits </code>SLASH<code>, and your token stream is corrupted. The implementation above calls </code>self.advance()<code>explicitly on the closing</code>/<code>. **4. Column counting inside strings with escape sequences** The lexeme </code>&quot;hello\\n&quot;<code>in source text is 9 characters:</code>&quot;<code>, </code>h<code>, </code>e<code>, </code>l<code>, </code>l<code>, </code>o<code>, </code>`, <code>n</code>, <code>&quot;</code>. The scanner advances through 9 source characters, incrementing <code>self.column</code> for each. The <code>\\n</code> in the source is a backslash followed by <code>n</code> â€” not an actual newline â€” so <code>self.line</code> is not incremented. If you mistakenly process the backslash as producing an actual newline character and feed it to <code>advance()</code>&#39;s newline detection, you would incorrectly increment the line counter. The fix: in <code>_scan_string()</code>, you call <code>self.advance()</code> to consume the raw source characters. The escape <em>interpretation</em> (turning <code>\\n</code> into <code>chr(10)</code>) happens elsewhere (in the interpreter), not here.\n<strong>5. <code>/</code> inside a string triggering comment detection</strong>\nThe string <code>&quot;http://example.com&quot;</code> contains <code>//</code>. If your scanner checks for <code>//</code> before it checks whether it is inside a string, it would incorrectly start a line comment. Your <code>next_token()</code> only reaches the <code>/</code> handling code for characters that are not inside strings â€” the <code>/</code> inside <code>&quot;http://example.com&quot;</code> is consumed by <code>_scan_string()</code> as part of the string content, long before <code>next_token()</code> has a chance to see it. The state machine architecture prevents this bug by design: <code>_scan_string()</code> has taken over; <code>next_token()</code> is not involved until the closing <code>&quot;</code> is found.\n<strong>6. Block comment ending with <code>**/</code></strong>\nThe source <code>/** body **/</code> (two asterisks before closing slash). Your scanner sees <code>*</code> in the loop, peeks <code>/</code> â€” match! â€” consumes <code>/</code>, closes comment. The extra <code>*</code> before the <code>*</code> that matched was just consumed as comment content. The final <code>*</code> in <code>**/</code> is the one that matched. This is correct per the spec: <code>*/</code> ends the comment, regardless of how many <code>*</code> precede it. No special handling needed.</h2>\n<h2 id=\"knowledge-cascade-one-insight-many-domains\">Knowledge Cascade: One Insight, Many Domains</h2>\n<p><strong>1. Sub-states within larger FSMs are everywhere â€” network protocols, game engines, UI.</strong>\nTCP&#39;s <code>ESTABLISHED</code> state contains an entire sub-state machine for handling in-order delivery, retransmission timeouts, and window scaling. A video game&#39;s <code>PLAYING</code> state contains sub-states for <code>WALKING</code>, <code>ATTACKING</code>, <code>JUMPING</code>. A UI framework&#39;s <code>ACTIVE</code> state contains sub-states for <code>FOCUSED</code>, <code>HOVERED</code>, <code>DRAGGING</code>. In every case, the pattern is the same: the outer state machine delegates to an inner one, which eventually hands control back. Your <code>_scan_string()</code> is the &quot;inner state machine&quot; that takes over when <code>next_token()</code> delegates string handling to it.\n<strong>2. The escape character pattern recurs in every text processing system.</strong>\nSQL escaping: <code>&#39;O&#39;&#39;Brien&#39;</code> uses a doubled quote to represent a literal single quote inside a string â€” same problem, different syntax. URL percent-encoding: <code>%20</code> represents a space â€” backslash replaced by <code>%</code>. Shell scripts: <code>echo &quot;hello\\nworld&quot;</code> vs <code>echo &#39;hello\\nworld&quot;</code> â€” single quotes prevent all escaping. Regex: <code>\\.</code> matches a literal dot, not &quot;any character.&quot; In every case, a special character (backslash, <code>%</code>, doubled quote) signals &quot;interpret the next character(s) specially.&quot; Now that you have implemented this from scratch, you understand the mechanism shared across all these systems.\n<strong>3. Error reporting principle: blame the cause, not the symptom.</strong>\nUnterminated strings and comments taught you to report the position of the <em>opening</em> delimiter, not the end-of-file where the scanner gave up. This &quot;blame the cause&quot; principle recurs everywhere in error design. A Rust borrow checker error points to where the borrow was <em>created</em>, not where it is <em>used</em> after its lifetime expired. A Java NullPointerException in a good framework points to where the null was <em>introduced</em>, not where it was <em>dereferenced</em>. A database deadlock detector reports which transaction <em>initiated</em> the conflicting lock sequence. The discipline of tracking cause positions (not just detection positions) is what separates usable error messages from useless ones.\n<strong>4. Comment stripping as a compilation phase â€” the phase structure of compilers.</strong>\nComments are eliminated at the lexical level â€” your scanner sees them and produces nothing. They are invisible to the parser, the type checker, and the code generator. This is intentional: it reflects the phase structure of compilers. Each phase has a specific job. The lexer&#39;s job is to produce a token stream. Comments are not tokens â€” they are annotations for the human reader. By the time the parser runs, comments are gone forever. This is why you cannot write a macro system that reads comments (unlike annotation systems in Java/Python that work at the <em>parser</em> level with structured syntax). The phase determines what information is available.\n<strong>5. Context-sensitivity and the limit of regular languages.</strong>\nString escape sequences technically require context: the character <code>n</code> after <code>\\</code> means &quot;newline,&quot; but <code>n</code> anywhere else means &quot;the letter n.&quot; This is one step toward context-sensitivity â€” the meaning of a character depends on what immediately precedes it. True context-sensitivity (in the formal language theory sense) would require the meaning to depend on an unbounded amount of preceding context. Here, it is bounded: only the immediately preceding character (<code>\\</code>) changes the meaning. This is encodable as a finite state machine by adding the <code>IN_ESCAPE</code> state. But it illustrates the boundary: if your language had tokens where the meaning of character <code>n</code> depended on <em>any previous part of the file</em>, your tokenizer would need a pushdown automaton or Turing machine. Regular languages are powerful enough for well-designed programming languages â€” but only barely.</p>\n<blockquote>\n<p>ðŸ”­ <strong>Deep Dive</strong>: The formal relationship between regular languages, context-free languages, and the Chomsky hierarchy â€” and why string escape sequences sit at the boundary â€” is covered rigorously in <em>Introduction to the Theory of Computation</em> by Michael Sipser, Chapter 1 (DFAs and NFAs) and Chapter 2 (Context-Free Languages). If you want to understand <em>why</em> your scanner is technically still regular despite escape sequences, the key insight is in Sipser&#39;s construction showing that any finite-memory context can be encoded into states.</p>\n</blockquote>\n<hr>\n<h2 id=\"summary-what-you-have-built\">Summary: What You Have Built</h2>\n<p>Your scanner is now feature-complete for the full token vocabulary of your C-like language. In this milestone you added:</p>\n<ul>\n<li><strong>String literal scanning</strong> with correct raw-lexeme storage (including surrounding quotes), character-by-character accumulation using <code>list</code> + <code>&quot;&quot;.join()</code> for O(n) performance, and detection of unterminated strings at EOF and at bare newlines.</li>\n<li><strong>Escape sequence validation</strong>: <code>\\n</code>, <code>\\t</code>, <code>\\r</code>, <code>\\&quot;</code>, <code>\\\\</code> are accepted; any other escape character causes an Error token. The escape character pattern leaves interpretation to higher compilation phases.</li>\n<li><strong>Single-line comment skipping</strong> via <code>_skip_line_comment()</code>: advances to (but does not consume) the newline, then recurses in <code>next_token()</code> to find the next real token.</li>\n<li><strong>Multi-line comment skipping</strong> via <code>_skip_block_comment()</code>: consumes all characters including newlines (maintaining line tracking through <code>advance()</code>), stops at the first <code>*/</code> (non-nesting), and returns an Error token at the opening <code>/*</code> position if unterminated.</li>\n<li><strong>The <code>/</code> disambiguation</strong>: correctly distinguishes <code>//</code> (line comment), <code>/*</code> (block comment), and <code>/</code> alone (division operator) using the <code>_match()</code> peek-and-consume pattern from Milestone 2.</li>\n<li><strong>Comment-inside-string immunity</strong>: the state machine architecture ensures that <code>//</code> and <code>/*</code> inside string literals are never mistaken for comment markers â€” <code>_scan_string()</code> has taken over by the time those characters are consumed.</li>\n<li><strong>Line tracking inside multi-line constructs</strong>: every character in strings and comments is consumed via <code>advance()</code>, which handles newline detection â€” no special cases required.</li>\n<li><strong>Error positions that blame the cause</strong>: unterminated strings and comments report the opening delimiter&#39;s position, not the end-of-file where the error was detected.\nIn Milestone 4, you will test this complete scanner against multi-line programs, implement error recovery strategies, verify position accuracy across long inputs, and confirm the 10,000-line performance benchmark.</li>\n</ul>\n<hr>\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m4 -->\n<!-- MS_ID: tokenizer-m4 -->\n<h1 id=\"milestone-4-integration-testing-amp-error-recovery\">Milestone 4: Integration Testing &amp; Error Recovery</h1>\n<h2 id=\"where-you-are-in-the-pipeline\">Where You Are in the Pipeline</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-satellite-map.svg\" alt=\"Tokenizer System Satellite Map â€” The Complete Pipeline\"></p>\n<h2 id=\"you-have-built-a-complete-tokenizer-milestones-1-through-3-gave-you-a-scanner-that-handles-every-character-in-your-c-like-language-single-character-tokens-multi-character-operators-via-maximal-munch-number-and-identifier-literals-keywords-string-literals-with-escape-sequences-and-both-styles-of-comments-every-feature-works-in-isolation-this-milestone-is-about-the-hardest-part-of-building-any-software-system-making-sure-the-whole-is-correct-not-just-the-parts-integration-testing-is-where-you-discover-that-test_plus_token-passes-but-test_expression_with_spaces-fails-because-of-a-column-tracking-bug-that-only-appears-after-whitespace-is-consumed-it-is-where-you-find-that-string-scanning-works-for-quothelloquot-but-corrupts-position-tracking-for-every-token-that-follows-it-on-the-same-line-it-is-where-you-learn-that-the-scanner-you-built-in-15-hours-of-careful-work-has-a-subtle-off-by-one-that-only-shows-up-in-multi-line-programs-this-milestone-also-asks-you-to-confront-a-design-decision-you-made-without-realizing-it-what-happens-when-the-scanner-encounters-invalid-input-you-already-emit-error-tokens-for-unrecognized-characters-but-does-scanning-continue-afterward-do-you-collect-all-errors-or-stop-at-the-first-one-these-choices-determine-whether-your-scanner-is-useful-in-real-world-tools-like-ide-syntax-highlighters-or-whether-it-fails-the-moment-a-user-makes-a-typo-by-the-end-of-this-milestone-you-will-have-a-fully-validated-scanner-a-working-error-recovery-strategy-a-test-suite-that-covers-edge-cases-and-complete-programs-and-a-performance-result-that-confirms-your-implementation-meets-production-requirements\">You have built a complete tokenizer. Milestones 1 through 3 gave you a scanner that handles every character in your C-like language: single-character tokens, multi-character operators via maximal munch, number and identifier literals, keywords, string literals with escape sequences, and both styles of comments. Every feature works â€” in isolation.\nThis milestone is about the hardest part of building any software system: <strong>making sure the whole is correct, not just the parts.</strong>\nIntegration testing is where you discover that <code>test_plus_token()</code> passes but <code>test_expression_with_spaces()</code> fails because of a column-tracking bug that only appears after whitespace is consumed. It is where you find that string scanning works for <code>&quot;hello&quot;</code> but corrupts position tracking for every token that follows it on the same line. It is where you learn that the scanner you built in 15 hours of careful work has a subtle off-by-one that only shows up in multi-line programs.\nThis milestone also asks you to confront a design decision you made without realizing it: <strong>what happens when the scanner encounters invalid input?</strong> You already emit <code>ERROR</code> tokens for unrecognized characters â€” but does scanning continue afterward? Do you collect all errors or stop at the first one? These choices determine whether your scanner is useful in real-world tools like IDE syntax highlighters, or whether it fails the moment a user makes a typo.\nBy the end of this milestone, you will have a fully validated scanner, a working error recovery strategy, a test suite that covers edge cases and complete programs, and a performance result that confirms your implementation meets production requirements.</h2>\n<h2 id=\"the-revelation-unit-tests-are-necessary-but-not-sufficient\">The Revelation: Unit Tests Are Necessary but Not Sufficient</h2>\n<p>Here is the misconception this milestone exists to correct.\nYou have written tests after each milestone. Each test verifies one thing: <code>&quot;==&quot;</code> produces <code>EQUAL</code>, <code>&quot;3.14&quot;</code> produces a <code>NUMBER</code> with the right lexeme, <code>&quot;if&quot;</code> produces a <code>KEYWORD</code>. All your tests pass. You believe your scanner is correct.\nNow you feed it this program:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"\"if (x >= 42) {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    /* check value */</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    return true;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">}\"\"\"</span></span></code></pre></div>\n<p>You expect a clean token stream. Instead, you get a position drift: the token <code>return</code> shows <code>line=3, column=4</code> but your test expected <code>line=3, column=5</code>. Or worse â€” <code>true</code> appears to be on <code>line=2</code> when it should be on <code>line=3</code>.\nThis bug does not appear in any of your unit tests because it only happens when:</p>\n<ol>\n<li>A multi-line block comment is consumed (updating <code>self.line</code>)</li>\n<li>Followed by whitespace on the next line (consuming the indentation)</li>\n<li>Followed by a keyword token\nThe interaction between those three things â€” comment scanning, whitespace skipping, and keyword recognition â€” produces a bug that none of the three unit tests for those features individually would catch.\n<strong>This is the fundamental limitation of unit testing.</strong> Unit tests verify that components work in isolation. They cannot verify that components work in combination. The interactions are what break in real programs, and interactions only appear in integration tests.<blockquote>\n<p>ðŸ”‘ <strong>Integration vs. Unit Testing in Practice</strong></p>\n<p>A <strong>unit test</strong> tests a single function or method with controlled inputs. Its job is to verify that the unit does what its contract says. A unit test for <code>_scan_number()</code> verifies that <code>&quot;3.14&quot;</code> returns <code>Token(NUMBER, &quot;3.14&quot;, ...)</code>.</p>\n<p>An <strong>integration test</strong> tests a complete flow â€” multiple components working together â€” with realistic inputs. Its job is to verify that the components compose correctly. An integration test feeds a complete program to <code>scan_tokens()</code> and verifies the entire output token list, position by position.</p>\n<p>The key insight: <strong>unit tests verify correctness of parts, integration tests verify correctness of interactions.</strong> You need both. A system with only unit tests may still fail in production. A system with only integration tests is hard to debug when it fails (too much to narrow down). The combination â€” unit tests for isolation, integration tests for composition â€” is what builds confidence.\nThe reveal applies to error recovery as well. You might assume: &quot;if the input has an error, stop scanning â€” the input is already invalid, so why produce more tokens?&quot; This assumption is natural. It is also wrong for any tool that processes real-world code.\n<strong>Production tokenizers never stop at the first error.</strong> Here is why: consider a 500-line source file. The programmer made a typo on line 3 â€” they wrote <code>@x</code> instead of <code>x</code>. If your scanner stops at the <code>@</code>, the IDE&#39;s syntax highlighter goes dark for all 497 remaining lines. The type checker cannot check anything. The linter cannot lint. The programmer sees a completely black file with one error message.\nContrast this with error recovery: the scanner emits <code>ERROR(&quot;@&quot;, 1, 3)</code>, then immediately resumes scanning from <code>x</code>. All remaining tokens are produced correctly. The IDE highlights syntax errors on line 3 while correctly rendering and checking everything else. The programmer sees their file with one red squiggle.\nError recovery transforms your scanner from a toy into a tool.</p>\n</blockquote>\n</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m4-error-recovery-trace.svg\" alt=\"Trace: Error Recovery in Action\"></p>\n<hr>\n<h2 id=\"error-recovery-the-minimal-viable-strategy\">Error Recovery: The Minimal Viable Strategy</h2>\n<p>Error recovery in a lexer is dramatically simpler than error recovery in a parser. A parser encountering an error must figure out how to re-synchronize with the grammar â€” a complex process involving panic mode, error productions, and synchronization tokens. A lexer encountering an error has a trivially simple strategy:</p>\n<ol>\n<li>Emit an <code>ERROR</code> token for the unrecognized character.</li>\n<li>Advance past that character.</li>\n<li>Resume normal scanning from the next character.\nThat is it. The bad character is consumed, reported, and forgotten. The scanner&#39;s state is unchanged: it is still in the <code>START</code> state, ready to scan the next token. No special re-synchronization logic is needed.\nYou have already implemented this! Look at the fallthrough case in <code>next_token()</code>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Unrecognized character â€” error token, continue scanning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, tok_line, tok_col)</span></span></code></pre></div>\n<p><code>char</code> is already consumed by the <code>advance()</code> call at the top of <code>next_token()</code>. After this return, the cursor sits at the character <em>after</em> the bad one. The next call to <code>next_token()</code> proceeds normally. Error recovery is the natural consequence of the consume-then-dispatch architecture you built in Milestone 1.\nThe question is: does your code actually do this, or does it accidentally stop? Let us verify with a test:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_recovery_continues_scanning</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"After an invalid character, scanning resumes from the next character.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span></code></pre></div>\n<p>If this test passes, your error recovery is working. The <code>@</code> is reported as an error and the <code>+</code> is scanned correctly on the next call.</p>\n<h3 id=\"collecting-multiple-errors\">Collecting Multiple Errors</h3>\n<p>The next step: ensure that multiple errors in a single input are all reported, not just the first. The <code>scan_tokens()</code> method already handles this â€” it loops until EOF, collecting every token including ERROR tokens. There is no early exit on error.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multiple_errors_all_reported</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All invalid characters in a file are reported as separate ERROR tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@#$\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(error_tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"#\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"$\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<h3 id=\"errors-mixed-with-valid-tokens\">Errors Mixed with Valid Tokens</h3>\n<p>A realistic scenario: invalid characters sprinkled throughout otherwise valid code. Your scanner should produce the correct valid tokens and the error tokens for the invalid ones, all in the correct order:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_errors_mixed_with_valid_tokens</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Invalid chars produce Error tokens; surrounding valid tokens are unaffected.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x @ y\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"y\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span></code></pre></div>\n<p>This is the key behavioral guarantee: an error does not corrupt the surrounding valid tokens. The column of <code>&quot;y&quot;</code> is still correct (column 5) even though an error occurred two characters earlier.</p>\n<h3 id=\"extracting-errors-from-a-token-stream\">Extracting Errors from a Token Stream</h3>\n<p>In a real compiler or IDE, you often want to process errors and valid tokens separately. A common pattern:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> partition_tokens</span><span style=\"color:#E1E4E8\">(tokens: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">) -> tuple[</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Separate a token stream into (valid_tokens, error_tokens).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Both lists maintain order. EOF is kept in valid_tokens.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    valid </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> valid, errors</span></span></code></pre></div>\n<h2 id=\"this-is-not-scanner-logic-it-is-consumer-logic-your-scanner39s-job-is-to-emit-all-tokens-including-errors-the-consumer-ide-compiler-linter-decides-what-to-do-with-them-keeping-the-concerns-separate-is-clean-api-design\">This is not scanner logic â€” it is consumer logic. Your scanner&#39;s job is to emit all tokens including errors. The consumer (IDE, compiler, linter) decides what to do with them. Keeping the concerns separate is clean API design.</h2>\n<h2 id=\"the-token-stream-as-an-interface-contract\">The Token Stream as an Interface Contract</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m4-token-stream-contract.svg\" alt=\"The Token Stream as Parser Interface Contract\"></p>\n<p>Before writing integration tests, it helps to understand exactly what you are testing and why exactness matters.\nYour scanner&#39;s public contract is this:</p>\n<blockquote>\n<p><strong>For any given source string, <code>scan_tokens()</code> returns a specific, deterministic sequence of tokens where every token has the correct type, lexeme, line, and column.</strong>\nThis is an <strong>interface contract</strong> â€” a promise your scanner makes to every piece of code that consumes its output. The parser you will eventually build (in a future project) will depend on this contract. If your scanner returns <code>KEYWORD(&quot;if&quot;, 1, 1)</code> for some inputs and <code>IDENTIFIER(&quot;if&quot;, 1, 1)</code> for others, the parser will misinterpret the input. If column numbers drift after line 10, the parser&#39;s error messages will point at wrong locations and confuse users.\nThe contract has four dimensions:</p>\n</blockquote>\n<ol>\n<li><strong>Token type</strong> â€” correct categorization</li>\n<li><strong>Lexeme</strong> â€” exact raw text from source</li>\n<li><strong>Line</strong> â€” 1-indexed line number of the token&#39;s first character</li>\n<li><strong>Column</strong> â€” 1-indexed column number of the token&#39;s first character\nWhen you write integration tests, you verify all four dimensions for every token in the stream. Not just &quot;does a KEYWORD appear?&quot; â€” but &quot;does <code>Token(KEYWORD, &#39;if&#39;, 3, 5)</code> appear?&quot; This level of precision is what catches interaction bugs that unit tests miss.<blockquote>\n<p>ðŸ”‘ <strong>Why &quot;Token Stream as Contract&quot; Matters Beyond This Project</strong></p>\n<p>The parser that consumes your token stream never sees the original source text. It only sees tokens. If the token stream is correct, the parser can be correct â€” regardless of what the source looks like. If the token stream has bugs (wrong types, wrong positions), the parser cannot compensate; it will propagate errors downstream. This separation â€” &quot;lexer produces tokens, parser consumes tokens, never characters&quot; â€” is an architectural boundary that allows the two components to evolve independently. You can change the scanner&#39;s implementation completely (say, switch from Python string indexing to a memory-mapped file) without changing the parser at all, as long as the token stream contract is preserved.</p>\n</blockquote>\n</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m4-integration-test-anatomy.svg\" alt=\"Integration Test Anatomy: Input â†’ Expected Token Stream\"></p>\n<hr>\n<h2 id=\"the-canonical-integration-test-token-by-token-verification\">The Canonical Integration Test: Token-by-Token Verification</h2>\n<p>The acceptance criteria for this milestone include a specific required test: the string <code>&quot;if (x &gt;= 42) { return true; }&quot;</code> must produce exactly the token stream listed. Let us write that test properly:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_canonical_statement</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Canonical acceptance test: exact token stream for a complete statement.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Verifies type, lexeme, line, and column for every token.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"if (x >= 42) { return true; }\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"(\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\")\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"{\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">14</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">23</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#9ECBFF\">\";\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">27</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"}\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">29</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected), (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(expected)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> tokens, got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(tokens)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tokens</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (got, exp) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(tokens, expected)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> got </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> exp, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">exp</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">got</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n<p>Notice how the assertion message includes useful debug context: which token index failed, what was expected, and what was actually produced. When this test fails (and the first time through, it often does), you need to know <em>which</em> token was wrong, not just that <em>a</em> token was wrong.\nA few things to verify manually by counting characters:</p>\n<ul>\n<li><code>if</code> starts at column 1 âœ“</li>\n<li><code>(</code> is at column 4: <code>if </code> = 3 chars, then <code>(</code> âœ“</li>\n<li><code>x</code> is at column 5: right after <code>(</code> âœ“</li>\n<li><code>&gt;=</code> is at column 7: <code>x </code> then <code>&gt;=</code> â€” <code>x</code> at 5, space at 6, <code>&gt;</code> at 7 âœ“</li>\n<li><code>42</code> is at column 10: <code>&gt;= </code> then <code>42</code> â€” <code>&gt;=</code> takes 7-8, space at 9, <code>4</code> at 10 âœ“\nManually walking through the columns before writing the test prevents you from testing the wrong expected values. A test with wrong expected values is worse than no test â€” it will pass incorrectly and hide bugs.</li>\n</ul>\n<hr>\n<h2 id=\"the-multi-line-integration-test\">The Multi-Line Integration Test</h2>\n<p>A single-line test does not exercise newline handling, multi-line comment tracking, or position drift. You need a complete multi-line program. Here is a representative test program:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">MULTI_LINE_PROGRAM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"\"</span><span style=\"color:#79B8FF\">\\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">// Simple function to check bounds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">/* This checks:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   - lower bound</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   - upper bound */</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if (x >= 0) {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = \"valid\";</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">} else {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = \"invalid\";</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">return result;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span></code></pre></div>\n<p>This program exercises:</p>\n<ul>\n<li>Single-line comment (line 1)</li>\n<li>Multi-line block comment spanning lines 2â€“4</li>\n<li>Identifiers, operators, numbers, string literals</li>\n<li>Keywords: <code>if</code>, <code>else</code>, <code>return</code></li>\n<li>Multiple lines with indentation\nHere is the complete verification test:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multi_line_program</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Full integration test: verifies exact token stream for a multi-line program</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    that exercises comments, strings, operators, keywords, and position tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"\"</span><span style=\"color:#79B8FF\">\\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">// Simple function to check bounds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">/* This checks:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   - lower bound</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   - upper bound */</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if (x >= 0) {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = \"valid\";</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">} else {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = \"invalid\";</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">return result;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Remove error tokens from the stream for this check (program is valid)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [], </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unexpected errors: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">error_tokens</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # After two comment lines (1 single-line + 3 lines of block comment = 4 lines total),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'if' appears on line 5.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    if_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> if_token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> if_token.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"if\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> if_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> if_token.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'x' follows '(' on line 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> x_token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> x_token.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"x\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> x_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # First string \"valid\" is on line 6</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    first_string </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> first_string.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"valid\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> first_string.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 6</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Second string \"invalid\" is on line 8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    strings </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(strings) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> strings[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"invalid\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> strings[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'return' appears on line 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    return_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"return\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> return_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> return_token.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Token stream ends with EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span></code></pre></div>\n<p>This test does not pin every single token â€” that would be hundreds of assertions for a 10-line program. Instead, it pins the tokens that exercise the most error-prone features: the first token after comments (position drift from multi-line comments), string tokens (lexeme content and position), and <code>return</code> after multiple lines (accumulated line tracking). Add more pinned tokens as you discover specific bugs.</p>\n<h3 id=\"building-the-full-expected-token-stream\">Building the Full Expected Token Stream</h3>\n<p>For the truly rigorous version, build the complete expected list and compare it all at once. This is more work to set up but catches any token-count discrepancy (extra or missing tokens) that selective pinning misses:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_canonical_token_count</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Verifies the total number of tokens in the multi-line program.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    A count mismatch means a token was doubled, skipped, or a comment ate a token.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"if (x >= 0) {</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">    return true;</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Manually counted: if, (, x, >=, 0, ), {, return, true, ;, }, EOF = 12</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 12</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(tokens)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> tokens: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tokens</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n<p>Count mismatch is one of the most common integration bugs. It happens when:</p>\n<ul>\n<li>A comment accidentally eats the token following it (comment skip consumes one character too many)</li>\n<li>A string&#39;s closing quote is consumed as the start of the next token (the string scanner doesn&#39;t stop at <code>&quot;</code>)</li>\n<li>A two-character operator is emitted as two single-character tokens (maximal munch not applied)\nThe count check catches all of these before you even look at individual token types.</li>\n</ul>\n<hr>\n<h2 id=\"position-tracking-the-invariant-approach\">Position Tracking: The Invariant Approach</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m4-position-drift.svg\" alt=\"Position Tracking Drift â€” How Bugs Accumulate\"></p>\n<p>Position tracking bugs are the trickiest class of scanner bugs because they are cumulative. A single off-by-one in column counting does not immediately cause a test failure â€” it causes every subsequent token on the same line to be off by one. After a newline, the bug resets. By the time you notice the problem (because a token on line 15 has the wrong column), the bug occurred on line 3 and has been drifting silently for 12 lines.\nThis is called <strong>position drift</strong>: a small error in position tracking accumulates over time and produces errors that are hard to trace back to their origin.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m4-position-drift.svg\" alt=\"Position Tracking Drift â€” How Bugs Accumulate\"></p>\n<p>The defense against position drift is <strong>invariant-based testing</strong>: instead of testing specific tokens, you test properties that must hold for <em>all</em> tokens in a scan result.\nHere are three invariants that are always true for a correct scanner:\n<strong>Invariant 1: Every token&#39;s line is between 1 and the total number of newlines plus 1.</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_line_bounds</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All token line numbers are within [1, max_line].\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">b</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">c\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    newline_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.count(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> newline_count </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # 3 newlines â†’ lines 1 through 4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#E1E4E8\"> token.line </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> max_line, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> has out-of-bounds line number\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n<p><strong>Invariant 2: Every token&#39;s column is at least 1.</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_column_at_least_one</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Column numbers are always 1-indexed (never 0 or negative).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a + b</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">x * y\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.column </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> has invalid column </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n<p><strong>Invariant 3: Tokens on the same line have non-decreasing column numbers.</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_columns_non_decreasing_per_line</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"On any given line, token columns are strictly non-decreasing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a + b * c\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prev_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prev_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> prev_line:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> token.column </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> prev_col, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"Column went backwards: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> after col </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prev_col</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            prev_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">  # reset for new line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        prev_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> token.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        prev_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> token.column</span></span></code></pre></div>\n<p>These invariants do not prove that specific positions are correct â€” they prove that position tracking is internally consistent. If an invariant fails, you know exactly which category of bug to look for. Passing all three invariants means your position tracking is at least self-consistent; then specific pinned tests verify exact values.</p>\n<blockquote>\n<p>ðŸ”­ <strong>Deep Dive</strong>: The technique of testing invariants rather than specific values is the foundation of <strong>property-based testing</strong> (PBT). Tools like <code>Hypothesis</code> (Python) generate random inputs and verify that invariants hold across all of them â€” not just the cases you thought of. Hypothesis&#39;s documentation has an excellent tutorial on using it for parsers and scanners. If you want to harden your scanner further, writing a Hypothesis test that checks the three invariants above against randomly generated source strings is an afternoon project that will find bugs you never thought to look for.</p>\n</blockquote>\n<hr>\n<h2 id=\"edge-cases-the-boundary-conditions\">Edge Cases: The Boundary Conditions</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m4-edge-cases-map.svg\" alt=\"Edge Case Decision Map\"></p>\n<p>Edge cases are inputs that sit at the boundary of what your scanner handles. They are disproportionately likely to contain bugs because they test the limits of the assumptions you made during implementation.</p>\n<h3 id=\"empty-input\">Empty Input</h3>\n<p>The simplest possible input: an empty string. This tests whether your scanner handles the zero-character case without crashing.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_empty_input</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Empty string produces exactly one EOF token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<p>The EOF token&#39;s position is <code>1:1</code> â€” even though there are no characters, the scanner starts at line 1, column 1. This is correct: the &quot;position&quot; of end-of-file is the position where the next character would have been.</p>\n<h3 id=\"single-character-input\">Single Character Input</h3>\n<p>Single character inputs test the boundary between &quot;one-character token&quot; and &quot;end of file&quot;:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_valid_char</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single valid character produces one token and EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_invalid_char</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single invalid character produces ERROR token and EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A single newline is whitespace â€” produces only EOF on line 2.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span></code></pre></div>\n<p>The last test is tricky: a single <code>\\n</code> is consumed as whitespace. After consuming it, <code>self.line</code> becomes 2 and <code>self.column</code> becomes 1. Then <code>is_at_end()</code> returns True and an EOF token is emitted at <code>(2, 1)</code>. This is correct â€” the &quot;position after the newline&quot; is the start of line 2.</p>\n<h3 id=\"maximum-length-identifiers\">Maximum-Length Identifiers</h3>\n<p>Identifiers in your scanner can be arbitrarily long (no maximum is specified). The test ensures long identifiers work correctly without truncation or other issues:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_long_identifier</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Long identifiers are scanned correctly as a single token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    long_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(long_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> long_name</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_long_identifier_not_keyword</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A long identifier that starts with a keyword prefix is still an IDENTIFIER.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'if' followed by 998 more characters â€” full lexeme is not a keyword</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    long_if </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"if\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#9ECBFF\"> \"x\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 998</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(long_if)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> long_if</span></span></code></pre></div>\n<h3 id=\"whitespace-only-input\">Whitespace-Only Input</h3>\n<p>What if the entire source is spaces and newlines? No tokens should be emitted except EOF:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_whitespace_only</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Source with only whitespace produces only an EOF token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"   </span><span style=\"color:#79B8FF\">\\t\\t\\n\\n</span><span style=\"color:#79B8FF\">  \\r\\n</span><span style=\"color:#9ECBFF\">  \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # After 3 newlines (\\n, \\n, and \\r\\n = 1), we're on line 4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_comment_only</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Source with only a comment produces only an EOF token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"// this is the entire file\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span></code></pre></div>\n<h3 id=\"tokens-at-end-of-file-without-trailing-newline\">Tokens at End of File Without Trailing Newline</h3>\n<p>Many real source files do not end with a newline. Your scanner must handle this:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_token_at_eof_no_trailing_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Last token in file (no trailing newline) is correctly scanned.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_number_at_eof</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Number literal at end of file (no newline) is complete.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<h3 id=\"adjacent-tokens-without-whitespace\">Adjacent Tokens Without Whitespace</h3>\n<p>Your scanner must correctly separate tokens that touch each other with no whitespace between them:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_adjacent_operators_no_space</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Operators with no whitespace are correctly separated.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"a+b\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"a\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"b\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_adjacent_comparison_no_space</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Two-character operator adjacent to identifier.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x>=y\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"y\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_number_adjacent_to_operator</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Number literal directly adjacent to an operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"42+3\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"3\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<hr>\n<h2 id=\"performance-the-10000-line-benchmark\">Performance: The 10,000-Line Benchmark</h2>\n<p>Your tokenizer must tokenize a 10,000-line source file in under 1 second. Let us measure this with a real benchmark:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_large_source</span><span style=\"color:#E1E4E8\">(num_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generate a realistic large source file.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Each line is a complete statement with identifiers, operators, and numbers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(num_lines):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Mix of different statement types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"// comment on line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"result_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = value_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> + </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2}</span><span style=\"color:#9ECBFF\">;\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'message_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = \"string literal </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\";'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"if (count_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> >= </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) </span><span style=\"color:#79B8FF\">{{</span><span style=\"color:#9ECBFF\"> return true; </span><span style=\"color:#79B8FF\">}}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"x_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">.</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 100}</span><span style=\"color:#9ECBFF\">;\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(lines)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_performance_10k_lines</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Tokenizing 10,000 lines must complete in under 1 second.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is the performance requirement from the project spec.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_large_source(</span><span style=\"color:#79B8FF\">10_000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Warm up (Python JIT and import caches)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source[:</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Performance: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(tokens)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> tokens in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">elapsed</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Source: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(source)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> characters, </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">source.count(</span><span style=\"color:#79B8FF\">chr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">))</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1}</span><span style=\"color:#9ECBFF\"> lines\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Rate: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(source) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1_000_000</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> MB/s\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Tokenizing </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">source.count(</span><span style=\"color:#79B8FF\">chr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">))</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1}</span><span style=\"color:#9ECBFF\"> lines took </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">elapsed</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s (limit: 1.0s)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Also verify the output is not trivially wrong</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 10_000</span><span style=\"color:#6A737D\">  # Sanity check: at least one token per line</span></span></code></pre></div>\n<p>Run this test with <code>python -m pytest test_scanner.py::test_performance_10k_lines -v -s</code> (the <code>-s</code> flag prints stdout so you see the performance stats).</p>\n<h3 id=\"what-to-expect\">What to Expect</h3>\n<p>On a modern machine, a Python scanner tokenizing a 10,000-line file will typically complete in 0.1â€“0.4 seconds â€” well under the 1-second limit. If your scanner is slower, the most likely culprits are:\n<strong>String concatenation in tight loops:</strong> If <code>_scan_identifier()</code> or <code>_scan_number()</code> uses <code>lexeme += char</code> in a loop for very long tokens, and your source has pathologically long identifiers (thousands of characters), the O(nÂ²) concatenation cost becomes visible. Switch to <code>list</code> + <code>&quot;&quot;.join()</code> for those methods.\n<strong>Excessive function call overhead:</strong> If you have added many layers of method calls for each character, Python&#39;s function call overhead accumulates. The character-level hot path (<code>advance()</code> â†’ <code>next_token()</code> â†’ dispatch) should be as direct as possible.\n<strong>Unnecessary work inside loops:</strong> If <code>_skip_whitespace()</code> does more than check <code>is_at_end()</code> and <code>peek()</code>, it will be slow because it runs once per token.\nFor the purposes of this project, the straightforward implementation passes the benchmark easily. Do not optimize prematurely â€” measure first, then optimize only if the benchmark fails.</p>\n<blockquote>\n<p><strong>On benchmarking methodology:</strong> <code>time.perf_counter()</code> measures wall-clock time, which can vary with system load. For precise benchmarking, run the timed section 3â€“5 times and take the minimum (not average) â€” the minimum reflects the best-case execution time unaffected by system noise. Python&#39;s <code>timeit</code> module automates this. The 1-second limit is generous enough that a single run of <code>perf_counter()</code> is sufficient here.</p>\n</blockquote>\n<hr>\n<h2 id=\"the-complete-integration-test-suite\">The Complete Integration Test Suite</h2>\n<p>Here is the full test file for Milestone 4, organized by category:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test_scanner_integration.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Integration tests for the complete scanner.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">These tests verify the scanner as a whole, not individual methods.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> scanner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Scanner, Token, TokenType</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Canonical acceptance tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_canonical_statement</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Exact token stream for the spec's required test case.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"if (x >= 42) { return true; }\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"(\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\")\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"{\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">14</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">23</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#9ECBFF\">\";\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">27</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#9ECBFF\">\"}\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">29</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected), </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Count mismatch: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tokens</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (got, exp) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(tokens, expected)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> got </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> exp, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Token[</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">]: expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">exp</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">got</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Error recovery tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_error_recovery</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"After one invalid char, scanning continues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@+\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">PLUS</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multiple_errors_all_collected</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All invalid characters in a file produce separate Error tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@#$\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"@\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"#\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"$\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_does_not_corrupt_surrounding_tokens</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Invalid character does not affect column of following token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x @ y\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"y\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_errors_on_multiple_lines</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors on different lines have correct line numbers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">#\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Edge case tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_empty_input</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_plus</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">PLUS</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_whitespace_only</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"   </span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">   \"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_long_identifier</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"x\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 500</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(name).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> name</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_keyword_prefix_is_identifier</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'iffy' starts with 'if' but is an IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"iffy\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"iffy\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_adjacent_tokens_no_space</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"a+b\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">PLUS</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Position accuracy tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_after_multiline_block_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tokens after a multi-line comment have correct line numbers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">/* line 2</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">line 3</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">*/</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">b\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> a_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> b_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">  # 'b' is on line 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_after_line_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token after a line comment is on the next line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"// comment</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">x\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_columns_reset_after_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Column resets to 1 at the start of each new line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"abc</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">def\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # 'abc' at col 1 on line 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # 'def' at col 1 on line 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_invariant_columns_nondecreasing</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"On any single line, token columns are strictly increasing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"a + b * c\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cols </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.column </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(cols) </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> cols[i] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> cols[i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Column went backwards at index </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cols</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_invariant_lines_valid</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All line numbers are in [1, total_lines].\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">b</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">c</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">d</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">e\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.count(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#E1E4E8\"> t.line </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> max_line, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid line in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">t</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_position_is_opening_quote</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"String token position is the opening '\"', not the closing one.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'   \"hello\"'</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">  # opening '\"' at column 4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_string_position_is_opening</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error token for unterminated string points at opening quote.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'x = \"hello'</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">  # '\"' is at column 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_comment_position_is_opening</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error token for unterminated comment points at '/*'.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x /* never closed\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"/*\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">  # '/*' starts at column 3</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Multi-line integration tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multi_line_program_no_errors</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"\"</span><span style=\"color:#79B8FF\">\\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if (x >= 0) {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = \"valid\";</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">} else {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = \"invalid\";</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">return result;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [], </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unexpected errors: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">errors</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multi_line_program_with_comments</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"\"</span><span style=\"color:#79B8FF\">\\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">// comment on line 1</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">/* block comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   spanning line 3 */</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">x = 42;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # After 3 comment lines, 'x' is on line 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    non_eof </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"x\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_program_token_types</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify the sequence of token types for a complete program.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"while (i != 0) { i = i - 1; }\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#6A737D\"># while</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">LPAREN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\"># i</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\"># i</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\"># i</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#6A737D\"># -</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># ;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> types </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Performance test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_large_source</span><span style=\"color:#E1E4E8\">(num_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(num_lines):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"// line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"var_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> + </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">2}</span><span style=\"color:#9ECBFF\">;\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'msg_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = \"value </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\";'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"if (x_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> >= </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) </span><span style=\"color:#79B8FF\">{{</span><span style=\"color:#9ECBFF\"> return false; </span><span style=\"color:#79B8FF\">}}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(lines)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_performance_10k_lines</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_large_source(</span><span style=\"color:#79B8FF\">10_000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"10k lines took </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">elapsed</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s (limit: 1.0s)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span></code></pre></div>\n<h2 id=\"run-the-full-suite-python-m-pytest-test_scanner_integrationpy-v\">Run the full suite: <code>python -m pytest test_scanner_integration.py -v</code></h2>\n<h2 id=\"diagnosing-failures-a-debugging-playbook\">Diagnosing Failures: A Debugging Playbook</h2>\n<p>When an integration test fails, the error message tells you what went wrong but not why. Here is a systematic approach to debugging.</p>\n<h3 id=\"symptom-wrong-token-count\">Symptom: Wrong token count</h3>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>AssertionError: Expected 12 tokens, got 11</code></pre></div>\n<p><strong>Diagnosis</strong>: A token was silently skipped. Check:</p>\n<ol>\n<li>Is the comment skip consuming one character past the end of the comment? Print every token from <code>Scanner(&quot;// comment\\nx&quot;).scan_tokens()</code> â€” is <code>x</code> present?</li>\n<li>Is the string scan consuming the character after the closing <code>&quot;</code>?</li>\n<li>Did you add a new keyword but forget to stop scanning the identifier at the keyword boundary?\n<strong>Debug technique</strong>: Add <code>print(tokens)</code> before the assertion, or use <code>pytest -v -s</code> to see stdout.</li>\n</ol>\n<h3 id=\"symptom-wrong-token-type\">Symptom: Wrong token type</h3>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>AssertionError: Token[3]: expected Token(GREATER_EQ, &quot;&gt;=&quot;, 1, 7), got Token(GREATER, &quot;&gt;&quot;, 1, 7)</code></pre></div>\n<p><strong>Diagnosis</strong>: Maximal munch is not working for this case. Check:</p>\n<ol>\n<li>Is the <code>_match(&quot;=&quot;)</code> call consuming the <code>=</code>? Add a breakpoint inside <code>_scan_operator()</code>.</li>\n<li>Is the operator character in <code>OPERATOR_CHARS</code>? If you modified the set and forgot to include <code>&gt;</code>, it falls through to the error case.</li>\n</ol>\n<h3 id=\"symptom-wrong-position-line-off\">Symptom: Wrong position (line off)</h3>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>AssertionError: Token[0]: expected Token(KEYWORD, &quot;if&quot;, 5, 1), got Token(KEYWORD, &quot;if&quot;, 4, 1)</code></pre></div>\n<p><strong>Diagnosis</strong>: A newline was not counted. Check:</p>\n<ol>\n<li>Is <code>advance()</code> being called for every character inside comments and strings? If you ever increment <code>self.current</code> directly (bypassing <code>advance()</code>), newlines in those regions are missed.</li>\n<li>Is <code>\\r\\n</code> being counted as two newlines? Add <code>test_windows_line_endings()</code> to verify.</li>\n</ol>\n<h3 id=\"symptom-wrong-position-column-off\">Symptom: Wrong position (column off)</h3>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>AssertionError: Token[2]: expected Token(IDENTIFIER, &quot;x&quot;, 1, 5), got Token(IDENTIFIER, &quot;x&quot;, 1, 6)</code></pre></div>\n<p><strong>Diagnosis</strong>: Column tracking has an off-by-one. Check:</p>\n<ol>\n<li>Is <code>tok_col = self.column</code> captured before or after <code>advance()</code>? It must be before.</li>\n<li>Does <code>advance()</code> increment column for the newline character itself? It should increment <code>self.line</code> and <em>set</em> <code>self.column = 1</code> (not increment from 0).</li>\n<li>Is whitespace skipping done before position capture? <code>_skip_whitespace()</code> must run before <code>tok_col = self.column</code>.</li>\n</ol>\n<hr>\n<h2 id=\"error-recovery-the-philosophy-behind-the-mechanism\">Error Recovery: The Philosophy Behind the Mechanism</h2>\n<p>You now have a working error recovery strategy. But <em>why</em> does this strategy â€” emit an error, keep going â€” work so well? And why do other strategies fail?\n<strong>Strategy 1: Stop at first error (the naive approach)</strong>\nEvery error raises an exception or returns immediately. Simple to implement. Catastrophically bad user experience: one typo makes the entire file un-parseable.\nIn practice, this means the IDE stops responding, the type checker gives up, and the user has no guidance on any other problems in the file. This is the behavior of Python 2&#39;s tokenizer for some error cases â€” and it was one of the complaints that motivated improvements in Python 3.\n<strong>Strategy 2: Skip until next &quot;safe&quot; character (panic mode)</strong>\nOn error, the scanner consumes characters until it finds something it recognizes â€” a semicolon, a newline, or a keyword. Then it resumes. This prevents the scanner from emitting cascading errors, but it may silently skip valid tokens. Used by some parsers (where the grammar is complex enough to need it), but overkill for a lexer.\n<strong>Strategy 3: Emit error token, advance one character, resume (your strategy)</strong>\nThis is the simplest strategy that produces correct behavior. The invariant is: every call to <code>next_token()</code> consumes at least one character (via the <code>advance()</code> at the top) and returns exactly one token. An error token corresponds to exactly one unrecognized character. No characters are skipped; no valid tokens are lost.\nThis strategy is sometimes called <strong>character-level error recovery</strong> â€” errors are at the granularity of a single character, the smallest possible unit. It is the correct strategy for a lexer because lexical errors are local: a <code>@</code> in the wrong place does not affect the meaning of the <code>+</code> two characters later.</p>\n<blockquote>\n<p>ðŸ”‘ <strong>The Circuit Breaker Analogy</strong></p>\n<p>Your error recovery strategy is the same principle as a <strong>circuit breaker</strong> in electrical engineering and distributed systems. A circuit breaker does not stop the entire electrical grid when one appliance fails â€” it isolates the fault (the bad token/appliance), reports it (error token/alert), and keeps the rest of the system running normally. In microservices, a circuit breaker pattern isolates failures in one service so that other services continue operating in a degraded mode rather than failing completely. TCP&#39;s retransmission mechanism is another example: a lost packet causes that packet to be retransmitted, but does not halt transmission of subsequent packets. The common principle is <strong>fault isolation</strong> â€” contain the damage, report it, and keep moving. Your scanner is a circuit breaker for lexical faults.</p>\n</blockquote>\n<hr>\n<h2 id=\"the-final-scanner-what-you-have-built\">The Final Scanner: What You Have Built</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-m4-integration-test-anatomy.svg\" alt=\"Integration Test Anatomy: Input â†’ Expected Token Stream\"></p>\n<p>Stand back and look at the complete system you have built across four milestones.\n<strong>The API your scanner exposes:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">...</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]: </span><span style=\"color:#79B8FF\">...</span></span></code></pre></div>\n<p>Two methods. One input (source text). One output (token stream). Everything else is implementation detail hidden inside the class. This is a clean, minimal API â€” a consumer only needs to know about these two things to use your scanner correctly.\n<strong>What the scanner guarantees:</strong></p>\n<ol>\n<li>Every character in the source is consumed exactly once (O(n) time, no backtracking)</li>\n<li>Every token in the output has a correct type, exact lexeme, and accurate start position</li>\n<li>Invalid characters produce ERROR tokens but do not halt scanning</li>\n<li>The last token in every stream is always EOF</li>\n<li>Comments and whitespace produce no tokens but update position tracking correctly\n<strong>The grammar your scanner recognizes:</strong>\nYour scanner handles the lexical grammar of a C-like language. The full vocabulary, after four milestones:</li>\n</ol>\n<ul>\n<li>Keywords: <code>if</code>, <code>else</code>, <code>while</code>, <code>return</code>, <code>true</code>, <code>false</code>, <code>null</code></li>\n<li>Identifiers: <code>[a-zA-Z_][a-zA-Z0-9_]*</code></li>\n<li>Numbers: <code>[0-9]+(\\.[0-9]+)?</code></li>\n<li>Strings: <code>&quot;([^&quot;\\\\]|\\\\.)*&quot;</code></li>\n<li>Operators: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>=</code>, <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>!</code></li>\n<li>Punctuation: <code>(</code>, <code>)</code>, <code>{</code>, <code>}</code>, <code>[</code>, <code>]</code>, <code>;</code>, <code>,</code></li>\n<li>Comments: <code>//[^\\n]*</code> and <code>/*.**/</code> (non-greedy)\nAll of this recognized in a single left-to-right pass with at most 2 characters of lookahead.</li>\n</ul>\n<hr>\n<h2 id=\"knowledge-cascade-what-this-unlocks\">Knowledge Cascade: What This Unlocks</h2>\n<p><strong>1. Your scanner is an IDE foundation.</strong>\nEvery language feature in modern IDEs â€” syntax highlighting, error underlines, code completion, &quot;go to definition&quot; â€” starts with a token stream that looks exactly like what you have built. Language Server Protocol (LSP), which powers VS Code and Neovim&#39;s language support, defines its position type as <code>{line: int, character: int}</code> â€” literally the same fields as your <code>Token.line</code> and <code>Token.column</code>. The infrastructure you built is the real infrastructure. Language servers in production build on the same primitive.\n<strong>2. Error recovery across domains: resilience over correctness.</strong>\nYour error recovery strategy embodies a principle that appears across all of distributed systems: <strong>prefer degraded operation over complete failure</strong>. RAID-5 keeps running after one disk fails. A Kubernetes pod in <code>CrashLoopBackoff</code> keeps retrying instead of bringing down the node. A CDN with one edge node down routes traffic to the nearest healthy node. TCP retransmits lost packets without halting the entire connection. In every case, the system isolates the fault, reports it, and continues operating in a reduced capacity. Your scanner does the same: it isolates bad characters, reports them as ERROR tokens, and keeps scanning. The parallel is not coincidental â€” it reflects a universal engineering principle: <strong>fault isolation is more valuable than fault prevention</strong>.\n<strong>3. Integration testing philosophy generalizes everywhere.</strong>\nThe lesson of this milestone â€” &quot;unit tests verify parts, integration tests verify interactions&quot; â€” applies to every system you will ever build. Microservice teams unit-test individual services and integration-test the service mesh. Frontend teams unit-test React components and integration-test complete user flows. Database teams unit-test query planning and integration-test multi-table joins. The specific failure modes change (column drift vs. network latency vs. lock contention), but the structure of the problem is always the same: emergent bugs only appear when components are combined. Build integration tests as early as unit tests, not as an afterthought.\n<strong>4. Position tracking is the foundation of developer experience.</strong>\nThe <code>(line, column)</code> pairs in your token stream are the foundation of every developer-facing error message in any language tool. Rust&#39;s famous &quot;borrow checker&quot; errors â€” the ones that point precisely to the conflicting borrow, the reborrow, and the use-after-move, all with line/column accuracy â€” are possible because every AST node carries position metadata that traces back to the original token&#39;s position. Python&#39;s <code>SyntaxError</code> with the <code>^</code> caret pointing at the offending token is the same mechanism. The invariant-based testing you did (columns are non-decreasing, lines are bounded, columns are &gt;= 1) is the same technique used to validate position accuracy in production compiler test suites. You have built that infrastructure.\n<strong>5. The token stream is your scanner&#39;s public contract â€” and API contracts are forever.</strong>\nOnce a parser is built that consumes your token stream, the contract is locked. You can rewrite the scanner from scratch â€” change the algorithm, the data structures, the implementation language â€” as long as <code>scan_tokens()</code> produces the same token stream for the same input. This is the definition of an implementation boundary: the outside world (the parser) depends on the contract (the token stream), not the implementation. This design principle â€” define interfaces in terms of observable behavior, not implementation â€” appears in REST APIs (URL contracts), database drivers (SQL semantics), operating system system calls (POSIX), and hardware instruction sets (x86 ABI). Your token stream is your first real experience designing an interface that must be stable.</p>\n<blockquote>\n<p>ðŸ”­ <strong>Deep Dive</strong>: The formal foundation of what you have built â€” the Chomsky hierarchy, regular language recognition, DFA construction, and the limits of finite automata â€” is covered rigorously in <em>Introduction to the Theory of Computation</em> by Michael Sipser, Chapters 1â€“2. If you want to understand why maximal munch is guaranteed to produce unique tokenizations for well-designed languages (and under what conditions it fails), Chapter 1&#39;s treatment of DFAs and Chapter 4&#39;s undecidability results together give you the complete picture.</p>\n</blockquote>\n<hr>\n<h2 id=\"summary-what-you-have-built-across-four-milestones\">Summary: What You Have Built Across Four Milestones</h2>\n<p>You set out to build a tokenizer. What you actually built is:</p>\n<ul>\n<li>A <strong>finite state machine</strong> that transforms raw text into structured tokens, one character at a time, with O(n) time and no backtracking</li>\n<li>A <strong>position tracking system</strong> that correctly maintains line and column numbers through whitespace, comments, and multi-line strings, forming the foundation of every error message and IDE feature the language will ever have</li>\n<li>An <strong>error recovery mechanism</strong> that treats lexical errors as isolated faults rather than fatal failures â€” the same resilience principle used in circuit breakers, TCP retransmission, and RAID</li>\n<li>A <strong>complete test suite</strong> that verifies the scanner at the unit level (individual token types), integration level (complete programs), property level (position invariants), and performance level (10,000-line benchmark)</li>\n<li>A <strong>clean public contract</strong> â€” <code>Scanner(source).scan_tokens()</code> returns a deterministic, reproducible token stream â€” that a parser can depend on without knowing any implementation details\nThe scanner is the first component in a compiler pipeline. Everything that follows â€” parsing, type checking, code generation â€” depends on the token stream being correct. By testing it rigorously and building in error recovery, you have given every downstream component a reliable foundation to build on.\nMore importantly, you have internalized how programming languages work at the lowest level: not as words or lines, but as individual characters consumed one at a time through a state machine that makes greedy decisions with minimal lookahead. That understanding cascades into parsers, compilers, language servers, syntax highlighters, configuration language parsers, and any tool that must understand source code structurally. The problem changes; the mechanism does not.</li>\n</ul>\n<hr>\n<!-- END_MS -->\n\n\n\n\n<h1 id=\"tdd\">TDD</h1>\n<p>A character-level finite state machine scanner that transforms raw source text into a categorized token stream via maximal munch and single-character lookahead. The design centers on a single-pass, O(n) cursor model with zero backtracking. Every architectural decision flows from the FSM discipline: states are encoded as call frames, transitions as method calls, and accepting states as return points. The public contract â€” Scanner(source).scan_tokens() â†’ list[Token] â€” is the stable interface that all downstream consumers depend on.</p>\n<!-- TDD_MOD_ID: tokenizer-m1 -->\n<h1 id=\"technical-design-specification-token-types-amp-scanner-foundation-tokenizer-m1\">TECHNICAL DESIGN SPECIFICATION: Token Types &amp; Scanner Foundation (tokenizer-m1)</h1>\n<h2 id=\"1-module-charter\">1. Module Charter</h2>\n<p>This module serves as the foundational entry point for the compiler&#39;s front-end. Its primary responsibility is the transformation of a raw, linear character stream into a structured sequence of atomic lexical units (Tokens). </p>\n<p><strong>What it does:</strong></p>\n<ul>\n<li>Defines the complete vocabulary of the target language via a <code>TokenType</code> enumeration.</li>\n<li>Implements the <code>Token</code> data structure to carry lexemes and source metadata (line, column).</li>\n<li>Provides a <code>Scanner</code> class that manages a stateful cursor over the source text.</li>\n<li>Implements the three primitive &quot;lookahead and consume&quot; operations: <code>peek</code>, <code>advance</code>, and <code>is_at_end</code>.</li>\n<li>Handles structural overhead: whitespace consumption and line/column tracking.</li>\n<li>Recognizes single-character operators and punctuation.</li>\n<li>Emits sentinel <code>EOF</code> tokens and <code>ERROR</code> tokens for unrecognized characters.</li>\n</ul>\n<p><strong>What it does NOT do:</strong></p>\n<ul>\n<li>It does not recognize multi-character operators (e.g., <code>==</code>, <code>&gt;=</code>).</li>\n<li>It does not recognize number literals, string literals, or identifiers (reserved for M2/M3).</li>\n<li>It does not perform any syntax analysis or AST construction.</li>\n<li>It does not interpret escape sequences.</li>\n</ul>\n<p><strong>Upstream/Downstream Dependencies:</strong></p>\n<ul>\n<li><strong>Upstream:</strong> Raw source string provided by the compiler driver.</li>\n<li><strong>Downstream:</strong> The Parser (future module) which consumes the <code>list[Token]</code> output.</li>\n</ul>\n<p><strong>Invariants:</strong></p>\n<ul>\n<li>Every character in the source string is visited exactly once by the <code>advance()</code> method.</li>\n<li>The <code>Scanner.current</code> pointer never exceeds <code>len(source)</code>.</li>\n<li>The <code>Token.line</code> and <code>Token.column</code> always refer to the <em>start</em> of the lexeme.</li>\n<li>The resulting token list always ends with exactly one <code>TokenType.EOF</code> token.</li>\n</ul>\n<h2 id=\"2-file-structure\">2. File Structure</h2>\n<p>The implementation follows a modular structure to separate data definitions from the scanning engine.</p>\n<ol>\n<li><code>tokens.py</code>: Definitions for <code>TokenType</code> and <code>Token</code>.</li>\n<li><code>scanner.py</code>: The <code>Scanner</code> class and character-dispatch logic.</li>\n<li><code>test_foundation.py</code>: Unit tests for the foundation and single-character scanning.</li>\n</ol>\n<h2 id=\"3-complete-data-model\">3. Complete Data Model</h2>\n<h3 id=\"31-tokentype-enum\">3.1 TokenType (Enum)</h3>\n<p>The <code>TokenType</code> enumeration defines the complete set of terminal symbols the scanner can produce.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Category</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Lexeme Examples</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Literals</strong></td>\n<td align=\"left\"><code>NUMBER</code>, <code>STRING</code></td>\n<td align=\"left\"><code>42</code>, <code>&quot;hi&quot;</code></td>\n<td align=\"left\">Values (logic in M2/M3)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Names</strong></td>\n<td align=\"left\"><code>IDENTIFIER</code>, <code>KEYWORD</code></td>\n<td align=\"left\"><code>x</code>, <code>while</code></td>\n<td align=\"left\">Variable names and reserved words</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Operators</strong></td>\n<td align=\"left\"><code>PLUS</code>, <code>MINUS</code>, <code>STAR</code>, <code>SLASH</code>, <code>ASSIGN</code>, <code>BANG</code>, <code>LESS</code>, <code>GREATER</code>, <code>EQUAL</code>, <code>NOT_EQUAL</code>, <code>LESS_EQ</code>, <code>GREATER_EQ</code></td>\n<td align=\"left\"><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>=</code>, <code>!</code>, <code>&lt;</code>, <code>&gt;</code>, <code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&gt;=</code></td>\n<td align=\"left\">Participates in expressions</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Punctuation</strong></td>\n<td align=\"left\"><code>LPAREN</code>, <code>RPAREN</code>, <code>LBRACE</code>, <code>RBRACE</code>, <code>LBRACKET</code>, <code>RBRACKET</code>, <code>SEMICOLON</code>, <code>COMMA</code></td>\n<td align=\"left\"><code>(</code>, <code>)</code>, <code>{</code>, <code>}</code>, <code>[</code>, <code>]</code>, <code>;</code>, <code>,</code></td>\n<td align=\"left\">Structural delimiters</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Sentinels</strong></td>\n<td align=\"left\"><code>EOF</code>, <code>ERROR</code></td>\n<td align=\"left\"><code>&quot;&quot;</code>, <code>@</code></td>\n<td align=\"left\">End of stream and invalid characters</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-1.svg\" alt=\"TokenType Enumeration â€” Complete Category Map\"></p>\n<h3 id=\"32-token-dataclass\">3.2 Token (Dataclass)</h3>\n<p>The <code>Token</code> is the primary carrier of information. It must be immutable to ensure that downstream passes (parser, optimizer) do not inadvertently corrupt the source representation.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType   </span><span style=\"color:#6A737D\"># The category (e.g., TokenType.PLUS)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">       # The raw text from source (e.g., \"+\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">         # 1-indexed line number</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">       # 1-indexed column number within that line</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span></code></pre></div>\n\n<p><strong>Memory Layout Note (Python):</strong> Each <code>Token</code> instance in Python 3.10+ with <code>@dataclass</code> occupies approximately 48-64 bytes plus the string overhead for the lexeme. Given the O(n) nature of the scanner, for a 10k line file (~300k characters), the token list will occupy ~10-15MB of RAM, which is well within intermediate performance constraints.</p>\n<h3 id=\"33-scanner-internal-state\">3.3 Scanner Internal State</h3>\n<p>The <code>Scanner</code> maintains a persistent cursor.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Initial Value</th>\n<th align=\"left\">Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>source</code></td>\n<td align=\"left\"><code>str</code></td>\n<td align=\"left\">Passed in</td>\n<td align=\"left\">The full input text</td>\n</tr>\n<tr>\n<td align=\"left\"><code>current</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\"><code>0</code></td>\n<td align=\"left\">Byte offset of the <em>next</em> character to read</td>\n</tr>\n<tr>\n<td align=\"left\"><code>line</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\"><code>1</code></td>\n<td align=\"left\">Current source line (1-indexed)</td>\n</tr>\n<tr>\n<td align=\"left\"><code>column</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\"><code>1</code></td>\n<td align=\"left\">Current source column (1-indexed)</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-2.svg\" alt=\"Token Dataclass Memory Layout\"></p>\n<h2 id=\"4-interface-contracts\">4. Interface Contracts</h2>\n<h3 id=\"41-primitive-operations\">4.1 Primitive Operations</h3>\n<p><strong><code>Scanner.is_at_end() -&gt; bool</code></strong></p>\n<ul>\n<li>Returns <code>True</code> if <code>current</code> has reached or exceeded <code>len(source)</code>.</li>\n<li>This is the primary safety guard for all string indexing.</li>\n</ul>\n<p><strong><code>Scanner.peek() -&gt; str</code></strong></p>\n<ul>\n<li>Returns the character at <code>source[current]</code> without advancing the cursor.</li>\n<li>If <code>is_at_end()</code> is true, returns the sentinel <code>&quot;\\0&quot;</code>.</li>\n<li>This allows for non-destructive lookahead.</li>\n</ul>\n<p><strong><code>Scanner.advance() -&gt; str</code></strong></p>\n<ul>\n<li>Consumes the character at <code>source[current]</code> and increments <code>current</code>.</li>\n<li><strong>Position Tracking Invariant:</strong> <ul>\n<li>If the character is <code>\\n</code>, increment <code>line</code> and reset <code>column</code> to <code>1</code>.</li>\n<li>Else, increment <code>column</code> by <code>1</code>.</li>\n</ul>\n</li>\n<li>Returns the consumed character.</li>\n<li>Precondition: Must not be called if <code>is_at_end()</code> is true.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-3.svg\" alt=\"Scanner Class Architecture â€” Fields and Methods\"></p>\n<h3 id=\"42-scanning-operations\">4.2 Scanning Operations</h3>\n<p><strong><code>Scanner.next_token() -&gt; Token</code></strong></p>\n<ul>\n<li>The core dispatcher.</li>\n<li>Procedure:<ol>\n<li>Call <code>_skip_whitespace()</code>.</li>\n<li>Snap current <code>line</code> and <code>column</code> into local variables <code>tok_line</code> and <code>tok_col</code>. This ensures the token points to its <em>start</em>.</li>\n<li>If <code>is_at_end()</code>, return <code>Token(TokenType.EOF, &quot;&quot;, tok_line, tok_col)</code>.</li>\n<li>Call <code>advance()</code> to get <code>char</code>.</li>\n<li>Match <code>char</code> against <code>SINGLE_CHAR_TOKENS</code> dictionary.</li>\n<li>If matched, return a new <code>Token</code>.</li>\n<li>If unmatched, return <code>Token(TokenType.ERROR, char, tok_line, tok_col)</code>.</li>\n</ol>\n</li>\n</ul>\n<p><strong><code>Scanner.scan_tokens() -&gt; list[Token]</code></strong></p>\n<ul>\n<li>Loops until <code>next_token()</code> produces <code>TokenType.EOF</code>.</li>\n<li>Collects all produced tokens into a list and returns them.</li>\n</ul>\n<h2 id=\"5-algorithm-specification-position-aware-scanning\">5. Algorithm Specification: Position-Aware Scanning</h2>\n<p>The most critical logic in Milestone 1 is the coordination between whitespace skipping and position capturing.</p>\n<h3 id=\"51-the-_skip_whitespace-algorithm\">5.1 The <code>_skip_whitespace</code> Algorithm</h3>\n<ol>\n<li>Enter a <code>while</code> loop that continues as long as <code>not is_at_end()</code>.</li>\n<li><code>peek()</code> at the current character.</li>\n<li>If character is in <code>{&#39; &#39;, &#39;\\t&#39;, &#39;\\r&#39;, &#39;\\n&#39;}</code>:<ul>\n<li>Call <code>advance()</code>. (This ensures the <code>line</code> and <code>column</code> counters are updated correctly for newlines).</li>\n</ul>\n</li>\n<li>If character is not whitespace, <code>break</code> the loop.</li>\n</ol>\n<h3 id=\"52-token-position-invariant-trace\">5.2 Token Position Invariant Trace</h3>\n<p>To avoid &quot;off-by-one&quot; errors in column reporting:</p>\n<ol>\n<li>Scanner starts at <code>line=1, col=1</code>.</li>\n<li>Input: <code>  +</code>.</li>\n<li><code>_skip_whitespace()</code> advances past two spaces.<ul>\n<li>After first space: <code>line=1, col=2</code>.</li>\n<li>After second space: <code>line=1, col=3</code>.</li>\n</ul>\n</li>\n<li><code>tok_line = self.line</code> (1), <code>tok_col = self.column</code> (3).</li>\n<li><code>advance()</code> consumes <code>+</code>. <code>self.column</code> becomes 4.</li>\n<li>Token is emitted with <code>column=3</code>. <strong>This is correct.</strong> The <code>+</code> starts at column 3.</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-4.svg\" alt=\"Cursor Model: source, current, line, column Invariants\"></p>\n<h2 id=\"6-error-handling-matrix\">6. Error Handling Matrix</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Error</th>\n<th align=\"left\">Detected By</th>\n<th align=\"left\">Recovery</th>\n<th align=\"left\">User-Visible?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>IndexError</code></td>\n<td align=\"left\"><code>is_at_end()</code> guard</td>\n<td align=\"left\">Return <code>True</code> to stop loops</td>\n<td align=\"left\">No (internal safety)</td>\n</tr>\n<tr>\n<td align=\"left\">Invalid character (e.g., <code>@</code>)</td>\n<td align=\"left\"><code>next_token()</code> default branch</td>\n<td align=\"left\">Emit <code>TokenType.ERROR</code> and continue scanning</td>\n<td align=\"left\">Yes (downstream reports)</td>\n</tr>\n<tr>\n<td align=\"left\">CRLF <code>\\r\\n</code> line count</td>\n<td align=\"left\"><code>advance()</code> logic</td>\n<td align=\"left\"><code>\\r</code> increments column, <code>\\n</code> resets it and increments line. Result is +1 line.</td>\n<td align=\"left\">No (correct behavior)</td>\n</tr>\n<tr>\n<td align=\"left\">Tab column width</td>\n<td align=\"left\"><code>advance()</code> logic</td>\n<td align=\"left\">Fixed increment of 1</td>\n<td align=\"left\">No (standard convention)</td>\n</tr>\n<tr>\n<td align=\"left\">Empty Source</td>\n<td align=\"left\"><code>scan_tokens()</code></td>\n<td align=\"left\">Emit <code>EOF</code> at 1:1 and return</td>\n<td align=\"left\">No</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-5.svg\" alt=\"advance() Step-by-Step State: Normal Character vs Newline\"></p>\n<h2 id=\"7-implementation-sequence-with-checkpoints\">7. Implementation Sequence with Checkpoints</h2>\n<h3 id=\"phase-1-data-definitions-075h\">Phase 1: Data Definitions (0.75h)</h3>\n<ul>\n<li>Create <code>tokens.py</code>.</li>\n<li>Define <code>TokenType</code> using <code>enum.Enum</code> and <code>enum.auto()</code>.</li>\n<li>Define <code>Token</code> using <code>@dataclass</code>.</li>\n<li><strong>Checkpoint:</strong> Verify you can instantiate a <code>Token(TokenType.PLUS, &quot;+&quot;, 1, 1)</code> and print it.</li>\n</ul>\n<h3 id=\"phase-2-scanner-foundation-075h\">Phase 2: Scanner Foundation (0.75h)</h3>\n<ul>\n<li>Create <code>scanner.py</code>.</li>\n<li>Implement <code>__init__</code>, <code>is_at_end()</code>, <code>peek()</code>, and <code>advance()</code>.</li>\n<li>Implement <code>_skip_whitespace()</code>.</li>\n<li><strong>Checkpoint:</strong> Instantiate <code>Scanner(&quot;  \\n  &quot;)</code>, call <code>_skip_whitespace()</code>, and assert <code>scanner.line == 2</code> and <code>scanner.column == 3</code>.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-6.svg\" alt=\"peek() vs advance() â€” Consuming vs Non-Consuming Operations\"></p>\n<h3 id=\"phase-3-single-character-dispatch-05h\">Phase 3: Single-Character Dispatch (0.5h)</h3>\n<ul>\n<li>Define a constant dictionary <code>SINGLE_CHAR_TOKENS</code> mapping characters to <code>TokenType</code>.</li>\n<li>Implement <code>next_token()</code>.</li>\n<li><strong>Checkpoint:</strong> Call <code>next_token()</code> on source <code>&quot;+&quot;</code>. Verify it returns a <code>Token</code> of type <code>PLUS</code>. Call it again, verify it returns <code>EOF</code>.</li>\n</ul>\n<h3 id=\"phase-4-integration-loop-10h\">Phase 4: Integration Loop (1.0h)</h3>\n<ul>\n<li>Implement <code>scan_tokens()</code>.</li>\n<li>Implement error fallthrough (unrecognized characters).</li>\n<li>Write the unit test suite.</li>\n<li><strong>Checkpoint:</strong> Run <code>scan_tokens()</code> on <code>&quot; ( ) @ &quot;</code> and verify list: <code>[LPAREN, RPAREN, ERROR, EOF]</code>.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-7.svg\" alt=\"Scanner FSM â€” START State Dispatch for Milestone 1\"></p>\n<h2 id=\"8-test-specification\">8. Test Specification</h2>\n<h3 id=\"81-unit-tests-pytest-format\">8.1 Unit Tests (<code>pytest</code> format)</h3>\n<p><strong>Happy Path: Single Characters</strong></p>\n<ul>\n<li>Input: <code>+ - * / ( ) { } [ ] ; ,</code></li>\n<li>Expected: 12 tokens with matching types, terminating in <code>EOF</code>.</li>\n</ul>\n<p><strong>Edge Case: Whitespace &amp; Newlines</strong></p>\n<ul>\n<li>Input: <code> \\t\\n  +</code></li>\n<li>Expected: <code>Token(PLUS, &quot;+&quot;, 2, 3)</code> then <code>EOF</code>.</li>\n<li>Verification: Ensures whitespace doesn&#39;t emit tokens and newlines reset columns.</li>\n</ul>\n<p><strong>Edge Case: Empty Input</strong></p>\n<ul>\n<li>Input: <code>&quot;&quot;</code></li>\n<li>Expected: <code>[Token(EOF, &quot;&quot;, 1, 1)]</code></li>\n<li>Verification: Ensures no crash and sentinel emission.</li>\n</ul>\n<p><strong>Failure Case: Invalid Characters</strong></p>\n<ul>\n<li>Input: <code>+ @ -</code></li>\n<li>Expected: <code>[PLUS, ERROR(&quot;@&quot;), MINUS, EOF]</code></li>\n<li>Verification: Ensures error recovery (scanning continues after <code>@</code>).</li>\n</ul>\n<p><strong>Failure Case: Windows Line Endings</strong></p>\n<ul>\n<li>Input: <code>+\\r\\n+</code></li>\n<li>Expected: <code>Token(PLUS, &quot;+&quot;, 1, 1)</code>, <code>Token(PLUS, &quot;+&quot;, 2, 1)</code>, <code>EOF</code>.</li>\n<li>Verification: Ensures <code>\\r\\n</code> isn&#39;t counted as two newlines.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-8.svg\" alt=\"next_token() Control Flow â€” M1 Complete\"></p>\n<h2 id=\"9-performance-targets\">9. Performance Targets</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Operation</th>\n<th align=\"left\">Target</th>\n<th align=\"left\">How to Measure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Parsing Rate</strong></td>\n<td align=\"left\">&gt; 100,000 chars/sec</td>\n<td align=\"left\"><code>timeit</code> on a repeated source string</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Memory Overhead</strong></td>\n<td align=\"left\">&lt; 200 bytes/token</td>\n<td align=\"left\"><code>sys.getsizeof</code> on token list / len(tokens)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Character Access</strong></td>\n<td align=\"left\">O(1)</td>\n<td align=\"left\">Algorithmic verification (no nested loops over source)</td>\n</tr>\n</tbody></table>\n<p>{{DIAGRAM:tdd-diag-9}}</p>\n<h2 id=\"10-the-quotformal-soulquot-of-milestone-1\">10. The &quot;Formal Soul&quot; of Milestone 1</h2>\n<p>Milestone 1 implements a <strong>Deterministic Finite Automaton (DFA)</strong> with a single primary state (<code>START</code>). </p>\n<ol>\n<li>From <code>START</code>, seeing whitespace causes a self-transition (stay in <code>START</code>).</li>\n<li>Seeing a known single character causes a transition to an <strong>Accepting State</strong> (the token-production return).</li>\n<li>Seeing an unknown character transitions to an <strong>Error State</strong>, which emits a token and returns to <code>START</code>.</li>\n<li>Seeing <code>EOF</code> transitions to the <strong>Final State</strong>.</li>\n</ol>\n<p>The &quot;Soul&quot; of this component is its <strong>linear time complexity</strong>. Because there is no lookahead beyond <code>peek()</code> and no backtracking (decrementing <code>current</code>), the scanner is guaranteed to terminate in exactly <code>N</code> steps where <code>N</code> is the number of characters in the source.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-10.svg\" alt=\"Windows Line Ending \\r\\n: One Newline, Not Two\"></p>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m2 -->\n<h1 id=\"technical-design-specification-multi-character-tokens-amp-maximal-munch-tokenizer-m2\">TECHNICAL DESIGN SPECIFICATION: Multi-Character Tokens &amp; Maximal Munch (tokenizer-m2)</h1>\n<h2 id=\"1-module-charter\">1. Module Charter</h2>\n<p>This module extends the core lexical scanner to handle complex, multi-character tokens through the application of the <strong>Maximal Munch</strong> (greedy consumption) principle. It transitions the tokenizer from a simple character-to-token mapper into a predictive Finite State Machine (FSM) capable of disambiguating overlapping lexemes. </p>\n<p><strong>What it does:</strong></p>\n<ul>\n<li>Recognizes two-character operators (<code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&gt;=</code>) using conditional lookahead.</li>\n<li>Implements a numeric scanner supporting both signed/unsigned integers and floating-point literals with mandatory fractional digits (e.g., <code>3.14</code> vs <code>3.</code>).</li>\n<li>Implements an identifier scanner that distinguishes between user-defined names and reserved keywords using a high-efficiency hash-map lookup.</li>\n<li>Formalizes the &quot;Maximal Munch&quot; strategy: the scanner always consumes the longest possible valid character sequence for a given start state.</li>\n<li>Provides a two-character lookahead mechanism (<code>_peek_next</code>) specifically for resolving decimal point ambiguity.</li>\n</ul>\n<p><strong>What it does NOT do:</strong></p>\n<ul>\n<li>It does not handle string literals (delimited by quotes) or escape sequences (M3).</li>\n<li>It does not handle comments (single-line or multi-line) or the specific <code>/</code> vs <code>//</code> vs <code>/*</code> ambiguity (M3).</li>\n<li>It does not perform numeric overflow checks or floating-point precision validation.</li>\n</ul>\n<p><strong>Upstream/Downstream Dependencies:</strong></p>\n<ul>\n<li><strong>Upstream:</strong> Depends on <code>Scanner</code> foundation from M1 (current position, line/col tracking).</li>\n<li><strong>Downstream:</strong> Outputs a complete <code>list[Token]</code> to the Parser; M3 will wrap this logic for string/comment support.</li>\n</ul>\n<p><strong>Invariants:</strong></p>\n<ul>\n<li>Every character is consumed exactly once via <code>advance()</code> or <code>_match()</code>.</li>\n<li>Keyword matching only occurs on complete identifier lexemes (no prefix matches like <code>if</code> inside <code>iffy</code>).</li>\n<li>Floating point scanning must verify the character <em>following</em> the dot is a digit before consuming it.</li>\n</ul>\n<h2 id=\"2-file-structure\">2. File Structure</h2>\n<p>The implementation follows a modular sequence, enhancing the existing Foundation.</p>\n<ol>\n<li><code>tokens.py</code> (updated): Added new <code>TokenType</code> variants for multi-character operators and keywords.</li>\n<li><code>scanner.py</code> (updated): Integrated <code>_match</code>, <code>_peek_next</code>, and scanning methods (<code>_scan_number</code>, <code>_scan_identifier</code>, <code>_scan_operator</code>).</li>\n<li><code>test_operators.py</code>: Comprehensive test suite for maximal munch on operator pairs.</li>\n<li><code>test_literals.py</code>: Unit and integration tests for numeric literals and identifier/keyword resolution.</li>\n</ol>\n<h2 id=\"3-complete-data-model\">3. Complete Data Model</h2>\n<h3 id=\"31-updated-tokentype-enum\">3.1 Updated TokenType (Enum)</h3>\n<p>The <code>TokenType</code> enumeration is expanded to encompass the full operator and keyword vocabulary.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Category</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Lexeme</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Literals</strong></td>\n<td align=\"left\"><code>NUMBER</code></td>\n<td align=\"left\"><code>42</code>, <code>3.14</code></td>\n<td align=\"left\">Both integers and floats use this type.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Names</strong></td>\n<td align=\"left\"><code>IDENTIFIER</code>, <code>KEYWORD</code></td>\n<td align=\"left\"><code>foo</code>, <code>if</code></td>\n<td align=\"left\">Keywords are reserved, identifiers are user-defined.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Operators</strong></td>\n<td align=\"left\"><code>ASSIGN</code>, <code>EQUAL</code>, <code>BANG</code>, <code>NOT_EQUAL</code>, <code>LESS</code>, <code>LESS_EQ</code>, <code>GREATER</code>, <code>GREATER_EQ</code></td>\n<td align=\"left\"><code>=</code>, <code>==</code>, <code>!</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code></td>\n<td align=\"left\">Requires 1-char lookahead to distinguish.</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-11.svg\" alt=\"Maximal Munch Decision Tree for All Operator Pairs\"></p>\n<h3 id=\"32-global-tables\">3.2 Global Tables</h3>\n<p>To ensure O(1) keyword lookup and centralized operator configuration, the following constants are defined.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Keywords are reserved names. A lookup table ensures O(1) performance.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># These must match exactly; prefixes are ignored.</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">KEYWORDS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"if\"</span><span style=\"color:#E1E4E8\">:     TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"else\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"while\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"return\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"true\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"false\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"null\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># The set of characters that *might* start a two-character operator.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># These characters are removed from M1's SINGLE_CHAR_TOKENS and handled explicitly.</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">OPERATOR_CHARS</span><span style=\"color:#E1E4E8\">: frozenset[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> frozenset</span><span style=\"color:#E1E4E8\">({</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"&#x3C;\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">\"</span><span style=\"color:#E1E4E8\">})</span></span></code></pre></div>\n\n<p><strong>Memory Layout Note (Python Hash Maps):</strong> Python 3.7+ uses a highly optimized, compact, and ordered dictionary implementation. The <code>KEYWORDS</code> dictionary will consume approximately 240-300 bytes of overhead but provides near-constant lookup time ($O(1)$). Given the small number of keywords, collisions in the hash table are statistically negligible.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-12.svg\" alt=\"Trace: Maximal Munch on '>==' Input â€” Three Tokens\"></p>\n<h2 id=\"4-interface-contracts\">4. Interface Contracts</h2>\n<h3 id=\"41-lookahead-amp-conditional-consumption\">4.1 Lookahead &amp; Conditional Consumption</h3>\n<p><strong><code>Scanner._match(self, expected: str) -&gt; bool</code></strong></p>\n<ul>\n<li><strong>Parameters:</strong> <code>expected</code> (single-character string).</li>\n<li><strong>Constraints:</strong> Must not be called at EOF.</li>\n<li><strong>Return:</strong> <code>True</code> if next character matches <code>expected</code> (consumes it), <code>False</code> otherwise (no consumption).</li>\n<li><strong>Invariant:</strong> Internally calls <code>self.advance()</code> when a match is found to maintain line/col counters.</li>\n<li><strong>Edge Cases:</strong> If <code>is_at_end()</code>, returns <code>False</code>.</li>\n</ul>\n<p><strong><code>Scanner._peek_next(self) -&gt; str</code></strong></p>\n<ul>\n<li><strong>Return:</strong> The character <em>two</em> positions ahead of the current cursor.</li>\n<li><strong>Constraints:</strong> Returns <code>&quot;\\0&quot;</code> if the cursor is at or near the end of the source.</li>\n<li><strong>Reasoning:</strong> Required to distinguish <code>3.</code> (Integer + Dot) from <code>3.14</code> (Float).</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-13.svg\" alt=\"_match() Internal Logic â€” Peek-and-Conditionally-Consume\"></p>\n<h3 id=\"42-scanner-method-signatures\">4.2 Scanner Method Signatures</h3>\n<p><strong><code>Scanner._scan_operator(self, char: str, tok_line: int, tok_col: int) -&gt; Token</code></strong></p>\n<ul>\n<li><strong>Parameters:</strong> <code>char</code> (the first character already consumed by <code>next_token</code>), <code>tok_line</code>, <code>tok_col</code>.</li>\n<li><strong>Return:</strong> A single <code>Token</code> representing either a one-character or two-character operator.</li>\n</ul>\n<p><strong><code>Scanner._scan_number(self, first_digit: str, tok_line: int, tok_col: int) -&gt; Token</code></strong></p>\n<ul>\n<li><strong>Parameters:</strong> <code>first_digit</code> (already consumed), <code>tok_line</code>, <code>tok_col</code>.</li>\n<li><strong>Logic:</strong> Consumes all subsequent digits. If it sees <code>.</code> and a following digit, transitions to floating-point scanning.</li>\n<li><strong>Return:</strong> A <code>Token(TokenType.NUMBER, lexeme, ...)</code>.</li>\n</ul>\n<p><strong><code>Scanner._scan_identifier(self, first_char: str, tok_line: int, tok_col: int) -&gt; Token</code></strong></p>\n<ul>\n<li><strong>Parameters:</strong> <code>first_char</code> (already consumed), <code>tok_line</code>, <code>tok_col</code>.</li>\n<li><strong>Logic:</strong> Consumes all alphanumeric characters and underscores. Performs keyword lookup on the final lexeme.</li>\n<li><strong>Return:</strong> A <code>Token</code> (either <code>TokenType.KEYWORD</code> or <code>TokenType.IDENTIFIER</code>).</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-14.svg\" alt=\"Number Literal Scanner FSM â€” INTEGER and FLOAT States\"></p>\n<h2 id=\"5-algorithm-specification-the-maximal-munch-logic\">5. Algorithm Specification: The Maximal Munch Logic</h2>\n<p>The core &quot;soul&quot; of this module is the <strong>Maximal Munch</strong> algorithm. This ensures that <code>==</code> is never scanned as <code>=</code> then <code>=</code>.</p>\n<h3 id=\"51-operator-disambiguation-algorithm\">5.1 Operator Disambiguation Algorithm</h3>\n<ol>\n<li><strong>Consume</strong> the first character <code>C</code>.</li>\n<li>Check if <code>C</code> is in <code>OPERATOR_CHARS</code>.</li>\n<li>If <code>C == &#39;=&#39;</code>:<ul>\n<li>Call <code>_match(&#39;=&#39;)</code>.</li>\n<li>If <code>True</code>, return <code>TokenType.EQUAL</code> with lexeme <code>&quot;==&quot;</code>.</li>\n<li>If <code>False</code>, return <code>TokenType.ASSIGN</code> with lexeme <code>&quot;=&quot;</code>.</li>\n</ul>\n</li>\n<li>Apply same logic for <code>!</code>, <code>&lt;</code>, <code>&gt;</code>.</li>\n</ol>\n<h3 id=\"52-two-state-float-scanning-algorithm\">5.2 Two-State Float Scanning Algorithm</h3>\n<p>To correctly handle numbers, the scanner uses a localized state machine for integers and decimals.</p>\n<ol>\n<li><strong>State 1: INTEGER</strong><ul>\n<li>While <code>peek().isdigit()</code>: <code>lexeme += advance()</code>.</li>\n</ul>\n</li>\n<li><strong>Transition: DECIMAL_POINT?</strong><ul>\n<li>If <code>peek() == &#39;.&#39;</code> AND <code>_peek_next().isdigit()</code>:<ul>\n<li><code>lexeme += advance()</code> (consumes <code>.</code>).</li>\n<li>Transition to <strong>State 2: FRACTION</strong>.</li>\n</ul>\n</li>\n<li>Else:<ul>\n<li>Return <code>TokenType.NUMBER</code> (it&#39;s an integer).</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>State 2: FRACTION</strong><ul>\n<li>While <code>peek().isdigit()</code>: <code>lexeme += advance()</code>.</li>\n<li>Return <code>TokenType.NUMBER</code>.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Invariant:</strong> A <code>.</code> not followed by a digit is NOT part of the number. The input <code>3.foo</code> produces <code>NUMBER(&quot;3&quot;)</code> and the next <code>next_token()</code> call starts at the <code>.</code>.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-15.svg\" alt=\"_peek_next() â€” Two-Character Lookahead Without Consumption\"></p>\n<h3 id=\"53-identifier-scan-then-lookup-algorithm\">5.3 Identifier Scan-then-Lookup Algorithm</h3>\n<p>This avoids the complexity of manual Trie-based keyword matching.</p>\n<ol>\n<li><strong>Consume</strong> <code>first_char</code>.</li>\n<li>While <code>peek().isalnum()</code> or <code>peek() == &#39;_&#39;</code>:<ul>\n<li><code>lexeme += advance()</code>.</li>\n</ul>\n</li>\n<li>Query <code>KEYWORDS</code> dictionary with <code>lexeme</code>.</li>\n<li>If found: <code>token_type = KEYWORDS[lexeme]</code>.</li>\n<li>Else: <code>token_type = TokenType.IDENTIFIER</code>.</li>\n<li>Return <code>Token(token_type, lexeme, ...)</code>.</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-16.svg\" alt=\"Identifier Scanning + Keyword Table Lookup â€” Scan-Then-Lookup Pattern\"></p>\n<h2 id=\"6-error-handling-matrix\">6. Error Handling Matrix</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Error Category</th>\n<th align=\"left\">Detected By</th>\n<th align=\"left\">Recovery Strategy</th>\n<th align=\"left\">User-Visible?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Maximal Munch Failure</strong></td>\n<td align=\"left\"><code>_match()</code> check in <code>_scan_operator</code></td>\n<td align=\"left\">Greedy consumption always prefers longer token; failure is logic-driven, not input-driven.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Prefix Keyword Match</strong></td>\n<td align=\"left\"><code>_scan_identifier</code> lookup</td>\n<td align=\"left\">Scan full alphanumeric lexeme before lookup. <code>iffy</code> never matches <code>if</code>.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Trailing Dot in Number</strong></td>\n<td align=\"left\"><code>_peek_next().isdigit()</code></td>\n<td align=\"left\">Stop scanning at the last digit. Leave <code>.</code> for the next token cycle.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Malformed Float (<code>3..4</code>)</strong></td>\n<td align=\"left\"><code>_scan_number</code> loop</td>\n<td align=\"left\">Consumes <code>3.4</code>. The second <code>.</code> is left for next token (produces an <code>ERROR</code> or operator).</td>\n<td align=\"left\">Yes (as downstream error)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Unknown Operator (<code>@</code>)</strong></td>\n<td align=\"left\"><code>next_token()</code> fallthrough</td>\n<td align=\"left\">Emit <code>TokenType.ERROR</code> and continue scanning from next char.</td>\n<td align=\"left\">Yes</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-17.svg\" alt=\"next_token() Dispatch Order â€” M2 Complete\"></p>\n<h2 id=\"7-implementation-sequence-with-checkpoints\">7. Implementation Sequence with Checkpoints</h2>\n<h3 id=\"phase-1-lookahead-infrastructure-05h\">Phase 1: Lookahead Infrastructure (0.5h)</h3>\n<ul>\n<li>Implement <code>_match(expected)</code> and <code>_peek_next()</code>.</li>\n<li><strong>Checkpoint:</strong> Verify <code>Scanner(&quot;==&quot;)._match(&quot;=&quot;)</code> returns <code>True</code> and <code>current</code> moves to 2.</li>\n</ul>\n<h3 id=\"phase-2-operator-expansion-05h\">Phase 2: Operator Expansion (0.5h)</h3>\n<ul>\n<li>Remove <code>=</code>, <code>!</code>, <code>&lt;</code>, <code>&gt;</code> from <code>SINGLE_CHAR_TOKENS</code>.</li>\n<li>Implement <code>_scan_operator</code> using <code>_match</code>.</li>\n<li><strong>Checkpoint:</strong> Scan <code>&quot;&gt;= &quot;</code>. Verify it produces <code>Token(TokenType.GREATER_EQ, &quot;&gt;=&quot;, 1, 1)</code>.</li>\n</ul>\n<h3 id=\"phase-3-numeric-literacy-10h\">Phase 3: Numeric Literacy (1.0h)</h3>\n<ul>\n<li>Implement <code>_scan_number</code> with float support.</li>\n<li>Handle the <code>_peek_next</code> logic for dots.</li>\n<li><strong>Checkpoint:</strong> Scan <code>3.14 42 3.</code>. Verify types and lexemes (especially that <code>3.</code> stops after the <code>3</code>).</li>\n</ul>\n<h3 id=\"phase-4-name-resolution-05h\">Phase 4: Name Resolution (0.5h)</h3>\n<ul>\n<li>Implement <code>KEYWORDS</code> table.</li>\n<li>Implement <code>_scan_identifier</code>.</li>\n<li><strong>Checkpoint:</strong> Scan <code>while x</code> â†’ <code>[KEYWORD(&quot;while&quot;), IDENTIFIER(&quot;x&quot;)]</code>.</li>\n</ul>\n<h3 id=\"phase-5-integration-amp-acceptance-15h\">Phase 5: Integration &amp; Acceptance (1.5h)</h3>\n<ul>\n<li>Integrate all methods into the main <code>next_token</code> loop.</li>\n<li>Run canonical statement test: <code>if (x &gt;= 42) { return true; }</code>.</li>\n<li><strong>Checkpoint:</strong> All integration tests green. Token count for the canonical string is exactly 12.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-18.svg\" alt=\"Extended Scanner FSM â€” Four Scanning States After M2\"></p>\n<h2 id=\"8-test-specification\">8. Test Specification</h2>\n<h3 id=\"81-operator-tests-maximal-munch\">8.1 Operator Tests (Maximal Munch)</h3>\n<ul>\n<li><strong>Case 1: <code>&gt;==</code></strong><ul>\n<li>Input: <code>&gt;==</code></li>\n<li>Expected: <code>[GREATER_EQ(&quot;&gt;=&quot;), ASSIGN(&quot;=&quot;), EOF]</code></li>\n<li>Failure: Scanned as <code>[GREATER(&quot;&gt;&quot;), EQUAL(&quot;==&quot;)]</code> or <code>[GREATER(&quot;&gt;&quot;), ASSIGN(&quot;=&quot;), ASSIGN(&quot;=&quot;)]</code>.</li>\n</ul>\n</li>\n<li><strong>Case 2: <code>!=</code> vs <code>!</code></strong><ul>\n<li>Input: <code>! !=</code></li>\n<li>Expected: <code>[BANG(&quot;!&quot;), NOT_EQUAL(&quot;!=&quot;)]</code></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"82-numeric-tests-float-ambiguity\">8.2 Numeric Tests (Float Ambiguity)</h3>\n<ul>\n<li><strong>Case 3: Float Literal</strong><ul>\n<li>Input: <code>3.14159</code></li>\n<li>Expected: <code>Token(NUMBER, &quot;3.14159&quot;, 1, 1)</code></li>\n</ul>\n</li>\n<li><strong>Case 4: Trailing Dot</strong><ul>\n<li>Input: <code>42.</code></li>\n<li>Expected: <code>[NUMBER(&quot;42&quot;), ERROR(&quot;.&quot;)]</code> (assuming <code>.</code> is not a valid start of another token).</li>\n</ul>\n</li>\n<li><strong>Case 5: Leading Zero Float</strong><ul>\n<li>Input: <code>0.5</code></li>\n<li>Expected: <code>Token(NUMBER, &quot;0.5&quot;, 1, 1)</code></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"83-identifierkeyword-tests\">8.3 Identifier/Keyword Tests</h3>\n<ul>\n<li><strong>Case 6: Prefix keyword</strong><ul>\n<li>Input: <code>return_value</code></li>\n<li>Expected: <code>Token(IDENTIFIER, &quot;return_value&quot;, 1, 1)</code> (NOT a <code>KEYWORD</code> plus an <code>IDENTIFIER</code>).</li>\n</ul>\n</li>\n<li><strong>Case 7: Mixed Case (if applicable)</strong><ul>\n<li>Input: <code>If</code> (assuming keywords are lowercase)</li>\n<li>Expected: <code>Token(IDENTIFIER, &quot;If&quot;, 1, 1)</code></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"9-performance-targets\">9. Performance Targets</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Operation</th>\n<th align=\"left\">Target</th>\n<th align=\"left\">Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Keyword Lookup</strong></td>\n<td align=\"left\">$O(1)$ amortized</td>\n<td align=\"left\">Algorithmic (Python dict usage)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Token Scan Speed</strong></td>\n<td align=\"left\">&gt; 500,000 tokens/sec</td>\n<td align=\"left\"><code>time.perf_counter</code> on a generated source file with 50,000 tokens.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Memory Allocation</strong></td>\n<td align=\"left\">Zero heap allocation during operator dispatch</td>\n<td align=\"left\">Code Review (use string slices or pre-defined lexemes).</td>\n</tr>\n</tbody></table>\n<h2 id=\"10-formal-quotsoulquot-deterministic-finite-automata-dfa\">10. Formal &quot;Soul&quot;: Deterministic Finite Automata (DFA)</h2>\n<p>The &quot;Soul&quot; of M2 is the conversion of an <strong>NFA (Nondeterministic Finite Automaton)</strong> representation of operators into a <strong>DFA</strong>. </p>\n<p>In an NFA, when we see <code>&gt;</code>, we don&#39;t know if we should stop (GREATER) or continue (GREATER_EQ). By using <code>_match</code> (one-character lookahead), we resolve this choice deterministically. We essentially traverse a state tree:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>(START) --'&gt;'--&gt; (SEEN_GREATER)\n                    |\n                    |--'='--&gt; (ACCEPT: GREATER_EQ)\n                    |\n                    |--OTHER--&gt; (ACCEPT: GREATER, UNCONSUME OTHER)</code></pre></div>\n\n<p>This ensures the tokenizer is predictive and never requires backtracking the cursor, maintaining the $O(n)$ time complexity guarantee across all source text.</p>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m3 -->\n<h1 id=\"technical-design-specification-strings-amp-comments-tokenizer-m3\">TECHNICAL DESIGN SPECIFICATION: Strings &amp; Comments (tokenizer-m3)</h1>\n<h2 id=\"1-module-charter\">1. Module Charter</h2>\n<p>This module implements context-sensitive scanning logic to handle string literals and comment blocks. It resolves the lexical ambiguity of the <code>/</code> character, which can represent a division operator, the start of a single-line comment, or the start of a multi-line block comment.</p>\n<p><strong>What it does:</strong></p>\n<ul>\n<li>Implements a sub-FSM for string literals supporting escape sequences (<code>\\n</code>, <code>\\t</code>, <code>\\r</code>, <code>\\&quot;</code>, <code>\\\\</code>).</li>\n<li>Validates escape sequences during scanning; invalid sequences trigger <code>TokenType.ERROR</code>.</li>\n<li>Processes single-line comments (<code>//</code>) by advancing the cursor to the next newline without producing tokens.</li>\n<li>Processes multi-line block comments (<code>/* ... */</code>) including cross-line position tracking.</li>\n<li>Emits <code>TokenType.STRING</code> tokens containing the raw lexeme (including quotes).</li>\n<li>Emits <code>TokenType.ERROR</code> for unterminated strings or comments, reporting the position of the <em>opening</em> delimiter.</li>\n<li>Resolves <code>/</code> into <code>TokenType.SLASH</code> when not followed by <code>/</code> or <code>*</code>.</li>\n</ul>\n<p><strong>What it does NOT do:</strong></p>\n<ul>\n<li>It does not &quot;unescape&quot; strings (e.g., converting the characters <code>\\</code> and <code>n</code> into a newline byte); it preserves the raw source text.</li>\n<li>It does not support nested block comments (follows C-style non-nesting rules).</li>\n<li>It does not handle &quot;raw&quot; strings or HEREDOCs.</li>\n</ul>\n<p><strong>Upstream/Downstream Dependencies:</strong></p>\n<ul>\n<li><strong>Upstream:</strong> <code>Scanner.advance()</code>, <code>Scanner.peek()</code>, and <code>Scanner.next_token()</code> dispatch logic from M1/M2.</li>\n<li><strong>Downstream:</strong> Provides a sanitized token stream (stripped of comments) for the Parser.</li>\n</ul>\n<p><strong>Invariants:</strong></p>\n<ul>\n<li>Every character within a string or comment is consumed exactly once via <code>advance()</code>.</li>\n<li>Line and column counters are updated correctly even when skipping comments or scanning multi-line strings.</li>\n<li>A string literal is terminated by a closing <code>&quot;</code> or a bare newline (illegal in this language).</li>\n<li><code>_skip_line_comment</code> stops <em>before</em> the newline character to allow the main whitespace loop to handle line increments.</li>\n</ul>\n<h2 id=\"2-file-structure\">2. File Structure</h2>\n<p>The implementation continues within the existing project structure, primarily modifying the <code>Scanner</code> class.</p>\n<ol>\n<li><code>tokens.py</code>: Ensure <code>STRING</code> and <code>SLASH</code> are present in <code>TokenType</code>.</li>\n<li><code>scanner.py</code>: Implement <code>_scan_string</code>, <code>_skip_line_comment</code>, and <code>_skip_block_comment</code>.</li>\n<li><code>test_strings.py</code>: Targeted unit tests for string edge cases and escape sequences.</li>\n<li><code>test_comments.py</code>: Targeted unit tests for single and multi-line comments.</li>\n</ol>\n<h2 id=\"3-complete-data-model\">3. Complete Data Model</h2>\n<h3 id=\"31-lexeme-accumulation-strategy\">3.1 Lexeme Accumulation Strategy</h3>\n<p>For string literals, the scanner must accumulate characters. To maintain $O(n)$ time complexity and avoid $O(n^2)$ string re-allocations in Python, we use a list-of-characters pattern.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>lexeme_chars</code></td>\n<td align=\"left\"><code>list[str]</code></td>\n<td align=\"left\">A buffer to collect characters before <code>&quot;&quot;.join()</code></td>\n</tr>\n<tr>\n<td align=\"left\"><code>VALID_ESCAPES</code></td>\n<td align=\"left\"><code>frozenset[str]</code></td>\n<td align=\"left\">Constant set: <code>{&#39;n&#39;, &#39;t&#39;, &#39;r&#39;, &#39;&quot;&#39;, &#39;\\\\&#39;}</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"32-state-machine-representation\">3.2 State Machine Representation</h3>\n<p>The &quot;state&quot; of the scanner is implicitly managed by the call stack (Recursive Descent Scanning).</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Method</th>\n<th align=\"left\">Logical State</th>\n<th align=\"left\">Exit Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>next_token</code></td>\n<td align=\"left\"><code>START</code></td>\n<td align=\"left\">Dispatch to sub-state or EOF</td>\n</tr>\n<tr>\n<td align=\"left\"><code>_scan_string</code></td>\n<td align=\"left\"><code>IN_STRING</code></td>\n<td align=\"left\"><code>&quot;</code> (Accept), <code>\\n</code> (Error), <code>EOF</code> (Error)</td>\n</tr>\n<tr>\n<td align=\"left\"><code>_skip_line_comment</code></td>\n<td align=\"left\"><code>IN_LINE_COMMENT</code></td>\n<td align=\"left\"><code>\\n</code> (Accept), <code>EOF</code> (Accept)</td>\n</tr>\n<tr>\n<td align=\"left\"><code>_skip_block_comment</code></td>\n<td align=\"left\"><code>IN_BLOCK_COMMENT</code></td>\n<td align=\"left\"><code>*/</code> (Accept), <code>EOF</code> (Error)</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-19.svg\" alt=\"Context Sensitivity: Same Character '/', Different Meaning by State\"></p>\n<h2 id=\"4-interface-contracts\">4. Interface Contracts</h2>\n<h3 id=\"41-sub-scanner-signatures\">4.1 Sub-Scanner Signatures</h3>\n<p><strong><code>Scanner._scan_string(self, tok_line: int, tok_col: int) -&gt; Token</code></strong></p>\n<ul>\n<li><strong>Input:</strong> The line and column where the opening <code>&quot;</code> was detected.</li>\n<li><strong>Output:</strong> A <code>Token</code> of type <code>STRING</code> or <code>ERROR</code>.</li>\n<li><strong>Lexeme Format:</strong> Must include the surrounding double quotes (e.g., <code>&quot;\\&quot;hello\\&quot;&quot;</code>).</li>\n<li><strong>Escape Logic:</strong> If <code>\\</code> is seen, consume the next char. If the next char is not in <code>VALID_ESCAPES</code>, return <code>ERROR</code>.</li>\n</ul>\n<p><strong><code>Scanner._skip_line_comment(self) -&gt; None</code></strong></p>\n<ul>\n<li><strong>Logic:</strong> Calls <code>advance()</code> until <code>peek() == &#39;\\n&#39;</code> or <code>is_at_end()</code>.</li>\n<li><strong>Note:</strong> Does not return a token. The caller (<code>next_token</code>) must recurse to find the next valid token.</li>\n</ul>\n<p><strong><code>Scanner._skip_block_comment(self, start_line: int, start_col: int) -&gt; Optional[Token]</code></strong></p>\n<ul>\n<li><strong>Input:</strong> Start position of <code>/*</code> for error reporting.</li>\n<li><strong>Output:</strong> <code>None</code> if successfully closed, or <code>Token(ERROR, &quot;/*&quot;, ...)</code> if EOF reached.</li>\n<li><strong>Greediness:</strong> Non-nesting. Stops at the first <code>*/</code>.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-20.svg\" alt=\"String Literal Scanner FSM â€” States, Transitions, Error Exits\"></p>\n<h2 id=\"5-algorithm-specification\">5. Algorithm Specification</h2>\n<h3 id=\"51-the-3939-disambiguation-maximal-munch\">5.1 The &#39;/&#39; Disambiguation (Maximal Munch)</h3>\n<p>This algorithm resides in <code>next_token()</code> and handles the three-way branch for the forward slash.</p>\n<ol>\n<li><code>char = advance()</code> sees <code>/</code>.</li>\n<li>If <code>_match(&#39;/&#39;)</code> returns <code>True</code>:<ul>\n<li>Call <code>_skip_line_comment()</code>.</li>\n<li><strong>Tail Call:</strong> <code>return self.next_token()</code>. (Implicitly skips the comment).</li>\n</ul>\n</li>\n<li>Else if <code>_match(&#39;*&#39;)</code> returns <code>True</code>:<ul>\n<li>Call <code>err = _skip_block_comment(tok_line, tok_col)</code>.</li>\n<li>If <code>err</code> is not <code>None</code>, <code>return err</code>.</li>\n<li><strong>Tail Call:</strong> <code>return self.next_token()</code>.</li>\n</ul>\n</li>\n<li>Else:<ul>\n<li><code>return Token(TokenType.SLASH, &quot;/&quot;, tok_line, tok_col)</code>.</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-21.svg\" alt=\"Escape Sequence Validation â€” VALID_ESCAPES Decision\"></p>\n<h3 id=\"52-string-scanning-with-escape-states\">5.2 String Scanning with Escape States</h3>\n<p>The string scanner must handle nested escape logic while maintaining line counts.</p>\n<ol>\n<li>Initialize <code>lexeme_chars = [&#39;&quot;&#39;]</code>.</li>\n<li><code>while True</code>:<ul>\n<li>If <code>is_at_end()</code>: Return <code>ERROR</code> token at <code>tok_line:tok_col</code>.</li>\n<li><code>c = advance()</code>.</li>\n<li>If <code>c == &#39;&quot;&#39;</code>: <ul>\n<li><code>lexeme_chars.append(&#39;&quot;&#39;)</code>.</li>\n<li><code>return Token(TokenType.STRING, &quot;&quot;.join(lexeme_chars), tok_line, tok_col)</code>.</li>\n</ul>\n</li>\n<li>If <code>c == &#39;\\n&#39;</code>: <ul>\n<li>Return <code>ERROR</code> (bare newline in string).</li>\n</ul>\n</li>\n<li>If <code>c == &#39;\\\\&#39;</code>:<ul>\n<li><code>lexeme_chars.append(&#39;\\\\&#39;)</code>.</li>\n<li>If <code>is_at_end()</code>: Return <code>ERROR</code>.</li>\n<li><code>esc = advance()</code>.</li>\n<li><code>lexeme_chars.append(esc)</code>.</li>\n<li>If <code>esc</code> not in <code>VALID_ESCAPES</code>: Return <code>ERROR</code>.</li>\n<li>Continue loop.</li>\n</ul>\n</li>\n<li>Else:<ul>\n<li><code>lexeme_chars.append(c)</code>.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-22.svg\" alt=\"Trace: Scanning '\"hello\\nworld\"' Character by Character\"></p>\n<h3 id=\"53-block-comment-skip-newline-aware\">5.3 Block Comment Skip (Newline Aware)</h3>\n<p>Crucially, this algorithm must use <code>advance()</code> to ensure multi-line comments don&#39;t break the scanner&#39;s <code>line</code> counter.</p>\n<ol>\n<li>While <code>not is_at_end()</code>:<ul>\n<li><code>c = advance()</code>.</li>\n<li>If <code>c == &#39;*&#39;</code>:<ul>\n<li>If <code>peek() == &#39;/&#39;</code>:<ul>\n<li><code>advance()</code> (consume the <code>/</code>).</li>\n<li><code>return None</code> (Success).</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>(Note: <code>advance()</code> automatically increments <code>self.line</code> if <code>c == &#39;\\n&#39;</code>).</li>\n</ul>\n</li>\n<li>If loop exits via <code>is_at_end()</code>:<ul>\n<li><code>return Token(TokenType.ERROR, &quot;/*&quot;, start_line, start_col)</code>.</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-23.svg\" alt=\"Block Comment Scanner â€” Two-State Inner FSM\"></p>\n<h2 id=\"6-error-handling-matrix\">6. Error Handling Matrix</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Error</th>\n<th align=\"left\">Detected By</th>\n<th align=\"left\">Recovery</th>\n<th align=\"left\">User-Visible?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Unterminated String</strong></td>\n<td align=\"left\"><code>is_at_end()</code> or <code>\\n</code> in <code>_scan_string</code></td>\n<td align=\"left\">Emit <code>ERROR</code> at quote start position. Resume at next char.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Invalid Escape Sequence</strong></td>\n<td align=\"left\"><code>esc not in VALID_ESCAPES</code></td>\n<td align=\"left\">Emit <code>ERROR</code> at quote start.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Unterminated Block Comment</strong></td>\n<td align=\"left\"><code>is_at_end()</code> in <code>_skip_block_comment</code></td>\n<td align=\"left\">Emit <code>ERROR</code> at <code>/*</code> start position.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Nested Comment Ambiguity</strong></td>\n<td align=\"left\">Logic (<code>*/</code> terminates)</td>\n<td align=\"left\">Treat inner <code>/*</code> as plain text.</td>\n<td align=\"left\">No (Standard C behavior)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Backslash at EOF</strong></td>\n<td align=\"left\"><code>is_at_end()</code> after <code>\\</code></td>\n<td align=\"left\">Emit <code>ERROR</code>.</td>\n<td align=\"left\">Yes</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-24.svg\" alt=\"Trace: Multi-line Block Comment with Line Number Updates\"></p>\n<h2 id=\"7-implementation-sequence-with-checkpoints\">7. Implementation Sequence with Checkpoints</h2>\n<h3 id=\"phase-1-the-slash-branch-05h\">Phase 1: The Slash Branch (0.5h)</h3>\n<ul>\n<li>Modify <code>next_token</code> to detect <code>/</code>.</li>\n<li>Implement <code>TokenType.SLASH</code>.</li>\n<li><strong>Checkpoint:</strong> <code>Scanner(&quot;/&quot;)</code> should produce one <code>SLASH</code> token. <code>Scanner(&quot;//&quot;)</code> should produce <code>EOF</code> (comment skipped).</li>\n</ul>\n<h3 id=\"phase-2-block-comment-logic-075h\">Phase 2: Block Comment Logic (0.75h)</h3>\n<ul>\n<li>Implement <code>_skip_block_comment</code>.</li>\n<li>Ensure <code>advance()</code> is used for newline tracking.</li>\n<li><strong>Checkpoint:</strong> <code>Scanner(&quot;/* \\n */ +&quot;)</code> should produce <code>PLUS</code> with <code>line=2</code>.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-25.svg\" alt=\"Error Position: Blame the Cause, Not the Symptom\"></p>\n<h3 id=\"phase-3-string-fsm-10h\">Phase 3: String FSM (1.0h)</h3>\n<ul>\n<li>Implement <code>_scan_string</code> with the list-buffer.</li>\n<li>Implement escape sequence branch.</li>\n<li><strong>Checkpoint:</strong> <code>Scanner(&#39;&quot;hi\\\\n&quot;&#39;)</code> should produce a <code>STRING</code> token with lexeme <code>&quot;\\&quot;hi\\\\n\\&quot;&quot;</code>.</li>\n</ul>\n<h3 id=\"phase-4-error-positioning-05h\">Phase 4: Error Positioning (0.5h)</h3>\n<ul>\n<li>Verify that unterminated strings report the column of the first <code>&quot;</code>.</li>\n<li><strong>Checkpoint:</strong> <code>Scanner(&#39;  &quot;hello&#39;)</code> should produce an <code>ERROR</code> at <code>line 1, col 3</code>.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-26.svg\" alt=\"Complete Scanner FSM â€” All States After M3\"></p>\n<h2 id=\"8-test-specification\">8. Test Specification</h2>\n<h3 id=\"81-string-escape-tests\">8.1 String Escape Tests</h3>\n<ul>\n<li><strong>Case: Valid Escapes</strong><ul>\n<li>Input: <code>&quot;a\\nb\\tc\\rd\\&quot;e\\\\f&quot;</code></li>\n<li>Expected: <code>STRING</code> token, len(lexeme) == 15.</li>\n</ul>\n</li>\n<li><strong>Case: Invalid Escape</strong><ul>\n<li>Input: <code>&quot;\\q&quot;</code></li>\n<li>Expected: <code>ERROR</code> token.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"82-comment-interaction-tests\">8.2 Comment Interaction Tests</h3>\n<ul>\n<li><strong>Case: &quot;/&quot; inside string</strong><ul>\n<li>Input: <code>&quot;http://&quot;</code></li>\n<li>Expected: One <code>STRING</code> token. The <code>//</code> must NOT be treated as a comment.</li>\n</ul>\n</li>\n<li><strong>Case: Multi-line string error</strong><ul>\n<li>Input: <code>&quot;first line\\nsecond line&quot;</code></li>\n<li>Expected: <code>ERROR</code> (since bare newlines are forbidden in strings).</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"83-block-comment-edge-cases\">8.3 Block Comment Edge Cases</h3>\n<ul>\n<li><strong>Case: Adjacent Stars</strong><ul>\n<li>Input: <code>/***/</code></li>\n<li>Expected: <code>EOF</code> (Successfully skipped).</li>\n</ul>\n</li>\n<li><strong>Case: Unterminated</strong><ul>\n<li>Input: <code>/* hello</code></li>\n<li>Expected: <code>ERROR</code> at line 1, col 1.</li>\n</ul>\n</li>\n</ul>\n<p>{{DIAGRAM:tdd-diag-27}}</p>\n<h2 id=\"9-performance-targets\">9. Performance Targets</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Operation</th>\n<th align=\"left\">Target</th>\n<th align=\"left\">Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>String Allocation</strong></td>\n<td align=\"left\">$O(n)$</td>\n<td align=\"left\">One list creation, one <code>.join()</code> per string token.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Comment Memory</strong></td>\n<td align=\"left\">$O(1)$</td>\n<td align=\"left\">Comments are skipped without allocation.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Line Tracking</strong></td>\n<td align=\"left\">Exact</td>\n<td align=\"left\">Position must be accurate across a 100-line comment.</td>\n</tr>\n</tbody></table>\n<p><img src=\"./diagrams/tdd-diag-28.svg\" alt=\"String Lexeme: list[str] + join vs String Concatenation â€” Performance\"></p>\n<h2 id=\"10-the-quotformal-soulquot-of-milestone-3\">10. The &quot;Formal Soul&quot; of Milestone 3</h2>\n<p>Milestone 3 introduces <strong>Pushdown-like Behavior</strong> within the FSM. While the language remains regular, the scanner now essentially &quot;pushes&quot; a context (<code>IN_STRING</code> or <code>IN_COMMENT</code>) onto a conceptual stack. </p>\n<p>The primary challenge is <strong>Semantic Transparency</strong>: Comments must be invisible to the parser, but perfectly visible to the line counter. Strings must be atomic to the parser, but internally complex to the scanner. By separating the <em>content</em> of the string from the <em>structure</em> of the lexer, we ensure that the Parser receives a clean stream of tokens where the complexity of escape characters and multi-line comments has already been abstracted away.</p>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m4 -->\n<h1 id=\"technical-design-specification-integration-testing-amp-error-recovery-tokenizer-m4\">TECHNICAL DESIGN SPECIFICATION: Integration Testing &amp; Error Recovery (tokenizer-m4)</h1>\n<h2 id=\"1-module-charter\">1. Module Charter</h2>\n<p>This module concludes the tokenizer implementation by transitioning from isolated unit verification to holistic system validation. Its primary mandate is to ensure the scanner remains robust, accurate, and performant when processing complex, real-world source files.</p>\n<p><strong>What it does:</strong></p>\n<ul>\n<li>Implements a comprehensive integration test suite that verifies the entire token stream (type, lexeme, line, column) for multi-line programs.</li>\n<li>Formalizes the <strong>Character-Level Error Recovery</strong> strategy: the scanner must emit an <code>ERROR</code> token for an invalid character, advance the cursor, and resume scanning immediately without state corruption.</li>\n<li>Validates <strong>Position Invariants</strong>: ensures that line and column tracking never drift, even after complex sequences of multi-line comments, strings with escapes, and whitespace.</li>\n<li>Conducts <strong>Performance Benchmarking</strong>: verifies that the Python implementation meets the 10,000-line-per-second requirement through automated timing.</li>\n<li>Provides utility functions for token stream manipulation, such as partitioning valid tokens from errors.</li>\n</ul>\n<p><strong>What it does NOT do:</strong></p>\n<ul>\n<li>It does not add new lexical features or token types (the grammar is frozen at M3).</li>\n<li>It does not attempt parser-level recovery (e.g., inserting missing semicolons).</li>\n<li>It does not handle file I/O; it operates on source strings in memory.</li>\n</ul>\n<p><strong>Invariants:</strong></p>\n<ul>\n<li><strong>Resumption Invariant:</strong> For any input $S$, the number of characters consumed (via <code>advance</code>) must equal <code>len(S)</code>.</li>\n<li><strong>Monotonicity Invariant:</strong> For any two tokens $T_i, T_{i+1}$ on the same line, $T_{i+1}.column &gt; T_i.column$.</li>\n<li><strong>Sentinel Invariant:</strong> Every call to <code>scan_tokens()</code> must return a list where the final element is <code>TokenType.EOF</code>.</li>\n</ul>\n<h2 id=\"2-file-structure\">2. File Structure</h2>\n<p>The user should follow this creation/update order to complete the project:</p>\n<ol>\n<li><code>scanner.py</code> (Update): Ensure <code>scan_tokens</code> is robust and error recovery is implicit in the loop.</li>\n<li><code>utils.py</code> (New): Utility functions for testing and stream partitioning.</li>\n<li><code>test_invariants.py</code> (New): Property-based and invariant tests for position tracking.</li>\n<li><code>test_integration.py</code> (New): The &quot;Canonical&quot; multi-line program tests.</li>\n<li><code>benchmark.py</code> (New): Performance testing script.</li>\n</ol>\n<h2 id=\"3-complete-data-model\">3. Complete Data Model</h2>\n<h3 id=\"31-token-refined-definition\">3.1 Token (Refined Definition)</h3>\n<p>While defined in M1, the <code>Token</code> dataclass is the &quot;wire format&quot; for this module.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ... all types from M1-M3 ...</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PLUS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">MINUS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">STAR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">SLASH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">BANG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">LESS_EQ</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">GREATER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">GREATER_EQ</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LPAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">RPAREN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">LBRACE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">RBRACE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LBRACKET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">RBRACKET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">SEMICOLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto(); </span><span style=\"color:#79B8FF\">COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span></code></pre></div>\n\n<h3 id=\"32-test-metadata-structures\">3.2 Test Metadata Structures</h3>\n<p>To facilitate clean integration testing, we define a &quot;Expected Token&quot; schema for comparison.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Constraint</th>\n<th align=\"left\">Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>expected_type</code></td>\n<td align=\"left\"><code>TokenType</code></td>\n<td align=\"left\">Not None</td>\n<td align=\"left\">Target category</td>\n</tr>\n<tr>\n<td align=\"left\"><code>expected_lexeme</code></td>\n<td align=\"left\"><code>str</code></td>\n<td align=\"left\">Exact match</td>\n<td align=\"left\">Verifies maximal munch</td>\n</tr>\n<tr>\n<td align=\"left\"><code>expected_line</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">&gt; 0</td>\n<td align=\"left\">Verifies newline tracking</td>\n</tr>\n<tr>\n<td align=\"left\"><code>expected_col</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">&gt; 0</td>\n<td align=\"left\">Verifies column tracking</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-29.svg\" alt=\"Token Stream as Parser Interface Contract\"></p>\n<h2 id=\"4-interface-contracts\">4. Interface Contracts</h2>\n<h3 id=\"41-utility-operations-utilspy\">4.1 Utility Operations (<code>utils.py</code>)</h3>\n<p><strong><code>partition_tokens(tokens: list[Token]) -&gt; tuple[list[Token], list[Token]]</code></strong></p>\n<ul>\n<li><strong>Parameters:</strong> A list of tokens from <code>scan_tokens()</code>.</li>\n<li><strong>Return:</strong> A tuple <code>(valid_tokens, error_tokens)</code>.</li>\n<li><strong>Constraint:</strong> <code>valid_tokens</code> must include <code>EOF</code>.</li>\n<li><strong>Usage:</strong> Allows tests to check for &quot;No errors expected&quot; vs &quot;Specific errors expected&quot; without complex filtering logic.</li>\n</ul>\n<p><strong><code>generate_large_source(num_lines: int) -&gt; str</code></strong></p>\n<ul>\n<li><strong>Parameters:</strong> Number of lines to generate.</li>\n<li><strong>Logic:</strong> Alternates between keywords, strings, and operators to create a representative workload.</li>\n<li><strong>Return:</strong> A string of approximately $30 \\times num_lines$ characters.</li>\n</ul>\n<h3 id=\"42-scanner-recovery-invariant\">4.2 Scanner Recovery Invariant</h3>\n<p><strong><code>Scanner.next_token(self) -&gt; Token</code></strong> (Implicitly Hardened)</p>\n<ul>\n<li><strong>Constraint:</strong> Every path through this method must consume at least one character unless at EOF.</li>\n<li><strong>Recovery Logic:</strong> If an unrecognized character is encountered, the <code>ERROR</code> token is returned. Since <code>advance()</code> was called at the start of <code>next_token</code>, the state is automatically ready for the next character on the next call. This is &quot;Character-Level Panic Mode&quot;.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-30.svg\" alt=\"Error Recovery: Circuit Breaker Pattern â€” Fault Isolation\"></p>\n<h2 id=\"5-algorithm-specification\">5. Algorithm Specification</h2>\n<h3 id=\"51-invariant-checking-algorithm\">5.1 Invariant Checking Algorithm</h3>\n<p>This algorithm is used in <code>test_invariants.py</code> to prove the scanner&#39;s position tracking is sound without knowing the specific tokens.</p>\n<ol>\n<li><strong>Initialize:</strong> <code>last_line = 1</code>, <code>last_col = 0</code>.</li>\n<li><strong>Iterate:</strong> For each <code>token</code> in <code>scan_tokens()</code>:<ul>\n<li><strong>Invariant A (Line Bounds):</strong> <code>1 &lt;= token.line &lt;= source.count(&#39;\\n&#39;) + 1</code>.</li>\n<li><strong>Invariant B (Column Bounds):</strong> <code>token.column &gt;= 1</code>.</li>\n<li><strong>Invariant C (Monotonicity):</strong> <ul>\n<li>If <code>token.line == last_line</code>: Assert <code>token.column &gt; last_col</code>.</li>\n<li>Else: Assert <code>token.line &gt; last_line</code>.</li>\n</ul>\n</li>\n<li><strong>Update:</strong> <code>last_line = token.line</code>, <code>last_col = token.column</code>.</li>\n</ul>\n</li>\n<li><strong>Post-condition:</strong> The final token must be <code>EOF</code> and its position must point exactly to the end of the source string.</li>\n</ol>\n<p>{{DIAGRAM:tdd-diag-31}}</p>\n<h3 id=\"52-performance-benchmarking-procedure\">5.2 Performance Benchmarking Procedure</h3>\n<p>To meet the &quot;Expert&quot; precision level, we use monotonic hardware clocks.</p>\n<ol>\n<li><strong>Setup:</strong> Generate source string via <code>generate_large_source(10000)</code>.</li>\n<li><strong>Warm-up:</strong> Run <code>Scanner(source).scan_tokens()</code> once to ensure Python byte-code caches/JIT (if using PyPy) are active.</li>\n<li><strong>Timer Start:</strong> <code>start_time = time.perf_counter()</code>.</li>\n<li><strong>Execute:</strong> <code>tokens = Scanner(source).scan_tokens()</code>.</li>\n<li><strong>Timer Stop:</strong> <code>end_time = time.perf_counter()</code>.</li>\n<li><strong>Calculate:</strong> <code>elapsed = end_time - start_time</code>.</li>\n<li><strong>Assert:</strong> <code>elapsed &lt; 1.0</code>.</li>\n<li><strong>Metric Report:</strong> Output <code>Tokens/Sec = len(tokens) / elapsed</code>.</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-32.svg\" alt=\"Position Drift: How Small Bugs Accumulate Over Lines\"></p>\n<h2 id=\"6-error-handling-matrix\">6. Error Handling Matrix</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Error</th>\n<th align=\"left\">Detected By</th>\n<th align=\"left\">Recovery</th>\n<th align=\"left\">User-Visible?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Position Drift</strong></td>\n<td align=\"left\">Invariant Test (C)</td>\n<td align=\"left\">Re-align <code>advance()</code> newline logic to reset column to 1.</td>\n<td align=\"left\">No (Dev-time fix)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Token Mismatch</strong></td>\n<td align=\"left\"><code>test_canonical_statement</code></td>\n<td align=\"left\">Check <code>_match()</code> greediness for 2-char ops.</td>\n<td align=\"left\">No (Dev-time fix)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Infinite Recursion</strong></td>\n<td align=\"left\"><code>next_token()</code> comment branch</td>\n<td align=\"left\">Ensure <code>_skip_comment</code> calls <code>advance()</code>.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Unterminated String</strong></td>\n<td align=\"left\">EOF check in <code>_scan_string</code></td>\n<td align=\"left\">Return <code>ERROR</code> with opening quote position.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Multiple Invalid Chars</strong></td>\n<td align=\"left\"><code>scan_tokens</code> loop</td>\n<td align=\"left\">Collect each into the list; don&#39;t break.</td>\n<td align=\"left\">Yes</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-33.svg\" alt=\"Position Invariant Suite â€” Three Checkable Properties\"></p>\n<h2 id=\"7-implementation-sequence-with-checkpoints\">7. Implementation Sequence with Checkpoints</h2>\n<h3 id=\"phase-1-canonical-verification-05h\">Phase 1: Canonical Verification (0.5h)</h3>\n<ul>\n<li>Create <code>test_integration.py</code>.</li>\n<li>Implement <code>test_canonical_statement</code> with the exact list from the project spec.</li>\n<li><strong>Checkpoint:</strong> <code>pytest test_integration.py</code> â†’ <code>test_canonical_statement</code> passes with exactly 12 tokens.</li>\n</ul>\n<h3 id=\"phase-2-error-recovery-hardening-05h\">Phase 2: Error Recovery Hardening (0.5h)</h3>\n<ul>\n<li>Add <code>test_multiple_errors_all_reported</code> with input <code>&quot;@#$&quot;</code>.</li>\n<li>Add <code>test_errors_mixed_with_valid_tokens</code> with input <code>&quot;x @ y&quot;</code>.</li>\n<li><strong>Checkpoint:</strong> Verify that <code>@</code> and <code>y</code> have correct relative positions.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-34.svg\" alt=\"Multi-line Program Test: Comment Lines â†’ Line Number Offset Calculation\"></p>\n<h3 id=\"phase-3-position-invariant-suite-10h\">Phase 3: Position Invariant Suite (1.0h)</h3>\n<ul>\n<li>Create <code>test_invariants.py</code>.</li>\n<li>Implement the monotonicity and line-bound checks.</li>\n<li>Test against multi-line block comments and strings.</li>\n<li><strong>Checkpoint:</strong> Run against a file containing <code>/* \\n\\n */ &quot; \\n &quot;</code>. Verify <code>line</code> transitions 1 -&gt; 4 -&gt; 5.</li>\n</ul>\n<h3 id=\"phase-4-boundary-amp-edge-cases-075h\">Phase 4: Boundary &amp; Edge Cases (0.75h)</h3>\n<ul>\n<li>Test empty string, single newline, long identifiers (1000+ chars).</li>\n<li>Verify <code>EOF</code> position on single-character inputs.</li>\n<li><strong>Checkpoint:</strong> <code>Scanner(&quot;x&quot;).scan_tokens()</code> results in <code>Token(ID, &quot;x&quot;, 1, 1)</code> and <code>Token(EOF, &quot;&quot;, 1, 2)</code>.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-35.svg\" alt=\"Performance Benchmark Architecture: 10k Line Generation and Timing\"></p>\n<h3 id=\"phase-5-performance-amp-scaling-075h\">Phase 5: Performance &amp; Scaling (0.75h)</h3>\n<ul>\n<li>Implement <code>benchmark.py</code>.</li>\n<li>Run for 10k lines.</li>\n<li><strong>Checkpoint:</strong> Execution time &lt; 1.0s. Report tokens/sec.</li>\n</ul>\n<p>{{DIAGRAM:tdd-diag-36}}</p>\n<h2 id=\"8-test-specification\">8. Test Specification</h2>\n<h3 id=\"81-integration-tests\">8.1 Integration Tests</h3>\n<ul>\n<li><strong>Input:</strong> <code>if (x &gt;= 0) { return &quot;ok&quot;; } // check</code></li>\n<li><strong>Verification:</strong><ul>\n<li><code>tokens[0].type == TokenType.KEYWORD</code></li>\n<li><code>tokens[3].lexeme == &quot;&gt;=&quot;</code></li>\n<li><code>tokens[7].type == TokenType.STRING</code></li>\n<li><code>len(tokens) == 11</code> (if counting EOF).</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"82-boundary-tests\">8.2 Boundary Tests</h3>\n<ul>\n<li><strong>Empty File:</strong> <code>&quot;&quot;</code> â†’ <code>[EOF(1,1)]</code>.</li>\n<li><strong>Long Literal:</strong> <code>&quot;a&quot; * 10000</code> â†’ <code>Token(IDENTIFIER, ..., len=10000)</code>.</li>\n<li><strong>Maximal Munch Edge:</strong> <code>&gt;==</code> â†’ <code>GREATER_EQ</code>, <code>ASSIGN</code>.</li>\n</ul>\n<h3 id=\"83-error-tests\">8.3 Error Tests</h3>\n<ul>\n<li><strong>Unterminated Comment:</strong> <code>/* hello</code> â†’ <code>Token(ERROR, &quot;/*&quot;, 1, 1)</code>.</li>\n<li><strong>Invalid Escape:</strong> <code>&quot;\\q&quot;</code> â†’ <code>Token(ERROR, &quot;\\&quot;\\\\q&quot;, 1, 1)</code>.</li>\n</ul>\n<h2 id=\"9-performance-targets\">9. Performance Targets</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Metric</th>\n<th align=\"left\">Target</th>\n<th align=\"left\">Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Throughput</strong></td>\n<td align=\"left\">&gt; 10,000 lines/sec</td>\n<td align=\"left\">10k lines source in &lt; 1.0s.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Complexity</strong></td>\n<td align=\"left\">$O(N)$</td>\n<td align=\"left\">Verify <code>current</code> only increments, never decrements.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Token Rate</strong></td>\n<td align=\"left\">&gt; 100,000 tokens/sec</td>\n<td align=\"left\">Benchmarked on standard Statement mix.</td>\n</tr>\n</tbody></table>\n<h2 id=\"10-the-quotformal-soulquot-of-integration\">10. The &quot;Formal Soul&quot; of Integration</h2>\n<p>The &quot;Formal Soul&quot; of Milestone 4 is the transition from <strong>Correctness-by-Logic</strong> to <strong>Correctness-by-Observation</strong>. </p>\n<p>The tokenizer&#39;s design is theoretically O(n) and deterministic, but the implementation is susceptible to <strong>Accumulated State Drift</strong>â€”specifically in the line/column counters. By treating the tokenizer as a &quot;black box&quot; that transforms Source $\\rightarrow$ TokenStream and verifying invariants (monotonicity, bounds, and sentinel presence), we ensure that the software matches the theoretical FSM. </p>\n<p>The error recovery strategy implemented here (Character-Level resync) is the most robust possible for a lexer because it treats lexical errors as <strong>localized noise</strong> that does not impact the global structure. This provides the highest possible availability for downstream tools (IDE squiggles, syntax highlighters) which must continue to function even in the presence of malformed source text.</p>\n<!-- END_TDD_MOD -->\n\n\n<h1 id=\"project-structure-tokenizer-lexer\">Project Structure: Tokenizer / Lexer</h1>\n<h2 id=\"directory-tree\">Directory Tree</h2>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">text</span><pre class=\"arch-pre shiki-highlighted\"><code>tokenizer-lexer/\nâ”œâ”€â”€ tokens.py               # Token &amp; TokenType definitions (M1-M3)\nâ”œâ”€â”€ scanner.py              # Main FSM scanning logic (M1-M4)\nâ”œâ”€â”€ utils.py                # Token stream partitioning utilities (M4)\nâ”œâ”€â”€ benchmark.py            # Performance timing script (M4)\nâ”œâ”€â”€ tests/                  # Test suite directory\nâ”‚   â”œâ”€â”€ test_foundation.py  # Single-char &amp; cursor tests (M1)\nâ”‚   â”œâ”€â”€ test_operators.py   # Multi-char operator munch tests (M2)\nâ”‚   â”œâ”€â”€ test_literals.py    # Number &amp; identifier tests (M2)\nâ”‚   â”œâ”€â”€ test_strings.py     # String escape sequence tests (M3)\nâ”‚   â”œâ”€â”€ test_comments.py    # Line &amp; block comment tests (M3)\nâ”‚   â”œâ”€â”€ test_invariants.py  # Position &amp; monotonicity checks (M4)\nâ”‚   â””â”€â”€ test_integration.py # Multi-line program validation (M4)\nâ”œâ”€â”€ requirements.txt        # Project dependencies (pytest)\nâ”œâ”€â”€ pytest.ini              # Test runner configuration\nâ”œâ”€â”€ .gitignore              # Python build/cache exclusions\nâ””â”€â”€ README.md               # Project overview and usage</code></pre></div>\n\n<h2 id=\"creation-order\">Creation Order</h2>\n<ol>\n<li><p><strong>Environment Setup</strong> (15 min)</p>\n<ul>\n<li>Create project root and <code>tests/</code> directory.</li>\n<li>Create <code>requirements.txt</code> (add <code>pytest</code>) and <code>.gitignore</code>.</li>\n</ul>\n</li>\n<li><p><strong>Milestone 1: Foundation</strong> (1.5h)</p>\n<ul>\n<li><code>tokens.py</code>: Define <code>TokenType</code> enum and <code>Token</code> dataclass.</li>\n<li><code>scanner.py</code>: Implement <code>Scanner</code> class with <code>advance()</code>, <code>peek()</code>, and basic <code>next_token()</code> loop.</li>\n<li><code>tests/test_foundation.py</code>: Verify single-character tokens and line/column tracking.</li>\n</ul>\n</li>\n<li><p><strong>Milestone 2: Multi-Character Logic</strong> (2h)</p>\n<ul>\n<li><code>tokens.py</code>: Update with operator and keyword variants.</li>\n<li><code>scanner.py</code>: Implement <code>_match()</code>, <code>_peek_next()</code>, <code>_scan_number()</code>, and <code>_scan_identifier()</code>.</li>\n<li><code>tests/test_operators.py</code> &amp; <code>tests/test_literals.py</code>: Verify greedy munch and keyword lookup.</li>\n</ul>\n</li>\n<li><p><strong>Milestone 3: Context-Sensitive Scanning</strong> (2h)</p>\n<ul>\n<li><code>scanner.py</code>: Implement <code>_scan_string()</code> with escape logic and comment skipping (<code>//</code>, <code>/* */</code>).</li>\n<li><code>tests/test_strings.py</code> &amp; <code>tests/test_comments.py</code>: Verify sub-FSM transitions and multi-line tracking.</li>\n</ul>\n</li>\n<li><p><strong>Milestone 4: Validation &amp; Performance</strong> (1.5h)</p>\n<ul>\n<li><code>utils.py</code>: Add <code>partition_tokens</code> for error separation.</li>\n<li><code>scanner.py</code>: Refine <code>scan_tokens()</code> for robust error collection.</li>\n<li><code>tests/test_invariants.py</code> &amp; <code>tests/test_integration.py</code>: Run holistic program tests.</li>\n<li><code>benchmark.py</code>: Execute the 10,000-line performance test.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"file-count-summary\">File Count Summary</h2>\n<ul>\n<li><strong>Total Files</strong>: 14</li>\n<li><strong>Directories</strong>: 2 (root, tests)</li>\n<li><strong>Estimated Lines of Code</strong>: ~500â€“700 lines (including tests)</li>\n</ul>\n<h1 id=\"-beyond-the-atlas-further-reading\">ðŸ“š Beyond the Atlas: Further Reading</h1>\n<h2 id=\"automata-theory-amp-formal-foundations\">Automata Theory &amp; Formal Foundations</h2>\n<p><strong>Paper</strong>: <em>Regular Expression Search Algorithm</em> (Ken Thompson, 1968).\n<strong>Code</strong>: <a href=\"https://github.com/google/re2/blob/main/re2/nfa.cc\">RE2 (Google) â€” <code>nfa.cc</code></a>\n<strong>Best Explanation</strong>: <em>Introduction to the Theory of Computation</em> (Michael Sipser), Chapter 1: Finite Automata.\n<strong>Why</strong>: This is the mathematical bedrock of all tokenization; it proves that the FSMs you built in this project are equivalent to Regular Expressions.\n<strong>Pedagogical Timing</strong>: Read <strong>BEFORE</strong> starting Milestone 1 to understand why a state-based cursor is more powerful than simple string splitting.</p>\n<p><strong>Best Explanation</strong>: <em>Compilers: Principles, Techniques, and Tools</em> (Aho, Lam, Sethi, Ullman), Section 3.3: Recognition of Tokens.\n<strong>Why</strong>: Known as the &quot;Dragon Book,&quot; this specific section formalizes the <strong>Maximal Munch</strong> principle and how to handle lookahead deterministically.\n<strong>Pedagogical Timing</strong>: Read <strong>AFTER</strong> Milestone 2 to see the formal proof for the greedy logic you implemented for operators.</p>\n<h2 id=\"real-world-lexer-implementations\">Real-World Lexer Implementations</h2>\n<p><strong>Code</strong>: <a href=\"https://github.com/golang/go/blob/master/src/go/scanner/scanner.go\">Go Language Scanner â€” <code>src/go/scanner/scanner.go</code></a>\n<strong>Best Explanation</strong>: <em>The Go Programming Language</em> (Donovan &amp; Kernighan), Chapter 11.5: Character-level Lexing.\n<strong>Why</strong>: This is the cleanest, most readable production-grade scanner in existence, using the exact &quot;Scan-Then-Lookup&quot; pattern from Milestone 2.\n<strong>Pedagogical Timing</strong>: Read <strong>AFTER</strong> Milestone 2 to compare your implementation with a language used by millions.</p>\n<p><strong>Code</strong>: <a href=\"https://github.com/python/cpython/blob/main/Parser/tokenizer.c\">CPython Tokenizer â€” <code>Parser/tokenizer.c</code></a>\n<strong>Why</strong>: It provides a masterclass in handling the &quot;grit&quot; of real-world source code, including complex indentation and multiline string states.\n<strong>Pedagogical Timing</strong>: Read <strong>AFTER</strong> Milestone 3 to see how Python handles indentation as a &quot;virtual token&quot; (DEDENT/INDENT).</p>\n<p><strong>Code</strong>: <a href=\"https://github.com/llvm/llvm-project/blob/main/clang/lib/Lex/Lexer.cpp\">LLVM/Clang Lexer â€” <code>lib/Lex/Lexer.cpp</code></a>\n<strong>Best Explanation</strong>: <a href=\"https://clang.llvm.org/docs/InternalsManual.html#the-lexer-and-preprocessor-library\">Clang&#39;s Internal Manual: The Lexer and Preprocessor library</a>.\n<strong>Why</strong>: Clang is the industry standard for high-speed lexing and precise diagnostic positioning.\n<strong>Pedagogical Timing</strong>: Read <strong>AFTER</strong> Milestone 4 to understand how &quot;Lexer Buffers&quot; enable the extreme performance required by modern C++ compilers.</p>\n<h2 id=\"string-literals-amp-escape-sequences\">String Literals &amp; Escape Sequences</h2>\n<p><strong>Spec</strong>: <a href=\"https://tc39.es/ecma262/#sec-literals-string-literals\">ECMA-262 (JavaScript), Section 12.8.4: String Literals</a>.\n<strong>Code</strong>: <a href=\"https://github.com/v8/v8/blob/main/src/parsing/scanner.cc\">V8 (Chrome) â€” <code>src/parsing/scanner.cc</code></a> (Search for <code>ScanString</code>).\n<strong>Best Explanation</strong>: <em>Crafting Interpreters</em> (Robert Nystrom), Chapter 4: Scanning.\n<strong>Why</strong>: Nystromâ€™s explanation of the &quot;inside-a-string&quot; state is the most intuitive pedagogical resource for context-sensitive scanning.\n<strong>Pedagogical Timing</strong>: Read <strong>DURING</strong> Milestone 3 if you struggle with the transition between the START and IN_STRING states.</p>\n<h2 id=\"error-recovery-amp-diagnostic-ux\">Error Recovery &amp; Diagnostic UX</h2>\n<p><strong>Paper</strong>: <em>A Guide to Improving the UX of Compiler Error Messages</em> (Esteban KÃ¼ber, 2019).\n<strong>Code</strong>: <a href=\"https://github.com/rust-lang/rust/blob/master/compiler/rustc_lex/src/lib.rs\">Rust Compiler â€” <code>compiler/rustc_lex/src/lib.rs</code></a>\n<strong>Best Explanation</strong>: <a href=\"https://doc.rust-lang.org/reference/tokens.html\">The Rust Reference: Tokens and Lexical Analysis</a>.\n<strong>Why</strong>: Rust is widely considered the gold standard for developer experience (DX) and precise error reporting using token position metadata.\n<strong>Pedagogical Timing</strong>: Read <strong>AFTER</strong> Milestone 4 to see how the position data you captured can be turned into helpful &quot;suggested fix&quot; squiggles.</p>\n<h2 id=\"performance-amp-benchmarking\">Performance &amp; Benchmarking</h2>\n<p><strong>Code</strong>: <a href=\"https://github.com/lua/lua/blob/master/llex.c\">Lua 5.4 Lexer â€” <code>llex.c</code></a>\n<strong>Why</strong>: Luaâ€™s scanner is incredibly fast and minimal; it demonstrates how to achieve extreme performance without the overhead of heavy abstractions.\n<strong>Pedagogical Timing</strong>: Read <strong>AFTER</strong> completing the 10,000-line benchmark in Milestone 4 if you want to see how to push performance further.</p>\n","toc":[{"level":1,"text":"ðŸŽ¯ Project Charter: C-Like Language Tokenizer","id":"-project-charter-c-like-language-tokenizer"},{"level":2,"text":"What You Are Building","id":"what-you-are-building"},{"level":2,"text":"Why This Project Exists","id":"why-this-project-exists"},{"level":2,"text":"What You Will Be Able to Do When Done","id":"what-you-will-be-able-to-do-when-done"},{"level":2,"text":"Final Deliverable","id":"final-deliverable"},{"level":2,"text":"Is This Project For You?","id":"is-this-project-for-you"},{"level":2,"text":"Estimated Effort","id":"estimated-effort"},{"level":2,"text":"Definition of Done","id":"definition-of-done"},{"level":1,"text":"Tokenizer / Lexer: Building a Character-Level Scanner for a C-like Language","id":"tokenizer-lexer-building-a-character-level-scanner-for-a-c-like-language"},{"level":1,"text":"Milestone 1: Token Types &amp; Scanner Foundation","id":"milestone-1-token-types-amp-scanner-foundation"},{"level":2,"text":"Where You Are in the Pipeline","id":"where-you-are-in-the-pipeline"},{"level":2,"text":"The Revelation: Tokenizers Are Not Split()","id":"the-revelation-tokenizers-are-not-split"},{"level":3,"text":"1. What it IS","id":"1-what-it-is"},{"level":3,"text":"2. WHY you need it right now","id":"2-why-you-need-it-right-now"},{"level":3,"text":"3. Key Insight: &quot;The State is your Memory&quot;","id":"3-key-insight-quotthe-state-is-your-memoryquot"},{"level":2,"text":"Designing the Token Type Enumeration","id":"designing-the-token-type-enumeration"},{"level":2,"text":"The Token Data Structure","id":"the-token-data-structure"},{"level":2,"text":"The EOF token has an empty lexeme â€” there is no source text that corresponds to end-of-file. The Error token&#39;s lexeme is the offending character.","id":"the-eof-token-has-an-empty-lexeme-there-is-no-source-text-that-corresponds-to-end-of-file-the-error-token39s-lexeme-is-the-offending-character"},{"level":2,"text":"The Scanner Class: Architecture First","id":"the-scanner-class-architecture-first"},{"level":3,"text":"is_at_end()","id":"is_at_end"},{"level":3,"text":"peek()","id":"peek"},{"level":3,"text":"advance()","id":"advance"},{"level":3,"text":"A Trace Through advance() and peek()","id":"a-trace-through-advance-and-peek"},{"level":2,"text":"Every character is visited exactly once. peek() is free â€” no movement. advance() is permanent â€” no going back.","id":"every-character-is-visited-exactly-once-peek-is-free-no-movement-advance-is-permanent-no-going-back"},{"level":2,"text":"Position Tracking in Depth","id":"position-tracking-in-depth"},{"level":2,"text":"By recording tok_line and tok_col from self.line / self.column before calling advance(), you capture the position of the first character of the token. All subsequent advance() calls within the same token scan will update self.line and self.column, but the token you emit will carry the start position.\nThis pattern will be critical in Milestone 3 when you scan string literals that span multiple lines â€” the token&#39;s reported position should be the opening quote, not the closing one.","id":"by-recording-tok_line-and-tok_col-from-selfline-selfcolumn-before-calling-advance-you-capture-the-position-of-the-first-character-of-the-token-all-subsequent-advance-calls-within-the-same-token-scan-will-update-selfline-and-selfcolumn-but-the-token-you-emit-will-carry-the-start-position-this-pattern-will-be-critical-in-milestone-3-when-you-scan-string-literals-that-span-multiple-lines-the-token39s-reported-position-should-be-the-opening-quote-not-the-closing-one"},{"level":2,"text":"The Finite State Machine View","id":"the-finite-state-machine-view"},{"level":2,"text":"Using a dictionary for single-character dispatch is both idiomatic Python and faster than a long if/elif chain â€” a dictionary lookup is O(1) hash table access.","id":"using-a-dictionary-for-single-character-dispatch-is-both-idiomatic-python-and-faster-than-a-long-ifelif-chain-a-dictionary-lookup-is-o1-hash-table-access"},{"level":2,"text":"Putting It Together: scan_tokens() and next_token()","id":"putting-it-together-scan_tokens-and-next_token"},{"level":2,"text":"The Full Scanner: Milestone 1 Complete Implementation","id":"the-full-scanner-milestone-1-complete-implementation"},{"level":2,"text":"Testing Your Foundation","id":"testing-your-foundation"},{"level":2,"text":"Run all tests with python -m pytest test_scanner.py -v. Every test should pass green before you proceed to Milestone 2.","id":"run-all-tests-with-python-m-pytest-test_scannerpy-v-every-test-should-pass-green-before-you-proceed-to-milestone-2"},{"level":2,"text":"Design Decisions: Why This Architecture?","id":"design-decisions-why-this-architecture"},{"level":3,"text":"Why a class rather than a function with a loop?","id":"why-a-class-rather-than-a-function-with-a-loop"},{"level":3,"text":"Why store the entire source string rather than reading character by character?","id":"why-store-the-entire-source-string-rather-than-reading-character-by-character"},{"level":3,"text":"Why &quot;\\0&quot; as the sentinel from peek()?","id":"why-quot0quot-as-the-sentinel-from-peek"},{"level":2,"text":"Alternatives include None (Pythonic but requires Optional[str] type annotation and None checks everywhere), raising an exception (verbose), or returning &quot;&quot; (empty string comparisons work but are confusing). The &quot;\\0&quot; convention comes from C tokenizers (Crafting Interpreters, GCC, Clang) and has one decisive advantage: all character comparisons like peek() == &quot;=&quot; and peek() in &quot;abc&quot; work correctly and safely at end-of-input â€” &quot;\\0&quot; will never match any valid source character.","id":"alternatives-include-none-pythonic-but-requires-optionalstr-type-annotation-and-none-checks-everywhere-raising-an-exception-verbose-or-returning-quotquot-empty-string-comparisons-work-but-are-confusing-the-quot0quot-convention-comes-from-c-tokenizers-crafting-interpreters-gcc-clang-and-has-one-decisive-advantage-all-character-comparisons-like-peek-quotquot-and-peek-in-quotabcquot-work-correctly-and-safely-at-end-of-input-quot0quot-will-never-match-any-valid-source-character"},{"level":2,"text":"The Three-Level View of Your Scanner","id":"the-three-level-view-of-your-scanner"},{"level":2,"text":"Python strings are immutable sequences stored in contiguous memory. source[i] is a direct offset calculation â€” not iteration. This is why character-level scanning in Python is fast enough to meet the 10,000-line benchmark.","id":"python-strings-are-immutable-sequences-stored-in-contiguous-memory-sourcei-is-a-direct-offset-calculation-not-iteration-this-is-why-character-level-scanning-in-python-is-fast-enough-to-meet-the-10000-line-benchmark"},{"level":2,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":2,"text":"The invariant is: self.column is always the column of the character that advance() will consume next, not the one just consumed. This is consistent with self.current being the index of the next character to consume. Both are &quot;one ahead&quot; pointers.","id":"the-invariant-is-selfcolumn-is-always-the-column-of-the-character-that-advance-will-consume-next-not-the-one-just-consumed-this-is-consistent-with-selfcurrent-being-the-index-of-the-next-character-to-consume-both-are-quotone-aheadquot-pointers"},{"level":2,"text":"Knowledge Cascade: One Concept, Ten Unlocks","id":"knowledge-cascade-one-concept-ten-unlocks"},{"level":2,"text":"Summary: What You Have Built","id":"summary-what-you-have-built"},{"level":1,"text":"Milestone 2: Multi-Character Tokens &amp; Maximal Munch","id":"milestone-2-multi-character-tokens-amp-maximal-munch"},{"level":2,"text":"Where You Are in the Pipeline","id":"where-you-are-in-the-pipeline"},{"level":2,"text":"In Milestone 1, you built the engine: a character-consuming scanner that can recognize single characters and track its position through source text. Every character your scanner touches is visited exactly once, left to right, with no going back.\nNow you face the first real challenge of language design: multi-character tokens. The characters &gt; and = each mean something on their own (GREATER, ASSIGN). But together, in that exact order, they mean something different (GREATER_EQ). How does your scanner â€” which reads one character at a time â€” decide which interpretation is correct?\nThis milestone answers that question with a principle called maximal munch: always consume as many characters as possible for the current token. You will apply it to two-character operators, number literals, and identifiers. By the end, your scanner will handle the full vocabulary of your C-like language except for strings and comments (those come in Milestone 3).","id":"in-milestone-1-you-built-the-engine-a-character-consuming-scanner-that-can-recognize-single-characters-and-track-its-position-through-source-text-every-character-your-scanner-touches-is-visited-exactly-once-left-to-right-with-no-going-back-now-you-face-the-first-real-challenge-of-language-design-multi-character-tokens-the-characters-gt-and-each-mean-something-on-their-own-greater-assign-but-together-in-that-exact-order-they-mean-something-different-greater_eq-how-does-your-scanner-which-reads-one-character-at-a-time-decide-which-interpretation-is-correct-this-milestone-answers-that-question-with-a-principle-called-maximal-munch-always-consume-as-many-characters-as-possible-for-the-current-token-you-will-apply-it-to-two-character-operators-number-literals-and-identifiers-by-the-end-your-scanner-will-handle-the-full-vocabulary-of-your-c-like-language-except-for-strings-and-comments-those-come-in-milestone-3"},{"level":2,"text":"The Revelation: Tokenization Is Not Pattern Matching","id":"the-revelation-tokenization-is-not-pattern-matching"},{"level":2,"text":"Two-Character Operators: Lookahead in Practice","id":"two-character-operators-lookahead-in-practice"},{"level":3,"text":"The &gt;== Trace: Maximal Munch in Slow Motion","id":"the-gt-trace-maximal-munch-in-slow-motion"},{"level":2,"text":"Result: [GREATER_EQ(&quot;&gt;=&quot;, 1:1), ASSIGN(&quot;=&quot;, 1:3), EOF(&quot;&quot;, 1:4)]\nThis is maximal munch in action. When the scanner sees &gt;, it greedily consumed the = that followed. It did not &quot;ask&quot; whether &gt;= should be followed by another = to form &gt;== (which is not a token in this language). It simply asked &quot;should I take one more character?&quot; at each step, and stopped as soon as the answer was no. The second = is left for the next call to next_token(), which correctly scans it as ASSIGN.","id":"result-greater_eqquotgtquot-11-assignquotquot-13-eofquotquot-14-this-is-maximal-munch-in-action-when-the-scanner-sees-gt-it-greedily-consumed-the-that-followed-it-did-not-quotaskquot-whether-gt-should-be-followed-by-another-to-form-gt-which-is-not-a-token-in-this-language-it-simply-asked-quotshould-i-take-one-more-characterquot-at-each-step-and-stopped-as-soon-as-the-answer-was-no-the-second-is-left-for-the-next-call-to-next_token-which-correctly-scans-it-as-assign"},{"level":2,"text":"Integrating Operator Scanning into next_token()","id":"integrating-operator-scanning-into-next_token"},{"level":2,"text":"Notice the dispatch order matters: digits before identifiers (so 4abc is scanned as a number then an identifier, not a single broken token), and the operator check before the fallthrough error case.\nAlso note that / is not yet in SINGLE_CHAR_TOKENS. In Milestone 3, a / might start a comment (// or /*) â€” so it needs the same two-character lookahead treatment as = and &gt;. For now, if you want to test division, you can temporarily add it back. The milestone 3 implementation will replace it.","id":"notice-the-dispatch-order-matters-digits-before-identifiers-so-4abc-is-scanned-as-a-number-then-an-identifier-not-a-single-broken-token-and-the-operator-check-before-the-fallthrough-error-case-also-note-that-is-not-yet-in-single_char_tokens-in-milestone-3-a-might-start-a-comment-or-so-it-needs-the-same-two-character-lookahead-treatment-as-and-gt-for-now-if-you-want-to-test-division-you-can-temporarily-add-it-back-the-milestone-3-implementation-will-replace-it"},{"level":2,"text":"Number Literals: The State Machine Inside _scan_number()","id":"number-literals-the-state-machine-inside-_scan_number"},{"level":3,"text":"Edge Cases to Decide Explicitly","id":"edge-cases-to-decide-explicitly"},{"level":2,"text":"Identifiers: The Scan-Then-Lookup Pattern","id":"identifiers-the-scan-then-lookup-pattern"},{"level":3,"text":"Why the Lookup Table Avoids a Classic Bug","id":"why-the-lookup-table-avoids-a-classic-bug"},{"level":3,"text":"Keyword Table Design: Two Variants","id":"keyword-table-design-two-variants"},{"level":2,"text":"Extended FSM: What Your Scanner Now Looks Like","id":"extended-fsm-what-your-scanner-now-looks-like"},{"level":2,"text":"The Complete Milestone 2 Scanner","id":"the-complete-milestone-2-scanner"},{"level":2,"text":"Lookahead as a Language Design Constraint","id":"lookahead-as-a-language-design-constraint"},{"level":2,"text":"Testing Milestone 2","id":"testing-milestone-2"},{"level":2,"text":"Design Decisions: The String Accumulation Question","id":"design-decisions-the-string-accumulation-question"},{"level":2,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":2,"text":"1. Consuming the character that terminates a token\nThe most common bug in _scan_number(): the loop condition is while self.peek().isdigit(), and inside the loop you call self.advance(). If you accidentally write while self.advance().isdigit(), you consume the terminating character (e.g., a space or )), and the next token starts in the wrong place. Always peek() to check, advance() to consume.\n2. _match() skipping position tracking\nIf you implement _match() by directly accessing self.source[self.current] and incrementing self.current without calling self.advance(), you bypass newline detection. This is safe for operators (which are never newlines), but it is a trap for future code. Always call self.advance() inside _match() when consuming.\n3. Keyword matching &#39;return&#39; inside &#39;returnValue&#39;\nThe scan-then-lookup pattern avoids this: _scan_identifier() consumes until a non-identifier character is found, producing the full lexeme &quot;returnValue&quot;. The keyword table does not contain &quot;returnValue&quot;, so it is correctly emitted as IDENTIFIER. The bug only appears if you try to match keywords character by character.\n4. Float 3. consuming the dot when it shouldn&#39;t\nThe condition if self.peek() == &quot;.&quot; and self._peek_next().isdigit() prevents this. If the character after . is not a digit, the dot is not consumed. Without _peek_next(), you might consume the dot and then find no digits â€” leaving you with the malformed lexeme &quot;3.&quot; and having &quot;stolen&quot; the dot from whatever follows.\n5. Operator position captured after the first advance()\nRemember from Milestone 1: tok_line and tok_col are captured before advance() is called in next_token(). By the time _scan_operator() is called, the first character is already consumed and the position is already captured. Do not capture position inside _scan_operator() â€” the positions come from next_token() as parameters.\n6. == at end of file\nWhen scanning == at the very end of the source, the first = is consumed by advance(), then _match(&quot;=&quot;) is called. _match calls is_at_end() â€” which returns False because there is still one character left. Then it checks self.source[self.current] != &quot;=&quot; â€” which is False (it is =), so it consumes it. This works correctly. The sentinel &quot;\\0&quot; from peek() is not used in _match() â€” _match() uses is_at_end() and direct source access. Make sure _match() checks is_at_end() first.","id":"1-consuming-the-character-that-terminates-a-token-the-most-common-bug-in-_scan_number-the-loop-condition-is-while-selfpeekisdigit-and-inside-the-loop-you-call-selfadvance-if-you-accidentally-write-while-selfadvanceisdigit-you-consume-the-terminating-character-eg-a-space-or-and-the-next-token-starts-in-the-wrong-place-always-peek-to-check-advance-to-consume-2-_match-skipping-position-tracking-if-you-implement-_match-by-directly-accessing-selfsourceselfcurrent-and-incrementing-selfcurrent-without-calling-selfadvance-you-bypass-newline-detection-this-is-safe-for-operators-which-are-never-newlines-but-it-is-a-trap-for-future-code-always-call-selfadvance-inside-_match-when-consuming-3-keyword-matching-39return39-inside-39returnvalue39-the-scan-then-lookup-pattern-avoids-this-_scan_identifier-consumes-until-a-non-identifier-character-is-found-producing-the-full-lexeme-quotreturnvaluequot-the-keyword-table-does-not-contain-quotreturnvaluequot-so-it-is-correctly-emitted-as-identifier-the-bug-only-appears-if-you-try-to-match-keywords-character-by-character-4-float-3-consuming-the-dot-when-it-shouldn39t-the-condition-if-selfpeek-quotquot-and-self_peek_nextisdigit-prevents-this-if-the-character-after-is-not-a-digit-the-dot-is-not-consumed-without-_peek_next-you-might-consume-the-dot-and-then-find-no-digits-leaving-you-with-the-malformed-lexeme-quot3quot-and-having-quotstolenquot-the-dot-from-whatever-follows-5-operator-position-captured-after-the-first-advance-remember-from-milestone-1-tok_line-and-tok_col-are-captured-before-advance-is-called-in-next_token-by-the-time-_scan_operator-is-called-the-first-character-is-already-consumed-and-the-position-is-already-captured-do-not-capture-position-inside-_scan_operator-the-positions-come-from-next_token-as-parameters-6-at-end-of-file-when-scanning-at-the-very-end-of-the-source-the-first-is-consumed-by-advance-then-_matchquotquot-is-called-_match-calls-is_at_end-which-returns-false-because-there-is-still-one-character-left-then-it-checks-selfsourceselfcurrent-quotquot-which-is-false-it-is-so-it-consumes-it-this-works-correctly-the-sentinel-quot0quot-from-peek-is-not-used-in-_match-_match-uses-is_at_end-and-direct-source-access-make-sure-_match-checks-is_at_end-first"},{"level":2,"text":"Knowledge Cascade: Maximal Munch Connects to Everything","id":"knowledge-cascade-maximal-munch-connects-to-everything"},{"level":2,"text":"Summary: What You Have Built","id":"summary-what-you-have-built"},{"level":1,"text":"Milestone 3: Strings &amp; Comments","id":"milestone-3-strings-amp-comments"},{"level":2,"text":"Where You Are in the Pipeline","id":"where-you-are-in-the-pipeline"},{"level":2,"text":"Your scanner can now handle every token in your C-like language except two: string literals and comments. At first glance, these seem simple â€” find the closing &quot;, find the closing */, done. But that intuition is the misconception this milestone exists to correct.\nString literals and comments are not just &quot;longer tokens.&quot; They are the first point in your scanner where the rules of scanning change depending on context. Inside a string, a / is just the division character &#39;/&#39; â€” not the start of a comment. Inside a comment, a &quot; is just a character â€” not the start of a string. The same character, in two different positions, means two entirely different things. Your scanner needs to know where it is to interpret what it sees.\nThis is the scanner&#39;s first encounter with context-sensitivity, and handling it correctly requires extending your finite state machine with new states â€” mini state machines that take over while you are inside a string or comment, apply different rules, and eventually hand control back to the main scanner.\nBy the end of this milestone, your scanner will handle every character in a well-formed source file, plus gracefully diagnose malformed ones (unterminated strings and comments). The gap between Milestone 2 and a production-ready lexer is almost entirely closed here.","id":"your-scanner-can-now-handle-every-token-in-your-c-like-language-except-two-string-literals-and-comments-at-first-glance-these-seem-simple-find-the-closing-quot-find-the-closing-done-but-that-intuition-is-the-misconception-this-milestone-exists-to-correct-string-literals-and-comments-are-not-just-quotlonger-tokensquot-they-are-the-first-point-in-your-scanner-where-the-rules-of-scanning-change-depending-on-context-inside-a-string-a-is-just-the-division-character-3939-not-the-start-of-a-comment-inside-a-comment-a-quot-is-just-a-character-not-the-start-of-a-string-the-same-character-in-two-different-positions-means-two-entirely-different-things-your-scanner-needs-to-know-where-it-is-to-interpret-what-it-sees-this-is-the-scanner39s-first-encounter-with-context-sensitivity-and-handling-it-correctly-requires-extending-your-finite-state-machine-with-new-states-mini-state-machines-that-take-over-while-you-are-inside-a-string-or-comment-apply-different-rules-and-eventually-hand-control-back-to-the-main-scanner-by-the-end-of-this-milestone-your-scanner-will-handle-every-character-in-a-well-formed-source-file-plus-gracefully-diagnose-malformed-ones-unterminated-strings-and-comments-the-gap-between-milestone-2-and-a-production-ready-lexer-is-almost-entirely-closed-here"},{"level":2,"text":"The Revelation: It&#39;s Not a Search Problem","id":"the-revelation-it39s-not-a-search-problem"},{"level":2,"text":"Context-Sensitivity: A Formal Aside (Worth Two Minutes)","id":"context-sensitivity-a-formal-aside-worth-two-minutes"},{"level":2,"text":"You have been building a regular language tokenizer â€” one that a finite state machine can handle. Regular languages are characterized by the fact that their recognition depends only on the current state, not on unbounded memory of what came before.\nHere is the formal tension: string escaping technically pushes beyond regular languages. A scanner must track whether the preceding character was \\ to know if the current &quot; terminates the string. This &quot;memory&quot; of one preceding character might seem to require more than a finite state machine â€” but it does not. You simply add states. The state IN_ESCAPE captures the fact that a \\ was just seen. This is a finite amount of memory (one bit of information: &quot;was the previous character a backslash?&quot;). A finite state machine can represent any finite amount of memory by encoding it into states.\nThe deeper point: the \\ escape mechanism is **context-sensitive at the character level**. The character after \\ is interpreted differently from any other character. When you implement _scan_string(), you are implementing this context-sensitivity explicitly â€” by changing what you do with each character based on the current scanning state (normal vs. after-backslash).\nThis is also why tokenizers are not &quot;purely regular&quot; in practice. The lexical structure of real languages â€” string escape sequences, here-documents, raw strings, backtick-quoted identifiers in SQL â€” all require some context tracking beyond a pure DFA. Production scanners handle this with flags and sub-states, exactly as you are about to do.","id":"you-have-been-building-a-regular-language-tokenizer-one-that-a-finite-state-machine-can-handle-regular-languages-are-characterized-by-the-fact-that-their-recognition-depends-only-on-the-current-state-not-on-unbounded-memory-of-what-came-before-here-is-the-formal-tension-string-escaping-technically-pushes-beyond-regular-languages-a-scanner-must-track-whether-the-preceding-character-was-to-know-if-the-current-quot-terminates-the-string-this-quotmemoryquot-of-one-preceding-character-might-seem-to-require-more-than-a-finite-state-machine-but-it-does-not-you-simply-add-states-the-state-in_escape-captures-the-fact-that-a-was-just-seen-this-is-a-finite-amount-of-memory-one-bit-of-information-quotwas-the-previous-character-a-backslashquot-a-finite-state-machine-can-represent-any-finite-amount-of-memory-by-encoding-it-into-states-the-deeper-point-the-escape-mechanism-is-context-sensitive-at-the-character-level-the-character-after-is-interpreted-differently-from-any-other-character-when-you-implement-_scan_string-you-are-implementing-this-context-sensitivity-explicitly-by-changing-what-you-do-with-each-character-based-on-the-current-scanning-state-normal-vs-after-backslash-this-is-also-why-tokenizers-are-not-quotpurely-regularquot-in-practice-the-lexical-structure-of-real-languages-string-escape-sequences-here-documents-raw-strings-backtick-quoted-identifiers-in-sql-all-require-some-context-tracking-beyond-a-pure-dfa-production-scanners-handle-this-with-flags-and-sub-states-exactly-as-you-are-about-to-do"},{"level":2,"text":"The / Character: The Critical Branch","id":"the-character-the-critical-branch"},{"level":2,"text":"Single-Line Comments: Skipping to End of Line","id":"single-line-comments-skipping-to-end-of-line"},{"level":2,"text":"This is the simplest scanning method in the project: advance as long as you are not at a newline or end-of-file. The newline will be consumed by _skip_whitespace() on the next next_token() call, which will then correctly increment self.line.\nWhy not consume the newline here? Because the newline belongs to the whitespace layer, not the comment layer. Separating concerns â€” &quot;comments skip content, whitespace handles line endings&quot; â€” keeps each method focused. It also means _skip_line_comment() never touches self.line, avoiding any risk of double-counting newlines.\nThe invisible case: What if a // comment appears at the very end of the file with no trailing newline? self.peek() != &quot;\\n&quot; will never be true, but self.is_at_end() will become true and exit the loop. This is handled correctly â€” no special case needed.","id":"this-is-the-simplest-scanning-method-in-the-project-advance-as-long-as-you-are-not-at-a-newline-or-end-of-file-the-newline-will-be-consumed-by-_skip_whitespace-on-the-next-next_token-call-which-will-then-correctly-increment-selfline-why-not-consume-the-newline-here-because-the-newline-belongs-to-the-whitespace-layer-not-the-comment-layer-separating-concerns-quotcomments-skip-content-whitespace-handles-line-endingsquot-keeps-each-method-focused-it-also-means-_skip_line_comment-never-touches-selfline-avoiding-any-risk-of-double-counting-newlines-the-invisible-case-what-if-a-comment-appears-at-the-very-end-of-the-file-with-no-trailing-newline-selfpeek-quotnquot-will-never-be-true-but-selfis_at_end-will-become-true-and-exit-the-loop-this-is-handled-correctly-no-special-case-needed"},{"level":2,"text":"Multi-Line Comments: The State Machine Within a State Machine","id":"multi-line-comments-the-state-machine-within-a-state-machine"},{"level":2,"text":"Escape Sequences: The Two-Character Dance","id":"escape-sequences-the-two-character-dance"},{"level":3,"text":"The Unterminated String: Blame the Cause, Not the Symptom","id":"the-unterminated-string-blame-the-cause-not-the-symptom"},{"level":2,"text":"The unterminated string starts on line 3. The EOF (or the next line) is where the scanner discovers the problem, but the cause is on line 3 â€” the unclosed &quot;. An error message saying &quot;unterminated string at line 5, column 1&quot; (pointing at return) would be useless. &quot;Unterminated string at line 3, column 15&quot; (pointing at the opening &quot;) is actionable.\nThis is a general principle in error reporting: report the position of the cause, not the position where the consequence was detected. The scanner detects the problem when it runs out of valid string content. It reports the problem where the string began.\nThe same principle applies to multi-line comments: the Error token for an unterminated /* ... */ reports the position of the opening /*, even if the scanner consumed thousands of characters before realizing no */ was coming.","id":"the-unterminated-string-starts-on-line-3-the-eof-or-the-next-line-is-where-the-scanner-discovers-the-problem-but-the-cause-is-on-line-3-the-unclosed-quot-an-error-message-saying-quotunterminated-string-at-line-5-column-1quot-pointing-at-return-would-be-useless-quotunterminated-string-at-line-3-column-15quot-pointing-at-the-opening-quot-is-actionable-this-is-a-general-principle-in-error-reporting-report-the-position-of-the-cause-not-the-position-where-the-consequence-was-detected-the-scanner-detects-the-problem-when-it-runs-out-of-valid-string-content-it-reports-the-problem-where-the-string-began-the-same-principle-applies-to-multi-line-comments-the-error-token-for-an-unterminated-reports-the-position-of-the-opening-even-if-the-scanner-consumed-thousands-of-characters-before-realizing-no-was-coming"},{"level":2,"text":"Integrating String and Comment Scanning into next_token()","id":"integrating-string-and-comment-scanning-into-next_token"},{"level":2,"text":"The order of checks matters. The &quot;/&quot; check must come after the single-character and operator checks (since +, -, * are not ambiguous), and before the number and identifier checks (since / is neither a digit nor a letter).","id":"the-order-of-checks-matters-the-quotquot-check-must-come-after-the-single-character-and-operator-checks-since-are-not-ambiguous-and-before-the-number-and-identifier-checks-since-is-neither-a-digit-nor-a-letter"},{"level":2,"text":"Line Number Tracking Inside Strings and Comments","id":"line-number-tracking-inside-strings-and-comments"},{"level":2,"text":"The token y correctly reports line 2, column 12 â€” even though it was scanned long after the comment started on line 1. Every advance() call inside the comment handled newline detection. You did not write a single special case for this.\nThis is the payoff of the advance() abstraction: all callers benefit from correct position tracking without having to think about it.","id":"the-token-y-correctly-reports-line-2-column-12-even-though-it-was-scanned-long-after-the-comment-started-on-line-1-every-advance-call-inside-the-comment-handled-newline-detection-you-did-not-write-a-single-special-case-for-this-this-is-the-payoff-of-the-advance-abstraction-all-callers-benefit-from-correct-position-tracking-without-having-to-think-about-it"},{"level":2,"text":"The Complete Milestone 3 Scanner","id":"the-complete-milestone-3-scanner"},{"level":2,"text":"The Complete FSM View","id":"the-complete-fsm-view"},{"level":2,"text":"Testing Milestone 3","id":"testing-milestone-3"},{"level":2,"text":"Run: python -m pytest test_scanner.py -v","id":"run-python-m-pytest-test_scannerpy-v"},{"level":2,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":2,"text":"1. Consuming the closing quote as part of the next token\nThe string scanner&#39;s closing-quote branch does lexeme_chars.append(&#39;&quot;&#39;) and then returns. After this return, self.current points at the character after the &quot;. The next call to next_token() correctly starts from there. The bug occurs if you forget to consume the closing quote â€” if the loop&#39;s exit condition is peek() == &#39;&quot;&#39; rather than advance() returning &#39;&quot;&#39;. Then self.current still points at &quot; and the next token is an empty string or a new string starting from the same quote.\n2. The escape-backslash-at-end-of-string bug\nThe source &quot;hello\\&quot; â€” a string that ends with a backslash and a quote. The scanner sees \\, enters escape handling, consumes &quot;. It just consumed the closing quote as an escape character. The loop continues, finds EOF, and returns an Error token. This is correct behavior: &quot;hello\\&quot; is unterminated (the \\&quot; is an escaped quote, not the string terminator). Your implementation handles this naturally because the \\\\ branch consumes the next character unconditionally via advance(), not via the main loop.\n*3. Multi-line comment starting with `/whereadvancesself.current** If _skip_block_comment()checksif char == &quot;&quot;and peeks at the next character, it must then *consume* the/withself.advance(). If it returns without consuming the /, the next call to next_token()sees/, emits SLASH, and your token stream is corrupted. The implementation above calls self.advance()explicitly on the closing/. **4. Column counting inside strings with escape sequences** The lexeme &quot;hello\\n&quot;in source text is 9 characters:&quot;, h, e, l, l, o, `, n, &quot;. The scanner advances through 9 source characters, incrementing self.column for each. The \\n in the source is a backslash followed by n â€” not an actual newline â€” so self.line is not incremented. If you mistakenly process the backslash as producing an actual newline character and feed it to advance()&#39;s newline detection, you would incorrectly increment the line counter. The fix: in _scan_string(), you call self.advance() to consume the raw source characters. The escape interpretation (turning \\n into chr(10)) happens elsewhere (in the interpreter), not here.\n5. / inside a string triggering comment detection\nThe string &quot;http://example.com&quot; contains //. If your scanner checks for // before it checks whether it is inside a string, it would incorrectly start a line comment. Your next_token() only reaches the / handling code for characters that are not inside strings â€” the / inside &quot;http://example.com&quot; is consumed by _scan_string() as part of the string content, long before next_token() has a chance to see it. The state machine architecture prevents this bug by design: _scan_string() has taken over; next_token() is not involved until the closing &quot; is found.\n6. Block comment ending with **/\nThe source /** body **/ (two asterisks before closing slash). Your scanner sees * in the loop, peeks / â€” match! â€” consumes /, closes comment. The extra * before the * that matched was just consumed as comment content. The final * in **/ is the one that matched. This is correct per the spec: */ ends the comment, regardless of how many * precede it. No special handling needed.","id":"1-consuming-the-closing-quote-as-part-of-the-next-token-the-string-scanner39s-closing-quote-branch-does-lexeme_charsappend39quot39-and-then-returns-after-this-return-selfcurrent-points-at-the-character-after-the-quot-the-next-call-to-next_token-correctly-starts-from-there-the-bug-occurs-if-you-forget-to-consume-the-closing-quote-if-the-loop39s-exit-condition-is-peek-39quot39-rather-than-advance-returning-39quot39-then-selfcurrent-still-points-at-quot-and-the-next-token-is-an-empty-string-or-a-new-string-starting-from-the-same-quote-2-the-escape-backslash-at-end-of-string-bug-the-source-quothelloquot-a-string-that-ends-with-a-backslash-and-a-quote-the-scanner-sees-enters-escape-handling-consumes-quot-it-just-consumed-the-closing-quote-as-an-escape-character-the-loop-continues-finds-eof-and-returns-an-error-token-this-is-correct-behavior-quothelloquot-is-unterminated-the-quot-is-an-escaped-quote-not-the-string-terminator-your-implementation-handles-this-naturally-because-the-branch-consumes-the-next-character-unconditionally-via-advance-not-via-the-main-loop-3-multi-line-comment-starting-with-whereadvancesselfcurrent-if-_skip_block_commentchecksif-char-quotquotand-peeks-at-the-next-character-it-must-then-consume-thewithselfadvance-if-it-returns-without-consuming-the-the-next-call-to-next_tokensees-emits-slash-and-your-token-stream-is-corrupted-the-implementation-above-calls-selfadvanceexplicitly-on-the-closing-4-column-counting-inside-strings-with-escape-sequences-the-lexeme-quothellonquotin-source-text-is-9-charactersquot-h-e-l-l-o-n-quot-the-scanner-advances-through-9-source-characters-incrementing-selfcolumn-for-each-the-n-in-the-source-is-a-backslash-followed-by-n-not-an-actual-newline-so-selfline-is-not-incremented-if-you-mistakenly-process-the-backslash-as-producing-an-actual-newline-character-and-feed-it-to-advance39s-newline-detection-you-would-incorrectly-increment-the-line-counter-the-fix-in-_scan_string-you-call-selfadvance-to-consume-the-raw-source-characters-the-escape-interpretation-turning-n-into-chr10-happens-elsewhere-in-the-interpreter-not-here-5-inside-a-string-triggering-comment-detection-the-string-quothttpexamplecomquot-contains-if-your-scanner-checks-for-before-it-checks-whether-it-is-inside-a-string-it-would-incorrectly-start-a-line-comment-your-next_token-only-reaches-the-handling-code-for-characters-that-are-not-inside-strings-the-inside-quothttpexamplecomquot-is-consumed-by-_scan_string-as-part-of-the-string-content-long-before-next_token-has-a-chance-to-see-it-the-state-machine-architecture-prevents-this-bug-by-design-_scan_string-has-taken-over-next_token-is-not-involved-until-the-closing-quot-is-found-6-block-comment-ending-with-the-source-body-two-asterisks-before-closing-slash-your-scanner-sees-in-the-loop-peeks-match-consumes-closes-comment-the-extra-before-the-that-matched-was-just-consumed-as-comment-content-the-final-in-is-the-one-that-matched-this-is-correct-per-the-spec-ends-the-comment-regardless-of-how-many-precede-it-no-special-handling-needed"},{"level":2,"text":"Knowledge Cascade: One Insight, Many Domains","id":"knowledge-cascade-one-insight-many-domains"},{"level":2,"text":"Summary: What You Have Built","id":"summary-what-you-have-built"},{"level":1,"text":"Milestone 4: Integration Testing &amp; Error Recovery","id":"milestone-4-integration-testing-amp-error-recovery"},{"level":2,"text":"Where You Are in the Pipeline","id":"where-you-are-in-the-pipeline"},{"level":2,"text":"You have built a complete tokenizer. Milestones 1 through 3 gave you a scanner that handles every character in your C-like language: single-character tokens, multi-character operators via maximal munch, number and identifier literals, keywords, string literals with escape sequences, and both styles of comments. Every feature works â€” in isolation.\nThis milestone is about the hardest part of building any software system: making sure the whole is correct, not just the parts.\nIntegration testing is where you discover that test_plus_token() passes but test_expression_with_spaces() fails because of a column-tracking bug that only appears after whitespace is consumed. It is where you find that string scanning works for &quot;hello&quot; but corrupts position tracking for every token that follows it on the same line. It is where you learn that the scanner you built in 15 hours of careful work has a subtle off-by-one that only shows up in multi-line programs.\nThis milestone also asks you to confront a design decision you made without realizing it: what happens when the scanner encounters invalid input? You already emit ERROR tokens for unrecognized characters â€” but does scanning continue afterward? Do you collect all errors or stop at the first one? These choices determine whether your scanner is useful in real-world tools like IDE syntax highlighters, or whether it fails the moment a user makes a typo.\nBy the end of this milestone, you will have a fully validated scanner, a working error recovery strategy, a test suite that covers edge cases and complete programs, and a performance result that confirms your implementation meets production requirements.","id":"you-have-built-a-complete-tokenizer-milestones-1-through-3-gave-you-a-scanner-that-handles-every-character-in-your-c-like-language-single-character-tokens-multi-character-operators-via-maximal-munch-number-and-identifier-literals-keywords-string-literals-with-escape-sequences-and-both-styles-of-comments-every-feature-works-in-isolation-this-milestone-is-about-the-hardest-part-of-building-any-software-system-making-sure-the-whole-is-correct-not-just-the-parts-integration-testing-is-where-you-discover-that-test_plus_token-passes-but-test_expression_with_spaces-fails-because-of-a-column-tracking-bug-that-only-appears-after-whitespace-is-consumed-it-is-where-you-find-that-string-scanning-works-for-quothelloquot-but-corrupts-position-tracking-for-every-token-that-follows-it-on-the-same-line-it-is-where-you-learn-that-the-scanner-you-built-in-15-hours-of-careful-work-has-a-subtle-off-by-one-that-only-shows-up-in-multi-line-programs-this-milestone-also-asks-you-to-confront-a-design-decision-you-made-without-realizing-it-what-happens-when-the-scanner-encounters-invalid-input-you-already-emit-error-tokens-for-unrecognized-characters-but-does-scanning-continue-afterward-do-you-collect-all-errors-or-stop-at-the-first-one-these-choices-determine-whether-your-scanner-is-useful-in-real-world-tools-like-ide-syntax-highlighters-or-whether-it-fails-the-moment-a-user-makes-a-typo-by-the-end-of-this-milestone-you-will-have-a-fully-validated-scanner-a-working-error-recovery-strategy-a-test-suite-that-covers-edge-cases-and-complete-programs-and-a-performance-result-that-confirms-your-implementation-meets-production-requirements"},{"level":2,"text":"The Revelation: Unit Tests Are Necessary but Not Sufficient","id":"the-revelation-unit-tests-are-necessary-but-not-sufficient"},{"level":2,"text":"Error Recovery: The Minimal Viable Strategy","id":"error-recovery-the-minimal-viable-strategy"},{"level":3,"text":"Collecting Multiple Errors","id":"collecting-multiple-errors"},{"level":3,"text":"Errors Mixed with Valid Tokens","id":"errors-mixed-with-valid-tokens"},{"level":3,"text":"Extracting Errors from a Token Stream","id":"extracting-errors-from-a-token-stream"},{"level":2,"text":"This is not scanner logic â€” it is consumer logic. Your scanner&#39;s job is to emit all tokens including errors. The consumer (IDE, compiler, linter) decides what to do with them. Keeping the concerns separate is clean API design.","id":"this-is-not-scanner-logic-it-is-consumer-logic-your-scanner39s-job-is-to-emit-all-tokens-including-errors-the-consumer-ide-compiler-linter-decides-what-to-do-with-them-keeping-the-concerns-separate-is-clean-api-design"},{"level":2,"text":"The Token Stream as an Interface Contract","id":"the-token-stream-as-an-interface-contract"},{"level":2,"text":"The Canonical Integration Test: Token-by-Token Verification","id":"the-canonical-integration-test-token-by-token-verification"},{"level":2,"text":"The Multi-Line Integration Test","id":"the-multi-line-integration-test"},{"level":3,"text":"Building the Full Expected Token Stream","id":"building-the-full-expected-token-stream"},{"level":2,"text":"Position Tracking: The Invariant Approach","id":"position-tracking-the-invariant-approach"},{"level":2,"text":"Edge Cases: The Boundary Conditions","id":"edge-cases-the-boundary-conditions"},{"level":3,"text":"Empty Input","id":"empty-input"},{"level":3,"text":"Single Character Input","id":"single-character-input"},{"level":3,"text":"Maximum-Length Identifiers","id":"maximum-length-identifiers"},{"level":3,"text":"Whitespace-Only Input","id":"whitespace-only-input"},{"level":3,"text":"Tokens at End of File Without Trailing Newline","id":"tokens-at-end-of-file-without-trailing-newline"},{"level":3,"text":"Adjacent Tokens Without Whitespace","id":"adjacent-tokens-without-whitespace"},{"level":2,"text":"Performance: The 10,000-Line Benchmark","id":"performance-the-10000-line-benchmark"},{"level":3,"text":"What to Expect","id":"what-to-expect"},{"level":2,"text":"The Complete Integration Test Suite","id":"the-complete-integration-test-suite"},{"level":2,"text":"Run the full suite: python -m pytest test_scanner_integration.py -v","id":"run-the-full-suite-python-m-pytest-test_scanner_integrationpy-v"},{"level":2,"text":"Diagnosing Failures: A Debugging Playbook","id":"diagnosing-failures-a-debugging-playbook"},{"level":3,"text":"Symptom: Wrong token count","id":"symptom-wrong-token-count"},{"level":3,"text":"Symptom: Wrong token type","id":"symptom-wrong-token-type"},{"level":3,"text":"Symptom: Wrong position (line off)","id":"symptom-wrong-position-line-off"},{"level":3,"text":"Symptom: Wrong position (column off)","id":"symptom-wrong-position-column-off"},{"level":2,"text":"Error Recovery: The Philosophy Behind the Mechanism","id":"error-recovery-the-philosophy-behind-the-mechanism"},{"level":2,"text":"The Final Scanner: What You Have Built","id":"the-final-scanner-what-you-have-built"},{"level":2,"text":"Knowledge Cascade: What This Unlocks","id":"knowledge-cascade-what-this-unlocks"},{"level":2,"text":"Summary: What You Have Built Across Four Milestones","id":"summary-what-you-have-built-across-four-milestones"},{"level":1,"text":"TDD","id":"tdd"},{"level":1,"text":"TECHNICAL DESIGN SPECIFICATION: Token Types &amp; Scanner Foundation (tokenizer-m1)","id":"technical-design-specification-token-types-amp-scanner-foundation-tokenizer-m1"},{"level":2,"text":"1. Module Charter","id":"1-module-charter"},{"level":2,"text":"2. File Structure","id":"2-file-structure"},{"level":2,"text":"3. Complete Data Model","id":"3-complete-data-model"},{"level":3,"text":"3.1 TokenType (Enum)","id":"31-tokentype-enum"},{"level":3,"text":"3.2 Token (Dataclass)","id":"32-token-dataclass"},{"level":3,"text":"3.3 Scanner Internal State","id":"33-scanner-internal-state"},{"level":2,"text":"4. Interface Contracts","id":"4-interface-contracts"},{"level":3,"text":"4.1 Primitive Operations","id":"41-primitive-operations"},{"level":3,"text":"4.2 Scanning Operations","id":"42-scanning-operations"},{"level":2,"text":"5. Algorithm Specification: Position-Aware Scanning","id":"5-algorithm-specification-position-aware-scanning"},{"level":3,"text":"5.1 The _skip_whitespace Algorithm","id":"51-the-_skip_whitespace-algorithm"},{"level":3,"text":"5.2 Token Position Invariant Trace","id":"52-token-position-invariant-trace"},{"level":2,"text":"6. Error Handling Matrix","id":"6-error-handling-matrix"},{"level":2,"text":"7. Implementation Sequence with Checkpoints","id":"7-implementation-sequence-with-checkpoints"},{"level":3,"text":"Phase 1: Data Definitions (0.75h)","id":"phase-1-data-definitions-075h"},{"level":3,"text":"Phase 2: Scanner Foundation (0.75h)","id":"phase-2-scanner-foundation-075h"},{"level":3,"text":"Phase 3: Single-Character Dispatch (0.5h)","id":"phase-3-single-character-dispatch-05h"},{"level":3,"text":"Phase 4: Integration Loop (1.0h)","id":"phase-4-integration-loop-10h"},{"level":2,"text":"8. Test Specification","id":"8-test-specification"},{"level":3,"text":"8.1 Unit Tests (pytest format)","id":"81-unit-tests-pytest-format"},{"level":2,"text":"9. Performance Targets","id":"9-performance-targets"},{"level":2,"text":"10. The &quot;Formal Soul&quot; of Milestone 1","id":"10-the-quotformal-soulquot-of-milestone-1"},{"level":1,"text":"TECHNICAL DESIGN SPECIFICATION: Multi-Character Tokens &amp; Maximal Munch (tokenizer-m2)","id":"technical-design-specification-multi-character-tokens-amp-maximal-munch-tokenizer-m2"},{"level":2,"text":"1. Module Charter","id":"1-module-charter"},{"level":2,"text":"2. File Structure","id":"2-file-structure"},{"level":2,"text":"3. Complete Data Model","id":"3-complete-data-model"},{"level":3,"text":"3.1 Updated TokenType (Enum)","id":"31-updated-tokentype-enum"},{"level":3,"text":"3.2 Global Tables","id":"32-global-tables"},{"level":2,"text":"4. Interface Contracts","id":"4-interface-contracts"},{"level":3,"text":"4.1 Lookahead &amp; Conditional Consumption","id":"41-lookahead-amp-conditional-consumption"},{"level":3,"text":"4.2 Scanner Method Signatures","id":"42-scanner-method-signatures"},{"level":2,"text":"5. Algorithm Specification: The Maximal Munch Logic","id":"5-algorithm-specification-the-maximal-munch-logic"},{"level":3,"text":"5.1 Operator Disambiguation Algorithm","id":"51-operator-disambiguation-algorithm"},{"level":3,"text":"5.2 Two-State Float Scanning Algorithm","id":"52-two-state-float-scanning-algorithm"},{"level":3,"text":"5.3 Identifier Scan-then-Lookup Algorithm","id":"53-identifier-scan-then-lookup-algorithm"},{"level":2,"text":"6. Error Handling Matrix","id":"6-error-handling-matrix"},{"level":2,"text":"7. Implementation Sequence with Checkpoints","id":"7-implementation-sequence-with-checkpoints"},{"level":3,"text":"Phase 1: Lookahead Infrastructure (0.5h)","id":"phase-1-lookahead-infrastructure-05h"},{"level":3,"text":"Phase 2: Operator Expansion (0.5h)","id":"phase-2-operator-expansion-05h"},{"level":3,"text":"Phase 3: Numeric Literacy (1.0h)","id":"phase-3-numeric-literacy-10h"},{"level":3,"text":"Phase 4: Name Resolution (0.5h)","id":"phase-4-name-resolution-05h"},{"level":3,"text":"Phase 5: Integration &amp; Acceptance (1.5h)","id":"phase-5-integration-amp-acceptance-15h"},{"level":2,"text":"8. Test Specification","id":"8-test-specification"},{"level":3,"text":"8.1 Operator Tests (Maximal Munch)","id":"81-operator-tests-maximal-munch"},{"level":3,"text":"8.2 Numeric Tests (Float Ambiguity)","id":"82-numeric-tests-float-ambiguity"},{"level":3,"text":"8.3 Identifier/Keyword Tests","id":"83-identifierkeyword-tests"},{"level":2,"text":"9. Performance Targets","id":"9-performance-targets"},{"level":2,"text":"10. Formal &quot;Soul&quot;: Deterministic Finite Automata (DFA)","id":"10-formal-quotsoulquot-deterministic-finite-automata-dfa"},{"level":1,"text":"TECHNICAL DESIGN SPECIFICATION: Strings &amp; Comments (tokenizer-m3)","id":"technical-design-specification-strings-amp-comments-tokenizer-m3"},{"level":2,"text":"1. Module Charter","id":"1-module-charter"},{"level":2,"text":"2. File Structure","id":"2-file-structure"},{"level":2,"text":"3. Complete Data Model","id":"3-complete-data-model"},{"level":3,"text":"3.1 Lexeme Accumulation Strategy","id":"31-lexeme-accumulation-strategy"},{"level":3,"text":"3.2 State Machine Representation","id":"32-state-machine-representation"},{"level":2,"text":"4. Interface Contracts","id":"4-interface-contracts"},{"level":3,"text":"4.1 Sub-Scanner Signatures","id":"41-sub-scanner-signatures"},{"level":2,"text":"5. Algorithm Specification","id":"5-algorithm-specification"},{"level":3,"text":"5.1 The &#39;/&#39; Disambiguation (Maximal Munch)","id":"51-the-3939-disambiguation-maximal-munch"},{"level":3,"text":"5.2 String Scanning with Escape States","id":"52-string-scanning-with-escape-states"},{"level":3,"text":"5.3 Block Comment Skip (Newline Aware)","id":"53-block-comment-skip-newline-aware"},{"level":2,"text":"6. Error Handling Matrix","id":"6-error-handling-matrix"},{"level":2,"text":"7. Implementation Sequence with Checkpoints","id":"7-implementation-sequence-with-checkpoints"},{"level":3,"text":"Phase 1: The Slash Branch (0.5h)","id":"phase-1-the-slash-branch-05h"},{"level":3,"text":"Phase 2: Block Comment Logic (0.75h)","id":"phase-2-block-comment-logic-075h"},{"level":3,"text":"Phase 3: String FSM (1.0h)","id":"phase-3-string-fsm-10h"},{"level":3,"text":"Phase 4: Error Positioning (0.5h)","id":"phase-4-error-positioning-05h"},{"level":2,"text":"8. Test Specification","id":"8-test-specification"},{"level":3,"text":"8.1 String Escape Tests","id":"81-string-escape-tests"},{"level":3,"text":"8.2 Comment Interaction Tests","id":"82-comment-interaction-tests"},{"level":3,"text":"8.3 Block Comment Edge Cases","id":"83-block-comment-edge-cases"},{"level":2,"text":"9. Performance Targets","id":"9-performance-targets"},{"level":2,"text":"10. The &quot;Formal Soul&quot; of Milestone 3","id":"10-the-quotformal-soulquot-of-milestone-3"},{"level":1,"text":"TECHNICAL DESIGN SPECIFICATION: Integration Testing &amp; Error Recovery (tokenizer-m4)","id":"technical-design-specification-integration-testing-amp-error-recovery-tokenizer-m4"},{"level":2,"text":"1. Module Charter","id":"1-module-charter"},{"level":2,"text":"2. File Structure","id":"2-file-structure"},{"level":2,"text":"3. Complete Data Model","id":"3-complete-data-model"},{"level":3,"text":"3.1 Token (Refined Definition)","id":"31-token-refined-definition"},{"level":3,"text":"3.2 Test Metadata Structures","id":"32-test-metadata-structures"},{"level":2,"text":"4. Interface Contracts","id":"4-interface-contracts"},{"level":3,"text":"4.1 Utility Operations (utils.py)","id":"41-utility-operations-utilspy"},{"level":3,"text":"4.2 Scanner Recovery Invariant","id":"42-scanner-recovery-invariant"},{"level":2,"text":"5. Algorithm Specification","id":"5-algorithm-specification"},{"level":3,"text":"5.1 Invariant Checking Algorithm","id":"51-invariant-checking-algorithm"},{"level":3,"text":"5.2 Performance Benchmarking Procedure","id":"52-performance-benchmarking-procedure"},{"level":2,"text":"6. Error Handling Matrix","id":"6-error-handling-matrix"},{"level":2,"text":"7. Implementation Sequence with Checkpoints","id":"7-implementation-sequence-with-checkpoints"},{"level":3,"text":"Phase 1: Canonical Verification (0.5h)","id":"phase-1-canonical-verification-05h"},{"level":3,"text":"Phase 2: Error Recovery Hardening (0.5h)","id":"phase-2-error-recovery-hardening-05h"},{"level":3,"text":"Phase 3: Position Invariant Suite (1.0h)","id":"phase-3-position-invariant-suite-10h"},{"level":3,"text":"Phase 4: Boundary &amp; Edge Cases (0.75h)","id":"phase-4-boundary-amp-edge-cases-075h"},{"level":3,"text":"Phase 5: Performance &amp; Scaling (0.75h)","id":"phase-5-performance-amp-scaling-075h"},{"level":2,"text":"8. Test Specification","id":"8-test-specification"},{"level":3,"text":"8.1 Integration Tests","id":"81-integration-tests"},{"level":3,"text":"8.2 Boundary Tests","id":"82-boundary-tests"},{"level":3,"text":"8.3 Error Tests","id":"83-error-tests"},{"level":2,"text":"9. Performance Targets","id":"9-performance-targets"},{"level":2,"text":"10. The &quot;Formal Soul&quot; of Integration","id":"10-the-quotformal-soulquot-of-integration"},{"level":1,"text":"Project Structure: Tokenizer / Lexer","id":"project-structure-tokenizer-lexer"},{"level":2,"text":"Directory Tree","id":"directory-tree"},{"level":2,"text":"Creation Order","id":"creation-order"},{"level":2,"text":"File Count Summary","id":"file-count-summary"},{"level":1,"text":"ðŸ“š Beyond the Atlas: Further Reading","id":"-beyond-the-atlas-further-reading"},{"level":2,"text":"Automata Theory &amp; Formal Foundations","id":"automata-theory-amp-formal-foundations"},{"level":2,"text":"Real-World Lexer Implementations","id":"real-world-lexer-implementations"},{"level":2,"text":"String Literals &amp; Escape Sequences","id":"string-literals-amp-escape-sequences"},{"level":2,"text":"Error Recovery &amp; Diagnostic UX","id":"error-recovery-amp-diagnostic-ux"},{"level":2,"text":"Performance &amp; Benchmarking","id":"performance-amp-benchmarking"}],"title":"ðŸŽ¯ Project Charter: C-Like Language Tokenizer","markdown":"# ðŸŽ¯ Project Charter: C-Like Language Tokenizer\n\n## What You Are Building\nYou are building a character-level lexical scanner (lexer) that transforms raw source text into a structured stream of categorized tokens. This is a single-pass, high-performance engine that implements a Deterministic Finite Automaton (DFA) to recognize identifiers, keywords, multi-character operators, number literals, and strings, while precisely tracking source positions for error reporting.\n\n## Why This Project Exists\nMost developers use language tools daily but treat the translation of text into meaning as a \"black box.\" Building a tokenizer from scratch is the only way to truly understand the Maximal Munch principle, the mechanics of lookahead, and how compilers maintain $O(n)$ performance while handling complex context-sensitive features like multi-line comments and escape sequences.\n\n## What You Will Be Able to Do When Done\n- **Implement an FSM:** Build a character-by-character scanner using a state-based cursor model.\n- **Apply Maximal Munch:** Resolve lexical ambiguity in operators (e.g., distinguishing `=` from `==`) using greedy consumption logic.\n- **Handle Context-Sensitivity:** Switch scanning \"modes\" to handle string literals and multi-line comments correctly.\n- **Implement Error Recovery:** Design a scanner that doesn't crash on typos but instead emits diagnostic tokens and resumes operation.\n- **Track Source Metadata:** Generate precise line and column data required for IDE squiggles and compiler error messages.\n\n## Final Deliverable\nA production-ready `Scanner` class (approx. 400-600 lines of code) that accepts a source string and returns a list of `Token` objects. It will include a comprehensive integration test suite and a performance benchmark capable of processing a 10,000-line C-like program in under one second.\n\n## Is This Project For You?\n**You should start this if you:**\n- Have a solid handle on basic string manipulation (indexing, slicing, concatenation).\n- Are comfortable with control flow structures (while-loops, if-else chains).\n- Want to understand the \"front-end\" of compilers and interpreters.\n\n**Come back after you've learned:**\n- Basic Object-Oriented Programming (defining classes and methods).\n- How to use Enumerations (Enums) and Dictionaries (Hash Maps) in your chosen language.\n\n## Estimated Effort\n| Phase | Time |\n|-------|------|\n| Token Types & Scanner Foundation | ~2.5 hours |\n| Multi-Character Tokens & Maximal Munch | ~3.5 hours |\n| Strings & Comments Sub-States | ~2.5 hours |\n| Integration Testing & Error Recovery | ~2.5 hours |\n| **Total** | **~11 hours** |\n\n## Definition of Done\nThe project is complete when:\n- The `scan_tokens()` method produces a deterministic list of tokens ending in a sentinel `EOF` for any valid input.\n- The \"Canonical Statement\" test (`if (x >= 42) { return \"ok\"; }`) produces 12 tokens with exact line/column matches.\n- Multi-line block comments (`/* ... */`) are stripped entirely while maintaining correct line counts for subsequent tokens.\n- The scanner successfully processes a 10,000-line generated source file in less than 1.0 second on standard hardware.\n- Every unrecognized character in a file produces a `TokenType.ERROR` without halting the scanning of subsequent valid tokens.\n\n---\n\n# Tokenizer / Lexer: Building a Character-Level Scanner for a C-like Language\n\nThis project builds a complete lexer from scratch â€” the first phase of any compiler or interpreter pipeline. You will implement a character-by-character finite state machine that transforms raw source text into a structured stream of categorized tokens. The tokenizer handles single and multi-character operators, number and string literals, identifiers, keywords, comments, escape sequences, and error recovery â€” all while tracking precise source positions for downstream error reporting.\n\nBy the end, you will have internalized how programming languages are read at the lowest level: not as words or lines, but as individual characters consumed one at a time through a state machine that makes greedy decisions (maximal munch) with minimal lookahead. This understanding cascades into parsers, compilers, language servers, syntax highlighters, and any tool that must understand source code structurally.\n\nThe project is structured as four milestones that progressively layer complexity: foundation (tokens, scanner, single-char), multi-character recognition (operators, numbers, identifiers, keywords), strings and comments (escape sequences, nested state), and finally integration testing with error recovery.\n\n\n\n<!-- MS_ID: tokenizer-m1 -->\n# Milestone 1: Token Types & Scanner Foundation\n## Where You Are in the Pipeline\n\n![Tokenizer System Satellite Map â€” The Complete Pipeline](./diagrams/diag-satellite-map.svg)\n\nBefore a compiler can parse expressions, type-check variables, or generate machine code, it needs to answer one deceptively simple question: *what is this source file made of?* Raw source text is just a sequence of characters â€” bytes with no inherent meaning. Your tokenizer is the component that transforms that flat stream of characters into a structured sequence of meaningful units called **tokens**.\nThink of it like reading a sentence in a human language. When you read \"the cat sat\", your brain does not process individual letters â€” it automatically groups letters into words and assigns each word a grammatical role (article, noun, verb). The tokenizer does exactly this for source code: it groups characters into lexemes and assigns each lexeme a type.\nIn this milestone, you will build the foundation that all subsequent work rests on:\n1. A **token type enumeration** â€” the vocabulary of your language\n2. A **Token data structure** â€” the carrier of information about each recognized unit\n3. A **Scanner class** â€” the character-consuming engine with its two primitive operations: `advance()` and `peek()`\nBy the end of this milestone, your scanner will successfully process single-character tokens, swallow whitespace silently, and emit sentinel tokens at the boundaries of valid and invalid input.\n---\n## The Revelation: Tokenizers Are Not Split()\nHere is what most developers assume when they first think about tokenizers:\n```python\n# What people think tokenizers do (DON'T do this)\ndef tokenize_naive(source):\n    return source.split()\n```\nMaybe with some regex sprinkled on top:\n```python\nimport re\n# Still wrong\ntokens = re.findall(r'\\w+|[^\\w\\s]', source)\n```\nThis feels reasonable. After all, Python's `split()` does break text into words, and regex can match patterns. The misconception is understandable.\n**Here is where this model breaks.** Consider the input:\n```\nx >= 42\n```\nA split-based approach gives you `['x', '>=', '42']`. That looks fine. Now consider:\n```\nx>=42\n```\nSame semantics â€” no spaces. `split()` gives you `['x>=42']`. One token. That is completely wrong.\nNow consider escape sequences inside strings:\n```\n\"hello\\nworld\"\n```\nNo regex operating on a substring can correctly distinguish whether the `\\n` is an escape sequence inside a string or the literal characters `\\` and `n`. To know that, you need to know *where you are* â€” are you inside a string literal right now? Regex has no memory of what it scanned two characters ago.\n**The real model**: A tokenizer is a **finite state machine** (FSM) that reads one character at a time, maintains a current *state*, and makes local decisions â€” emit a token, change state, advance the cursor â€” based only on the current character and the immediately next character (one character of lookahead). It never looks at the \"whole string.\" The cursor moves forward, one character at a time, and never goes back.\nThis is why tokenizers are O(n) with constant memory: every character is visited exactly once. It is also why they are fast enough to process millions of lines per second in production compilers.\n\n> **ðŸ”‘ Foundation: Finite State Machines as applied to tokenization: states**\n> \n> ### 1. What it IS\nA **Finite State Machine (FSM)** is a mathematical model used to design logic that moves through a sequence of \"states\" based on input. In tokenization, an FSM treats a stream of characters as input and determines where one \"token\" (like a keyword, variable name, or operator) ends and the next begins.\n\nIt consists of three core components:\n*   **States**: The \"modes\" the tokenizer can be in (e.g., `START`, `READING_NUMBER`, `READING_STRING`).\n*   **Transitions**: Rules that move the machine from one state to another based on the current character (e.g., \"If in `START` and see a digit, move to `READING_NUMBER`\").\n*   **Accepting (Final) States**: Specific states that signify a valid token has been successfully identified (e.g., \"We hit a space after some digits; the current buffer is a valid `INTEGER`\").\n\n### 2. WHY you need it right now\nAs you move from simple string splitting to building a robust lexer, manual `if-else` or `switch` statements become \"spaghetti code\" that is difficult to debug. For example, distinguishing between a decimal `10.5`, an integer `10`, and a range `10..20` requires looking ahead or keeping track of what you've already seen. \n\nAn FSM provides a formal structure to manage this complexity. It ensures that your tokenizer is **predictable** and **exhaustive**, meaning it handles every possible character sequence without falling into ambiguous logic traps.\n\n### 3. Key Insight: \"The State is your Memory\"\nThe most important thing to remember is that **the current state represents everything the tokenizer knows about the past.** \n\nYou donâ€™t need to look back at the last five characters to know if you are inside a comment; the fact that you are currently in the `IN_COMMENT` state already tells you everything you need to know. When you receive a new character, you only need two pieces of information to decide what to do next: **\"What state am I in?\"** and **\"What is this character?\"**\n\n---\n## Designing the Token Type Enumeration\n\n![Token Type Enumeration â€” Complete Category Map](./diagrams/diag-m1-token-enum.svg)\n\nThe first thing you need is a vocabulary â€” a fixed set of categories into which every lexeme in your C-like language will be classified. In Python, this is a natural fit for `enum.Enum`.\nEvery token in your language falls into one of these categories:\n| Category | What it represents | Examples |\n|---|---|---|\n| `NUMBER` | Integer or floating-point literals | `42`, `3.14`, `0` |\n| `STRING` | String literals (quoted) | `\"hello\"`, `\"x\\n\"` |\n| `IDENTIFIER` | Variable/function names | `x`, `myVar`, `_count` |\n| `KEYWORD` | Reserved words | `if`, `while`, `return` |\n| `OPERATOR` | Arithmetic and comparison operators | `+`, `-`, `>=`, `==` |\n| `PUNCTUATION` | Structural delimiters | `(`, `)`, `{`, `;`, `,` |\n| `EOF` | End of input â€” the sentinel | (no lexeme) |\n| `ERROR` | Unrecognized character | `@`, `#`, `$` |\nNotice that `OPERATOR` and `PUNCTUATION` are both \"symbols.\" You might wonder: why separate them? Convention and downstream utility. An operator participates in expressions and has associativity and precedence. A punctuation mark is structural glue â€” a semicolon terminates a statement, a comma separates arguments. When your parser eventually consumes this token stream, it will care about this distinction. Design your token types for the consumer (the parser), not just for the producer (the scanner).\nHere is the complete enumeration:\n```python\nfrom enum import Enum, auto\nclass TokenType(Enum):\n    # Literals\n    NUMBER     = auto()\n    STRING     = auto()\n    # Names\n    IDENTIFIER = auto()\n    KEYWORD    = auto()\n    # Symbols â€” operators (participate in expressions)\n    PLUS       = auto()   # +\n    MINUS      = auto()   # -\n    STAR       = auto()   # *\n    SLASH      = auto()   # /\n    ASSIGN     = auto()   # =\n    EQUAL      = auto()   # ==\n    NOT_EQUAL  = auto()   # !=\n    LESS       = auto()   # <\n    LESS_EQ    = auto()   # <=\n    GREATER    = auto()   # >\n    GREATER_EQ = auto()   # >=\n    BANG       = auto()   # !\n    # Symbols â€” punctuation (structural)\n    LPAREN     = auto()   # (\n    RPAREN     = auto()   # )\n    LBRACE     = auto()   # {\n    RBRACE     = auto()   # }\n    LBRACKET   = auto()   # [\n    RBRACKET   = auto()   # ]\n    SEMICOLON  = auto()   # ;\n    COMMA      = auto()   # ,\n    # Sentinel / diagnostic\n    EOF        = auto()\n    ERROR      = auto()\n```\n> **Design note:** `auto()` assigns sequential integer values automatically. You never need to remember which integer corresponds to which token â€” you always compare by name (`TokenType.PLUS`, not `5`). This is the correct way to use enums in Python: names over values.\nYou will notice that operators are *individually named* (`PLUS`, `MINUS`, `SLASH`) rather than using a generic `OPERATOR` type with a value string. This is intentional. A parser matching a `+` node does not want to check `token.type == TokenType.OPERATOR and token.lexeme == \"+\"`. It wants to check `token.type == TokenType.PLUS`. Be specific. Your parser will thank you.\n---\n## The Token Data Structure\n\n![Token Data Structure Layout](./diagrams/diag-m1-token-struct.svg)\n\nA token is not just a type. It carries four pieces of information:\n1. **type** â€” which category this token belongs to (`TokenType.PLUS`, `TokenType.KEYWORD`, etc.)\n2. **lexeme** â€” the exact raw text from the source that this token was scanned from (`\"+\"`, `\"while\"`, `\"3.14\"`)\n3. **line** â€” the line number in the source file where this token starts (1-indexed)\n4. **column** â€” the column number in the source file where this token starts (1-indexed)\nThe lexeme is the raw text â€” for a string literal `\"hello\"`, the lexeme includes the quote characters: `'\"hello\"'`. For an integer `42`, the lexeme is `\"42\"`. This matters because when you report an error, you want to show the user exactly what they wrote.\nLine and column are metadata that the tokenizer produces and the parser (and error reporter) consume. Without them, an error message can only say \"invalid syntax\" â€” with them, it says \"invalid syntax at line 12, column 8.\" This is the difference between a usable and an unusable language tool.\n```python\nfrom dataclasses import dataclass\n@dataclass\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\n```\n`@dataclass` gives you `__init__` and `__eq__` for free â€” you can construct a token with `Token(TokenType.PLUS, \"+\", 1, 5)` and compare tokens in tests with `==`. The custom `__repr__` makes debugging output readable.\nA few token constructions you will use often enough to make helper functions:\n```python\ndef make_eof(line: int, column: int) -> Token:\n    return Token(TokenType.EOF, \"\", line, column)\ndef make_error(char: str, line: int, column: int) -> Token:\n    return Token(TokenType.ERROR, char, line, column)\n```\nThe EOF token has an empty lexeme â€” there is no source text that corresponds to end-of-file. The Error token's lexeme is the offending character.\n---\n## The Scanner Class: Architecture First\n\n![Scanner Anatomy: The Character Cursor Machine](./diagrams/diag-m1-scanner-anatomy.svg)\n\nThe scanner holds:\n- The **source string** â€” the complete source text, indexed as a Python string\n- A **current position** cursor â€” an integer index into the source string\n- **Line** and **column** counters â€” updated as the cursor advances\nHere is the skeleton:\n```python\nclass Scanner:\n    def __init__(self, source: str) -> None:\n        self.source: str = source\n        self.current: int = 0   # index of next character to consume\n        self.line: int = 1      # current line number (1-indexed)\n        self.column: int = 1    # current column number (1-indexed)\n```\nNotice `current` is the index of the *next* character to consume â€” it has not been consumed yet. When the scanner starts, `current = 0` points at the first character in the source.\n### is_at_end()\n```python\ndef is_at_end(self) -> bool:\n    return self.current >= len(self.source)\n```\nThis is the simplest guard. Every loop and conditional in the scanner begins by checking `is_at_end()` before touching `self.source[self.current]` â€” accessing past the end of a Python string raises `IndexError`.\n### peek()\n```python\ndef peek(self) -> str:\n    if self.is_at_end():\n        return \"\\0\"   # null character as \"no character\" sentinel\n    return self.source[self.current]\n```\n`peek()` reads the character at `self.current` without moving the cursor. Call it ten times in a row â€” you get the same character. This is your window into the future: before you decide what to do, you can look at the next character without committing to consuming it.\nThe return value `\"\\0\"` (the null character, ASCII 0) when at end is a convention borrowed from C tokenizers. It is a value that will never appear in real source code, so any comparison like `peek() == '='` will safely return `False` at end-of-input without needing a separate `if is_at_end()` guard in every caller. This is the **sentinel value pattern** â€” returning a special out-of-band value to signal a boundary condition.\n### advance()\n```python\ndef advance(self) -> str:\n    char = self.source[self.current]\n    self.current += 1\n    if char == \"\\n\":\n        self.line += 1\n        self.column = 1\n    else:\n        self.column += 1\n    return char\n```\n`advance()` is the consuming operation. It reads the current character, moves the cursor forward by one, updates the position counters, and returns the character. After `advance()`, the cursor points at the character *after* the one just consumed.\n> **Critical detail about `\\r\\n`:** Windows line endings are the two-character sequence `\\r` followed by `\\n`. If you naively count every `\\n` as a new line, you will double-count Windows newlines. The standard approach: treat `\\r` as whitespace that does NOT increment the line counter (only `\\n` does). The `\\r\\n` pair will then correctly increment the line counter exactly once (when `\\n` is consumed). This is already handled by the code above â€” `\\r` falls through to the `else` branch and increments `column`, which is harmless.\n> **About tab width:** The code above increments `column` by 1 for every character, including tab (`\\t`). This is the standard approach in most modern compilers (including LLVM's Clang). An alternative is to advance to the next tab stop (column rounded up to the nearest multiple of 8 or 4), but this makes position reporting dependent on editor settings and creates confusing mismatch between what the user sees and what the compiler reports. Consistent 1-per-character is simpler and more predictable.\n\n![Trace: advance() and peek() in Action](./diagrams/diag-m1-advance-peek-trace.svg)\n\n### A Trace Through advance() and peek()\nSuppose your source is `\"x+1\"`. Here is the scanner state at each step:\n```\nInitial:  current=0, line=1, col=1\nsource:   x + 1\n          ^\n          current\npeek()    â†’ 'x'      (current stays at 0)\nadvance() â†’ 'x'      current=1, col=2\npeek()    â†’ '+'      (current stays at 1)\nadvance() â†’ '+'      current=2, col=3\npeek()    â†’ '1'      (current stays at 2)\nadvance() â†’ '1'      current=3, col=4\nis_at_end() â†’ True\npeek()    â†’ '\\0'\n```\nEvery character is visited exactly once. `peek()` is free â€” no movement. `advance()` is permanent â€” no going back.\n---\n## Position Tracking in Depth\n\n![Line and Column Tracking Through Newlines and Tabs](./diagrams/diag-m1-position-tracking.svg)\n\nPosition tracking has one subtle complication: the `line` and `column` you record in a token should be the position where the token *starts*, not where it ends. By the time you finish scanning a multi-character token like `while` or `42.5`, your cursor is past the end of the lexeme. So you need to capture the start position *before* you begin scanning.\nThe pattern you will use throughout this project:\n```python\ndef next_token(self) -> Token:\n    # Capture position BEFORE advancing\n    tok_line = self.line\n    tok_col = self.column\n    if self.is_at_end():\n        return Token(TokenType.EOF, \"\", tok_line, tok_col)\n    char = self.advance()\n    # ... dispatch on char ...\n```\nBy recording `tok_line` and `tok_col` from `self.line` / `self.column` *before* calling `advance()`, you capture the position of the first character of the token. All subsequent `advance()` calls within the same token scan will update `self.line` and `self.column`, but the token you emit will carry the *start* position.\nThis pattern will be critical in Milestone 3 when you scan string literals that span multiple lines â€” the token's reported position should be the opening quote, not the closing one.\n---\n## The Finite State Machine View\n\n![Scanner as Finite State Machine â€” Basic States](./diagrams/diag-m1-fsm-basic.svg)\n\nYour scanner's `scan_tokens()` method is a finite state machine. The *states* correspond to what the scanner is currently \"inside\":\n- **START** â€” at the beginning of a new token, deciding what to scan\n- **IN_NUMBER** â€” currently consuming digit characters\n- **IN_IDENTIFIER** â€” currently consuming alphanumeric characters\n- **IN_STRING** â€” currently consuming characters inside double quotes\n- **IN_COMMENT** â€” skipping characters until comment ends\nIn this milestone, you only implement the START state (dispatching on the first character of each token) and the simple case where a single character completely determines the token. Multi-character states will come in Milestones 2 and 3.\nThe dispatch logic is a match (or if/elif chain) on the character returned by `advance()`:\n```python\n# Single-character tokens â€” one character, one decision, one token\nSINGLE_CHAR_TOKENS = {\n    \"+\": TokenType.PLUS,\n    \"-\": TokenType.MINUS,\n    \"*\": TokenType.STAR,\n    \"/\": TokenType.SLASH,   # '/' alone is division; '//' and '/*' handled in M3\n    \"(\": TokenType.LPAREN,\n    \")\": TokenType.RPAREN,\n    \"{\": TokenType.LBRACE,\n    \"}\": TokenType.RBRACE,\n    \"[\": TokenType.LBRACKET,\n    \"]\": TokenType.RBRACKET,\n    \";\": TokenType.SEMICOLON,\n    \",\": TokenType.COMMA,\n}\n```\nUsing a dictionary for single-character dispatch is both idiomatic Python and faster than a long `if/elif` chain â€” a dictionary lookup is O(1) hash table access.\n---\n## Putting It Together: scan_tokens() and next_token()\nThe top-level API of your scanner produces a complete list of tokens from the source:\n```python\ndef scan_tokens(self) -> list[Token]:\n    tokens: list[Token] = []\n    while True:\n        token = self.next_token()\n        tokens.append(token)\n        if token.type == TokenType.EOF:\n            break\n    return tokens\n```\nThe loop is driven by `next_token()`, which produces one token per call. It terminates when `next_token()` returns an EOF token. Importantly, the EOF token is appended to the list before the loop exits â€” the parser downstream will expect to find `EOF` as the last element and will use it as its own termination signal. Forgetting the EOF token is a classic first-timer mistake that causes downstream parsers to crash with an `IndexError`.\nNow the complete `next_token()` for this milestone:\n```python\ndef next_token(self) -> Token:\n    # Skip whitespace â€” consume without emitting\n    self._skip_whitespace()\n    # Capture the start position of this token\n    tok_line = self.line\n    tok_col = self.column\n    if self.is_at_end():\n        return Token(TokenType.EOF, \"\", tok_line, tok_col)\n    char = self.advance()\n    # Single-character tokens via lookup table\n    if char in SINGLE_CHAR_TOKENS:\n        return Token(SINGLE_CHAR_TOKENS[char], char, tok_line, tok_col)\n    # Unrecognized character â€” emit Error token, continue scanning\n    return Token(TokenType.ERROR, char, tok_line, tok_col)\n```\nAnd the whitespace consumer:\n```python\ndef _skip_whitespace(self) -> None:\n    while not self.is_at_end() and peek() in (\" \", \"\\t\", \"\\r\", \"\\n\"):\n        self.advance()\n```\nWait â€” there is a subtle issue above. Inside `_skip_whitespace`, you should call `self.peek()`, not a standalone `peek()`. Let us write the complete, correct version:\n```python\ndef _skip_whitespace(self) -> None:\n    while not self.is_at_end() and self.peek() in (\" \", \"\\t\", \"\\r\", \"\\n\"):\n        self.advance()\n```\nWhy does `_skip_whitespace` call `advance()` rather than simply incrementing `self.current`? Because `advance()` handles position tracking. If you increment `self.current` directly, you bypass the newline detection and your line counter falls out of sync.\n> **Position capture timing:** Notice that `_skip_whitespace()` is called *before* capturing `tok_line` and `tok_col`. This is intentional â€” you skip past any leading whitespace, *then* record where the actual token starts. If you captured position first, then skipped whitespace, the token's reported position would point at the space rather than the token itself. Always skip whitespace before snapping the position.\n---\n## The Full Scanner: Milestone 1 Complete Implementation\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Optional\nclass TokenType(Enum):\n    # Literals\n    NUMBER     = auto()\n    STRING     = auto()\n    # Names\n    IDENTIFIER = auto()\n    KEYWORD    = auto()\n    # Operators\n    PLUS       = auto()\n    MINUS      = auto()\n    STAR       = auto()\n    SLASH      = auto()\n    ASSIGN     = auto()\n    EQUAL      = auto()\n    NOT_EQUAL  = auto()\n    LESS       = auto()\n    LESS_EQ    = auto()\n    GREATER    = auto()\n    GREATER_EQ = auto()\n    BANG       = auto()\n    # Punctuation\n    LPAREN     = auto()\n    RPAREN     = auto()\n    LBRACE     = auto()\n    RBRACE     = auto()\n    LBRACKET   = auto()\n    RBRACKET   = auto()\n    SEMICOLON  = auto()\n    COMMA      = auto()\n    # Sentinel / diagnostic\n    EOF        = auto()\n    ERROR      = auto()\n@dataclass\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\nSINGLE_CHAR_TOKENS: dict[str, TokenType] = {\n    \"+\": TokenType.PLUS,\n    \"-\": TokenType.MINUS,\n    \"*\": TokenType.STAR,\n    \"/\": TokenType.SLASH,\n    \"(\": TokenType.LPAREN,\n    \")\": TokenType.RPAREN,\n    \"{\": TokenType.LBRACE,\n    \"}\": TokenType.RBRACE,\n    \"[\": TokenType.LBRACKET,\n    \"]\": TokenType.RBRACKET,\n    \";\": TokenType.SEMICOLON,\n    \",\": TokenType.COMMA,\n}\nWHITESPACE: frozenset[str] = frozenset({\" \", \"\\t\", \"\\r\", \"\\n\"})\nclass Scanner:\n    def __init__(self, source: str) -> None:\n        self.source: str = source\n        self.current: int = 0\n        self.line: int = 1\n        self.column: int = 1\n    # â”€â”€ Primitive Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def is_at_end(self) -> bool:\n        \"\"\"True when the cursor has reached or passed the end of source.\"\"\"\n        return self.current >= len(self.source)\n    def peek(self) -> str:\n        \"\"\"Return the current character without advancing. Returns '\\\\0' at end.\"\"\"\n        if self.is_at_end():\n            return \"\\0\"\n        return self.source[self.current]\n    def advance(self) -> str:\n        \"\"\"\n        Consume and return the current character.\n        Updates line and column counters.\n        \"\"\"\n        char = self.source[self.current]\n        self.current += 1\n        if char == \"\\n\":\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n        return char\n    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _skip_whitespace(self) -> None:\n        \"\"\"Consume whitespace characters without emitting tokens.\"\"\"\n        while not self.is_at_end() and self.peek() in WHITESPACE:\n            self.advance()\n    # â”€â”€ Token Production â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def next_token(self) -> Token:\n        \"\"\"Scan and return the next token from the source.\"\"\"\n        self._skip_whitespace()\n        tok_line = self.line\n        tok_col = self.column\n        if self.is_at_end():\n            return Token(TokenType.EOF, \"\", tok_line, tok_col)\n        char = self.advance()\n        if char in SINGLE_CHAR_TOKENS:\n            return Token(SINGLE_CHAR_TOKENS[char], char, tok_line, tok_col)\n        # Unrecognized character: emit Error, continue (error recovery)\n        return Token(TokenType.ERROR, char, tok_line, tok_col)\n    def scan_tokens(self) -> list[Token]:\n        \"\"\"\n        Scan the entire source and return a list of tokens.\n        Always ends with an EOF token.\n        \"\"\"\n        tokens: list[Token] = []\n        while True:\n            token = self.next_token()\n            tokens.append(token)\n            if token.type == TokenType.EOF:\n                break\n        return tokens\n```\n---\n## Testing Your Foundation\nGood tests for a tokenizer are not \"does it return *something*?\" â€” they are \"does it return the *exact* token stream, with correct types, lexemes, lines, and columns?\" Every test should pin down the complete token, not just its type.\n```python\ndef test_single_char_tokens():\n    scanner = Scanner(\"+ - * /\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.PLUS,      \"+\", 1, 1)\n    assert tokens[1] == Token(TokenType.MINUS,     \"-\", 1, 3)\n    assert tokens[2] == Token(TokenType.STAR,      \"*\", 1, 5)\n    assert tokens[3] == Token(TokenType.SLASH,     \"/\", 1, 7)\n    assert tokens[4] == Token(TokenType.EOF,       \"\",  1, 8)\n    assert len(tokens) == 5\ndef test_whitespace_is_not_emitted():\n    scanner = Scanner(\"   \\t\\t  +\")\n    tokens = scanner.scan_tokens()\n    # Only PLUS and EOF â€” whitespace produces nothing\n    assert len(tokens) == 2\n    assert tokens[0].type == TokenType.PLUS\n    assert tokens[0].column == 8   # 7 whitespace chars before, so col 8\ndef test_newline_resets_column():\n    scanner = Scanner(\"{\\n}\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.LBRACE,  \"{\", 1, 1)\n    assert tokens[1] == Token(TokenType.RBRACE,  \"}\", 2, 1)   # line 2, col 1\n    assert tokens[2] == Token(TokenType.EOF,     \"\",  2, 2)\ndef test_eof_on_empty_input():\n    scanner = Scanner(\"\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0] == Token(TokenType.EOF, \"\", 1, 1)\ndef test_error_token_for_invalid_char():\n    scanner = Scanner(\"@\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.ERROR, \"@\", 1, 1)\n    assert tokens[1].type == TokenType.EOF\ndef test_error_recovery_continues():\n    # After an error token, scanning continues normally\n    scanner = Scanner(\"@+\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR   # '@' is invalid\n    assert tokens[1].type == TokenType.PLUS    # '+' is still recognized\n    assert tokens[2].type == TokenType.EOF\ndef test_all_punctuation():\n    scanner = Scanner(\"(){}[];,\")\n    tokens = scanner.scan_tokens()\n    expected_types = [\n        TokenType.LPAREN, TokenType.RPAREN,\n        TokenType.LBRACE, TokenType.RBRACE,\n        TokenType.LBRACKET, TokenType.RBRACKET,\n        TokenType.SEMICOLON, TokenType.COMMA,\n        TokenType.EOF,\n    ]\n    assert [t.type for t in tokens] == expected_types\ndef test_multiline_position_tracking():\n    source = \"+\\n+\\n+\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.PLUS, \"+\", 1, 1)\n    assert tokens[1] == Token(TokenType.PLUS, \"+\", 2, 1)\n    assert tokens[2] == Token(TokenType.PLUS, \"+\", 3, 1)\ndef test_windows_line_endings():\n    # \\r\\n should count as ONE newline, not two\n    scanner = Scanner(\"+\\r\\n+\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].line == 1\n    assert tokens[1].line == 2   # one newline, not two\n```\nRun all tests with `python -m pytest test_scanner.py -v`. Every test should pass green before you proceed to Milestone 2.\n---\n## Design Decisions: Why This Architecture?\n### Why a class rather than a function with a loop?\nThe scanner maintains mutable state (`current`, `line`, `column`) that must persist across calls to `next_token()`. A class is the natural Python idiom for encapsulating mutable state that evolves over time. An alternative would be to pass a mutable dictionary around, but that is less readable and less type-safe.\n### Why store the entire source string rather than reading character by character?\n| Option | Pros | Cons | Used by |\n|---|---|---|---|\n| **Store full source in memory** âœ“ | Simple indexing, peek is trivial, restartable | Source must fit in RAM | CPython, Go compiler, LLVM Clang |\n| Stream (file handle, generator) | Handles huge files, lower memory | Peek requires buffering; no restart | Streaming compilers for very large codebases |\nFor source files up to tens of megabytes (which is essentially all real-world source code), storing the full string in memory is correct. The performance requirement for this project (10,000-line file in under 1 second) is trivially met by either approach in Python.\n### Why `\"\\0\"` as the sentinel from peek()?\nAlternatives include `None` (Pythonic but requires `Optional[str]` type annotation and `None` checks everywhere), raising an exception (verbose), or returning `\"\"` (empty string comparisons work but are confusing). The `\"\\0\"` convention comes from C tokenizers (Crafting Interpreters, GCC, Clang) and has one decisive advantage: all character comparisons like `peek() == \"=\"` and `peek() in \"abc\"` work correctly and safely at end-of-input â€” `\"\\0\"` will never match any valid source character.\n---\n## The Three-Level View of Your Scanner\nLet us look at what your scanner does from three levels:\n**Level 1 â€” Source Language (programmer's view):**\n```\nif (x >= 42) { return true; }\n```\nA human sees keywords, variable names, operators, and braces.\n**Level 2 â€” Scanner Internal (your scanner's view):**\n```\ncursor=0, line=1, col=1\nread 'i' â†’ start identifier scan...\nread 'f' â†’ continue identifier...\nend of alphanumerics â†’ lexeme = \"if\" â†’ lookup table â†’ KEYWORD\nemit Token(KEYWORD, \"if\", 1, 1)\nread ' ' â†’ whitespace, skip\nread '(' â†’ LPAREN\nemit Token(LPAREN, \"(\", 1, 4)\n...\n```\nThe scanner sees a sequence of characters, a cursor, and dispatch decisions.\n**Level 3 â€” Python Runtime (what actually executes):**\n```\nString indexing: source[self.current]  â†’  O(1) pointer arithmetic\ndict lookup: char in SINGLE_CHAR_TOKENS  â†’  O(1) hash table probe\ndataclass __init__: Token(...)  â†’  __new__ + field assignment\nlist.append(token)  â†’  amortized O(1)\n```\nPython strings are immutable sequences stored in contiguous memory. `source[i]` is a direct offset calculation â€” not iteration. This is why character-level scanning in Python is fast enough to meet the 10,000-line benchmark.\n---\n## Common Pitfalls\n**1. Capturing position after advance instead of before**\n```python\n# WRONG: tok_col is already 1 ahead of where the token started\nchar = self.advance()\ntok_col = self.column   # off by one!\n```\nAlways capture `tok_line` and `tok_col` before calling `advance()`.\n**2. Forgetting the EOF token**\nIf `scan_tokens()` does not append the EOF token, the parser will eventually call `tokens[i]` on an out-of-bounds index and crash with `IndexError`. The EOF token is both a termination signal and a guard against the parser running off the end of the token list.\n**3. Skipping whitespace but forgetting to update column**\nIf you manually increment `self.current` to skip whitespace (bypassing `advance()`), newlines will not be detected and `self.line` will stay at 1 forever. Always use `self.advance()` â€” even for whitespace.\n**4. Windows line endings counting as two newlines**\nThe sequence `\\r\\n` should produce exactly one line increment. Since only `\\n` increments `self.line` in the code above, and `\\r` is consumed as whitespace (column increments by 1, then resets when `\\n` follows), this is handled correctly â€” but only if `\\r` is in your whitespace set. Verify this case with a test.\n**5. Column not resetting to 1 â€” resetting to 0**\nAfter a newline, the next character is at column 1 (one-indexed). Many implementations accidentally set `self.column = 0` in the newline branch of `advance()`, then increment to 1 when the next character is consumed. This works but requires careful reasoning. Setting `self.column = 1` directly in the newline branch and *not* incrementing in the same `advance()` call (since the `\\n` itself is not on the next line) is cleaner. The code above does this correctly by setting `self.column = 1` when `char == \"\\n\"` â€” this means the *next* character consumed will see `self.column = 1` as a starting point, then the `else: self.column += 1` branch will increment it to 2... \nWait â€” re-examine the code carefully. When `advance()` processes `\\n`, it sets `self.column = 1`. The `\\n` is consumed. The *next* call to `advance()` will process the first character of the new line. In that call, `char != \"\\n\"`, so it takes the `else: self.column += 1` branch, making column = 2. But the first character of the new line should be at column 1!\nThis is a real off-by-one bug. The fix: `self.column` should represent the column of the *next character to be consumed*, and `tok_col = self.column` captures the column of the current character *before* advancing. So when `\\n` is processed, we want the next character's column to be 1. Setting `self.column = 1` achieves this correctly â€” the next `advance()` call will read column 1, then increment to 2. But `tok_col` is captured *before* `advance()` is called, so the next token will correctly record column = 1.\nLet us trace it precisely:\n```\nState after newline processed:  line=2, column=1\n                                                 â†‘ \"next char is at col 1\"\nnext call to next_token():\n  _skip_whitespace() â€” suppose no whitespace\n  tok_col = self.column  â†’  tok_col = 1         âœ“ correct!\n  char = self.advance()\n    â†’ char = 'x' (first char on line 2)\n    â†’ column becomes 2                           âœ“ (column now points PAST 'x')\n  â†’ token emitted with column = 1               âœ“ correct!\n```\nThe invariant is: **`self.column` is always the column of the character that `advance()` will consume next, not the one just consumed.** This is consistent with `self.current` being the index of the next character to consume. Both are \"one ahead\" pointers.\n---\n## Knowledge Cascade: One Concept, Ten Unlocks\nYou just built a finite state machine that reads one character at a time and emits structured output. Here is what this unlocks:\n**1. Regex engines are scanners in disguise.** Every regular expression engine implements the same FSM model you just built, but auto-generates the state machine from a pattern description. Now that you understand the underlying model, regex is no longer magic â€” it is a compiled FSM where the scanner's `next_token()` loop is the FSM execution loop. Non-deterministic finite automata (NFAs) are the theoretical model; your scanner is a hand-coded deterministic finite automaton (DFA).\n**2. Stream processors work identically.** Apache Kafka consumers, Python generators, Unix pipe filters â€” all of these process data one element at a time, maintain local state, and emit output. Your scanner is O(n) in time and O(1) in auxiliary memory (ignoring the output list). This is the same property that makes stream processors able to handle infinite data: they never buffer the whole input.\n**3. Position metadata enables IDE tooling.** Every feature in your IDE that involves source positions â€” error underlines (squiggles), \"go to definition,\" hover documentation, inline type hints â€” is built on exactly the `(line, column)` pairs you are generating right now. The Language Server Protocol (LSP), which powers VS Code's language support, defines its entire API in terms of `{line, character}` positions. You are generating the raw material.\n**4. The EOF sentinel appears everywhere.** The pattern of using a sentinel value to signal \"no more data\" appears in: SQL `NULL`, TCP's `FIN` flag (connection termination), Unix's `EOF` byte (`Ctrl+D`), C's `null` terminator in strings, and Python's `StopIteration` exception in generators. The common insight is: give the consumer a guaranteed terminal signal rather than making it check a boolean \"are we done?\" flag separately. Your EOF token does exactly this for the parser.\n**5. Error recovery as a design choice.** Your scanner emits an `ERROR` token and *continues* rather than raising an exception and halting. This is called **panic mode recovery** in compiler design â€” it is a deliberate choice to let the scanner (and later the parser) report as many errors as possible in one compilation run, rather than stopping at the first problem. This trade-off (more errors reported vs. possibility of cascading false positives) is one of the fundamental design decisions in compiler error handling. You just made it, probably without realizing it was a choice.\n> ðŸ”­ **Deep Dive**: If you want to understand FSMs formally â€” including how NFA-to-DFA conversion works and how this relates to regex compilation â€” see *Introduction to the Theory of Computation* by Michael Sipser, Chapter 1 (Finite Automata). It is the clearest mathematical treatment of the theory underlying everything you are building.\n---\n## Summary: What You Have Built\nBy completing this milestone, you have built a working character-level scanning foundation. Specifically:\n- A `TokenType` enumeration covering every token category in your C-like language, with individually named operator and punctuation variants\n- A `Token` dataclass that carries type, lexeme (raw source text), and precise source position (line and column, 1-indexed)\n- A `Scanner` class with `peek()` (non-consuming look-ahead), `advance()` (consuming with position tracking), and `is_at_end()` (boundary guard)\n- Position tracking that correctly handles newlines (incrementing line, resetting column) and Windows line endings (`\\r\\n` counts as one newline)\n- Single-character token recognition via a dictionary dispatch table â€” O(1) per character\n- Silent whitespace consumption â€” spaces, tabs, carriage returns, and newlines are consumed without producing tokens\n- EOF sentinel emission â€” the final token in every scan is always `EOF`\n- Error token emission for unrecognized characters â€” scanning continues after the error (error recovery)\nIn Milestone 2, you will extend `next_token()` to handle multi-character tokens: two-character operators like `==` and `>=`, number literals, and identifiers â€” applying the **maximal munch** principle to always prefer the longest matching token.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m2 -->\n<!-- MS_ID: tokenizer-m2 -->\n# Milestone 2: Multi-Character Tokens & Maximal Munch\n## Where You Are in the Pipeline\n\n![Tokenizer System Satellite Map â€” The Complete Pipeline](./diagrams/diag-satellite-map.svg)\n\nIn Milestone 1, you built the engine: a character-consuming scanner that can recognize single characters and track its position through source text. Every character your scanner touches is visited exactly once, left to right, with no going back.\nNow you face the first real challenge of language design: **multi-character tokens**. The characters `>` and `=` each mean something on their own (`GREATER`, `ASSIGN`). But together, in that exact order, they mean something different (`GREATER_EQ`). How does your scanner â€” which reads one character at a time â€” decide which interpretation is correct?\nThis milestone answers that question with a principle called **maximal munch**: *always consume as many characters as possible for the current token.* You will apply it to two-character operators, number literals, and identifiers. By the end, your scanner will handle the full vocabulary of your C-like language except for strings and comments (those come in Milestone 3).\n---\n## The Revelation: Tokenization Is Not Pattern Matching\nHere is the mental model that most developers arrive with when they first think about multi-character tokens:\n> *\"My scanner should look at the source text, try all the patterns it knows (like `==`, `>=`, `!=`), and pick the one that matches best at the current position.\"*\nThis search-and-match framing is natural â€” it is how you would think about the problem if you were searching a document for a word. And it sounds reasonable enough. So let us follow it to its logical consequence and see where it breaks.\nSuppose your language had a token `<==` (less-than-or-equal followed by assignment, hypothetically). And you are at position where the source reads `<==`. The search-and-match approach says: try `<==` first (length 3), then `<=` (length 2), then `<` (length 1). Pick the longest match.\nNow imagine doing this for every character. You need to try every pattern, every time, at every position. If you have 30 token types, you try 30 patterns at each character. For a 10,000-line source file with ~300,000 characters, that is potentially 9 million pattern trials. And you would need regex or string matching for each trial.\nBut the real problem is deeper: **this framing is architecturally wrong**. It treats tokenization as a repeated search â€” \"find the next token starting here\" â€” which implies that you could look arbitrarily far ahead to determine what the next token is. Some tokens could require looking 10 characters ahead, or 100. The search cost grows with the length of the longest pattern.\n**The actual model is different.** Your scanner does not search. It *consumes*. The moment `advance()` is called, a decision has been made: this character is part of the current token. The question is only ever: \"given what I have consumed so far, should I consume one more character, or stop?\"\nThis is the greedy consumption model. At each step, the scanner asks a single, local question: *\"Should I consume the next character too?\"* The answer is always one character of lookahead â€” `peek()`. If the answer is yes, consume and ask again. If no, stop and emit the token.\n**This is why tokenizers need zero backtracking for well-designed languages.** The decision to consume a character is never reversed. There is no \"oops, I should not have taken that character.\" The language is designed so that greedy consumption always produces the correct result.\n> ðŸ”‘ **The Insight: \"Maximal Munch Is Greedy\"**\n>\n> **Maximal munch** is a specific greedy algorithm: among all valid tokens that could start at the current position, always take the longest one. It is implemented not by trying multiple patterns, but by consuming characters one at a time and stopping only when the next character would not extend the current token. Single character of lookahead. No search. No backtracking.\n>\n> **Why \"greedy\"?** In algorithm design, a greedy algorithm makes the locally optimal choice at each step without reconsidering previous decisions. Maximal munch makes the locally longest choice: \"can I take one more character?\" If yes, take it. This is the same principle as the greedy interval scheduling algorithm (take the earliest-ending interval), Huffman coding (always merge the two smallest trees), or TCP's Nagle algorithm (send when buffer is full, not before). Greedy works here because the language is designed to make it work â€” no valid tokenization requires the scanner to prefer a shorter match over a longer one.\n---\n## Two-Character Operators: Lookahead in Practice\n\n![Maximal Munch Decision Tree for Operators](./diagrams/diag-m2-maximal-munch-decision.svg)\n\nYour C-like language has these paired operators, where one character alone is a valid token and two characters together form a different valid token:\n| First Char | Alone â†’ Token | With `=` â†’ Token |\n|---|---|---|\n| `=` | `ASSIGN` | `EQUAL` (`==`) |\n| `!` | `BANG` | `NOT_EQUAL` (`!=`) |\n| `<` | `LESS` | `LESS_EQ` (`<=`) |\n| `>` | `GREATER` | `GREATER_EQ` (`>=`) |\nIn Milestone 1, your `next_token()` used a dictionary lookup for single-character tokens. That approach breaks here because by the time you know you have seen `>`, you do not yet know if it should be `GREATER` or `GREATER_EQ`. You must look at the character *after* `>` without consuming it.\nThis is exactly what `peek()` is for.\nThe pattern is always the same:\n1. `advance()` consumes the first character (e.g., `>`).\n2. `peek()` inspects the *next* character without consuming it.\n3. If `peek()` returns `=`, call `advance()` again (consuming `=`) and emit the two-character token.\n4. Otherwise, emit the one-character token â€” `peek()` is not consumed, and the next call to `next_token()` will see it fresh.\nHere is a helper method that encodes this pattern:\n```python\ndef _match(self, expected: str) -> bool:\n    \"\"\"\n    If the next character equals `expected`, consume it and return True.\n    Otherwise, leave it unconsumed and return False.\n    \"\"\"\n    if self.is_at_end():\n        return False\n    if self.source[self.current] != expected:\n        return False\n    # Consume the character â€” update position tracking\n    self.advance()\n    return True\n```\n`_match()` is the workhorse of two-character operator recognition. It is peek-and-consume in one operation: it only consumes if the next character is what you expected. If it returns `True`, the two-character token is complete. If `False`, the single-character token stands alone.\nNow the operator scanning logic becomes almost readable as English:\n```python\ndef _scan_operator(self, char: str, tok_line: int, tok_col: int) -> Token:\n    \"\"\"\n    Scan an operator character, applying maximal munch for two-char variants.\n    `char` is already consumed. `tok_line` and `tok_col` are start position.\n    \"\"\"\n    if char == \"=\":\n        if self._match(\"=\"):\n            return Token(TokenType.EQUAL, \"==\", tok_line, tok_col)\n        return Token(TokenType.ASSIGN, \"=\", tok_line, tok_col)\n    if char == \"!\":\n        if self._match(\"=\"):\n            return Token(TokenType.NOT_EQUAL, \"!=\", tok_line, tok_col)\n        return Token(TokenType.BANG, \"!\", tok_line, tok_col)\n    if char == \"<\":\n        if self._match(\"=\"):\n            return Token(TokenType.LESS_EQ, \"<=\", tok_line, tok_col)\n        return Token(TokenType.LESS, \"<\", tok_line, tok_col)\n    if char == \">\":\n        if self._match(\"=\"):\n            return Token(TokenType.GREATER_EQ, \">=\", tok_line, tok_col)\n        return Token(TokenType.GREATER, \">\", tok_line, tok_col)\n    # Should not reach here if called correctly\n    return Token(TokenType.ERROR, char, tok_line, tok_col)\n```\nNotice the lexeme argument. When you emit `Token(TokenType.EQUAL, \"==\", ...)`, you construct the lexeme `\"==\"` explicitly â€” you are not going back to re-read the source. You know the lexeme because you know exactly which characters you consumed. This is another consequence of the consume-as-you-go model: the lexeme is always exactly the characters you `advance()`-d past.\n### The `>==` Trace: Maximal Munch in Slow Motion\n\n![Trace: Maximal Munch on '>==' Input](./diagrams/diag-m2-munch-trace.svg)\n\nLet us trace through what happens when your scanner encounters the input `>==`:\n```\nSource:   > = =\n          ^\n          current=0, line=1, col=1\nnext_token() call #1:\n  _skip_whitespace()  â†’ nothing to skip\n  tok_line=1, tok_col=1\n  char = advance()    â†’ '>', current=1, col=2\n  char == '>'  â†’  call _match('=')\n    peek() â†’ '='  (source[1] == '=')  â†’ matches!\n    advance()  â†’ '=', current=2, col=3\n    return True\n  emit Token(GREATER_EQ, \">=\", 1, 1)\nnext_token() call #2:\n  _skip_whitespace()  â†’ nothing to skip\n  tok_line=1, tok_col=3\n  char = advance()    â†’ '=', current=3, col=4\n  char == '='  â†’  call _match('=')\n    is_at_end()  â†’ True  (current=3 >= len(\">===\")==3)\n    return False\n  emit Token(ASSIGN, \"=\", 1, 3)\nnext_token() call #3:\n  is_at_end()  â†’ True\n  emit Token(EOF, \"\", 1, 4)\n```\nResult: `[GREATER_EQ(\">=\", 1:1), ASSIGN(\"=\", 1:3), EOF(\"\", 1:4)]`\nThis is maximal munch in action. When the scanner sees `>`, it greedily consumed the `=` that followed. It did not \"ask\" whether `>=` should be followed by another `=` to form `>==` (which is not a token in this language). It simply asked \"should I take one more character?\" at each step, and stopped as soon as the answer was no. The second `=` is left for the next call to `next_token()`, which correctly scans it as `ASSIGN`.\n---\n## Integrating Operator Scanning into next_token()\nYour `next_token()` from Milestone 1 handled single-character tokens via a dictionary. The two-character operators need special handling â€” they cannot be in the dictionary because the decision depends on the *next* character, which is not yet consumed.\nThe cleanest approach: remove the ambiguous characters from `SINGLE_CHAR_TOKENS` and handle them explicitly in `next_token()`:\n```python\n# Updated SINGLE_CHAR_TOKENS â€” unambiguous single-char tokens only\nSINGLE_CHAR_TOKENS: dict[str, TokenType] = {\n    \"+\": TokenType.PLUS,\n    \"-\": TokenType.MINUS,\n    \"*\": TokenType.STAR,\n    # '/' removed â€” will be handled for comments in Milestone 3\n    # '=' removed â€” could be ASSIGN or part of EQUAL\n    # '!' removed â€” could be BANG or part of NOT_EQUAL\n    # '<' removed â€” could be LESS or part of LESS_EQ\n    # '>' removed â€” could be GREATER or part of GREATER_EQ\n    \"(\": TokenType.LPAREN,\n    \")\": TokenType.RPAREN,\n    \"{\": TokenType.LBRACE,\n    \"}\": TokenType.RBRACE,\n    \"[\": TokenType.LBRACKET,\n    \"]\": TokenType.RBRACKET,\n    \";\": TokenType.SEMICOLON,\n    \",\": TokenType.COMMA,\n}\n# Characters that start two-character operators\nOPERATOR_CHARS: frozenset[str] = frozenset({\"=\", \"!\", \"<\", \">\"})\n```\nAnd `next_token()` extended:\n```python\ndef next_token(self) -> Token:\n    self._skip_whitespace()\n    tok_line = self.line\n    tok_col = self.column\n    if self.is_at_end():\n        return Token(TokenType.EOF, \"\", tok_line, tok_col)\n    char = self.advance()\n    # Single-character tokens (unambiguous)\n    if char in SINGLE_CHAR_TOKENS:\n        return Token(SINGLE_CHAR_TOKENS[char], char, tok_line, tok_col)\n    # Two-character operators (maximal munch with peek)\n    if char in OPERATOR_CHARS:\n        return self._scan_operator(char, tok_line, tok_col)\n    # Number literals\n    if char.isdigit():\n        return self._scan_number(char, tok_line, tok_col)\n    # Identifiers and keywords\n    if char.isalpha() or char == \"_\":\n        return self._scan_identifier(char, tok_line, tok_col)\n    # Unrecognized â€” error token, continue scanning\n    return Token(TokenType.ERROR, char, tok_line, tok_col)\n```\nNotice the dispatch order matters: digits before identifiers (so `4abc` is scanned as a number then an identifier, not a single broken token), and the operator check before the fallthrough error case.\nAlso note that `/` is not yet in `SINGLE_CHAR_TOKENS`. In Milestone 3, a `/` might start a comment (`//` or `/*`) â€” so it needs the same two-character lookahead treatment as `=` and `>`. For now, if you want to test division, you can temporarily add it back. The milestone 3 implementation will replace it.\n---\n## Number Literals: The State Machine Inside _scan_number()\n\n![Number Literal Scanner â€” State Machine](./diagrams/diag-m2-number-scanning-fsm.svg)\n\nNumber literals in your C-like language come in two forms:\n- **Integer**: one or more digits â€” `0`, `42`, `1000`\n- **Float**: digits, a single decimal point, digits â€” `3.14`, `0.5`, `100.0`\nBoth begin with a digit. Your scanner has already consumed the first digit (it was used to recognize this as a number in `next_token()`). Now `_scan_number()` receives that first character and must consume the rest.\nThe scanning logic follows a simple two-state machine:\n**State 1 â€” INTEGER**: Consume digits until you see something that is not a digit. If that non-digit is `.` and the character *after* the dot is also a digit, transition to the float state. Otherwise, emit an integer token.\n**State 2 â€” FLOAT**: After consuming the `.`, continue consuming digits until you see something that is not a digit. Emit a float token.\n```python\ndef _scan_number(self, first_digit: str, tok_line: int, tok_col: int) -> Token:\n    \"\"\"\n    Scan a number literal. `first_digit` is already consumed.\n    Handles both integers (42) and floats (3.14).\n    \"\"\"\n    lexeme = first_digit\n    # Consume remaining integer digits\n    while not self.is_at_end() and self.peek().isdigit():\n        lexeme += self.advance()\n    # Check for decimal point followed by digits (float)\n    if self.peek() == \".\" and self._peek_next().isdigit():\n        lexeme += self.advance()  # consume the '.'\n        # Consume fractional digits\n        while not self.is_at_end() and self.peek().isdigit():\n            lexeme += self.advance()\n    return Token(TokenType.NUMBER, lexeme, tok_line, tok_col)\n```\nThis requires a second lookahead method, `_peek_next()`, which looks *two* characters ahead:\n```python\ndef _peek_next(self) -> str:\n    \"\"\"Look two characters ahead without consuming. Returns '\\\\0' if not available.\"\"\"\n    if self.current + 1 >= len(self.source):\n        return \"\\0\"\n    return self.source[self.current + 1]\n```\n> **Why do we need two-character lookahead here?** When the scanner is in the INTEGER state and sees a `.`, it needs to decide: is this a decimal point in a float literal, or a `.` that belongs to something else (like a method call, or a range operator in another language)? With only one character of lookahead (peek), you see `.` â€” but you don't know if it's `3.14` (float) or `3.toString()` (integer followed by method access). Looking one more character ahead to check if the character *after* the dot is a digit resolves the ambiguity without consuming anything. If the next-next character is not a digit, the `.` is left alone for the next `next_token()` call.\n>\n> This is a deliberate design point: **most languages need no more than one or two characters of lookahead in the lexer**. If your language required five characters of lookahead to tokenize, that would signal a design problem â€” it means tokens are ambiguous from their first few characters and the grammar is harder to reason about.\n### Edge Cases to Decide Explicitly\nNumber scanning has a few cases where the spec is silent and you must make a choice:\n| Input | Question | Recommendation |\n|---|---|---|\n| `3.` | Trailing dot â€” is this `3.` or `3` + `.`? | Scan as integer `3`, leave `.` for next token. The `_peek_next().isdigit()` check handles this â€” if nothing follows the dot, don't consume it. |\n| `.5` | Leading dot â€” is this `.5` or `.` + `5`? | Not a number: `.` is not a digit, so `_scan_number` is never triggered. `.` will likely be an error token. Document this decision. |\n| `3.14.15` | Two dots â€” invalid float? | Scan `3.14` as a float, then `.` as an error or punctuation, then `15` as an integer. Your scanner naturally produces this: after consuming `3.14`, the next `peek()` is `.`, which is not a digit, so the float scan stops. |\n| `42abc` | Digit followed by letter | Scan `42` as NUMBER, then `abc` as IDENTIFIER. The identifier check in `next_token()` will pick up `a` on the next call. |\nThe key rule: **make a choice, document it in a comment, and write a test for it**. Undefined behavior is the enemy.\n---\n## Identifiers: The Scan-Then-Lookup Pattern\n\n![Identifier Scanning + Keyword Table Lookup](./diagrams/diag-m2-keyword-vs-identifier.svg)\n\nIdentifiers in your language follow a simple rule: start with a letter or underscore, followed by zero or more letters, digits, or underscores. Examples: `x`, `myVar`, `_count`, `item2`.\nKeywords â€” `if`, `else`, `while`, `return`, `true`, `false`, `null` â€” look exactly like identifiers when you first encounter them character-by-character. The character sequence `i`, `f` is indistinguishable from the beginning of `iffy` or `if_x`. You cannot know whether you are scanning a keyword until you have consumed all its characters.\n**The wrong approach:** try to match keywords character-by-character in the scanner's state machine. This means adding states like `IN_KEYWORD_IF_SEEN_I`, `IN_KEYWORD_IF_SEEN_IF`. With 7 keywords, you'd add dozens of states. The state machine becomes unmanageable. Production compilers like GCC's original scanner tried this â€” it was abandoned.\n**The right approach:** scan the entire identifier first, then look up the result in a table. This is the **scan-then-lookup** pattern. It has two steps:\n1. Consume all characters that are valid identifier characters (letter, digit, underscore). This produces the full lexeme.\n2. Look up the lexeme in a `KEYWORDS` dictionary. If found, emit a `KEYWORD` token. If not found, emit an `IDENTIFIER` token.\n```python\nKEYWORDS: dict[str, TokenType] = {\n    \"if\":     TokenType.KEYWORD,\n    \"else\":   TokenType.KEYWORD,\n    \"while\":  TokenType.KEYWORD,\n    \"return\": TokenType.KEYWORD,\n    \"true\":   TokenType.KEYWORD,\n    \"false\":  TokenType.KEYWORD,\n    \"null\":   TokenType.KEYWORD,\n}\ndef _scan_identifier(self, first_char: str, tok_line: int, tok_col: int) -> Token:\n    \"\"\"\n    Scan an identifier or keyword. `first_char` is already consumed.\n    Returns KEYWORD if the lexeme matches a reserved word, IDENTIFIER otherwise.\n    \"\"\"\n    lexeme = first_char\n    # Consume all valid identifier characters\n    while not self.is_at_end() and (self.peek().isalnum() or self.peek() == \"_\"):\n        lexeme += self.advance()\n    # Keyword lookup â€” O(1) hash table probe\n    token_type = KEYWORDS.get(lexeme, TokenType.IDENTIFIER)\n    return Token(token_type, lexeme, tok_line, tok_col)\n```\nThe lookup `KEYWORDS.get(lexeme, TokenType.IDENTIFIER)` is clean Python: it returns `TokenType.IDENTIFIER` as the default if the key is not found. One line, O(1), no if/else needed.\n### Why the Lookup Table Avoids a Classic Bug\nConsider the identifier `iffy`. If you tried to match the keyword `if` character-by-character, your scanner might see `i`, `f` and think \"keyword `if` found!\" â€” before noticing there are more characters. This is called a **prefix match bug**. The scan-then-lookup approach is immune: you consume `i`, `f`, `f`, `y` as a complete identifier, then look up `\"iffy\"` in the keyword table â€” which returns nothing â€” and correctly emit `IDENTIFIER(\"iffy\")`.\nThe rule: **keyword recognition must operate on the complete lexeme, never on a prefix.** The lookup table enforces this naturally.\n### Keyword Table Design: Two Variants\nYour implementation above stores all keywords with the same value (`TokenType.KEYWORD`). This means the keyword's identity lives in the lexeme: `token.type == KEYWORD` and `token.lexeme == \"if\"`. The parser will check both.\nAn alternative: give each keyword its own token type (`IF`, `ELSE`, `WHILE`, `RETURN`, `TRUE`, `FALSE`, `NULL`). Then the parser checks `token.type == TokenType.IF` without examining the lexeme.\n| Design | Pros | Cons | Used By |\n|---|---|---|---|\n| **Single KEYWORD type + lexeme** âœ“ | Fewer enum variants, easy to add new keywords | Parser must check lexeme string | CPython tokenizer, many small language runtimes |\n| Per-keyword token types | Parser logic cleaner, no string comparison | Enum grows with language, more boilerplate | Go compiler, Clang, LLVM tools |\nFor this project, the single `KEYWORD` type is fine. The acceptance criteria specify it explicitly. If you were building a production compiler with a full parser, per-keyword types would be the right call â€” parsers matching on `TokenType.IF` are faster and clearer than parsers matching on `token.type == KEYWORD and token.lexeme == \"if\"`.\n> ðŸ”­ **Deep Dive**: Go's scanner uses the scan-then-lookup pattern with per-keyword token types. You can read the exact implementation in `go/src/go/scanner/scanner.go` in the Go standard library â€” search for the `Lookup` function. It's under 50 lines and maps string â†’ token type via a plain array (since Go's keywords are few and densely packed). The design philosophy is described in *The Go Programming Language* (Donovan & Kernighan), Chapter 1.\n---\n## Extended FSM: What Your Scanner Now Looks Like\n{{DIAGRAM:diag-m2-fsm-extended}}\nAfter Milestone 1, your scanner had one effective \"state\" â€” `START` â€” from which it dispatched on single characters. Now it has four meaningful scanning states:\n- **START** â€” between tokens, deciding what the next token is based on the first character\n- **IN_NUMBER** â€” consuming digit characters (or a decimal point + more digits)\n- **IN_IDENTIFIER** â€” consuming alphanumeric/underscore characters\n- **IN_OPERATOR** â€” peeking at a second character to apply maximal munch\nThese states are not explicit enum values in your code â€” they are implicit in which scanning function is currently executing. `_scan_number()` represents the IN_NUMBER state. `_scan_identifier()` represents the IN_IDENTIFIER state. The state machine's \"state\" is the call stack.\nThis is called a **recursive descent** approach to scanning: instead of an explicit state enum with a transition table, you use function calls to represent state transitions. It is easier to write and debug, and it is exactly how production scanners in compilers like GCC, Clang, and Go work.\n---\n## The Complete Milestone 2 Scanner\nHere is `scanner.py` with all Milestone 2 additions integrated:\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nclass TokenType(Enum):\n    # Literals\n    NUMBER     = auto()\n    STRING     = auto()\n    # Names\n    IDENTIFIER = auto()\n    KEYWORD    = auto()\n    # Operators\n    PLUS       = auto()\n    MINUS      = auto()\n    STAR       = auto()\n    SLASH      = auto()\n    ASSIGN     = auto()   # =\n    EQUAL      = auto()   # ==\n    NOT_EQUAL  = auto()   # !=\n    LESS       = auto()   # <\n    LESS_EQ    = auto()   # <=\n    GREATER    = auto()   # >\n    GREATER_EQ = auto()   # >=\n    BANG       = auto()   # !\n    # Punctuation\n    LPAREN     = auto()\n    RPAREN     = auto()\n    LBRACE     = auto()\n    RBRACE     = auto()\n    LBRACKET   = auto()\n    RBRACKET   = auto()\n    SEMICOLON  = auto()\n    COMMA      = auto()\n    # Sentinels\n    EOF        = auto()\n    ERROR      = auto()\n@dataclass\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\n# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSINGLE_CHAR_TOKENS: dict[str, TokenType] = {\n    \"+\": TokenType.PLUS,\n    \"-\": TokenType.MINUS,\n    \"*\": TokenType.STAR,\n    \"(\": TokenType.LPAREN,\n    \")\": TokenType.RPAREN,\n    \"{\": TokenType.LBRACE,\n    \"}\": TokenType.RBRACE,\n    \"[\": TokenType.LBRACKET,\n    \"]\": TokenType.RBRACKET,\n    \";\": TokenType.SEMICOLON,\n    \",\": TokenType.COMMA,\n}\nOPERATOR_CHARS: frozenset[str] = frozenset({\"=\", \"!\", \"<\", \">\"})\nWHITESPACE: frozenset[str] = frozenset({\" \", \"\\t\", \"\\r\", \"\\n\"})\nKEYWORDS: dict[str, TokenType] = {\n    \"if\":     TokenType.KEYWORD,\n    \"else\":   TokenType.KEYWORD,\n    \"while\":  TokenType.KEYWORD,\n    \"return\": TokenType.KEYWORD,\n    \"true\":   TokenType.KEYWORD,\n    \"false\":  TokenType.KEYWORD,\n    \"null\":   TokenType.KEYWORD,\n}\n# â”€â”€ Scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass Scanner:\n    def __init__(self, source: str) -> None:\n        self.source: str = source\n        self.current: int = 0\n        self.line: int = 1\n        self.column: int = 1\n    # â”€â”€ Primitive Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def is_at_end(self) -> bool:\n        return self.current >= len(self.source)\n    def peek(self) -> str:\n        if self.is_at_end():\n            return \"\\0\"\n        return self.source[self.current]\n    def _peek_next(self) -> str:\n        \"\"\"Look two characters ahead without consuming.\"\"\"\n        if self.current + 1 >= len(self.source):\n            return \"\\0\"\n        return self.source[self.current + 1]\n    def advance(self) -> str:\n        char = self.source[self.current]\n        self.current += 1\n        if char == \"\\n\":\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n        return char\n    def _match(self, expected: str) -> bool:\n        \"\"\"Consume next char if it equals `expected`. Return True if consumed.\"\"\"\n        if self.is_at_end():\n            return False\n        if self.source[self.current] != expected:\n            return False\n        self.advance()\n        return True\n    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _skip_whitespace(self) -> None:\n        while not self.is_at_end() and self.peek() in WHITESPACE:\n            self.advance()\n    # â”€â”€ Scanning Methods â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_operator(self, char: str, tok_line: int, tok_col: int) -> Token:\n        if char == \"=\":\n            if self._match(\"=\"):\n                return Token(TokenType.EQUAL, \"==\", tok_line, tok_col)\n            return Token(TokenType.ASSIGN, \"=\", tok_line, tok_col)\n        if char == \"!\":\n            if self._match(\"=\"):\n                return Token(TokenType.NOT_EQUAL, \"!=\", tok_line, tok_col)\n            return Token(TokenType.BANG, \"!\", tok_line, tok_col)\n        if char == \"<\":\n            if self._match(\"=\"):\n                return Token(TokenType.LESS_EQ, \"<=\", tok_line, tok_col)\n            return Token(TokenType.LESS, \"<\", tok_line, tok_col)\n        if char == \">\":\n            if self._match(\"=\"):\n                return Token(TokenType.GREATER_EQ, \">=\", tok_line, tok_col)\n            return Token(TokenType.GREATER, \">\", tok_line, tok_col)\n        return Token(TokenType.ERROR, char, tok_line, tok_col)\n    def _scan_number(self, first_digit: str, tok_line: int, tok_col: int) -> Token:\n        lexeme = first_digit\n        # Integer part\n        while not self.is_at_end() and self.peek().isdigit():\n            lexeme += self.advance()\n        # Optional fractional part: dot followed by at least one digit\n        if self.peek() == \".\" and self._peek_next().isdigit():\n            lexeme += self.advance()   # consume '.'\n            while not self.is_at_end() and self.peek().isdigit():\n                lexeme += self.advance()\n        return Token(TokenType.NUMBER, lexeme, tok_line, tok_col)\n    def _scan_identifier(self, first_char: str, tok_line: int, tok_col: int) -> Token:\n        lexeme = first_char\n        while not self.is_at_end() and (self.peek().isalnum() or self.peek() == \"_\"):\n            lexeme += self.advance()\n        token_type = KEYWORDS.get(lexeme, TokenType.IDENTIFIER)\n        return Token(token_type, lexeme, tok_line, tok_col)\n    # â”€â”€ Token Production â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def next_token(self) -> Token:\n        self._skip_whitespace()\n        tok_line = self.line\n        tok_col = self.column\n        if self.is_at_end():\n            return Token(TokenType.EOF, \"\", tok_line, tok_col)\n        char = self.advance()\n        if char in SINGLE_CHAR_TOKENS:\n            return Token(SINGLE_CHAR_TOKENS[char], char, tok_line, tok_col)\n        if char in OPERATOR_CHARS:\n            return self._scan_operator(char, tok_line, tok_col)\n        if char.isdigit():\n            return self._scan_number(char, tok_line, tok_col)\n        if char.isalpha() or char == \"_\":\n            return self._scan_identifier(char, tok_line, tok_col)\n        return Token(TokenType.ERROR, char, tok_line, tok_col)\n    def scan_tokens(self) -> list[Token]:\n        tokens: list[Token] = []\n        while True:\n            token = self.next_token()\n            tokens.append(token)\n            if token.type == TokenType.EOF:\n                break\n        return tokens\n```\n---\n## Lookahead as a Language Design Constraint\nYou have now used two different amounts of lookahead in this milestone:\n- **One character** (`peek()`) â€” for two-character operators\n- **Two characters** (`peek()` + `_peek_next()`) â€” for deciding whether a `.` after an integer starts a float\nThis is not accidental. The amount of lookahead a scanner needs is a property of the *language being tokenized*, not of the scanner implementation. And that amount has a direct connection to parser theory.\nA scanner that needs at most *k* characters of lookahead is called an **LL(k) scanner** (left-to-right, leftmost derivation, k tokens ahead). Most well-designed languages are LL(1) at the lexical level â€” one character of lookahead suffices. Your two-character lookahead for floats is technically LL(2) for that one case, but it is so localized that it has no practical impact.\nWhy does this matter? Because the number of lookahead characters determines how predictably the scanner can operate. An LL(1) scanner never has to \"wait\" to decide â€” it always knows the current token's type from at most the next character. A scanner requiring unbounded lookahead would need to buffer arbitrarily much input before emitting any token â€” not compatible with streaming or incremental parsing.\n> **The design principle**: When you design a language (or a DSL), keep the lexical grammar LL(1). If you find yourself needing more than one or two characters of lookahead at the scanner level, reconsider the token design. The cost of getting this wrong is not just implementation complexity â€” it is that your scanner can no longer be a clean, stateless, streaming component.\nThis principle directly constrains language design. C++ famously requires complex, context-sensitive tokenization because `>>` means both \"right shift\" and \"end of two nested templates.\" The lexer cannot know which without parser context. This is called **maximal munch ambiguity** â€” and C++ compilers have to special-case it. A well-designed language avoids this by choosing operator syntax that is unambiguous from the first character.\n---\n## Testing Milestone 2\nTests in this milestone must verify not just token types but also the exact token stream ordering and positions. Every test should be a complete, pinned scenario.\n```python\ndef test_equal_operator():\n    scanner = Scanner(\"==\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.EQUAL, \"==\", 1, 1)\n    assert tokens[1].type == TokenType.EOF\ndef test_assign_not_equal():\n    # Single '=' must NOT consume the next non-'=' character\n    scanner = Scanner(\"=+\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.ASSIGN, \"=\", 1, 1)\n    assert tokens[1] == Token(TokenType.PLUS,   \"+\", 1, 2)\ndef test_greater_equal():\n    scanner = Scanner(\">=\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.GREATER_EQ, \">=\", 1, 1)\ndef test_maximal_munch_gee_assign():\n    # '>== ' must tokenize as GREATER_EQ, ASSIGN (not GREATER, EQUAL)\n    scanner = Scanner(\">==\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.GREATER_EQ, \">=\", 1, 1)\n    assert tokens[1] == Token(TokenType.ASSIGN,     \"=\",  1, 3)\n    assert tokens[2].type == TokenType.EOF\ndef test_not_equal():\n    scanner = Scanner(\"!=\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NOT_EQUAL, \"!=\", 1, 1)\ndef test_bang_alone():\n    # '!' not followed by '=' is BANG\n    scanner = Scanner(\"! \")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.BANG, \"!\", 1, 1)\ndef test_less_and_less_eq():\n    scanner = Scanner(\"< <=\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.LESS,    \"<\",  1, 1)\n    assert tokens[1] == Token(TokenType.LESS_EQ, \"<=\", 1, 3)\ndef test_integer_literal():\n    scanner = Scanner(\"42\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NUMBER, \"42\", 1, 1)\ndef test_integer_zero():\n    scanner = Scanner(\"0\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NUMBER, \"0\", 1, 1)\ndef test_float_literal():\n    scanner = Scanner(\"3.14\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NUMBER, \"3.14\", 1, 1)\ndef test_float_with_leading_zero():\n    scanner = Scanner(\"0.5\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NUMBER, \"0.5\", 1, 1)\ndef test_integer_no_trailing_dot():\n    # '3.' should scan as INTEGER '3', leaving '.' for next token\n    scanner = Scanner(\"3.\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NUMBER, \"3\", 1, 1)\n    # '.' is not a digit start â€” it becomes an error or punctuation\n    assert tokens[1].lexeme == \".\"\ndef test_number_followed_by_identifier():\n    # '42abc' â†’ NUMBER(42), IDENTIFIER(abc)\n    scanner = Scanner(\"42abc\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NUMBER,     \"42\",  1, 1)\n    assert tokens[1] == Token(TokenType.IDENTIFIER, \"abc\", 1, 3)\ndef test_identifier_simple():\n    scanner = Scanner(\"myVar\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"myVar\", 1, 1)\ndef test_identifier_with_underscore():\n    scanner = Scanner(\"_count\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"_count\", 1, 1)\ndef test_identifier_with_digits():\n    scanner = Scanner(\"item2\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"item2\", 1, 1)\ndef test_keyword_if():\n    scanner = Scanner(\"if\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.KEYWORD, \"if\", 1, 1)\ndef test_keyword_return():\n    scanner = Scanner(\"return\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.KEYWORD, \"return\", 1, 1)\ndef test_keyword_not_a_prefix():\n    # 'iffy' must NOT match keyword 'if' â€” full lexeme lookup required\n    scanner = Scanner(\"iffy\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"iffy\", 1, 1)\ndef test_keyword_inside_identifier():\n    # 'if_x' contains 'if' as a prefix but is an identifier\n    scanner = Scanner(\"if_x\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"if_x\", 1, 1)\ndef test_all_keywords():\n    source = \"if else while return true false null\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    keyword_lexemes = [t.lexeme for t in tokens if t.type == TokenType.KEYWORD]\n    assert keyword_lexemes == [\"if\", \"else\", \"while\", \"return\", \"true\", \"false\", \"null\"]\ndef test_complete_statement():\n    # The canonical acceptance test from the spec\n    source = \"if (x >= 42) { return true; }\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    expected = [\n        Token(TokenType.KEYWORD,    \"if\",     1,  1),\n        Token(TokenType.LPAREN,     \"(\",      1,  4),\n        Token(TokenType.IDENTIFIER, \"x\",      1,  5),\n        Token(TokenType.GREATER_EQ, \">=\",     1,  7),\n        Token(TokenType.NUMBER,     \"42\",     1,  10),\n        Token(TokenType.RPAREN,     \")\",      1,  12),\n        Token(TokenType.LBRACE,     \"{\",      1,  14),\n        Token(TokenType.KEYWORD,    \"return\", 1,  16),\n        Token(TokenType.KEYWORD,    \"true\",   1,  23),\n        Token(TokenType.SEMICOLON,  \";\",      1,  27),\n        Token(TokenType.RBRACE,     \"}\",      1,  29),\n        Token(TokenType.EOF,        \"\",       1,  30),\n    ]\n    assert tokens == expected\n```\nThe last test â€” `test_complete_statement` â€” is the canonical acceptance test for Milestone 2. It exercises every feature: keywords, identifiers, a two-character operator, a number literal, punctuation, and correct column positions. Run it last. If it passes, every other test will pass too.\n> **Testing philosophy for scanners**: The canonical test is more valuable than ten isolated tests. It catches interaction bugs â€” for example, a column tracking error that only shows up after processing whitespace after a two-character operator. Always include at least one full-sentence integration test.\n---\n## Design Decisions: The String Accumulation Question\nIn `_scan_number()` and `_scan_identifier()`, you accumulate the lexeme character-by-character with string concatenation (`lexeme += self.advance()`). In Python, strings are immutable â€” each `+=` creates a new string object, copies the old content, and appends the new character. For a 30-character identifier, this means 30 string allocations.\nIs this a problem? Let's be quantitative.\n| Approach | Implementation | Cost per token | Used by |\n|---|---|---|---|\n| **String concat** âœ“ | `lexeme += char` | O(nÂ²) in worst case, small constant | CPython's tokenizer for short tokens |\n| StringBuilder (`io.StringIO`) | `buf.write(char)` then `buf.getvalue()` | O(n) total | Java scanners (Java's StringBuilder) |\n| Slice the source | `source[start:current]` | O(n) copy, but only one allocation | Go's scanner, Clang's lexer |\nFor identifiers and numbers in real source code, typical token length is 1â€“20 characters. The O(nÂ²) behavior of string concatenation is O(400) operations in the worst case â€” completely negligible. The `+=` approach is readable, correct, and fast enough.\nThe **slice approach** is worth knowing: instead of accumulating characters one by one, record `start = self.current` before the scan loop, then emit `Token(..., self.source[start:self.current], ...)` after the loop. This produces the lexeme in one O(n) allocation. Go's scanner uses this exact technique â€” it never accumulates characters into a buffer, it always produces lexemes by slicing the source string directly. For this project, the readability difference is small, so choose whichever you prefer.\n---\n## Common Pitfalls\n**1. Consuming the character that terminates a token**\nThe most common bug in `_scan_number()`: the loop condition is `while self.peek().isdigit()`, and inside the loop you call `self.advance()`. If you accidentally write `while self.advance().isdigit()`, you consume the terminating character (e.g., a space or `)`), and the next token starts in the wrong place. Always `peek()` to check, `advance()` to consume.\n**2. `_match()` skipping position tracking**\nIf you implement `_match()` by directly accessing `self.source[self.current]` and incrementing `self.current` without calling `self.advance()`, you bypass newline detection. This is safe for operators (which are never newlines), but it is a trap for future code. Always call `self.advance()` inside `_match()` when consuming.\n**3. Keyword matching 'return' inside 'returnValue'**\nThe scan-then-lookup pattern avoids this: `_scan_identifier()` consumes until a non-identifier character is found, producing the full lexeme `\"returnValue\"`. The keyword table does not contain `\"returnValue\"`, so it is correctly emitted as `IDENTIFIER`. The bug only appears if you try to match keywords character by character.\n**4. Float `3.` consuming the dot when it shouldn't**\nThe condition `if self.peek() == \".\" and self._peek_next().isdigit()` prevents this. If the character after `.` is not a digit, the dot is not consumed. Without `_peek_next()`, you might consume the dot and then find no digits â€” leaving you with the malformed lexeme `\"3.\"` and having \"stolen\" the dot from whatever follows.\n**5. Operator position captured after the first advance()**\nRemember from Milestone 1: `tok_line` and `tok_col` are captured *before* `advance()` is called in `next_token()`. By the time `_scan_operator()` is called, the first character is already consumed and the position is already captured. Do not capture position inside `_scan_operator()` â€” the positions come from `next_token()` as parameters.\n**6. `==` at end of file**\nWhen scanning `==` at the very end of the source, the first `=` is consumed by `advance()`, then `_match(\"=\")` is called. `_match` calls `is_at_end()` â€” which returns `False` because there is still one character left. Then it checks `self.source[self.current] != \"=\"` â€” which is `False` (it is `=`), so it consumes it. This works correctly. The sentinel `\"\\0\"` from `peek()` is not used in `_match()` â€” `_match()` uses `is_at_end()` and direct source access. Make sure `_match()` checks `is_at_end()` first.\n---\n## Knowledge Cascade: Maximal Munch Connects to Everything\nYou have built greedy, one-directional token scanning. Here is where this knowledge radiates outward:\n**1. Maximal munch is a greedy algorithm â€” and greedy algorithms appear everywhere.**\nThe same \"take the longest valid choice now without reconsidering\" principle drives Huffman coding (always merge the smallest trees), the Unix `find -name '*.c'` glob matching algorithm, interval scheduling (always take the earliest-finishing task), and TCP's Nagle algorithm (buffer until maximum segment, then send). Greedy works when you can prove that local optimality implies global optimality. For tokenization, the proof is simple: well-designed languages guarantee that maximal munch produces the unique correct tokenization. If your language has a token ambiguity that maximal munch cannot resolve, you have a language design bug, not a scanner bug.\n**2. Lookahead depth determines grammar class â€” and connects to parser design.**\nYour scanner uses LL(1) lookahead for operators and LL(2) for float detection. When you build a parser in a future project, you will encounter the same concept: LL(1) parsers can be built without backtracking because one token of lookahead is always enough to decide which grammar rule to apply. LL(k) parsers need k tokens. The connection is direct: the lookahead discipline you are practicing in the scanner recurs in every subsequent layer of the compiler pipeline.\n**3. The keyword table is a hash map â€” and hash maps power language implementations.**\nThe `KEYWORDS` dictionary is a Python `dict`, which is a hash table. The O(1) keyword lookup is what makes scanners fast. This same pattern appears in symbol tables (looking up variable names in the compiler's environment), runtime method dispatch tables (finding which method to call for a given class), and CPU branch prediction tables (mapping instruction addresses to predicted branch directions). Whenever a language implementation needs to map names to meanings in constant time, a hash table is the answer.\n**4. Scan-then-lookup vs. keyword states is a language implementer's design decision.**\nGo's scanner, Clang's scanner, CPython's tokenizer, and V8's JavaScript scanner all use scan-then-lookup. Hand-coded state machines for individual keywords exist in some older compilers and in auto-generated scanners from tools like Flex â€” but modern practice heavily favors the lookup table approach because it is trivially extensible (add a keyword by adding a line to the table) and correct by construction (no prefix-match bugs). This is a real decision you have just made the right call on.\n**5. Ambiguity in `>>=` and C++'s template problem is a famous real-world consequence.**\nIn C++, `vector<vector<int>>` was illegal before C++11 because `>>` was tokenized as the right-shift operator before the parser could tell the scanner it was inside a template parameter list. This required a workaround: `vector<vector<int> >` (space before the closing `>`). C++11 fixed this by adding context-sensitive tokenization â€” the lexer now cooperates with the parser to decide whether `>>` is two template-closing `>`s or a right-shift. This is a famous example of what happens when maximal munch produces the wrong result, and the language has to add complexity to compensate. Keep your language's token syntax unambiguous at the lexical level, and you will never face this problem.\n> ðŸ”­ **Deep Dive**: The formal theory behind maximal munch and regular language tokenization â€” including proofs of correctness and the relationship to DFAs â€” is covered in *Compilers: Principles, Techniques, and Tools* (Aho, Lam, Sethi, Ullman â€” the \"Dragon Book\"), Section 3.3: \"Recognition of Tokens.\" If you want to understand why your scanner is guaranteed to work correctly (not just in practice but in theory), this section provides the rigorous foundation.\n---\n## Summary: What You Have Built\nBy completing this milestone, your scanner now handles the full operator and literal vocabulary of your C-like language:\n- **Two-character operator recognition** via the `_match()` peek-and-consume helper. `==`, `!=`, `<=`, `>=` are scanned as single tokens. Single `=`, `!`, `<`, `>` are correctly emitted alone when not followed by `=`.\n- **Maximal munch applied universally**: `>==` tokenizes as `GREATER_EQ` + `ASSIGN`, never as `GREATER` + `EQUAL`. The scanner is greedy and never backtracks.\n- **Integer and float literal scanning** with two-character lookahead to handle the decimal point edge case. Trailing dots (`3.`) are not consumed; the dot is left for the next token.\n- **Identifier scanning** consuming letters, digits, and underscores in sequence.\n- **Keyword table lookup** distinguishing `if`, `else`, `while`, `return`, `true`, `false`, `null` from identifiers after the full lexeme is scanned. Prefix matches (`iffy`, `returnValue`) are correctly treated as identifiers.\n- **Position accuracy** maintained throughout: all tokens carry correct line and column corresponding to their first character.\nIn Milestone 3, you will add the two most complex scanning states: string literals (with escape sequences) and comments (both single-line `//` and multi-line `/* */`). These introduce a challenge you have not faced yet â€” the scanner must track what kind of thing it is *inside* (a string? a comment?) across many characters, and the meaning of `//` inside a string is completely different from `//` outside one.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m3 -->\n<!-- MS_ID: tokenizer-m3 -->\n# Milestone 3: Strings & Comments\n## Where You Are in the Pipeline\n\n![Tokenizer System Satellite Map â€” The Complete Pipeline](./diagrams/diag-satellite-map.svg)\n\nYour scanner can now handle every token in your C-like language *except* two: string literals and comments. At first glance, these seem simple â€” find the closing `\"`, find the closing `*/`, done. But that intuition is the misconception this milestone exists to correct.\nString literals and comments are not just \"longer tokens.\" They are the first point in your scanner where the *rules of scanning change depending on context*. Inside a string, a `/` is just the division character `'/'` â€” not the start of a comment. Inside a comment, a `\"` is just a character â€” not the start of a string. The same character, in two different positions, means two entirely different things. Your scanner needs to know *where it is* to interpret what it sees.\nThis is the scanner's first encounter with **context-sensitivity**, and handling it correctly requires extending your finite state machine with new states â€” mini state machines that take over while you are inside a string or comment, apply different rules, and eventually hand control back to the main scanner.\nBy the end of this milestone, your scanner will handle every character in a well-formed source file, plus gracefully diagnose malformed ones (unterminated strings and comments). The gap between Milestone 2 and a production-ready lexer is almost entirely closed here.\n---\n## The Revelation: It's Not a Search Problem\nHere is the mental model most developers start with:\n> *\"Scanning a string literal is easy â€” find the opening `\"`, scan forward until you find the closing `\"`, and return everything in between as the token. Comments are the same: find `/*`, scan until `*/`.\"*\nThis sounds right. Python itself has something like this:\n```python\n# \"What people think string scanning looks like\"\ndef scan_string_naive(source, start):\n    end = source.index('\"', start + 1)  # find closing quote\n    return source[start:end + 1]\n```\nFollow this model to its consequences and see where it breaks.\n**Scenario 1:** The source contains `\"hello\\\"world\"`. There is a `\\\"` â€” a backslash followed by a quote. Your naive scanner calls `source.index('\"', 1)` and finds the `\\\"` â€” the escaped quote â€” and stops. But that quote is *inside* the string. It is not the terminator. The naive search returns `\"hello\\\"`, which is incomplete. The real string is `\"hello\\\"world\"` â€” the backslash \"escapes\" the quote, turning it from a string terminator into a literal quote character.\n**Scenario 2:** The source contains `\"hello // world\"`. Your naive scanner will eventually reach this string. But if your comment-detection logic runs before string scanning, it will see `//` and strip out everything after it â€” including the closing `\"`. Now the string is unterminated. The comment detector has eaten part of the string because it did not know it was inside one.\n**Scenario 3:** A multi-line comment `/* starts here\\n... and ends here */`. Your naive `source.index(\"*/\")` approach would work â€” but while you are scanning past those newlines, you need to update `self.line`. A simple index-based search skips character-by-character traversal and therefore cannot update position counters.\n**The real model:** String and comment scanning are not search operations. They are **sub-state machines**. When your scanner enters a `\"`, it transitions into a new state â€” call it `IN_STRING` â€” where the rules change completely. In this state:\n- Every character is a potential part of the string content.\n- `\"` terminates the string (exit state).\n- `\\` initiates an escape sequence (enter another sub-state: `IN_ESCAPE`).\n- `//` and `/*` are just characters, not comment starters.\n- Newline or EOF terminates the string with an *error* (unterminated).\nOnly when the closing `\"` is found does the scanner leave `IN_STRING` and return to the normal `START` state.\nThis is the key insight: **the same character means different things depending on the current state.** And the state is determined by what the scanner has consumed so far â€” not what pattern the current character matches in isolation.\n\n![Context-Sensitivity: Same Character, Different Meaning](./diagrams/diag-m3-context-sensitivity.svg)\n\n---\n## Context-Sensitivity: A Formal Aside (Worth Two Minutes)\nYou have been building a **regular language** tokenizer â€” one that a finite state machine can handle. Regular languages are characterized by the fact that their recognition depends only on the current state, not on unbounded memory of what came before.\nHere is the formal tension: **string escaping technically pushes beyond regular languages**. A scanner must track whether the preceding character was `\\` to know if the current `\"` terminates the string. This \"memory\" of one preceding character might seem to require more than a finite state machine â€” but it does not. You simply add states. The state `IN_ESCAPE` captures the fact that a `\\` was just seen. This is a finite amount of memory (one bit of information: \"was the previous character a backslash?\"). A finite state machine can represent any finite amount of memory by encoding it into states.\nThe deeper point: the `\\` escape mechanism is **context-sensitive at the character level**. The character after `\\` is interpreted differently from any other character. When you implement `_scan_string()`, you are implementing this context-sensitivity explicitly â€” by changing what you do with each character based on the current scanning state (normal vs. after-backslash).\nThis is also why tokenizers are not \"purely regular\" in practice. The **lexical structure of real languages** â€” string escape sequences, here-documents, raw strings, backtick-quoted identifiers in SQL â€” all require some context tracking beyond a pure DFA. Production scanners handle this with flags and sub-states, exactly as you are about to do.\n---\n## The `/` Character: The Critical Branch\n\n![Comment Detection: The '/' Ambiguity](./diagrams/diag-m3-comment-scanning.svg)\n\nIn Milestone 2, you deliberately left `/` out of `SINGLE_CHAR_TOKENS` with a note that it would be handled in Milestone 3. Now you understand why: when the scanner sees `/`, it cannot immediately emit a `SLASH` (division) token because the next character might be `/` or `*` â€” starting a comment.\nThis is the same maximal munch pattern you used for `>=` in Milestone 2, extended to a three-way decision:\n| Seen so far | Next character | Decision |\n|---|---|---|\n| `/` | `/` | Single-line comment: skip to end of line |\n| `/` | `*` | Multi-line comment: skip to `*/` |\n| `/` | anything else | Division operator: emit `SLASH` |\nIn code, this is a natural extension of `next_token()`:\n```python\nif char == \"/\":\n    if self._match(\"/\"):\n        self._skip_line_comment()\n        return self.next_token()   # comments produce no token; recurse for next\n    elif self._match(\"*\"):\n        start_line = tok_line\n        start_col = tok_col\n        err = self._skip_block_comment(start_line, start_col)\n        if err is not None:\n            return err             # unterminated comment â†’ Error token\n        return self.next_token()   # comment consumed; get next token\n    else:\n        return Token(TokenType.SLASH, \"/\", tok_line, tok_col)\n```\nNotice the recursive call `return self.next_token()` after consuming a comment. Comments produce no tokens â€” they are discarded. After skipping a comment, the scanner needs to find the next *real* token. Calling `self.next_token()` again achieves this cleanly. The recursion depth is bounded: each call consumes at least one comment, so the recursion terminates when the source is exhausted or a non-comment token is found.\n> **Alternative to recursion**: some scanners use a loop inside `_skip_whitespace()` to also skip comments, so that both whitespace and comments are consumed before capturing `tok_line`/`tok_col`. Both approaches are correct. The recursive approach is more explicit about what is happening: a comment is \"transparent\" to the token stream.\n---\n## Single-Line Comments: Skipping to End of Line\nSingle-line comments begin with `//` (both `/` characters already consumed by the time we enter this logic) and extend to the end of the current line. \"End of line\" means the character `\\n` or end of file â€” the newline itself is *not* consumed by the comment skip (it will be consumed as whitespace by `_skip_whitespace()` on the next call).\n```python\ndef _skip_line_comment(self) -> None:\n    \"\"\"\n    Skip characters until end of line or end of file.\n    The '//' has already been consumed. Newline is NOT consumed here.\n    \"\"\"\n    while not self.is_at_end() and self.peek() != \"\\n\":\n        self.advance()\n```\nThis is the simplest scanning method in the project: advance as long as you are not at a newline or end-of-file. The newline will be consumed by `_skip_whitespace()` on the next `next_token()` call, which will then correctly increment `self.line`.\nWhy not consume the newline here? Because the newline belongs to the whitespace layer, not the comment layer. Separating concerns â€” \"comments skip content, whitespace handles line endings\" â€” keeps each method focused. It also means `_skip_line_comment()` never touches `self.line`, avoiding any risk of double-counting newlines.\n**The invisible case:** What if a `//` comment appears at the very end of the file with no trailing newline? `self.peek() != \"\\n\"` will never be true, but `self.is_at_end()` will become true and exit the loop. This is handled correctly â€” no special case needed.\n---\n## Multi-Line Comments: The State Machine Within a State Machine\n\n![Trace: Multi-line Comment with Line Tracking](./diagrams/diag-m3-multiline-comment-trace.svg)\n\nMulti-line comments are more complex for three reasons:\n1. **Two-character delimiter**: The comment ends with `*/`, not just `*` or `/`. You need to track \"did I just see `*`?\" to know when a subsequent `/` closes the comment.\n2. **Line tracking**: The comment can span multiple lines. Every `\\n` inside the comment must update `self.line` â€” even though you are not emitting any tokens.\n3. **Unterminated detection**: If the file ends before `*/` is found, you must emit an Error token at the position of the opening `/*` â€” not at the end of the file.\nThe implementation uses a small two-state machine: either you are reading normally inside the comment, or you just saw a `*` and are waiting to see if the next character is `/`.\n```python\ndef _skip_block_comment(self, start_line: int, start_col: int) -> \"Token | None\":\n    \"\"\"\n    Skip a block comment. '/*' has already been consumed.\n    Returns None on success, Error token on unterminated comment.\n    Line/column tracking: every character is consumed via advance(),\n    so newlines inside the comment correctly update self.line.\n    \"\"\"\n    while not self.is_at_end():\n        char = self.advance()\n        if char == \"*\" and self.peek() == \"/\":\n            self.advance()   # consume the closing '/'\n            return None      # comment closed successfully\n    # Reached end of file without finding '*/'\n    return Token(TokenType.ERROR, \"/*\", start_line, start_col)\n```\nWalk through the logic:\n- `advance()` consumes each character â€” this is why line tracking works. `advance()` handles `\\n` detection and updates `self.line`.\n- After consuming `*`, `peek()` checks if the next character is `/` without consuming it.\n- If yes, `advance()` consumes the `/` and returns `None` (success).\n- If no, the loop continues â€” the `*` was part of the comment content, not the closing delimiter.\n- If `is_at_end()` becomes true without having returned, the comment was unterminated. Return an Error token with the position of the opening `/*`.\n**The non-nesting rule:** Multi-line comments in your language do *not* nest. `/* outer /* inner */ still in comment? */` â€” the comment ends at the *first* `*/`, leaving `still in comment? */` as source code (likely generating errors). Your implementation above naturally achieves this: it stops at the first `*/` it encounters, regardless of how many `/*` are inside. There is no counter. There are no nested states.\n> **Why no nesting?** Allowing nested comments (`/* /* */ */`) is attractive but requires a counter (`depth++` on `/*`, `depth--` on `*/`, stop when depth reaches 0). This counter is theoretically unbounded â€” which means a nested-comment scanner is no longer a finite state machine; it requires a pushdown automaton (PDA). This is the difference between regular languages (FSM, O(1) memory) and context-free languages (PDA, stack memory). Languages like Haskell and D support nested block comments, but they do so at a cost: the lexer is no longer purely regular. For your C-like language, following C's convention of non-nesting keeps the lexer in the regular language class. Document this in your code.\n---\n## Escape Sequences: The Two-Character Dance\n\n![String Literal Scanner â€” State Machine with Escape Handling](./diagrams/diag-m3-string-fsm.svg)\n\nString literal scanning introduces a new sub-state: the **escape state**. When the scanner is inside a string and encounters a `\\`, the next character does not have its normal meaning â€” it is the second half of an escape sequence. The pair `\\n` does not mean backslash then n; it means the newline character (ASCII 10).\nYour language supports these escape sequences:\n| Escape sequence | Meaning | Character produced |\n|---|---|---|\n| `\\n` | Newline | `\\n` (ASCII 10) |\n| `\\t` | Tab | `\\t` (ASCII 9) |\n| `\\r` | Carriage return | `\\r` (ASCII 13) |\n| `\\\"` | Literal double quote | `\"` |\n| `\\\\` | Literal backslash | `\\` |\nThe implementation requires a decision about what the `Token.lexeme` field stores. There are two choices:\n**Option A â€” Raw lexeme**: `lexeme` stores the exact source characters, including escape sequences. For the source `\"hello\\nworld\"`, the lexeme is the string `'\"hello\\\\nworld\"'` (14 characters including the backslash, the n, and the quotes). The downstream parser or interpreter is responsible for interpreting escape sequences later.\n**Option B â€” Processed value**: `lexeme` stores the interpreted string content. For `\"hello\\nworld\"`, the lexeme is `'\"hello\\nworld\"'` where the `\\n` is the actual newline character (13 characters plus the quotes).\nThe tradeoff:\n| Option | Pros | Cons |\n|---|---|---|\n| **Raw lexeme** | Faithful to source; error messages show what user typed | Parser must do escape processing; two places that understand escapes |\n| **Processed value** | Interpreter can use lexeme directly | Error positions inside strings are harder; loss of original source text |\nFor this project, use **raw lexeme** â€” store exactly what appeared in the source. The lexeme of `\"hello\\nworld\"` is the literal source text `\"hello\\nworld\"` including the backslash and `n`. Escape interpretation happens at a higher level (the interpreter or constant-folding pass). This is the approach used by CPython's tokenizer, Go's scanner, and LLVM's Clang.\n> **What to validate at scan time?** Even with raw lexemes, the scanner must validate that escape sequences are *legal*. If the source contains `\"\\q\"` â€” backslash followed by `q`, which is not a valid escape â€” the scanner should emit an Error token. Unknown escapes are not silently passed through; they are caught here.\n\n![Trace: Scanning 'hello\\nworld' with Escape Sequence](./diagrams/diag-m3-escape-trace.svg)\n\nHere is the complete string scanning method:\n```python\ndef _scan_string(self, tok_line: int, tok_col: int) -> Token:\n    \"\"\"\n    Scan a string literal. The opening '\"' has already been consumed.\n    tok_line and tok_col point to the opening quote position.\n    Returns a STRING token with the full lexeme including quotes,\n    or an ERROR token if the string is unterminated or contains an invalid escape.\n    \"\"\"\n    lexeme_chars = ['\"']   # start with the opening quote\n    while True:\n        if self.is_at_end():\n            # EOF before closing quote â€” unterminated string\n            return Token(TokenType.ERROR, \"\".join(lexeme_chars), tok_line, tok_col)\n        char = self.advance()\n        if char == '\"':\n            # Closing quote found â€” string complete\n            lexeme_chars.append('\"')\n            return Token(TokenType.STRING, \"\".join(lexeme_chars), tok_line, tok_col)\n        if char == '\\n':\n            # Newline before closing quote â€” unterminated string\n            # Note: self.line has already been updated by advance()\n            # The Error token reports the OPENING quote position\n            return Token(TokenType.ERROR, \"\".join(lexeme_chars), tok_line, tok_col)\n        if char == '\\\\':\n            # Escape sequence â€” consume and validate the next character\n            lexeme_chars.append('\\\\')\n            if self.is_at_end():\n                # Backslash at EOF â€” unterminated\n                return Token(TokenType.ERROR, \"\".join(lexeme_chars), tok_line, tok_col)\n            escape_char = self.advance()\n            if escape_char not in ('n', 't', 'r', '\"', '\\\\'):\n                # Invalid escape sequence â€” emit error at opening quote position\n                # Could alternatively emit at the backslash position; document your choice\n                lexeme_chars.append(escape_char)\n                return Token(TokenType.ERROR, \"\".join(lexeme_chars), tok_line, tok_col)\n            lexeme_chars.append(escape_char)\n            continue\n        # Normal character inside string\n        lexeme_chars.append(char)\n    # Unreachable â€” loop exits via return in all cases\n```\n> **Why `list` then `\"\".join()` instead of `lexeme += char`?** In `_scan_number()` and `_scan_identifier()`, you used `lexeme += char`. For short tokens (numbers, identifiers), this is fine. For strings, which can be arbitrarily long, string concatenation inside a loop creates O(nÂ²) work â€” each `+=` allocates a new string and copies the old one. Using a `list` as a character buffer and joining at the end is O(n). In Python, `\"\".join(list_of_chars)` is the idiomatic efficient string-building pattern for loops. For a 10,000-character string, the difference is measurable.\n### The Unterminated String: Blame the Cause, Not the Symptom\nNotice that when an unterminated string is detected â€” whether because of EOF or a newline â€” the Error token's position (`tok_line`, `tok_col`) points to the **opening quote**, not to where the scanner stopped. This is deliberate.\nThink about the user experience. If a programmer writes:\n```\nif (x > 0) {\n    message = \"error occurred\n    return false;\n}\n```\nThe unterminated string starts on line 3. The EOF (or the next line) is where the scanner *discovers* the problem, but the *cause* is on line 3 â€” the unclosed `\"`. An error message saying \"unterminated string at line 5, column 1\" (pointing at `return`) would be useless. \"Unterminated string at line 3, column 15\" (pointing at the opening `\"`) is actionable.\nThis is a general principle in error reporting: **report the position of the cause, not the position where the consequence was detected.** The scanner detects the problem when it runs out of valid string content. It reports the problem where the string began.\nThe same principle applies to multi-line comments: the Error token for an unterminated `/* ... */` reports the position of the opening `/*`, even if the scanner consumed thousands of characters before realizing no `*/` was coming.\n---\n## Integrating String and Comment Scanning into next_token()\nHere is the updated `next_token()` incorporating all Milestone 3 additions. The changes are concentrated in two places: the `/` handling and the `\"` handling.\n```python\ndef next_token(self) -> Token:\n    self._skip_whitespace()\n    tok_line = self.line\n    tok_col  = self.column\n    if self.is_at_end():\n        return Token(TokenType.EOF, \"\", tok_line, tok_col)\n    char = self.advance()\n    # â”€â”€ Single-character unambiguous tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if char in SINGLE_CHAR_TOKENS:\n        return Token(SINGLE_CHAR_TOKENS[char], char, tok_line, tok_col)\n    # â”€â”€ Two-character operators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if char in OPERATOR_CHARS:\n        return self._scan_operator(char, tok_line, tok_col)\n    # â”€â”€ Division operator or comment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if char == \"/\":\n        if self._match(\"/\"):\n            # Single-line comment: skip to end of line, get next token\n            self._skip_line_comment()\n            return self.next_token()\n        if self._match(\"*\"):\n            # Multi-line comment: skip to */, get next token (or error)\n            err = self._skip_block_comment(tok_line, tok_col)\n            if err is not None:\n                return err\n            return self.next_token()\n        # Just a division operator\n        return Token(TokenType.SLASH, \"/\", tok_line, tok_col)\n    # â”€â”€ String literal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if char == '\"':\n        return self._scan_string(tok_line, tok_col)\n    # â”€â”€ Number literals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if char.isdigit():\n        return self._scan_number(char, tok_line, tok_col)\n    # â”€â”€ Identifiers and keywords â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if char.isalpha() or char == \"_\":\n        return self._scan_identifier(char, tok_line, tok_col)\n    # â”€â”€ Unrecognized character â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    return Token(TokenType.ERROR, char, tok_line, tok_col)\n```\nThe order of checks matters. The `\"/\"` check must come after the single-character and operator checks (since `+`, `-`, `*` are not ambiguous), and before the number and identifier checks (since `/` is neither a digit nor a letter).\n---\n## Line Number Tracking Inside Strings and Comments\n\n![Trace: Multi-line Comment with Line Tracking](./diagrams/diag-m3-multiline-comment-trace.svg)\n\nBoth `_scan_string()` and `_skip_block_comment()` consume characters using `self.advance()`. Because `advance()` handles newline detection (incrementing `self.line` and resetting `self.column`), line tracking works automatically inside both constructs â€” you do not need any special handling.\nHere is a concrete trace to verify your intuition. Suppose the source is:\n```\nx = /* first\n  second */ y\n```\n```\nScanning starts: line=1, col=1\nadvance() 'x'  â†’ line=1, col=2\nadvance() ' '  â†’ line=1, col=3   (whitespace, skipped)\nadvance() '='  â†’ line=1, col=4   â†’ Token(ASSIGN, \"=\", 1, 3)\nadvance() ' '  â†’ line=1, col=5   (whitespace, skipped)\n-- next_token() sees '/' at line=1, col=5 --\ntok_line=1, tok_col=5\nadvance() '/'  â†’ line=1, col=6\n_match('*')    â†’ True, advance() '*' â†’ line=1, col=7\n_skip_block_comment(start_line=1, start_col=5):\n  advance() ' '   â†’ line=1, col=8\n  advance() 'f'   â†’ line=1, col=9\n  advance() 'i'   â†’ line=1, col=10\n  advance() 'r'   â†’ line=1, col=11\n  advance() 's'   â†’ line=1, col=12\n  advance() 't'   â†’ line=1, col=13\n  advance() '\\n'  â†’ line=2, col=1    â† LINE INCREMENTED\n  advance() ' '   â†’ line=2, col=2\n  advance() ' '   â†’ line=2, col=3\n  advance() 's'   â†’ line=2, col=4\n  ...\n  advance() '*'   â†’ line=2, col=10\n  peek() == '/'   â†’ True\n  advance() '/'   â†’ line=2, col=11\n  return None   â† comment closed\nnext_token() recurses:\nadvance() ' '   â†’ line=2, col=12  (whitespace)\ntok_line=2, tok_col=12\nadvance() 'y'   â†’ line=2, col=13\nâ†’ Token(IDENTIFIER, \"y\", 2, 12)\n```\nThe token `y` correctly reports line 2, column 12 â€” even though it was scanned long after the comment started on line 1. Every `advance()` call inside the comment handled newline detection. You did not write a single special case for this.\nThis is the payoff of the `advance()` abstraction: all callers benefit from correct position tracking without having to think about it.\n---\n## The Complete Milestone 3 Scanner\nHere is the full `scanner.py` with all three milestones integrated:\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Optional\nclass TokenType(Enum):\n    # Literals\n    NUMBER     = auto()\n    STRING     = auto()\n    # Names\n    IDENTIFIER = auto()\n    KEYWORD    = auto()\n    # Operators\n    PLUS       = auto()\n    MINUS      = auto()\n    STAR       = auto()\n    SLASH      = auto()\n    ASSIGN     = auto()   # =\n    EQUAL      = auto()   # ==\n    NOT_EQUAL  = auto()   # !=\n    LESS       = auto()   # <\n    LESS_EQ    = auto()   # <=\n    GREATER    = auto()   # >\n    GREATER_EQ = auto()   # >=\n    BANG       = auto()   # !\n    # Punctuation\n    LPAREN     = auto()\n    RPAREN     = auto()\n    LBRACE     = auto()\n    RBRACE     = auto()\n    LBRACKET   = auto()\n    RBRACKET   = auto()\n    SEMICOLON  = auto()\n    COMMA      = auto()\n    # Sentinels\n    EOF        = auto()\n    ERROR      = auto()\n@dataclass\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\n# â”€â”€ Module-level constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSINGLE_CHAR_TOKENS: dict[str, TokenType] = {\n    \"+\": TokenType.PLUS,\n    \"-\": TokenType.MINUS,\n    \"*\": TokenType.STAR,\n    \"(\": TokenType.LPAREN,\n    \")\": TokenType.RPAREN,\n    \"{\": TokenType.LBRACE,\n    \"}\": TokenType.RBRACE,\n    \"[\": TokenType.LBRACKET,\n    \"]\": TokenType.RBRACKET,\n    \";\": TokenType.SEMICOLON,\n    \",\": TokenType.COMMA,\n}\nOPERATOR_CHARS: frozenset[str] = frozenset({\"=\", \"!\", \"<\", \">\"})\nWHITESPACE: frozenset[str] = frozenset({\" \", \"\\t\", \"\\r\", \"\\n\"})\nKEYWORDS: dict[str, TokenType] = {\n    \"if\":     TokenType.KEYWORD,\n    \"else\":   TokenType.KEYWORD,\n    \"while\":  TokenType.KEYWORD,\n    \"return\": TokenType.KEYWORD,\n    \"true\":   TokenType.KEYWORD,\n    \"false\":  TokenType.KEYWORD,\n    \"null\":   TokenType.KEYWORD,\n}\nVALID_ESCAPES: frozenset[str] = frozenset({\"n\", \"t\", \"r\", '\"', \"\\\\\"})\n# â”€â”€ Scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass Scanner:\n    def __init__(self, source: str) -> None:\n        self.source: str = source\n        self.current: int = 0\n        self.line: int = 1\n        self.column: int = 1\n    # â”€â”€ Primitive operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def is_at_end(self) -> bool:\n        return self.current >= len(self.source)\n    def peek(self) -> str:\n        if self.is_at_end():\n            return \"\\0\"\n        return self.source[self.current]\n    def _peek_next(self) -> str:\n        if self.current + 1 >= len(self.source):\n            return \"\\0\"\n        return self.source[self.current + 1]\n    def advance(self) -> str:\n        char = self.source[self.current]\n        self.current += 1\n        if char == \"\\n\":\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n        return char\n    def _match(self, expected: str) -> bool:\n        if self.is_at_end():\n            return False\n        if self.source[self.current] != expected:\n            return False\n        self.advance()\n        return True\n    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _skip_whitespace(self) -> None:\n        while not self.is_at_end() and self.peek() in WHITESPACE:\n            self.advance()\n    # â”€â”€ Comment skipping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _skip_line_comment(self) -> None:\n        \"\"\"Skip to end of line. '//' already consumed. Newline NOT consumed.\"\"\"\n        while not self.is_at_end() and self.peek() != \"\\n\":\n            self.advance()\n    def _skip_block_comment(\n        self, start_line: int, start_col: int\n    ) -> Optional[Token]:\n        \"\"\"\n        Skip block comment body. '/*' already consumed.\n        Returns None on success, Error token at opening '/*' if unterminated.\n        Non-nesting: stops at the FIRST '*/' encountered.\n        \"\"\"\n        while not self.is_at_end():\n            char = self.advance()\n            if char == \"*\" and self.peek() == \"/\":\n                self.advance()   # consume closing '/'\n                return None\n        # Unterminated â€” report at opening '/*'\n        return Token(TokenType.ERROR, \"/*\", start_line, start_col)\n    # â”€â”€ Scanning methods â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_string(self, tok_line: int, tok_col: int) -> Token:\n        \"\"\"\n        Scan string literal. Opening '\"' already consumed.\n        Lexeme includes surrounding quotes and raw escape sequences.\n        Unterminated (EOF or bare newline) â†’ Error token at opening quote.\n        Invalid escape sequence â†’ Error token at opening quote.\n        \"\"\"\n        lexeme_chars: list[str] = ['\"']\n        while True:\n            if self.is_at_end():\n                return Token(TokenType.ERROR, \"\".join(lexeme_chars), tok_line, tok_col)\n            char = self.advance()\n            if char == '\"':\n                lexeme_chars.append('\"')\n                return Token(TokenType.STRING, \"\".join(lexeme_chars), tok_line, tok_col)\n            if char == \"\\n\":\n                # Bare newline terminates string as an error\n                return Token(TokenType.ERROR, \"\".join(lexeme_chars), tok_line, tok_col)\n            if char == \"\\\\\":\n                lexeme_chars.append(\"\\\\\")\n                if self.is_at_end():\n                    return Token(TokenType.ERROR, \"\".join(lexeme_chars), tok_line, tok_col)\n                escape_char = self.advance()\n                lexeme_chars.append(escape_char)\n                if escape_char not in VALID_ESCAPES:\n                    # Invalid escape â€” report error at opening quote\n                    return Token(TokenType.ERROR, \"\".join(lexeme_chars), tok_line, tok_col)\n                continue\n            lexeme_chars.append(char)\n    def _scan_number(self, first_digit: str, tok_line: int, tok_col: int) -> Token:\n        lexeme = first_digit\n        while not self.is_at_end() and self.peek().isdigit():\n            lexeme += self.advance()\n        if self.peek() == \".\" and self._peek_next().isdigit():\n            lexeme += self.advance()   # consume '.'\n            while not self.is_at_end() and self.peek().isdigit():\n                lexeme += self.advance()\n        return Token(TokenType.NUMBER, lexeme, tok_line, tok_col)\n    def _scan_identifier(self, first_char: str, tok_line: int, tok_col: int) -> Token:\n        lexeme = first_char\n        while not self.is_at_end() and (self.peek().isalnum() or self.peek() == \"_\"):\n            lexeme += self.advance()\n        token_type = KEYWORDS.get(lexeme, TokenType.IDENTIFIER)\n        return Token(token_type, lexeme, tok_line, tok_col)\n    def _scan_operator(self, char: str, tok_line: int, tok_col: int) -> Token:\n        if char == \"=\":\n            return Token(\n                TokenType.EQUAL if self._match(\"=\") else TokenType.ASSIGN,\n                \"==\" if self.source[self.current - 1] == \"=\" and\n                        self.current >= 2 and\n                        self.source[self.current - 2] == \"=\" else \"=\",\n                tok_line, tok_col,\n            )\n        # Simpler explicit version:\n        if char == \"=\":\n            if self._match(\"=\"):\n                return Token(TokenType.EQUAL, \"==\", tok_line, tok_col)\n            return Token(TokenType.ASSIGN, \"=\", tok_line, tok_col)\n        if char == \"!\":\n            if self._match(\"=\"):\n                return Token(TokenType.NOT_EQUAL, \"!=\", tok_line, tok_col)\n            return Token(TokenType.BANG, \"!\", tok_line, tok_col)\n        if char == \"<\":\n            if self._match(\"=\"):\n                return Token(TokenType.LESS_EQ, \"<=\", tok_line, tok_col)\n            return Token(TokenType.LESS, \"<\", tok_line, tok_col)\n        if char == \">\":\n            if self._match(\"=\"):\n                return Token(TokenType.GREATER_EQ, \">=\", tok_line, tok_col)\n            return Token(TokenType.GREATER, \">\", tok_line, tok_col)\n        return Token(TokenType.ERROR, char, tok_line, tok_col)\n    # â”€â”€ Token production â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def next_token(self) -> Token:\n        self._skip_whitespace()\n        tok_line = self.line\n        tok_col  = self.column\n        if self.is_at_end():\n            return Token(TokenType.EOF, \"\", tok_line, tok_col)\n        char = self.advance()\n        if char in SINGLE_CHAR_TOKENS:\n            return Token(SINGLE_CHAR_TOKENS[char], char, tok_line, tok_col)\n        if char in OPERATOR_CHARS:\n            return self._scan_operator(char, tok_line, tok_col)\n        if char == \"/\":\n            if self._match(\"/\"):\n                self._skip_line_comment()\n                return self.next_token()\n            if self._match(\"*\"):\n                err = self._skip_block_comment(tok_line, tok_col)\n                if err is not None:\n                    return err\n                return self.next_token()\n            return Token(TokenType.SLASH, \"/\", tok_line, tok_col)\n        if char == '\"':\n            return self._scan_string(tok_line, tok_col)\n        if char.isdigit():\n            return self._scan_number(char, tok_line, tok_col)\n        if char.isalpha() or char == \"_\":\n            return self._scan_identifier(char, tok_line, tok_col)\n        return Token(TokenType.ERROR, char, tok_line, tok_col)\n    def scan_tokens(self) -> list[Token]:\n        tokens: list[Token] = []\n        while True:\n            token = self.next_token()\n            tokens.append(token)\n            if token.type == TokenType.EOF:\n                break\n        return tokens\n```\n> **Note on the `_scan_operator` method above:** The first `if char == \"=\"` block contains a demonstration-then-replacement comment pattern to show the right approach. In your actual code, use the clean explicit version (the one with the plain `if self._match(\"=\")` pattern from Milestone 2). The tangled first version is shown only to highlight why the explicit form is clearer.\n---\n## The Complete FSM View\n\n![Complete Scanner FSM â€” All States](./diagrams/diag-m3-full-fsm.svg)\n\nAfter Milestone 3, your scanner's state machine has the following explicit states:\n| State | Entered when | Exit condition |\n|---|---|---|\n| **START** | Between tokens | Always â€” dispatch on first char |\n| **IN_STRING** | `\"` consumed | Closing `\"`, newline (error), or EOF (error) |\n| **IN_ESCAPE** | `\\` inside string | One character consumed (valid or invalid) |\n| **IN_NUMBER** | Digit consumed | Non-digit, non-dot, or dot without following digit |\n| **IN_FLOAT** | `.` after digits consumed | Non-digit |\n| **IN_IDENTIFIER** | Letter or `_` consumed | Non-alphanumeric, non-`_` |\n| **IN_OPERATOR** | `=`, `!`, `<`, `>` consumed | One character peeked/consumed |\n| **IN_LINE_COMMENT** | `//` consumed | `\\n` or EOF |\n| **IN_BLOCK_COMMENT** | `/*` consumed | `*/` or EOF (error) |\nIn your code, these states are not explicit enum values â€” they are represented by which method is currently executing. The call stack *is* the state. `_scan_string()` executing means you are in `IN_STRING`. `_skip_block_comment()` executing means you are in `IN_BLOCK_COMMENT`. This is the recursive descent encoding of a state machine: states become functions, transitions become calls.\n---\n## Testing Milestone 3\nTests for strings and comments require verifying not just that the right token type is emitted, but that the lexeme is correct, positions are accurate, and unterminated constructs produce errors at the right position.\n```python\n# â”€â”€ String literal tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_simple_string():\n    scanner = Scanner('\"hello\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.STRING, '\"hello\"', 1, 1)\n    assert tokens[1].type == TokenType.EOF\ndef test_empty_string():\n    scanner = Scanner('\"\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.STRING, '\"\"', 1, 1)\ndef test_string_with_escape_newline():\n    scanner = Scanner('\"hello\\\\nworld\"')\n    tokens = scanner.scan_tokens()\n    # Raw lexeme: the source characters '\"hello\\nworld\"' (with literal backslash-n)\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"hello\\\\nworld\"'\ndef test_string_with_escaped_quote():\n    scanner = Scanner('\"say \\\\\"hi\\\\\"\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"say \\\\\"hi\\\\\"\"'\ndef test_string_with_escaped_backslash():\n    scanner = Scanner('\"path\\\\\\\\file\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"path\\\\\\\\file\"'\ndef test_all_valid_escapes():\n    scanner = Scanner('\"\\\\n\\\\t\\\\r\\\\\"\\\\\\\\\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\ndef test_unterminated_string_eof():\n    \"\"\"EOF before closing quote â†’ Error token at opening quote position.\"\"\"\n    scanner = Scanner('\"hello')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1   # opening quote position\ndef test_unterminated_string_newline():\n    \"\"\"Bare newline inside string â†’ Error token at opening quote position.\"\"\"\n    scanner = Scanner('\"hello\\nworld\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1   # opening quote on line 1\ndef test_invalid_escape():\n    \"\"\"'\\\\q' is not a valid escape â€” Error token.\"\"\"\n    scanner = Scanner('\"\\\\q\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\ndef test_backslash_at_eof():\n    \"\"\"String with trailing backslash at EOF â€” unterminated.\"\"\"\n    scanner = Scanner('\"hello\\\\')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\ndef test_comment_inside_string_not_a_comment():\n    \"\"\"// inside a string is part of the string content, not a comment.\"\"\"\n    scanner = Scanner('\"hello // world\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"hello // world\"'\ndef test_block_comment_markers_inside_string():\n    \"\"\"/* and */ inside a string are string content.\"\"\"\n    scanner = Scanner('\"/* not a comment */\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"/* not a comment */\"'\ndef test_string_position_accuracy():\n    \"\"\"Position of string token is the opening quote.\"\"\"\n    scanner = Scanner('x = \"hello\"')\n    tokens = scanner.scan_tokens()\n    string_token = next(t for t in tokens if t.type == TokenType.STRING)\n    assert string_token.line == 1\n    assert string_token.column == 5   # the '\"' is at column 5\n# â”€â”€ Comment tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_line_comment_produces_no_token():\n    \"\"\"// comment is completely invisible to the token stream.\"\"\"\n    scanner = Scanner(\"x // this is a comment\\ny\")\n    tokens = scanner.scan_tokens()\n    types = [t.type for t in tokens]\n    assert types == [TokenType.IDENTIFIER, TokenType.IDENTIFIER, TokenType.EOF]\n    assert tokens[0].lexeme == \"x\"\n    assert tokens[1].lexeme == \"y\"\ndef test_line_comment_at_eof():\n    \"\"\"Line comment with no trailing newline â€” scanner reaches EOF cleanly.\"\"\"\n    scanner = Scanner(\"// comment with no newline\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\ndef test_line_comment_does_not_consume_newline():\n    \"\"\"Token after comment starts on a new line, not the comment's line.\"\"\"\n    scanner = Scanner(\"// comment\\nx\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].line == 2   # 'x' is on line 2\n    assert tokens[0].column == 1\ndef test_slash_alone_is_division():\n    \"\"\"'/' not followed by '/' or '*' is a SLASH token.\"\"\"\n    scanner = Scanner(\"a / b\")\n    tokens = scanner.scan_tokens()\n    assert tokens[1] == Token(TokenType.SLASH, \"/\", 1, 3)\ndef test_block_comment_produces_no_token():\n    \"\"\"/* comment */ is invisible to the token stream.\"\"\"\n    scanner = Scanner(\"x /* ignored */ y\")\n    tokens = scanner.scan_tokens()\n    types = [t.type for t in tokens]\n    assert types == [TokenType.IDENTIFIER, TokenType.IDENTIFIER, TokenType.EOF]\ndef test_block_comment_updates_line_numbers():\n    \"\"\"Line numbers are tracked inside multi-line block comments.\"\"\"\n    scanner = Scanner(\"x /* line1\\nline2\\nline3 */ y\")\n    tokens = scanner.scan_tokens()\n    y_token = tokens[1]\n    assert y_token.lexeme == \"y\"\n    assert y_token.line == 3   # 'y' is on line 3 after two newlines in comment\ndef test_block_comment_non_nesting():\n    \"\"\"/* /* */ ends at the FIRST */ â€” second /* is not matched.\"\"\"\n    scanner = Scanner(\"/* /* */ x\")\n    tokens = scanner.scan_tokens()\n    # Comment ends at first */; 'x' is scanned normally\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"x\", 1, 10)\ndef test_unterminated_block_comment():\n    \"\"\"/* with no closing */ â†’ Error token at opening /* position.\"\"\"\n    scanner = Scanner(\"x /* unterminated\")\n    tokens = scanner.scan_tokens()\n    # First token is 'x', second is Error for unterminated comment\n    assert tokens[0].type == TokenType.IDENTIFIER\n    error = tokens[1]\n    assert error.type == TokenType.ERROR\n    assert error.lexeme == \"/*\"\n    assert error.line == 1\n    assert error.column == 3   # '/*' starts at column 3\ndef test_unterminated_comment_reports_opening_position():\n    \"\"\"Multi-line unterminated comment: error reports at '/*', not at EOF.\"\"\"\n    scanner = Scanner(\"a\\nb\\n/* starts here\\nand never ends\")\n    tokens = scanner.scan_tokens()\n    error_tokens = [t for t in tokens if t.type == TokenType.ERROR]\n    assert len(error_tokens) == 1\n    assert error_tokens[0].line == 3    # line where /* appears\n    assert error_tokens[0].column == 1  # column where /* appears\ndef test_adjacent_line_comments():\n    \"\"\"Multiple line comments on consecutive lines, all ignored.\"\"\"\n    scanner = Scanner(\"// line 1\\n// line 2\\nx\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2   # x + EOF\n    assert tokens[0].lexeme == \"x\"\n    assert tokens[0].line == 3\ndef test_string_after_block_comment():\n    \"\"\"Token positions after a multi-line block comment are correct.\"\"\"\n    scanner = Scanner('/* comment\\n */ \"hello\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].line == 2   # string is on line 2\ndef test_comment_between_tokens():\n    \"\"\"Comments between tokens do not affect surrounding token positions.\"\"\"\n    scanner = Scanner(\"x /* ignored */ + y\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"x\", 1, 1)\n    assert tokens[1] == Token(TokenType.PLUS,       \"+\", 1, 17)\n    assert tokens[2] == Token(TokenType.IDENTIFIER, \"y\", 1, 19)\n```\nRun: `python -m pytest test_scanner.py -v`\n---\n## Common Pitfalls\n**1. Consuming the closing quote as part of the next token**\nThe string scanner's closing-quote branch does `lexeme_chars.append('\"')` and then returns. After this return, `self.current` points at the character *after* the `\"`. The next call to `next_token()` correctly starts from there. The bug occurs if you forget to consume the closing quote â€” if the loop's exit condition is `peek() == '\"'` rather than `advance()` returning `'\"'`. Then `self.current` still points at `\"` and the next token is an empty string or a new string starting from the same quote.\n**2. The escape-backslash-at-end-of-string bug**\nThe source `\"hello\\\"` â€” a string that ends with a backslash *and* a quote. The scanner sees `\\`, enters escape handling, consumes `\"`. It just consumed the closing quote as an escape character. The loop continues, finds EOF, and returns an Error token. This is correct behavior: `\"hello\\\"` is unterminated (the `\\\"` is an escaped quote, not the string terminator). Your implementation handles this naturally because the `\\\\` branch consumes the next character unconditionally via `advance()`, not via the main loop.\n**3. Multi-line comment starting with `/*` where `*` advances `self.current`**\nIf `_skip_block_comment()` checks `if char == \"*\"` and peeks at the next character, it must then *consume* the `/` with `self.advance()`. If it returns without consuming the `/`, the next call to `next_token()` sees `/`, emits `SLASH`, and your token stream is corrupted. The implementation above calls `self.advance()` explicitly on the closing `/`.\n**4. Column counting inside strings with escape sequences**\nThe lexeme `\"hello\\n\"` in source text is 9 characters: `\"`, `h`, `e`, `l`, `l`, `o`, `\\`, `n`, `\"`. The scanner advances through 9 source characters, incrementing `self.column` for each. The `\\n` in the source is a backslash followed by `n` â€” not an actual newline â€” so `self.line` is not incremented. If you mistakenly process the backslash as producing an actual newline character and feed it to `advance()`'s newline detection, you would incorrectly increment the line counter. The fix: in `_scan_string()`, you call `self.advance()` to consume the raw source characters. The escape *interpretation* (turning `\\n` into `chr(10)`) happens elsewhere (in the interpreter), not here.\n**5. `/` inside a string triggering comment detection**\nThe string `\"http://example.com\"` contains `//`. If your scanner checks for `//` before it checks whether it is inside a string, it would incorrectly start a line comment. Your `next_token()` only reaches the `/` handling code for characters that are not inside strings â€” the `/` inside `\"http://example.com\"` is consumed by `_scan_string()` as part of the string content, long before `next_token()` has a chance to see it. The state machine architecture prevents this bug by design: `_scan_string()` has taken over; `next_token()` is not involved until the closing `\"` is found.\n**6. Block comment ending with `**/`**\nThe source `/** body **/` (two asterisks before closing slash). Your scanner sees `*` in the loop, peeks `/` â€” match! â€” consumes `/`, closes comment. The extra `*` before the `*` that matched was just consumed as comment content. The final `*` in `**/` is the one that matched. This is correct per the spec: `*/` ends the comment, regardless of how many `*` precede it. No special handling needed.\n---\n## Knowledge Cascade: One Insight, Many Domains\n**1. Sub-states within larger FSMs are everywhere â€” network protocols, game engines, UI.**\nTCP's `ESTABLISHED` state contains an entire sub-state machine for handling in-order delivery, retransmission timeouts, and window scaling. A video game's `PLAYING` state contains sub-states for `WALKING`, `ATTACKING`, `JUMPING`. A UI framework's `ACTIVE` state contains sub-states for `FOCUSED`, `HOVERED`, `DRAGGING`. In every case, the pattern is the same: the outer state machine delegates to an inner one, which eventually hands control back. Your `_scan_string()` is the \"inner state machine\" that takes over when `next_token()` delegates string handling to it.\n**2. The escape character pattern recurs in every text processing system.**\nSQL escaping: `'O''Brien'` uses a doubled quote to represent a literal single quote inside a string â€” same problem, different syntax. URL percent-encoding: `%20` represents a space â€” backslash replaced by `%`. Shell scripts: `echo \"hello\\nworld\"` vs `echo 'hello\\nworld\"` â€” single quotes prevent all escaping. Regex: `\\.` matches a literal dot, not \"any character.\" In every case, a special character (backslash, `%`, doubled quote) signals \"interpret the next character(s) specially.\" Now that you have implemented this from scratch, you understand the mechanism shared across all these systems.\n**3. Error reporting principle: blame the cause, not the symptom.**\nUnterminated strings and comments taught you to report the position of the *opening* delimiter, not the end-of-file where the scanner gave up. This \"blame the cause\" principle recurs everywhere in error design. A Rust borrow checker error points to where the borrow was *created*, not where it is *used* after its lifetime expired. A Java NullPointerException in a good framework points to where the null was *introduced*, not where it was *dereferenced*. A database deadlock detector reports which transaction *initiated* the conflicting lock sequence. The discipline of tracking cause positions (not just detection positions) is what separates usable error messages from useless ones.\n**4. Comment stripping as a compilation phase â€” the phase structure of compilers.**\nComments are eliminated at the lexical level â€” your scanner sees them and produces nothing. They are invisible to the parser, the type checker, and the code generator. This is intentional: it reflects the phase structure of compilers. Each phase has a specific job. The lexer's job is to produce a token stream. Comments are not tokens â€” they are annotations for the human reader. By the time the parser runs, comments are gone forever. This is why you cannot write a macro system that reads comments (unlike annotation systems in Java/Python that work at the *parser* level with structured syntax). The phase determines what information is available.\n**5. Context-sensitivity and the limit of regular languages.**\nString escape sequences technically require context: the character `n` after `\\` means \"newline,\" but `n` anywhere else means \"the letter n.\" This is one step toward context-sensitivity â€” the meaning of a character depends on what immediately precedes it. True context-sensitivity (in the formal language theory sense) would require the meaning to depend on an unbounded amount of preceding context. Here, it is bounded: only the immediately preceding character (`\\`) changes the meaning. This is encodable as a finite state machine by adding the `IN_ESCAPE` state. But it illustrates the boundary: if your language had tokens where the meaning of character `n` depended on *any previous part of the file*, your tokenizer would need a pushdown automaton or Turing machine. Regular languages are powerful enough for well-designed programming languages â€” but only barely.\n> ðŸ”­ **Deep Dive**: The formal relationship between regular languages, context-free languages, and the Chomsky hierarchy â€” and why string escape sequences sit at the boundary â€” is covered rigorously in *Introduction to the Theory of Computation* by Michael Sipser, Chapter 1 (DFAs and NFAs) and Chapter 2 (Context-Free Languages). If you want to understand *why* your scanner is technically still regular despite escape sequences, the key insight is in Sipser's construction showing that any finite-memory context can be encoded into states.\n---\n## Summary: What You Have Built\nYour scanner is now feature-complete for the full token vocabulary of your C-like language. In this milestone you added:\n- **String literal scanning** with correct raw-lexeme storage (including surrounding quotes), character-by-character accumulation using `list` + `\"\".join()` for O(n) performance, and detection of unterminated strings at EOF and at bare newlines.\n- **Escape sequence validation**: `\\n`, `\\t`, `\\r`, `\\\"`, `\\\\` are accepted; any other escape character causes an Error token. The escape character pattern leaves interpretation to higher compilation phases.\n- **Single-line comment skipping** via `_skip_line_comment()`: advances to (but does not consume) the newline, then recurses in `next_token()` to find the next real token.\n- **Multi-line comment skipping** via `_skip_block_comment()`: consumes all characters including newlines (maintaining line tracking through `advance()`), stops at the first `*/` (non-nesting), and returns an Error token at the opening `/*` position if unterminated.\n- **The `/` disambiguation**: correctly distinguishes `//` (line comment), `/*` (block comment), and `/` alone (division operator) using the `_match()` peek-and-consume pattern from Milestone 2.\n- **Comment-inside-string immunity**: the state machine architecture ensures that `//` and `/*` inside string literals are never mistaken for comment markers â€” `_scan_string()` has taken over by the time those characters are consumed.\n- **Line tracking inside multi-line constructs**: every character in strings and comments is consumed via `advance()`, which handles newline detection â€” no special cases required.\n- **Error positions that blame the cause**: unterminated strings and comments report the opening delimiter's position, not the end-of-file where the error was detected.\nIn Milestone 4, you will test this complete scanner against multi-line programs, implement error recovery strategies, verify position accuracy across long inputs, and confirm the 10,000-line performance benchmark.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m4 -->\n<!-- MS_ID: tokenizer-m4 -->\n# Milestone 4: Integration Testing & Error Recovery\n## Where You Are in the Pipeline\n\n![Tokenizer System Satellite Map â€” The Complete Pipeline](./diagrams/diag-satellite-map.svg)\n\nYou have built a complete tokenizer. Milestones 1 through 3 gave you a scanner that handles every character in your C-like language: single-character tokens, multi-character operators via maximal munch, number and identifier literals, keywords, string literals with escape sequences, and both styles of comments. Every feature works â€” in isolation.\nThis milestone is about the hardest part of building any software system: **making sure the whole is correct, not just the parts.**\nIntegration testing is where you discover that `test_plus_token()` passes but `test_expression_with_spaces()` fails because of a column-tracking bug that only appears after whitespace is consumed. It is where you find that string scanning works for `\"hello\"` but corrupts position tracking for every token that follows it on the same line. It is where you learn that the scanner you built in 15 hours of careful work has a subtle off-by-one that only shows up in multi-line programs.\nThis milestone also asks you to confront a design decision you made without realizing it: **what happens when the scanner encounters invalid input?** You already emit `ERROR` tokens for unrecognized characters â€” but does scanning continue afterward? Do you collect all errors or stop at the first one? These choices determine whether your scanner is useful in real-world tools like IDE syntax highlighters, or whether it fails the moment a user makes a typo.\nBy the end of this milestone, you will have a fully validated scanner, a working error recovery strategy, a test suite that covers edge cases and complete programs, and a performance result that confirms your implementation meets production requirements.\n---\n## The Revelation: Unit Tests Are Necessary but Not Sufficient\nHere is the misconception this milestone exists to correct.\nYou have written tests after each milestone. Each test verifies one thing: `\"==\"` produces `EQUAL`, `\"3.14\"` produces a `NUMBER` with the right lexeme, `\"if\"` produces a `KEYWORD`. All your tests pass. You believe your scanner is correct.\nNow you feed it this program:\n```python\nsource = \"\"\"if (x >= 42) {\n    /* check value */\n    return true;\n}\"\"\"\n```\nYou expect a clean token stream. Instead, you get a position drift: the token `return` shows `line=3, column=4` but your test expected `line=3, column=5`. Or worse â€” `true` appears to be on `line=2` when it should be on `line=3`.\nThis bug does not appear in any of your unit tests because it only happens when:\n1. A multi-line block comment is consumed (updating `self.line`)\n2. Followed by whitespace on the next line (consuming the indentation)\n3. Followed by a keyword token\nThe interaction between those three things â€” comment scanning, whitespace skipping, and keyword recognition â€” produces a bug that none of the three unit tests for those features individually would catch.\n**This is the fundamental limitation of unit testing.** Unit tests verify that components work in isolation. They cannot verify that components work in combination. The interactions are what break in real programs, and interactions only appear in integration tests.\n> ðŸ”‘ **Integration vs. Unit Testing in Practice**\n>\n> A **unit test** tests a single function or method with controlled inputs. Its job is to verify that the unit does what its contract says. A unit test for `_scan_number()` verifies that `\"3.14\"` returns `Token(NUMBER, \"3.14\", ...)`.\n>\n> An **integration test** tests a complete flow â€” multiple components working together â€” with realistic inputs. Its job is to verify that the components compose correctly. An integration test feeds a complete program to `scan_tokens()` and verifies the entire output token list, position by position.\n>\n> The key insight: **unit tests verify correctness of parts, integration tests verify correctness of interactions.** You need both. A system with only unit tests may still fail in production. A system with only integration tests is hard to debug when it fails (too much to narrow down). The combination â€” unit tests for isolation, integration tests for composition â€” is what builds confidence.\nThe reveal applies to error recovery as well. You might assume: \"if the input has an error, stop scanning â€” the input is already invalid, so why produce more tokens?\" This assumption is natural. It is also wrong for any tool that processes real-world code.\n**Production tokenizers never stop at the first error.** Here is why: consider a 500-line source file. The programmer made a typo on line 3 â€” they wrote `@x` instead of `x`. If your scanner stops at the `@`, the IDE's syntax highlighter goes dark for all 497 remaining lines. The type checker cannot check anything. The linter cannot lint. The programmer sees a completely black file with one error message.\nContrast this with error recovery: the scanner emits `ERROR(\"@\", 1, 3)`, then immediately resumes scanning from `x`. All remaining tokens are produced correctly. The IDE highlights syntax errors on line 3 while correctly rendering and checking everything else. The programmer sees their file with one red squiggle.\nError recovery transforms your scanner from a toy into a tool.\n\n![Trace: Error Recovery in Action](./diagrams/diag-m4-error-recovery-trace.svg)\n\n---\n## Error Recovery: The Minimal Viable Strategy\nError recovery in a lexer is dramatically simpler than error recovery in a parser. A parser encountering an error must figure out how to re-synchronize with the grammar â€” a complex process involving panic mode, error productions, and synchronization tokens. A lexer encountering an error has a trivially simple strategy:\n1. Emit an `ERROR` token for the unrecognized character.\n2. Advance past that character.\n3. Resume normal scanning from the next character.\nThat is it. The bad character is consumed, reported, and forgotten. The scanner's state is unchanged: it is still in the `START` state, ready to scan the next token. No special re-synchronization logic is needed.\nYou have already implemented this! Look at the fallthrough case in `next_token()`:\n```python\n# Unrecognized character â€” error token, continue scanning\nreturn Token(TokenType.ERROR, char, tok_line, tok_col)\n```\n`char` is already consumed by the `advance()` call at the top of `next_token()`. After this return, the cursor sits at the character *after* the bad one. The next call to `next_token()` proceeds normally. Error recovery is the natural consequence of the consume-then-dispatch architecture you built in Milestone 1.\nThe question is: does your code actually do this, or does it accidentally stop? Let us verify with a test:\n```python\ndef test_error_recovery_continues_scanning():\n    \"\"\"After an invalid character, scanning resumes from the next character.\"\"\"\n    scanner = Scanner(\"@+\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.ERROR, \"@\", 1, 1)\n    assert tokens[1] == Token(TokenType.PLUS,  \"+\", 1, 2)\n    assert tokens[2].type == TokenType.EOF\n```\nIf this test passes, your error recovery is working. The `@` is reported as an error and the `+` is scanned correctly on the next call.\n### Collecting Multiple Errors\nThe next step: ensure that multiple errors in a single input are all reported, not just the first. The `scan_tokens()` method already handles this â€” it loops until EOF, collecting every token including ERROR tokens. There is no early exit on error.\n```python\ndef test_multiple_errors_all_reported():\n    \"\"\"All invalid characters in a file are reported as separate ERROR tokens.\"\"\"\n    scanner = Scanner(\"@#$\")\n    tokens = scanner.scan_tokens()\n    error_tokens = [t for t in tokens if t.type == TokenType.ERROR]\n    assert len(error_tokens) == 3\n    assert error_tokens[0] == Token(TokenType.ERROR, \"@\", 1, 1)\n    assert error_tokens[1] == Token(TokenType.ERROR, \"#\", 1, 2)\n    assert error_tokens[2] == Token(TokenType.ERROR, \"$\", 1, 3)\n```\n### Errors Mixed with Valid Tokens\nA realistic scenario: invalid characters sprinkled throughout otherwise valid code. Your scanner should produce the correct valid tokens and the error tokens for the invalid ones, all in the correct order:\n```python\ndef test_errors_mixed_with_valid_tokens():\n    \"\"\"Invalid chars produce Error tokens; surrounding valid tokens are unaffected.\"\"\"\n    scanner = Scanner(\"x @ y\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"x\", 1, 1)\n    assert tokens[1] == Token(TokenType.ERROR,      \"@\", 1, 3)\n    assert tokens[2] == Token(TokenType.IDENTIFIER, \"y\", 1, 5)\n    assert tokens[3].type == TokenType.EOF\n```\nThis is the key behavioral guarantee: an error does not corrupt the surrounding valid tokens. The column of `\"y\"` is still correct (column 5) even though an error occurred two characters earlier.\n### Extracting Errors from a Token Stream\nIn a real compiler or IDE, you often want to process errors and valid tokens separately. A common pattern:\n```python\ndef partition_tokens(tokens: list) -> tuple[list, list]:\n    \"\"\"\n    Separate a token stream into (valid_tokens, error_tokens).\n    Both lists maintain order. EOF is kept in valid_tokens.\n    \"\"\"\n    valid = [t for t in tokens if t.type != TokenType.ERROR]\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    return valid, errors\n```\nThis is not scanner logic â€” it is consumer logic. Your scanner's job is to emit all tokens including errors. The consumer (IDE, compiler, linter) decides what to do with them. Keeping the concerns separate is clean API design.\n---\n## The Token Stream as an Interface Contract\n\n![The Token Stream as Parser Interface Contract](./diagrams/diag-m4-token-stream-contract.svg)\n\nBefore writing integration tests, it helps to understand exactly what you are testing and why exactness matters.\nYour scanner's public contract is this:\n> **For any given source string, `scan_tokens()` returns a specific, deterministic sequence of tokens where every token has the correct type, lexeme, line, and column.**\nThis is an **interface contract** â€” a promise your scanner makes to every piece of code that consumes its output. The parser you will eventually build (in a future project) will depend on this contract. If your scanner returns `KEYWORD(\"if\", 1, 1)` for some inputs and `IDENTIFIER(\"if\", 1, 1)` for others, the parser will misinterpret the input. If column numbers drift after line 10, the parser's error messages will point at wrong locations and confuse users.\nThe contract has four dimensions:\n1. **Token type** â€” correct categorization\n2. **Lexeme** â€” exact raw text from source\n3. **Line** â€” 1-indexed line number of the token's first character\n4. **Column** â€” 1-indexed column number of the token's first character\nWhen you write integration tests, you verify all four dimensions for every token in the stream. Not just \"does a KEYWORD appear?\" â€” but \"does `Token(KEYWORD, 'if', 3, 5)` appear?\" This level of precision is what catches interaction bugs that unit tests miss.\n> ðŸ”‘ **Why \"Token Stream as Contract\" Matters Beyond This Project**\n>\n> The parser that consumes your token stream never sees the original source text. It only sees tokens. If the token stream is correct, the parser can be correct â€” regardless of what the source looks like. If the token stream has bugs (wrong types, wrong positions), the parser cannot compensate; it will propagate errors downstream. This separation â€” \"lexer produces tokens, parser consumes tokens, never characters\" â€” is an architectural boundary that allows the two components to evolve independently. You can change the scanner's implementation completely (say, switch from Python string indexing to a memory-mapped file) without changing the parser at all, as long as the token stream contract is preserved.\n\n![Integration Test Anatomy: Input â†’ Expected Token Stream](./diagrams/diag-m4-integration-test-anatomy.svg)\n\n---\n## The Canonical Integration Test: Token-by-Token Verification\nThe acceptance criteria for this milestone include a specific required test: the string `\"if (x >= 42) { return true; }\"` must produce exactly the token stream listed. Let us write that test properly:\n```python\ndef test_canonical_statement():\n    \"\"\"\n    Canonical acceptance test: exact token stream for a complete statement.\n    Verifies type, lexeme, line, and column for every token.\n    \"\"\"\n    source = \"if (x >= 42) { return true; }\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    expected = [\n        Token(TokenType.KEYWORD,    \"if\",     1,  1),\n        Token(TokenType.LPAREN,     \"(\",      1,  4),\n        Token(TokenType.IDENTIFIER, \"x\",      1,  5),\n        Token(TokenType.GREATER_EQ, \">=\",     1,  7),\n        Token(TokenType.NUMBER,     \"42\",     1,  10),\n        Token(TokenType.RPAREN,     \")\",      1,  12),\n        Token(TokenType.LBRACE,     \"{\",      1,  14),\n        Token(TokenType.KEYWORD,    \"return\", 1,  16),\n        Token(TokenType.KEYWORD,    \"true\",   1,  23),\n        Token(TokenType.SEMICOLON,  \";\",      1,  27),\n        Token(TokenType.RBRACE,     \"}\",      1,  29),\n        Token(TokenType.EOF,        \"\",       1,  30),\n    ]\n    assert len(tokens) == len(expected), (\n        f\"Expected {len(expected)} tokens, got {len(tokens)}: {tokens}\"\n    )\n    for i, (got, exp) in enumerate(zip(tokens, expected)):\n        assert got == exp, (\n            f\"Token {i}: expected {exp!r}, got {got!r}\"\n        )\n```\nNotice how the assertion message includes useful debug context: which token index failed, what was expected, and what was actually produced. When this test fails (and the first time through, it often does), you need to know *which* token was wrong, not just that *a* token was wrong.\nA few things to verify manually by counting characters:\n- `if` starts at column 1 âœ“\n- `(` is at column 4: `if ` = 3 chars, then `(` âœ“\n- `x` is at column 5: right after `(` âœ“\n- `>=` is at column 7: `x ` then `>=` â€” `x` at 5, space at 6, `>` at 7 âœ“\n- `42` is at column 10: `>= ` then `42` â€” `>=` takes 7-8, space at 9, `4` at 10 âœ“\nManually walking through the columns before writing the test prevents you from testing the wrong expected values. A test with wrong expected values is worse than no test â€” it will pass incorrectly and hide bugs.\n---\n## The Multi-Line Integration Test\nA single-line test does not exercise newline handling, multi-line comment tracking, or position drift. You need a complete multi-line program. Here is a representative test program:\n```python\nMULTI_LINE_PROGRAM = \"\"\"\\\n// Simple function to check bounds\n/* This checks:\n   - lower bound\n   - upper bound */\nif (x >= 0) {\n    result = \"valid\";\n} else {\n    result = \"invalid\";\n}\nreturn result;\n\"\"\"\n```\nThis program exercises:\n- Single-line comment (line 1)\n- Multi-line block comment spanning lines 2â€“4\n- Identifiers, operators, numbers, string literals\n- Keywords: `if`, `else`, `return`\n- Multiple lines with indentation\nHere is the complete verification test:\n```python\ndef test_multi_line_program():\n    \"\"\"\n    Full integration test: verifies exact token stream for a multi-line program\n    that exercises comments, strings, operators, keywords, and position tracking.\n    \"\"\"\n    source = \"\"\"\\\n// Simple function to check bounds\n/* This checks:\n   - lower bound\n   - upper bound */\nif (x >= 0) {\n    result = \"valid\";\n} else {\n    result = \"invalid\";\n}\nreturn result;\n\"\"\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    # Remove error tokens from the stream for this check (program is valid)\n    error_tokens = [t for t in tokens if t.type == TokenType.ERROR]\n    assert error_tokens == [], f\"Unexpected errors: {error_tokens}\"\n    # After two comment lines (1 single-line + 3 lines of block comment = 4 lines total),\n    # 'if' appears on line 5.\n    if_token = tokens[0]\n    assert if_token.type == TokenType.KEYWORD\n    assert if_token.lexeme == \"if\"\n    assert if_token.line == 5\n    assert if_token.column == 1\n    # 'x' follows '(' on line 5\n    x_token = tokens[2]\n    assert x_token.type == TokenType.IDENTIFIER\n    assert x_token.lexeme == \"x\"\n    assert x_token.line == 5\n    # First string \"valid\" is on line 6\n    first_string = next(t for t in tokens if t.type == TokenType.STRING)\n    assert first_string.lexeme == '\"valid\"'\n    assert first_string.line == 6\n    # Second string \"invalid\" is on line 8\n    strings = [t for t in tokens if t.type == TokenType.STRING]\n    assert len(strings) == 2\n    assert strings[1].lexeme == '\"invalid\"'\n    assert strings[1].line == 8\n    # 'return' appears on line 10\n    return_token = next(t for t in tokens if t.lexeme == \"return\")\n    assert return_token.line == 10\n    assert return_token.column == 1\n    # Token stream ends with EOF\n    assert tokens[-1].type == TokenType.EOF\n```\nThis test does not pin every single token â€” that would be hundreds of assertions for a 10-line program. Instead, it pins the tokens that exercise the most error-prone features: the first token after comments (position drift from multi-line comments), string tokens (lexeme content and position), and `return` after multiple lines (accumulated line tracking). Add more pinned tokens as you discover specific bugs.\n### Building the Full Expected Token Stream\nFor the truly rigorous version, build the complete expected list and compare it all at once. This is more work to set up but catches any token-count discrepancy (extra or missing tokens) that selective pinning misses:\n```python\ndef test_canonical_token_count():\n    \"\"\"\n    Verifies the total number of tokens in the multi-line program.\n    A count mismatch means a token was doubled, skipped, or a comment ate a token.\n    \"\"\"\n    source = \"if (x >= 0) {\\n    return true;\\n}\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    # Manually counted: if, (, x, >=, 0, ), {, return, true, ;, }, EOF = 12\n    assert len(tokens) == 12, f\"Got {len(tokens)} tokens: {tokens}\"\n```\nCount mismatch is one of the most common integration bugs. It happens when:\n- A comment accidentally eats the token following it (comment skip consumes one character too many)\n- A string's closing quote is consumed as the start of the next token (the string scanner doesn't stop at `\"`)\n- A two-character operator is emitted as two single-character tokens (maximal munch not applied)\nThe count check catches all of these before you even look at individual token types.\n---\n## Position Tracking: The Invariant Approach\n\n![Position Tracking Drift â€” How Bugs Accumulate](./diagrams/diag-m4-position-drift.svg)\n\nPosition tracking bugs are the trickiest class of scanner bugs because they are cumulative. A single off-by-one in column counting does not immediately cause a test failure â€” it causes every subsequent token on the same line to be off by one. After a newline, the bug resets. By the time you notice the problem (because a token on line 15 has the wrong column), the bug occurred on line 3 and has been drifting silently for 12 lines.\nThis is called **position drift**: a small error in position tracking accumulates over time and produces errors that are hard to trace back to their origin.\n\n![Position Tracking Drift â€” How Bugs Accumulate](./diagrams/diag-m4-position-drift.svg)\n\nThe defense against position drift is **invariant-based testing**: instead of testing specific tokens, you test properties that must hold for *all* tokens in a scan result.\nHere are three invariants that are always true for a correct scanner:\n**Invariant 1: Every token's line is between 1 and the total number of newlines plus 1.**\n```python\ndef test_position_line_bounds():\n    \"\"\"All token line numbers are within [1, max_line].\"\"\"\n    source = \"a\\nb\\nc\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    newline_count = source.count(\"\\n\")\n    max_line = newline_count + 1  # 3 newlines â†’ lines 1 through 4\n    for token in tokens:\n        assert 1 <= token.line <= max_line, (\n            f\"Token {token!r} has out-of-bounds line number\"\n        )\n```\n**Invariant 2: Every token's column is at least 1.**\n```python\ndef test_position_column_at_least_one():\n    \"\"\"Column numbers are always 1-indexed (never 0 or negative).\"\"\"\n    source = \"a + b\\nx * y\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    for token in tokens:\n        assert token.column >= 1, (\n            f\"Token {token!r} has invalid column {token.column}\"\n        )\n```\n**Invariant 3: Tokens on the same line have non-decreasing column numbers.**\n```python\ndef test_position_columns_non_decreasing_per_line():\n    \"\"\"On any given line, token columns are strictly non-decreasing.\"\"\"\n    source = \"a + b * c\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    prev_line = 0\n    prev_col = 0\n    for token in tokens:\n        if token.type == TokenType.EOF:\n            break\n        if token.line == prev_line:\n            assert token.column > prev_col, (\n                f\"Column went backwards: {token!r} after col {prev_col}\"\n            )\n        else:\n            prev_col = 0  # reset for new line\n        prev_line = token.line\n        prev_col = token.column\n```\nThese invariants do not prove that specific positions are correct â€” they prove that position tracking is internally consistent. If an invariant fails, you know exactly which category of bug to look for. Passing all three invariants means your position tracking is at least self-consistent; then specific pinned tests verify exact values.\n> ðŸ”­ **Deep Dive**: The technique of testing invariants rather than specific values is the foundation of **property-based testing** (PBT). Tools like `Hypothesis` (Python) generate random inputs and verify that invariants hold across all of them â€” not just the cases you thought of. Hypothesis's documentation has an excellent tutorial on using it for parsers and scanners. If you want to harden your scanner further, writing a Hypothesis test that checks the three invariants above against randomly generated source strings is an afternoon project that will find bugs you never thought to look for.\n---\n## Edge Cases: The Boundary Conditions\n\n![Edge Case Decision Map](./diagrams/diag-m4-edge-cases-map.svg)\n\nEdge cases are inputs that sit at the boundary of what your scanner handles. They are disproportionately likely to contain bugs because they test the limits of the assumptions you made during implementation.\n### Empty Input\nThe simplest possible input: an empty string. This tests whether your scanner handles the zero-character case without crashing.\n```python\ndef test_empty_input():\n    \"\"\"Empty string produces exactly one EOF token.\"\"\"\n    scanner = Scanner(\"\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0] == Token(TokenType.EOF, \"\", 1, 1)\n```\nThe EOF token's position is `1:1` â€” even though there are no characters, the scanner starts at line 1, column 1. This is correct: the \"position\" of end-of-file is the position where the next character would have been.\n### Single Character Input\nSingle character inputs test the boundary between \"one-character token\" and \"end of file\":\n```python\ndef test_single_valid_char():\n    \"\"\"Single valid character produces one token and EOF.\"\"\"\n    scanner = Scanner(\"+\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.PLUS, \"+\", 1, 1)\n    assert tokens[1] == Token(TokenType.EOF,  \"\",  1, 2)\ndef test_single_invalid_char():\n    \"\"\"Single invalid character produces ERROR token and EOF.\"\"\"\n    scanner = Scanner(\"@\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.ERROR, \"@\", 1, 1)\n    assert tokens[1] == Token(TokenType.EOF,   \"\",  1, 2)\ndef test_single_newline():\n    \"\"\"A single newline is whitespace â€” produces only EOF on line 2.\"\"\"\n    scanner = Scanner(\"\\n\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\n    assert tokens[0].line == 2\n    assert tokens[0].column == 1\n```\nThe last test is tricky: a single `\\n` is consumed as whitespace. After consuming it, `self.line` becomes 2 and `self.column` becomes 1. Then `is_at_end()` returns True and an EOF token is emitted at `(2, 1)`. This is correct â€” the \"position after the newline\" is the start of line 2.\n### Maximum-Length Identifiers\nIdentifiers in your scanner can be arbitrarily long (no maximum is specified). The test ensures long identifiers work correctly without truncation or other issues:\n```python\ndef test_long_identifier():\n    \"\"\"Long identifiers are scanned correctly as a single token.\"\"\"\n    long_name = \"a\" * 1000\n    scanner = Scanner(long_name)\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == long_name\n    assert len(tokens[0].lexeme) == 1000\ndef test_long_identifier_not_keyword():\n    \"\"\"A long identifier that starts with a keyword prefix is still an IDENTIFIER.\"\"\"\n    # 'if' followed by 998 more characters â€” full lexeme is not a keyword\n    long_if = \"if\" + \"x\" * 998\n    scanner = Scanner(long_if)\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == long_if\n```\n### Whitespace-Only Input\nWhat if the entire source is spaces and newlines? No tokens should be emitted except EOF:\n```python\ndef test_whitespace_only():\n    \"\"\"Source with only whitespace produces only an EOF token.\"\"\"\n    scanner = Scanner(\"   \\t\\t\\n\\n  \\r\\n  \")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\n    # After 3 newlines (\\n, \\n, and \\r\\n = 1), we're on line 4\n    assert tokens[0].line == 4\ndef test_comment_only():\n    \"\"\"Source with only a comment produces only an EOF token.\"\"\"\n    scanner = Scanner(\"// this is the entire file\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\n```\n### Tokens at End of File Without Trailing Newline\nMany real source files do not end with a newline. Your scanner must handle this:\n```python\ndef test_token_at_eof_no_trailing_newline():\n    \"\"\"Last token in file (no trailing newline) is correctly scanned.\"\"\"\n    scanner = Scanner(\"return\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.KEYWORD, \"return\", 1, 1)\n    assert tokens[1] == Token(TokenType.EOF, \"\", 1, 7)\ndef test_number_at_eof():\n    \"\"\"Number literal at end of file (no newline) is complete.\"\"\"\n    scanner = Scanner(\"42\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NUMBER, \"42\", 1, 1)\n    assert tokens[1] == Token(TokenType.EOF, \"\", 1, 3)\n```\n### Adjacent Tokens Without Whitespace\nYour scanner must correctly separate tokens that touch each other with no whitespace between them:\n```python\ndef test_adjacent_operators_no_space():\n    \"\"\"Operators with no whitespace are correctly separated.\"\"\"\n    scanner = Scanner(\"a+b\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"a\", 1, 1)\n    assert tokens[1] == Token(TokenType.PLUS,       \"+\", 1, 2)\n    assert tokens[2] == Token(TokenType.IDENTIFIER, \"b\", 1, 3)\ndef test_adjacent_comparison_no_space():\n    \"\"\"Two-character operator adjacent to identifier.\"\"\"\n    scanner = Scanner(\"x>=y\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"x\",  1, 1)\n    assert tokens[1] == Token(TokenType.GREATER_EQ, \">=\", 1, 2)\n    assert tokens[2] == Token(TokenType.IDENTIFIER, \"y\",  1, 4)\ndef test_number_adjacent_to_operator():\n    \"\"\"Number literal directly adjacent to an operator.\"\"\"\n    scanner = Scanner(\"42+3\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0] == Token(TokenType.NUMBER, \"42\", 1, 1)\n    assert tokens[1] == Token(TokenType.PLUS,   \"+\",  1, 3)\n    assert tokens[2] == Token(TokenType.NUMBER, \"3\",  1, 4)\n```\n---\n## Performance: The 10,000-Line Benchmark\nYour tokenizer must tokenize a 10,000-line source file in under 1 second. Let us measure this with a real benchmark:\n```python\nimport time\ndef generate_large_source(num_lines: int) -> str:\n    \"\"\"\n    Generate a realistic large source file.\n    Each line is a complete statement with identifiers, operators, and numbers.\n    \"\"\"\n    lines = []\n    for i in range(num_lines):\n        # Mix of different statement types\n        if i % 5 == 0:\n            lines.append(f\"// comment on line {i}\")\n        elif i % 5 == 1:\n            lines.append(f\"result_{i} = value_{i} + {i * 2};\")\n        elif i % 5 == 2:\n            lines.append(f'message_{i} = \"string literal {i}\";')\n        elif i % 5 == 3:\n            lines.append(f\"if (count_{i} >= {i}) {{ return true; }}\")\n        else:\n            lines.append(f\"x_{i} = {i}.{i % 100};\")\n    return \"\\n\".join(lines)\ndef test_performance_10k_lines():\n    \"\"\"\n    Tokenizing 10,000 lines must complete in under 1 second.\n    This is the performance requirement from the project spec.\n    \"\"\"\n    source = generate_large_source(10_000)\n    # Warm up (Python JIT and import caches)\n    scanner = Scanner(source[:100])\n    scanner.scan_tokens()\n    start = time.perf_counter()\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    elapsed = time.perf_counter() - start\n    print(f\"\\nPerformance: {len(tokens)} tokens in {elapsed:.3f}s\")\n    print(f\"Source: {len(source)} characters, {source.count(chr(10))+1} lines\")\n    print(f\"Rate: {len(source) / elapsed / 1_000_000:.2f} MB/s\")\n    assert elapsed < 1.0, (\n        f\"Tokenizing {source.count(chr(10))+1} lines took {elapsed:.3f}s (limit: 1.0s)\"\n    )\n    # Also verify the output is not trivially wrong\n    assert tokens[-1].type == TokenType.EOF\n    assert len(tokens) > 10_000  # Sanity check: at least one token per line\n```\nRun this test with `python -m pytest test_scanner.py::test_performance_10k_lines -v -s` (the `-s` flag prints stdout so you see the performance stats).\n### What to Expect\nOn a modern machine, a Python scanner tokenizing a 10,000-line file will typically complete in 0.1â€“0.4 seconds â€” well under the 1-second limit. If your scanner is slower, the most likely culprits are:\n**String concatenation in tight loops:** If `_scan_identifier()` or `_scan_number()` uses `lexeme += char` in a loop for very long tokens, and your source has pathologically long identifiers (thousands of characters), the O(nÂ²) concatenation cost becomes visible. Switch to `list` + `\"\".join()` for those methods.\n**Excessive function call overhead:** If you have added many layers of method calls for each character, Python's function call overhead accumulates. The character-level hot path (`advance()` â†’ `next_token()` â†’ dispatch) should be as direct as possible.\n**Unnecessary work inside loops:** If `_skip_whitespace()` does more than check `is_at_end()` and `peek()`, it will be slow because it runs once per token.\nFor the purposes of this project, the straightforward implementation passes the benchmark easily. Do not optimize prematurely â€” measure first, then optimize only if the benchmark fails.\n> **On benchmarking methodology:** `time.perf_counter()` measures wall-clock time, which can vary with system load. For precise benchmarking, run the timed section 3â€“5 times and take the minimum (not average) â€” the minimum reflects the best-case execution time unaffected by system noise. Python's `timeit` module automates this. The 1-second limit is generous enough that a single run of `perf_counter()` is sufficient here.\n---\n## The Complete Integration Test Suite\nHere is the full test file for Milestone 4, organized by category:\n```python\n# test_scanner_integration.py\n\"\"\"\nIntegration tests for the complete scanner.\nThese tests verify the scanner as a whole, not individual methods.\n\"\"\"\nimport time\nimport pytest\nfrom scanner import Scanner, Token, TokenType\n# â”€â”€ Canonical acceptance tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_canonical_statement():\n    \"\"\"Exact token stream for the spec's required test case.\"\"\"\n    source = \"if (x >= 42) { return true; }\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    expected = [\n        Token(TokenType.KEYWORD,    \"if\",     1,  1),\n        Token(TokenType.LPAREN,     \"(\",      1,  4),\n        Token(TokenType.IDENTIFIER, \"x\",      1,  5),\n        Token(TokenType.GREATER_EQ, \">=\",     1,  7),\n        Token(TokenType.NUMBER,     \"42\",     1,  10),\n        Token(TokenType.RPAREN,     \")\",      1,  12),\n        Token(TokenType.LBRACE,     \"{\",      1,  14),\n        Token(TokenType.KEYWORD,    \"return\", 1,  16),\n        Token(TokenType.KEYWORD,    \"true\",   1,  23),\n        Token(TokenType.SEMICOLON,  \";\",      1,  27),\n        Token(TokenType.RBRACE,     \"}\",      1,  29),\n        Token(TokenType.EOF,        \"\",       1,  30),\n    ]\n    assert len(tokens) == len(expected), f\"Count mismatch: {tokens}\"\n    for i, (got, exp) in enumerate(zip(tokens, expected)):\n        assert got == exp, f\"Token[{i}]: expected {exp!r}, got {got!r}\"\n# â”€â”€ Error recovery tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_single_error_recovery():\n    \"\"\"After one invalid char, scanning continues.\"\"\"\n    tokens = Scanner(\"@+\").scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[1].type == TokenType.PLUS\n    assert tokens[2].type == TokenType.EOF\ndef test_multiple_errors_all_collected():\n    \"\"\"All invalid characters in a file produce separate Error tokens.\"\"\"\n    tokens = Scanner(\"@#$\").scan_tokens()\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    assert len(errors) == 3\n    assert errors[0].lexeme == \"@\"\n    assert errors[1].lexeme == \"#\"\n    assert errors[2].lexeme == \"$\"\ndef test_error_does_not_corrupt_surrounding_tokens():\n    \"\"\"Invalid character does not affect column of following token.\"\"\"\n    tokens = Scanner(\"x @ y\").scan_tokens()\n    assert tokens[0] == Token(TokenType.IDENTIFIER, \"x\", 1, 1)\n    assert tokens[1] == Token(TokenType.ERROR,      \"@\", 1, 3)\n    assert tokens[2] == Token(TokenType.IDENTIFIER, \"y\", 1, 5)\ndef test_errors_on_multiple_lines():\n    \"\"\"Errors on different lines have correct line numbers.\"\"\"\n    tokens = Scanner(\"@\\n#\").scan_tokens()\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    assert errors[0].line == 1\n    assert errors[1].line == 2\n# â”€â”€ Edge case tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_empty_input():\n    tokens = Scanner(\"\").scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0] == Token(TokenType.EOF, \"\", 1, 1)\ndef test_single_plus():\n    tokens = Scanner(\"+\").scan_tokens()\n    assert tokens[0] == Token(TokenType.PLUS, \"+\", 1, 1)\n    assert tokens[1] == Token(TokenType.EOF, \"\", 1, 2)\ndef test_single_newline():\n    tokens = Scanner(\"\\n\").scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\n    assert tokens[0].line == 2\ndef test_whitespace_only():\n    tokens = Scanner(\"   \\t   \").scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\ndef test_long_identifier():\n    name = \"x\" * 500\n    tokens = Scanner(name).scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == name\ndef test_keyword_prefix_is_identifier():\n    # 'iffy' starts with 'if' but is an IDENTIFIER\n    tokens = Scanner(\"iffy\").scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == \"iffy\"\ndef test_adjacent_tokens_no_space():\n    tokens = Scanner(\"a+b\").scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[1].type == TokenType.PLUS\n    assert tokens[2].type == TokenType.IDENTIFIER\n# â”€â”€ Position accuracy tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_position_after_multiline_block_comment():\n    \"\"\"Tokens after a multi-line comment have correct line numbers.\"\"\"\n    source = \"a\\n/* line 2\\nline 3\\n*/\\nb\"\n    tokens = Scanner(source).scan_tokens()\n    a_token = tokens[0]\n    b_token = tokens[1]\n    assert a_token.line == 1\n    assert b_token.line == 5  # 'b' is on line 5\ndef test_position_after_line_comment():\n    \"\"\"Token after a line comment is on the next line.\"\"\"\n    tokens = Scanner(\"// comment\\nx\").scan_tokens()\n    assert tokens[0].line == 2\n    assert tokens[0].column == 1\ndef test_columns_reset_after_newline():\n    \"\"\"Column resets to 1 at the start of each new line.\"\"\"\n    tokens = Scanner(\"abc\\ndef\").scan_tokens()\n    assert tokens[0].column == 1  # 'abc' at col 1 on line 1\n    assert tokens[1].column == 1  # 'def' at col 1 on line 2\ndef test_position_invariant_columns_nondecreasing():\n    \"\"\"On any single line, token columns are strictly increasing.\"\"\"\n    tokens = Scanner(\"a + b * c\").scan_tokens()\n    cols = [t.column for t in tokens if t.type != TokenType.EOF]\n    for i in range(len(cols) - 1):\n        assert cols[i] < cols[i + 1], f\"Column went backwards at index {i}: {cols}\"\ndef test_position_invariant_lines_valid():\n    \"\"\"All line numbers are in [1, total_lines].\"\"\"\n    source = \"a\\nb\\nc\\nd\\ne\"\n    tokens = Scanner(source).scan_tokens()\n    max_line = source.count(\"\\n\") + 1\n    for t in tokens:\n        assert 1 <= t.line <= max_line, f\"Invalid line in {t!r}\"\ndef test_string_position_is_opening_quote():\n    \"\"\"String token position is the opening '\"', not the closing one.\"\"\"\n    tokens = Scanner('   \"hello\"').scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].column == 4  # opening '\"' at column 4\ndef test_unterminated_string_position_is_opening():\n    \"\"\"Error token for unterminated string points at opening quote.\"\"\"\n    tokens = Scanner('x = \"hello').scan_tokens()\n    error = next(t for t in tokens if t.type == TokenType.ERROR)\n    assert error.column == 5  # '\"' is at column 5\ndef test_unterminated_comment_position_is_opening():\n    \"\"\"Error token for unterminated comment points at '/*'.\"\"\"\n    tokens = Scanner(\"x /* never closed\").scan_tokens()\n    error = next(t for t in tokens if t.type == TokenType.ERROR)\n    assert error.lexeme == \"/*\"\n    assert error.column == 3  # '/*' starts at column 3\n# â”€â”€ Multi-line integration tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_multi_line_program_no_errors():\n    source = \"\"\"\\\nif (x >= 0) {\n    result = \"valid\";\n} else {\n    result = \"invalid\";\n}\nreturn result;\n\"\"\"\n    tokens = Scanner(source).scan_tokens()\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    assert errors == [], f\"Unexpected errors: {errors}\"\n    assert tokens[-1].type == TokenType.EOF\ndef test_multi_line_program_with_comments():\n    source = \"\"\"\\\n// comment on line 1\n/* block comment\n   spanning line 3 */\nx = 42;\n\"\"\"\n    tokens = Scanner(source).scan_tokens()\n    # After 3 comment lines, 'x' is on line 4\n    non_eof = [t for t in tokens if t.type != TokenType.EOF]\n    assert non_eof[0].lexeme == \"x\"\n    assert non_eof[0].line == 4\ndef test_complete_program_token_types():\n    \"\"\"Verify the sequence of token types for a complete program.\"\"\"\n    source = \"while (i != 0) { i = i - 1; }\"\n    tokens = Scanner(source).scan_tokens()\n    types = [t.type for t in tokens]\n    expected_types = [\n        TokenType.KEYWORD,    # while\n        TokenType.LPAREN,     # (\n        TokenType.IDENTIFIER, # i\n        TokenType.NOT_EQUAL,  # !=\n        TokenType.NUMBER,     # 0\n        TokenType.RPAREN,     # )\n        TokenType.LBRACE,     # {\n        TokenType.IDENTIFIER, # i\n        TokenType.ASSIGN,     # =\n        TokenType.IDENTIFIER, # i\n        TokenType.MINUS,      # -\n        TokenType.NUMBER,     # 1\n        TokenType.SEMICOLON,  # ;\n        TokenType.RBRACE,     # }\n        TokenType.EOF,\n    ]\n    assert types == expected_types\n# â”€â”€ Performance test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef generate_large_source(num_lines: int) -> str:\n    lines = []\n    for i in range(num_lines):\n        if i % 4 == 0:\n            lines.append(f\"// line {i}\")\n        elif i % 4 == 1:\n            lines.append(f\"var_{i} = {i} + {i*2};\")\n        elif i % 4 == 2:\n            lines.append(f'msg_{i} = \"value {i}\";')\n        else:\n            lines.append(f\"if (x_{i} >= {i}) {{ return false; }}\")\n    return \"\\n\".join(lines)\ndef test_performance_10k_lines():\n    source = generate_large_source(10_000)\n    start = time.perf_counter()\n    tokens = Scanner(source).scan_tokens()\n    elapsed = time.perf_counter() - start\n    assert elapsed < 1.0, f\"10k lines took {elapsed:.3f}s (limit: 1.0s)\"\n    assert tokens[-1].type == TokenType.EOF\n```\nRun the full suite: `python -m pytest test_scanner_integration.py -v`\n---\n## Diagnosing Failures: A Debugging Playbook\nWhen an integration test fails, the error message tells you what went wrong but not why. Here is a systematic approach to debugging.\n### Symptom: Wrong token count\n```\nAssertionError: Expected 12 tokens, got 11\n```\n**Diagnosis**: A token was silently skipped. Check:\n1. Is the comment skip consuming one character past the end of the comment? Print every token from `Scanner(\"// comment\\nx\").scan_tokens()` â€” is `x` present?\n2. Is the string scan consuming the character after the closing `\"`?\n3. Did you add a new keyword but forget to stop scanning the identifier at the keyword boundary?\n**Debug technique**: Add `print(tokens)` before the assertion, or use `pytest -v -s` to see stdout.\n### Symptom: Wrong token type\n```\nAssertionError: Token[3]: expected Token(GREATER_EQ, \">=\", 1, 7), got Token(GREATER, \">\", 1, 7)\n```\n**Diagnosis**: Maximal munch is not working for this case. Check:\n1. Is the `_match(\"=\")` call consuming the `=`? Add a breakpoint inside `_scan_operator()`.\n2. Is the operator character in `OPERATOR_CHARS`? If you modified the set and forgot to include `>`, it falls through to the error case.\n### Symptom: Wrong position (line off)\n```\nAssertionError: Token[0]: expected Token(KEYWORD, \"if\", 5, 1), got Token(KEYWORD, \"if\", 4, 1)\n```\n**Diagnosis**: A newline was not counted. Check:\n1. Is `advance()` being called for every character inside comments and strings? If you ever increment `self.current` directly (bypassing `advance()`), newlines in those regions are missed.\n2. Is `\\r\\n` being counted as two newlines? Add `test_windows_line_endings()` to verify.\n### Symptom: Wrong position (column off)\n```\nAssertionError: Token[2]: expected Token(IDENTIFIER, \"x\", 1, 5), got Token(IDENTIFIER, \"x\", 1, 6)\n```\n**Diagnosis**: Column tracking has an off-by-one. Check:\n1. Is `tok_col = self.column` captured before or after `advance()`? It must be before.\n2. Does `advance()` increment column for the newline character itself? It should increment `self.line` and *set* `self.column = 1` (not increment from 0).\n3. Is whitespace skipping done before position capture? `_skip_whitespace()` must run before `tok_col = self.column`.\n---\n## Error Recovery: The Philosophy Behind the Mechanism\nYou now have a working error recovery strategy. But *why* does this strategy â€” emit an error, keep going â€” work so well? And why do other strategies fail?\n**Strategy 1: Stop at first error (the naive approach)**\nEvery error raises an exception or returns immediately. Simple to implement. Catastrophically bad user experience: one typo makes the entire file un-parseable.\nIn practice, this means the IDE stops responding, the type checker gives up, and the user has no guidance on any other problems in the file. This is the behavior of Python 2's tokenizer for some error cases â€” and it was one of the complaints that motivated improvements in Python 3.\n**Strategy 2: Skip until next \"safe\" character (panic mode)**\nOn error, the scanner consumes characters until it finds something it recognizes â€” a semicolon, a newline, or a keyword. Then it resumes. This prevents the scanner from emitting cascading errors, but it may silently skip valid tokens. Used by some parsers (where the grammar is complex enough to need it), but overkill for a lexer.\n**Strategy 3: Emit error token, advance one character, resume (your strategy)**\nThis is the simplest strategy that produces correct behavior. The invariant is: every call to `next_token()` consumes at least one character (via the `advance()` at the top) and returns exactly one token. An error token corresponds to exactly one unrecognized character. No characters are skipped; no valid tokens are lost.\nThis strategy is sometimes called **character-level error recovery** â€” errors are at the granularity of a single character, the smallest possible unit. It is the correct strategy for a lexer because lexical errors are local: a `@` in the wrong place does not affect the meaning of the `+` two characters later.\n> ðŸ”‘ **The Circuit Breaker Analogy**\n>\n> Your error recovery strategy is the same principle as a **circuit breaker** in electrical engineering and distributed systems. A circuit breaker does not stop the entire electrical grid when one appliance fails â€” it isolates the fault (the bad token/appliance), reports it (error token/alert), and keeps the rest of the system running normally. In microservices, a circuit breaker pattern isolates failures in one service so that other services continue operating in a degraded mode rather than failing completely. TCP's retransmission mechanism is another example: a lost packet causes that packet to be retransmitted, but does not halt transmission of subsequent packets. The common principle is **fault isolation** â€” contain the damage, report it, and keep moving. Your scanner is a circuit breaker for lexical faults.\n---\n## The Final Scanner: What You Have Built\n\n![Integration Test Anatomy: Input â†’ Expected Token Stream](./diagrams/diag-m4-integration-test-anatomy.svg)\n\nStand back and look at the complete system you have built across four milestones.\n**The API your scanner exposes:**\n```python\nclass Scanner:\n    def __init__(self, source: str) -> None: ...\n    def scan_tokens(self) -> list[Token]: ...\n```\nTwo methods. One input (source text). One output (token stream). Everything else is implementation detail hidden inside the class. This is a clean, minimal API â€” a consumer only needs to know about these two things to use your scanner correctly.\n**What the scanner guarantees:**\n1. Every character in the source is consumed exactly once (O(n) time, no backtracking)\n2. Every token in the output has a correct type, exact lexeme, and accurate start position\n3. Invalid characters produce ERROR tokens but do not halt scanning\n4. The last token in every stream is always EOF\n5. Comments and whitespace produce no tokens but update position tracking correctly\n**The grammar your scanner recognizes:**\nYour scanner handles the lexical grammar of a C-like language. The full vocabulary, after four milestones:\n- Keywords: `if`, `else`, `while`, `return`, `true`, `false`, `null`\n- Identifiers: `[a-zA-Z_][a-zA-Z0-9_]*`\n- Numbers: `[0-9]+(\\.[0-9]+)?`\n- Strings: `\"([^\"\\\\]|\\\\.)*\"`\n- Operators: `+`, `-`, `*`, `/`, `=`, `==`, `!=`, `<`, `<=`, `>`, `>=`, `!`\n- Punctuation: `(`, `)`, `{`, `}`, `[`, `]`, `;`, `,`\n- Comments: `//[^\\n]*` and `/*.**/` (non-greedy)\nAll of this recognized in a single left-to-right pass with at most 2 characters of lookahead.\n---\n## Knowledge Cascade: What This Unlocks\n**1. Your scanner is an IDE foundation.**\nEvery language feature in modern IDEs â€” syntax highlighting, error underlines, code completion, \"go to definition\" â€” starts with a token stream that looks exactly like what you have built. Language Server Protocol (LSP), which powers VS Code and Neovim's language support, defines its position type as `{line: int, character: int}` â€” literally the same fields as your `Token.line` and `Token.column`. The infrastructure you built is the real infrastructure. Language servers in production build on the same primitive.\n**2. Error recovery across domains: resilience over correctness.**\nYour error recovery strategy embodies a principle that appears across all of distributed systems: **prefer degraded operation over complete failure**. RAID-5 keeps running after one disk fails. A Kubernetes pod in `CrashLoopBackoff` keeps retrying instead of bringing down the node. A CDN with one edge node down routes traffic to the nearest healthy node. TCP retransmits lost packets without halting the entire connection. In every case, the system isolates the fault, reports it, and continues operating in a reduced capacity. Your scanner does the same: it isolates bad characters, reports them as ERROR tokens, and keeps scanning. The parallel is not coincidental â€” it reflects a universal engineering principle: **fault isolation is more valuable than fault prevention**.\n**3. Integration testing philosophy generalizes everywhere.**\nThe lesson of this milestone â€” \"unit tests verify parts, integration tests verify interactions\" â€” applies to every system you will ever build. Microservice teams unit-test individual services and integration-test the service mesh. Frontend teams unit-test React components and integration-test complete user flows. Database teams unit-test query planning and integration-test multi-table joins. The specific failure modes change (column drift vs. network latency vs. lock contention), but the structure of the problem is always the same: emergent bugs only appear when components are combined. Build integration tests as early as unit tests, not as an afterthought.\n**4. Position tracking is the foundation of developer experience.**\nThe `(line, column)` pairs in your token stream are the foundation of every developer-facing error message in any language tool. Rust's famous \"borrow checker\" errors â€” the ones that point precisely to the conflicting borrow, the reborrow, and the use-after-move, all with line/column accuracy â€” are possible because every AST node carries position metadata that traces back to the original token's position. Python's `SyntaxError` with the `^` caret pointing at the offending token is the same mechanism. The invariant-based testing you did (columns are non-decreasing, lines are bounded, columns are >= 1) is the same technique used to validate position accuracy in production compiler test suites. You have built that infrastructure.\n**5. The token stream is your scanner's public contract â€” and API contracts are forever.**\nOnce a parser is built that consumes your token stream, the contract is locked. You can rewrite the scanner from scratch â€” change the algorithm, the data structures, the implementation language â€” as long as `scan_tokens()` produces the same token stream for the same input. This is the definition of an implementation boundary: the outside world (the parser) depends on the contract (the token stream), not the implementation. This design principle â€” define interfaces in terms of observable behavior, not implementation â€” appears in REST APIs (URL contracts), database drivers (SQL semantics), operating system system calls (POSIX), and hardware instruction sets (x86 ABI). Your token stream is your first real experience designing an interface that must be stable.\n> ðŸ”­ **Deep Dive**: The formal foundation of what you have built â€” the Chomsky hierarchy, regular language recognition, DFA construction, and the limits of finite automata â€” is covered rigorously in *Introduction to the Theory of Computation* by Michael Sipser, Chapters 1â€“2. If you want to understand why maximal munch is guaranteed to produce unique tokenizations for well-designed languages (and under what conditions it fails), Chapter 1's treatment of DFAs and Chapter 4's undecidability results together give you the complete picture.\n---\n## Summary: What You Have Built Across Four Milestones\nYou set out to build a tokenizer. What you actually built is:\n- A **finite state machine** that transforms raw text into structured tokens, one character at a time, with O(n) time and no backtracking\n- A **position tracking system** that correctly maintains line and column numbers through whitespace, comments, and multi-line strings, forming the foundation of every error message and IDE feature the language will ever have\n- An **error recovery mechanism** that treats lexical errors as isolated faults rather than fatal failures â€” the same resilience principle used in circuit breakers, TCP retransmission, and RAID\n- A **complete test suite** that verifies the scanner at the unit level (individual token types), integration level (complete programs), property level (position invariants), and performance level (10,000-line benchmark)\n- A **clean public contract** â€” `Scanner(source).scan_tokens()` returns a deterministic, reproducible token stream â€” that a parser can depend on without knowing any implementation details\nThe scanner is the first component in a compiler pipeline. Everything that follows â€” parsing, type checking, code generation â€” depends on the token stream being correct. By testing it rigorously and building in error recovery, you have given every downstream component a reliable foundation to build on.\nMore importantly, you have internalized how programming languages work at the lowest level: not as words or lines, but as individual characters consumed one at a time through a state machine that makes greedy decisions with minimal lookahead. That understanding cascades into parsers, compilers, language servers, syntax highlighters, configuration language parsers, and any tool that must understand source code structurally. The problem changes; the mechanism does not.\n---\n<!-- END_MS -->\n\n\n\n\n# TDD\n\nA character-level finite state machine scanner that transforms raw source text into a categorized token stream via maximal munch and single-character lookahead. The design centers on a single-pass, O(n) cursor model with zero backtracking. Every architectural decision flows from the FSM discipline: states are encoded as call frames, transitions as method calls, and accepting states as return points. The public contract â€” Scanner(source).scan_tokens() â†’ list[Token] â€” is the stable interface that all downstream consumers depend on.\n\n\n\n<!-- TDD_MOD_ID: tokenizer-m1 -->\n# TECHNICAL DESIGN SPECIFICATION: Token Types & Scanner Foundation (tokenizer-m1)\n\n## 1. Module Charter\n\nThis module serves as the foundational entry point for the compiler's front-end. Its primary responsibility is the transformation of a raw, linear character stream into a structured sequence of atomic lexical units (Tokens). \n\n**What it does:**\n- Defines the complete vocabulary of the target language via a `TokenType` enumeration.\n- Implements the `Token` data structure to carry lexemes and source metadata (line, column).\n- Provides a `Scanner` class that manages a stateful cursor over the source text.\n- Implements the three primitive \"lookahead and consume\" operations: `peek`, `advance`, and `is_at_end`.\n- Handles structural overhead: whitespace consumption and line/column tracking.\n- Recognizes single-character operators and punctuation.\n- Emits sentinel `EOF` tokens and `ERROR` tokens for unrecognized characters.\n\n**What it does NOT do:**\n- It does not recognize multi-character operators (e.g., `==`, `>=`).\n- It does not recognize number literals, string literals, or identifiers (reserved for M2/M3).\n- It does not perform any syntax analysis or AST construction.\n- It does not interpret escape sequences.\n\n**Upstream/Downstream Dependencies:**\n- **Upstream:** Raw source string provided by the compiler driver.\n- **Downstream:** The Parser (future module) which consumes the `list[Token]` output.\n\n**Invariants:**\n- Every character in the source string is visited exactly once by the `advance()` method.\n- The `Scanner.current` pointer never exceeds `len(source)`.\n- The `Token.line` and `Token.column` always refer to the *start* of the lexeme.\n- The resulting token list always ends with exactly one `TokenType.EOF` token.\n\n## 2. File Structure\n\nThe implementation follows a modular structure to separate data definitions from the scanning engine.\n\n1.  `tokens.py`: Definitions for `TokenType` and `Token`.\n2.  `scanner.py`: The `Scanner` class and character-dispatch logic.\n3.  `test_foundation.py`: Unit tests for the foundation and single-character scanning.\n\n## 3. Complete Data Model\n\n### 3.1 TokenType (Enum)\nThe `TokenType` enumeration defines the complete set of terminal symbols the scanner can produce.\n\n| Category | Type | Lexeme Examples | Description |\n| :--- | :--- | :--- | :--- |\n| **Literals** | `NUMBER`, `STRING` | `42`, `\"hi\"` | Values (logic in M2/M3) |\n| **Names** | `IDENTIFIER`, `KEYWORD` | `x`, `while` | Variable names and reserved words |\n| **Operators** | `PLUS`, `MINUS`, `STAR`, `SLASH`, `ASSIGN`, `BANG`, `LESS`, `GREATER`, `EQUAL`, `NOT_EQUAL`, `LESS_EQ`, `GREATER_EQ` | `+`, `-`, `*`, `/`, `=`, `!`, `<`, `>`, `==`, `!=`, `<=`, `>=` | Participates in expressions |\n| **Punctuation** | `LPAREN`, `RPAREN`, `LBRACE`, `RBRACE`, `LBRACKET`, `RBRACKET`, `SEMICOLON`, `COMMA` | `(`, `)`, `{`, `}`, `[`, `]`, `;`, `,` | Structural delimiters |\n| **Sentinels** | `EOF`, `ERROR` | `\"\"`, `@` | End of stream and invalid characters |\n\n\n![TokenType Enumeration â€” Complete Category Map](./diagrams/tdd-diag-1.svg)\n\n\n### 3.2 Token (Dataclass)\nThe `Token` is the primary carrier of information. It must be immutable to ensure that downstream passes (parser, optimizer) do not inadvertently corrupt the source representation.\n\n```python\nfrom dataclasses import dataclass\nfrom tokens import TokenType\n\n@dataclass(frozen=True)\nclass Token:\n    type: TokenType   # The category (e.g., TokenType.PLUS)\n    lexeme: str       # The raw text from source (e.g., \"+\")\n    line: int         # 1-indexed line number\n    column: int       # 1-indexed column number within that line\n\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\n```\n\n**Memory Layout Note (Python):** Each `Token` instance in Python 3.10+ with `@dataclass` occupies approximately 48-64 bytes plus the string overhead for the lexeme. Given the O(n) nature of the scanner, for a 10k line file (~300k characters), the token list will occupy ~10-15MB of RAM, which is well within intermediate performance constraints.\n\n### 3.3 Scanner Internal State\nThe `Scanner` maintains a persistent cursor.\n\n| Field | Type | Initial Value | Purpose |\n| :--- | :--- | :--- | :--- |\n| `source` | `str` | Passed in | The full input text |\n| `current` | `int` | `0` | Byte offset of the *next* character to read |\n| `line` | `int` | `1` | Current source line (1-indexed) |\n| `column` | `int` | `1` | Current source column (1-indexed) |\n\n\n![Token Dataclass Memory Layout](./diagrams/tdd-diag-2.svg)\n\n\n## 4. Interface Contracts\n\n### 4.1 Primitive Operations\n\n**`Scanner.is_at_end() -> bool`**\n- Returns `True` if `current` has reached or exceeded `len(source)`.\n- This is the primary safety guard for all string indexing.\n\n**`Scanner.peek() -> str`**\n- Returns the character at `source[current]` without advancing the cursor.\n- If `is_at_end()` is true, returns the sentinel `\"\\0\"`.\n- This allows for non-destructive lookahead.\n\n**`Scanner.advance() -> str`**\n- Consumes the character at `source[current]` and increments `current`.\n- **Position Tracking Invariant:** \n    - If the character is `\\n`, increment `line` and reset `column` to `1`.\n    - Else, increment `column` by `1`.\n- Returns the consumed character.\n- Precondition: Must not be called if `is_at_end()` is true.\n\n\n![Scanner Class Architecture â€” Fields and Methods](./diagrams/tdd-diag-3.svg)\n\n\n### 4.2 Scanning Operations\n\n**`Scanner.next_token() -> Token`**\n- The core dispatcher.\n- Procedure:\n    1. Call `_skip_whitespace()`.\n    2. Snap current `line` and `column` into local variables `tok_line` and `tok_col`. This ensures the token points to its *start*.\n    3. If `is_at_end()`, return `Token(TokenType.EOF, \"\", tok_line, tok_col)`.\n    4. Call `advance()` to get `char`.\n    5. Match `char` against `SINGLE_CHAR_TOKENS` dictionary.\n    6. If matched, return a new `Token`.\n    7. If unmatched, return `Token(TokenType.ERROR, char, tok_line, tok_col)`.\n\n**`Scanner.scan_tokens() -> list[Token]`**\n- Loops until `next_token()` produces `TokenType.EOF`.\n- Collects all produced tokens into a list and returns them.\n\n## 5. Algorithm Specification: Position-Aware Scanning\n\nThe most critical logic in Milestone 1 is the coordination between whitespace skipping and position capturing.\n\n### 5.1 The `_skip_whitespace` Algorithm\n1.  Enter a `while` loop that continues as long as `not is_at_end()`.\n2.  `peek()` at the current character.\n3.  If character is in `{' ', '\\t', '\\r', '\\n'}`:\n    - Call `advance()`. (This ensures the `line` and `column` counters are updated correctly for newlines).\n4.  If character is not whitespace, `break` the loop.\n\n### 5.2 Token Position Invariant Trace\nTo avoid \"off-by-one\" errors in column reporting:\n1.  Scanner starts at `line=1, col=1`.\n2.  Input: `  +`.\n3.  `_skip_whitespace()` advances past two spaces.\n    - After first space: `line=1, col=2`.\n    - After second space: `line=1, col=3`.\n4.  `tok_line = self.line` (1), `tok_col = self.column` (3).\n5.  `advance()` consumes `+`. `self.column` becomes 4.\n6.  Token is emitted with `column=3`. **This is correct.** The `+` starts at column 3.\n\n\n![Cursor Model: source, current, line, column Invariants](./diagrams/tdd-diag-4.svg)\n\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| `IndexError` | `is_at_end()` guard | Return `True` to stop loops | No (internal safety) |\n| Invalid character (e.g., `@`) | `next_token()` default branch | Emit `TokenType.ERROR` and continue scanning | Yes (downstream reports) |\n| CRLF `\\r\\n` line count | `advance()` logic | `\\r` increments column, `\\n` resets it and increments line. Result is +1 line. | No (correct behavior) |\n| Tab column width | `advance()` logic | Fixed increment of 1 | No (standard convention) |\n| Empty Source | `scan_tokens()` | Emit `EOF` at 1:1 and return | No |\n\n\n![advance() Step-by-Step State: Normal Character vs Newline](./diagrams/tdd-diag-5.svg)\n\n\n## 7. Implementation Sequence with Checkpoints\n\n### Phase 1: Data Definitions (0.75h)\n- Create `tokens.py`.\n- Define `TokenType` using `enum.Enum` and `enum.auto()`.\n- Define `Token` using `@dataclass`.\n- **Checkpoint:** Verify you can instantiate a `Token(TokenType.PLUS, \"+\", 1, 1)` and print it.\n\n### Phase 2: Scanner Foundation (0.75h)\n- Create `scanner.py`.\n- Implement `__init__`, `is_at_end()`, `peek()`, and `advance()`.\n- Implement `_skip_whitespace()`.\n- **Checkpoint:** Instantiate `Scanner(\"  \\n  \")`, call `_skip_whitespace()`, and assert `scanner.line == 2` and `scanner.column == 3`.\n\n\n![peek() vs advance() â€” Consuming vs Non-Consuming Operations](./diagrams/tdd-diag-6.svg)\n\n\n### Phase 3: Single-Character Dispatch (0.5h)\n- Define a constant dictionary `SINGLE_CHAR_TOKENS` mapping characters to `TokenType`.\n- Implement `next_token()`.\n- **Checkpoint:** Call `next_token()` on source `\"+\"`. Verify it returns a `Token` of type `PLUS`. Call it again, verify it returns `EOF`.\n\n### Phase 4: Integration Loop (1.0h)\n- Implement `scan_tokens()`.\n- Implement error fallthrough (unrecognized characters).\n- Write the unit test suite.\n- **Checkpoint:** Run `scan_tokens()` on `\" ( ) @ \"` and verify list: `[LPAREN, RPAREN, ERROR, EOF]`.\n\n\n![Scanner FSM â€” START State Dispatch for Milestone 1](./diagrams/tdd-diag-7.svg)\n\n\n## 8. Test Specification\n\n### 8.1 Unit Tests (`pytest` format)\n\n**Happy Path: Single Characters**\n- Input: `+ - * / ( ) { } [ ] ; ,`\n- Expected: 12 tokens with matching types, terminating in `EOF`.\n\n**Edge Case: Whitespace & Newlines**\n- Input: ` \\t\\n  +`\n- Expected: `Token(PLUS, \"+\", 2, 3)` then `EOF`.\n- Verification: Ensures whitespace doesn't emit tokens and newlines reset columns.\n\n**Edge Case: Empty Input**\n- Input: `\"\"`\n- Expected: `[Token(EOF, \"\", 1, 1)]`\n- Verification: Ensures no crash and sentinel emission.\n\n**Failure Case: Invalid Characters**\n- Input: `+ @ -`\n- Expected: `[PLUS, ERROR(\"@\"), MINUS, EOF]`\n- Verification: Ensures error recovery (scanning continues after `@`).\n\n**Failure Case: Windows Line Endings**\n- Input: `+\\r\\n+`\n- Expected: `Token(PLUS, \"+\", 1, 1)`, `Token(PLUS, \"+\", 2, 1)`, `EOF`.\n- Verification: Ensures `\\r\\n` isn't counted as two newlines.\n\n\n![next_token() Control Flow â€” M1 Complete](./diagrams/tdd-diag-8.svg)\n\n\n## 9. Performance Targets\n\n| Operation | Target | How to Measure |\n| :--- | :--- | :--- |\n| **Parsing Rate** | > 100,000 chars/sec | `timeit` on a repeated source string |\n| **Memory Overhead** | < 200 bytes/token | `sys.getsizeof` on token list / len(tokens) |\n| **Character Access** | O(1) | Algorithmic verification (no nested loops over source) |\n\n{{DIAGRAM:tdd-diag-9}}\n\n## 10. The \"Formal Soul\" of Milestone 1\n\nMilestone 1 implements a **Deterministic Finite Automaton (DFA)** with a single primary state (`START`). \n\n1.  From `START`, seeing whitespace causes a self-transition (stay in `START`).\n2.  Seeing a known single character causes a transition to an **Accepting State** (the token-production return).\n3.  Seeing an unknown character transitions to an **Error State**, which emits a token and returns to `START`.\n4.  Seeing `EOF` transitions to the **Final State**.\n\nThe \"Soul\" of this component is its **linear time complexity**. Because there is no lookahead beyond `peek()` and no backtracking (decrementing `current`), the scanner is guaranteed to terminate in exactly `N` steps where `N` is the number of characters in the source.\n\n\n![Windows Line Ending \\r\\n: One Newline, Not Two](./diagrams/tdd-diag-10.svg)\n\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m2 -->\n# TECHNICAL DESIGN SPECIFICATION: Multi-Character Tokens & Maximal Munch (tokenizer-m2)\n\n## 1. Module Charter\n\nThis module extends the core lexical scanner to handle complex, multi-character tokens through the application of the **Maximal Munch** (greedy consumption) principle. It transitions the tokenizer from a simple character-to-token mapper into a predictive Finite State Machine (FSM) capable of disambiguating overlapping lexemes. \n\n**What it does:**\n- Recognizes two-character operators (`==`, `!=`, `<=`, `>=`) using conditional lookahead.\n- Implements a numeric scanner supporting both signed/unsigned integers and floating-point literals with mandatory fractional digits (e.g., `3.14` vs `3.`).\n- Implements an identifier scanner that distinguishes between user-defined names and reserved keywords using a high-efficiency hash-map lookup.\n- Formalizes the \"Maximal Munch\" strategy: the scanner always consumes the longest possible valid character sequence for a given start state.\n- Provides a two-character lookahead mechanism (`_peek_next`) specifically for resolving decimal point ambiguity.\n\n**What it does NOT do:**\n- It does not handle string literals (delimited by quotes) or escape sequences (M3).\n- It does not handle comments (single-line or multi-line) or the specific `/` vs `//` vs `/*` ambiguity (M3).\n- It does not perform numeric overflow checks or floating-point precision validation.\n\n**Upstream/Downstream Dependencies:**\n- **Upstream:** Depends on `Scanner` foundation from M1 (current position, line/col tracking).\n- **Downstream:** Outputs a complete `list[Token]` to the Parser; M3 will wrap this logic for string/comment support.\n\n**Invariants:**\n- Every character is consumed exactly once via `advance()` or `_match()`.\n- Keyword matching only occurs on complete identifier lexemes (no prefix matches like `if` inside `iffy`).\n- Floating point scanning must verify the character *following* the dot is a digit before consuming it.\n\n## 2. File Structure\n\nThe implementation follows a modular sequence, enhancing the existing Foundation.\n\n1.  `tokens.py` (updated): Added new `TokenType` variants for multi-character operators and keywords.\n2.  `scanner.py` (updated): Integrated `_match`, `_peek_next`, and scanning methods (`_scan_number`, `_scan_identifier`, `_scan_operator`).\n3.  `test_operators.py`: Comprehensive test suite for maximal munch on operator pairs.\n4.  `test_literals.py`: Unit and integration tests for numeric literals and identifier/keyword resolution.\n\n## 3. Complete Data Model\n\n### 3.1 Updated TokenType (Enum)\nThe `TokenType` enumeration is expanded to encompass the full operator and keyword vocabulary.\n\n| Category | Type | Lexeme | Description |\n| :--- | :--- | :--- | :--- |\n| **Literals** | `NUMBER` | `42`, `3.14` | Both integers and floats use this type. |\n| **Names** | `IDENTIFIER`, `KEYWORD` | `foo`, `if` | Keywords are reserved, identifiers are user-defined. |\n| **Operators** | `ASSIGN`, `EQUAL`, `BANG`, `NOT_EQUAL`, `LESS`, `LESS_EQ`, `GREATER`, `GREATER_EQ` | `=`, `==`, `!`, `!=`, `<`, `<=`, `>`, `>=` | Requires 1-char lookahead to distinguish. |\n\n\n![Maximal Munch Decision Tree for All Operator Pairs](./diagrams/tdd-diag-11.svg)\n\n\n### 3.2 Global Tables\nTo ensure O(1) keyword lookup and centralized operator configuration, the following constants are defined.\n\n```python\nfrom tokens import TokenType\n\n# Keywords are reserved names. A lookup table ensures O(1) performance.\n# These must match exactly; prefixes are ignored.\nKEYWORDS: dict[str, TokenType] = {\n    \"if\":     TokenType.KEYWORD,\n    \"else\":   TokenType.KEYWORD,\n    \"while\":  TokenType.KEYWORD,\n    \"return\": TokenType.KEYWORD,\n    \"true\":   TokenType.KEYWORD,\n    \"false\":  TokenType.KEYWORD,\n    \"null\":   TokenType.KEYWORD,\n}\n\n# The set of characters that *might* start a two-character operator.\n# These characters are removed from M1's SINGLE_CHAR_TOKENS and handled explicitly.\nOPERATOR_CHARS: frozenset[str] = frozenset({\"=\", \"!\", \"<\", \">\"})\n```\n\n**Memory Layout Note (Python Hash Maps):** Python 3.7+ uses a highly optimized, compact, and ordered dictionary implementation. The `KEYWORDS` dictionary will consume approximately 240-300 bytes of overhead but provides near-constant lookup time ($O(1)$). Given the small number of keywords, collisions in the hash table are statistically negligible.\n\n\n![Trace: Maximal Munch on '>==' Input â€” Three Tokens](./diagrams/tdd-diag-12.svg)\n\n\n## 4. Interface Contracts\n\n### 4.1 Lookahead & Conditional Consumption\n\n**`Scanner._match(self, expected: str) -> bool`**\n- **Parameters:** `expected` (single-character string).\n- **Constraints:** Must not be called at EOF.\n- **Return:** `True` if next character matches `expected` (consumes it), `False` otherwise (no consumption).\n- **Invariant:** Internally calls `self.advance()` when a match is found to maintain line/col counters.\n- **Edge Cases:** If `is_at_end()`, returns `False`.\n\n**`Scanner._peek_next(self) -> str`**\n- **Return:** The character *two* positions ahead of the current cursor.\n- **Constraints:** Returns `\"\\0\"` if the cursor is at or near the end of the source.\n- **Reasoning:** Required to distinguish `3.` (Integer + Dot) from `3.14` (Float).\n\n\n![_match() Internal Logic â€” Peek-and-Conditionally-Consume](./diagrams/tdd-diag-13.svg)\n\n\n### 4.2 Scanner Method Signatures\n\n**`Scanner._scan_operator(self, char: str, tok_line: int, tok_col: int) -> Token`**\n- **Parameters:** `char` (the first character already consumed by `next_token`), `tok_line`, `tok_col`.\n- **Return:** A single `Token` representing either a one-character or two-character operator.\n\n**`Scanner._scan_number(self, first_digit: str, tok_line: int, tok_col: int) -> Token`**\n- **Parameters:** `first_digit` (already consumed), `tok_line`, `tok_col`.\n- **Logic:** Consumes all subsequent digits. If it sees `.` and a following digit, transitions to floating-point scanning.\n- **Return:** A `Token(TokenType.NUMBER, lexeme, ...)`.\n\n**`Scanner._scan_identifier(self, first_char: str, tok_line: int, tok_col: int) -> Token`**\n- **Parameters:** `first_char` (already consumed), `tok_line`, `tok_col`.\n- **Logic:** Consumes all alphanumeric characters and underscores. Performs keyword lookup on the final lexeme.\n- **Return:** A `Token` (either `TokenType.KEYWORD` or `TokenType.IDENTIFIER`).\n\n\n![Number Literal Scanner FSM â€” INTEGER and FLOAT States](./diagrams/tdd-diag-14.svg)\n\n\n## 5. Algorithm Specification: The Maximal Munch Logic\n\nThe core \"soul\" of this module is the **Maximal Munch** algorithm. This ensures that `==` is never scanned as `=` then `=`.\n\n### 5.1 Operator Disambiguation Algorithm\n1.  **Consume** the first character `C`.\n2.  Check if `C` is in `OPERATOR_CHARS`.\n3.  If `C == '='`:\n    - Call `_match('=')`.\n    - If `True`, return `TokenType.EQUAL` with lexeme `\"==\"`.\n    - If `False`, return `TokenType.ASSIGN` with lexeme `\"=\"`.\n4.  Apply same logic for `!`, `<`, `>`.\n\n### 5.2 Two-State Float Scanning Algorithm\nTo correctly handle numbers, the scanner uses a localized state machine for integers and decimals.\n\n1.  **State 1: INTEGER**\n    - While `peek().isdigit()`: `lexeme += advance()`.\n2.  **Transition: DECIMAL_POINT?**\n    - If `peek() == '.'` AND `_peek_next().isdigit()`:\n        - `lexeme += advance()` (consumes `.`).\n        - Transition to **State 2: FRACTION**.\n    - Else:\n        - Return `TokenType.NUMBER` (it's an integer).\n3.  **State 2: FRACTION**\n    - While `peek().isdigit()`: `lexeme += advance()`.\n    - Return `TokenType.NUMBER`.\n\n**Invariant:** A `.` not followed by a digit is NOT part of the number. The input `3.foo` produces `NUMBER(\"3\")` and the next `next_token()` call starts at the `.`.\n\n\n![_peek_next() â€” Two-Character Lookahead Without Consumption](./diagrams/tdd-diag-15.svg)\n\n\n### 5.3 Identifier Scan-then-Lookup Algorithm\nThis avoids the complexity of manual Trie-based keyword matching.\n\n1.  **Consume** `first_char`.\n2.  While `peek().isalnum()` or `peek() == '_'`:\n    - `lexeme += advance()`.\n3.  Query `KEYWORDS` dictionary with `lexeme`.\n4.  If found: `token_type = KEYWORDS[lexeme]`.\n5.  Else: `token_type = TokenType.IDENTIFIER`.\n6.  Return `Token(token_type, lexeme, ...)`.\n\n\n![Identifier Scanning + Keyword Table Lookup â€” Scan-Then-Lookup Pattern](./diagrams/tdd-diag-16.svg)\n\n\n## 6. Error Handling Matrix\n\n| Error Category | Detected By | Recovery Strategy | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| **Maximal Munch Failure** | `_match()` check in `_scan_operator` | Greedy consumption always prefers longer token; failure is logic-driven, not input-driven. | No |\n| **Prefix Keyword Match** | `_scan_identifier` lookup | Scan full alphanumeric lexeme before lookup. `iffy` never matches `if`. | No |\n| **Trailing Dot in Number** | `_peek_next().isdigit()` | Stop scanning at the last digit. Leave `.` for the next token cycle. | No |\n| **Malformed Float (`3..4`)** | `_scan_number` loop | Consumes `3.4`. The second `.` is left for next token (produces an `ERROR` or operator). | Yes (as downstream error) |\n| **Unknown Operator (`@`)** | `next_token()` fallthrough | Emit `TokenType.ERROR` and continue scanning from next char. | Yes |\n\n\n![next_token() Dispatch Order â€” M2 Complete](./diagrams/tdd-diag-17.svg)\n\n\n## 7. Implementation Sequence with Checkpoints\n\n### Phase 1: Lookahead Infrastructure (0.5h)\n- Implement `_match(expected)` and `_peek_next()`.\n- **Checkpoint:** Verify `Scanner(\"==\")._match(\"=\")` returns `True` and `current` moves to 2.\n\n### Phase 2: Operator Expansion (0.5h)\n- Remove `=`, `!`, `<`, `>` from `SINGLE_CHAR_TOKENS`.\n- Implement `_scan_operator` using `_match`.\n- **Checkpoint:** Scan `\">= \"`. Verify it produces `Token(TokenType.GREATER_EQ, \">=\", 1, 1)`.\n\n### Phase 3: Numeric Literacy (1.0h)\n- Implement `_scan_number` with float support.\n- Handle the `_peek_next` logic for dots.\n- **Checkpoint:** Scan `3.14 42 3.`. Verify types and lexemes (especially that `3.` stops after the `3`).\n\n### Phase 4: Name Resolution (0.5h)\n- Implement `KEYWORDS` table.\n- Implement `_scan_identifier`.\n- **Checkpoint:** Scan `while x` â†’ `[KEYWORD(\"while\"), IDENTIFIER(\"x\")]`.\n\n### Phase 5: Integration & Acceptance (1.5h)\n- Integrate all methods into the main `next_token` loop.\n- Run canonical statement test: `if (x >= 42) { return true; }`.\n- **Checkpoint:** All integration tests green. Token count for the canonical string is exactly 12.\n\n\n![Extended Scanner FSM â€” Four Scanning States After M2](./diagrams/tdd-diag-18.svg)\n\n\n## 8. Test Specification\n\n### 8.1 Operator Tests (Maximal Munch)\n- **Case 1: `>==`**\n    - Input: `>==`\n    - Expected: `[GREATER_EQ(\">=\"), ASSIGN(\"=\"), EOF]`\n    - Failure: Scanned as `[GREATER(\">\"), EQUAL(\"==\")]` or `[GREATER(\">\"), ASSIGN(\"=\"), ASSIGN(\"=\")]`.\n- **Case 2: `!=` vs `!`**\n    - Input: `! !=`\n    - Expected: `[BANG(\"!\"), NOT_EQUAL(\"!=\")]`\n\n### 8.2 Numeric Tests (Float Ambiguity)\n- **Case 3: Float Literal**\n    - Input: `3.14159`\n    - Expected: `Token(NUMBER, \"3.14159\", 1, 1)`\n- **Case 4: Trailing Dot**\n    - Input: `42.`\n    - Expected: `[NUMBER(\"42\"), ERROR(\".\")]` (assuming `.` is not a valid start of another token).\n- **Case 5: Leading Zero Float**\n    - Input: `0.5`\n    - Expected: `Token(NUMBER, \"0.5\", 1, 1)`\n\n### 8.3 Identifier/Keyword Tests\n- **Case 6: Prefix keyword**\n    - Input: `return_value`\n    - Expected: `Token(IDENTIFIER, \"return_value\", 1, 1)` (NOT a `KEYWORD` plus an `IDENTIFIER`).\n- **Case 7: Mixed Case (if applicable)**\n    - Input: `If` (assuming keywords are lowercase)\n    - Expected: `Token(IDENTIFIER, \"If\", 1, 1)`\n\n## 9. Performance Targets\n\n| Operation | Target | Measurement Method |\n| :--- | :--- | :--- |\n| **Keyword Lookup** | $O(1)$ amortized | Algorithmic (Python dict usage) |\n| **Token Scan Speed** | > 500,000 tokens/sec | `time.perf_counter` on a generated source file with 50,000 tokens. |\n| **Memory Allocation** | Zero heap allocation during operator dispatch | Code Review (use string slices or pre-defined lexemes). |\n\n## 10. Formal \"Soul\": Deterministic Finite Automata (DFA)\n\nThe \"Soul\" of M2 is the conversion of an **NFA (Nondeterministic Finite Automaton)** representation of operators into a **DFA**. \n\nIn an NFA, when we see `>`, we don't know if we should stop (GREATER) or continue (GREATER_EQ). By using `_match` (one-character lookahead), we resolve this choice deterministically. We essentially traverse a state tree:\n\n```\n(START) --'>'--> (SEEN_GREATER)\n                    |\n                    |--'='--> (ACCEPT: GREATER_EQ)\n                    |\n                    |--OTHER--> (ACCEPT: GREATER, UNCONSUME OTHER)\n```\n\nThis ensures the tokenizer is predictive and never requires backtracking the cursor, maintaining the $O(n)$ time complexity guarantee across all source text.\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m3 -->\n# TECHNICAL DESIGN SPECIFICATION: Strings & Comments (tokenizer-m3)\n\n## 1. Module Charter\n\nThis module implements context-sensitive scanning logic to handle string literals and comment blocks. It resolves the lexical ambiguity of the `/` character, which can represent a division operator, the start of a single-line comment, or the start of a multi-line block comment.\n\n**What it does:**\n- Implements a sub-FSM for string literals supporting escape sequences (`\\n`, `\\t`, `\\r`, `\\\"`, `\\\\`).\n- Validates escape sequences during scanning; invalid sequences trigger `TokenType.ERROR`.\n- Processes single-line comments (`//`) by advancing the cursor to the next newline without producing tokens.\n- Processes multi-line block comments (`/* ... */`) including cross-line position tracking.\n- Emits `TokenType.STRING` tokens containing the raw lexeme (including quotes).\n- Emits `TokenType.ERROR` for unterminated strings or comments, reporting the position of the *opening* delimiter.\n- Resolves `/` into `TokenType.SLASH` when not followed by `/` or `*`.\n\n**What it does NOT do:**\n- It does not \"unescape\" strings (e.g., converting the characters `\\` and `n` into a newline byte); it preserves the raw source text.\n- It does not support nested block comments (follows C-style non-nesting rules).\n- It does not handle \"raw\" strings or HEREDOCs.\n\n**Upstream/Downstream Dependencies:**\n- **Upstream:** `Scanner.advance()`, `Scanner.peek()`, and `Scanner.next_token()` dispatch logic from M1/M2.\n- **Downstream:** Provides a sanitized token stream (stripped of comments) for the Parser.\n\n**Invariants:**\n- Every character within a string or comment is consumed exactly once via `advance()`.\n- Line and column counters are updated correctly even when skipping comments or scanning multi-line strings.\n- A string literal is terminated by a closing `\"` or a bare newline (illegal in this language).\n- `_skip_line_comment` stops *before* the newline character to allow the main whitespace loop to handle line increments.\n\n## 2. File Structure\n\nThe implementation continues within the existing project structure, primarily modifying the `Scanner` class.\n\n1.  `tokens.py`: Ensure `STRING` and `SLASH` are present in `TokenType`.\n2.  `scanner.py`: Implement `_scan_string`, `_skip_line_comment`, and `_skip_block_comment`.\n3.  `test_strings.py`: Targeted unit tests for string edge cases and escape sequences.\n4.  `test_comments.py`: Targeted unit tests for single and multi-line comments.\n\n## 3. Complete Data Model\n\n### 3.1 Lexeme Accumulation Strategy\nFor string literals, the scanner must accumulate characters. To maintain $O(n)$ time complexity and avoid $O(n^2)$ string re-allocations in Python, we use a list-of-characters pattern.\n\n| Component | Type | Purpose |\n| :--- | :--- | :--- |\n| `lexeme_chars` | `list[str]` | A buffer to collect characters before `\"\".join()` |\n| `VALID_ESCAPES` | `frozenset[str]` | Constant set: `{'n', 't', 'r', '\"', '\\\\'}` |\n\n### 3.2 State Machine Representation\nThe \"state\" of the scanner is implicitly managed by the call stack (Recursive Descent Scanning).\n\n| Method | Logical State | Exit Condition |\n| :--- | :--- | :--- |\n| `next_token` | `START` | Dispatch to sub-state or EOF |\n| `_scan_string` | `IN_STRING` | `\"` (Accept), `\\n` (Error), `EOF` (Error) |\n| `_skip_line_comment` | `IN_LINE_COMMENT` | `\\n` (Accept), `EOF` (Accept) |\n| `_skip_block_comment`| `IN_BLOCK_COMMENT`| `*/` (Accept), `EOF` (Error) |\n\n\n![Context Sensitivity: Same Character '/', Different Meaning by State](./diagrams/tdd-diag-19.svg)\n\n\n## 4. Interface Contracts\n\n### 4.1 Sub-Scanner Signatures\n\n**`Scanner._scan_string(self, tok_line: int, tok_col: int) -> Token`**\n- **Input:** The line and column where the opening `\"` was detected.\n- **Output:** A `Token` of type `STRING` or `ERROR`.\n- **Lexeme Format:** Must include the surrounding double quotes (e.g., `\"\\\"hello\\\"\"`).\n- **Escape Logic:** If `\\` is seen, consume the next char. If the next char is not in `VALID_ESCAPES`, return `ERROR`.\n\n**`Scanner._skip_line_comment(self) -> None`**\n- **Logic:** Calls `advance()` until `peek() == '\\n'` or `is_at_end()`.\n- **Note:** Does not return a token. The caller (`next_token`) must recurse to find the next valid token.\n\n**`Scanner._skip_block_comment(self, start_line: int, start_col: int) -> Optional[Token]`**\n- **Input:** Start position of `/*` for error reporting.\n- **Output:** `None` if successfully closed, or `Token(ERROR, \"/*\", ...)` if EOF reached.\n- **Greediness:** Non-nesting. Stops at the first `*/`.\n\n\n![String Literal Scanner FSM â€” States, Transitions, Error Exits](./diagrams/tdd-diag-20.svg)\n\n\n## 5. Algorithm Specification\n\n### 5.1 The '/' Disambiguation (Maximal Munch)\nThis algorithm resides in `next_token()` and handles the three-way branch for the forward slash.\n\n1.  `char = advance()` sees `/`.\n2.  If `_match('/')` returns `True`:\n    - Call `_skip_line_comment()`.\n    - **Tail Call:** `return self.next_token()`. (Implicitly skips the comment).\n3.  Else if `_match('*')` returns `True`:\n    - Call `err = _skip_block_comment(tok_line, tok_col)`.\n    - If `err` is not `None`, `return err`.\n    - **Tail Call:** `return self.next_token()`.\n4.  Else:\n    - `return Token(TokenType.SLASH, \"/\", tok_line, tok_col)`.\n\n\n![Escape Sequence Validation â€” VALID_ESCAPES Decision](./diagrams/tdd-diag-21.svg)\n\n\n### 5.2 String Scanning with Escape States\nThe string scanner must handle nested escape logic while maintaining line counts.\n\n1.  Initialize `lexeme_chars = ['\"']`.\n2.  `while True`:\n    - If `is_at_end()`: Return `ERROR` token at `tok_line:tok_col`.\n    - `c = advance()`.\n    - If `c == '\"'`: \n        - `lexeme_chars.append('\"')`.\n        - `return Token(TokenType.STRING, \"\".join(lexeme_chars), tok_line, tok_col)`.\n    - If `c == '\\n'`: \n        - Return `ERROR` (bare newline in string).\n    - If `c == '\\\\'`:\n        - `lexeme_chars.append('\\\\')`.\n        - If `is_at_end()`: Return `ERROR`.\n        - `esc = advance()`.\n        - `lexeme_chars.append(esc)`.\n        - If `esc` not in `VALID_ESCAPES`: Return `ERROR`.\n        - Continue loop.\n    - Else:\n        - `lexeme_chars.append(c)`.\n\n\n![Trace: Scanning '\"hello\\\\nworld\"' Character by Character](./diagrams/tdd-diag-22.svg)\n\n\n### 5.3 Block Comment Skip (Newline Aware)\nCrucially, this algorithm must use `advance()` to ensure multi-line comments don't break the scanner's `line` counter.\n\n1.  While `not is_at_end()`:\n    - `c = advance()`.\n    - If `c == '*'`:\n        - If `peek() == '/'`:\n            - `advance()` (consume the `/`).\n            - `return None` (Success).\n    - (Note: `advance()` automatically increments `self.line` if `c == '\\n'`).\n2.  If loop exits via `is_at_end()`:\n    - `return Token(TokenType.ERROR, \"/*\", start_line, start_col)`.\n\n\n![Block Comment Scanner â€” Two-State Inner FSM](./diagrams/tdd-diag-23.svg)\n\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| **Unterminated String** | `is_at_end()` or `\\n` in `_scan_string` | Emit `ERROR` at quote start position. Resume at next char. | Yes |\n| **Invalid Escape Sequence** | `esc not in VALID_ESCAPES` | Emit `ERROR` at quote start. | Yes |\n| **Unterminated Block Comment** | `is_at_end()` in `_skip_block_comment` | Emit `ERROR` at `/*` start position. | Yes |\n| **Nested Comment Ambiguity** | Logic (`*/` terminates) | Treat inner `/*` as plain text. | No (Standard C behavior) |\n| **Backslash at EOF** | `is_at_end()` after `\\` | Emit `ERROR`. | Yes |\n\n\n![Trace: Multi-line Block Comment with Line Number Updates](./diagrams/tdd-diag-24.svg)\n\n\n## 7. Implementation Sequence with Checkpoints\n\n### Phase 1: The Slash Branch (0.5h)\n- Modify `next_token` to detect `/`.\n- Implement `TokenType.SLASH`.\n- **Checkpoint:** `Scanner(\"/\")` should produce one `SLASH` token. `Scanner(\"//\")` should produce `EOF` (comment skipped).\n\n### Phase 2: Block Comment Logic (0.75h)\n- Implement `_skip_block_comment`.\n- Ensure `advance()` is used for newline tracking.\n- **Checkpoint:** `Scanner(\"/* \\n */ +\")` should produce `PLUS` with `line=2`.\n\n\n![Error Position: Blame the Cause, Not the Symptom](./diagrams/tdd-diag-25.svg)\n\n\n### Phase 3: String FSM (1.0h)\n- Implement `_scan_string` with the list-buffer.\n- Implement escape sequence branch.\n- **Checkpoint:** `Scanner('\"hi\\\\n\"')` should produce a `STRING` token with lexeme `\"\\\"hi\\\\n\\\"\"`.\n\n### Phase 4: Error Positioning (0.5h)\n- Verify that unterminated strings report the column of the first `\"`.\n- **Checkpoint:** `Scanner('  \"hello')` should produce an `ERROR` at `line 1, col 3`.\n\n\n![Complete Scanner FSM â€” All States After M3](./diagrams/tdd-diag-26.svg)\n\n\n## 8. Test Specification\n\n### 8.1 String Escape Tests\n- **Case: Valid Escapes**\n    - Input: `\"a\\nb\\tc\\rd\\\"e\\\\f\"`\n    - Expected: `STRING` token, len(lexeme) == 15.\n- **Case: Invalid Escape**\n    - Input: `\"\\q\"`\n    - Expected: `ERROR` token.\n\n### 8.2 Comment Interaction Tests\n- **Case: \"/\" inside string**\n    - Input: `\"http://\"`\n    - Expected: One `STRING` token. The `//` must NOT be treated as a comment.\n- **Case: Multi-line string error**\n    - Input: `\"first line\\nsecond line\"`\n    - Expected: `ERROR` (since bare newlines are forbidden in strings).\n\n### 8.3 Block Comment Edge Cases\n- **Case: Adjacent Stars**\n    - Input: `/***/`\n    - Expected: `EOF` (Successfully skipped).\n- **Case: Unterminated**\n    - Input: `/* hello`\n    - Expected: `ERROR` at line 1, col 1.\n\n{{DIAGRAM:tdd-diag-27}}\n\n## 9. Performance Targets\n\n| Operation | Target | Measurement |\n| :--- | :--- | :--- |\n| **String Allocation** | $O(n)$ | One list creation, one `.join()` per string token. |\n| **Comment Memory** | $O(1)$ | Comments are skipped without allocation. |\n| **Line Tracking** | Exact | Position must be accurate across a 100-line comment. |\n\n\n![String Lexeme: list[str] + join vs String Concatenation â€” Performance](./diagrams/tdd-diag-28.svg)\n\n\n## 10. The \"Formal Soul\" of Milestone 3\n\nMilestone 3 introduces **Pushdown-like Behavior** within the FSM. While the language remains regular, the scanner now essentially \"pushes\" a context (`IN_STRING` or `IN_COMMENT`) onto a conceptual stack. \n\nThe primary challenge is **Semantic Transparency**: Comments must be invisible to the parser, but perfectly visible to the line counter. Strings must be atomic to the parser, but internally complex to the scanner. By separating the *content* of the string from the *structure* of the lexer, we ensure that the Parser receives a clean stream of tokens where the complexity of escape characters and multi-line comments has already been abstracted away.\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m4 -->\n# TECHNICAL DESIGN SPECIFICATION: Integration Testing & Error Recovery (tokenizer-m4)\n\n## 1. Module Charter\n\nThis module concludes the tokenizer implementation by transitioning from isolated unit verification to holistic system validation. Its primary mandate is to ensure the scanner remains robust, accurate, and performant when processing complex, real-world source files.\n\n**What it does:**\n- Implements a comprehensive integration test suite that verifies the entire token stream (type, lexeme, line, column) for multi-line programs.\n- Formalizes the **Character-Level Error Recovery** strategy: the scanner must emit an `ERROR` token for an invalid character, advance the cursor, and resume scanning immediately without state corruption.\n- Validates **Position Invariants**: ensures that line and column tracking never drift, even after complex sequences of multi-line comments, strings with escapes, and whitespace.\n- Conducts **Performance Benchmarking**: verifies that the Python implementation meets the 10,000-line-per-second requirement through automated timing.\n- Provides utility functions for token stream manipulation, such as partitioning valid tokens from errors.\n\n**What it does NOT do:**\n- It does not add new lexical features or token types (the grammar is frozen at M3).\n- It does not attempt parser-level recovery (e.g., inserting missing semicolons).\n- It does not handle file I/O; it operates on source strings in memory.\n\n**Invariants:**\n- **Resumption Invariant:** For any input $S$, the number of characters consumed (via `advance`) must equal `len(S)`.\n- **Monotonicity Invariant:** For any two tokens $T_i, T_{i+1}$ on the same line, $T_{i+1}.column > T_i.column$.\n- **Sentinel Invariant:** Every call to `scan_tokens()` must return a list where the final element is `TokenType.EOF`.\n\n## 2. File Structure\n\nThe user should follow this creation/update order to complete the project:\n\n1.  `scanner.py` (Update): Ensure `scan_tokens` is robust and error recovery is implicit in the loop.\n2.  `utils.py` (New): Utility functions for testing and stream partitioning.\n3.  `test_invariants.py` (New): Property-based and invariant tests for position tracking.\n4.  `test_integration.py` (New): The \"Canonical\" multi-line program tests.\n5.  `benchmark.py` (New): Performance testing script.\n\n## 3. Complete Data Model\n\n### 3.1 Token (Refined Definition)\nWhile defined in M1, the `Token` dataclass is the \"wire format\" for this module.\n\n```python\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\n\nclass TokenType(Enum):\n    # ... all types from M1-M3 ...\n    NUMBER = auto()\n    STRING = auto()\n    IDENTIFIER = auto()\n    KEYWORD = auto()\n    PLUS = auto(); MINUS = auto(); STAR = auto(); SLASH = auto()\n    ASSIGN = auto(); EQUAL = auto(); BANG = auto(); NOT_EQUAL = auto()\n    LESS = auto(); LESS_EQ = auto(); GREATER = auto(); GREATER_EQ = auto()\n    LPAREN = auto(); RPAREN = auto(); LBRACE = auto(); RBRACE = auto()\n    LBRACKET = auto(); RBRACKET = auto(); SEMICOLON = auto(); COMMA = auto()\n    EOF = auto()\n    ERROR = auto()\n\n@dataclass(frozen=True)\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n```\n\n### 3.2 Test Metadata Structures\nTo facilitate clean integration testing, we define a \"Expected Token\" schema for comparison.\n\n| Field | Type | Constraint | Purpose |\n| :--- | :--- | :--- | :--- |\n| `expected_type` | `TokenType` | Not None | Target category |\n| `expected_lexeme` | `str` | Exact match | Verifies maximal munch |\n| `expected_line` | `int` | > 0 | Verifies newline tracking |\n| `expected_col` | `int` | > 0 | Verifies column tracking |\n\n\n![Token Stream as Parser Interface Contract](./diagrams/tdd-diag-29.svg)\n\n\n## 4. Interface Contracts\n\n### 4.1 Utility Operations (`utils.py`)\n\n**`partition_tokens(tokens: list[Token]) -> tuple[list[Token], list[Token]]`**\n- **Parameters:** A list of tokens from `scan_tokens()`.\n- **Return:** A tuple `(valid_tokens, error_tokens)`.\n- **Constraint:** `valid_tokens` must include `EOF`.\n- **Usage:** Allows tests to check for \"No errors expected\" vs \"Specific errors expected\" without complex filtering logic.\n\n**`generate_large_source(num_lines: int) -> str`**\n- **Parameters:** Number of lines to generate.\n- **Logic:** Alternates between keywords, strings, and operators to create a representative workload.\n- **Return:** A string of approximately $30 \\times num\\_lines$ characters.\n\n### 4.2 Scanner Recovery Invariant\n\n**`Scanner.next_token(self) -> Token`** (Implicitly Hardened)\n- **Constraint:** Every path through this method must consume at least one character unless at EOF.\n- **Recovery Logic:** If an unrecognized character is encountered, the `ERROR` token is returned. Since `advance()` was called at the start of `next_token`, the state is automatically ready for the next character on the next call. This is \"Character-Level Panic Mode\".\n\n\n![Error Recovery: Circuit Breaker Pattern â€” Fault Isolation](./diagrams/tdd-diag-30.svg)\n\n\n## 5. Algorithm Specification\n\n### 5.1 Invariant Checking Algorithm\nThis algorithm is used in `test_invariants.py` to prove the scanner's position tracking is sound without knowing the specific tokens.\n\n1.  **Initialize:** `last_line = 1`, `last_col = 0`.\n2.  **Iterate:** For each `token` in `scan_tokens()`:\n    - **Invariant A (Line Bounds):** `1 <= token.line <= source.count('\\n') + 1`.\n    - **Invariant B (Column Bounds):** `token.column >= 1`.\n    - **Invariant C (Monotonicity):** \n        - If `token.line == last_line`: Assert `token.column > last_col`.\n        - Else: Assert `token.line > last_line`.\n    - **Update:** `last_line = token.line`, `last_col = token.column`.\n3.  **Post-condition:** The final token must be `EOF` and its position must point exactly to the end of the source string.\n\n{{DIAGRAM:tdd-diag-31}}\n\n### 5.2 Performance Benchmarking Procedure\nTo meet the \"Expert\" precision level, we use monotonic hardware clocks.\n\n1.  **Setup:** Generate source string via `generate_large_source(10000)`.\n2.  **Warm-up:** Run `Scanner(source).scan_tokens()` once to ensure Python byte-code caches/JIT (if using PyPy) are active.\n3.  **Timer Start:** `start_time = time.perf_counter()`.\n4.  **Execute:** `tokens = Scanner(source).scan_tokens()`.\n5.  **Timer Stop:** `end_time = time.perf_counter()`.\n6.  **Calculate:** `elapsed = end_time - start_time`.\n7.  **Assert:** `elapsed < 1.0`.\n8.  **Metric Report:** Output `Tokens/Sec = len(tokens) / elapsed`.\n\n\n![Position Drift: How Small Bugs Accumulate Over Lines](./diagrams/tdd-diag-32.svg)\n\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| **Position Drift** | Invariant Test (C) | Re-align `advance()` newline logic to reset column to 1. | No (Dev-time fix) |\n| **Token Mismatch** | `test_canonical_statement` | Check `_match()` greediness for 2-char ops. | No (Dev-time fix) |\n| **Infinite Recursion** | `next_token()` comment branch | Ensure `_skip_comment` calls `advance()`. | No |\n| **Unterminated String** | EOF check in `_scan_string` | Return `ERROR` with opening quote position. | Yes |\n| **Multiple Invalid Chars** | `scan_tokens` loop | Collect each into the list; don't break. | Yes |\n\n\n![Position Invariant Suite â€” Three Checkable Properties](./diagrams/tdd-diag-33.svg)\n\n\n## 7. Implementation Sequence with Checkpoints\n\n### Phase 1: Canonical Verification (0.5h)\n- Create `test_integration.py`.\n- Implement `test_canonical_statement` with the exact list from the project spec.\n- **Checkpoint:** `pytest test_integration.py` â†’ `test_canonical_statement` passes with exactly 12 tokens.\n\n### Phase 2: Error Recovery Hardening (0.5h)\n- Add `test_multiple_errors_all_reported` with input `\"@#$\"`.\n- Add `test_errors_mixed_with_valid_tokens` with input `\"x @ y\"`.\n- **Checkpoint:** Verify that `@` and `y` have correct relative positions.\n\n\n![Multi-line Program Test: Comment Lines â†’ Line Number Offset Calculation](./diagrams/tdd-diag-34.svg)\n\n\n### Phase 3: Position Invariant Suite (1.0h)\n- Create `test_invariants.py`.\n- Implement the monotonicity and line-bound checks.\n- Test against multi-line block comments and strings.\n- **Checkpoint:** Run against a file containing `/* \\n\\n */ \" \\n \"`. Verify `line` transitions 1 -> 4 -> 5.\n\n### Phase 4: Boundary & Edge Cases (0.75h)\n- Test empty string, single newline, long identifiers (1000+ chars).\n- Verify `EOF` position on single-character inputs.\n- **Checkpoint:** `Scanner(\"x\").scan_tokens()` results in `Token(ID, \"x\", 1, 1)` and `Token(EOF, \"\", 1, 2)`.\n\n\n![Performance Benchmark Architecture: 10k Line Generation and Timing](./diagrams/tdd-diag-35.svg)\n\n\n### Phase 5: Performance & Scaling (0.75h)\n- Implement `benchmark.py`.\n- Run for 10k lines.\n- **Checkpoint:** Execution time < 1.0s. Report tokens/sec.\n\n{{DIAGRAM:tdd-diag-36}}\n\n## 8. Test Specification\n\n### 8.1 Integration Tests\n- **Input:** `if (x >= 0) { return \"ok\"; } // check`\n- **Verification:**\n    - `tokens[0].type == TokenType.KEYWORD`\n    - `tokens[3].lexeme == \">=\"`\n    - `tokens[7].type == TokenType.STRING`\n    - `len(tokens) == 11` (if counting EOF).\n\n### 8.2 Boundary Tests\n- **Empty File:** `\"\"` â†’ `[EOF(1,1)]`.\n- **Long Literal:** `\"a\" * 10000` â†’ `Token(IDENTIFIER, ..., len=10000)`.\n- **Maximal Munch Edge:** `>==` â†’ `GREATER_EQ`, `ASSIGN`.\n\n### 8.3 Error Tests\n- **Unterminated Comment:** `/* hello` â†’ `Token(ERROR, \"/*\", 1, 1)`.\n- **Invalid Escape:** `\"\\q\"` â†’ `Token(ERROR, \"\\\"\\\\q\", 1, 1)`.\n\n## 9. Performance Targets\n\n| Metric | Target | Method |\n| :--- | :--- | :--- |\n| **Throughput** | > 10,000 lines/sec | 10k lines source in < 1.0s. |\n| **Complexity** | $O(N)$ | Verify `current` only increments, never decrements. |\n| **Token Rate** | > 100,000 tokens/sec | Benchmarked on standard Statement mix. |\n\n## 10. The \"Formal Soul\" of Integration\n\nThe \"Formal Soul\" of Milestone 4 is the transition from **Correctness-by-Logic** to **Correctness-by-Observation**. \n\nThe tokenizer's design is theoretically O(n) and deterministic, but the implementation is susceptible to **Accumulated State Drift**â€”specifically in the line/column counters. By treating the tokenizer as a \"black box\" that transforms Source $\\rightarrow$ TokenStream and verifying invariants (monotonicity, bounds, and sentinel presence), we ensure that the software matches the theoretical FSM. \n\nThe error recovery strategy implemented here (Character-Level resync) is the most robust possible for a lexer because it treats lexical errors as **localized noise** that does not impact the global structure. This provides the highest possible availability for downstream tools (IDE squiggles, syntax highlighters) which must continue to function even in the presence of malformed source text.\n<!-- END_TDD_MOD -->\n\n\n# Project Structure: Tokenizer / Lexer\n\n## Directory Tree\n\n```text\ntokenizer-lexer/\nâ”œâ”€â”€ tokens.py               # Token & TokenType definitions (M1-M3)\nâ”œâ”€â”€ scanner.py              # Main FSM scanning logic (M1-M4)\nâ”œâ”€â”€ utils.py                # Token stream partitioning utilities (M4)\nâ”œâ”€â”€ benchmark.py            # Performance timing script (M4)\nâ”œâ”€â”€ tests/                  # Test suite directory\nâ”‚   â”œâ”€â”€ test_foundation.py  # Single-char & cursor tests (M1)\nâ”‚   â”œâ”€â”€ test_operators.py   # Multi-char operator munch tests (M2)\nâ”‚   â”œâ”€â”€ test_literals.py    # Number & identifier tests (M2)\nâ”‚   â”œâ”€â”€ test_strings.py     # String escape sequence tests (M3)\nâ”‚   â”œâ”€â”€ test_comments.py    # Line & block comment tests (M3)\nâ”‚   â”œâ”€â”€ test_invariants.py  # Position & monotonicity checks (M4)\nâ”‚   â””â”€â”€ test_integration.py # Multi-line program validation (M4)\nâ”œâ”€â”€ requirements.txt        # Project dependencies (pytest)\nâ”œâ”€â”€ pytest.ini              # Test runner configuration\nâ”œâ”€â”€ .gitignore              # Python build/cache exclusions\nâ””â”€â”€ README.md               # Project overview and usage\n```\n\n## Creation Order\n\n1.  **Environment Setup** (15 min)\n    *   Create project root and `tests/` directory.\n    *   Create `requirements.txt` (add `pytest`) and `.gitignore`.\n\n2.  **Milestone 1: Foundation** (1.5h)\n    *   `tokens.py`: Define `TokenType` enum and `Token` dataclass.\n    *   `scanner.py`: Implement `Scanner` class with `advance()`, `peek()`, and basic `next_token()` loop.\n    *   `tests/test_foundation.py`: Verify single-character tokens and line/column tracking.\n\n3.  **Milestone 2: Multi-Character Logic** (2h)\n    *   `tokens.py`: Update with operator and keyword variants.\n    *   `scanner.py`: Implement `_match()`, `_peek_next()`, `_scan_number()`, and `_scan_identifier()`.\n    *   `tests/test_operators.py` & `tests/test_literals.py`: Verify greedy munch and keyword lookup.\n\n4.  **Milestone 3: Context-Sensitive Scanning** (2h)\n    *   `scanner.py`: Implement `_scan_string()` with escape logic and comment skipping (`//`, `/* */`).\n    *   `tests/test_strings.py` & `tests/test_comments.py`: Verify sub-FSM transitions and multi-line tracking.\n\n5.  **Milestone 4: Validation & Performance** (1.5h)\n    *   `utils.py`: Add `partition_tokens` for error separation.\n    *   `scanner.py`: Refine `scan_tokens()` for robust error collection.\n    *   `tests/test_invariants.py` & `tests/test_integration.py`: Run holistic program tests.\n    *   `benchmark.py`: Execute the 10,000-line performance test.\n\n## File Count Summary\n*   **Total Files**: 14\n*   **Directories**: 2 (root, tests)\n*   **Estimated Lines of Code**: ~500â€“700 lines (including tests)\n\n# ðŸ“š Beyond the Atlas: Further Reading\n\n## Automata Theory & Formal Foundations\n**Paper**: *Regular Expression Search Algorithm* (Ken Thompson, 1968).\n**Code**: [RE2 (Google) â€” `nfa.cc`](https://github.com/google/re2/blob/main/re2/nfa.cc)\n**Best Explanation**: *Introduction to the Theory of Computation* (Michael Sipser), Chapter 1: Finite Automata.\n**Why**: This is the mathematical bedrock of all tokenization; it proves that the FSMs you built in this project are equivalent to Regular Expressions.\n**Pedagogical Timing**: Read **BEFORE** starting Milestone 1 to understand why a state-based cursor is more powerful than simple string splitting.\n\n**Best Explanation**: *Compilers: Principles, Techniques, and Tools* (Aho, Lam, Sethi, Ullman), Section 3.3: Recognition of Tokens.\n**Why**: Known as the \"Dragon Book,\" this specific section formalizes the **Maximal Munch** principle and how to handle lookahead deterministically.\n**Pedagogical Timing**: Read **AFTER** Milestone 2 to see the formal proof for the greedy logic you implemented for operators.\n\n## Real-World Lexer Implementations\n**Code**: [Go Language Scanner â€” `src/go/scanner/scanner.go`](https://github.com/golang/go/blob/master/src/go/scanner/scanner.go)\n**Best Explanation**: *The Go Programming Language* (Donovan & Kernighan), Chapter 11.5: Character-level Lexing.\n**Why**: This is the cleanest, most readable production-grade scanner in existence, using the exact \"Scan-Then-Lookup\" pattern from Milestone 2.\n**Pedagogical Timing**: Read **AFTER** Milestone 2 to compare your implementation with a language used by millions.\n\n**Code**: [CPython Tokenizer â€” `Parser/tokenizer.c`](https://github.com/python/cpython/blob/main/Parser/tokenizer.c)\n**Why**: It provides a masterclass in handling the \"grit\" of real-world source code, including complex indentation and multiline string states.\n**Pedagogical Timing**: Read **AFTER** Milestone 3 to see how Python handles indentation as a \"virtual token\" (DEDENT/INDENT).\n\n**Code**: [LLVM/Clang Lexer â€” `lib/Lex/Lexer.cpp`](https://github.com/llvm/llvm-project/blob/main/clang/lib/Lex/Lexer.cpp)\n**Best Explanation**: [Clang's Internal Manual: The Lexer and Preprocessor library](https://clang.llvm.org/docs/InternalsManual.html#the-lexer-and-preprocessor-library).\n**Why**: Clang is the industry standard for high-speed lexing and precise diagnostic positioning.\n**Pedagogical Timing**: Read **AFTER** Milestone 4 to understand how \"Lexer Buffers\" enable the extreme performance required by modern C++ compilers.\n\n## String Literals & Escape Sequences\n**Spec**: [ECMA-262 (JavaScript), Section 12.8.4: String Literals](https://tc39.es/ecma262/#sec-literals-string-literals).\n**Code**: [V8 (Chrome) â€” `src/parsing/scanner.cc`](https://github.com/v8/v8/blob/main/src/parsing/scanner.cc) (Search for `ScanString`).\n**Best Explanation**: *Crafting Interpreters* (Robert Nystrom), Chapter 4: Scanning.\n**Why**: Nystromâ€™s explanation of the \"inside-a-string\" state is the most intuitive pedagogical resource for context-sensitive scanning.\n**Pedagogical Timing**: Read **DURING** Milestone 3 if you struggle with the transition between the START and IN_STRING states.\n\n## Error Recovery & Diagnostic UX\n**Paper**: *A Guide to Improving the UX of Compiler Error Messages* (Esteban KÃ¼ber, 2019).\n**Code**: [Rust Compiler â€” `compiler/rustc_lex/src/lib.rs`](https://github.com/rust-lang/rust/blob/master/compiler/rustc_lex/src/lib.rs)\n**Best Explanation**: [The Rust Reference: Tokens and Lexical Analysis](https://doc.rust-lang.org/reference/tokens.html).\n**Why**: Rust is widely considered the gold standard for developer experience (DX) and precise error reporting using token position metadata.\n**Pedagogical Timing**: Read **AFTER** Milestone 4 to see how the position data you captured can be turned into helpful \"suggested fix\" squiggles.\n\n## Performance & Benchmarking\n**Code**: [Lua 5.4 Lexer â€” `llex.c`](https://github.com/lua/lua/blob/master/llex.c)\n**Why**: Luaâ€™s scanner is incredibly fast and minimal; it demonstrates how to achieve extreme performance without the overhead of heavy abstractions.\n**Pedagogical Timing**: Read **AFTER** completing the 10,000-line benchmark in Milestone 4 if you want to see how to push performance further."}