{"html":"<h1 id=\"-project-charter-the-c-like-tokenizer\">ðŸŽ¯ Project Charter: The C-Like Tokenizer</h1>\n<h2 id=\"what-you-are-building\">What You Are Building</h2>\n<p>A robust, character-by-character lexical scanner that transforms raw source text into a structured stream of categorized tokens. You will implement a Deterministic Finite Automaton (DFA) in code to recognize complex lexemes, including multi-character operators like <code>&gt;=</code> using the &quot;maximal munch&quot; principle, numeric literals (integers and floats), and string literals with backslash escape sequences.</p>\n<h2 id=\"why-this-project-exists\">Why This Project Exists</h2>\n<p>Lexical analysis is the &quot;front door&quot; of every compiler, interpreter, and syntax highlighter. By building a tokenizer from scratch, you demystify how programming languages resolve ambiguity at the character level and learn why a simple <code>split(&#39; &#39;)</code> is insufficient for parsing real-world code.</p>\n<h2 id=\"what-you-will-be-able-to-do-when-done\">What You Will Be Able to Do When Done</h2>\n<ul>\n<li><strong>Transform Raw Text:</strong> Convert a flat string of source code into a sequence of typed <code>Token</code> objects.</li>\n<li><strong>Implement Maximal Munch:</strong> Resolve lexical ambiguity (e.g., distinguishing between <code>&gt;</code> followed by <code>=</code> and the single <code>&gt;=</code> operator).</li>\n<li><strong>Manage Contextual Modes:</strong> Build a scanner that switches behavior when entering &quot;string mode&quot; or &quot;comment mode.&quot;</li>\n<li><strong>Track Precise Metadata:</strong> Calculate 1-based line and column numbers for every token to power actionable error messages.</li>\n<li><strong>Build Resilient Tools:</strong> Implement &quot;Skip-One&quot; error recovery so the scanner can report multiple errors in a single pass without halting.</li>\n</ul>\n<h2 id=\"final-deliverable\">Final Deliverable</h2>\n<p>A standalone <code>Scanner</code> module (~250â€“400 lines of code) that processes a multi-line C-like source file. It must pass an integration suite verifying it can tokenize a complex program (containing nested logic, comments, and escaped strings) into a precise list of tokens ending with an <code>EOF</code> sentinel.</p>\n<h2 id=\"is-this-project-for-you\">Is This Project For You?</h2>\n<p><strong>You should start this if you:</strong></p>\n<ul>\n<li>Are comfortable with string indexing and character-by-character loops.</li>\n<li>Understand how to use Enums and Hash Maps (Dictionaries) in your chosen language.</li>\n<li>Want to learn the architectural foundations of developer tools and DSLs.</li>\n</ul>\n<p><strong>Come back after you&#39;ve learned:</strong></p>\n<ul>\n<li>Basic data structure implementation (Classes/Structs).</li>\n<li><a href=\"https://docs.python.org/3/library/enum.html\">Python Enum Module</a> (if using the recommended language).</li>\n</ul>\n<h2 id=\"estimated-effort\">Estimated Effort</h2>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>M1: Foundation &amp; Single-Char Tokens</strong></td>\n<td>~3 hours</td>\n</tr>\n<tr>\n<td><strong>M2: Multi-Character Operators &amp; Numbers</strong></td>\n<td>~4 hours</td>\n</tr>\n<tr>\n<td><strong>M3: Strings &amp; Comments (Context Tracking)</strong></td>\n<td>~4 hours</td>\n</tr>\n<tr>\n<td><strong>M4: Integration Testing &amp; Performance</strong></td>\n<td>~3 hours</td>\n</tr>\n<tr>\n<td><strong>Total</strong></td>\n<td><strong>~14 hours</strong></td>\n</tr>\n</tbody></table>\n<h2 id=\"definition-of-done\">Definition of Done</h2>\n<p>The project is complete when:</p>\n<ul>\n<li>The scanner produces the exact expected token stream for the provided &quot;Golden Program&quot; integration test.</li>\n<li>Every token includes accurate <code>line</code> and <code>column</code> metadata that does not &quot;drift&quot; over a 100+ line file.</li>\n<li>Unterminated strings or comments produce an <code>ERROR</code> token at the <em>opening</em> delimiter&#39;s position.</li>\n<li>The scanner processes a 10,000-line synthetic source file in under 1 second.</li>\n<li>The system emits a final <code>EOF</code> token regardless of whether the input was valid or empty.</li>\n</ul>\n<hr>\n<h1 id=\"tokenizer-lexer-interactive-atlas\">Tokenizer / Lexer â€” Interactive Atlas</h1>\n<p>This project builds a complete character-level lexer for a simple C-like language. You will implement a finite state machine that reads source code one character at a time, applies the maximal munch principle to resolve ambiguities, and emits a categorized stream of tokens. Along the way you will handle string escape sequences, nested comment edge cases, position tracking, and error recovery â€” the same problems faced by every real-world compiler frontend from GCC to V8.</p>\n<p>The tokenizer is the first stage of any compiler or interpreter pipeline: it transforms a flat string of characters into structured tokens that a parser can consume. Understanding this stage deeply unlocks insight into how programming languages actually work, from syntax highlighting in your editor to the error messages your compiler produces.</p>\n<p>By project&#39;s end you will have a robust, tested lexer that can tokenize complete multi-line programs, report multiple errors with accurate line/column information, and recover gracefully from invalid input â€” a genuine tool, not a toy.</p>\n<!-- MS_ID: tokenizer-m1 -->\n<h1 id=\"milestone-1-token-types-amp-scanner-foundation\">Milestone 1: Token Types &amp; Scanner Foundation</h1>\n<h2 id=\"where-you39re-starting\">Where You&#39;re Starting</h2>\n<p>Before writing a single line of scanning logic, you need to answer two questions: <strong>What are you scanning for?</strong> and <strong>How do you move through the input?</strong> This milestone answers both.\nYou&#39;ll define the complete vocabulary of your tokenizer â€” the <code>TokenType</code> enumeration â€” and the data structure that holds a recognized token. Then you&#39;ll build the core engine: the <code>Scanner</code> class with the two primitive operations (<code>advance</code> and <code>peek</code>) that everything else is built from. Finally, you&#39;ll handle the simplest cases: single-character tokens, whitespace, end-of-file, and error recovery for unrecognized input.\nBy the end of this milestone, you&#39;ll have a working system that can scan source code character-by-character and emit a stream of tokens. It won&#39;t handle multi-character operators, number literals, strings, or comments yet â€” that&#39;s Milestones 2 and 3. But the foundation you lay here determines how cleanly everything else falls into place.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-satellite-map.svg\" alt=\"Tokenizer System â€” Satellite Map (Home Base)\"></p>\n<hr>\n<h2 id=\"the-core-misconception-tokenizing-is-not-splitting\">The Core Misconception: Tokenizing Is Not Splitting</h2>\n<p>You&#39;ve almost certainly used Python&#39;s <code>str.split()</code> before. It takes a string and slices it apart at whitespace (or a delimiter you specify). When you first hear &quot;a tokenizer breaks source code into tokens,&quot; it&#39;s natural to imagine something similar â€” maybe <code>source.split()</code> with some extra logic for operators.\nThis mental model breaks the moment you try it:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"if(x>=42){return true;}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.split()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Result: [\"if(x>=42){return\", \"true;}\"]</span></span></code></pre></div>\n<h2 id=\"zero-whitespace-means-zero-splits-two-quottokensquot-neither-is-useful-but-a-real-tokenizer-handles-this-correctly-producing-keywordif-lparen-identifierx-greaterequal-number42-rparen-lbrace-keywordreturn-keywordtrue-semicolon-rbrace-eof-with-no-whitespace-anywhere-in-the-input-the-real-model-a-tokenizer-is-a-character-by-character-finite-state-machine-it-does-not-look-for-spaces-it-reads-one-character-at-a-time-asks-quotwhat-state-am-i-in-and-what-does-this-character-meanquot-and-decides-extend-the-current-token-or-emit-what-i-have-and-start-fresh-whitespace-is-just-another-character-one-that-happens-to-trigger-quotemit-and-don39t-include-this-characterquot-a-digit-followed-by-a-letter-triggers-quotemit-the-number-start-an-identifierquot-an-followed-by-another-triggers-quotemit-a-two-character-quot-the-boundaries-are-state-transitions-not-spaces-that39s-why-xgt42-tokenizes-correctly-with-zero-whitespace-the-scanner-changes-state-every-time-the-character-class-changes-and-those-state-changes-are-the-boundaries-keep-this-model-in-your-head-as-you-build-every-branch-in-your-scanner39s-main-loop-is-a-state-transition-in-a-finite-automaton-you-are-building-a-dfa-deterministic-finite-automaton-a-machine-that-reads-input-character-by-character-and-transitions-between-a-finite-number-of-states-deterministically-compiled-to-imperative-code\">Zero whitespace means zero splits. Two &quot;tokens.&quot; Neither is useful.\nBut a real tokenizer handles this correctly â€” producing <code>Keyword(if)</code>, <code>LParen</code>, <code>Identifier(x)</code>, <code>GreaterEqual</code>, <code>Number(42)</code>, <code>RParen</code>, <code>LBrace</code>, <code>Keyword(return)</code>, <code>Keyword(true)</code>, <code>Semicolon</code>, <code>RBrace</code>, <code>EOF</code> â€” with no whitespace anywhere in the input.\n<strong>The real model</strong>: a tokenizer is a character-by-character finite state machine. It does not look for spaces. It reads one character at a time, asks &quot;what state am I in and what does this character mean?&quot;, and decides: extend the current token, or emit what I have and start fresh. Whitespace is just another character â€” one that happens to trigger &quot;emit and don&#39;t include this character.&quot; A digit followed by a letter triggers &quot;emit the number, start an identifier.&quot; An <code>=</code> followed by another <code>=</code> triggers &quot;emit a two-character <code>==</code>.&quot;\nThe boundaries are <strong>state transitions</strong>, not spaces. That&#39;s why <code>x&gt;=42</code> tokenizes correctly with zero whitespace â€” the scanner changes state every time the character class changes, and those state changes are the boundaries.\nKeep this model in your head as you build: every branch in your scanner&#39;s main loop is a state transition in a finite automaton. You are building a DFA (deterministic finite automaton â€” a machine that reads input character by character and transitions between a finite number of states deterministically) compiled to imperative code.</h2>\n<h2 id=\"finite-state-machines-the-theoretical-heart\">Finite State Machines: The Theoretical Heart</h2>\n<blockquote>\n<p><strong>ðŸ”‘ Foundation: FSMs as the foundation of lexical analysis</strong></p>\n<p><strong>What it IS</strong>\nA Finite State Machine (FSM) is a mathematical model of computation that exists in exactly one of a finite number of <strong>states</strong> at any given time. It moves from one state to another (a <strong>transition</strong>) in response to external inputs. </p>\n</blockquote>\n<p>In the context of lexical analysis, an FSM acts as a &quot;character eater.&quot; It starts in an initial state and consumes characters one by one. If it lands in an <strong>accepting state</strong> (often drawn with a double circle) when the input ends or a delimiter is reached, it has successfully recognized a valid pattern, such as a keyword or an integer. </p>\n<p>There are two primary flavors:</p>\n<ul>\n<li><strong>Deterministic Finite Automata (DFA):</strong> For every state and input, there is exactly one transition to a next state. It is fast and predictable.</li>\n<li><strong>Non-deterministic Finite Automata (NFA):</strong> There can be multiple possible transitions for the same input, or transitions that happen without any input at all. While easier to design for complex patterns, computers usually convert NFAs into DFAs to actually run them.</li>\n</ul>\n<p><strong>WHY you need it right now</strong>\nYou are building a lexer. Without an FSM, your code would likely become a tangled &quot;if-else&quot; nightmare of lookaheads and nested loops. FSMs provide a formal, bug-resistant way to map raw text into tokens. Instead of writing logic for every possible edge case, you define the states (e.g., <code>IN_STRING</code>, <code>IN_NUMBER</code>, <code>START</code>) and let the state transitions handle the complexity of the input stream.</p>\n<p><strong>Key Insight: The &quot;Memoryless&quot; Model</strong>\nAn FSM doesn&#39;t need to remember the entire history of how it got to its current state; the current state <em>is</em> the history. If you are in the <code>IDENTIFIER</code> state, it doesn&#39;t matter if you&#39;ve read two letters or twentyâ€”the machine only cares about what the next character allows it to do.</p>\n<h2 id=\"the-practical-upshot-your-scanner-has-implicit-states-encoded-as-control-flow-branches-when-you39re-in-the-middle-of-scanning-a-number-you39re-in-the-quotscanning-numberquot-state-when-you-hit-a-non-digit-you-transition-to-quotemit-number-return-to-startquot-you-won39t-build-an-explicit-state-table-in-this-milestone-but-every-ifelif-in-your-main-scan-loop-is-a-state-transition-keep-that-mapping-in-mind-it-explains-why-the-code-looks-the-way-it-does\">The practical upshot: your scanner has <strong>implicit states</strong> encoded as control flow branches. When you&#39;re in the middle of scanning a number, you&#39;re in the &quot;scanning number&quot; state. When you hit a non-digit, you transition to &quot;emit number, return to start.&quot; You won&#39;t build an explicit state table in this milestone, but every <code>if/elif</code> in your main scan loop IS a state transition. Keep that mapping in mind â€” it explains why the code looks the way it does.</h2>\n<h2 id=\"designing-the-token-type-enumeration\">Designing the Token Type Enumeration</h2>\n<p>Your scanner&#39;s job is to categorize every lexeme (raw text fragment) it finds. The categories form a closed, exhaustive set â€” there&#39;s no token type &quot;other than all of these.&quot; That makes an <strong>enum</strong> the perfect tool.</p>\n<blockquote>\n<p><strong>ðŸ”‘ Foundation: Python Enum as a way to define closed</strong></p>\n<p><strong>What it IS</strong>\nA Python <code>Enum</code> (from the built-in <code>enum</code> module) is a way to create a set of symbolic names bound to unique, constant values. Unlike a standard variable or a string, an Enum represents a <strong>closed set</strong> of possibilities. </p>\n</blockquote>\n<p>Using <code>from enum import Enum, auto</code>, you can define your categories:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTEGER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PLUS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span></code></pre></div>\n<p>The <code>auto()</code> helper assigns unique values to the members so you don&#39;t have to manage integers manually.</p>\n<p><strong>WHY you need it right now</strong>\nIn a compiler or lexer, you deal with categories of data (Token Types) constantly. You <em>could</em> use &quot;magic strings&quot; like <code>&quot;INT&quot;</code> or <code>&quot;PLUS&quot;</code>, but strings are dangerous: a single typo like <code>&quot;ITN&quot;</code> will fail at runtime and can be hard to debug. </p>\n<p>Enums provide:</p>\n<ol>\n<li><strong>Exhaustiveness:</strong> You can use tools like MyPy to ensure your <code>match</code> or <code>if</code> statements cover every possible token type.</li>\n<li><strong>Readability:</strong> <code>TokenType.INTEGER</code> is self-documenting.</li>\n<li><strong>Identity over Equality:</strong> Comparing Enums (<code>token.type is TokenType.INTEGER</code>) is faster and safer than string comparison.</li>\n</ol>\n<p><strong>Key Insight: The Fixed Universe</strong>\nThink of an Enum as a &quot;Fixed Universe.&quot; By using an Enum for your Token Types, you are telling the computer: &quot;In this program, theseâ€”and ONLY theseâ€”categories exist.&quot; This prevents &quot;category creep&quot; and ensures that your logic remains predictable across the entire pipeline.</p>\n<p>Here are the categories your C-like language needs:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># 42, 3.14</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># \"hello world\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># x, myVariable, count</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># if, else, while, return, true, false, null</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators (single and multi-character)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># +, -, *, /, ==, !=, &#x3C;, >, &#x3C;=, >=, =</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Punctuation / grouping / delimiters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()  </span><span style=\"color:#6A737D\"># (, ), {, }, [, ], ;, ,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Control</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># end of input â€” sentinel for the parser</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># unrecognized character â€” carries error message</span></span></code></pre></div>\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-token-type-enum.svg\" alt=\"Token Type Enumeration â€” Complete Category Map\"></p>\n<h2 id=\"a-few-design-notes-why-operator-and-punctuation-as-separate-categories-semantically-operators-participate-in-expressions-they-have-operands-and-produce-values-while-punctuation-is-purely-structural-parentheses-group-semicolons-terminate-a-parser-cares-about-this-distinction-when-building-its-grammar-rules-grouping-them-together-would-make-the-parser39s-job-harder-why-a-single-keyword-type-instead-of-if-else-while-etc-both-approaches-are-used-in-real-compilers-a-flat-enum-like-this-is-simpler-to-start-with-you-look-at-the-lexeme-string-to-know-which-keyword-many-production-lexers-including-python39s-own-use-per-keyword-variants-for-faster-parser-dispatch-for-a-teaching-tokenizer-keyword-with-a-lexeme-check-is-clean-why-eof-as-a-token-type-this-is-critical-when-the-scanner-exhausts-the-input-it-could-return-none-raise-an-exception-or-emit-a-special-sentinel-token-none-forces-the-parser-to-null-check-every-token-access-a-raised-exception-makes-lookahead-awkward-an-eof-token-lets-the-parser-read-naturally-while-tokentype-tokentypeeof-and-the-sentinel-serves-as-an-unconditional-stopping-condition-without-it-parsers-crash-in-mysterious-ways-why-error-because-the-tokenizer-should-keep-running-after-encountering-an-unrecognized-character-if-it-raises-an-exception-immediately-you-can-only-report-one-error-per-compilation-instead-emit-an-error-token-with-position-and-the-offending-character-then-continue-scanning-by-the-end-you39ve-collected-all-the-lexical-errors-in-a-single-pass\">A few design notes:\n<strong>Why <code>OPERATOR</code> and <code>PUNCTUATION</code> as separate categories?</strong> Semantically, operators participate in expressions (they have operands and produce values) while punctuation is purely structural (parentheses group, semicolons terminate). A parser cares about this distinction when building its grammar rules. Grouping them together would make the parser&#39;s job harder.\n<strong>Why a single <code>KEYWORD</code> type instead of <code>IF</code>, <code>ELSE</code>, <code>WHILE</code>, etc.?</strong> Both approaches are used in real compilers. A flat enum like this is simpler to start with; you look at the lexeme string to know <em>which</em> keyword. Many production lexers (including Python&#39;s own) use per-keyword variants for faster parser dispatch. For a teaching tokenizer, <code>KEYWORD</code> with a lexeme check is clean.\n<strong>Why <code>EOF</code> as a token type?</strong> This is critical. When the scanner exhausts the input, it could return <code>None</code>, raise an exception, or emit a special sentinel token. <code>None</code> forces the parser to null-check every token access. A raised exception makes lookahead awkward. An <code>EOF</code> token lets the parser read naturally â€” <code>while token.type != TokenType.EOF: ...</code> â€” and the sentinel serves as an unconditional stopping condition. Without it, parsers crash in mysterious ways.\n<strong>Why <code>ERROR</code>?</strong> Because the tokenizer should keep running after encountering an unrecognized character. If it raises an exception immediately, you can only report one error per compilation. Instead, emit an <code>ERROR</code> token (with position and the offending character), then continue scanning. By the end you&#39;ve collected <em>all</em> the lexical errors in a single pass.</h2>\n<h2 id=\"the-token-data-structure\">The Token Data Structure</h2>\n<p>A token is not just a type. It needs to carry enough information for every downstream consumer â€” the parser, the error reporter, the syntax highlighter. The four fields you need are:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>type</code></td>\n<td><code>TokenType</code></td>\n<td>Category â€” what kind of token is this?</td>\n</tr>\n<tr>\n<td><code>lexeme</code></td>\n<td><code>str</code></td>\n<td>The exact raw text from source â€” <code>&quot;42&quot;</code>, <code>&quot;&gt;=&quot;</code>, <code>&quot;if&quot;</code></td>\n</tr>\n<tr>\n<td><code>line</code></td>\n<td><code>int</code></td>\n<td>1-based line number where this token starts</td>\n</tr>\n<tr>\n<td><code>column</code></td>\n<td><code>int</code></td>\n<td>1-based column offset where this token starts</td>\n</tr>\n</tbody></table>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span></code></pre></div>\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-token-struct-layout.svg\" alt=\"Token Data Structure â€” Memory/Field Layout\"></p>\n<p>A few decisions worth understanding:\n<strong>Why store <code>lexeme</code> (raw text) rather than an already-processed value?</strong> Because transforming the lexeme into a value (e.g., converting <code>&quot;42&quot;</code> to the integer <code>42</code>) is the parser&#39;s or evaluator&#39;s job, not the tokenizer&#39;s. The tokenizer&#39;s job is categorization. Storing raw text also preserves the exact source characters, which matters for error messages: if you report <code>unexpected token</code>, you want to show the user exactly what appeared in their source file, not your normalized version of it.\n<strong>Why <code>line</code> and <code>column</code> at the token level?</strong> Position metadata transforms useless error messages into actionable ones. <code>SyntaxError</code> is worthless. <code>SyntaxError at line 42, column 7: unexpected &#39;]&#39;</code> tells the user exactly where to look. This information is essentially free to collect â€” you&#39;re scanning character-by-character anyway, so incrementing two counters costs nothing. Failing to collect it here means you&#39;ll never have it downstream.\n<strong>Why <code>@dataclass</code>?</strong> Python&#39;s <code>dataclass</code> decorator auto-generates <code>__init__</code>, <code>__repr__</code>, and <code>__eq__</code> from field declarations â€” exactly what you need for a value object like a token. No boilerplate.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-token-lifecycle.svg\" alt=\"Lifecycle of a Token â€” From Character to Structured Data\"></p>\n<hr>\n<h2 id=\"building-the-scanner-infrastructure\">Building the Scanner Infrastructure</h2>\n<p>Now the engine. Your <code>Scanner</code> class wraps the source string and provides a clean interface for consuming characters. Everything else â€” multi-character operators, number scanning, string literals â€” will be built on top of exactly two primitive operations.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: list[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Position in source string</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">    # start of the current lexeme being scanned</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">  # next character to read</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Human-readable position for error messages</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Column at the start of the current lexeme</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span></code></pre></div>\n<p>Three index concepts to keep straight:</p>\n<ul>\n<li><strong><code>start</code></strong>: where the current token began (marks the left edge of the lexeme)</li>\n<li><strong><code>current</code></strong>: the position we&#39;re about to read next (right edge, exclusive)</li>\n<li><strong><code>source[start:current]</code></strong>: the lexeme accumulated so far\nThink of <code>start</code> as pinned to the first character of the token being scanned, and <code>current</code> as a cursor advancing one character at a time.</li>\n</ul>\n<h3 id=\"the-two-primitives-advance-and-peek\">The Two Primitives: <code>advance()</code> and <code>peek()</code></h3>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-scanner-advance-peek.svg\" alt=\"Scanner Core: advance() vs peek() â€” Consumption Model\"></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"True when the cursor has consumed all input.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Consume the current character and return it. Updates position tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Return the current character WITHOUT consuming it. Returns '' at end.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> ''</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span></code></pre></div>\n<h2 id=\"the-asymmetry-between-advance-and-peek-is-the-entire-basis-for-lookahead-you-can-look-at-the-next-character-to-make-a-decision-without-committing-to-consuming-it-you39ll-use-this-pattern-constantly-in-later-milestones-quotif-the-next-character-is-then-this-is-not-just-quot-why-does-peek-return-3939-at-end-of-input-instead-of-raising-an-exception-because-you39ll-call-peek-in-conditions-while-peek-39n39-should-terminate-cleanly-at-eof-without-needing-a-separate-is_at_end-check-in-every-loop-an-empty-string-compares-false-to-any-non-empty-character-string-so-the-termination-condition-handles-eof-naturally\">The asymmetry between <code>advance</code> and <code>peek</code> is the entire basis for <strong>lookahead</strong>: you can look at the next character to make a decision without committing to consuming it. You&#39;ll use this pattern constantly in later milestones â€” &quot;if the next character is <code>=</code>, then this is <code>==</code>, not just <code>=</code>.&quot;\n<strong>Why does <code>peek()</code> return <code>&#39;&#39;</code> at end-of-input instead of raising an exception?</strong> Because you&#39;ll call <code>peek()</code> in conditions: <code>while peek() != &#39;\\n&#39;</code> should terminate cleanly at EOF without needing a separate <code>is_at_end()</code> check in every loop. An empty string compares <code>False</code> to any non-empty character string, so the termination condition handles EOF naturally.</h2>\n<h2 id=\"position-tracking-line-and-column\">Position Tracking: Line and Column</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-position-tracking.svg\" alt=\"Line & Column Tracking â€” State Evolution Through Characters\"></p>\n<p>Position tracking deserves its own discussion because it&#39;s where most beginner implementations drift and produce wrong error messages.\nThe rule is simple: <strong>update position when you consume a character, not when you emit a token.</strong> In <code>advance()</code>, after reading a <code>\\n</code>, you increment <code>line</code> and reset <code>column</code> to 1. For any other character, you increment <code>column</code>.\nBut there&#39;s a subtlety with <strong>token start position</strong>. When you&#39;re about to scan a new token, you record the position <em>before</em> starting to consume characters. That&#39;s why you need <code>start_column</code> in addition to <code>column</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _make_token</span><span style=\"color:#E1E4E8\">(self, token_type: TokenType) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create a token from the characters consumed since `start`.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(token_type, lexeme, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span></code></pre></div>\n<p>And at the beginning of each token scan, you snapshot the current column:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column  </span><span style=\"color:#6A737D\"># snapshot BEFORE advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ... dispatch on char</span></span></code></pre></div>\n<p><strong>Common pitfall â€” Windows line endings (<code>\\r\\n</code>)</strong>: The sequence <code>\\r\\n</code> is a single logical newline in Windows text files. If you increment <code>line</code> for both <code>\\r</code> and <code>\\n</code>, a Windows file will appear to have twice as many lines as it actually does. The fix: consume <code>\\r</code> silently (or as whitespace) and only count <code>\\n</code> as a newline. An alternative: normalize the source string to <code>\\n</code>-only before scanning begins.\n<strong>Common pitfall â€” Tab characters</strong>: A tab character advances the cursor by one position in the source string but might represent 4 or 8 columns visually. The simplest approach is to count a tab as 1 column (consistent with cursor/byte position) and document that behavior. If you want visual accuracy, use a configurable <code>tab_width</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tab_width </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # -1 because advance() already did +1</span></span></code></pre></div>\n<h2 id=\"for-this-project-advancing-tabs-by-1-column-is-fine-document-it-and-move-on\">For this project, advancing tabs by 1 column is fine. Document it and move on.</h2>\n<h2 id=\"the-main-scanning-loop\">The Main Scanning Loop</h2>\n<p>The <code>scan_tokens()</code> method drives the entire process. It&#39;s a loop that runs until input is exhausted, calling <code>_scan_token()</code> for each token, and then appends the EOF sentinel.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Scan the entire source and return the token list.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_token()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># whitespace returns None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Always emit EOF as the final sentinel</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.tokens.append(Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens</span></span></code></pre></div>\n<h2 id=\"note-that-_scan_token-returns-none-for-whitespace-consumed-and-discarded-silently-every-other-case-returns-a-token-object-including-errors\">Note that <code>_scan_token()</code> returns <code>None</code> for whitespace â€” consumed and discarded silently. Every other case returns a <code>Token</code> object (including errors).</h2>\n<h2 id=\"single-character-token-dispatch\">Single-Character Token Dispatch</h2>\n<p>For single-character tokens, dispatch is a simple table lookup. Python&#39;s <code>match</code> statement (3.10+) or a dictionary both work well. Here&#39;s the dispatch using a dictionary for clarity:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Class-level constant â€” defined once, not rebuilt every call</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '+'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '-'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '*'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '/'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">NOTE</span><span style=\"color:#6A737D\">: will need special handling in M3 for comments</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '('</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    ')'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '{'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '}'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '['</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    ']'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    ';'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    ','</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-single-char-dispatch.svg\" alt=\"Single-Character Token Dispatch Table\"></p>\n<p>And the dispatch logic inside <code>_scan_token()</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Single-character tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Whitespace â€” consume silently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Error â€” unrecognized character</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            char,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.start_column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-whitespace-consumption.svg\" alt=\"Whitespace Consumption â€” The Silent Consumer\"></p>\n<h2 id=\"why-put-in-the-single-character-dispatch-now-knowing-it-will-need-special-handling-for-and-comments-later-this-is-a-deliberate-scaffolding-choice-in-milestone-3-you39ll-replace-the-3939-entry-with-a-more-complex-handler-until-then-simply-produces-a-division-operator-token-which-is-correct-for-expressions-like-x-2-building-incrementally-with-each-milestone-adding-behavior-to-a-working-foundation-is-better-than-trying-to-handle-every-case-at-once-why-is-r-in-the-whitespace-set-to-handle-windows-rn-endings-the-r-is-consumed-silently-here-the-n-will-be-consumed-on-the-next-call-to-advance-and-will-trigger-the-line-increment-this-prevents-double-counting\"><strong>Why put <code>/</code> in the single-character dispatch now, knowing it will need special handling for <code>//</code> and <code>/*</code> comments later?</strong> This is a deliberate scaffolding choice. In Milestone 3, you&#39;ll replace the <code>&#39;/&#39;</code> entry with a more complex handler. Until then, <code>/</code> simply produces a division operator token, which is correct for expressions like <code>x / 2</code>. Building incrementally â€” with each milestone adding behavior to a working foundation â€” is better than trying to handle every case at once.\n<strong>Why is <code>\\r</code> in the whitespace set?</strong> To handle Windows <code>\\r\\n</code> endings. The <code>\\r</code> is consumed silently here; the <code>\\n</code> will be consumed on the next call to <code>advance()</code> and will trigger the line increment. This prevents double-counting.</h2>\n<h2 id=\"putting-it-together-the-complete-m1-scanner\">Putting It Together: The Complete M1 Scanner</h2>\n<p>Here is the complete, runnable implementation of everything covered in this milestone:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">         =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    _SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '+'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '-'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '*'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '/'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '('</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ')'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '{'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '}'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '['</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ']'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ';'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ','</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tab_width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tab_width </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tab_width</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: list[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Primitives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tab_width</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> ''</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Token construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _make_token</span><span style=\"color:#E1E4E8\">(self, token_type: TokenType) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(token_type, lexeme, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Core scan loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_token()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens.append(Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens</span></span></code></pre></div>\n<hr>\n<h2 id=\"testing-your-foundation\">Testing Your Foundation</h2>\n<p>Before moving on, verify each behavior explicitly. Tests document expected behavior and catch regressions when you add new scanning logic in later milestones.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_empty_input</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_char_operators</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"+-*/\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 4 operators + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens[:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '+'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_punctuation</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"()</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">;,\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 7</span><span style=\"color:#6A737D\">  # 6 punctuation + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens[:</span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_whitespace_consumed</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"  </span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#79B8FF\">  \\n</span><span style=\"color:#9ECBFF\">  \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # only EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_tracking</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"(</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lparen, plus, eof </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> lparen.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> lparen.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> lparen.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> plus.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> plus.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">       # after the newline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> plus.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">     # first character on new line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_token</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">  # ERROR + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '@'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_then_valid</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"After an error, scanning continues normally.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#6A737D\">   # '@'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#6A737D\">  # '+'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_mixed_single_char</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"( ) { } [ ]\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexemes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.lexeme </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens[:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]]  </span><span style=\"color:#6A737D\"># exclude EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> lexemes </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'('</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">')'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'{'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'}'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'['</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">']'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_column_reset_on_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\";</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">;\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # first ';' at column 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # second ';' at column 1 of new line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multiple_errors</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All errors in input are collected, not just the first.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@#$\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(error_tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span></span></code></pre></div>\n<h2 id=\"run-all-of-these-they-should-pass-with-the-implementation-above-if-any-fail-the-position-tracking-is-the-most-common-culprit-check-the-advance-method-and-the-start_column-snapshot-timing-carefully\">Run all of these. They should pass with the implementation above. If any fail, the position tracking is the most common culprit â€” check the <code>advance()</code> method and the <code>start_column</code> snapshot timing carefully.</h2>\n<h2 id=\"three-level-view-what39s-really-happening\">Three-Level View: What&#39;s Really Happening</h2>\n<h2 id=\"it39s-worth-seeing-this-milestone-from-all-three-levels-of-the-compiler-stack-level-1-source-language-your-user-types-x-42-they-see-five-tokens-with-three-spaces-the-spaces-are-invisible-scaffolding-boundary-hints-but-not-the-actual-boundaries-level-2-the-scanner-what-you39re-building-now-the-scanner-reads-8-characters-it-has-no-concept-of-quota-tokenquot-when-it-starts-it-builds-up-the-token-character-by-character-emitting-when-the-state-transition-fires-single-char-dispatch-emit-immediately-x-not-in-dispatch-table-in-m2-will-become-identifier-scanner-single-char-emit-4-digit-in-m2-will-enter-number-scanning-state-single-char-emit-level-3-the-parser-downstream-consumer-the-parser-receives-a-listtoken-and-never-sees-characters-again-it-thinks-in-terms-of-token-types-calling-something-like-consumetokentypepunctuation-3939-the-token-stream-is-the-interface-contract-between-lexer-and-parser-every-mistake-in-your-tokentype-design-forces-the-parser-to-work-around-it\">It&#39;s worth seeing this milestone from all three levels of the compiler stack.\n<strong>Level 1 â€” Source Language</strong>: Your user types <code>(x + 42)</code>. They see five tokens with three spaces. The spaces are invisible scaffolding â€” boundary hints, but not the actual boundaries.\n<strong>Level 2 â€” The Scanner (what you&#39;re building now)</strong>: The scanner reads 8 characters. It has no concept of &quot;a token&quot; when it starts â€” it builds up the token character by character, emitting when the state transition fires. <code>(</code> â†’ single-char dispatch â†’ emit immediately. <code>x</code> â†’ not in dispatch table â†’ (in M2, will become identifier scanner). <code>+</code> â†’ single-char â†’ emit. <code>4</code> â†’ digit â†’ (in M2, will enter number-scanning state). <code>)</code> â†’ single-char â†’ emit.\n<strong>Level 3 â€” The Parser (downstream consumer)</strong>: The parser receives a <code>list[Token]</code> and never sees characters again. It thinks in terms of token types, calling something like <code>consume(TokenType.PUNCTUATION, &#39;(&#39;)</code>. The token stream is the interface contract between lexer and parser. Every mistake in your <code>TokenType</code> design forces the parser to work around it.</h2>\n<h2 id=\"design-decision-why-advance-updates-position\">Design Decision: Why <code>advance()</code> Updates Position</h2>\n<p>You might wonder: why does <code>advance()</code> update <code>line</code> and <code>column</code>? Why not update them separately, or in <code>scan_token()</code>?</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Update in <code>advance()</code> (chosen)</strong></td>\n<td>Position is always correct right after consuming a char; no chance of forgetting to update</td>\n<td>Must be careful about <code>start_column</code> snapshot timing</td>\n</tr>\n<tr>\n<td>Update in <code>scan_token()</code> after each token</td>\n<td>Simpler to reason about</td>\n<td>Position is wrong <em>during</em> multi-character token scanning â€” string literals spanning lines would report wrong end position</td>\n</tr>\n<tr>\n<td>Update lazily (recount from <code>start</code>)</td>\n<td>Simple <code>advance()</code></td>\n<td>O(n) position computation â€” unacceptable for large files</td>\n</tr>\n<tr>\n<td>Updating position in <code>advance()</code> is the right choice because it keeps position <strong>in sync with the character stream</strong>, not the token stream. Multi-character tokens (strings, comments, identifiers) span multiple characters. If you only updated position at token boundaries, position tracking inside those tokens would be wrong.</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"knowledge-cascade-quotlearn-one-unlock-tenquot\">Knowledge Cascade â€” &quot;Learn One, Unlock Ten&quot;</h2>\n<h2 id=\"you39ve-built-a-character-level-scanner-here39s-what-this-unlocks-formal-automata-theory-your-scanner-is-a-dfa-implementation-the-_single_char_tokens-dispatch-table-is-the-dfa39s-transition-function-for-the-quotstartquot-state-the-ifelifelse-structure-in-_scan_token-enumerates-all-transitions-from-that-state-when-you-add-multi-character-operators-in-m2-you39ll-add-transitions-to-sub-states-the-quotseen-one-quot-state-formal-language-theory-tells-you-that-any-regular-language-the-class-that-contains-all-tokenizable-languages-can-be-recognized-by-a-dfa-your-c-like-language39s-lexical-grammar-is-regular-that39s-why-this-approach-works-parser-expectations-downstream-contract-the-token-stream-you-emit-in-this-milestone-is-the-alphabet-for-the-parser-that-consumes-it-if-your-tokentype-categories-are-coarse-eg-mixing-keyword-and-identifier-into-a-single-name-type-the-parser-must-inspect-every-name-token39s-lexeme-to-decide-what-grammar-rule-applies-if-your-categories-are-fine-grained-the-parser-can-dispatch-purely-on-type-the-parser-is-written-against-the-contract-you-establish-here-changing-tokentype-in-m3-would-require-changing-the-parser-too-syntax-highlighting-in-your-editor-every-editor-that-does-syntax-highlighting-vs-code-vim-emacs-runs-a-tokenizer-on-every-keystroke-the-quotconfused-mid-stringquot-highlighting-you39ve-seen-where-adding-an-unmatched-quot-suddenly-turns-half-your-file-orange-is-exactly-the-error-and-string-token-types-interacting-when-the-tokenizer-can39t-find-the-closing-quote-it-emits-an-unterminated-string-that-swallows-everything-until-the-next-quot-now-you-know-why-you-also-know-the-fix-language-servers-use-incremental-re-tokenization-re-scan-only-from-the-changed-position-to-handle-this-efficiently-the-linker39s-symbol-table-cross-domain-the-keyword-lookup-table-you39ll-build-in-m2-mapping-strings-like-quotifquot-to-tokentypekeyword-is-the-same-fundamental-data-structure-as-a-linker39s-symbol-table-a-hash-map-from-name-to-meaning-in-the-linker-it-maps-symbol-names-to-addresses-in-your-tokenizer-it-maps-lexeme-strings-to-token-types-hash-tables-as-quotname-meaningquot-registries-are-one-of-the-most-universal-data-structures-in-systems-programming-error-recovery-as-a-design-philosophy-you-made-error-a-token-type-rather-than-an-exception-this-is-the-same-philosophy-behind-rust39s-resultltt-egt-errors-are-values-not-exceptional-control-flow-languages-that-lean-on-exceptions-for-lexical-errors-like-early-implementations-of-many-scripting-languages-can-only-report-one-error-per-run-compilers-that-treat-errors-as-values-gcc-clang-rust39s-compiler-collect-all-errors-and-report-them-all-at-once-your-choice-here-reflects-that-philosophy-at-the-smallest-scale-position-metadata-as-a-universal-pattern-the-line-and-column-you39re-tracking-are-metadata-information-about-the-data-rather-than-the-data-itself-this-pattern-appears-everywhere-http-headers-metadata-about-the-body-database-index-pages-metadata-about-row-locations-git-commits-metadata-about-diffs-collecting-metadata-eagerly-and-cheaply-while-the-primary-computation-is-happening-anyway-is-a-general-principle-retrofitting-it-later-is-always-expensive\">You&#39;ve built a character-level scanner. Here&#39;s what this unlocks:\n<strong>â†’ Formal automata theory</strong>: Your scanner IS a DFA implementation. The <code>_SINGLE_CHAR_TOKENS</code> dispatch table is the DFA&#39;s transition function for the &quot;start&quot; state. The <code>if/elif/else</code> structure in <code>_scan_token()</code> enumerates all transitions from that state. When you add multi-character operators in M2, you&#39;ll add transitions to sub-states (the &quot;seen one <code>=</code>&quot; state). Formal language theory tells you that any regular language â€” the class that contains all tokenizable languages â€” can be recognized by a DFA. Your C-like language&#39;s lexical grammar is regular; that&#39;s <em>why</em> this approach works.\n<strong>â†’ Parser expectations (downstream contract)</strong>: The token stream you emit in this milestone is the alphabet for the parser that consumes it. If your <code>TokenType</code> categories are coarse (e.g., mixing <code>KEYWORD</code> and <code>IDENTIFIER</code> into a single <code>NAME</code> type), the parser must inspect every <code>NAME</code> token&#39;s lexeme to decide what grammar rule applies. If your categories are fine-grained, the parser can dispatch purely on type. The parser is written against the contract you establish here â€” changing <code>TokenType</code> in M3 would require changing the parser too.\n<strong>â†’ Syntax highlighting in your editor</strong>: Every editor that does syntax highlighting (VS Code, Vim, Emacs) runs a tokenizer on every keystroke. The &quot;confused mid-string&quot; highlighting you&#39;ve seen â€” where adding an unmatched <code>&quot;</code> suddenly turns half your file orange â€” is exactly the <code>ERROR</code> and <code>STRING</code> token types interacting. When the tokenizer can&#39;t find the closing quote, it emits an unterminated string that swallows everything until the next <code>&quot;</code>. Now you know why. You also know the fix: language servers use incremental re-tokenization â€” re-scan only from the changed position â€” to handle this efficiently.\n<strong>â†’ The linker&#39;s symbol table (cross-domain)</strong>: The keyword lookup table you&#39;ll build in M2 (mapping strings like <code>&quot;if&quot;</code> to <code>TokenType.KEYWORD</code>) is the same fundamental data structure as a linker&#39;s symbol table â€” a hash map from name to meaning. In the linker, it maps symbol names to addresses. In your tokenizer, it maps lexeme strings to token types. Hash tables as &quot;name â†’ meaning&quot; registries are one of the most universal data structures in systems programming.\n<strong>â†’ Error recovery as a design philosophy</strong>: You made <code>ERROR</code> a token type rather than an exception. This is the same philosophy behind Rust&#39;s <code>Result&lt;T, E&gt;</code> â€” errors are values, not exceptional control flow. Languages that lean on exceptions for lexical errors (like early implementations of many scripting languages) can only report one error per run. Compilers that treat errors as values (GCC, Clang, Rust&#39;s compiler) collect all errors and report them all at once. Your choice here reflects that philosophy at the smallest scale.\n<strong>â†’ Position metadata as a universal pattern</strong>: The <code>line</code> and <code>column</code> you&#39;re tracking are metadata â€” information about the data rather than the data itself. This pattern appears everywhere: HTTP headers (metadata about the body), database index pages (metadata about row locations), Git commits (metadata about diffs). Collecting metadata eagerly and cheaply, while the primary computation is happening anyway, is a general principle. Retrofitting it later is always expensive.</h2>\n<h2 id=\"what-you39ve-built\">What You&#39;ve Built</h2>\n<p>You now have a functioning scanner skeleton. It can:</p>\n<ul>\n<li>Categorize every lexeme in a closed type system (<code>TokenType</code>)</li>\n<li>Store tokens with full position information (<code>Token</code>)</li>\n<li>Consume source code character-by-character with <code>advance()</code></li>\n<li>Look ahead without consuming with <code>peek()</code></li>\n<li>Track <code>line</code> and <code>column</code> accurately through newlines and tabs</li>\n<li>Emit single-character operator and punctuation tokens</li>\n<li>Silently consume whitespace</li>\n<li>Emit a sentinel <code>EOF</code> token when input is exhausted</li>\n<li>Emit <code>ERROR</code> tokens for unrecognized characters and continue scanning\nIn Milestone 2, you&#39;ll build on <code>peek()</code> to implement multi-character operators (<code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&gt;=</code>), number literal scanning, and identifier + keyword recognition. Everything in M2 uses <code>advance()</code> and <code>peek()</code> as its only interface to the source string â€” the primitives you built here carry the entire scanner.</li>\n</ul>\n<hr>\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m2 -->\n<!-- MS_ID: tokenizer-m2 -->\n<h1 id=\"milestone-2-multi-character-tokens-amp-maximal-munch\">Milestone 2: Multi-Character Tokens &amp; Maximal Munch</h1>\n<h2 id=\"where-you39re-starting\">Where You&#39;re Starting</h2>\n<p>You have a working scanner skeleton. It reads characters, tracks positions, emits single-character tokens, swallows whitespace, and reports errors â€” all in under 100 lines of clean Python. The foundation is solid.\nBut try to scan <code>x &gt;= 42</code> with what you&#39;ve built and you&#39;ll get: <code>Identifier(x)</code>, <code>OPERATOR(&gt;)</code>, <code>ERROR(=)</code>, <code>Number(42)</code>. The <code>&gt;=</code> falls apart into two pieces because your scanner doesn&#39;t look ahead. That broken output would crash any parser trying to consume it.\nThis milestone fixes that â€” and in doing so, teaches you the single most important principle in lexical analysis: <strong>maximal munch</strong>.\nBy the end of M2, your scanner will correctly handle:</p>\n<ul>\n<li>Two-character operators (<code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&gt;=</code>) as single indivisible tokens</li>\n<li>Integer and floating-point number literals</li>\n<li>Identifiers and keyword recognition via a lookup table</li>\n<li>The tricky edge case <code>&gt;==</code> (maximal munch gives you <code>&gt;=</code> then <code>=</code>, never <code>&gt;</code> then <code>==</code>)\nEverything builds on <code>advance()</code> and <code>peek()</code> â€” the two primitives you already have.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-satellite-map.svg\" alt=\"Tokenizer System â€” Satellite Map (Home Base)\"></p>\n<hr>\n<h2 id=\"the-core-misconception-characters-know-what-they-are\">The Core Misconception: Characters Know What They Are</h2>\n<p>Here&#39;s what most beginners instinctively do when they see <code>=</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ The intuitive but wrong approach</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '='</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<p>It feels right. You saw <code>=</code>, you know what <code>=</code> is, you emit <code>ASSIGN</code>. Done.\nExcept it&#39;s not done. <code>=</code> doesn&#39;t exist in isolation â€” it exists in a stream. The character <em>after</em> it changes everything:</p>\n<ul>\n<li><code>=</code> followed by <code>=</code> â†’ <code>==</code> (equality comparison, <code>GreaterEqual</code>)</li>\n<li><code>=</code> followed by anything else â†’ <code>=</code> (assignment, <code>ASSIGN</code>)\nThe moment you emit <code>ASSIGN</code> without checking what comes next, you&#39;ve made an irrecoverable commitment. You can&#39;t un-emit a token. You can&#39;t &quot;put it back.&quot; The parser will see <code>ASSIGN</code> + <code>ASSIGN</code> instead of <code>EQUALS</code> and it will fail â€” or worse, silently misparse the code.\n<strong>The real model</strong>: a character&#39;s identity is not determined by what it is alone, but by what it is <em>plus</em> the context of what follows. This is the insight maximal munch encodes.</li>\n</ul>\n<hr>\n<h2 id=\"the-maximal-munch-principle\">The Maximal Munch Principle</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-maximal-munch-principle.svg\" alt=\"Maximal Munch â€” Why Greedy Wins\"></p>\n<p>Maximal munch is a rule for resolving ambiguity in lexical analysis: <strong>always consume the longest sequence of characters that forms a valid token.</strong>\nWhen you reach a character that <em>could</em> begin multiple different tokens, you don&#39;t emit immediately. You look ahead. If the next character extends the current token into a longer valid token, you consume it. You keep extending until the next character would break the pattern. Only then do you emit.\nFormally: among all possible tokenizations of the input, choose the one where each token is as long as possible.\nHere&#39;s the same input, two strategies side by side:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Input: &quot;==&quot;\nGreedy (correct, maximal munch):\n  Read '=' â†’ could be ASSIGN or start of EQUALS\n  Peek '=' â†’ extends to EQUALS, consume it\n  Peek ';' â†’ stops, emit EQUALS(&quot;==&quot;)\nNon-greedy (wrong):\n  Read '=' â†’ emit ASSIGN(&quot;=&quot;)\n  Read '=' â†’ emit ASSIGN(&quot;=&quot;)\n  Result: two ASSIGN tokens â€” broken</code></pre></div>\n<p>The principle sounds obvious when stated plainly, but implementing it requires discipline: <strong>every character that might begin a multi-character token must peek before emitting.</strong></p>\n<h3 id=\"connection-to-regular-expression-engines\">Connection to Regular Expression Engines</h3>\n<p>Maximal munch is not an invention of compiler theory. You&#39;ve already seen it in action if you&#39;ve used regular expressions.\nWhen you write the pattern <code>a+</code> (one or more <code>a</code>s) and apply it to the string <code>&quot;aaab&quot;</code>, the regex engine matches <code>&quot;aaa&quot;</code> â€” not just <code>&quot;a&quot;</code>. That behavior is called <strong>greedy matching</strong>, and it is exactly maximal munch. The engine consumes as many <code>a</code>s as possible before stopping.\nEvery regex engine implements maximal munch by default. Understanding this connection means you now understand two things for the price of one: regex greediness and lexer tokenization are the same algorithmic principle applied in two different contexts.</p>\n<blockquote>\n<p>ðŸ”­ <strong>Deep Dive</strong>: If you want to understand the formal connection between regular expressions and finite automata (the mathematical machinery underneath both), see Sipser&#39;s <em>Introduction to the Theory of Computation</em>, Chapter 1. The key result â€” that any pattern expressible as a regex can be recognized by a DFA â€” is the theorem that makes lexical analysis tractable.</p>\n</blockquote>\n<hr>\n<h2 id=\"why-the-parser-needs-this\">Why the Parser Needs This</h2>\n<p>Before diving into implementation, understand <em>why</em> the parser needs <code>==</code> as a single token.\nConsider a grammar rule for comparison:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>equality_expr := expr '==' expr</code></pre></div>\n<h2 id=\"the-parser-consumes-tokens-one-at-a-time-when-it39s-looking-for-an-it-calls-something-like-consumetokentypeequals-if-your-lexer-emits-two-assign-tokens-instead-the-parser-receives-assign-assign-and-has-no-rule-that-matches-it-fails-or-worse-it-misparses-in-a-language-where-a-b-means-assignment-and-a-b-means-equality-check-the-single-token-vs-two-token-distinction-is-the-semantic-difference-between-modifying-state-and-reading-state-getting-it-wrong-doesn39t-just-produce-a-syntax-error-in-a-more-permissive-parser-it-could-silently-change-what-the-program-means-the-lexer39s-job-is-to-make-that-distinction-correctly-and-irreversibly-the-parser-trusts-the-token-stream-completely\">The parser consumes tokens one at a time. When it&#39;s looking for an <code>==</code>, it calls something like <code>consume(TokenType.EQUALS)</code>. If your lexer emits two <code>ASSIGN</code> tokens instead, the parser receives <code>ASSIGN</code> + <code>ASSIGN</code> and has no rule that matches â€” it fails. Or worse: it misparses.\nIn a language where <code>a = b</code> means assignment and <code>a == b</code> means equality check, the <em>single-token vs. two-token distinction is the semantic difference between modifying state and reading state</em>. Getting it wrong doesn&#39;t just produce a syntax error â€” in a more permissive parser, it could silently change what the program means.\nThe lexer&#39;s job is to make that distinction correctly and irreversibly. The parser trusts the token stream completely.</h2>\n<h2 id=\"implementing-two-character-operators-with-lookahead\">Implementing Two-Character Operators with Lookahead</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-lookahead-decision-tree.svg\" alt=\"Lookahead Decision Tree for '=', '!', '<', '>'\"></p>\n<p>The pattern for every two-character operator is identical: consume the first character, peek at the second, branch on whether the second extends the token.\nHere&#39;s a helper method that encodes this pattern once:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _match</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    If the current character (not yet consumed) equals `expected`,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    consume it and return True. Otherwise return False.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is the single-character lookahead for maximal munch.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # It matches â€” consume it</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n<p><code>_match()</code> is &quot;conditional advance&quot;: it peeks, and if the character is what you expect, it consumes. This is your lookahead primitive for the two-character operator cases.\nNow add the operator dispatch to <code>_scan_token()</code>. Replace the existing <code>_SINGLE_CHAR_TOKENS</code> dispatch for the ambiguous characters (<code>=</code>, <code>!</code>, <code>&lt;</code>, <code>&gt;</code>) with branching logic:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Two-character operator candidates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '='</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '!'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # '!' alone is not a valid token in our C-like language</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '&#x3C;'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">LESS_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '>'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Single-character tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Error â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span></code></pre></div>\n<p>You&#39;ll also need to extend <code>TokenType</code> with the new operator variants. Update the enum:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Distinguish operators semantically</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># arithmetic: +, -, *, /</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ==</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUAL</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_THAN</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQUAL</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_THAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()  </span><span style=\"color:#6A737D\"># >=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">          =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span></code></pre></div>\n<blockquote>\n<p><strong>Design note â€” Why split operators into individual types instead of keeping a single <code>OPERATOR</code>?</strong>\nEither approach works for the <em>tokenizer</em>, but the parser downstream benefits enormously from distinct types. A parser rule for <code>comparison := expr (&#39;&lt;&#39; | &#39;&lt;=&#39; | &#39;&gt;&#39; | &#39;&gt;=&#39; | &#39;==&#39; | &#39;!=&#39;) expr</code> can dispatch on <code>token.type</code> directly. With a single <code>OPERATOR</code> type, every parser rule would need to check <code>token.lexeme</code> to distinguish <code>&lt;</code> from <code>&lt;=</code> from <code>==</code>. Individual token types move that discrimination cost to where it&#39;s cheapest: the lexer, which is already reading character-by-character.</p>\n</blockquote>\n<hr>\n<h2 id=\"the-gt-case-maximal-munch-in-action\">The <code>&gt;==</code> Case: Maximal Munch in Action</h2>\n<p>Let&#39;s trace through <code>&gt;==</code> step by step. This is the canonical test for whether your maximal munch implementation is correct.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-gteqeq-trace.svg\" alt=\"Tricky Input: '>==' â€” Maximal Munch Step-by-Step\"></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Input: &quot;&gt;==&quot;\n       ^\n       current=0, start=0\nStep 1: _scan_token() called\n  start = 0, start_column = 1\n  advance() â†’ returns '&gt;', current=1, column=2\n  char == '&gt;' â†’ branch to '&gt;'-handler\n  _match('='):\n    source[1] == '=' â†’ YES\n    advance() â†’ consumes '=', current=2, column=3\n    returns True\n  _make_token(GREATER_EQUAL)\n  lexeme = source[0:2] = &quot;&gt;=&quot;\n  â†’ emit Token(GREATER_EQUAL, &quot;&gt;=&quot;, 1, 1)\nStep 2: _scan_token() called again\n  start = 2, start_column = 3\n  advance() â†’ returns '=', current=3, column=4\n  char == '=' â†’ branch to '='-handler\n  _match('='):\n    is_at_end() â†’ True (current=3 == len(&quot;&gt;==&quot;))\n    returns False\n  _make_token(ASSIGN)\n  lexeme = source[2:3] = &quot;=&quot;\n  â†’ emit Token(ASSIGN, &quot;=&quot;, 1, 3)\nFinal token stream: [GREATER_EQUAL(&quot;&gt;=&quot;), ASSIGN(&quot;=&quot;), EOF]</code></pre></div>\n<h2 id=\"the-key-moment-is-in-step-1-when-_match3939-is-called-it-looks-at-source1-which-is-the-second-in-the-input-it-consumes-that-character-current-advances-to-2-locking-in-gt-as-greater_equal-then-in-step-2-the-scanner-starts-fresh-at-position-2-reads-the-third-character-peeks-ahead-end-of-input-and-emits-assign-this-is-exactly-right-gt-is-unambiguously-gt-followed-by-under-maximal-munch-a-human-programmer-writing-if-x-gt-1-would-get-a-parse-error-not-a-lex-error-but-the-tokenizer39s-job-is-correct-regardless-of-what-the-parser-does-with-it\">The key moment is in Step 1: when <code>_match(&#39;=&#39;)</code> is called, it looks at <code>source[1]</code> which is <code>=</code> â€” the <em>second</em> <code>=</code> in the input. It consumes that character (current advances to 2), locking in <code>&gt;=</code> as <code>GREATER_EQUAL</code>. Then in Step 2, the scanner starts fresh at position 2, reads the <em>third</em> character (<code>=</code>), peeks ahead (end of input), and emits <code>ASSIGN</code>.\nThis is exactly right. <code>&gt;==</code> is unambiguously <code>&gt;=</code> followed by <code>=</code> under maximal munch. A human programmer writing <code>if (x &gt;= = 1)</code> would get a parse error (not a lex error), but the tokenizer&#39;s job is correct regardless of what the parser does with it.</h2>\n<h2 id=\"number-literal-scanning\">Number Literal Scanning</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-number-scanning-fsm.svg\" alt=\"Number Literal Scanning â€” Finite State Machine\"></p>\n<p>Number literals are the first case where you need to stay in a scanning loop â€” consuming multiple characters before emitting. The FSM has three states:</p>\n<ul>\n<li><strong>INTEGER</strong>: consuming digits</li>\n<li><strong>SAW_DOT</strong>: consumed a dot after digits (might be float)</li>\n<li><strong>FLOAT</strong>: consuming digits after the dot\nHere&#39;s the scanning logic:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Called after consuming the first digit. Scans the rest of the number.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Handles integers (42, 0) and floats (3.14).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Consume remaining integer digits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#E1E4E8\"> peek_is_digit(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peek()):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for a decimal point followed by more digits (float)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> peek_is_digit(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._peek_next()):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the '.'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#E1E4E8\"> peek_is_digit(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peek()):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<p>You need two helper functions and a <code>_peek_next()</code> method:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _peek_next</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Look TWO characters ahead without consuming. Returns '' at/past end.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Used to distinguish '3.' (trailing dot, integer) from '3.14' (float).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> >=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> ''</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> peek_is_digit</span><span style=\"color:#E1E4E8\">(ch: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ch.isdigit()  </span><span style=\"color:#6A737D\"># or: ch in '0123456789'</span></span></code></pre></div>\n<p>And integrate into <code>_scan_token()</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">elif</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number()</span></span></code></pre></div>\n<h3 id=\"the-trailing-dot-decision\">The Trailing-Dot Decision</h3>\n<p>What should your scanner do with <code>3.</code>? There&#39;s a dot after the digits, but no digits after the dot. You have three options:</p>\n<table>\n<thead>\n<tr>\n<th>Decision</th>\n<th>Example</th>\n<th>Emits</th>\n<th>Used By</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Reject (chosen âœ“)</strong></td>\n<td><code>3.</code></td>\n<td><code>NUMBER(3)</code> then <code>PUNCTUATION(.)</code></td>\n<td>This project â€” simplest, consistent</td>\n</tr>\n<tr>\n<td>Accept as float</td>\n<td><code>3.</code></td>\n<td><code>NUMBER(3.)</code></td>\n<td>JavaScript, Python</td>\n</tr>\n<tr>\n<td>Reject with error</td>\n<td><code>3.</code></td>\n<td><code>NUMBER(3)</code> + ERROR or just NUMBER(3)</td>\n<td>Some strict compilers</td>\n</tr>\n<tr>\n<td>The condition <code>self.peek() == &#39;.&#39; and peek_is_digit(self._peek_next())</code> implements the reject-and-backtrack approach: you only consume the dot if a digit follows it. If <code>3.</code> appears in source, the scanner emits <code>NUMBER(&quot;3&quot;)</code> and leaves the <code>.</code> for the next call â€” which will see a lone <code>.</code> and emit an <code>ERROR</code> token (since <code>.</code> alone isn&#39;t a valid token in this C-like language).</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>This uses <strong>two characters of lookahead</strong> (<code>peek()</code> for the dot, <code>_peek_next()</code> for the digit after it) â€” a brief excursion beyond LA(1). This is common in real lexers; the formal claim that &quot;lexers use LA(1)&quot; is a simplification. The important point: you document the behavior and apply it consistently.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>What about leading dots?</strong> <code>.5</code> as a float literal? Not supported in this language. If the scanner sees <code>.</code> in the character dispatch, it&#39;s not a digit, so it won&#39;t reach <code>_scan_number()</code>. It will fall through to the error case. This is a deliberate design choice â€” document it, and it&#39;s a valid language decision.</p>\n</blockquote>\n<hr>\n<h2 id=\"identifier-scanning-and-the-keyword-lookup-table\">Identifier Scanning and the Keyword Lookup Table</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-keyword-vs-identifier.svg\" alt=\"Keyword Detection â€” Scan First, Lookup Second\"></p>\n<p>Identifiers â€” the variable names, function names, and type names a programmer writes â€” follow a simple rule: start with a letter or underscore, then any mix of letters, digits, and underscores.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_identifier</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Called after consuming the first character (letter or underscore).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Scans the rest of the identifier, then checks the keyword table.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Extract the complete identifier text</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    text </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Keyword check â€” lookup AFTER scanning the full identifier</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_KEYWORDS</span><span style=\"color:#E1E4E8\">.get(text, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(token_type)</span></span></code></pre></div>\n<p>The keyword table:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">_KEYWORDS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'if'</span><span style=\"color:#E1E4E8\">:     TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'else'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'while'</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'return'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'true'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'false'</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'null'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n<p>Integrate into <code>_scan_token()</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">elif</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier()</span></span></code></pre></div>\n<h3 id=\"the-critical-rule-scan-first-lookup-second\">The Critical Rule: Scan First, Lookup Second</h3>\n<p>This ordering â€” scan the complete identifier, <em>then</em> check if it&#39;s a keyword â€” is not just implementation convenience. It is the <strong>correct</strong> approach, and doing it in the opposite order causes a real bug.\nImagine a naive implementation that checks for keywords character-by-character:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ WRONG â€” checking mid-scan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'i'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'f'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># emits 'if' keyword</span></span></code></pre></div>\n<p>What happens with the identifier <code>iffy</code>?</p>\n<ul>\n<li>Scanner reads <code>i</code></li>\n<li>Peeks <code>f</code> â†’ triggers keyword match for <code>if</code></li>\n<li>Emits <code>KEYWORD(&quot;if&quot;)</code></li>\n<li>Continues from <code>fy</code> â†’ emits <code>IDENTIFIER(&quot;fy&quot;)</code>\nWrong. <code>iffy</code> is a single identifier. The scanner has silently broken it.\nThe scan-first approach handles this correctly:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Input: \"iffy\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># _scan_identifier() consumes 'i', 'f', 'f', 'y'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># text = \"iffy\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># _KEYWORDS.get(\"iffy\", IDENTIFIER) â†’ IDENTIFIER (not in table)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â†’ emit IDENTIFIER(\"iffy\")  âœ“</span></span></code></pre></div>\n<p>And for actual <code>if</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Input: \"if \"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># _scan_identifier() consumes 'i', 'f'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># peek() == ' ' â†’ not alnum or _, stops</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># text = \"if\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># _KEYWORDS.get(\"if\", IDENTIFIER) â†’ KEYWORD</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â†’ emit KEYWORD(\"if\")  âœ“</span></span></code></pre></div>\n<p>The identifier scanner is greedy (maximal munch again): it consumes every valid identifier character before stopping. The keyword check is a post-processing step on the complete text. This guarantees that <code>iffy</code>, <code>iff</code>, <code>i</code>, <code>if_</code>, and <code>if</code> are all handled correctly by the same code path.</p>\n<h3 id=\"why-a-dictionary-not-a-long-ifelif-chain\">Why a Dictionary, Not a Long <code>if/elif</code> Chain?</h3>\n<p>You could write:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> text </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'if'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">elif</span><span style=\"color:#E1E4E8\"> text </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'else'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ... etc.</span></span></code></pre></div>\n<p>A dictionary lookup is strictly better:</p>\n<ul>\n<li><strong>O(1) average</strong> versus O(n) for a chain of n keywords</li>\n<li><strong>Extensible</strong>: adding a keyword is one line in the dict, not a new <code>elif</code></li>\n<li><strong>Data separate from logic</strong>: the keyword table can be loaded from configuration or extended by a macro system without touching scanner code\nProduction lexers (GCC, Clang, the CPython tokenizer) all use hash maps for keyword lookup. Some use <strong>perfect hashing</strong> â€” a hash function computed specifically for the known keyword set â€” to guarantee O(1) with no collision chains. For seven keywords, a plain Python dict is more than fast enough.</li>\n</ul>\n<hr>\n<h2 id=\"the-lookahead-budget-la1\">The Lookahead Budget: LA(1)</h2>\n<p>Your lexer is described as using <strong>LA(1)</strong> â€” one character of lookahead. This is worth understanding precisely, because it explains why your design works and where its limits are.\nLA(k) means: to decide what token to emit, you need to look at most <em>k</em> characters beyond the current position. In your scanner:</p>\n<ul>\n<li>Single-character tokens: LA(0) â€” you know what to emit the moment you consume the character</li>\n<li>Two-character operators: LA(1) â€” you consume one char, peek one more</li>\n<li>Number literals: LA(1) for the integer path, LA(2) for the trailing-dot check (is it <code>3.1</code> or <code>3.</code>)</li>\n<li>Identifiers: LA(1) â€” peek one char ahead to see if the identifier continues\nThe LA(2) case for number literals is a pragmatic extension. Purists could eliminate it by accepting <code>3.</code> as a float, but that changes the language semantics. In practice, LA(1) is the <em>default</em> and LA(2) is the <em>exception for one specific disambiguation</em>. Real lexers like Clang&#39;s also occasionally peek two characters ahead for specific cases.<blockquote>\n<p><strong>Formal note</strong>: If your language&#39;s lexical grammar requires unbounded lookahead to tokenize, you can&#39;t implement it with a finite state machine â€” you&#39;d need a pushdown automaton, which is for context-free languages. The fact that our lexer works with LA(1) or LA(2) confirms that the lexical grammar is regular (or close enough to it). Parsers, which handle hierarchical structure, need LA(k) for tokens (not characters) and are where full context-free power is needed.</p>\n</blockquote>\n</li>\n</ul>\n<hr>\n<h2 id=\"complete-m2-_scan_token-putting-it-all-together\">Complete M2 <code>_scan_token()</code> â€” Putting It All Together</h2>\n<p>Here is the full updated <code>_scan_token()</code> and all helpers:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Arithmetic operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># +, -, *, /</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Comparison &#x26; assignment operators (split for parser convenience)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ==</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUAL</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_THAN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQUAL</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_THAN</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># >=</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Structural</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Control</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">           =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">         =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    _SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '+'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '-'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '*'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '/'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># will become comment-aware in M3</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '('</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ')'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '{'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '}'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '['</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ']'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ';'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ','</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    _KEYWORDS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'if'</span><span style=\"color:#E1E4E8\">:     TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'else'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'while'</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'return'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'true'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'false'</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'null'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tab_width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tab_width </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tab_width</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: list[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Primitives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tab_width</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Current character, not consumed. Returns '' at end.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> ''</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _peek_next</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"One character beyond current, not consumed. Returns '' at/past end.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> >=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> ''</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _match</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume current character if it equals `expected`. Return success.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Token construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _make_token</span><span style=\"color:#E1E4E8\">(self, token_type: TokenType) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(token_type, lexeme, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Scanners for multi-character tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Scans remaining digits after the first digit has been consumed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Float detection: dot must be followed by at least one digit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._peek_next().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume '.'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_identifier</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Scans remaining identifier chars after the first has been consumed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        text </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        token_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_KEYWORDS</span><span style=\"color:#E1E4E8\">.get(text, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(token_type)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Core scan loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Two-character operators (must check before single-char table)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '='</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '!'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '&#x3C;'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">LESS_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '>'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Number literals</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Identifiers and keywords</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Single-character tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Whitespace â€” consume silently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Error â€” unrecognized character</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_token()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens.append(Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens</span></span></code></pre></div>\n<hr>\n<h2 id=\"three-level-view-what-gt-means-at-each-layer\">Three-Level View: What <code>&gt;=</code> Means at Each Layer</h2>\n<h2 id=\"level-1-source-language-the-programmer-writes-x-gt-42-to-them-gt-is-a-single-operator-with-a-single-meaning-quotgreater-than-or-equal-toquot-they-think-in-terms-of-mathematical-comparisons-level-2-the-scanner-what-you39re-building-the-scanner-reads-gt-then-peeks-then-consumes-at-this-level-gt-is-two-characters-that-get-collapsed-into-one-token-the-scanner39s-job-is-exactly-this-collapse-taking-the-continuous-character-stream-and-imposing-structure-level-3-the-parser-and-runtime-downstream-the-parser-sees-tokengreater_equal-quotgtquot-as-a-single-atomic-unit-its-grammar-rule-for-comparisons-matches-on-greater_equal-as-a-token-type-the-evaluator-eventually-turns-greater_equal-into-a-machine-instruction-usually-a-jge-or-setge-on-x86-by-the-time-it-reaches-silicon-gt-has-been-through-three-representations-characters-token-machine-instruction\"><strong>Level 1 â€” Source Language</strong>: The programmer writes <code>x &gt;= 42</code>. To them, <code>&gt;=</code> is a single operator with a single meaning: &quot;greater than or equal to.&quot; They think in terms of mathematical comparisons.\n<strong>Level 2 â€” The Scanner (what you&#39;re building)</strong>: The scanner reads <code>&gt;</code>, then peeks <code>=</code>, then consumes <code>=</code>. At this level, <code>&gt;=</code> is two <em>characters</em> that get collapsed into one <em>token</em>. The scanner&#39;s job is exactly this collapse â€” taking the continuous character stream and imposing structure.\n<strong>Level 3 â€” The Parser and Runtime (downstream)</strong>: The parser sees <code>Token(GREATER_EQUAL, &quot;&gt;=&quot;, ...)</code> as a single atomic unit. Its grammar rule for comparisons matches on <code>GREATER_EQUAL</code> as a token type. The evaluator eventually turns <code>GREATER_EQUAL</code> into a machine instruction (usually a <code>jge</code> or <code>setge</code> on x86). By the time it reaches silicon, <code>&gt;=</code> has been through three representations: characters â†’ token â†’ machine instruction.</h2>\n<h2 id=\"common-pitfalls-and-how-to-avoid-them\">Common Pitfalls and How to Avoid Them</h2>\n<h3 id=\"pitfall-1-forgetting-to-snapshot-start_column-before-advance\">Pitfall 1: Forgetting to Snapshot <code>start_column</code> Before <code>advance()</code></h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ Wrong ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()           </span><span style=\"color:#6A737D\"># advance() changes self.column</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column  </span><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">BUG</span><span style=\"color:#6A737D\">: column is already past the first char</span></span></code></pre></div>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âœ… Correct ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column  </span><span style=\"color:#6A737D\"># snapshot BEFORE advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span></code></pre></div>\n<p>The <code>start_column</code> must be captured <em>before</em> consuming the first character. After <code>advance()</code>, <code>self.column</code> has already moved forward.</p>\n<h3 id=\"pitfall-2-42abc-number-immediately-followed-by-identifier\">Pitfall 2: <code>42abc</code> â€” Number Immediately Followed by Identifier</h3>\n<p>Your <code>_scan_number()</code> stops when <code>peek()</code> is not a digit or <code>.</code>. So <code>42abc</code> would produce <code>NUMBER(&quot;42&quot;)</code> then <code>IDENTIFIER(&quot;abc&quot;)</code>. This is arguably correct â€” most C-like languages treat <code>42abc</code> as a lex error, but our simple approach produces two valid tokens.\nIf you want a strict error here, add a check after <code>_scan_number()</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._peek_next().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Optional: flag identifier immediately after number</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # consume the rest and report error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                     self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                     self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<p>For this project, the simpler &quot;emit NUMBER then IDENTIFIER&quot; is acceptable. Document the behavior either way.</p>\n<h3 id=\"pitfall-3-keyword-inside-identifiers-must-not-match\">Pitfall 3: <code>keyword</code> Inside <code>identifiers</code> Must Not Match</h3>\n<p>This pitfall was already addressed architecturally (scan first, lookup second), but it&#39;s worth testing explicitly:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># These must ALL be IDENTIFIER, not KEYWORD:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># \"iffy\", \"while_loop\", \"return_value\", \"nullify\", \"trueness\", \"elsewhere\"</span></span></code></pre></div>\n<p>If any of these tokenize incorrectly, your keyword matching is running <em>during</em> scanning rather than <em>after</em>.</p>\n<h3 id=\"pitfall-4-float-with-trailing-dot-3\">Pitfall 4: Float With Trailing Dot (<code>3.</code>)</h3>\n<h2 id=\"with-the-current-implementation-3-tokenizes-as-numberquot3quot-followed-by-errorquotquot-since-bare-is-not-a-valid-token-this-is-correct-by-design-but-test-it-explicitly-it39s-easy-to-accidentally-consume-the-dot-inside-_scan_number-without-the-guard-_peek_nextisdigit\">With the current implementation, <code>3.</code> tokenizes as <code>NUMBER(&quot;3&quot;)</code> followed by <code>ERROR(&quot;.&quot;)</code> (since bare <code>.</code> is not a valid token). This is correct by design. But test it explicitly â€” it&#39;s easy to accidentally consume the dot inside <code>_scan_number()</code> without the guard <code>_peek_next().isdigit()</code>.</h2>\n<h2 id=\"testing-m2\">Testing M2</h2>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_two_char_operators</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"== != &#x3C;= >=\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens[:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]]  </span><span style=\"color:#6A737D\"># exclude EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> types </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">LESS_EQUAL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_maximal_munch_equals</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'==' must be ONE token, not two ASSIGN tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"==\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">  # EQUALS + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"==\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_equals</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"=\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_gteqeq_maximal_munch</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\">== must tokenize as GREATER_EQUAL + ASSIGN, not GREATER_THAN + EQUALS.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\">==\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \">=\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"=\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_integer_literal</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"42\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_float_literal</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"3.14\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"3.14\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_trailing_dot_not_float</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'3.' should emit NUMBER('3') then ERROR('.') â€” dot not consumed as part of float.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"3.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"3\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \".\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_identifier_basic</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"myVar\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"myVar\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_identifier_with_underscore</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"_count\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_count\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_keyword_if</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"if\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_keyword_not_in_identifier</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'iffy' must be IDENTIFIER, not KEYWORD('if') + IDENTIFIER('fy').\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"iffy\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">  # IDENTIFIER + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"iffy\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_all_keywords</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keywords </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'if'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'else'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'while'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'return'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'true'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'false'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'null'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> kw </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> keywords:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(kw)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"'</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">kw</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' should be KEYWORD\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> kw</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_full_expression</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"The milestone's canonical integration test.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"if (x >= 42) { return true; }\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"(\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\")\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"{\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\";\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"}\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,           </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected), (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(expected)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> tokens, got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(tokens)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tokens</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> token, (exp_type, exp_lexeme) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> zip</span><span style=\"color:#E1E4E8\">(tokens, expected):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> exp_type, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">exp_type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> for '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.lexeme</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> exp_lexeme, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Expected lexeme </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">exp_lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_less_than_no_consume</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'&#x3C;' alone emits LESS_THAN, not consuming the next char.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"&#x3C; \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"&#x3C;\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_not_equal</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"!=\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"!=\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_bare_exclamation_is_error</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'!' not followed by '=' is not a valid token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"!\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_number_position</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"  42\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">   # two spaces before '4'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_identifier_position</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">foo\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span></code></pre></div>\n<h2 id=\"all-of-these-should-pass-with-the-implementation-above-run-them-before-moving-to-m3\">All of these should pass with the implementation above. Run them before moving to M3.</h2>\n<h2 id=\"knowledge-cascade-quotlearn-one-unlock-tenquot\">Knowledge Cascade â€” &quot;Learn One, Unlock Ten&quot;</h2>\n<h2 id=\"you39ve-implemented-maximal-munch-two-character-operators-number-scanning-and-keyword-recognition-here39s-what-those-unlock-regular-expression-engines-the-_match-primitive-you-wrote-peek-ahead-consume-if-match-is-exactly-what-a-regex-engine39s-greedy-quantifier-does-internally-when-you-write-d-in-a-regex-the-engine39s-nfa-to-dfa-compiled-code-runs-a-loop-identical-to-your-while-selfpeekisdigit-selfadvance-you39ve-written-a-regex-engine39s-inner-loop-by-hand-for-a-specific-pattern-understanding-one-gives-you-the-other-for-free-operator-precedence-in-parsers-downstream-impact-the-parser-that-consumes-your-token-stream-will-have-precedence-rules-like-quotmultiplication-binds-tighter-than-additionquot-those-rules-operate-on-token-types-by-emitting-equals-as-a-single-token-not-two-assigns-you39ve-made-it-possible-for-the-parser-to-write-a-single-grammar-rule-comparison-expr-equals-expr-if-you39d-emitted-two-assign-tokens-the-parser-would-need-special-case-logic-to-reassemble-them-which-is-fragile-and-error-prone-correct-lexing-is-the-foundation-of-correct-parsing-the-keywordidentifier-boundary-in-real-compilers-gcc-and-clang-use-the-exact-same-quotscan-full-identifier-then-hash-lookupquot-strategy-clang39s-lexerlexidentifier-scans-all-alphanumeric-characters-then-calls-lookupidentifierinfo-which-hashes-the-result-and-returns-a-toktokenkind-the-structure-is-identical-to-_scan_identifier-real-world-lexers-differ-in-performance-perfect-hashing-simd-character-classification-not-in-algorithm-lak-in-parsing-theory-your-lexer-uses-la1-with-a-brief-excursion-to-la2-for-the-trailing-dot-case-parsers-use-lak-over-tokens-not-characters-ll1-parsers-the-simplest-useful-parsers-use-la1-look-at-one-token-ahead-to-decide-which-grammar-rule-to-apply-lr1-parsers-used-by-yaccbison-also-use-one-token-of-lookahead-but-in-a-more-powerful-way-the-concept-scales-the-principle-of-quotlook-ahead-by-k-to-resolve-ambiguityquot-is-the-same-whether-k-is-1-character-or-1-token-lexer-generators-what-you-could-build-next-tools-like-lex-flex-and-antlr39s-lexer-take-a-set-of-regex-patterns-and-generate-a-lexer-automatically-internally-they-compile-each-regex-to-an-nfa-merge-the-nfas-into-one-combined-nfa-convert-to-a-dfa-using-subset-construction-and-minimize-the-dfa-the-result-is-a-state-transition-table-a-2d-array-indexed-by-state-character-that-runs-faster-than-any-hand-written-ifelif-chain-you39ve-hand-written-the-conceptual-equivalent-the-generator-just-automates-the-machinery-and-optimizes-it-crafting-interpreters-chapter-4-by-robert-nystrom-walks-through-this-hand-written-approach-in-java-the-llvm-tutorial-shows-the-c-equivalent-number-literal-complexity-in-real-languages-your-number-scanner-handles-integers-and-simple-floats-real-languages-are-significantly-more-complex-c-supports-0xff-hex-0755-octal-0b1010-binary-in-c23-15e10-scientific-notation-15f-float-suffix-1ull-unsigned-long-long-python-adds-1_000_000-underscores-for-readability-and-0o755-explicit-octal-each-of-these-requires-additional-states-in-the-number-scanning-fsm-the-structure-you39ve-built-a-loop-consuming-valid-chars-branching-on-special-characters-extends-naturally-to-all-of-these-you39ve-built-the-right-architecture-adding-cases-is-mechanical\">You&#39;ve implemented maximal munch, two-character operators, number scanning, and keyword recognition. Here&#39;s what those unlock:\n<strong>â†’ Regular expression engines</strong>: The <code>_match()</code> primitive you wrote â€” peek ahead, consume if match â€” is exactly what a regex engine&#39;s greedy quantifier does internally. When you write <code>\\d+</code> in a regex, the engine&#39;s NFA-to-DFA compiled code runs a loop identical to your <code>while self.peek().isdigit(): self.advance()</code>. You&#39;ve written a regex engine&#39;s inner loop by hand, for a specific pattern. Understanding one gives you the other for free.\n<strong>â†’ Operator precedence in parsers (downstream impact)</strong>: The parser that consumes your token stream will have precedence rules like &quot;multiplication binds tighter than addition.&quot; Those rules operate on <em>token types</em>. By emitting <code>EQUALS</code> as a single token (not two <code>ASSIGN</code>s), you&#39;ve made it possible for the parser to write a single grammar rule <code>comparison := expr EQUALS expr</code>. If you&#39;d emitted two <code>ASSIGN</code> tokens, the parser would need special-case logic to reassemble them â€” which is fragile and error-prone. Correct lexing is the foundation of correct parsing.\n<strong>â†’ The keyword/identifier boundary in real compilers</strong>: GCC and Clang use the exact same &quot;scan full identifier, then hash-lookup&quot; strategy. Clang&#39;s <code>Lexer::LexIdentifier()</code> scans all alphanumeric characters, then calls <code>LookUpIdentifierInfo()</code> which hashes the result and returns a <code>tok::TokenKind</code>. The structure is identical to <code>_scan_identifier()</code>. Real-world lexers differ in performance (perfect hashing, SIMD character classification), not in algorithm.\n<strong>â†’ LA(k) in parsing theory</strong>: Your lexer uses LA(1) with a brief excursion to LA(2) for the trailing-dot case. Parsers use LA(k) over <em>tokens</em> (not characters). LL(1) parsers â€” the simplest useful parsers â€” use LA(1): look at one token ahead to decide which grammar rule to apply. LR(1) parsers (used by yacc/bison) also use one token of lookahead but in a more powerful way. The concept scales: the principle of &quot;look ahead by k to resolve ambiguity&quot; is the same whether k is 1 character or 1 token.\n<strong>â†’ Lexer generators (what you could build next)</strong>: Tools like <code>lex</code>, <code>flex</code>, and ANTLR&#39;s lexer take a set of regex patterns and generate a lexer automatically. Internally, they compile each regex to an NFA, merge the NFAs into one combined NFA, convert to a DFA using subset construction, and minimize the DFA. The result is a state transition table â€” a 2D array indexed by (state, character) â€” that runs faster than any hand-written if/elif chain. You&#39;ve hand-written the conceptual equivalent; the generator just automates the machinery and optimizes it. <em>Crafting Interpreters</em> Chapter 4 by Robert Nystrom walks through this hand-written approach in Java; the LLVM tutorial shows the C++ equivalent.\n<strong>â†’ Number literal complexity in real languages</strong>: Your number scanner handles integers and simple floats. Real languages are significantly more complex. C supports <code>0xFF</code> (hex), <code>0755</code> (octal), <code>0b1010</code> (binary in C23), <code>1.5e10</code> (scientific notation), <code>1.5f</code> (float suffix), <code>1ULL</code> (unsigned long long). Python adds <code>1_000_000</code> (underscores for readability) and <code>0o755</code> (explicit octal). Each of these requires additional states in the number-scanning FSM. The structure you&#39;ve built â€” a loop consuming valid chars, branching on special characters â€” extends naturally to all of these. You&#39;ve built the right architecture; adding cases is mechanical.</h2>\n<h2 id=\"what-you39ve-built\">What You&#39;ve Built</h2>\n<p>Your scanner now handles the full range of single and multi-character tokens:</p>\n<ul>\n<li><strong>Two-character operators</strong> (<code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&gt;=</code>) recognized as single indivisible tokens via single-character lookahead</li>\n<li><strong>Maximal munch</strong> applied consistently â€” <code>&gt;==</code> becomes <code>GREATER_EQUAL</code> + <code>ASSIGN</code>, never <code>GREATER_THAN</code> + <code>EQUALS</code></li>\n<li><strong>Integer literals</strong> (<code>42</code>, <code>0</code>, <code>1000</code>) scanned by looping over digits</li>\n<li><strong>Float literals</strong> (<code>3.14</code>, <code>0.5</code>) scanned by detecting dot-then-digit</li>\n<li><strong>Identifiers</strong> scanned by consuming alphanumeric and underscore sequences</li>\n<li><strong>Keywords</strong> recognized by hash-map lookup after scanning the complete identifier â€” <code>iffy</code> stays <code>IDENTIFIER</code>, <code>if</code> becomes <code>KEYWORD</code>\nIn Milestone 3, you&#39;ll build the two remaining complex cases: string literals with escape sequences, and comment filtering (both single-line <code>//</code> and multi-line <code>/* */</code>). String scanning introduces a new kind of state â€” you&#39;re inside a quoted region where most normal dispatch rules are suspended â€” and comments require you to distinguish <code>/</code> (division) from <code>//</code> (line comment) from <code>/*</code> (block comment start) using exactly the <code>_match()</code> primitive you just built.</li>\n</ul>\n<hr>\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m3 -->\n<!-- MS_ID: tokenizer-m3 -->\n<h1 id=\"milestone-3-strings-amp-comments\">Milestone 3: Strings &amp; Comments</h1>\n<h2 id=\"where-you39re-starting\">Where You&#39;re Starting</h2>\n<p>Your scanner is genuinely useful now. It handles single-character tokens, two-character operators, number literals, identifiers, and keywords. You can tokenize <code>if (x &gt;= 42) { return true; }</code> perfectly, producing exactly the right token stream.\nBut try scanning a real program and you&#39;ll hit two categories of input your scanner cannot yet handle:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> '''</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">// Find the maximum value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">x = getValue(); /* default */</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">greeting = \"hello // not a comment\";</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">result = /* start */ 42;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'''</span></span></code></pre></div>\n<p>The <code>//</code> line comment makes your scanner emit <code>OPERATOR(&#39;/&#39;)</code> + <code>OPERATOR(&#39;/&#39;)</code> + <code>IDENTIFIER(&#39;Find&#39;)</code> + ... â€” noise. The string <code>&quot;hello // not a comment&quot;</code> starts a string, then triggers comment logic mid-string. The <code>/* */</code> block comment gets consumed as <code>/</code>, <code>*</code>, <code>/</code>, more tokens.\nEverything breaks. The scanner doesn&#39;t know it&#39;s &quot;inside&quot; anything special.\nThis milestone fixes that â€” and in doing so, teaches you the deepest concept in lexical analysis: <strong>the tokenizer is a state machine with distinct modes, and inside each mode, all the normal rules are suspended.</strong></p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-satellite-map.svg\" alt=\"Tokenizer System â€” Satellite Map (Home Base)\"></p>\n<hr>\n<h2 id=\"the-core-misconception-the-tokenizer-has-one-set-of-rules\">The Core Misconception: The Tokenizer Has One Set of Rules</h2>\n<p>Here&#39;s what the previous two milestones might have led you to believe: the scanner reads characters, consults its dispatch table, calls the right handler, and emits a token. Strings and comments are just &quot;different character patterns&quot; â€” maybe a bit more complex, but handled the same way.\nThis model is wrong, and breaking it is the revelation of this milestone.\nConsider what happens when you&#39;re scanning this input and you reach the <code>/</code> inside the string:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>&quot;hello // world&quot;</code></pre></div>\n<p>Your current <code>_scan_token()</code> dispatch would see <code>/</code> and emit <code>OPERATOR(&#39;/&#39;)</code>. But <code>/</code> here is not an operator â€” it&#39;s just a character that happens to appear inside a string literal. The rule for <code>/</code> is completely different depending on <strong>where you are</strong>.\nThe same applies in reverse. When you&#39;re inside a <code>/* comment */</code>, you see <code>&quot;</code> characters â€” but they do not begin strings. The <code>&quot;</code> inside a comment is inert. It&#39;s just a character.\n<strong>The fundamental insight</strong>: once you enter a string or comment, you are in a completely different context with its own rules. The normal dispatch table â€” the one that maps characters to token types â€” is <em>suspended</em>. You stay in this special context until a specific exit condition is met (the closing <code>&quot;</code>, <code>\\n</code>, or <code>*/</code>). Only then do you return to normal scanning.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-scanner-state-modes.svg\" alt=\"Scanner State Machine â€” NORMAL vs STRING vs COMMENT Modes\"></p>\n<h2 id=\"this-is-not-an-engineering-choice-it-is-a-formal-necessity-let39s-understand-why\">This is not an engineering choice â€” it is a formal necessity. Let&#39;s understand why.</h2>\n<h2 id=\"why-regular-expressions-cannot-tokenize-real-languages\">Why Regular Expressions Cannot Tokenize Real Languages</h2>\n<blockquote>\n<p>ðŸ”­ <strong>Deep Dive</strong>: The formal proof that balanced delimiters (matching quotes, brackets, etc.) cannot be expressed by any regular expression is given by the Pumping Lemma for regular languages. For a solid foundation, see Sipser&#39;s <em>Introduction to the Theory of Computation</em>, Chapter 1 (Section 1.4, &quot;Nonregular Languages&quot;). It&#39;s about 10 pages and provides the canonical result.\nHere&#39;s the intuition, without the formalism.\nA <strong>regular language</strong> is a language that can be recognized by a finite automaton â€” a machine with a fixed, finite number of states and no memory beyond which state it&#39;s currently in. Your scanner so far IS a finite automaton: it has a &quot;start&quot; state, transitions based on the current character, and reaches an accepting state (emitting a token) at the right moment.\nThe critical word is <strong>finite</strong>. A finite automaton cannot count. It cannot remember &quot;I opened a string literal three characters ago.&quot; It has no stack, no counter, no history beyond its current state.\nBut recognizing <code>&quot;...&quot;</code> â€” a string that starts with <code>&quot;</code> and ends with <code>&quot;</code> â€” <em>requires</em> remembering that you opened a quote. You need to know: &quot;I am inside a string right now.&quot; That knowledge is <strong>context</strong> â€” exactly what a finite automaton lacks.\nThe workaround is to encode that context into the state itself. Instead of having one &quot;start&quot; state, you have multiple states: <code>NORMAL</code>, <code>IN_STRING</code>, <code>IN_SINGLE_COMMENT</code>, <code>IN_MULTI_COMMENT</code>. Each is a different mode of the machine. Within <code>IN_STRING</code>, the character <code>&quot;</code> means &quot;exit string mode and emit a STRING token&quot; â€” but in <code>NORMAL</code> mode, the same character means &quot;enter string mode.&quot;\nThe key insight is that you are not making the language more complex â€” you are making your <em>representation</em> of the language richer by adding states. The machine is still finite (you have a fixed set of modes), but the modes themselves encode the &quot;I am inside a string&quot; knowledge that a pure regular expression cannot express.\nThis is formally why &quot;a regex can&#39;t tokenize real programming languages.&quot; It&#39;s not a limitation of skill â€” it&#39;s a theorem. The workaround is mode-tracking in an augmented DFA. That&#39;s what you&#39;re building in this milestone.</p>\n</blockquote>\n<hr>\n<h2 id=\"the-three-modes-a-state-machine-with-context\">The Three Modes: A State Machine with Context</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-scanner-state-modes.svg\" alt=\"Scanner State Machine â€” NORMAL vs STRING vs COMMENT Modes\"></p>\n<p>Your scanner after this milestone will operate in four conceptually distinct modes:</p>\n<table>\n<thead>\n<tr>\n<th>Mode</th>\n<th>Trigger to Enter</th>\n<th>Trigger to Exit</th>\n<th>Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>NORMAL</code></td>\n<td>start of input</td>\n<td>â€”</td>\n<td>Normal dispatch table applies</td>\n</tr>\n<tr>\n<td><code>IN_STRING</code></td>\n<td><code>&quot;</code> in NORMAL mode</td>\n<td><code>&quot;</code> or EOF/newline</td>\n<td>All chars are string content; <code>\\</code> triggers escape</td>\n</tr>\n<tr>\n<td><code>IN_SINGLE_COMMENT</code></td>\n<td><code>//</code> in NORMAL mode</td>\n<td><code>\\n</code> or EOF</td>\n<td>All chars discarded; no tokens emitted</td>\n</tr>\n<tr>\n<td><code>IN_MULTI_COMMENT</code></td>\n<td><code>/*</code> in NORMAL mode</td>\n<td><code>*/</code> or EOF</td>\n<td>All chars discarded; newlines update line count</td>\n</tr>\n<tr>\n<td>You won&#39;t implement these as explicit state variables â€” instead, they manifest as separate methods (<code>_scan_string()</code>, <code>_skip_line_comment()</code>, <code>_skip_block_comment()</code>) that each own their scanning loop. The &quot;mode&quot; is implicit in which method is currently executing.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>The beauty of this design: each method is a complete mini-scanner for its context. It doesn&#39;t need to know about the main dispatch table. When it finishes (emits a token or runs out of input), control returns to <code>_scan_token()</code> in NORMAL mode.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"the-ambiguity-division-line-comment-or-block-comment\">The <code>/</code> Ambiguity: Division, Line Comment, or Block Comment?</h2>\n<p>Before diving into strings, let&#39;s resolve the <code>&#39;/&#39;</code> problem â€” because it&#39;s the gateway to comments and the cleanest example of mode-switching.\nIn your current scanner, <code>&#39;/&#39;</code> appears in <code>_SINGLE_CHAR_TOKENS</code> and emits <code>OPERATOR</code>. But <code>/</code> can mean three completely different things depending on what follows it:</p>\n<ul>\n<li><code>/</code> followed by anything other than <code>/</code> or <code>*</code> â†’ Division operator</li>\n<li><code>/</code> followed by <code>/</code> â†’ Start of a single-line comment (consume everything until <code>\\n</code>)</li>\n<li><code>/</code> followed by <code>*</code> â†’ Start of a multi-line comment (consume everything until <code>*/</code>)</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-comment-vs-division.svg\" alt=\"The '/' Ambiguity â€” Division, Single-Line, or Multi-Line?\"></p>\n<p>This is maximal munch applied to comment detection â€” the same principle from Milestone 2, now extended to a three-way branch. Here&#39;s the dispatch:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'/'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Single-line comment: consume until newline or EOF</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._skip_line_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # comments produce no token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'*'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Multi-line comment: consume until */ or EOF</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._skip_block_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result  </span><span style=\"color:#6A737D\"># might be None (success) or ERROR (unterminated)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Just a division operator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<h2 id=\"remove-3939-from-_single_char_tokens-it-now-has-its-own-branch-the-first-character-is-3939-already-consumed-by-advance-then-_match3939-or-_match3939-peeks-at-the-next-character-using-the-same-conditional-consume-primitive-you-built-in-milestone-2\">Remove <code>&#39;/&#39;</code> from <code>_SINGLE_CHAR_TOKENS</code> â€” it now has its own branch. The first character is <code>&#39;/&#39;</code> (already consumed by <code>advance()</code>); then <code>_match(&#39;/&#39;)</code> or <code>_match(&#39;*&#39;)</code> peeks at the next character using the same conditional-consume primitive you built in Milestone 2.</h2>\n<h2 id=\"single-line-comments-the-simple-case\">Single-Line Comments: The Simple Case</h2>\n<p>A single-line comment starting with <code>//</code> consumes everything until end-of-line. It produces no token â€” it simply advances <code>current</code> past the comment characters and returns <code>None</code> to the main loop (which silently discards <code>None</code> results).</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _skip_line_comment</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Called after consuming '//'. Advances past all characters until</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    newline or EOF. Does NOT consume the newline itself â€” the main loop</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    will encounter </span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\"> and handle whitespace normally.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # The '\\n' (if present) is left for the main loop to consume as whitespace.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This ensures line counting remains accurate.</span></span></code></pre></div>\n<h2 id=\"note-the-deliberate-choice-do-not-consume-the-n-let-it-be-picked-up-by-the-main-loop39s-whitespace-handling-this-keeps-the-newline-handling-in-one-place-advance-rather-than-scattering-it-across-multiple-methods-why-this-matters-for-correctness-the-advance-method-is-responsible-for-incrementing-selfline-when-it-sees-n-if-you-consume-n-inside-_skip_line_comment-using-advance-the-line-counter-updates-correctly-if-you-somehow-consume-it-without-going-through-advance-you-introduce-a-line-counting-bug-routing-all-character-consumption-through-advance-is-the-key-invariant-that-keeps-position-tracking-accurate\">Note the deliberate choice: do NOT consume the <code>\\n</code>. Let it be picked up by the main loop&#39;s whitespace handling. This keeps the newline handling in one place â€” <code>advance()</code> â€” rather than scattering it across multiple methods.\n<strong>Why this matters for correctness</strong>: the <code>advance()</code> method is responsible for incrementing <code>self.line</code> when it sees <code>\\n</code>. If you consume <code>\\n</code> inside <code>_skip_line_comment()</code> using <code>advance()</code>, the line counter updates correctly. If you somehow consume it without going through <code>advance()</code>, you introduce a line-counting bug. Routing all character consumption through <code>advance()</code> is the key invariant that keeps position tracking accurate.</h2>\n<h2 id=\"multi-line-comments-the-hunt\">Multi-Line Comments: The <code>*/</code> Hunt</h2>\n<p>Multi-line comments are more interesting. You must scan forward looking for the two-character sequence <code>*/</code>, consuming everything â€” including newlines â€” until you find it. If you reach EOF before finding <code>*/</code>, that&#39;s an error.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _skip_block_comment</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Called after consuming '/*'. Advances past all characters until</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '*/' is found or EOF is reached.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns None on success (comment fully consumed).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns an ERROR token if the comment is unterminated.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    The error position is the position of the opening '/*', stored</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    in self.start_column / self.line at the time this is called.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Capture the opening position for potential error reporting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '*'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the closing '/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">     # comment successfully terminated</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Reached EOF without finding '*/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '/*'</span><span style=\"color:#E1E4E8\">,           </span><span style=\"color:#6A737D\"># the lexeme that caused the problem</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_line,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_col</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-line-tracking-in-multiline.svg\" alt=\"Line Number Updates Inside Multi-Line Comments and Strings\"></p>\n<p><strong>Line tracking inside comments</strong>: Notice that <code>self.advance()</code> handles newlines everywhere, including inside the comment loop. When the scanner reads <code>\\n</code> inside a block comment, <code>advance()</code> increments <code>self.line</code> and resets <code>self.column</code> â€” exactly as it does everywhere else. You don&#39;t write any special line-tracking code in <code>_skip_block_comment()</code>. The invariant holds automatically.\nThis is the payoff for the design decision made in Milestone 1: routing all character consumption through <code>advance()</code>. Every path through the scanner â€” normal tokens, string literals, comments â€” goes through the same function, so position tracking is always correct.</p>\n<h3 id=\"the-non-nesting-rule\">The Non-Nesting Rule</h3>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-multiline-comment-nonnesting.svg\" alt=\"Multi-Line Comments Do NOT Nest â€” The /* /* */ Trap\"></p>\n<p>Multi-line comments in C and most C-like languages do <strong>not</strong> nest. This surprises many developers who assume that <code>/* /* */ */</code> is a complete comment. It is not:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>/* /* */    â† this closes the comment (first */ encountered)\n*/          â† this is now outside the comment, and */ is a syntax error</code></pre></div>\n<p>Your implementation above already handles this correctly by design. The loop searches for <code>*</code> followed by <code>/</code> â€” the moment it finds the first <code>*/</code>, it exits. It does not maintain a nesting counter. It does not look for additional <code>/*</code> openers inside the comment.\nIf you wanted nested comments (like OCaml or Rust&#39;s doc comments support), you&#39;d maintain a depth counter:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># HOW nested comments WOULD work (not implemented here):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">depth </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> depth </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '*'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        depth </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # nested open</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '*'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        depth </span><span style=\"color:#F97583\">-=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # nested close</span></span></code></pre></div>\n<h2 id=\"but-for-this-c-like-language-nesting-is-explicitly-not-supported-the-simpler-implementation-is-correct-by-specification\">But for this C-like language, nesting is explicitly not supported. The simpler implementation is correct by specification.</h2>\n<h2 id=\"string-literal-scanning-entering-a-new-world\">String Literal Scanning: Entering a New World</h2>\n<p>Strings are the most complex single construct in this milestone because they involve:</p>\n<ol>\n<li>Entering a new mode where all normal dispatch rules are suspended</li>\n<li>Processing escape sequences â€” special two-character sequences that represent single logical characters</li>\n<li>Detecting two error conditions: EOF before closing <code>&quot;</code>, and newline before closing <code>&quot;</code></li>\n<li>Reporting errors with the position of the <em>opening</em> quote, not where the problem was detected\nLet&#39;s build this up step by step.</li>\n</ol>\n<h3 id=\"the-basic-loop\">The Basic Loop</h3>\n<p>After consuming the opening <code>&quot;</code>, you enter a loop that consumes characters until one of three things happens:</p>\n<ul>\n<li>You see <code>&quot;</code> â†’ emit the complete string token</li>\n<li>You see <code>\\n</code> or hit EOF â†’ unterminated string, emit error</li>\n<li>You see any other character â†’ it&#39;s part of the string, keep consuming</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_string</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Called after consuming the opening '\"'. Scans the string body</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    and closing quote.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns a STRING token on success.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns an ERROR token if the string is unterminated.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Newline inside string is not allowed in this language.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Report error at the opening quote position (start_column).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.line,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.start_column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the backslash</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span><span style=\"color:#6A737D\">  # will be caught as unterminated below</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the escape character</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # EOF before closing quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.start_column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Consume the closing '\"'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<p>And in <code>_scan_token()</code>, add before the single-character dispatch:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_string()</span></span></code></pre></div>\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-context-overlap.svg\" alt=\"Comments Inside Strings vs Strings Inside Comments\"></p>\n<h2 id=\"the-crucial-invariant-inside-_scan_string-you-never-look-at-the-dispatch-table-you-never-check-if-is-a-comment-start-you-never-check-if-is-punctuation-every-character-you-encounter-anything-is-consumed-as-part-of-the-string-the-only-characters-that-have-special-meaning-inside-a-string-are-quot-exit-n-error-and-escape-prefix-everything-else-is-content-this-is-what-quotmode-suspensionquot-means-in-practice-you39ve-entered-a-different-scanning-context-where-the-dispatch-table-simply-doesn39t-apply\"><strong>The crucial invariant</strong>: Inside <code>_scan_string()</code>, you never look at the dispatch table. You never check if <code>//</code> is a comment start. You never check if <code>{</code> is punctuation. Every character you encounter â€” <code>/</code>, <code>{</code>, <code>+</code>, anything â€” is consumed as part of the string. The only characters that have special meaning inside a string are <code>&quot;</code> (exit), <code>\\n</code> (error), and <code>\\</code> (escape prefix). Everything else is content.\nThis is what &quot;mode suspension&quot; means in practice: you&#39;ve entered a different scanning context where the dispatch table simply doesn&#39;t apply.</h2>\n<h2 id=\"escape-sequences-two-characters-one-meaning\">Escape Sequences: Two Characters, One Meaning</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-string-escape-processing.svg\" alt=\"Escape Sequence Processing Inside Strings\"></p>\n<p>When you see a backslash <code>\\</code> inside a string, it means &quot;the next character is special.&quot; The pair <code>\\</code> + <code>n</code> represents a newline character. The pair <code>\\</code> + <code>&quot;</code> represents a literal double-quote (without ending the string). The pair <code>\\</code> + <code>\\</code> represents a literal backslash.\nThis two-character â†’ one-character substitution is called an <strong>escape sequence</strong> (the backslash &quot;escapes&quot; the normal meaning of the following character). It appears everywhere in computing: Python strings, C strings, JSON, CSV, HTTP headers, shell quoting, SQL queries. The mechanism is always the same: a special escape character announces that the following character should be interpreted differently.</p>\n<blockquote>\n<p><strong>Cross-domain connection</strong>: JSON uses the exact same escape mechanism for strings: <code>\\&quot;</code> for a literal quote, <code>\\\\</code> for a literal backslash, <code>\\n</code> for newline, <code>\\t</code> for tab. Any JSON parser you&#39;ve ever used has the same <code>_scan_string()</code> loop with the same backslash-detection logic. If you&#39;ve ever wondered how JSON distinguishes <code>&quot;hello&quot;</code> (the string <code>hello</code>) from <code>&quot;he said \\&quot;hi\\&quot;&quot;</code> (the string <code>he said &quot;hi&quot;</code>), now you know: it&#39;s the same escape processing you&#39;re implementing here.\nThe escape sequences your C-like language supports:</p>\n<table>\n<thead>\n<tr>\n<th>Written in source</th>\n<th>Represents</th>\n<th>ASCII code</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>\\n</code></td>\n<td>Newline</td>\n<td>10</td>\n</tr>\n<tr>\n<td><code>\\t</code></td>\n<td>Tab</td>\n<td>9</td>\n</tr>\n<tr>\n<td><code>\\r</code></td>\n<td>Carriage return</td>\n<td>13</td>\n</tr>\n<tr>\n<td><code>\\&quot;</code></td>\n<td>Double quote</td>\n<td>34</td>\n</tr>\n<tr>\n<td><code>\\\\</code></td>\n<td>Backslash</td>\n<td>92</td>\n</tr>\n<tr>\n<td>For this tokenizer, you&#39;re storing the raw lexeme â€” the exact characters from source â€” rather than processing escape sequences into their final values. The lexeme for <code>&quot;hello\\nworld&quot;</code> will be the 15-character string <code>&quot;hello\\nworld&quot;</code> (with a literal backslash-n), not a 12-character string with an actual newline embedded.</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>This is correct tokenizer behavior. <strong>The tokenizer&#39;s job is categorization, not interpretation.</strong> Transforming <code>\\n</code> into a newline character is the job of the evaluator or code generator â€” a later stage. For now, the tokenizer just needs to correctly identify where the string ends.</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>The escape handling in <code>_scan_string()</code> above does the right thing:</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n</blockquote>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the backslash</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        break</span><span style=\"color:#6A737D\">  # unterminated: backslash at end of input</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume whatever follows the backslash</span></span></code></pre></div>\n<p>When you see <code>\\</code>, consume it, then unconditionally consume the next character â€” whatever it is. This correctly handles <code>\\&quot;</code> (consumes <code>\\</code> then <code>&quot;</code>, leaving the string open) and <code>\\\\</code> (consumes <code>\\</code> then another <code>\\</code>, leaving the string open). The double-consume is the key: it prevents the <code>&quot;</code> after <code>\\</code> from being seen as the string terminator.\n<strong>What about invalid escapes like <code>\\z</code>?</strong> For this project, silently accept them â€” consume the two characters and include them in the string lexeme. Production compilers typically warn here but continue, because crashing on a bad escape would prevent reporting all other errors. Document the behavior in your code.</p>\n<h3 id=\"the-critical-error-reporting-insight\">The Critical Error Reporting Insight</h3>\n<p>When a string is unterminated, which position do you report in the error?\nOption A â€” the position of the character that revealed the problem (EOF, or the newline):</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Error at line 5, column 80: unterminated string</code></pre></div>\n<p>Option B â€” the position of the opening quote:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Error at line 5, column 12: unterminated string</code></pre></div>\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-unterminated-string.svg\" alt=\"Unterminated String â€” Error Detection & Position Reporting\"></p>\n<h2 id=\"option-b-is-dramatically-more-useful-line-5-column-80-might-be-the-end-of-the-file-that-tells-you-nothing-about-where-to-look-line-5-column-12-tells-you-exactly-quotgo-to-the-quote-at-column-12-and-find-the-matching-closing-quote-that-you-forgotquot-this-is-why-the-implementation-stores-selfstart_column-before-entering-_scan_string-and-uses-it-in-all-error-tokens-the-start_column-snapshot-taken-at-the-top-of-_scan_token-before-the-first-advance-captures-the-position-of-the-opening-quot-every-error-from-inside-_scan_string-uses-this-stored-position-this-is-not-just-a-nice-to-have-it39s-the-difference-between-a-usable-error-message-and-a-frustrating-one-gcc-clang-and-rustc-all-report-the-position-of-the-opening-delimiter-when-signaling-unterminated-strings-because-experience-has-proven-it39s-the-right-choice\">Option B is dramatically more useful. Line 5, column 80 might be the end of the file â€” that tells you nothing about where to look. Line 5, column 12 tells you exactly: &quot;go to the quote at column 12, and find the matching closing quote that you forgot.&quot;\nThis is why the implementation stores <code>self.start_column</code> <em>before</em> entering <code>_scan_string()</code>, and uses it in all error tokens. The <code>start_column</code> snapshot â€” taken at the top of <code>_scan_token()</code> before the first <code>advance()</code> â€” captures the position of the opening <code>&quot;</code>. Every error from inside <code>_scan_string()</code> uses this stored position.\nThis is not just a nice-to-have. It&#39;s the difference between a usable error message and a frustrating one. GCC, Clang, and rustc all report the position of the opening delimiter when signaling unterminated strings â€” because experience has proven it&#39;s the right choice.</h2>\n<h2 id=\"putting-it-together-the-complete-_scan_token-for-m3\">Putting It Together: The Complete <code>_scan_token()</code> for M3</h2>\n<p>Here is the full updated <code>_scan_token()</code> with all three milestones integrated:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Two-character operators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '='</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '!'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '&#x3C;'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">LESS_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '>'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Division or comment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'/'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._skip_line_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'*'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._skip_block_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ String literals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_string()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Number literals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Identifiers and keywords â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Single-character tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Error: unrecognized character â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span></code></pre></div>\n<h2 id=\"notice-the-structure-the-division-operator-case-elif-char-3939-now-replaces-the-3939-entry-in-_single_char_tokens-the-string-literal-case-elif-char-39quot39-is-added-before-number-and-identifier-scanning\">Notice the structure: the division operator case (<code>elif char == &#39;/&#39;</code>) now replaces the <code>&#39;/&#39;</code> entry in <code>_SINGLE_CHAR_TOKENS</code>. The string literal case (<code>elif char == &#39;&quot;&#39;</code>) is added before number and identifier scanning.</h2>\n<h2 id=\"complete-scanner-implementation-for-m3\">Complete Scanner Implementation for M3</h2>\n<p>Here is the full, self-contained implementation incorporating all three milestones:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Arithmetic operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># +, -, *, /</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Comparison &#x26; assignment operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ==</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUAL</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_THAN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQUAL</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_THAN</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># >=</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Structural</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Control</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">   =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Scanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    _SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '+'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '-'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '*'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # '/' is NOT here â€” handled separately for comment detection</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '('</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ')'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '{'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '}'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '['</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ']'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ';'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ','</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    _KEYWORDS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'if'</span><span style=\"color:#E1E4E8\">:     TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'else'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'while'</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'return'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'true'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'false'</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'null'</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tab_width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tab_width </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tab_width</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: list[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Primitives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_at_end</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tab_width</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> ''</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _peek_next</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> >=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> ''</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _match</span><span style=\"color:#E1E4E8\">(self, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Token construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _make_token</span><span style=\"color:#E1E4E8\">(self, token_type: TokenType) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(token_type, lexeme, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Comment scanners â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_line_comment</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Called after consuming '//'. Advances past all characters until</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        newline or EOF. Does NOT consume the newline itself.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _skip_block_comment</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Called after consuming '/*'. Advances past all characters until</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '*/' or EOF.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns None on success, ERROR token if unterminated.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Error position is the opening '/*' (stored in self.start_column).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '*'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the closing '/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">     # successfully terminated</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # EOF reached without finding '*/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'/*'</span><span style=\"color:#E1E4E8\">, error_line, error_col)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ String scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_string</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Called after consuming the opening '\"'. Scans string body</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        and the closing '\"'.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handles escape sequences by consuming '</span><span style=\"color:#79B8FF\">\\'</span><span style=\"color:#9ECBFF\"> + next char as a unit</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        (raw lexeme stored; escape interpretation is the evaluator's job).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns STRING token on success.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns ERROR token if unterminated (newline or EOF before closing '\"').</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Error position is the opening '\"' (self.start_column).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Newline before closing quote â€” unterminated string.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Report error at the OPENING quote position.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> Token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.line,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.start_column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the backslash</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the escaped character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # If EOF after backslash, the outer loop catches it next iteration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # EOF before closing quote â€” unterminated string.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.line,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.start_column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Consume the closing '\"'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Number scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._peek_next().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume '.'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Identifier scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_identifier</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        text </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        token_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_KEYWORDS</span><span style=\"color:#E1E4E8\">.get(text, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(token_type)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # â”€â”€ Core scan loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '='</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '!'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '&#x3C;'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">LESS_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '>'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'/'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._skip_line_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'*'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._skip_block_comment()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_string()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">_SINGLE_CHAR_TOKENS</span><span style=\"color:#E1E4E8\">[char])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_token()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens.append(Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens</span></span></code></pre></div>\n<hr>\n<h2 id=\"three-level-view-what-a-string-literal-means-at-each-layer\">Three-Level View: What a String Literal Means at Each Layer</h2>\n<h2 id=\"level-1-source-language-the-programmer-writes-greeting-quothellonworldquot-to-them-this-is-assigning-a-two-line-string-to-a-variable-the-n-is-the-quotnewline-characterquot-a-single-logical-thing-level-2-the-scanner-what-you39re-building-the-scanner-sees-22-characters-it-enters-_scan_string-when-it-hits-the-quot-it-reads-h-e-l-l-o-as-ordinary-content-when-it-hits-it-enters-escape-processing-consumes-then-consumes-n-two-characters-but-they39re-both-part-of-the-string-content-it-continues-until-the-closing-quot-the-emitted-token-has-lexeme-39quothellonworldquot39-a-14-character-string-including-the-quotes-and-the-raw-backslash-n-the-scanner-preserves-exactly-what-was-written-level-3-the-evaluator-downstream-when-the-interpreter-or-code-generator-processes-the-string-token-it-strips-the-outer-quotes-and-processes-n-newline-t-tab-etc-the-runtime-string-object-will-contain-an-actual-newline-byte-ascii-10-this-is-where-quothellonworldquot-becomes-a-12-character-string-with-a-newline-in-the-middle-the-scanner-deliberately-does-not-perform-level-339s-job-keeping-responsibilities-separated-means-the-scanner-is-simple-testable-and-independent-of-the-evaluation-semantics\"><strong>Level 1 â€” Source Language</strong>: The programmer writes <code>greeting = &quot;hello\\nworld&quot;</code>. To them, this is assigning a two-line string to a variable. The <code>\\n</code> is the &quot;newline character&quot; â€” a single logical thing.\n<strong>Level 2 â€” The Scanner (what you&#39;re building)</strong>: The scanner sees 22 characters. It enters <code>_scan_string()</code> when it hits the <code>&quot;</code>. It reads <code>h</code>, <code>e</code>, <code>l</code>, <code>l</code>, <code>o</code> as ordinary content. When it hits <code>\\</code>, it enters escape-processing: consumes <code>\\</code>, then consumes <code>n</code> â€” two characters, but they&#39;re both part of the string content. It continues until the closing <code>&quot;</code>. The emitted token has <code>lexeme = &#39;&quot;hello\\\\nworld&quot;&#39;</code> â€” a 14-character string including the quotes and the raw backslash-n. The scanner preserves exactly what was written.\n<strong>Level 3 â€” The Evaluator (downstream)</strong>: When the interpreter or code generator processes the <code>STRING</code> token, it strips the outer quotes and processes <code>\\n</code> â†’ newline, <code>\\t</code> â†’ tab, etc. The runtime string object will contain an actual newline byte (ASCII 10). This is where <code>&quot;hello\\nworld&quot;</code> becomes a 12-character string with a newline in the middle.\nThe scanner deliberately does <em>not</em> perform Level 3&#39;s job. Keeping responsibilities separated means the scanner is simple, testable, and independent of the evaluation semantics.</h2>\n<h2 id=\"common-pitfalls-and-how-to-avoid-them\">Common Pitfalls and How to Avoid Them</h2>\n<h3 id=\"pitfall-1-consuming-the-closing-quot-and-leaving-it-for-the-next-token\">Pitfall 1: Consuming the Closing <code>&quot;</code> and Leaving It for the Next Token</h3>\n<p>This is a subtle off-by-one that produces confusing behavior:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ Wrong: forgetting to consume the closing quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_string</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Bug: we stop when peek() == '\"', but don't consume it</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># lexeme is missing the closing '\"'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Next call to _scan_token() will see '\"' and start ANOTHER string!</span></span></code></pre></div>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âœ… Correct: consume the closing '\"' before emitting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ... loop ...</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume the closing '\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<p>If you forget to consume the closing <code>&quot;</code>, the next <code>_scan_token()</code> call sees it and starts scanning another string â€” producing cascading garbage tokens for everything that follows.</p>\n<h3 id=\"pitfall-2-a-backslash-at-the-end-of-the-string\">Pitfall 2: A Backslash at the End of the String</h3>\n<p>Input: <code>&quot;hello\\</code> (backslash at EOF, no closing quote)\nWithout careful handling, this could cause an out-of-bounds access:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ Dangerous: consuming the character after '\\' without checking EOF first</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume '\\'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume next char -- but what if is_at_end() is now True?</span></span></code></pre></div>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âœ… Safe: guard the second advance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume '\\'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># only consume escaped char if it exists</span></span></code></pre></div>\n<p>Python&#39;s string indexing raises <code>IndexError</code> for out-of-bounds access, so this would crash without the guard. After the guarded version, <code>is_at_end()</code> will be true, and the main while-loop condition catches the unterminated string.</p>\n<h3 id=\"pitfall-3-detecting-inside-a-block-comment\">Pitfall 3: Detecting <code>*/</code> Inside a Block Comment</h3>\n<p>The naive approach â€” checking if <code>self.peek() == &#39;*&#39;</code> and then checking the character after that â€” requires two peek operations:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ Naive: requires peek-ahead of two characters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '*'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._peek_next() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume '*'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># consume '/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()</span></span></code></pre></div>\n<p>The implementation given earlier uses a cleaner one-consume-then-peek pattern:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âœ… Cleaner: advance then peek</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '*'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance()   </span><span style=\"color:#6A737D\"># consume the '/'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n<p>Both are correct, but the second version uses <code>advance()</code> consistently as the primary consumer and <code>peek()</code> for the single-character lookahead â€” consistent with the rest of the scanner.</p>\n<h3 id=\"pitfall-4-comments-inside-strings\">Pitfall 4: Comments Inside Strings</h3>\n<p>This is the whole point of mode-based scanning, but it&#39;s easy to accidentally break:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ Wrong design: checking for '//' INSIDE the string loop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_string</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end() </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._peek_next() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span><span style=\"color:#6A737D\">  # Bug: treating comment syntax inside string as a comment!</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()</span></span></code></pre></div>\n<p>The correct <code>_scan_string()</code> has NO knowledge of <code>//</code> or <code>/*</code>. It only knows about <code>&quot;</code>, <code>\\n</code>, and <code>\\</code>. Any other character â€” including <code>/</code> â€” is consumed unconditionally as string content. The mode isolation is total.</p>\n<h3 id=\"pitfall-5-line-count-drift-in-multi-line-comments\">Pitfall 5: Line Count Drift in Multi-Line Comments</h3>\n<p>If your multi-line comment scanner uses any mechanism other than <code>advance()</code> to consume characters, newlines won&#39;t be counted:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ Wrong: skipping characters without updating line/column</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _skip_block_comment</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # manual update -- easy to forget or get wrong</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.current </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> ...</span><span style=\"color:#E1E4E8\">:</span></span></code></pre></div>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âœ… Correct: always use advance(), which handles newlines automatically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()   </span><span style=\"color:#6A737D\"># advance() updates line/column for every char</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ...</span></span></code></pre></div>\n<h2 id=\"the-invariant-is-every-character-consumed-goes-through-advance-if-you-maintain-this-position-tracking-is-automatically-correct-in-every-context\">The invariant is: <strong>every character consumed goes through <code>advance()</code></strong>. If you maintain this, position tracking is automatically correct in every context.</h2>\n<h2 id=\"testing-m3\">Testing M3</h2>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ String literal tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_simple_string</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"hello\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_empty_string</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_with_escape_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">\"hello</span><span style=\"color:#85E89D;font-weight:bold\">\\n</span><span style=\"color:#DBEDFF\">world\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#F97583\"> r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">\"hello</span><span style=\"color:#85E89D;font-weight:bold\">\\n</span><span style=\"color:#DBEDFF\">world\"</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_with_escaped_quote</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">\"say </span><span style=\"color:#85E89D;font-weight:bold\">\\\"</span><span style=\"color:#DBEDFF\">hi</span><span style=\"color:#85E89D;font-weight:bold\">\\\"</span><span style=\"color:#DBEDFF\">\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # The lexeme includes the raw backslash-quote sequences</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_with_escaped_backslash</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">\"path</span><span style=\"color:#85E89D;font-weight:bold\">\\\\</span><span style=\"color:#DBEDFF\">file\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_string_eof</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"String without closing quote before EOF produces ERROR at opening quote.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # position of the opening '\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_string_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"String without closing quote before newline produces ERROR at opening quote.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">world\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # position of the opening '\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_position</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'  \"hi\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">   # '\"' is at column 3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_comment_inside_string_not_treated_as_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'hello // world' is a single STRING token, not STRING + comment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello // world\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # STRING + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"hello // world\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_marker_inside_string</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'/* inside */' within quotes is a string, not a comment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"/* inside */\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # STRING + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_all_escape_sequences</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All supported escape sequences remain in raw lexeme.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">\"</span><span style=\"color:#85E89D;font-weight:bold\">\\n\\t\\r\\\"\\\\</span><span style=\"color:#DBEDFF\">\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Single-line comment tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_comment_no_tokens</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A line comment produces no tokens (only EOF).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"// this is a comment\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_comment_then_code</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Code after a comment on the next line is tokenized correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"// comment</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">x = 1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # x, =, 1, EOF â€” the comment produces nothing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'x'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_comment_mixed</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tokens before a comment are unaffected.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x // comment\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # IDENTIFIER('x') + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'x'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_comment_does_not_consume_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"The newline after '//' is NOT consumed by the comment scanner.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"// comment</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Only '+' and EOF â€” '+' is on line 2, column 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_slash_not_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'/' alone (not '//' or '/*') is a division operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x / y\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens[:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> types </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Multi-line comment tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_no_tokens</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A block comment produces no tokens (only EOF).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"/* comment */\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_with_newlines</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-line block comment updates line counter correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"/* line one</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">line two</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">line three */</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">x\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'x' is on line 4 (3 newlines inside comment + starting line 1)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_inline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Code before and after a block comment tokenizes correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x /* comment */ y\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">   # IDENTIFIER('x'), IDENTIFIER('y'), EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'x'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'y'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_block_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Block comment without closing '*/' produces ERROR at opening '/*'.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"/* unterminated\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '/*'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_does_not_nest</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'/* /* */' ends at the FIRST '*/', not the second.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"/* /* */ x\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Comment closes at first '*/'; 'x' is a real token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # The remaining ' x' after '*/': one IDENTIFIER and EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'x'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_block_comment_star_inside</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"'*' inside a block comment that isn't followed by '/' doesn't close it.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"/* a * b */\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_unterminated_block_comment_position</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Unterminated comment error reports the position of '/*', not EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">  /* unterminated\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'x' is first token, then ERROR for the unterminated comment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">  # '/*' starts at column 3 on line 2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Context isolation tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_inside_comment_ignored</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A '</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">' inside a block comment does not start a string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'/* \"not a string\" */'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # only EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_numbers_accurate_after_multiline_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify line tracking across multi-line constructs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">/* line 2</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">line 3</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">*/</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">b\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'a' is on line 1, 'b' is on line 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'a'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'b'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_then_comment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"String followed by comment: both handled correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello\" // rest is comment'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # STRING + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"hello\"'</span></span></code></pre></div>\n<h2 id=\"run-all-of-these-the-ones-that-test-position-inside-multi-line-constructs-are-the-most-demanding-if-your-advance-invariant-holds-they-pass-automatically\">Run all of these. The ones that test position inside multi-line constructs are the most demanding â€” if your <code>advance()</code> invariant holds, they pass automatically.</h2>\n<h2 id=\"syntax-highlighting-why-your-editor-sometimes-goes-wrong\">Syntax Highlighting: Why Your Editor Sometimes Goes Wrong</h2>\n<p>Now you understand something you&#39;ve always seen but probably never knew the reason for.\nOpen any code editor. Type:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"hello</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 42</span></span></code></pre></div>\n<h2 id=\"the-moment-you-type-the-opening-quot-without-a-closing-quot-watch-what-happens-to-the-lines-below-they-often-turn-orange-or-whatever-your-string-color-is-and-y-and-42-all-look-like-they39re-inside-the-string-sometimes-the-entire-file-changes-color-this-is-your-scanner39s-behavior-displayed-visually-the-editor39s-tokenizer-entered-in_string-mode-when-it-saw-quot-it-couldn39t-find-a-closing-quot-before-the-end-of-the-line-or-end-of-file-it-emitted-an-error-or-a-very-long-string-token-the-syntax-highlighter-colored-everything-in-that-quotstringquot-as-string-content-when-you-add-the-closing-quot-the-tokenizer-re-runs-in-modern-editors-incrementally-from-the-changed-point-the-string-terminates-correctly-and-everything-after-it-returns-to-normal-colors-this-is-not-a-bug-in-your-editor-it39s-the-correct-behavior-of-a-tokenizer-that39s-lost-context-the-quotfixquot-in-real-editors-is-to-limit-how-far-a-string-can-extend-without-re-synchronization-essentially-the-same-non-nesting-rule-you-implemented-for-comments-applied-to-strings-some-editors-use-heuristic-re-synchronization-quotif-we-haven39t-found-a-closing-quote-after-n-lines-assume-the-string-ended-and-restart-from-herequot-this-produces-wrong-highlighting-sometimes-but-it39s-better-than-highlighting-an-entire-10000-line-file-as-a-string-because-of-one-missing-quote-understanding-this-transforms-a-mysterious-editor-behavior-into-a-comprehensible-engineering-tradeoff-how-much-context-do-you-maintain-and-when-do-you-sacrifice-accuracy-for-usability\">The moment you type the opening <code>&quot;</code> without a closing <code>&quot;</code>, watch what happens to the lines below: they often turn orange (or whatever your string-color is), and <code>y</code>, <code>=</code>, and <code>42</code> all look like they&#39;re inside the string. Sometimes the entire file changes color.\nThis is your scanner&#39;s behavior, displayed visually. The editor&#39;s tokenizer entered <code>IN_STRING</code> mode when it saw <code>&quot;</code>. It couldn&#39;t find a closing <code>&quot;</code> before the end of the line (or end of file). It emitted an ERROR or a very long STRING token. The syntax highlighter colored everything in that &quot;string&quot; as string content.\nWhen you add the closing <code>&quot;</code>, the tokenizer re-runs (in modern editors, incrementally from the changed point), the string terminates correctly, and everything after it returns to normal colors.\nThis is not a bug in your editor. It&#39;s the correct behavior of a tokenizer that&#39;s lost context. The &quot;fix&quot; in real editors is to limit how far a string can extend without re-synchronization â€” essentially the same non-nesting rule you implemented for comments, applied to strings. Some editors use heuristic re-synchronization: &quot;if we haven&#39;t found a closing quote after N lines, assume the string ended and restart from here.&quot; This produces wrong highlighting sometimes, but it&#39;s better than highlighting an entire 10,000-line file as a string because of one missing quote.\nUnderstanding this transforms a mysterious editor behavior into a comprehensible engineering tradeoff: how much context do you maintain, and when do you sacrifice accuracy for usability?</h2>\n<h2 id=\"three-level-view-what-a-comment-really-is\">Three-Level View: What a Comment Really Is</h2>\n<h2 id=\"level-1-source-language-the-programmer-writes-todo-fix-this-to-them-it39s-documentation-invisible-to-the-compiler-meaningful-to-humans-level-2-the-scanner-what-you39re-building-a-comment-is-a-mode-change-the-scanner-enters-in_line_comment-state-discards-all-characters-until-newline-then-returns-to-normal-no-token-is-emitted-from-the-scanner39s-perspective-the-comment-literally-does-not-exist-it39s-as-if-those-characters-were-whitespace-level-3-the-token-stream-downstream-the-parser-type-checker-and-code-generator-never-see-comments-they-operate-entirely-on-the-token-stream-which-is-comment-free-the-only-place-comments-survive-is-in-specialized-tools-documentation-generators-javadoc-rust39s-doc-comments-ides-building-a-comment-index-and-some-preprocessors-those-tools-run-their-own-passes-over-the-raw-source-separate-from-the-compilation-pipeline-this-three-level-view-reveals-something-interesting-comments-are-stripped-at-level-2-earlier-than-almost-everything-else-they39re-not-quotignored-by-the-parserquot-they-never-reach-the-parser-they-vanish-in-the-scanner\"><strong>Level 1 â€” Source Language</strong>: The programmer writes <code>// TODO: fix this</code>. To them, it&#39;s documentation â€” invisible to the compiler, meaningful to humans.\n<strong>Level 2 â€” The Scanner (what you&#39;re building)</strong>: A comment is a mode change. The scanner enters <code>IN_LINE_COMMENT</code> state, discards all characters until newline, then returns to <code>NORMAL</code>. No token is emitted. From the scanner&#39;s perspective, the comment literally does not exist â€” it&#39;s as if those characters were whitespace.\n<strong>Level 3 â€” The Token Stream (downstream)</strong>: The parser, type checker, and code generator never see comments. They operate entirely on the token stream, which is comment-free. The only place comments survive is in specialized tools: documentation generators (Javadoc, Rust&#39;s <code>///</code> doc comments), IDEs building a comment index, and some preprocessors. Those tools run their own passes over the raw source, separate from the compilation pipeline.\nThis three-level view reveals something interesting: comments are stripped at Level 2, earlier than almost everything else. They&#39;re not &quot;ignored by the parser&quot; â€” they never reach the parser. They vanish in the scanner.</h2>\n<h2 id=\"knowledge-cascade-quotlearn-one-unlock-tenquot\">Knowledge Cascade â€” &quot;Learn One, Unlock Ten&quot;</h2>\n<h2 id=\"you39ve-implemented-string-scanning-with-escape-sequences-line-comments-and-block-comments-here39s-what-those-unlock-json-parsing-you39ve-just-built-the-hardest-part-json39s-string-parsing-is-nearly-identical-to-what-you-implemented-the-json-specification-rfc-8259-defines-strings-as-sequences-between-quotquot-with-n-t-r-quot-and-uxxxx-escape-sequences-every-json-parser-in-production-python39s-json-module-javascript39s-jsonparse-jackson-in-java-has-a-_scan_string-loop-that-looks-exactly-like-yours-the-only-addition-is-uxxxx-unicode-escapes-you-now-understand-the-core-of-every-json-parser-ever-written-regular-expressions-have-limits-and-you39ve-found-the-edge-your-in_string-and-in_multi_comment-modes-encode-context-that-a-single-pass-regular-expression-cannot-express-this-is-not-opinion-it39s-the-pumping-lemma-for-regular-languages-which-proves-that-quotstring-with-matching-delimitersquot-is-not-a-regular-language-the-way-you39ve-worked-around-this-adding-state-variables-modes-to-extend-the-dfa-is-the-same-workaround-used-by-every-real-lexer-when-someone-says-quotyou-can39t-parse-html-with-a-regexquot-this-is-the-formal-reason-html39s-nested-tags-are-even-further-beyond-regular-languages-than-matched-quotes-network-protocols-and-escape-sequences-the-same-pattern-everywhere-http11-uses-rn-as-a-line-terminator-and-percent-encoding-20-for-space-as-its-escape-mechanism-csv-uses-quotquot-doubled-quote-to-escape-a-literal-quote-inside-a-quoted-field-shell-uses-for-escaping-and-treats-quotquot-and-3939-as-distinct-quoting-modes-sql-uses-3939-to-escape-a-quote-inside-a-string-every-protocol-that-transmits-text-with-special-characters-has-an-escape-mechanism-the-structure-of-your-_scan_string-loop-quotconsume-characters-until-delimiter-but-treat-escape-prefix-speciallyquot-applies-to-all-of-them-you-can-now-implement-a-csv-parser-an-http-header-parser-and-a-shell-tokenizer-using-the-exact-same-pattern-error-message-quality-as-ux-engineering-the-choice-to-report-unterminated-string-errors-at-the-opening-quote-position-not-eof-is-a-user-experience-decision-disguised-as-a-compiler-feature-gcc39s-error-messages-improved-dramatically-between-version-3-and-4-partly-because-of-exactly-this-kind-of-positional-accuracy-work-clang-was-designed-from-the-start-with-quotbest-in-class-error-messagesquot-as-an-explicit-goal-clang39s-error-reporting-infrastructure-is-more-complex-than-many-compilers39-entire-frontends-the-insight-here-is-that-where-you-store-and-propagate-metadata-determines-the-quality-of-your-error-messages-the-start_column-snapshot-you-take-at-the-top-of-_scan_token-is-metadata-infrastructure-its-value-is-entirely-in-the-error-messages-it-enables-incremental-tokenization-in-language-servers-in-your-current-scanner-scan_tokens-re-processes-the-entire-source-every-time-in-a-language-server-lsp-the-language-server-protocol-that-powers-vs-code39s-quotgo-to-definitionquot-tokenizing-is-triggered-on-every-keypress-for-large-files-re-tokenizing-everything-is-too-slow-incremental-tokenizers-track-which-tokens-are-quotdirtyquot-potentially-affected-by-the-edit-and-re-scan-only-from-the-last-quotcleanquot-state-the-mode-system-you39ve-built-in-this-milestone-is-exactly-what-makes-incremental-tokenization-possible-if-an-edit-is-outside-a-string-or-comment-the-surrounding-context-is-unaffected-if-it39s-inside-a-string-only-the-string-changes-mode-boundaries-are-re-synchronization-points-the-formal-language-hierarchy-where-you-are-the-chomsky-hierarchy-classifies-formal-languages-into-four-types-type-3-regular-languages-can-be-recognized-by-finite-automata-and-expressed-as-regular-expressions-type-2-context-free-languages-need-pushdown-automata-stack-based-machines-this-is-where-most-programming-language-grammars-live-type-1-and-type-0-are-more-powerful-your-tokenizer-extends-a-dfa-with-explicit-mode-tracking-to-handle-constructs-that-are-technically-at-the-boundary-of-type-3-the-parser-you39d-write-next-would-be-a-full-type-2-machine-now-you-can-place-yourself-on-the-formal-language-map-and-understand-why-the-tokenizer-and-parser-are-separate-stages\">You&#39;ve implemented string scanning with escape sequences, line comments, and block comments. Here&#39;s what those unlock:\n<strong>â†’ JSON parsing â€” you&#39;ve just built the hardest part</strong>: JSON&#39;s string parsing is nearly identical to what you implemented. The JSON specification (RFC 8259) defines strings as sequences between <code>&quot;...&quot;</code> with <code>\\n</code>, <code>\\t</code>, <code>\\r</code>, <code>\\&quot;</code>, <code>\\\\</code>, and <code>\\uXXXX</code> escape sequences. Every JSON parser in production â€” Python&#39;s <code>json</code> module, JavaScript&#39;s <code>JSON.parse()</code>, Jackson in Java â€” has a <code>_scan_string()</code> loop that looks exactly like yours. The only addition is <code>\\uXXXX</code> Unicode escapes. You now understand the core of every JSON parser ever written.\n<strong>â†’ Regular expressions have limits â€” and you&#39;ve found the edge</strong>: Your <code>IN_STRING</code> and <code>IN_MULTI_COMMENT</code> modes encode context that a single-pass regular expression cannot express. This is not opinion â€” it&#39;s the Pumping Lemma for regular languages, which proves that &quot;string with matching delimiters&quot; is not a regular language. The way you&#39;ve worked around this â€” adding state variables (modes) to extend the DFA â€” is the same workaround used by every real lexer. When someone says &quot;you can&#39;t parse HTML with a regex,&quot; this is the formal reason: HTML&#39;s nested tags are even further beyond regular languages than matched quotes.\n<strong>â†’ Network protocols and escape sequences â€” the same pattern everywhere</strong>: HTTP/1.1 uses <code>\\r\\n</code> as a line terminator and percent-encoding (<code>%20</code> for space) as its escape mechanism. CSV uses <code>&quot;&quot;</code> (doubled quote) to escape a literal quote inside a quoted field. Shell uses <code>\\</code> for escaping and treats <code>&quot;...&quot;</code> and <code>&#39;...&#39;</code> as distinct quoting modes. SQL uses <code>&#39;&#39;</code> to escape a quote inside a string. Every protocol that transmits text-with-special-characters has an escape mechanism. The structure of your <code>_scan_string()</code> loop â€” &quot;consume characters until delimiter, but treat escape prefix specially&quot; â€” applies to all of them. You can now implement a CSV parser, an HTTP header parser, and a shell tokenizer using the exact same pattern.\n**â†’ Error message quality as UX engineering**: The choice to report unterminated string errors at the opening quote position (not EOF) is a user experience decision disguised as a compiler feature. GCC&#39;s error messages improved dramatically between version 3 and 4 partly because of exactly this kind of positional accuracy work. Clang was designed from the start with &quot;best-in-class error messages&quot; as an explicit goal â€” Clang&#39;s error reporting infrastructure is more complex than many compilers&#39; entire frontends. The insight here is that **where you store and propagate metadata determines the quality of your error messages**. The <code>start_column</code> snapshot you take at the top of <code>_scan_token()</code> is metadata infrastructure. Its value is entirely in the error messages it enables.\n**â†’ Incremental tokenization in language servers**: In your current scanner, <code>scan_tokens()</code> re-processes the entire source every time. In a language server (LSP â€” the Language Server Protocol that powers VS Code&#39;s &quot;go to definition&quot;), tokenizing is triggered on every keypress. For large files, re-tokenizing everything is too slow. Incremental tokenizers track which tokens are &quot;dirty&quot; (potentially affected by the edit) and re-scan only from the last &quot;clean&quot; state. The mode system you&#39;ve built in this milestone is exactly what makes incremental tokenization possible: if an edit is outside a string or comment, the surrounding context is unaffected. If it&#39;s inside a string, only the string changes. Mode boundaries are re-synchronization points.\n<strong>â†’ The formal language hierarchy â€” where you are</strong>: The Chomsky hierarchy classifies formal languages into four types. Type 3 (Regular) languages can be recognized by finite automata and expressed as regular expressions. Type 2 (Context-Free) languages need pushdown automata (stack-based machines) â€” this is where most programming language <em>grammars</em> live. Type 1 and Type 0 are more powerful. Your tokenizer extends a DFA with explicit mode-tracking to handle constructs that are technically at the boundary of Type 3. The parser you&#39;d write next would be a full Type 2 machine. Now you can place yourself on the formal language map â€” and understand why the tokenizer and parser are separate stages.</h2>\n<h2 id=\"design-decision-why-store-raw-lexeme-not-processed-value\">Design Decision: Why Store Raw Lexeme, Not Processed Value?</h2>\n<p>You might wonder: why store <code>&#39;&quot;hello\\\\nworld&quot;&#39;</code> (the raw 15 characters) instead of <code>&#39;hello\\nworld&#39;</code> (the 12-character processed string)? Couldn&#39;t you process escape sequences right here in the scanner, saving the evaluator work?</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Used By</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Store raw lexeme (chosen âœ“)</strong></td>\n<td>Scanner stays simple; evaluation deferred; raw source preserved</td>\n<td>Downstream must process escapes</td>\n<td>CPython, Clang</td>\n</tr>\n<tr>\n<td>Store processed value</td>\n<td>Downstream simpler</td>\n<td>Scanner does two jobs; source text lost; harder to re-emit source</td>\n<td>Some embedded language implementations</td>\n</tr>\n<tr>\n<td>Store both</td>\n<td>Maximum flexibility</td>\n<td>Memory doubled; complexity added for little gain</td>\n<td>Some IDEs for source-accurate formatting</td>\n</tr>\n<tr>\n<td>The chosen approach respects the <strong>single responsibility principle</strong>: the scanner categorizes, the evaluator interprets. If you process escapes in the scanner, what do you do with invalid escapes like <code>\\z</code>? Emit an error? Return a partial token? The scanner&#39;s job is to find the token boundary â€” not to validate or transform the content. Keeping those separate makes both halves easier to reason about and test.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Real-world validation: CPython&#39;s tokenizer (<code>Lib/tokenize.py</code>) stores the raw token string. The compiler&#39;s AST builder processes string escapes when building the AST. Clang&#39;s lexer stores the raw spelling, and a separate <code>getSpelling()</code> call processes escape sequences when needed. The pattern is universal across production compilers.</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"what-you39ve-built\">What You&#39;ve Built</h2>\n<p>Your scanner now handles the complete set of lexical constructs for a real C-like language:</p>\n<ul>\n<li><strong>String literals</strong> scanned character-by-character with the closing <code>&quot;</code> consumed correctly</li>\n<li><strong>Escape sequences</strong> (<code>\\n</code>, <code>\\t</code>, <code>\\r</code>, <code>\\&quot;</code>, <code>\\\\</code>) handled by consuming backslash + next character as a unit, preserving raw lexeme</li>\n<li><strong>Unterminated strings</strong> (no closing <code>&quot;</code> before newline or EOF) reported as ERROR tokens with the position of the <em>opening</em> quote</li>\n<li><strong>Single-line comments</strong> (<code>//</code>) consuming all characters to end-of-line, emitting no token</li>\n<li><strong>Multi-line block comments</strong> (<code>/* */</code>) consuming all characters including newlines until <code>*/</code>, emitting no token</li>\n<li><strong>Unterminated block comments</strong> (no <code>*/</code> before EOF) reported as ERROR at the position of <code>/*</code></li>\n<li><strong>Non-nesting comments</strong> â€” <code>/* /* */</code> closes at the first <code>*/</code>; there is no nesting depth counter</li>\n<li><strong>Mode isolation</strong> â€” <code>//</code> inside <code>&quot;...&quot;</code> is string content, not a comment; <code>&quot;</code> inside <code>/* */</code> is comment content, not a string</li>\n<li><strong>Line tracking</strong> accurate inside both multi-line strings and multi-line comments, because every character passes through <code>advance()</code>\nIn Milestone 4, you&#39;ll test this complete scanner against full multi-line programs, implement error recovery strategies, verify position accuracy across long inputs, and measure performance. The hard algorithmic work is done â€” M4 is about validation and robustness.</li>\n</ul>\n<hr>\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m4 -->\n<!-- MS_ID: tokenizer-m4 -->\n<h1 id=\"milestone-4-integration-testing-amp-error-recovery\">Milestone 4: Integration Testing &amp; Error Recovery</h1>\n<h2 id=\"where-you39re-starting\">Where You&#39;re Starting</h2>\n<p>You have a complete tokenizer. It handles single-character tokens, two-character operators with maximal munch, number literals (integer and float), identifiers, keyword lookup, string literals with escape sequences, single-line comments, and multi-line block comments. The individual pieces exist and work in isolation.\nBut software built from individually-tested pieces can still fail when those pieces run together. A real source file is not a sequence of isolated test cases â€” it&#39;s a continuous stream of characters where identifiers sit next to operators, comments precede expressions, strings contain characters that look like tokens, and newlines scatter across hundreds of lines. The pieces must work <em>in combination</em>.\nThis milestone is about proving they do. You&#39;ll write integration tests that feed complete multi-line programs through the scanner and verify the exact token stream produced. You&#39;ll build and measure error recovery â€” the mechanism that keeps the scanner running through bad input. You&#39;ll verify that position tracking doesn&#39;t drift over long inputs. And you&#39;ll run a performance sanity check to confirm the scanner completes in under a second on 10,000 lines.\nBy the end, your scanner is not just functionally complete â€” it&#39;s <em>validated</em>. That&#39;s a different and higher bar.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-satellite-map.svg\" alt=\"Tokenizer System â€” Satellite Map (Home Base)\"></p>\n<hr>\n<h2 id=\"the-core-misconception-a-lexer-either-succeeds-or-fails\">The Core Misconception: A Lexer Either Succeeds or Fails</h2>\n<p>Here&#39;s what most developers instinctively expect from a compiler tool when it encounters bad input:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>$ compile myfile.c\nError: unrecognized character '@' at line 42, column 7.</code></pre></div>\n<p>And then nothing. One error, then stop. The compiler either processes your file successfully, or it stops at the first problem and makes you fix it before it&#39;ll tell you anything more.\nThis model is deeply ingrained from experiences with build systems, linters, and simple interpreters. It feels natural â€” the moment something is wrong, the tool&#39;s output can&#39;t be trusted, so why continue?\n<strong>It is also wrong for production-quality tools.</strong> Stopping at the first error forces developers into an agonizing loop:</p>\n<ol>\n<li>Fix error #1</li>\n<li>Recompile</li>\n<li>Discover error #2 (which was always there)</li>\n<li>Fix error #2</li>\n<li>Recompile</li>\n<li>Discover error #3</li>\n<li>Repeat...\nIf a file has ten lexical errors, stopping at the first forces ten compile-fix cycles. A developer who just made ten mistakes in a hurry â€” perhaps they&#39;re learning the language, or they copied a large block of code from a different language â€” would spend far more time discovering errors than fixing them.\n<strong>Production lexers never stop at the first error.</strong> They emit an <code>ERROR</code> token, skip the problematic character(s), and continue scanning. A single tokenizer pass over a file with ten errors produces a token stream containing ten <code>ERROR</code> tokens alongside all the valid tokens. The parser and error reporter can then display all ten errors in one shot.\nThis strategy has a name: <strong>panic mode recovery</strong> at the lexical level. The word &quot;panic&quot; comes from compiler theory â€” when the scanner hits an unrecognized character, it &quot;panics,&quot; emits what information it can (the error token with position), skips forward to try to re-synchronize, and then resumes normal operation. The scanner doesn&#39;t halt; it recovers.</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-error-recovery-flow.svg\" alt=\"Error Recovery â€” Skip and Continue\"></p>\n<h2 id=\"the-key-insight-the-architect-flagged-the-error-token-is-a-valid-token-in-your-token-stream-it39s-not-a-special-out-of-band-signal-it39s-a-regular-entry-in-the-listtoken-returned-by-scan_tokens-the-scanner-produces-it-the-parser-receives-it-and-the-error-reporter-processes-it-error-tokens-are-part-of-the-api-you39ve-actually-already-implemented-this-your-scanner-emits-error-tokens-and-keeps-running-this-milestone-is-about-testing-that-behavior-rigorously-understanding-why-it-matters-and-making-sure-your-recovery-strategy-doesn39t-skip-too-much-or-too-little\">The key insight the Architect flagged: <strong>the <code>ERROR</code> token IS a valid token in your token stream.</strong> It&#39;s not a special out-of-band signal. It&#39;s a regular entry in the <code>list[Token]</code> returned by <code>scan_tokens()</code>. The scanner produces it, the parser receives it, and the error reporter processes it. Error tokens are part of the API.\nYou&#39;ve actually already implemented this â€” your scanner emits <code>ERROR</code> tokens and keeps running. This milestone is about testing that behavior rigorously, understanding <em>why</em> it matters, and making sure your recovery strategy doesn&#39;t skip too much or too little.</h2>\n<h2 id=\"error-recovery-the-three-variables\">Error Recovery: The Three Variables</h2>\n<p>When your scanner hits an unrecognized character, it faces three decisions:\n<strong>1. What to emit?</strong>\nYou emit an <code>ERROR</code> token containing the offending character and its exact position. This is the right answer for single unrecognized characters â€” it gives the error reporter everything it needs to tell the user &quot;unexpected <code>@</code> at line 42, column 7.&quot;\n<strong>2. How much to skip?</strong>\nThis is the hard part. After emitting the error, how far do you advance before resuming normal scanning?</p>\n<ul>\n<li><strong>Skip one character (chosen âœ“)</strong>: Emit <code>ERROR</code> for the unrecognized char, then let <code>_scan_token()</code> restart normally from the very next character.</li>\n<li><strong>Skip to next whitespace</strong>: Faster re-synchronization but risks swallowing valid tokens immediately after the bad character.</li>\n<li><strong>Skip to next newline</strong>: Very aggressive â€” useful for recovering from malformed directives, but skips too much for inline errors.</li>\n<li><strong>Skip to next statement boundary</strong>: Parser-level recovery, not lexer-level â€” the lexer doesn&#39;t know what a &quot;statement boundary&quot; is.\nFor lexical errors (single unrecognized characters), <strong>skip-one is the right answer</strong>. The reason: most lexical errors are isolated â€” a stray <code>@</code> in the middle of otherwise valid code. After the <code>@</code>, the scanner is perfectly positioned to continue normally. There&#39;s no cascading damage to recover from. Skip too much and you&#39;ll miss real subsequent tokens.\nThe situation would be different if you were recovering from a multi-character malformed construct. But for isolated unrecognized characters, skip-one is the production choice â€” used by GCC, Clang, and the CPython tokenizer.\n<strong>3. Keep state or reset?</strong>\nSkip-one naturally resets state, because you return from <code>_scan_token()</code> with an <code>ERROR</code> token, the main loop calls <code>_scan_token()</code> again, and <code>start</code>/<code>start_column</code> are re-snapshotted at the new position. No explicit state reset needed. The architecture handles it.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-multi-error-collection.svg\" alt=\"Multi-Error Reporting â€” Collecting All Errors in One Pass\"></p>\n<p>Here&#39;s the critical implementation point: <strong>your scanner already implements skip-one recovery</strong> because of how <code>_scan_token()</code> is structured. When it hits an unrecognized character in the <code>else</code> branch, it returns an <code>ERROR</code> token for that single character. The main <code>scan_tokens()</code> loop then calls <code>_scan_token()</code> again, starting fresh from <code>current</code> (which is already advanced past the bad character). No additional code is needed.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># This is ALREADY in your scanner â€” error recovery is built in</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()           </span><span style=\"color:#6A737D\"># consumes exactly one character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ... all the normal dispatch ...</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Unknown character: emit ERROR for just this one char.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 'advance()' already moved past it. Next call to _scan_token()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # starts fresh at 'current' â€” which is the character AFTER the bad one.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span></code></pre></div>\n<h2 id=\"the-recovery-is-implicit-in-the-design-advance-always-moves-forward-by-exactly-one-character-and-_scan_token-always-returns-control-to-scan_tokens-after-one-token-there39s-no-way-to-get-quotstuckquot-on-a-bad-character-error-recovery-was-free-you-got-it-from-the-architecture\">The recovery is implicit in the design. <code>advance()</code> always moves forward by exactly one character, and <code>_scan_token()</code> always returns control to <code>scan_tokens()</code> after one token. There&#39;s no way to get &quot;stuck&quot; on a bad character. Error recovery was free â€” you got it from the architecture.</h2>\n<h2 id=\"collecting-all-errors-the-accumulator-pattern\">Collecting All Errors: The Accumulator Pattern</h2>\n<p>The main <code>scan_tokens()</code> loop appends every non-<code>None</code> token to <code>self.tokens</code>, regardless of its type. <code>ERROR</code> tokens go in exactly like <code>NUMBER</code> or <code>KEYWORD</code> tokens:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> scan_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_token()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.tokens.append(token)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.tokens.append(Token(TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens</span></span></code></pre></div>\n<p>If you want to separate errors from valid tokens after the fact â€” for a cleaner API â€” you can filter the returned list:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_errors</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Extract all ERROR tokens from the scanned token stream.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_valid_tokens</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token stream with ERROR tokens removed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n<h2 id=\"but-the-important-point-is-that-scan_tokens-itself-doesn39t-filter-it-accumulates-everything-the-caller-decides-what-to-do-with-errors-a-compiler39s-driver-might-print-all-errors-and-abort-before-calling-the-parser-a-language-server-might-pass-the-entire-stream-including-errors-to-the-parser-so-it-can-attempt-partial-analysis-for-code-completion-this-quoterrors-as-values-not-exceptionsquot-design-is-the-same-principle-behind-python39s-exception-classes-rust39s-resultltt-egt-and-go39s-value-error-return-tuples-errors-are-data-they-flow-through-the-system-the-same-way-correct-results-flow-they-can-be-collected-counted-filtered-and-reported-in-batch\">But the important point is that <code>scan_tokens()</code> itself doesn&#39;t filter â€” it accumulates everything. The <em>caller</em> decides what to do with errors. A compiler&#39;s driver might print all errors and abort before calling the parser. A language server might pass the entire stream (including errors) to the parser so it can attempt partial analysis for code completion.\nThis &quot;errors as values, not exceptions&quot; design is the same principle behind Python&#39;s <code>Exception</code> classes, Rust&#39;s <code>Result&lt;T, E&gt;</code>, and Go&#39;s <code>(value, error)</code> return tuples. <strong>Errors are data.</strong> They flow through the system the same way correct results flow. They can be collected, counted, filtered, and reported in batch.</h2>\n<h2 id=\"the-canonical-integration-test\">The Canonical Integration Test</h2>\n<p>Before writing edge cases, write the canonical test: the exact token stream for a known program, verified token-by-token. This is the most important test in your suite because it exercises the entire scanner end-to-end with no mocking or isolation.\n{{DIAGRAM:diag-full-program-trace}}\nThe acceptance criteria specify this test explicitly:</p>\n<blockquote>\n<p>Token stream for <code>if (x &gt;= 42) { return true; }</code> produces exactly: <code>Keyword(if)</code>, <code>LParen</code>, <code>Ident(x)</code>, <code>GreaterEqual</code>, <code>Number(42)</code>, <code>RParen</code>, <code>LBrace</code>, <code>Keyword(return)</code>, <code>Keyword(true)</code>, <code>Semicolon</code>, <code>RBrace</code>, <code>EOF</code>\nLet&#39;s write it formally:</p>\n</blockquote>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_canonical_expression</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    The single most important integration test.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Verifies every token type, every lexeme, and proves the full pipeline works.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"if (x >= 42) { return true; }\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"(\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\">=\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\")\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"{\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\";\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#9ECBFF\">\"}\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,           </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected), (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Token count mismatch: expected </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(expected)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(tokens)</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Actual: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tokens</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (token, (exp_type, exp_lexeme)) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">zip</span><span style=\"color:#E1E4E8\">(tokens, expected)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> exp_type, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: type mismatch. \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">exp_type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"(lexeme=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> exp_lexeme, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: lexeme mismatch. \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">exp_lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"(type=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n<p>Notice the error messages inside each assertion. When a test fails, &quot;AssertionError&quot; tells you nothing. &quot;Token 3: type mismatch. Expected GREATER_EQUAL, got OPERATOR (lexeme=&#39;&gt;&#39;)&quot; tells you exactly which token is wrong and why. Good assertions are part of good testing.\nNow write the integration test for a complete multi-line program. This is what makes the test a real end-to-end check:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">MULTILINE_PROGRAM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"\"</span><span style=\"color:#79B8FF\">\\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">// Compute absolute value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if (x >= 0) {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = x;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">} else {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = 0 - x;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">/* Return the result */</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">return result;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multiline_program_complete</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Full multi-line program tokenized token-by-token.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Tests interaction of: line comments, identifiers, operators,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    numbers, keywords, punctuation, and block comments.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#79B8FF\">MULTILINE_PROGRAM</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Exclude EOF for easier assertion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    non_eof </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify no ERROR tokens in a valid program</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [], </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unexpected errors in valid program: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">errors</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Spot-check key positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Line 1 is the comment â€” should produce NO tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # First token is 'if' on line 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"if\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify '0 - x' on line 5: NUMBER(0), OPERATOR(-), IDENTIFIER(x)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Find the '0' token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    zero_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                   if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> t.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"0\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(zero_tokens) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Find the 'return' keyword â€” should be on line 8 (after block comment)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    return_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                     if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> t.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"return\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(return_tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> return_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"'return' should be on line 8 (after 7 lines), \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"got line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">return_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Last non-EOF token is ';' after 'result'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \";\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # EOF should be present</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span></code></pre></div>\n<h2 id=\"this-test-catches-the-bugs-that-unit-tests-miss-does-the-line-comment-on-line-1-correctly-produce-zero-tokens-so-that-if-lands-on-line-2-does-the-block-comment-on-line-7-update-the-line-counter-so-that-return-lands-on-line-8-these-are-the-interactions-that-only-surface-when-the-full-input-runs-together\">This test catches the bugs that unit tests miss: does the line comment on line 1 correctly produce zero tokens so that <code>if</code> lands on line 2? Does the block comment on line 7 update the line counter so that <code>return</code> lands on line 8? These are the interactions that only surface when the full input runs together.</h2>\n<h2 id=\"testing-error-recovery-explicitly\">Testing Error Recovery Explicitly</h2>\n<p>Your error recovery is implicit in the architecture, but you must test it explicitly. Tests prove behavior; architecture proves nothing.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_error_continues</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"After one error, scanning resumes and finds the next valid token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"@\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"+\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_multiple_errors_all_collected</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    All errors in a single input are collected in one pass.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is the fundamental multi-error reporting test.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@#$%\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(error_tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Expected 4 errors (one per bad char), got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(error_tokens)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify each error has a distinct, correct position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"@\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"#\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"$\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"%\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> error_tokens[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_errors_interspersed_with_valid</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Errors interspersed with valid tokens: all tokens (valid and error)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    appear in the correct order.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x @ y # z\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># x</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#6A737D\"># @</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># y</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#6A737D\"># #</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># z</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> types </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_types, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Token type sequence wrong: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">types</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_does_not_consume_next_valid_char</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Error recovery skips exactly ONE character.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    The character immediately after the bad one is NOT consumed by recovery.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@if\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # '@' is ERROR, 'if' is KEYWORD â€” NOT '@i' as error and 'f' as identifier</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"@\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"if\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_fifteen_errors_all_reported</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Simulate a realistic scenario: 15 errors in one file.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Confirms multi-error collection scales.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bad_chars </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"@#$%^&#x26;~`|</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">??\"</span><span style=\"color:#6A737D\">  # 12 unrecognized characters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(bad_chars)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Every bad character should produce exactly one ERROR token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(bad_chars), (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Expected </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(bad_chars)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> errors, got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(errors)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_position_on_second_line</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Error on line 2 reports the correct line and column,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    proving line tracking is accurate even after a newline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"valid</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">@bad\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    at_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> at_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected line 2, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">at_token.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> at_token.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected column 1, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">at_token.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n<h2 id=\"the-test-test_error_does_not_consume_next_valid_char-is-particularly-important-it-verifies-that-skip-one-recovery-is-truly-skip-one-it-doesn39t-accidentally-swallow-the-character-after-the-error-this-test-would-fail-if-advance-were-called-twice-in-the-error-branch\">The test <code>test_error_does_not_consume_next_valid_char</code> is particularly important. It verifies that skip-one recovery is truly skip-<em>one</em> â€” it doesn&#39;t accidentally swallow the character after the error. This test would fail if <code>advance()</code> were called twice in the error branch.</h2>\n<h2 id=\"edge-cases-the-boundary-tests\">Edge Cases: The Boundary Tests</h2>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-edge-cases-map.svg\" alt=\"Edge Case Gallery â€” Empty Input, Single Char, Boundaries\"></p>\n<p>Edge cases are the inputs that live at the boundary of your specification. They&#39;re where bugs hide because they&#39;re the inputs nobody thinks to test when they&#39;re writing the main logic.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_empty_input</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Empty string produces exactly one token: EOF.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    EOF must be present even with zero characters of input.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Empty input should produce exactly 1 token, got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(tokens)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_character_valid</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single valid character produces the right token + EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> char, expected_type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"(\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"x\"</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(char)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Single '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">char</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' should produce 1 token + EOF\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_type, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"'</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">char</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' â†’ expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_character_invalid</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single invalid character produces ERROR + EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"@\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_single_character_whitespace</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single whitespace character produces only EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> ws </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\" \"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(ws)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Whitespace </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">ws</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> should produce only EOF\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_maximum_length_identifier</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Very long identifiers are handled correctly.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Python strings have no practical length limit, so this tests</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    that the scanner loop doesn't have an artificial cutoff.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    long_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 10_000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(long_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # IDENTIFIER + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> long_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 10_000</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_maximum_length_keyword_prefix</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    An identifier that STARTS with a keyword but is longer must be IDENTIFIER.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'returning' starts with 'return' but is not the keyword.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> kw, longer </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"iffy\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"else\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"elsewhere\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"while\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"while_true\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"returning\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"trueness\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"false\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"falsehood\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (</span><span style=\"color:#9ECBFF\">\"null\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"nullify\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(longer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"'</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">longer</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' starts with keyword '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">kw</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' but must be IDENTIFIER\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> longer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_all_whitespace_types</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All whitespace characters (space, tab, CR, LF) produce only EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"   </span><span style=\"color:#79B8FF\">\\t\\t\\r\\n\\t</span><span style=\"color:#79B8FF\">  \\r\\n</span><span style=\"color:#9ECBFF\">  \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_operator_at_eof</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A valid operator as the last character (before EOF) is handled correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"+\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_number_zero</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"The number '0' is a valid integer literal.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"0\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_consecutive_operators</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multiple operators with no whitespace are tokenized individually.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"+-*/\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexemes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.lexeme </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens[:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> lexemes </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"-\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_string_immediately_followed_by_token</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"String literal followed immediately by a token, no whitespace.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">'\"hello\"+'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"hello\"'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"+\"</span></span></code></pre></div>\n<hr>\n<h2 id=\"position-accuracy-verification-preventing-drift\">Position Accuracy Verification: Preventing Drift</h2>\n<p>Position tracking is the feature that&#39;s hardest to test manually but most important to get right. Errors in line and column numbers accumulate â€” a single off-by-one at line 5 means everything from line 5 onward reports the wrong position.\n{{DIAGRAM:diag-position-drift}}\nThis category of test verifies that position information doesn&#39;t <em>drift</em> â€” that the accumulated error over a multi-line input is zero, not just small.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_line_tracking_across_newlines</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Each newline increments the line counter by exactly 1.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Verify multiple tokens across multiple lines.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">b</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">c</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">d</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">e\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Tokens: a(line1), b(line2), c(line3), d(line4), e(line5), EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, token </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(tokens[:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]):   </span><span style=\"color:#6A737D\"># skip EOF</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_line, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: expected line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: expected column 1, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_column_tracking_across_tokens</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tokens on the same line have correctly incrementing columns.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"a + b\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 'a' at column 1, '+' at column 3, 'b' at column 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # 'a'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">   # '+' (space between)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">   # 'b'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_column_resets_after_newline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"After a newline, column resets to 1 for the first token on the new line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"ab</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">cd\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"ab\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"cd\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # reset after '\\n'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_in_multiline_string_block</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Position after a multi-line comment is correct,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    demonstrating that newlines inside comments are tracked.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"a</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#6A737D\">                # line 1: identifier 'a'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"/* line 2</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#6A737D\">        # line 2: start of block comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"line 3</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#6A737D\">           # line 3: inside block comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"line 4 */</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#6A737D\">        # line 4: end of block comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"b\"</span><span style=\"color:#6A737D\">                  # line 5: identifier 'b'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    non_eof </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(non_eof) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # 'a' and 'b', comment produced nothing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"a\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"b\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"'b' should be on line 5 (after 4-line comment block), \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"got line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">non_eof[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_fifty_line_program</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Verify position accuracy on a 50-line program.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Simulates a real multi-line input and checks the final token's position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"x</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">;\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">51</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Find all tokens on line 50</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line_50_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> line_50_tokens, </span><span style=\"color:#9ECBFF\">\"Should have tokens on line 50\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # First token on line 50 should be identifier 'x50'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    first_on_50 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line_50_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> first_on_50.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> first_on_50.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"x50\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> first_on_50.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error_position_accuracy_midline</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    An error character in the middle of a line reports the exact column.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"abc @ def\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">   # 'abc ' = 4 chars, '@' is at column 5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> error.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"@\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_windows_line_endings_not_double_counted</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Windows CRLF line endings (</span><span style=\"color:#79B8FF\">\\r\\n</span><span style=\"color:#9ECBFF\">) count as ONE line, not two.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">' is whitespace, '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">' triggers line increment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"a</span><span style=\"color:#79B8FF\">\\r\\n</span><span style=\"color:#9ECBFF\">b</span><span style=\"color:#79B8FF\">\\r\\n</span><span style=\"color:#9ECBFF\">c\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    non_eof </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">   # 'a'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # 'b' â€” not line 3 (if \\r\\n counted twice)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> non_eof[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">   # 'c'</span></span></code></pre></div>\n<h2 id=\"the-test_windows_line_endings_not_double_counted-test-is-particularly-important-this-bug-counting-rn-as-two-newlines-is-remarkably-common-and-it39s-invisible-until-someone-runs-your-tokenizer-on-a-file-edited-in-windows-notepad-the-test-catches-it\">The <code>test_windows_line_endings_not_double_counted</code> test is particularly important. This bug â€” counting <code>\\r\\n</code> as two newlines â€” is remarkably common, and it&#39;s invisible until someone runs your tokenizer on a file edited in Windows Notepad. The test catches it.</h2>\n<h2 id=\"the-complete-integration-test-suite-a-real-program\">The Complete Integration Test Suite: A Real Program</h2>\n<p>Now write the comprehensive test that ties everything together. This is the test that a compiler project would use as its regression suite â€” the &quot;golden test&quot; that must pass before any change is merged.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">COMPLETE_TEST_PROGRAM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"\"</span><span style=\"color:#79B8FF\">\\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">// Program: factorial</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">// Tests all token types</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">/* Multi-line comment block</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">   testing comment handling */</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">x = 0;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">result = null;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if (x != 0) {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    while (x >= 1) {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        result = result * x;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        x = x - 1;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    }</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">} else {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    result = 1;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">greeting = \"hello</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">nworld\";</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">check = (result == 42);</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">valid = true;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">return result;</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_program_no_errors</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete program produces no ERROR tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#79B8FF\">COMPLETE_TEST_PROGRAM</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [], (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Expected no errors in valid program, got: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">errors</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_program_ends_with_eof</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token stream always ends with EOF.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#79B8FF\">COMPLETE_TEST_PROGRAM</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_program_token_types</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All expected token types appear in the complete program.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#79B8FF\">COMPLETE_TEST_PROGRAM</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_types_present </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Every type used in the program should appear</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,       </span><span style=\"color:#6A737D\"># if, else, while, return, true, null</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#6A737D\"># x, result, greeting, check, valid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#6A737D\"># 0, 1, 42</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#6A737D\"># \"hello\\nworld\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#6A737D\"># *, -</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\"># >=</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#E1E4E8\">,        </span><span style=\"color:#6A737D\"># ==</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># (, ), {, }, ;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    missing </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected_types </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> token_types_present</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> missing </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected token types not found: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">missing</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_program_keyword_counts</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify keyword occurrences match source.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#79B8FF\">COMPLETE_TEST_PROGRAM</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keywords </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t.lexeme </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> keywords.count(</span><span style=\"color:#9ECBFF\">\"if\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> keywords.count(</span><span style=\"color:#9ECBFF\">\"else\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> keywords.count(</span><span style=\"color:#9ECBFF\">\"while\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> keywords.count(</span><span style=\"color:#9ECBFF\">\"return\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> keywords.count(</span><span style=\"color:#9ECBFF\">\"true\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> keywords.count(</span><span style=\"color:#9ECBFF\">\"null\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_program_string_token</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"The string literal in the program is tokenized correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#79B8FF\">COMPLETE_TEST_PROGRAM</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    string_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(string_tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Raw lexeme includes the quotes and the raw backslash-n</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> string_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '\"hello</span><span style=\"color:#79B8FF\">\\\\\\\\</span><span style=\"color:#9ECBFF\">nworld\"'</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">           \"hello\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> string_tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme  </span><span style=\"color:#6A737D\"># raw lexeme check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_program_comment_lines_produce_no_tokens</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Lines 1, 2, and 4-5 are comments.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    First real token ('x') should be on line 7.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#79B8FF\">COMPLETE_TEST_PROGRAM</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    first_real_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> first_real_token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> first_real_token.lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"x\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Line 7 in the source (after 2 line comments + blank + block comment + blank)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> first_real_token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 7</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"First token should be on line 7, got line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">first_real_token.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n<hr>\n<h2 id=\"performance-the-10000-line-benchmark\">Performance: The 10,000-Line Benchmark</h2>\n<p>A tokenizer that works correctly but takes 30 seconds on a typical source file is not useful. The performance requirement is concrete: 10,000 lines in under 1 second.\nLet&#39;s understand why this is achievable, and why it&#39;s the right threshold.\n<strong>Why a well-written lexer is I/O bound, not CPU bound:</strong>\nA Python lexer reading a 10,000-line file (roughly 300,000 characters at 30 chars/line average) performs about 300,000 character comparisons. Python executes roughly 10â€“50 million simple operations per second. At the lower bound, 300,000 comparisons takes about 6 milliseconds of CPU time. Even accounting for Python&#39;s overhead, function call costs, and list appends, the total time should be well under 1 second.\nIn production (C/C++/Rust implementations), lexers process gigabytes per second because they&#39;re limited by memory bandwidth â€” data transfer from RAM â€” not computation. CPython&#39;s tokenizer can process over 100,000 lines per second. GCC&#39;s lexer processes millions of lines per second. The 1-second/10,000-line requirement is a very generous budget.\n<strong>What would make a lexer slow?</strong></p>\n<ul>\n<li><strong>Excessive string allocation</strong>: creating many small intermediate strings in the inner loop</li>\n<li><strong>Quadratic operations</strong>: searching or slicing in O(n) inside an O(n) loop = O(nÂ²)</li>\n<li><strong>Regex compilation</strong>: compiling a regex pattern inside the scan loop rather than once at startup</li>\n<li><strong>Unnecessary I/O</strong>: reading from disk inside the scan loop (rare, but catastrophic)\nYour implementation avoids all of these by design: you maintain a <code>current</code> index into the already-loaded source string and slice only at token boundaries (<code>source[start:current]</code>). The inner loop does constant-time work per character.</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_test_program</span><span style=\"color:#E1E4E8\">(num_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generate a synthetic program with num_lines lines.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Each line contains a variety of token types to exercise all scanner paths.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(num_lines):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Rotate through different line patterns for realistic coverage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 6</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> pattern </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"    x</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">;\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> pattern </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"    // comment on line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> pattern </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'    label</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = \"string </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\";'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> pattern </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"    if (x</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> >= </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) </span><span style=\"color:#79B8FF\">{{</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> pattern </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"        result = result + x</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">;\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"    </span><span style=\"color:#79B8FF\">}}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(lines)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_performance_ten_thousand_lines</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Tokenizing 10,000 lines must complete in under 1 second.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This verifies the scanner has no O(nÂ²) behavior or pathological cases.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_test_program(</span><span style=\"color:#79B8FF\">10_000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Warm up Python's JIT-like optimizations (though CPython doesn't JIT,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # this ensures module imports and initial overhead don't count)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Scanner(source[:</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">]).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Basic correctness: should have many tokens and end with EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 10_000</span><span style=\"color:#6A737D\">  # at least one token per line on average</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Performance failure: 10,000-line input took </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">elapsed</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"(limit: 1.0s). Token count: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(tokens)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Also report the performance (useful even when test passes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines_per_second </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10_000</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> elapsed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Performance: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">lines_per_second</span><span style=\"color:#F97583\">:.0f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> lines/second \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">          f</span><span style=\"color:#9ECBFF\">\"(</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">elapsed</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#F97583\">:.1f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms for 10,000 lines)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_performance_pathological_string</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    A string literal with 10,000 characters shouldn't cause O(nÂ²) behavior.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    The string scanner is a simple loop â€” this verifies no quadratic slicing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    long_string </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#F97583\"> +</span><span style=\"color:#9ECBFF\"> \"x\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 10_000</span><span style=\"color:#F97583\"> +</span><span style=\"color:#9ECBFF\"> '\"'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(long_string)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 10_002</span><span style=\"color:#6A737D\">   # 10000 chars + 2 quotes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Long string took </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">elapsed</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s (too slow)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_performance_many_errors</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    A file full of error characters doesn't cause degenerate behavior.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Error recovery (skip-one) should be O(n) on the number of bad chars.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    all_errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"@#$%\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 2_500</span><span style=\"color:#6A737D\">   # 10,000 error characters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(all_errors)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 10_000</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Error-heavy input took </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">elapsed</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s (limit: 1.0s)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n<p><strong>Interpreting the performance numbers:</strong>\nIf your scanner takes 0.05 seconds on 10,000 lines, that&#39;s 200,000 lines/second â€” excellent for Python. If it takes 0.5 seconds, that&#39;s 20,000 lines/second â€” still well within spec. If it takes 3+ seconds, there&#39;s an O(nÂ²) bug somewhere, likely in string slicing inside an inner loop.\nThe most common O(nÂ²) mistake is accumulating the lexeme character-by-character using string concatenation inside the scan loop:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ O(nÂ²) â€” string concatenation creates a new string each time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_identifier</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lexeme </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># new string object every iteration!</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ...</span></span></code></pre></div>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âœ… O(n) â€” slice once at the end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_identifier</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek().isalnum() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.advance()  </span><span style=\"color:#6A737D\"># just advance the index</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    text </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start:</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current]  </span><span style=\"color:#6A737D\"># ONE slice at the end</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ...</span></span></code></pre></div>\n<h2 id=\"your-implementation-already-uses-the-slice-approach-but-if-the-performance-test-fails-this-is-the-first-thing-to-check\">Your implementation already uses the slice approach â€” but if the performance test fails, this is the first thing to check.</h2>\n<h2 id=\"the-full-test-runner\">The Full Test Runner</h2>\n<p>Organize all your tests into a runnable suite. Python&#39;s built-in <code>unittest</code> module or <code>pytest</code> (installed separately) both work:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test_scanner.py â€” complete test file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> scanner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Scanner, Token, TokenType  </span><span style=\"color:#6A737D\"># adjust import to your file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Milestone 1: Foundation tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m1_empty_input</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m1_single_char_operators</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"+-*/\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">  # 4 operators + EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens[:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Milestone 2: Multi-character tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m2_canonical_expression</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"if (x >= 42) { return true; }\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">, TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> [t.type </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Milestone 3: Strings and comments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m3_string_with_escape</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">\"hello</span><span style=\"color:#85E89D;font-weight:bold\">\\n</span><span style=\"color:#DBEDFF\">world\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m3_block_comment_updates_line</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"/* line1</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">line2</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">*/x\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># â”€â”€ Milestone 4: Integration and error recovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m4_error_recovery_skip_one</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"@+\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"@\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m4_fifteen_errors_all_reported</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bad </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"@#$%^&#x26;~`|</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">?? \"</span><span style=\"color:#6A737D\">  # mix of bad chars and whitespace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(bad).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">  # at least some errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m4_max_length_identifier</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    long_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"x\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 10_000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(long_id).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 10_000</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m4_position_fifty_lines</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"x</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">51</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> last_id.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 50</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_m4_performance</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"x = 42;</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 10_000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Too slow: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">elapsed</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Simple test runner without pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tests </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [v </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">globals</span><span style=\"color:#E1E4E8\">().items()) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> k.startswith(</span><span style=\"color:#9ECBFF\">\"test_\"</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    passed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> failed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> test </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tests:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            test()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  âœ“ </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test.</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            passed </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> AssertionError</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  âœ— </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test.</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            failed </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  âœ— </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test.</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\">: EXCEPTION: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            failed </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n{</span><span style=\"color:#E1E4E8\">passed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> passed, </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">failed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> failed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sys.exit(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> failed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n<h2 id=\"run-this-file-every-test-should-pass-if-any-fail-the-error-messages-in-the-assertions-will-tell-you-exactly-which-expectation-was-violated\">Run this file. Every test should pass. If any fail, the error messages in the assertions will tell you exactly which expectation was violated.</h2>\n<h2 id=\"three-level-view-what-testing-means-at-each-layer\">Three-Level View: What Testing Means at Each Layer</h2>\n<p><strong>Level 1 â€” Source Language (the program under test)</strong>: To the programmer, <code>if (x &gt;= 42) { return true; }</code> is a conditional expression. They expect their compiler to understand it. If the tokenizer breaks it, they see a mysterious parse error even though the syntax is correct. Correct tokenization is invisible â€” users only notice it when it&#39;s wrong.\n<strong>Level 2 â€” The Scanner (what you&#39;re testing)</strong>: At this level, the integration test is a verification of the <em>interface contract</em>: given this exact input string, produce this exact token list. The token list is the scanner&#39;s public API. Tests at this level pin down the API&#39;s behavior completely â€” they&#39;re the spec expressed as executable code.\n<strong>Level 3 â€” The Parser and Downstream (future consumers)</strong>: The parser that will consume your token stream needs to trust it completely. If your scanner emits <code>OPERATOR(&#39;&gt;&#39;)</code> + <code>ASSIGN(&#39;=&#39;)</code> instead of <code>GREATER_EQUAL(&#39;&gt;=&#39;)</code>, the parser fails. The integration tests you write here are simultaneously tests of the scanner <em>and</em> documentation of the interface that the parser will rely on. Every test is a contract between the scanner and its consumers.</p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Fdiag-tokenizer-in-compiler-pipeline.svg\" alt=\"Where the Tokenizer Fits â€” The Full Compiler Pipeline\"></p>\n<hr>\n<h2 id=\"common-pitfalls-in-integration-testing\">Common Pitfalls in Integration Testing</h2>\n<h3 id=\"pitfall-1-testing-only-the-happy-path\">Pitfall 1: Testing Only the Happy Path</h3>\n<p>The most dangerous gap in test suites is missing error cases. Unit tests for string scanning often only test valid strings. Integration tests often only use valid programs. But the acceptance criteria explicitly require error recovery â€” which means you need programs with errors.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_program_with_deliberate_errors</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    A program containing invalid characters should tokenize everything</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    it CAN tokenize correctly, and report all errors.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"x = @42;</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">y = #5;\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Two error tokens: '@' and '#'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"@\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"#\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> errors[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].line </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Valid tokens still present: x, =, 42, ;, y, =, 5, ;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    valid </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ERROR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">             and</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    identifiers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> valid </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(identifiers) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # 'x' and 'y'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numbers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> valid </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(numbers) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">   # '42' and '5'</span></span></code></pre></div>\n<h3 id=\"pitfall-2-position-drift-that-only-shows-up-late\">Pitfall 2: Position Drift That Only Shows Up Late</h3>\n<p>A position tracking bug might be invisible for the first 5 lines and only manifest on line 100. This is why the 50-line and performance tests are valuable even if they seem redundant with the earlier line-tracking tests. Accumulated drift â€” a bug that&#39;s off by 1 line every 10 lines â€” is caught by longer tests.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_no_position_drift_over_long_input</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generate 100 single-token lines and verify EVERY token's line number.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Catches any drift that accumulates over multiple lines.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"x</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(num_lines)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    non_eof </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [t </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> t.type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(non_eof) </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> num_lines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, token </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(non_eof):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_line, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> ('</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.lexeme</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'): expected line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">. Position drift detected.\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> token.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, (</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Token </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> ('</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.lexeme</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'): expected column 1, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n<h3 id=\"pitfall-3-asserting-token-count-instead-of-token-content\">Pitfall 3: Asserting Token Count Instead of Token Content</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># âŒ Fragile: tests only that SOME tokens exist, not what they are</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_expression</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x + y\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">   # wrong: tells you nothing about content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># âœ… Thorough: tests type AND lexeme of every token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_expression</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(</span><span style=\"color:#9ECBFF\">\"x + y\"</span><span style=\"color:#E1E4E8\">).scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"x\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"+\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">IDENTIFIER</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">].lexeme </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"y\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> tokens[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">].type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">EOF</span></span></code></pre></div>\n<h2 id=\"token-count-tests-pass-even-when-the-scanner-emits-keyword-instead-of-identifier-or-assign-instead-of-equals-content-tests-catch-those-bugs-test-content-not-count\">Token count tests pass even when the scanner emits <code>KEYWORD</code> instead of <code>IDENTIFIER</code>, or <code>ASSIGN</code> instead of <code>EQUALS</code>. Content tests catch those bugs. Test content, not count.</h2>\n<h2 id=\"knowledge-cascade-quotlearn-one-unlock-tenquot\">Knowledge Cascade â€” &quot;Learn One, Unlock Ten&quot;</h2>\n<h2 id=\"you39ve-validated-a-complete-tokenizer-with-integration-tests-error-recovery-position-accuracy-and-performance-here39s-what-that-knowledge-unlocks-error-recovery-in-parsers-uses-the-same-principle-at-a-higher-level-your-lexer-recovers-from-unknown-characters-by-skipping-one-character-and-continuing-parsers-face-the-same-problem-with-invalid-token-sequences-they-hit-a-token-that-doesn39t-fit-any-grammar-rule-and-must-decide-how-to-continue-the-parser-level-version-of-panic-mode-synchronizes-on-statement-boundaries-typically-semicolons-and-closing-braces-rather-than-single-characters-when-a-parser-sees-an-unexpected-token-it-discards-tokens-until-it-finds-a-or-that-marks-the-end-of-a-statement-then-resumes-your-lexer39s-skip-one-strategy-and-the-parser39s-synchronize-on-boundary-strategy-are-the-same-algorithm-applied-at-different-granularities-if-you-go-on-to-build-a-parser-you39ll-implement-this-exact-pattern-again-multi-error-reporting-is-a-product-decision-as-much-as-an-engineering-one-the-rust-compiler39s-reputation-for-excellent-error-messages-is-not-an-accident-it-was-a-deliberate-design-goal-described-in-early-rfc-discussions-rust39s-compiler-team-invested-heavily-in-collecting-multiple-errors-explaining-them-in-natural-language-and-suggesting-fixes-that-quality-starts-at-the-lexer-which-emits-error-tokens-instead-of-halting-go39s-compiler-also-reports-multiple-errors-but-is-notably-more-terse-cpython39s-tokenizer-traditionally-stops-at-the-first-error-which-is-why-python-c-quotsyntaxerroranotherquot-only-reports-the-first-problem-the-technical-infrastructure-for-multi-error-reporting-is-the-same-whether-the-ux-is-excellent-or-poor-but-rust-put-ux-investment-on-top-of-that-infrastructure-understanding-the-difference-helps-you-recognize-that-compiler-quality-is-partly-a-values-decision-quotexpected-token-streamquot-tests-are-how-gcc-clang-and-rustc-test-their-frontends-the-test-pattern-you39ve-built-here-assert-token_list-expected-is-called-a-quotgolden-testquot-or-quotsnapshot-testquot-gcc39s-test-suite-contains-thousands-of-source-files-paired-with-expected-diagnostic-output-clang39s-filecheck-utility-verifies-that-specific-patterns-appear-in-compiler-output-rust39s-compiletest-framework-runs-programs-and-checks-the-exact-error-messages-when-you-read-quotthis-pr-fixes-a-regression-in-the-lexer39s-handling-of-unicode-identifiersquot-in-a-compiler-changelog-someone-added-a-new-golden-test-that-exposed-the-bug-and-verified-the-fix-you39ve-built-the-same-infrastructure-the-token-stream-is-an-api-and-error-tokens-are-part-of-that-api-software-engineers-who-build-microservices-spend-a-lot-of-time-thinking-about-error-contracts-in-apis-what-does-a-400-error-mean-what-does-a-500-error-mean-what-fields-does-the-error-response-include-your-token-stream-has-the-same-structure-error-tokens-are-a-defined-part-of-the-api-contract-with-defined-fields-lexeme-line-column-a-parser-written-to-consume-your-token-stream-must-handle-error-tokens-gracefully-perhaps-by-skipping-them-perhaps-by-triggering-its-own-error-recovery-perhaps-by-collecting-them-the-important-point-is-that-the-parser-must-handle-them-because-they39re-defined-in-the-contract-treating-errors-as-values-tokens-rather-than-exceptions-halts-is-the-architectural-decision-that-makes-this-possible-lexer-performance-is-io-bound-in-practice-cpu-bound-in-python-a-c-or-rust-lexer-processing-a-10000-line-file-is-typically-limited-by-memory-bandwidth-the-time-to-read-300kb-of-data-from-ram-into-cache-not-by-computation-that39s-why-production-lexers-can-process-gigabytes-per-second-they39re-essentially-doing-a-memcpy-with-some-light-branching-cpython-is-different-the-interpreter-overhead-makes-pure-python-code-significantly-slower-than-native-code-so-a-python-lexer-is-cpu-bound-in-practice-if-you-needed-to-make-your-python-lexer-faster-the-right-approach-is-to-move-the-hot-loop-to-c-extension-code-as-cpython39s-own-tokenize-module-does-for-its-scanner-or-use-pypy-which-jit-compiles-python-to-native-code-and-would-make-your-scanner-510x-faster-understanding-what-bottleneck-you39re-fighting-memory-bandwidth-vs-interpreter-overhead-vs-algorithm-complexity-changes-how-you-approach-optimization\">You&#39;ve validated a complete tokenizer with integration tests, error recovery, position accuracy, and performance. Here&#39;s what that knowledge unlocks.\n<strong>â†’ Error recovery in parsers uses the same principle, at a higher level.</strong> Your lexer recovers from unknown characters by skipping one character and continuing. Parsers face the same problem with invalid token sequences â€” they hit a token that doesn&#39;t fit any grammar rule and must decide how to continue. The parser-level version of panic mode synchronizes on <em>statement boundaries</em> â€” typically semicolons and closing braces â€” rather than single characters. When a parser sees an unexpected token, it discards tokens until it finds a <code>;</code> or <code>}</code> that marks the end of a statement, then resumes. Your lexer&#39;s skip-one strategy and the parser&#39;s synchronize-on-boundary strategy are the same algorithm applied at different granularities. If you go on to build a parser, you&#39;ll implement this exact pattern again.\n<strong>â†’ Multi-error reporting is a product decision as much as an engineering one.</strong> The Rust compiler&#39;s reputation for excellent error messages is not an accident â€” it was a deliberate design goal, described in early RFC discussions. Rust&#39;s compiler team invested heavily in collecting multiple errors, explaining them in natural language, and suggesting fixes. That quality starts at the lexer, which emits error tokens instead of halting. Go&#39;s compiler also reports multiple errors but is notably more terse. CPython&#39;s tokenizer traditionally stops at the first error (which is why <code>python -c &quot;syntaxerror;another&quot;</code> only reports the first problem). The technical infrastructure for multi-error reporting is the same whether the UX is excellent or poor â€” but Rust put UX investment on top of that infrastructure. Understanding the difference helps you recognize that compiler quality is partly a values decision.\n<strong>â†’ &quot;Expected token stream&quot; tests are how GCC, Clang, and rustc test their frontends.</strong> The test pattern you&#39;ve built here â€” <code>assert token_list == expected</code> â€” is called a &quot;golden test&quot; or &quot;snapshot test.&quot; GCC&#39;s test suite contains thousands of source files paired with expected diagnostic output. Clang&#39;s <code>FileCheck</code> utility verifies that specific patterns appear in compiler output. Rust&#39;s <code>compiletest</code> framework runs programs and checks the exact error messages. When you read &quot;this PR fixes a regression in the lexer&#39;s handling of Unicode identifiers&quot; in a compiler changelog, someone added a new golden test that exposed the bug and verified the fix. You&#39;ve built the same infrastructure.\n<strong>â†’ The token stream is an API, and Error tokens are part of that API.</strong> Software engineers who build microservices spend a lot of time thinking about error contracts in APIs â€” what does a 400 error mean? What does a 500 error mean? What fields does the error response include? Your token stream has the same structure: <code>ERROR</code> tokens are a defined part of the API contract, with defined fields (<code>lexeme</code>, <code>line</code>, <code>column</code>). A parser written to consume your token stream must handle <code>ERROR</code> tokens gracefully â€” perhaps by skipping them, perhaps by triggering its own error recovery, perhaps by collecting them. The important point is that the parser must handle them, because they&#39;re defined in the contract. Treating errors as values (tokens) rather than exceptions (halts) is the architectural decision that makes this possible.\n<strong>â†’ Lexer performance is I/O bound in practice; CPU-bound in Python.</strong> A C or Rust lexer processing a 10,000-line file is typically limited by memory bandwidth â€” the time to read 300KB of data from RAM into cache â€” not by computation. That&#39;s why production lexers can process gigabytes per second: they&#39;re essentially doing a memcpy with some light branching. CPython is different: the interpreter overhead makes pure Python code significantly slower than native code, so a Python lexer IS CPU-bound in practice. If you needed to make your Python lexer faster, the right approach is to move the hot loop to C extension code (as CPython&#39;s own <code>tokenize</code> module does for its scanner) or use PyPy (which JIT-compiles Python to native code and would make your scanner 5â€“10x faster). Understanding what bottleneck you&#39;re fighting â€” memory bandwidth vs. interpreter overhead vs. algorithm complexity â€” changes how you approach optimization.</h2>\n<h2 id=\"what-you39ve-built\">What You&#39;ve Built</h2>\n<p>With Milestone 4 complete, you have:</p>\n<ul>\n<li><strong>Integration tests</strong> that verify the complete token stream for multi-line programs token-by-token, catching interactions that unit tests miss</li>\n<li><strong>Proven error recovery</strong>: after any unrecognized character, scanning continues from the next character, collecting all errors in a single pass</li>\n<li><strong>Multi-error reporting</strong>: a file with 15 lexical errors produces a token stream with 15 <code>ERROR</code> tokens â€” developers see all their mistakes at once</li>\n<li><strong>The canonical expression test</strong>: <code>if (x &gt;= 42) { return true; }</code> produces exactly the right 12 tokens, verified type by type and lexeme by lexeme</li>\n<li><strong>Edge case coverage</strong>: empty input, single characters, maximum-length identifiers, whitespace-only input, Windows line endings</li>\n<li><strong>Position accuracy verification</strong>: no drift over 50+ line inputs, correct column reset on newlines, correct line increment inside multi-line comments</li>\n<li><strong>Performance validation</strong>: 10,000 lines in under 1 second, with no O(nÂ²) bottlenecks\nYour tokenizer is complete. It transforms a flat string of characters into a structured, categorized, position-annotated token stream â€” the same fundamental transformation that powers every compiler frontend from GCC to V8, every IDE&#39;s syntax highlighting, every linter&#39;s error reporting, and every language server&#39;s autocomplete. You&#39;ve built the real thing.\nThe next stage in the compiler pipeline â€” the parser â€” will receive this token stream and build an Abstract Syntax Tree from it, imposing hierarchical structure on the flat sequence you&#39;ve produced. Every bug your tokenizer would have introduced â€” wrong token types, wrong positions, missed errors â€” would have corrupted the AST silently. Because your tokenizer is tested and correct, the parser gets a clean, reliable input.</li>\n</ul>\n<hr>\n<!-- END_MS -->\n\n\n\n\n<h1 id=\"tdd\">TDD</h1>\n<p>A character-level finite state machine lexer for a simple C-like language, built milestone-by-milestone in strict TDD fashion. Every data structure is pinned by a memory layout diagram, every state transition by a state machine diagram, every algorithm by a step-by-step trace. The token stream is the public API contract; tests are the spec expressed as executable code.</p>\n<!-- TDD_MOD_ID: tokenizer-m1 -->\n<h1 id=\"module-specification-token-types-amp-scanner-foundation-tokenizer-m1\">MODULE SPECIFICATION: Token Types &amp; Scanner Foundation (tokenizer-m1)</h1>\n<h2 id=\"1-module-charter\">1. Module Charter</h2>\n<p>The <strong>Token Types &amp; Scanner Foundation</strong> module serves as the entry point for the lexical analysis phase. Its primary responsibility is to define the structured vocabulary of the compiler (the <code>TokenType</code>) and the basic infrastructure required to traverse raw source text character-by-character. </p>\n<p><strong>Core Responsibilities:</strong></p>\n<ul>\n<li>Define a closed set of categories for source code fragments (Lexemes).</li>\n<li>Implement a stateful <code>Scanner</code> that maintains a cursor and human-readable position metadata (line/column).</li>\n<li>Provide low-level primitives for &quot;consuming&quot; characters (<code>advance</code>) and &quot;inspecting&quot; characters (<code>peek</code>).</li>\n<li>Distinguish between significant tokens (operators, punctuation) and insignificant characters (whitespace).</li>\n<li>Emit a sentinel <code>EOF</code> (End Of File) token to prevent parser overrun.</li>\n<li>Implement &quot;Skip-One&quot; error recovery for unrecognized characters.</li>\n</ul>\n<p><strong>Out of Scope:</strong></p>\n<ul>\n<li>Scanning multi-character tokens such as <code>&gt;=</code> or <code>!=</code> (reserved for M2).</li>\n<li>Scanning numeric literals or identifiers (reserved for M2).</li>\n<li>Handling strings, escape sequences, or comments (reserved for M3).</li>\n</ul>\n<p><strong>Invariants:</strong></p>\n<ul>\n<li>The <code>current</code> pointer must never exceed the length of the source string.</li>\n<li>The <code>line</code> and <code>column</code> metadata must stay synchronized with every character consumed via <code>advance()</code>.</li>\n<li>Every successful call to <code>scan_tokens()</code> must result in a list ending with a <code>TokenType.EOF</code> token.</li>\n</ul>\n<hr>\n<h2 id=\"2-file-structure\">2. File Structure</h2>\n<p>The implementation shall be organized into the following files, created in this order:</p>\n<ol>\n<li><code>token_type.py</code>: Definition of the <code>TokenType</code> enumeration.</li>\n<li><code>token_class.py</code>: Definition of the <code>Token</code> data structure.</li>\n<li><code>scanner.py</code>: The core <code>Scanner</code> class logic and character traversal.</li>\n<li><code>test_foundation.py</code>: Unit tests for foundation-level scanning.</li>\n</ol>\n<hr>\n<h2 id=\"3-complete-data-model\">3. Complete Data Model</h2>\n<h3 id=\"31-tokentype-enum\">3.1 TokenType (Enum)</h3>\n<p>We use a Python <code>Enum</code> to define the closed universe of token categories. Using <code>auto()</code> ensures unique integer identity without manual value management.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literals (Foundational types)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators (M1 only handles single-char versions)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># +, -, *, /</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Structural Punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># (, ), {, }, [, ], ;, ,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Special Tokens</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">         =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span></code></pre></div>\n\n<h3 id=\"32-token-dataclass\">3.2 Token (Dataclass)</h3>\n<p>The <code>Token</code> is a value object representing a recognized lexeme.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType   </span><span style=\"color:#6A737D\"># The category</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexeme: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">       # The raw substring from source</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">         # 1-based line number</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">       # 1-based column number (at start of lexeme)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Token(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.lexeme</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, line=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, col=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span></code></pre></div>\n\n<h3 id=\"33-scanner-state\">3.3 Scanner State</h3>\n<p>The <code>Scanner</code> is a stateful iterator over the source string.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>source</code></td>\n<td align=\"left\"><code>str</code></td>\n<td align=\"left\">The raw input text.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>tokens</code></td>\n<td align=\"left\"><code>list[Token]</code></td>\n<td align=\"left\">The accumulated list of recognized tokens.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>start</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Byte offset of the first character in the current lexeme being scanned.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>current</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Byte offset of the next character to be read.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>line</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Current 1-based line number in the source.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>column</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Current 1-based column number in the source.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>start_column</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Snapshot of <code>column</code> at the start of the current lexeme.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>tab_width</code></td>\n<td align=\"left\"><code>int</code></td>\n<td align=\"left\">Configurable number of columns per <code>\\t</code> character.</td>\n</tr>\n</tbody></table>\n<h3 id=\"34-memory-amp-object-layout-expert-note\">3.4 Memory &amp; Object Layout (Expert Note)</h3>\n<p>While Python handles memory, it is critical to understand the overhead for large-scale tokenization:</p>\n<ul>\n<li><strong>String Interning</strong>: Python may intern short lexemes, but every <code>Token</code> instance is a full heap object. A 100k line file may generate 500k+ tokens.</li>\n<li><strong>Scanner Overhead</strong>: The scanner itself is O(1) in space (excluding the output list), as it merely holds an reference to the input string and a few integers.</li>\n<li><strong>Byte Offsets</strong>: <code>start</code> and <code>current</code> are indices into Python&#39;s UTF-8/UTF-16/UTF-32 internal string representation.</li>\n</ul>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-1.svg\" alt=\"TokenType Enum â€” Complete Variant Map\"></p>\n<p><em>(Visual representation of Scanner pointers: <code>start</code> pinned at lexeme beginning, <code>current</code> advancing character by character, <code>source[start:current]</code> forming the lexeme)</em></p>\n<hr>\n<h2 id=\"4-interface-contracts\">4. Interface Contracts</h2>\n<h3 id=\"41-scannerinit\">4.1 Scanner.<strong>init</strong></h3>\n<ul>\n<li><strong>Signature</strong>: <code>def __init__(self, source: str, tab_width: int = 1) -&gt; None</code></li>\n<li><strong>Preconditions</strong>: <code>source</code> must be a valid string (can be empty).</li>\n<li><strong>Postconditions</strong>: Scanner initialized at (1,1) with <code>current = 0</code>.</li>\n</ul>\n<h3 id=\"42-scanneradvance\">4.2 Scanner.advance</h3>\n<ul>\n<li><strong>Signature</strong>: <code>def advance(self) -&gt; str</code></li>\n<li><strong>Logic</strong>: Consumes and returns <code>source[current]</code>. Increments <code>current</code>.</li>\n<li><strong>Position Tracking</strong>: <ul>\n<li>If consumed char is <code>\\n</code>: <code>line += 1</code>, <code>column = 1</code>.</li>\n<li>If consumed char is <code>\\t</code>: <code>column += tab_width</code>.</li>\n<li>Else: <code>column += 1</code>.</li>\n</ul>\n</li>\n<li><strong>Constraints</strong>: Must not be called if <code>is_at_end()</code> is true.</li>\n</ul>\n<h3 id=\"43-scannerpeek\">4.3 Scanner.peek</h3>\n<ul>\n<li><strong>Signature</strong>: <code>def peek(self) -&gt; str</code></li>\n<li><strong>Logic</strong>: Returns <code>source[current]</code> without incrementing the pointer.</li>\n<li><strong>Boundary</strong>: Returns <code>\\0</code> or <code>&#39;&#39;</code> if <code>is_at_end()</code> is true to avoid index errors.</li>\n</ul>\n<h3 id=\"44-scannerscan_tokens\">4.4 Scanner.scan_tokens</h3>\n<ul>\n<li><strong>Signature</strong>: <code>def scan_tokens(self) -&gt; list[Token]</code></li>\n<li><strong>Logic</strong>: Main loop calling <code>_scan_token</code> until <code>is_at_end()</code>.</li>\n<li><strong>Termination</strong>: Appends a <code>TokenType.EOF</code> token at the final <code>line</code> and <code>column</code>.</li>\n</ul>\n<hr>\n<h2 id=\"5-algorithm-specification-the-foundation-scan-loop\">5. Algorithm Specification: The Foundation Scan Loop</h2>\n<p>The foundation scan loop implements a basic Deterministic Finite Automaton (DFA) where each transition is a simple character match or a fallthrough to whitespace/error.</p>\n<h3 id=\"51-the-_scan_token-procedure\">5.1 The <code>_scan_token</code> Procedure</h3>\n<ol>\n<li><strong>Reset Lexeme Boundaries</strong>: Set <code>self.start = self.current</code> and <code>self.start_column = self.column</code>.</li>\n<li><strong>Fetch Character</strong>: <code>char = self.advance()</code>.</li>\n<li><strong>Dispatch Table Check</strong>:<ul>\n<li>If <code>char</code> in <code>{&#39;+&#39;, &#39;-&#39;, &#39;*&#39;, &#39;/&#39;, &#39;(&#39;, &#39;)&#39;, &#39;{&#39;, &#39;}&#39;, &#39;[&#39;, &#39;]&#39;, &#39;;&#39;, &#39;,&#39;}</code>:<ul>\n<li>Identify <code>TokenType</code> (OPERATOR or PUNCTUATION).</li>\n<li>Call <code>_make_token(type)</code>.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Whitespace Handling</strong>:<ul>\n<li>If <code>char</code> in <code>{&#39; &#39;, &#39;\\r&#39;, &#39;\\t&#39;, &#39;\\n&#39;}</code>:<ul>\n<li>Return <code>None</code> (indicates no token emitted, loop continues).</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Error Recovery</strong>:<ul>\n<li>If no match:<ul>\n<li>Emit <code>Token(TokenType.ERROR, char, line, start_column)</code>.</li>\n<li>The <code>advance()</code> call in step 2 already moved the cursor, effectively &quot;skipping&quot; the invalid char.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"52-position-tracking-invariant\">5.2 Position Tracking Invariant</h3>\n<p>The scanner maintains a &quot;Human-Visible Coordinate&quot; system. </p>\n<ul>\n<li><strong>Rule 1</strong>: Newlines are the only characters that reset the column.</li>\n<li><strong>Rule 2</strong>: The <code>start_column</code> must be captured <em>before</em> the first <code>advance()</code> of a token to ensure the token position points to its first character.</li>\n</ul>\n<p>{{DIAGRAM:tdd-diag-2}}\n<em>(Sequence diagram showing: scan_tokens -&gt; _scan_token -&gt; advance -&gt; update line/col -&gt; _make_token)</em></p>\n<hr>\n<h2 id=\"6-error-handling-matrix\">6. Error Handling Matrix</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Error</th>\n<th align=\"left\">Detected By</th>\n<th align=\"left\">Recovery</th>\n<th align=\"left\">User-Visible?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Unrecognized Character</td>\n<td align=\"left\"><code>_scan_token</code> else-branch</td>\n<td align=\"left\">Emit <code>ERROR</code> token, continue to next char.</td>\n<td align=\"left\">Yes (as ERROR token)</td>\n</tr>\n<tr>\n<td align=\"left\">Unexpected EOF</td>\n<td align=\"left\"><code>is_at_end()</code> check</td>\n<td align=\"left\">Stop loop, emit <code>EOF</code> sentinel.</td>\n<td align=\"left\">No</td>\n</tr>\n<tr>\n<td align=\"left\">Tab/Newline Drift</td>\n<td align=\"left\"><code>advance()</code> logic</td>\n<td align=\"left\">Standardize increments in one method.</td>\n<td align=\"left\">Indirectly (wrong error positions)</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"7-implementation-sequence-with-checkpoints\">7. Implementation Sequence with Checkpoints</h2>\n<h3 id=\"phase-1-data-structures-1-hour\">Phase 1: Data Structures (1 Hour)</h3>\n<ol>\n<li>Define <code>TokenType</code> enum with M1-required variants.</li>\n<li>Define <code>Token</code> dataclass with <code>__repr__</code> for debugging.</li>\n<li><strong>Checkpoint</strong>: Instantiate a <code>Token</code> and print it. Ensure fields are accessible.</li>\n</ol>\n<h3 id=\"phase-2-primitive-cursor-1-hour\">Phase 2: Primitive Cursor (1 Hour)</h3>\n<ol>\n<li>Implement <code>Scanner.__init__</code> and <code>is_at_end()</code>.</li>\n<li>Implement <code>advance()</code> with line/column tracking logic.</li>\n<li>Implement <code>peek()</code>.</li>\n<li><strong>Checkpoint</strong>: Create a scanner for <code>&quot;A\\nB&quot;</code>. Call <code>advance()</code> three times. Verify <code>line</code> becomes 2 and <code>column</code> becomes 2.</li>\n</ol>\n<h3 id=\"phase-3-token-generation-1-hour\">Phase 3: Token Generation (1 Hour)</h3>\n<ol>\n<li>Implement <code>_make_token()</code> helper using string slicing <code>source[start:current]</code>.</li>\n<li>Implement <code>_scan_token()</code> with single-character dispatch.</li>\n<li>Implement <code>scan_tokens()</code> main loop.</li>\n<li><strong>Checkpoint</strong>: Scan <code>&quot;+-&quot;</code>. Verify output is <code>[Token(OPERATOR, &quot;+&quot;), Token(OPERATOR, &quot;-&quot;), Token(EOF, &quot;&quot;)]</code>.</li>\n</ol>\n<h3 id=\"phase-4-whitespace-amp-errors-05-hour\">Phase 4: Whitespace &amp; Errors (0.5 Hour)</h3>\n<ol>\n<li>Add whitespace cases to <code>_scan_token()</code> (return <code>None</code>).</li>\n<li>Add the <code>else</code> branch for <code>ERROR</code> tokens.</li>\n<li><strong>Checkpoint</strong>: Scan <code>&quot; + @ &quot;</code>. Verify list is <code>[Token(OPERATOR, &quot;+&quot;), Token(ERROR, &quot;@&quot;), Token(EOF, &quot;&quot;)]</code>.</li>\n</ol>\n<hr>\n<h2 id=\"8-test-specification\">8. Test Specification</h2>\n<h3 id=\"81-happy-path-single-character-symbols\">8.1 Happy Path: Single-Character Symbols</h3>\n<ul>\n<li><strong>Input</strong>: <code>( ) { } ; , + - * /</code></li>\n<li><strong>Expected</strong>: A list of 10 tokens + EOF. Lexemes must match inputs exactly. Line/Column must increment.</li>\n</ul>\n<h3 id=\"82-edge-case-multi-line-whitespace\">8.2 Edge Case: Multi-line Whitespace</h3>\n<ul>\n<li><strong>Input</strong>: <code>\\n  \\t  \\r</code></li>\n<li><strong>Expected</strong>: Exactly one token: <code>EOF</code>.</li>\n<li><strong>Verification</strong>: <code>tokens[0].line</code> should be 2. <code>tokens[0].column</code> should be accurate based on <code>tab_width</code>.</li>\n</ul>\n<h3 id=\"83-failure-case-invalid-characters\">8.3 Failure Case: Invalid Characters</h3>\n<ul>\n<li><strong>Input</strong>: <code>+ @ -</code></li>\n<li><strong>Expected</strong>: <code>[Token(OPERATOR, &quot;+&quot;), Token(ERROR, &quot;@&quot;), Token(OPERATOR, &quot;-&quot;), Token(EOF)]</code>.</li>\n<li><strong>Positioning</strong>: The error token must report column 3 (if space is col 2).</li>\n</ul>\n<h3 id=\"84-boundary-case-empty-file\">8.4 Boundary Case: Empty File</h3>\n<ul>\n<li><strong>Input</strong>: <code>&quot;&quot;</code></li>\n<li><strong>Expected</strong>: <code>[Token(EOF, &quot;&quot;, 1, 1)]</code>.</li>\n</ul>\n<hr>\n<h2 id=\"9-performance-targets\">9. Performance Targets</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Operation</th>\n<th align=\"left\">Target</th>\n<th align=\"left\">Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>advance()</code></td>\n<td align=\"left\">O(1)</td>\n<td align=\"left\">No loops or complex logic inside.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>scan_tokens()</code></td>\n<td align=\"left\">O(N)</td>\n<td align=\"left\">Single pass over string of length N.</td>\n</tr>\n<tr>\n<td align=\"left\">Memory Overhead</td>\n<td align=\"left\">&lt; 2x Source Size</td>\n<td align=\"left\">Tokens store references to source slices; metadata is minimal.</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"10-concurrency-amp-thread-safety\">10. Concurrency &amp; Thread Safety</h2>\n<p>The <code>Scanner</code> is <strong>not thread-safe</strong>. It is intended to be used as a short-lived, single-threaded object per source file. If multiple files need to be scanned, instantiate multiple <code>Scanner</code> objects.</p>\n<hr>\n<h2 id=\"11-dispatch-map-summary\">11. Dispatch Map Summary</h2>\n<p>For M1, the scanner uses a rigid mapping for the first character encountered:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Character</th>\n<th align=\"left\">TokenType</th>\n<th align=\"left\">Category</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>(</code></td>\n<td align=\"left\">PUNCTUATION</td>\n<td align=\"left\">Grouping</td>\n</tr>\n<tr>\n<td align=\"left\"><code>)</code></td>\n<td align=\"left\">PUNCTUATION</td>\n<td align=\"left\">Grouping</td>\n</tr>\n<tr>\n<td align=\"left\"><code>{</code></td>\n<td align=\"left\">PUNCTUATION</td>\n<td align=\"left\">Grouping</td>\n</tr>\n<tr>\n<td align=\"left\"><code>}</code></td>\n<td align=\"left\">PUNCTUATION</td>\n<td align=\"left\">Grouping</td>\n</tr>\n<tr>\n<td align=\"left\"><code>[</code></td>\n<td align=\"left\">PUNCTUATION</td>\n<td align=\"left\">Grouping</td>\n</tr>\n<tr>\n<td align=\"left\"><code>]</code></td>\n<td align=\"left\">PUNCTUATION</td>\n<td align=\"left\">Grouping</td>\n</tr>\n<tr>\n<td align=\"left\"><code>;</code></td>\n<td align=\"left\">PUNCTUATION</td>\n<td align=\"left\">Delimiter</td>\n</tr>\n<tr>\n<td align=\"left\"><code>,</code></td>\n<td align=\"left\">PUNCTUATION</td>\n<td align=\"left\">Delimiter</td>\n</tr>\n<tr>\n<td align=\"left\"><code>+</code></td>\n<td align=\"left\">OPERATOR</td>\n<td align=\"left\">Arithmetic</td>\n</tr>\n<tr>\n<td align=\"left\"><code>-</code></td>\n<td align=\"left\">OPERATOR</td>\n<td align=\"left\">Arithmetic</td>\n</tr>\n<tr>\n<td align=\"left\"><code>*</code></td>\n<td align=\"left\">OPERATOR</td>\n<td align=\"left\">Arithmetic</td>\n</tr>\n<tr>\n<td align=\"left\"><code>/</code></td>\n<td align=\"left\">OPERATOR</td>\n<td align=\"left\">Arithmetic</td>\n</tr>\n<tr>\n<td align=\"left\"><code> </code></td>\n<td align=\"left\"><code>None</code></td>\n<td align=\"left\">Skip</td>\n</tr>\n<tr>\n<td align=\"left\"><code>\\t</code></td>\n<td align=\"left\"><code>None</code></td>\n<td align=\"left\">Skip</td>\n</tr>\n<tr>\n<td align=\"left\"><code>\\r</code></td>\n<td align=\"left\"><code>None</code></td>\n<td align=\"left\">Skip</td>\n</tr>\n<tr>\n<td align=\"left\"><code>\\n</code></td>\n<td align=\"left\"><code>None</code></td>\n<td align=\"left\">Skip</td>\n</tr>\n</tbody></table>\n<p>Any character not in this table triggers a <code>TokenType.ERROR</code>.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Implementation Reference for the Dispatch Map</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">_SINGLE_CHAR_MAP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '('</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">')'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '{'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'}'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '['</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">']'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    ';'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">','</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">PUNCTUATION</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '+'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">'-'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    '*'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#9ECBFF\">'/'</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">OPERATOR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m2 -->\n<h1 id=\"module-specification-multi-character-tokens-amp-maximal-munch-tokenizer-m2\">MODULE SPECIFICATION: Multi-Character Tokens &amp; Maximal Munch (tokenizer-m2)</h1>\n<h2 id=\"1-module-charter\">1. Module Charter</h2>\n<p>The <strong>Multi-Character Tokens &amp; Maximal Munch</strong> module extends the foundational scanner to handle ambiguous lexical structures. It implements the &quot;Maximal Munch&quot; principle, ensuring the scanner always prefers the longest possible valid token (e.g., <code>==</code> over <code>=</code> followed by <code>=</code>). This module introduces lookahead primitives (<code>_match</code>, <code>_peek_next</code>) and specialized sub-scanners for complex lexemes like numeric literals (integers and floats) and identifiers. It also handles keyword recognition by post-processing scanned identifiers against a reserved hash table. By the end of this module, the scanner can process nearly all C-like expression syntax excluding strings and comments.</p>\n<p><strong>Invariants:</strong></p>\n<ul>\n<li>Every multi-character operator must be resolved using at most LA(1) (one character lookahead) via <code>_match()</code>.</li>\n<li>Numeric literals allow LA(2) specifically for disambiguating a trailing decimal point from a valid floating-point fractional part.</li>\n<li>Keywords are never recognized as partial identifiers; the full identifier must be scanned before keyword categorization occurs.</li>\n<li>The scanner state (<code>current</code>, <code>line</code>, <code>column</code>) must remain consistent even when lookahead fails to consume a character.</li>\n</ul>\n<hr>\n<h2 id=\"2-file-structure\">2. File Structure</h2>\n<p>The implementation follows this creation order, building upon the <code>tokenizer-m1</code> foundation:</p>\n<ol>\n<li><code>token_type.py</code>: Update existing <code>TokenType</code> Enum with new operator and keyword-related variants.</li>\n<li><code>scanner.py</code>: Add <code>_match</code>, <code>_peek_next</code>, <code>_scan_number</code>, and <code>_scan_identifier</code> methods. Update <code>_scan_token</code> dispatch logic.</li>\n<li><code>test_multi_char.py</code>: New integration tests for multi-character sequences, number literals, and keywords.</li>\n</ol>\n<hr>\n<h2 id=\"3-complete-data-model\">3. Complete Data Model</h2>\n<h3 id=\"31-updated-tokentype-enum\">3.1 Updated TokenType (Enum)</h3>\n<p>We expand the <code>TokenType</code> enumeration from M1. Note the specific distinction between <code>ASSIGN</code> (<code>=</code>) and <code>EQUALS</code> (<code>==</code>).</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Foundational (from M1)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\">      =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KEYWORD</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPERATOR</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># Used for +, -, *, /</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUNCTUATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\">         =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR</span><span style=\"color:#F97583\">       =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # New Multi-character / Ambiguous Operators (M2)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ASSIGN</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\">        =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ==</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NOT_EQUAL</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_THAN</span><span style=\"color:#F97583\">     =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LESS_EQUAL</span><span style=\"color:#F97583\">    =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># &#x3C;=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_THAN</span><span style=\"color:#F97583\">  =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># ></span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GREATER_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># >=</span></span></code></pre></div>\n\n<h3 id=\"32-keyword-lookup-table\">3.2 Keyword Lookup Table</h3>\n<p>To maintain O(1) average lookup performance and keep the dispatch logic clean, keywords are stored in a static dictionary.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Inside Scanner class or as a module-level constant</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">_KEYWORDS</span><span style=\"color:#E1E4E8\">: dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TokenType] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"if\"</span><span style=\"color:#E1E4E8\">:     TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"else\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"while\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"return\"</span><span style=\"color:#E1E4E8\">: TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"true\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"false\"</span><span style=\"color:#E1E4E8\">:  TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"null\"</span><span style=\"color:#E1E4E8\">:   TokenType.</span><span style=\"color:#79B8FF\">KEYWORD</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"33-scanner-internal-state-logic-view\">3.3 Scanner Internal State (Logic View)</h3>\n<p>While the <code>Token</code> struct remains unchanged from M1, the Scanner methods now operate with a &quot;lookahead budget&quot;:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Operation</th>\n<th align=\"left\">Lookahead Depth</th>\n<th align=\"left\">Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>_match(c)</code></td>\n<td align=\"left\">LA(1)</td>\n<td align=\"left\">Check if next char matches <code>c</code>; consume if yes.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>peek()</code></td>\n<td align=\"left\">LA(1)</td>\n<td align=\"left\">Inspect next char without consuming.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>_peek_next()</code></td>\n<td align=\"left\">LA(2)</td>\n<td align=\"left\">Inspect char after next (used for float dot check).</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-11.svg\" alt=\"Maximal Munch Principle â€” Greedy Token Selection\"></p>\n<p><em>(Diagram: Decision tree for &#39;&gt;&#39;, &#39;&gt;=&#39;, and &#39;&gt;==&#39;. Shows how LA(1) resolves the first two and how the main loop restarts to resolve the third as GTE + ASSIGN)</em></p>\n<hr>\n<h2 id=\"4-interface-contracts\">4. Interface Contracts</h2>\n<h3 id=\"41-_matchexpected-str-gt-bool\">4.1 <code>_match(expected: str) -&gt; bool</code></h3>\n<ul>\n<li><strong>Goal</strong>: Conditional consumption for maximal munch.</li>\n<li><strong>Parameters</strong>: <code>expected</code> (single-character string).</li>\n<li><strong>Logic</strong>: <ol>\n<li>If <code>is_at_end()</code>, return <code>False</code>.</li>\n<li>If <code>source[current] != expected</code>, return <code>False</code>.</li>\n<li>Call <code>advance()</code> and return <code>True</code>.</li>\n</ol>\n</li>\n<li><strong>Note</strong>: This is the atomic &quot;greedy step&quot; for operators.</li>\n</ul>\n<h3 id=\"42-_peek_next-gt-str\">4.2 <code>_peek_next() -&gt; str</code></h3>\n<ul>\n<li><strong>Goal</strong>: Look ahead two characters.</li>\n<li><strong>Logic</strong>: <ol>\n<li>If <code>current + 1 &gt;= len(source)</code>, return <code>&#39;&#39;</code>.</li>\n<li>Return <code>source[current + 1]</code>.</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"43-_scan_number-gt-token\">4.3 <code>_scan_number() -&gt; Token</code></h3>\n<ul>\n<li><strong>Precondition</strong>: <code>advance()</code> has already consumed the first digit.</li>\n<li><strong>Logic</strong>:<ol>\n<li>Loop <code>advance()</code> while <code>peek()</code> is a digit.</li>\n<li>Look ahead: If <code>peek() == &#39;.&#39;</code> AND <code>_peek_next()</code> is a digit:<ul>\n<li><code>advance()</code> to consume the dot.</li>\n<li>Loop <code>advance()</code> while <code>peek()</code> is a digit.</li>\n</ul>\n</li>\n<li>Return <code>_make_token(TokenType.NUMBER)</code>.</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"44-_scan_identifier-gt-token\">4.4 <code>_scan_identifier() -&gt; Token</code></h3>\n<ul>\n<li><strong>Precondition</strong>: <code>advance()</code> has already consumed a letter or underscore.</li>\n<li><strong>Logic</strong>:<ol>\n<li>Loop <code>advance()</code> while <code>peek()</code> is alphanumeric or <code>_</code>.</li>\n<li>Extract lexeme <code>text = source[start:current]</code>.</li>\n<li><code>type = _KEYWORDS.get(text, TokenType.IDENTIFIER)</code>.</li>\n<li>Return <code>_make_token(type)</code>.</li>\n</ol>\n</li>\n</ul>\n<hr>\n<h2 id=\"5-algorithm-specification\">5. Algorithm Specification</h2>\n<h3 id=\"51-maximal-munch-operator-dispatch\">5.1 Maximal Munch Operator Dispatch</h3>\n<p>The <code>_scan_token</code> method must be updated to handle multi-character operators <em>before</em> falling back to the single-character map.</p>\n<p><strong>Step-by-Step Dispatch:</strong></p>\n<ol>\n<li><strong>Read First Char</strong>: <code>char = advance()</code>.</li>\n<li><strong>Case <code>=</code></strong>: Return <code>EQUALS</code> if <code>_match(&#39;=&#39;)</code>, else <code>ASSIGN</code>.</li>\n<li><strong>Case <code>!</code></strong>: If <code>_match(&#39;=&#39;)</code>, return <code>NOT_EQUAL</code>. Else return <code>ERROR</code> (bare <code>!</code> is invalid).</li>\n<li><strong>Case <code>&lt;</code></strong>: Return <code>LESS_EQUAL</code> if <code>_match(&#39;=&#39;)</code>, else <code>LESS_THAN</code>.</li>\n<li><strong>Case <code>&gt;</code></strong>: Return <code>GREATER_EQUAL</code> if <code>_match(&#39;=&#39;)</code>, else <code>GREATER_THAN</code>.</li>\n<li><strong>Case Digit</strong>: Call <code>_scan_number()</code>.</li>\n<li><strong>Case Alpha/Underscore</strong>: Call <code>_scan_identifier()</code>.</li>\n</ol>\n<h3 id=\"52-floating-point-disambiguation-the-trailing-dot-guard\">5.2 Floating Point Disambiguation (The Trailing-Dot Guard)</h3>\n<p>The language grammar defines a float as <code>DIGIT+ &quot;.&quot; DIGIT+</code>. A trailing dot (e.g., <code>3.</code>) is invalid as a float.</p>\n<ol>\n<li>Current: <code>3.</code></li>\n<li>Scanner consumes <code>3</code>.</li>\n<li>Scanner peeks <code>.</code>.</li>\n<li>Scanner calls <code>_peek_next()</code>. It is not a digit (it is EOF or whitespace).</li>\n<li>Scanner <strong>stops</strong>. It emits <code>NUMBER(&quot;3&quot;)</code>.</li>\n<li>Main loop starts. Scanner reads <code>.</code>. It is not in the dispatch map.</li>\n<li>Scanner emits <code>ERROR(&quot;.&quot;)</code>.</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-12.svg\" alt=\"_match() Helper â€” Conditional Consume State Machine\"></p>\n<p><em>(State Machine for Numbers: States [START] -&gt; [INT] --(&#39;.&#39;) and peek_next(DIGIT)--&gt; [FLOAT] --(NON_DIGIT)--&gt; [ACCEPT])</em></p>\n<hr>\n<h2 id=\"6-error-handling-matrix\">6. Error Handling Matrix</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Error</th>\n<th align=\"left\">Detected By</th>\n<th align=\"left\">Recovery</th>\n<th align=\"left\">User-Visible?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Bare Exclamation</td>\n<td align=\"left\"><code>_scan_token</code> branch <code>!</code></td>\n<td align=\"left\">Emit <code>ERROR</code> for <code>!</code>, continue.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\">Trailing Dot</td>\n<td align=\"left\"><code>_scan_number</code> lookahead</td>\n<td align=\"left\">Emit <code>NUMBER</code> for digits, let dot fall to <code>ERROR</code>.</td>\n<td align=\"left\">Yes (as separate ERROR)</td>\n</tr>\n<tr>\n<td align=\"left\">Digit-prefixed Identifier</td>\n<td align=\"left\">Dispatch logic</td>\n<td align=\"left\">Emits <code>NUMBER</code> then <code>IDENTIFIER</code>.</td>\n<td align=\"left\">No (Standard lexer behavior)</td>\n</tr>\n<tr>\n<td align=\"left\">Invalid Start Char</td>\n<td align=\"left\"><code>_scan_token</code> else</td>\n<td align=\"left\">Emit <code>ERROR</code>, skip char.</td>\n<td align=\"left\">Yes</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"7-implementation-sequence-with-checkpoints\">7. Implementation Sequence with Checkpoints</h2>\n<h3 id=\"phase-1-enum-expansion-025-hours\">Phase 1: Enum Expansion (0.25 Hours)</h3>\n<ol>\n<li>Update <code>TokenType</code> in <code>token_type.py</code> with 7 new operator types.</li>\n<li><strong>Checkpoint</strong>: Verify <code>TokenType.GREATER_EQUAL</code> exists and is distinct.</li>\n</ol>\n<h3 id=\"phase-2-lookahead-helpers-05-hours\">Phase 2: Lookahead Helpers (0.5 Hours)</h3>\n<ol>\n<li>Implement <code>_match(expected)</code> in <code>Scanner</code>.</li>\n<li>Implement <code>_peek_next()</code> in <code>Scanner</code>.</li>\n<li><strong>Checkpoint</strong>: Test <code>_match</code> on a string <code>&quot;&gt;=&quot;</code>. Call <code>advance()</code>, then <code>_match(&#39;=&#39;)</code>. Verify it returns <code>True</code> and <code>current</code> is 2.</li>\n</ol>\n<h3 id=\"phase-3-operator-dispatch-1-hour\">Phase 3: Operator Dispatch (1 Hour)</h3>\n<ol>\n<li>Implement the <code>if/elif</code> branches for <code>=</code>, <code>!</code>, <code>&lt;</code>, <code>&gt;</code> in <code>_scan_token</code>.</li>\n<li><strong>Checkpoint</strong>: Scan <code>&quot;&gt;==&quot;</code>. Verify tokens are <code>GREATER_EQUAL</code> and <code>ASSIGN</code>.</li>\n</ol>\n<h3 id=\"phase-4-literals-amp-identifiers-1-hour\">Phase 4: Literals &amp; Identifiers (1 Hour)</h3>\n<ol>\n<li>Implement <code>_scan_number</code> with float lookahead.</li>\n<li>Implement <code>_scan_identifier</code> with keyword lookup.</li>\n<li><strong>Checkpoint</strong>: Scan <code>&quot;if 3.14 x&quot;</code>. Verify tokens are <code>KEYWORD(if)</code>, <code>NUMBER(3.14)</code>, <code>IDENTIFIER(x)</code>.</li>\n</ol>\n<hr>\n<h2 id=\"8-test-specification\">8. Test Specification</h2>\n<h3 id=\"81-happy-path-comparison-operators\">8.1 Happy Path: Comparison Operators</h3>\n<ul>\n<li><strong>Input</strong>: <code>== != &lt;= &gt;= &lt; &gt; =</code></li>\n<li><strong>Expected</strong>: <code>[EQUALS, NOT_EQUAL, LESS_EQUAL, GREATER_EQUAL, LESS_THAN, GREATER_THAN, ASSIGN, EOF]</code></li>\n<li><strong>Invariants</strong>: Ensure no token lexeme is truncated.</li>\n</ul>\n<h3 id=\"82-happy-path-numbers\">8.2 Happy Path: Numbers</h3>\n<ul>\n<li><strong>Input</strong>: <code>123 3.1415 0</code></li>\n<li><strong>Expected</strong>: <code>[NUMBER(&quot;123&quot;), NUMBER(&quot;3.1415&quot;), NUMBER(&quot;0&quot;), EOF]</code></li>\n</ul>\n<h3 id=\"83-happy-path-identifiers-amp-keywords\">8.3 Happy Path: Identifiers &amp; Keywords</h3>\n<ul>\n<li><strong>Input</strong>: <code>while iffy while_loop return true</code></li>\n<li><strong>Expected</strong>: <code>[KEYWORD(while), IDENTIFIER(iffy), IDENTIFIER(while_loop), KEYWORD(return), KEYWORD(true), EOF]</code></li>\n</ul>\n<h3 id=\"84-edge-case-maximal-munch-ambiguity\">8.4 Edge Case: Maximal Munch Ambiguity</h3>\n<ul>\n<li><strong>Input</strong>: <code>&gt;==</code></li>\n<li><strong>Expected</strong>: <code>[GREATER_EQUAL(&quot;&gt;=&quot;), ASSIGN(&quot;=&quot;), EOF]</code></li>\n<li><strong>Trace</strong>:<ul>\n<li><code>advance()</code> -&gt; <code>&gt;</code></li>\n<li><code>_match(&#39;=&#39;)</code> -&gt; True. Consume <code>=</code>. Lexeme <code>&quot;&gt;=&quot;</code>.</li>\n<li>Next loop: <code>advance()</code> -&gt; <code>=</code>. <code>_match(&#39;=&#39;)</code> -&gt; False. Lexeme <code>&quot;=&quot;</code>.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"85-failure-case-trailing-dot\">8.5 Failure Case: Trailing Dot</h3>\n<ul>\n<li><strong>Input</strong>: <code>42.</code></li>\n<li><strong>Expected</strong>: <code>[NUMBER(&quot;42&quot;), ERROR(&quot;.&quot;), EOF]</code></li>\n<li><strong>Reasoning</strong>: <code>_peek_next()</code> check prevents consumption of the dot.</li>\n</ul>\n<hr>\n<h2 id=\"9-performance-targets\">9. Performance Targets</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Operation</th>\n<th align=\"left\">Target</th>\n<th align=\"left\">Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Keyword Lookup</td>\n<td align=\"left\">O(1)</td>\n<td align=\"left\">Python dictionary <code>get()</code> is constant time average.</td>\n</tr>\n<tr>\n<td align=\"left\">Number Scanning</td>\n<td align=\"left\">O(K)</td>\n<td align=\"left\">Where K is number of digits. Single pass.</td>\n</tr>\n<tr>\n<td align=\"left\">Memory usage</td>\n<td align=\"left\">Minimal</td>\n<td align=\"left\">No temporary string buffers; uses integer offsets into source.</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"10-trace-table-identifier-vs-keyword\">10. Trace Table: Identifier vs Keyword</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Step</th>\n<th align=\"left\">Action</th>\n<th align=\"left\"><code>current</code></th>\n<th align=\"left\"><code>lexeme</code></th>\n<th align=\"left\">Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\"><code>advance()</code> -&gt; &#39;i&#39;</td>\n<td align=\"left\">1</td>\n<td align=\"left\">&quot;i&quot;</td>\n<td align=\"left\">Start Identifier</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\"><code>peek()</code> -&gt; &#39;f&#39; (isalnum)</td>\n<td align=\"left\">1</td>\n<td align=\"left\">&quot;i&quot;</td>\n<td align=\"left\">Continue</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\"><code>advance()</code> -&gt; &#39;f&#39;</td>\n<td align=\"left\">2</td>\n<td align=\"left\">&quot;if&quot;</td>\n<td align=\"left\">Continue</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\"><code>peek()</code> -&gt; &#39;f&#39; (isalnum)</td>\n<td align=\"left\">2</td>\n<td align=\"left\">&quot;if&quot;</td>\n<td align=\"left\">Continue</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\"><code>advance()</code> -&gt; &#39;f&#39;</td>\n<td align=\"left\">3</td>\n<td align=\"left\">&quot;iff&quot;</td>\n<td align=\"left\">Continue</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\"><code>peek()</code> -&gt; &#39; &#39; (not alnum)</td>\n<td align=\"left\">3</td>\n<td align=\"left\">&quot;iff&quot;</td>\n<td align=\"left\"><strong>Stop</strong></td>\n</tr>\n<tr>\n<td align=\"left\">7</td>\n<td align=\"left\"><code>_KEYWORDS.get(&quot;iff&quot;)</code></td>\n<td align=\"left\">3</td>\n<td align=\"left\">&quot;iff&quot;</td>\n<td align=\"left\">Not found -&gt; <code>IDENTIFIER</code></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"11-dispatch-logic-implementation-snippet\">11. Dispatch Logic (Implementation Snippet)</h2>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_token</span><span style=\"color:#E1E4E8\">(self) -> Token </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.start_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Ambiguous Operators</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '='</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">EQUALS</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">ASSIGN</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '!'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">NOT_EQUAL</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Token(TokenType.</span><span style=\"color:#79B8FF\">ERROR</span><span style=\"color:#E1E4E8\">, char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.start_column)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '&#x3C;'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">LESS_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">LESS_THAN</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '>'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">GREATER_EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._match(</span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">GREATER_THAN</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Multi-character Literals/Names</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char.isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_number()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> char.isalpha() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '_'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._scan_identifier()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # M1 Dispatch Fallback...</span></span></code></pre></div>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m3 -->\n<h1 id=\"module-specification-strings-amp-comments-tokenizer-m1\">MODULE SPECIFICATION: Strings &amp; Comments (tokenizer-m1)</h1>\n<h2 id=\"1-module-charter\">1. Module Charter</h2>\n<p>The <strong>Strings &amp; Comments</strong> module upgrades the scanner from a simple mapping-based dispatcher to a <strong>Context-Aware State Machine</strong>. This module is responsible for identifying and processing regions of source code where the standard lexical rules are suspended: string literals and comments. </p>\n<p><strong>Core Responsibilities:</strong></p>\n<ul>\n<li>Implement a three-way dispatch for the <code>/</code> character to distinguish between division, single-line comments (<code>//</code>), and multi-line block comments (<code>/* */</code>).</li>\n<li>Implement <code>_scan_string()</code> to recognize double-quoted literals, supporting internal escape sequences (e.g., <code>\\&quot;</code>, <code>\\n</code>) without terminating the string.</li>\n<li>Enforce <strong>Mode Isolation</strong>: ensuring that token-like sequences inside strings (like <code>//</code>) or comments (like <code>&quot;...&quot;</code>) do not trigger the scanner&#39;s standard dispatch logic.</li>\n<li>Maintain accurate line and column metadata through multi-line constructs by routing all internal consumption through the <code>advance()</code> primitive.</li>\n<li>Implement precise error reporting for unterminated constructs, pinning the error location to the opening delimiter (<code>&quot;</code> or <code>/*</code>) rather than the point of failure (EOF).</li>\n</ul>\n<p><strong>Out of Scope:</strong></p>\n<ul>\n<li>Processing escape sequences into their character values (e.g., converting <code>\\n</code> to a byte 10). This is deferred to the Evaluator/AST phase.</li>\n<li>Supporting nested block comments (Standard C-style behavior: non-nesting).</li>\n</ul>\n<p><strong>Invariants:</strong></p>\n<ul>\n<li>Comments produce NO tokens; they return <code>None</code> to the main loop and are discarded.</li>\n<li>String lexemes include the surrounding double quotes.</li>\n<li>The scanner must return to the <code>NORMAL</code> state (standard dispatch) immediately after a closing delimiter.</li>\n</ul>\n<hr>\n<h2 id=\"2-file-structure\">2. File Structure</h2>\n<p>Implementation proceeds by modifying existing foundation files and adding specific test suites:</p>\n<ol>\n<li><code>token_type.py</code>: No new types needed (uses existing <code>STRING</code>, <code>OPERATOR</code>, <code>ERROR</code>).</li>\n<li><code>scanner.py</code>: <ul>\n<li>Modification: Remove <code>/</code> from <code>_SINGLE_CHAR_TOKENS</code>.</li>\n<li>Addition: <code>_scan_string()</code>, <code>_skip_line_comment()</code>, <code>_skip_block_comment()</code>.</li>\n<li>Modification: Update <code>_scan_token()</code> dispatch logic.</li>\n</ul>\n</li>\n<li><code>test_strings_comments.py</code>: Comprehensive test suite for multi-line context and escape sequences.</li>\n</ol>\n<hr>\n<h2 id=\"3-complete-data-model\">3. Complete Data Model</h2>\n<h3 id=\"31-scanner-mode-transitions\">3.1 Scanner Mode Transitions</h3>\n<p>The scanner transitions from the <code>NORMAL</code> dispatch state into specialized loops. While these are not explicit variables, the instruction pointer residing in a specific sub-method constitutes the &quot;Mode&quot;.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Mode</th>\n<th align=\"left\">Trigger (in NORMAL)</th>\n<th align=\"left\">Exit Condition</th>\n<th align=\"left\">Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>NORMAL</strong></td>\n<td align=\"left\">â€”</td>\n<td align=\"left\">EOF</td>\n<td align=\"left\">Various Tokens</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>STRING</strong></td>\n<td align=\"left\"><code>&quot;</code></td>\n<td align=\"left\"><code>&quot;</code> or <code>\\n</code> or EOF</td>\n<td align=\"left\"><code>STRING</code> or <code>ERROR</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>LINE_COMMENT</strong></td>\n<td align=\"left\"><code>//</code></td>\n<td align=\"left\"><code>\\n</code> or EOF</td>\n<td align=\"left\"><code>None</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>BLOCK_COMMENT</strong></td>\n<td align=\"left\"><code>/*</code></td>\n<td align=\"left\"><code>*/</code> or EOF</td>\n<td align=\"left\"><code>None</code> or <code>ERROR</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"32-token-representation-for-m3\">3.2 Token Representation for M3</h3>\n<p>Strings are stored as raw lexemes.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example Lexeme Storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Source: \"Line1\\nLine2\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Token.lexeme -> '\"Line1\\\\nLine2\"' (Length 14)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Token.type   -> TokenType.STRING</span></span></code></pre></div>\n\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-20.svg\" alt=\"Scanner Mode State Machine â€” NORMAL / IN_STRING / IN_LINE_COMMENT / IN_BLOCK_COMMENT\"></p>\n<p><em>(State Machine Diagram: Shows NORMAL state branching into STRING, LINE_COMMENT, and BLOCK_COMMENT. Highlights that while in STRING or COMMENT modes, the standard dispatch transitions are disabled.)</em></p>\n<hr>\n<h2 id=\"4-interface-contracts\">4. Interface Contracts</h2>\n<h3 id=\"41-_skip_line_commentself-gt-none\">4.1 <code>_skip_line_comment(self) -&gt; None</code></h3>\n<ul>\n<li><strong>Logic</strong>: Consumes characters using <code>self.advance()</code> as long as <code>peek() != &#39;\\n&#39;</code> and <code>not is_at_end()</code>.</li>\n<li><strong>Note</strong>: Does NOT consume the <code>\\n</code>. The <code>\\n</code> is left for the <code>NORMAL</code> loop to handle as whitespace, ensuring line-counting consistency.</li>\n</ul>\n<h3 id=\"42-_skip_block_commentself-gt-token-none\">4.2 <code>_skip_block_comment(self) -&gt; Token | None</code></h3>\n<ul>\n<li><strong>Logic</strong>: Hunts for the <code>*/</code> sequence.</li>\n<li><strong>Success</strong>: Returns <code>None</code>.</li>\n<li><strong>Failure</strong>: If EOF is reached before <code>*/</code>, returns <code>Token(TokenType.ERROR, &quot;/*&quot;, self.line, self.start_column)</code>.</li>\n<li><strong>Invariant</strong>: Must handle internal <code>\\n</code> via <code>advance()</code> to prevent position drift.</li>\n</ul>\n<h3 id=\"43-_scan_stringself-gt-token\">4.3 <code>_scan_string(self) -&gt; Token</code></h3>\n<ul>\n<li><strong>Logic</strong>:<ol>\n<li>Loop while <code>peek() != &#39;&quot;&#39;</code> and <code>not is_at_end()</code>.</li>\n<li>If <code>peek() == &#39;\\n&#39;</code>: Terminate and return <code>ERROR</code> (Language Spec: No multi-line strings without escapes).</li>\n<li>If <code>peek() == &#39;\\\\&#39;</code> (Backslash):<ul>\n<li><code>advance()</code> (consumes <code>\\</code>).</li>\n<li>If <code>not is_at_end()</code>, <code>advance()</code> (consumes the escaped character, e.g., <code>&quot;</code> or <code>n</code>).</li>\n</ul>\n</li>\n<li>If loop exits due to <code>&quot;</code>, <code>advance()</code> the closing quote and return <code>STRING</code>.</li>\n<li>Else, return <code>ERROR</code>.</li>\n</ol>\n</li>\n</ul>\n<hr>\n<h2 id=\"5-algorithm-specification\">5. Algorithm Specification</h2>\n<h3 id=\"51-the-three-way-dispatch\">5.1 The Three-Way <code>/</code> Dispatch</h3>\n<p>The character <code>/</code> is the most ambiguous start-character in the C-family grammar. The implementation must follow this priority:</p>\n<ol>\n<li><strong>Consume</strong> <code>/</code>.</li>\n<li><strong>Match</strong> <code>/</code>: If successful, enter <code>_skip_line_comment()</code>.</li>\n<li><strong>Match</strong> <code>*</code>: If successful, enter <code>_skip_block_comment()</code>.</li>\n<li><strong>Else</strong>: Return <code>TokenType.OPERATOR</code> (Division).</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-21.svg\" alt=\"The '/' Ambiguity â€” Three-Way Dispatch\"></p>\n<p><em>(Decision Tree: Root &#39;/&#39; -&gt; Path A: &#39;/&#39; (Line Comm) -&gt; Path B: &#39;</em>&#39; (Block Comm) -&gt; Path C: Default (DIV Operator))*</p>\n<h3 id=\"52-escape-sequence-jump-over\">5.2 Escape Sequence Jump-Over</h3>\n<p>To correctly handle <code>&quot;</code> inside a string (e.g., <code>&quot;He said \\&quot;Hi\\&quot;&quot;</code>), the algorithm uses a &quot;Jump-Over&quot; strategy.</p>\n<p><strong>Step-by-Step Procedure:</strong></p>\n<ol>\n<li>Enter <code>_scan_string()</code>.</li>\n<li><code>peek()</code> sees <code>\\</code>.</li>\n<li>Call <code>advance()</code>: Cursor moves past <code>\\</code>.</li>\n<li>Check <code>is_at_end()</code>: If true, the loop will terminate and report an unterminated string.</li>\n<li>Call <code>advance()</code>: Cursor moves past the next character (e.g., <code>&quot;</code>).</li>\n<li><strong>Crucial Result</strong>: The closing quote check <code>peek() != &#39;&quot;&#39;</code> in the next loop iteration is bypassed because the cursor is now <em>past</em> the escaped quote.</li>\n</ol>\n<h3 id=\"53-error-position-pinning\">5.3 Error Position Pinning</h3>\n<p>Standard error reporting often points to where the error was <em>discovered</em> (e.g., EOF). For UX, we must point to where the construct <em>started</em>.</p>\n<p><strong>Algorithm:</strong></p>\n<ol>\n<li>In <code>_scan_token()</code>, snapshot <code>self.start_column = self.column</code> and <code>self.start_line = self.line</code>.</li>\n<li>Enter sub-scanner (e.g., <code>_scan_string</code>).</li>\n<li>On error, instantiate <code>Token</code> using the snapshotted <code>self.start_line</code> and <code>self.start_column</code>.</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-22.svg\" alt=\"_scan_string() State Machine â€” Escape and Termination Logic\"></p>\n<p><em>(Memory/Pointer Snapshot: Shows <code>start_column</code> pointing to the opening <code>&quot;</code> at index 10, while <code>current</code> is at index 500 (EOF). The resulting ERROR token uses index 10 coordinates.)</em></p>\n<hr>\n<h2 id=\"6-error-handling-matrix\">6. Error Handling Matrix</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Error</th>\n<th align=\"left\">Detected By</th>\n<th align=\"left\">Recovery</th>\n<th align=\"left\">User-Visible?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Unterminated String (EOF)</td>\n<td align=\"left\"><code>_scan_string</code> while-loop</td>\n<td align=\"left\">Emit <code>ERROR</code> at start position, stop.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\">Unterminated String (Newline)</td>\n<td align=\"left\"><code>_scan_string</code> <code>peek() == &#39;\\n&#39;</code></td>\n<td align=\"left\">Emit <code>ERROR</code> at start position, resume NORMAL.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\">Backslash at EOF</td>\n<td align=\"left\"><code>_scan_string</code> escape check</td>\n<td align=\"left\">Emit <code>ERROR</code>, stop.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\">Unterminated Block Comment</td>\n<td align=\"left\"><code>_skip_block_comment</code> loop</td>\n<td align=\"left\">Emit <code>ERROR</code> at <code>/*</code> position, stop.</td>\n<td align=\"left\">Yes</td>\n</tr>\n<tr>\n<td align=\"left\">Nested Block Comment</td>\n<td align=\"left\">N/A (By Design)</td>\n<td align=\"left\">First <code>*/</code> closes; remainder is scanned as code.</td>\n<td align=\"left\">Yes (likely Parse Error)</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"7-implementation-sequence-with-checkpoints\">7. Implementation Sequence with Checkpoints</h2>\n<h3 id=\"phase-1-dispatch-overhaul-05-hours\">Phase 1: Dispatch Overhaul (0.5 Hours)</h3>\n<ol>\n<li>Remove <code>&#39;/&#39;</code> from the <code>_SINGLE_CHAR_MAP</code>.</li>\n<li>Update <code>_scan_token</code> to handle <code>/</code> with <code>_match</code> logic.</li>\n<li><strong>Checkpoint</strong>: Scanning <code>1 / 2</code> should still produce <code>NUMBER, OPERATOR, NUMBER</code>. Scanning <code>1 // 2</code> should produce <code>NUMBER, EOF</code>.</li>\n</ol>\n<h3 id=\"phase-2-comment-logic-1-hour\">Phase 2: Comment Logic (1 Hour)</h3>\n<ol>\n<li>Implement <code>_skip_line_comment</code>.</li>\n<li>Implement <code>_skip_block_comment</code> with <code>*/</code> detection.</li>\n<li>Ensure block comments update <code>self.line</code> via <code>advance()</code>.</li>\n<li><strong>Checkpoint</strong>: Scan <code>/* \\n */ +</code>. Verify <code>+</code> is on <code>line: 2</code>.</li>\n</ol>\n<h3 id=\"phase-3-string-scanner-15-hours\">Phase 3: String Scanner (1.5 Hours)</h3>\n<ol>\n<li>Implement <code>_scan_string</code> with simple quote matching.</li>\n<li>Add escape sequence handling (backslash + next char consumption).</li>\n<li>Add <code>\\n</code> error check.</li>\n<li><strong>Checkpoint</strong>: Scan <code>&quot;a\\&quot;b&quot;</code>. Verify lexeme is <code>&quot;a\\&quot;b&quot;</code> (length 5).</li>\n</ol>\n<h3 id=\"phase-4-integration-amp-multi-line-1-hour\">Phase 4: Integration &amp; Multi-line (1 Hour)</h3>\n<ol>\n<li>Verify interaction: Strings containing <code>//</code>, Comments containing <code>&quot;</code>.</li>\n<li>Run position-drift tests on large multi-line comment blocks.</li>\n<li><strong>Checkpoint</strong>: Scan code where a string follows a block comment. Verify column reset.</li>\n</ol>\n<hr>\n<h2 id=\"8-test-specification\">8. Test Specification</h2>\n<h3 id=\"81-string-escape-isolation\">8.1 String Escape Isolation</h3>\n<ul>\n<li><strong>Input</strong>: <code>&quot;line1\\nline2\\&quot;quote\\&quot;&quot;</code></li>\n<li><strong>Expected</strong>: <code>[TokenType.STRING]</code></li>\n<li><strong>Lexeme</strong>: Exact match of input including quotes.</li>\n<li><strong>Isolation</strong>: Ensure <code>\\n</code> inside the string is not caught by the <code>_scan_token</code> newline handler.</li>\n</ul>\n<h3 id=\"82-block-comment-boundaries\">8.2 Block Comment Boundaries</h3>\n<ul>\n<li><strong>Input</strong>: <code>/* *** */</code></li>\n<li><strong>Expected</strong>: <code>[]</code> (No tokens).</li>\n<li><strong>Internal</strong>: Test <code>/* / */</code> (should close) vs <code>/* * / */</code> (should close).</li>\n</ul>\n<h3 id=\"83-the-quotfalse-commentquot-test\">8.3 The &quot;False Comment&quot; Test</h3>\n<ul>\n<li><strong>Input</strong>: <code>&quot;http://example.com&quot;</code></li>\n<li><strong>Expected</strong>: <code>[TokenType.STRING]</code></li>\n<li><strong>Verification</strong>: If it produces <code>STRING(&quot;http:&quot;)</code> followed by a comment, Mode Isolation has failed.</li>\n</ul>\n<h3 id=\"84-unterminated-errors\">8.4 Unterminated Errors</h3>\n<ul>\n<li><strong>Input</strong>: <code>/* unterminated ...</code></li>\n<li><strong>Expected</strong>: <code>[TokenType.ERROR]</code></li>\n<li><strong>Requirement</strong>: <code>token.line</code> and <code>token.column</code> must point to the start of <code>/*</code>.</li>\n</ul>\n<hr>\n<h2 id=\"9-performance-targets\">9. Performance Targets</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Operation</th>\n<th align=\"left\">Target</th>\n<th align=\"left\">Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">String Scanning</td>\n<td align=\"left\">O(N)</td>\n<td align=\"left\">Single pass, no nested loops, no string building.</td>\n</tr>\n<tr>\n<td align=\"left\">Comment Skipping</td>\n<td align=\"left\">O(N)</td>\n<td align=\"left\">Single pass, standard <code>advance()</code> overhead.</td>\n</tr>\n<tr>\n<td align=\"left\">Memory</td>\n<td align=\"left\">Zero Allocation</td>\n<td align=\"left\">Uses <code>source</code> slicing only at the very end of <code>_scan_string</code>.</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"10-concurrency-specification\">10. Concurrency Specification</h2>\n<ul>\n<li><strong>Thread Safety</strong>: The Scanner remains <strong>not thread-safe</strong>.</li>\n<li><strong>Context Handling</strong>: Method-local loops for strings/comments ensure that &quot;Mode&quot; is handled via the call stack. This prevents complex state-flag management and minimizes bug surface area.</li>\n</ul>\n<hr>\n<h2 id=\"11-mode-isolation-logic-implementation-snippet\">11. Mode Isolation Logic (Implementation Snippet)</h2>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_string</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # We are now in STRING mode. Standard dispatch rules DO NOT APPLY.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._err_at_start(</span><span style=\"color:#9ECBFF\">\"Unterminated string (newline)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance() </span><span style=\"color:#6A737D\"># Skip backslash</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance() </span><span style=\"color:#6A737D\"># Skip escaped char (even if it is '\"')</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_at_end():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._err_at_start(</span><span style=\"color:#9ECBFF\">\"Unterminated string (EOF)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.advance() </span><span style=\"color:#6A737D\"># The closing quote</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._make_token(TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<hr>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m4 -->\n<h1 id=\"module-specification-integration-testing-amp-error-recovery-tokenizer-m4\">MODULE SPECIFICATION: Integration Testing &amp; Error Recovery (tokenizer-m4)</h1>\n<h2 id=\"1-module-charter\">1. Module Charter</h2>\n<p>The <strong>Integration Testing &amp; Error Recovery</strong> module is the final validation phase of the tokenizer. Its primary purpose is to ensure that the individual scanning logic components from Milestones 1-3 function correctly in aggregate when processing complex, multi-line source files. This module does not introduce new lexical features; instead, it formalizes the &quot;Skip-One&quot; error recovery mechanism and subjects the scanner to rigorous boundary testing and performance benchmarking. It verifies that position tracking (line/column) remains accurate without drift over long inputs and that the scanner maintains $O(N)$ time complexity, specifically meeting the target of 10,000 lines per second. By the end of this module, the tokenizer is considered &quot;Production-Ready&quot; for consumption by a downstream parser.</p>\n<p><strong>Core Responsibilities:</strong></p>\n<ul>\n<li>Implement &quot;Golden Path&quot; integration tests for complete C-like programs.</li>\n<li>Validate the implicit &quot;Skip-One&quot; error recovery strategy to ensure no valid tokens are swallowed after an error.</li>\n<li>Verify multi-error collection in a single pass (errors as values in the token stream).</li>\n<li>Detect and prevent &quot;Position Drift&quot; in line/column tracking across multi-line constructs (strings/comments).</li>\n<li>Benchmark throughput to ensure no $O(N^2)$ algorithmic bottlenecks exist in the scanner&#39;s inner loops.</li>\n</ul>\n<p><strong>Invariants:</strong></p>\n<ul>\n<li>Every input, regardless of validity, must result in a <code>list[Token]</code> ending in <code>TokenType.EOF</code>.</li>\n<li>Error recovery must never consume more than one character per <code>ERROR</code> token emitted.</li>\n<li>The <code>column</code> counter must reset to <code>1</code> exactly after a <code>\\n</code> character is consumed, regardless of context (Normal, String, or Comment).</li>\n</ul>\n<hr>\n<h2 id=\"2-file-structure\">2. File Structure</h2>\n<p>The integration and testing suite shall be organized as follows:</p>\n<ol>\n<li><code>tests/integration_harness.py</code>: Helper utilities for asserting token stream equality.</li>\n<li><code>tests/test_integration.py</code>: &quot;Golden Path&quot; and multi-error tests.</li>\n<li><code>tests/test_position.py</code>: Specific tests for line/column drift and CRLF handling.</li>\n<li><code>tests/test_boundaries.py</code>: Edge cases for empty files, max-length identifiers, etc.</li>\n<li><code>benchmarks/perf_benchmark.py</code>: Throughput tests for 10k lines and 10k strings.</li>\n</ol>\n<hr>\n<h2 id=\"3-complete-data-model\">3. Complete Data Model</h2>\n<p>While the core <code>Token</code> and <code>Scanner</code> classes were defined in M1-M3, M4 introduces a specific &quot;Expectation Format&quot; used for high-density testing.</p>\n<h3 id=\"31-test-expectation-schema\">3.1 Test Expectation Schema</h3>\n<p>To avoid verbose object instantiation in tests, we use a tuple-based comparison format.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, List, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> token_type </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># An Expectation is a tuple of (Type, Lexeme, [Optional: Line, Optional: Column])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">TokenExpectation </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Union[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tuple[TokenType, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">],           </span><span style=\"color:#6A737D\"># Basic check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tuple[TokenType, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># Full positional check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<h3 id=\"32-position-metadata-mapping\">3.2 Position Metadata Mapping</h3>\n<p>The scanner must maintain absolute synchronization between the byte-offset <code>current</code> and the human-readable <code>(line, column)</code>.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Property</th>\n<th align=\"left\">Mapped From</th>\n<th align=\"left\">Constraint</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>line</code></td>\n<td align=\"left\">Number of <code>\\n</code> seen + 1</td>\n<td align=\"left\">Must increment inside <code>_scan_string</code> and <code>_skip_block_comment</code>.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>column</code></td>\n<td align=\"left\">Chars since last <code>\\n</code></td>\n<td align=\"left\">Must reset to 1 immediately after <code>\\n</code>.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>start_column</code></td>\n<td align=\"left\">Snapshot of <code>column</code></td>\n<td align=\"left\">Must be taken BEFORE the first <code>advance()</code> of a lexeme.</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-30.svg\" alt=\"Skip-One Error Recovery â€” Mechanism and Invariant\"></p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-30.svg\" alt=\"Skip-One Error Recovery â€” Mechanism and Invariant\"></p>\n<hr>\n<h2 id=\"4-interface-contracts\">4. Interface Contracts</h2>\n<h3 id=\"41-integration_harnessassert_tokensactual-listtoken-expected-listtokenexpectation\">4.1 <code>integration_harness.assert_tokens(actual: List[Token], expected: List[TokenExpectation])</code></h3>\n<ul>\n<li><strong>Goal</strong>: Provide deep-diffing for token streams.</li>\n<li><strong>Logic</strong>: <ul>\n<li>Iterates through both lists. </li>\n<li>Compares <code>TokenType</code> and <code>lexeme</code>. </li>\n<li>If <code>expected</code> tuple has 4 elements, also compares <code>line</code> and <code>column</code>.</li>\n</ul>\n</li>\n<li><strong>Error Reporting</strong>: Must raise <code>AssertionError</code> with a detailed message showing the index of the first mismatching token and the surrounding context.</li>\n</ul>\n<h3 id=\"42-perf_benchmarkgenerate_synthetic_programline_count-int-gt-str\">4.2 <code>perf_benchmark.generate_synthetic_program(line_count: int) -&gt; str</code></h3>\n<ul>\n<li><strong>Goal</strong>: Generate a deterministic, large-scale test file.</li>\n<li><strong>Logic</strong>: Repeats a set of 10 patterns (if-statements, assignments, comments) to reach the target line count.</li>\n</ul>\n<h3 id=\"43-scannerscan_tokens-re-verification\">4.3 <code>Scanner.scan_tokens</code> (Re-verification)</h3>\n<ul>\n<li><strong>Error Recovery Path</strong>: When <code>_scan_token</code> enters the <code>else</code> branch (unrecognized character):<ul>\n<li>It captures the character into an <code>ERROR</code> token.</li>\n<li>It returns that token.</li>\n<li>The main loop appends it and calls <code>_scan_token</code> again.</li>\n</ul>\n</li>\n<li><strong>Recovery Invariant</strong>: Ensure no recursive calls or &quot;skip-to-newline&quot; behavior exists unless explicitly added.</li>\n</ul>\n<hr>\n<h2 id=\"5-algorithm-specification-error-recovery-amp-benchmarking\">5. Algorithm Specification: Error Recovery &amp; Benchmarking</h2>\n<h3 id=\"51-skip-one-recovery-trace\">5.1 Skip-One Recovery Trace</h3>\n<p>The &quot;Skip-One&quot; strategy is the most robust for lexical analysis because lexical errors are usually local (a stray character).</p>\n<p><strong>Procedure for Input <code>@if</code></strong>:</p>\n<ol>\n<li><code>_scan_token()</code> starts. <code>start=0</code>, <code>start_column=1</code>.</li>\n<li><code>advance()</code> returns <code>@</code>. <code>current=1</code>, <code>column=2</code>.</li>\n<li>Dispatch hits <code>else</code> branch.</li>\n<li>Returns <code>Token(ERROR, &quot;@&quot;, line=1, col=1)</code>.</li>\n<li><code>scan_tokens()</code> appends error.</li>\n<li><code>_scan_token()</code> starts again. <code>start=1</code>, <code>start_column=2</code>.</li>\n<li><code>advance()</code> returns <code>i</code>. <code>current=2</code>, <code>column=3</code>.</li>\n<li><code>_scan_identifier()</code> consumes <code>f</code>.</li>\n<li>Returns <code>Token(KEYWORD, &quot;if&quot;, line=1, col=2)</code>.</li>\n</ol>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-31.svg\" alt=\"Multi-Error Collection â€” '@#$%' All Reported\"></p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-31.svg\" alt=\"Multi-Error Collection â€” '@#$%' All Reported\"></p>\n<h3 id=\"52-performance-avoiding-quadratic-slicing\">5.2 Performance: Avoiding Quadratic Slicing</h3>\n<p>In Python, string slicing <code>source[start:current]</code> is $O(K)$ where $K$ is the length of the slice. If a user tokenizes a 1MB file containing a single string literal, a naive implementation that slices <em>inside</em> the loop would become $O(N^2)$.</p>\n<p><strong>Correct Approach (M4 Requirement)</strong>:</p>\n<ul>\n<li>All sub-scanners (<code>_scan_number</code>, <code>_scan_string</code>, <code>_scan_identifier</code>) must only slice the source string <strong>once</strong> at the end of their loop.</li>\n<li><strong>Benchmark Test</strong>: A 10,000 character string literal must be scanned in near-constant time relative to its length (linear).</li>\n</ul>\n<hr>\n<h2 id=\"6-error-handling-matrix-integration-focus\">6. Error Handling Matrix (Integration Focus)</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Failure Mode</th>\n<th align=\"left\">Detection</th>\n<th align=\"left\">Recovery</th>\n<th align=\"left\">User Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Position Drift</strong></td>\n<td align=\"left\"><code>test_position_fifty_line_program</code></td>\n<td align=\"left\">Fix <code>advance()</code> to handle <code>\\n</code> globally.</td>\n<td align=\"left\">Wrong error locations in large files.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Over-Consumption</strong></td>\n<td align=\"left\"><code>test_error_does_not_consume_next_char</code></td>\n<td align=\"left\">Ensure <code>ERROR</code> branch doesn&#39;t call <code>advance()</code> twice.</td>\n<td align=\"left\">Missing valid code after an error.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Infinite Loop</strong></td>\n<td align=\"left\"><code>test_empty_input</code> / <code>is_at_end</code></td>\n<td align=\"left\">Ensure <code>advance()</code> always moves <code>current</code>.</td>\n<td align=\"left\">Compiler hangs.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Memory Exhaustion</strong></td>\n<td align=\"left\"><code>test_max_length_identifier</code></td>\n<td align=\"left\">Rely on Python heap (approx 100MB for 1M tokens).</td>\n<td align=\"left\">Crash on massive files.</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"7-implementation-sequence-with-checkpoints\">7. Implementation Sequence with Checkpoints</h2>\n<h3 id=\"phase-1-canonical-integration-1-hour\">Phase 1: Canonical Integration (1 Hour)</h3>\n<ol>\n<li>Implement the <code>assert_tokens</code> harness.</li>\n<li>Create <code>COMPLETE_TEST_PROGRAM</code> containing every feature from M1-M3.</li>\n<li>Run <code>scan_tokens()</code> and compare against the master expectation list.</li>\n<li><strong>Checkpoint</strong>: <code>test_canonical_expression</code> passes with 12 specific tokens.</li>\n</ol>\n<h3 id=\"phase-2-error-recovery-validation-1-hour\">Phase 2: Error Recovery Validation (1 Hour)</h3>\n<ol>\n<li>Write tests for <code>@#$%</code> (consecutive errors).</li>\n<li>Write tests for <code>x @ y</code> (interleaved errors).</li>\n<li>Write <code>test_error_does_not_consume_next_valid_char</code> for <code>@if</code>.</li>\n<li><strong>Checkpoint</strong>: Scanner correctly identifies <code>if</code> as a keyword even when preceded by a junk character.</li>\n</ol>\n<h3 id=\"phase-3-position-tracking-amp-crlf-1-hour\">Phase 3: Position Tracking &amp; CRLF (1 Hour)</h3>\n<ol>\n<li>Implement <code>test_windows_line_endings_not_double_counted</code>.</li>\n<li>Write a 100-line test where every line is a single identifier <code>x1</code> ... <code>x100</code>. Verify <code>token.line == i</code>.</li>\n<li><strong>Checkpoint</strong>: Zero drift detected over 100 lines.</li>\n</ol>\n<h3 id=\"phase-4-edge-cases-1-hour\">Phase 4: Edge Cases (1 Hour)</h3>\n<ol>\n<li><code>test_empty_input</code>: Verify only <code>EOF</code> at (1,1).</li>\n<li><code>test_maximum_length_identifier</code>: 10k character name.</li>\n<li><code>test_keyword_prefix_identifiers</code>: <code>iffy</code>, <code>returning</code>.</li>\n<li><strong>Checkpoint</strong>: All boundary tests pass.</li>\n</ol>\n<h3 id=\"phase-5-performance-benchmarking-1-hour\">Phase 5: Performance Benchmarking (1 Hour)</h3>\n<ol>\n<li>Implement <code>perf_benchmark.py</code>.</li>\n<li>Generate 10k line file.</li>\n<li>Measure <code>time.perf_counter()</code> for <code>scan_tokens()</code>.</li>\n<li><strong>Checkpoint</strong>: Execution time &lt; 1.0s. If slower, profile for $O(N^2)$ slicing.</li>\n</ol>\n<hr>\n<h2 id=\"8-test-specification-high-precision\">8. Test Specification (High Precision)</h2>\n<h3 id=\"81-happy-path-the-quotgolden-programquot\">8.1 Happy Path: The &quot;Golden Program&quot;</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">if (count >= 10) {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print(\"Success\"); // comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected (Excerpt):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># [</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#   (TokenType.KEYWORD, \"if\", 2, 1),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#   (TokenType.PUNCTUATION, \"(\", 2, 4),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#   (TokenType.IDENTIFIER, \"count\", 2, 5),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#   ...</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ]</span></span></code></pre></div>\n\n<h3 id=\"82-edge-case-max-length-identifier\">8.2 Edge Case: Max-Length Identifier</h3>\n<ul>\n<li><strong>Input</strong>: <code>&quot;a&quot; * 1_000_000</code></li>\n<li><strong>Target</strong>: Ensure Python handles the 1MB string and slice without <code>RecursionError</code> or timeout.</li>\n<li><strong>Verification</strong>: <code>tokens[0].lexeme == source</code>.</li>\n</ul>\n<h3 id=\"83-failure-case-multi-line-comment-unterminated-at-eof\">8.3 Failure Case: Multi-line Comment Unterminated at EOF</h3>\n<ul>\n<li><strong>Input</strong>: <code>/* line 1\\n line 2</code></li>\n<li><strong>Expectation</strong>: <code>Token(ERROR, &quot;/*&quot;, 1, 1)</code> followed by <code>EOF</code>.</li>\n<li><strong>Note</strong>: The error must be reported on line 1, not line 2.</li>\n</ul>\n<h3 id=\"84-boundary-case-whitespace-only-file\">8.4 Boundary Case: Whitespace-only file</h3>\n<ul>\n<li><strong>Input</strong>: <code>&quot;   \\n   \\t  &quot;</code></li>\n<li><strong>Expectation</strong>: <code>[Token(EOF, &quot;&quot;, 2, 6)]</code>.</li>\n</ul>\n<hr>\n<h2 id=\"9-performance-targets\">9. Performance Targets</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Metric</th>\n<th align=\"left\">Target</th>\n<th align=\"left\">measurement_method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Throughput</strong></td>\n<td align=\"left\">&gt; 10,000 lines / sec</td>\n<td align=\"left\"><code>perf_counter()</code> over <code>generate_synthetic_program(10000)</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Tokenization Speed</strong></td>\n<td align=\"left\">&lt; 100ns / character</td>\n<td align=\"left\">Total time / source length (CPython 3.10)</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Recovery Overhead</strong></td>\n<td align=\"left\">&lt; 1.5x Normal</td>\n<td align=\"left\">Compare time for 10k valid lines vs 10k error lines.</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"10-concurrency-specification\">10. Concurrency Specification</h2>\n<p>While the <code>Scanner</code> is single-threaded, the testing harness should be compatible with <code>pytest-xdist</code> for parallel execution.</p>\n<ul>\n<li><strong>Statelessness</strong>: Every test case must instantiate its own <code>Scanner</code>. </li>\n<li><strong>No Shared Buffers</strong>: Do not use global variables for the keyword table or single-character map if they are mutable.</li>\n</ul>\n<hr>\n<h2 id=\"11-implementation-detail-position-drift-test\">11. Implementation Detail: Position Drift Test</h2>\n<p>This logic is mandatory for Phase 3:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_position_drift_validation</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Construct source: 100 lines of \"x;\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"x;</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Scanner(source)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scanner.scan_tokens()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # We expect 2 tokens per line (ID, PUNCT) + EOF</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Total tokens = 201</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(tokens) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 201</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> line_idx </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[line_idx </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">]     </span><span style=\"color:#6A737D\"># 'x'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens[line_idx </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#6A737D\"># ';'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line_idx </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> t1.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> t1.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> t2.line </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> t2.column </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span></code></pre></div>\n\n\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-32.svg\" alt=\"Golden Test Pattern â€” Token Stream Verification Structure\"></p>\n<p><img src=\"/api/project/tokenizer/architecture-doc/asset?path=diagrams%2Ftdd-diag-32.svg\" alt=\"Golden Test Pattern â€” Token Stream Verification Structure\"></p>\n<hr>\n<h2 id=\"12-skip-one-recovery-vs-multi-character-logic\">12. Skip-One Recovery vs. Multi-Character Logic</h2>\n<p>One critical integration risk is that a multi-character scanner (like <code>_scan_number</code>) might fail halfway and leave the <code>current</code> pointer in an invalid state.</p>\n<p><strong>Validation Criteria</strong>:</p>\n<ul>\n<li>If <code>3.a</code> is scanned:<ul>\n<li><code>_scan_number</code> sees <code>3</code>.</li>\n<li><code>peek()</code> sees <code>.</code>. <code>_peek_next()</code> sees <code>a</code>. </li>\n<li><code>_scan_number</code> stops. Emits <code>NUMBER(&quot;3&quot;)</code>.</li>\n<li><code>_scan_token</code> restarts. Sees <code>.</code>. Emits <code>ERROR(&quot;.&quot;)</code>.</li>\n<li><code>_scan_token</code> restarts. Sees <code>a</code>. Emits <code>IDENTIFIER(&quot;a&quot;)</code>.</li>\n</ul>\n</li>\n<li><strong>Test Case</strong>: <code>test_recovery_from_malformed_float</code>.</li>\n</ul>\n<hr>\n<h2 id=\"13-comprehensive-canonical-test-program\">13. Comprehensive Canonical Test Program</h2>\n<p>The following source must be used for Phase 1 integration verification:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># canonical_source.txt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">/*</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  Multi</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">line block</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">*/</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> (x </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 42.0</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    //</span><span style=\"color:#E1E4E8\"> Single line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"Result: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> x;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">} </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> null;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Required Token Sequence (Condensed)</strong>:</p>\n<ol>\n<li><code>KEYWORD(&quot;if&quot;)</code> @ 5:1</li>\n<li><code>PUNCTUATION(&quot;(&quot;)</code> @ 5:4</li>\n<li><code>IDENTIFIER(&quot;x&quot;)</code> @ 5:5</li>\n<li><code>GREATER_EQUAL(&quot;&gt;=&quot;)</code> @ 5:7</li>\n<li><code>NUMBER(&quot;42.0&quot;)</code> @ 5:10</li>\n<li><code>PUNCTUATION(&quot;)&quot;)</code> @ 5:14</li>\n<li><code>PUNCTUATION(&quot;{&quot;)</code> @ 5:16</li>\n<li><code>KEYWORD(&quot;return&quot;)</code> @ 7:5</li>\n<li><code>STRING(&quot;\\&quot;Result: \\&quot;&quot;)</code> @ 7:12</li>\n<li><code>OPERATOR(&quot;+&quot;)</code> @ 7:23</li>\n<li><code>IDENTIFIER(&quot;x&quot;)</code> @ 7:25</li>\n<li><code>PUNCTUATION(&quot;;&quot;)</code> @ 7:26</li>\n<li><code>PUNCTUATION(&quot;}&quot;)</code> @ 8:1</li>\n<li><code>KEYWORD(&quot;else&quot;)</code> @ 8:3</li>\n<li><code>PUNCTUATION(&quot;{&quot;)</code> @ 8:8</li>\n<li><code>KEYWORD(&quot;return&quot;)</code> @ 9:5</li>\n<li><code>KEYWORD(&quot;null&quot;)</code> @ 9:12</li>\n<li><code>PUNCTUATION(&quot;;&quot;)</code> @ 9:16</li>\n<li><code>PUNCTUATION(&quot;}&quot;)</code> @ 10:1</li>\n<li><code>EOF(&quot;&quot;)</code> @ 11:1</li>\n</ol>\n<hr>\n<h2 id=\"14-performance-hotspot-check-the-__repr__-trap\">14. Performance Hotspot Check: The <code>__repr__</code> Trap</h2>\n<p>In Python, generating string representations for thousands of tokens can be slower than the scanning itself.</p>\n<ul>\n<li><strong>Guidance</strong>: Tests should compare <code>token.type</code> and <code>token.lexeme</code> directly rather than comparing <code>repr(token_list)</code>.</li>\n<li><strong>Target</strong>: Benchmarks must measure only <code>scan_tokens()</code>, excluding the time spent printing or verifying results.</li>\n</ul>\n<!-- END_TDD_MOD -->\n\n\n<h1 id=\"project-structure-tokenizer-lexer\">Project Structure: Tokenizer / Lexer</h1>\n<h2 id=\"directory-tree\">Directory Tree</h2>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">markdown</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">tokenizer-lexer/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”œâ”€â”€ src/                     # Core Implementation Logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ </span><span style=\"color:#E1E4E8;font-weight:bold\">__init__</span><span style=\"color:#E1E4E8\">.py          # Package initialization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ token_type.py        # TokenType Enum (M1: Foundations, M2: Operators/Keywords)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ token_class.py       # Token dataclass (M1: Lexeme/Line/Col storage)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â””â”€â”€ scanner.py           # Core FSM logic (M1: Primitives, M2: Lookahead, M3: Modes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”œâ”€â”€ tests/                   # Test Suites</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ </span><span style=\"color:#E1E4E8;font-weight:bold\">__init__</span><span style=\"color:#E1E4E8\">.py          # Test package initialization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ test_foundation.py   # M1: Single-char &#x26; whitespace tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ test_multi_char.py   # M2: Operators, numbers, and keywords</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ test_strings.py      # M3: Escape sequences &#x26; quote logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ test_comments.py     # M3: Single/Multi-line comment isolation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ integration_harness.py # M4: Deep-diffing utilities for token streams</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ test_integration.py  # M4: \"Golden Path\" full program tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ test_position.py     # M4: Line/Column drift &#x26; CRLF validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â””â”€â”€ test_boundaries.py   # M4: Edge cases (empty files, max-length IDs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”œâ”€â”€ benchmarks/              # Performance Validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â””â”€â”€ perf_benchmark.py    # M4: Throughput tests (10k lines/sec target)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”œâ”€â”€ diagrams/                # Technical Reference (SVG/Mermaid)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â”œâ”€â”€ fsm_states.svg       # State machine transitions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”‚   â””â”€â”€ scanner_modes.svg    # Mode isolation (Normal/String/Comment)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”œâ”€â”€ Makefile                 # Build automation (test execution, linting)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”œâ”€â”€ .gitignore               # Python-specific ignore rules (</span><span style=\"color:#E1E4E8;font-weight:bold\">__pycache__</span><span style=\"color:#E1E4E8\">, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â”œâ”€â”€ requirements.txt         # Minimal dependencies (pytest, mypy)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">â””â”€â”€ README.md                # Project overview and usage instructions</span></span></code></pre></div>\n\n<h2 id=\"creation-order\">Creation Order</h2>\n<ol>\n<li><p><strong>Project Foundation</strong> (30 min)</p>\n<ul>\n<li>Create directory structure: <code>mkdir -p src tests benchmarks diagrams</code></li>\n<li>Initialize <code>requirements.txt</code> and <code>.gitignore</code>.</li>\n<li>Set up <code>Makefile</code> with a <code>test</code> target.</li>\n</ul>\n</li>\n<li><p><strong>Module 1: Basic Scanning</strong> (1-2 hours)</p>\n<ul>\n<li><code>src/token_type.py</code>: Define M1 variants.</li>\n<li><code>src/token_class.py</code>: Implement <code>Token</code> dataclass.</li>\n<li><code>src/scanner.py</code>: Implement <code>advance()</code>, <code>peek()</code>, and single-char dispatch.</li>\n<li><code>tests/test_foundation.py</code>: Verify whitespace and punctuation.</li>\n</ul>\n</li>\n<li><p><strong>Module 2: Greedy Matching</strong> (1-2 hours)</p>\n<ul>\n<li><code>src/token_type.py</code>: Add M2 operator and keyword variants.</li>\n<li><code>src/scanner.py</code>: Add <code>_match()</code>, <code>_peek_next()</code>, <code>_scan_number()</code>, and <code>_scan_identifier()</code>.</li>\n<li><code>tests/test_multi_char.py</code>: Verify &quot;Maximal Munch&quot; and keyword lookup.</li>\n</ul>\n</li>\n<li><p><strong>Module 3: Context-Aware Modes</strong> (2 hours)</p>\n<ul>\n<li><code>src/scanner.py</code>: Implement <code>_scan_string()</code>, <code>_skip_line_comment()</code>, and <code>_skip_block_comment()</code>.</li>\n<li><code>tests/test_strings.py</code> &amp; <code>tests/test_comments.py</code>: Verify mode isolation (e.g., comments inside strings).</li>\n</ul>\n</li>\n<li><p><strong>Module 4: Hardening &amp; Benchmarks</strong> (1-2 hours)</p>\n<ul>\n<li><code>tests/integration_harness.py</code>: Build the <code>assert_tokens</code> helper.</li>\n<li><code>tests/test_integration.py</code>: Run the &quot;Golden Program&quot; trace.</li>\n<li><code>tests/test_position.py</code>: Check for line/column drift over large files.</li>\n<li><code>benchmarks/perf_benchmark.py</code>: Run 10k line performance test.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"file-count-summary\">File Count Summary</h2>\n<ul>\n<li><strong>Total Logic Files</strong>: 3 (<code>src/</code>)</li>\n<li><strong>Total Test Files</strong>: 8 (<code>tests/</code> + <code>benchmarks/</code>)</li>\n<li><strong>Total Configuration Files</strong>: 4 (<code>Makefile</code>, <code>README</code>, etc.)</li>\n<li><strong>Estimated Lines of Code</strong>: ~400â€“600 lines (Logic) / ~800 lines (Tests)</li>\n<li><strong>Target Language</strong>: Python 3.10+ (requires <code>dataclasses</code> and <code>enum</code>)</li>\n</ul>\n<h1 id=\"-beyond-the-atlas-further-reading\">ðŸ“š Beyond the Atlas: Further Reading</h1>\n<h3 id=\"-foundational-theory-automata-amp-formal-languages\">ðŸ§  Foundational Theory: Automata &amp; Formal Languages</h3>\n<p><strong>Paper</strong>: <a href=\"https://dl.acm.org/doi/10.1145/363347.363387\">A Technique for General Writing of Code Generators</a> (Brooker &amp; Morris, 1962).</p>\n<ul>\n<li><strong>Code</strong>: <a href=\"https://github.com/google/re2/blob/main/re2/parse.cc\">The RE2 Lexer Logic</a> (specifically the <code>Parse</code> function).</li>\n<li><strong>Best Explanation</strong>: <em>Introduction to the Theory of Computation</em> by Michael Sipser, <strong>Chapter 1: Regular Languages</strong>.</li>\n<li><strong>Why</strong>: This is the rigorous mathematical proof that finite automata can recognize exactly the class of languages definable by regular expressions.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>BEFORE Milestone 1</strong> to understand the mathematical constraints of what you are building.</li>\n</ul>\n<p><strong>Paper</strong>: <a href=\"https://dl.acm.org/doi/10.1145/363347.363387\">Regular Expression Search Algorithm</a> (Ken Thompson, 1968).</p>\n<ul>\n<li><strong>Best Explanation</strong>: <a href=\"https://swtch.com/~rsc/regexp/regexp1.html\">Regular Expression Matching Can Be Simple and Fast</a> by Russ Cox.</li>\n<li><strong>Why</strong>: It explains how to build an NFA from a regex, which is the automated version of the hand-written state machine you are building.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>AFTER Milestone 2</strong> to see how the &quot;Maximal Munch&quot; logic you wrote manually can be generated automatically from patterns.</li>\n</ul>\n<hr>\n<h3 id=\"-the-craft-of-lexing-implementation-guides\">ðŸ› ï¸ The Craft of Lexing: Implementation Guides</h3>\n<p><strong>Best Explanation</strong>: <em>Crafting Interpreters</em> by Robert Nystrom, <strong>Chapter 4: Scanning</strong>.</p>\n<ul>\n<li><strong>Code</strong>: <a href=\"https://github.com/munificent/craftinginterpreters/blob/master/java/com/craftinginterpreters/lox/Scanner.java\">Lox Scanner.java</a>.</li>\n<li><strong>Why</strong>: The most modern, approachable, and pedagogically sound walkthrough of building a hand-written scanner for a C-like language.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>DURING Milestone 1</strong>; it serves as a parallel guide to the logic implemented in this Atlas.</li>\n</ul>\n<p><strong>Spec</strong>: <a href=\"https://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf\">The Lexical Grammar of C11</a> (ISO/IEC 9899:201x, <strong>Section 6.4</strong>).</p>\n<ul>\n<li><strong>Code</strong>: <a href=\"https://github.com/llvm/llvm-project/blob/main/clang/lib/Lex/Lexer.cpp\">Clangâ€™s Lexer.cpp</a>.</li>\n<li><strong>Why</strong>: It defines the &quot;gold standard&quot; rules for the language your project is mimicking, including tricky edge cases for numbers and strings.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>DURING Milestone 2</strong> to see how a professional language defines the boundaries of integers and floating-point literals.</li>\n</ul>\n<hr>\n<h3 id=\"-real-world-tokenizers-case-studies\">ðŸ—ï¸ Real-World Tokenizers: Case Studies</h3>\n<p><strong>Code</strong>: <a href=\"https://github.com/python/cpython/blob/main/Lib/tokenize.py\">CPythonâ€™s Lib/tokenize.py</a>.</p>\n<ul>\n<li><strong>Best Explanation</strong>: <a href=\"https://docs.python.org/3/library/tokenize.html\">The Python Tokenizer Documentation</a>.</li>\n<li><strong>Why</strong>: Shows how a production-grade language uses a hybrid approach (regex + manual logic) to handle indentation-sensitive lexing.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>AFTER Milestone 2</strong> to compare your identifier and keyword lookup logic with Python&#39;s implementation.</li>\n</ul>\n<p><strong>Code</strong>: <a href=\"https://github.com/v8/v8/blob/master/src/parsing/scanner.cc\">V8â€™s scanner.cc</a>.</p>\n<ul>\n<li><strong>Why</strong>: V8&#39;s scanner is optimized for extreme performance, showing how to handle Unicode/UTF-8 characters at scale.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>AFTER Milestone 4</strong> once you have met your performance benchmarks to see how &quot;The Big Boys&quot; do it.</li>\n</ul>\n<hr>\n<h3 id=\"-error-handling-amp-recovery\">âš ï¸ Error Handling &amp; Recovery</h3>\n<p><strong>Paper</strong>: <a href=\"https://dl.acm.org/doi/10.1145/359545.359565\">Panic Mode Recovery in Lexical Analysis</a> (James, 1972).</p>\n<ul>\n<li><strong>Best Explanation</strong>: <em>Compilers: Principles, Techniques, and Tools</em> (The Dragon Book), <strong>Section 3.1.4: Lexical Errors</strong>.</li>\n<li><strong>Why</strong>: It introduces the &quot;Panic Mode&quot; philosophy used in your project, where the scanner recovers by skipping to the next &quot;safe&quot; character.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>BEFORE Milestone 4</strong> to understand why emitting an <code>ERROR</code> token is superior to throwing an exception.</li>\n</ul>\n<hr>\n<h3 id=\"-strings-escapes-and-character-sets\">ðŸ§µ Strings, Escapes, and Character Sets</h3>\n<p><strong>Spec</strong>: <a href=\"https://www.unicode.org/versions/Unicode15.0.0/\">The Unicode Standard, Version 15.0</a>, <strong>Chapter 3: Conformance (UTF-8)</strong>.</p>\n<ul>\n<li><strong>Best Explanation</strong>: <a href=\"https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\">The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets</a> by Joel Spolsky.</li>\n<li><strong>Why</strong>: Explains why &quot;character-at-a-time&quot; scanning is significantly more complex once you leave the ASCII range.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>BEFORE Milestone 3</strong> to prepare for the complexities of scanning string content.</li>\n</ul>\n<p><strong>Spec</strong>: <a href=\"https://datatracker.ietf.org/doc/html/rfc8259#section-7\">RFC 8259: The JavaScript Object Notation (JSON) Data Interchange Format</a>.</p>\n<ul>\n<li><strong>Why</strong>: Defines the most widely used standard for string escape sequences (<code>\\n</code>, <code>\\uXXXX</code>), which mirrors the logic in M3.</li>\n<li><strong>Pedagogical Timing</strong>: Read <strong>DURING Milestone 3</strong> to validate your escape sequence logic against the JSON standard.</li>\n</ul>\n","toc":[{"level":1,"text":"ðŸŽ¯ Project Charter: The C-Like Tokenizer","id":"-project-charter-the-c-like-tokenizer"},{"level":2,"text":"What You Are Building","id":"what-you-are-building"},{"level":2,"text":"Why This Project Exists","id":"why-this-project-exists"},{"level":2,"text":"What You Will Be Able to Do When Done","id":"what-you-will-be-able-to-do-when-done"},{"level":2,"text":"Final Deliverable","id":"final-deliverable"},{"level":2,"text":"Is This Project For You?","id":"is-this-project-for-you"},{"level":2,"text":"Estimated Effort","id":"estimated-effort"},{"level":2,"text":"Definition of Done","id":"definition-of-done"},{"level":1,"text":"Tokenizer / Lexer â€” Interactive Atlas","id":"tokenizer-lexer-interactive-atlas"},{"level":1,"text":"Milestone 1: Token Types &amp; Scanner Foundation","id":"milestone-1-token-types-amp-scanner-foundation"},{"level":2,"text":"Where You&#39;re Starting","id":"where-you39re-starting"},{"level":2,"text":"The Core Misconception: Tokenizing Is Not Splitting","id":"the-core-misconception-tokenizing-is-not-splitting"},{"level":2,"text":"Zero whitespace means zero splits. Two &quot;tokens.&quot; Neither is useful.\nBut a real tokenizer handles this correctly â€” producing Keyword(if), LParen, Identifier(x), GreaterEqual, Number(42), RParen, LBrace, Keyword(return), Keyword(true), Semicolon, RBrace, EOF â€” with no whitespace anywhere in the input.\nThe real model: a tokenizer is a character-by-character finite state machine. It does not look for spaces. It reads one character at a time, asks &quot;what state am I in and what does this character mean?&quot;, and decides: extend the current token, or emit what I have and start fresh. Whitespace is just another character â€” one that happens to trigger &quot;emit and don&#39;t include this character.&quot; A digit followed by a letter triggers &quot;emit the number, start an identifier.&quot; An = followed by another = triggers &quot;emit a two-character ==.&quot;\nThe boundaries are state transitions, not spaces. That&#39;s why x&gt;=42 tokenizes correctly with zero whitespace â€” the scanner changes state every time the character class changes, and those state changes are the boundaries.\nKeep this model in your head as you build: every branch in your scanner&#39;s main loop is a state transition in a finite automaton. You are building a DFA (deterministic finite automaton â€” a machine that reads input character by character and transitions between a finite number of states deterministically) compiled to imperative code.","id":"zero-whitespace-means-zero-splits-two-quottokensquot-neither-is-useful-but-a-real-tokenizer-handles-this-correctly-producing-keywordif-lparen-identifierx-greaterequal-number42-rparen-lbrace-keywordreturn-keywordtrue-semicolon-rbrace-eof-with-no-whitespace-anywhere-in-the-input-the-real-model-a-tokenizer-is-a-character-by-character-finite-state-machine-it-does-not-look-for-spaces-it-reads-one-character-at-a-time-asks-quotwhat-state-am-i-in-and-what-does-this-character-meanquot-and-decides-extend-the-current-token-or-emit-what-i-have-and-start-fresh-whitespace-is-just-another-character-one-that-happens-to-trigger-quotemit-and-don39t-include-this-characterquot-a-digit-followed-by-a-letter-triggers-quotemit-the-number-start-an-identifierquot-an-followed-by-another-triggers-quotemit-a-two-character-quot-the-boundaries-are-state-transitions-not-spaces-that39s-why-xgt42-tokenizes-correctly-with-zero-whitespace-the-scanner-changes-state-every-time-the-character-class-changes-and-those-state-changes-are-the-boundaries-keep-this-model-in-your-head-as-you-build-every-branch-in-your-scanner39s-main-loop-is-a-state-transition-in-a-finite-automaton-you-are-building-a-dfa-deterministic-finite-automaton-a-machine-that-reads-input-character-by-character-and-transitions-between-a-finite-number-of-states-deterministically-compiled-to-imperative-code"},{"level":2,"text":"Finite State Machines: The Theoretical Heart","id":"finite-state-machines-the-theoretical-heart"},{"level":2,"text":"The practical upshot: your scanner has implicit states encoded as control flow branches. When you&#39;re in the middle of scanning a number, you&#39;re in the &quot;scanning number&quot; state. When you hit a non-digit, you transition to &quot;emit number, return to start.&quot; You won&#39;t build an explicit state table in this milestone, but every if/elif in your main scan loop IS a state transition. Keep that mapping in mind â€” it explains why the code looks the way it does.","id":"the-practical-upshot-your-scanner-has-implicit-states-encoded-as-control-flow-branches-when-you39re-in-the-middle-of-scanning-a-number-you39re-in-the-quotscanning-numberquot-state-when-you-hit-a-non-digit-you-transition-to-quotemit-number-return-to-startquot-you-won39t-build-an-explicit-state-table-in-this-milestone-but-every-ifelif-in-your-main-scan-loop-is-a-state-transition-keep-that-mapping-in-mind-it-explains-why-the-code-looks-the-way-it-does"},{"level":2,"text":"Designing the Token Type Enumeration","id":"designing-the-token-type-enumeration"},{"level":2,"text":"A few design notes:\nWhy OPERATOR and PUNCTUATION as separate categories? Semantically, operators participate in expressions (they have operands and produce values) while punctuation is purely structural (parentheses group, semicolons terminate). A parser cares about this distinction when building its grammar rules. Grouping them together would make the parser&#39;s job harder.\nWhy a single KEYWORD type instead of IF, ELSE, WHILE, etc.? Both approaches are used in real compilers. A flat enum like this is simpler to start with; you look at the lexeme string to know which keyword. Many production lexers (including Python&#39;s own) use per-keyword variants for faster parser dispatch. For a teaching tokenizer, KEYWORD with a lexeme check is clean.\nWhy EOF as a token type? This is critical. When the scanner exhausts the input, it could return None, raise an exception, or emit a special sentinel token. None forces the parser to null-check every token access. A raised exception makes lookahead awkward. An EOF token lets the parser read naturally â€” while token.type != TokenType.EOF: ... â€” and the sentinel serves as an unconditional stopping condition. Without it, parsers crash in mysterious ways.\nWhy ERROR? Because the tokenizer should keep running after encountering an unrecognized character. If it raises an exception immediately, you can only report one error per compilation. Instead, emit an ERROR token (with position and the offending character), then continue scanning. By the end you&#39;ve collected all the lexical errors in a single pass.","id":"a-few-design-notes-why-operator-and-punctuation-as-separate-categories-semantically-operators-participate-in-expressions-they-have-operands-and-produce-values-while-punctuation-is-purely-structural-parentheses-group-semicolons-terminate-a-parser-cares-about-this-distinction-when-building-its-grammar-rules-grouping-them-together-would-make-the-parser39s-job-harder-why-a-single-keyword-type-instead-of-if-else-while-etc-both-approaches-are-used-in-real-compilers-a-flat-enum-like-this-is-simpler-to-start-with-you-look-at-the-lexeme-string-to-know-which-keyword-many-production-lexers-including-python39s-own-use-per-keyword-variants-for-faster-parser-dispatch-for-a-teaching-tokenizer-keyword-with-a-lexeme-check-is-clean-why-eof-as-a-token-type-this-is-critical-when-the-scanner-exhausts-the-input-it-could-return-none-raise-an-exception-or-emit-a-special-sentinel-token-none-forces-the-parser-to-null-check-every-token-access-a-raised-exception-makes-lookahead-awkward-an-eof-token-lets-the-parser-read-naturally-while-tokentype-tokentypeeof-and-the-sentinel-serves-as-an-unconditional-stopping-condition-without-it-parsers-crash-in-mysterious-ways-why-error-because-the-tokenizer-should-keep-running-after-encountering-an-unrecognized-character-if-it-raises-an-exception-immediately-you-can-only-report-one-error-per-compilation-instead-emit-an-error-token-with-position-and-the-offending-character-then-continue-scanning-by-the-end-you39ve-collected-all-the-lexical-errors-in-a-single-pass"},{"level":2,"text":"The Token Data Structure","id":"the-token-data-structure"},{"level":2,"text":"Building the Scanner Infrastructure","id":"building-the-scanner-infrastructure"},{"level":3,"text":"The Two Primitives: advance() and peek()","id":"the-two-primitives-advance-and-peek"},{"level":2,"text":"The asymmetry between advance and peek is the entire basis for lookahead: you can look at the next character to make a decision without committing to consuming it. You&#39;ll use this pattern constantly in later milestones â€” &quot;if the next character is =, then this is ==, not just =.&quot;\nWhy does peek() return &#39;&#39; at end-of-input instead of raising an exception? Because you&#39;ll call peek() in conditions: while peek() != &#39;\\n&#39; should terminate cleanly at EOF without needing a separate is_at_end() check in every loop. An empty string compares False to any non-empty character string, so the termination condition handles EOF naturally.","id":"the-asymmetry-between-advance-and-peek-is-the-entire-basis-for-lookahead-you-can-look-at-the-next-character-to-make-a-decision-without-committing-to-consuming-it-you39ll-use-this-pattern-constantly-in-later-milestones-quotif-the-next-character-is-then-this-is-not-just-quot-why-does-peek-return-3939-at-end-of-input-instead-of-raising-an-exception-because-you39ll-call-peek-in-conditions-while-peek-39n39-should-terminate-cleanly-at-eof-without-needing-a-separate-is_at_end-check-in-every-loop-an-empty-string-compares-false-to-any-non-empty-character-string-so-the-termination-condition-handles-eof-naturally"},{"level":2,"text":"Position Tracking: Line and Column","id":"position-tracking-line-and-column"},{"level":2,"text":"For this project, advancing tabs by 1 column is fine. Document it and move on.","id":"for-this-project-advancing-tabs-by-1-column-is-fine-document-it-and-move-on"},{"level":2,"text":"The Main Scanning Loop","id":"the-main-scanning-loop"},{"level":2,"text":"Note that _scan_token() returns None for whitespace â€” consumed and discarded silently. Every other case returns a Token object (including errors).","id":"note-that-_scan_token-returns-none-for-whitespace-consumed-and-discarded-silently-every-other-case-returns-a-token-object-including-errors"},{"level":2,"text":"Single-Character Token Dispatch","id":"single-character-token-dispatch"},{"level":2,"text":"Why put / in the single-character dispatch now, knowing it will need special handling for // and /* comments later? This is a deliberate scaffolding choice. In Milestone 3, you&#39;ll replace the &#39;/&#39; entry with a more complex handler. Until then, / simply produces a division operator token, which is correct for expressions like x / 2. Building incrementally â€” with each milestone adding behavior to a working foundation â€” is better than trying to handle every case at once.\nWhy is \\r in the whitespace set? To handle Windows \\r\\n endings. The \\r is consumed silently here; the \\n will be consumed on the next call to advance() and will trigger the line increment. This prevents double-counting.","id":"why-put-in-the-single-character-dispatch-now-knowing-it-will-need-special-handling-for-and-comments-later-this-is-a-deliberate-scaffolding-choice-in-milestone-3-you39ll-replace-the-3939-entry-with-a-more-complex-handler-until-then-simply-produces-a-division-operator-token-which-is-correct-for-expressions-like-x-2-building-incrementally-with-each-milestone-adding-behavior-to-a-working-foundation-is-better-than-trying-to-handle-every-case-at-once-why-is-r-in-the-whitespace-set-to-handle-windows-rn-endings-the-r-is-consumed-silently-here-the-n-will-be-consumed-on-the-next-call-to-advance-and-will-trigger-the-line-increment-this-prevents-double-counting"},{"level":2,"text":"Putting It Together: The Complete M1 Scanner","id":"putting-it-together-the-complete-m1-scanner"},{"level":2,"text":"Testing Your Foundation","id":"testing-your-foundation"},{"level":2,"text":"Run all of these. They should pass with the implementation above. If any fail, the position tracking is the most common culprit â€” check the advance() method and the start_column snapshot timing carefully.","id":"run-all-of-these-they-should-pass-with-the-implementation-above-if-any-fail-the-position-tracking-is-the-most-common-culprit-check-the-advance-method-and-the-start_column-snapshot-timing-carefully"},{"level":2,"text":"Three-Level View: What&#39;s Really Happening","id":"three-level-view-what39s-really-happening"},{"level":2,"text":"It&#39;s worth seeing this milestone from all three levels of the compiler stack.\nLevel 1 â€” Source Language: Your user types (x + 42). They see five tokens with three spaces. The spaces are invisible scaffolding â€” boundary hints, but not the actual boundaries.\nLevel 2 â€” The Scanner (what you&#39;re building now): The scanner reads 8 characters. It has no concept of &quot;a token&quot; when it starts â€” it builds up the token character by character, emitting when the state transition fires. ( â†’ single-char dispatch â†’ emit immediately. x â†’ not in dispatch table â†’ (in M2, will become identifier scanner). + â†’ single-char â†’ emit. 4 â†’ digit â†’ (in M2, will enter number-scanning state). ) â†’ single-char â†’ emit.\nLevel 3 â€” The Parser (downstream consumer): The parser receives a list[Token] and never sees characters again. It thinks in terms of token types, calling something like consume(TokenType.PUNCTUATION, &#39;(&#39;). The token stream is the interface contract between lexer and parser. Every mistake in your TokenType design forces the parser to work around it.","id":"it39s-worth-seeing-this-milestone-from-all-three-levels-of-the-compiler-stack-level-1-source-language-your-user-types-x-42-they-see-five-tokens-with-three-spaces-the-spaces-are-invisible-scaffolding-boundary-hints-but-not-the-actual-boundaries-level-2-the-scanner-what-you39re-building-now-the-scanner-reads-8-characters-it-has-no-concept-of-quota-tokenquot-when-it-starts-it-builds-up-the-token-character-by-character-emitting-when-the-state-transition-fires-single-char-dispatch-emit-immediately-x-not-in-dispatch-table-in-m2-will-become-identifier-scanner-single-char-emit-4-digit-in-m2-will-enter-number-scanning-state-single-char-emit-level-3-the-parser-downstream-consumer-the-parser-receives-a-listtoken-and-never-sees-characters-again-it-thinks-in-terms-of-token-types-calling-something-like-consumetokentypepunctuation-3939-the-token-stream-is-the-interface-contract-between-lexer-and-parser-every-mistake-in-your-tokentype-design-forces-the-parser-to-work-around-it"},{"level":2,"text":"Design Decision: Why advance() Updates Position","id":"design-decision-why-advance-updates-position"},{"level":2,"text":"Knowledge Cascade â€” &quot;Learn One, Unlock Ten&quot;","id":"knowledge-cascade-quotlearn-one-unlock-tenquot"},{"level":2,"text":"You&#39;ve built a character-level scanner. Here&#39;s what this unlocks:\nâ†’ Formal automata theory: Your scanner IS a DFA implementation. The _SINGLE_CHAR_TOKENS dispatch table is the DFA&#39;s transition function for the &quot;start&quot; state. The if/elif/else structure in _scan_token() enumerates all transitions from that state. When you add multi-character operators in M2, you&#39;ll add transitions to sub-states (the &quot;seen one =&quot; state). Formal language theory tells you that any regular language â€” the class that contains all tokenizable languages â€” can be recognized by a DFA. Your C-like language&#39;s lexical grammar is regular; that&#39;s why this approach works.\nâ†’ Parser expectations (downstream contract): The token stream you emit in this milestone is the alphabet for the parser that consumes it. If your TokenType categories are coarse (e.g., mixing KEYWORD and IDENTIFIER into a single NAME type), the parser must inspect every NAME token&#39;s lexeme to decide what grammar rule applies. If your categories are fine-grained, the parser can dispatch purely on type. The parser is written against the contract you establish here â€” changing TokenType in M3 would require changing the parser too.\nâ†’ Syntax highlighting in your editor: Every editor that does syntax highlighting (VS Code, Vim, Emacs) runs a tokenizer on every keystroke. The &quot;confused mid-string&quot; highlighting you&#39;ve seen â€” where adding an unmatched &quot; suddenly turns half your file orange â€” is exactly the ERROR and STRING token types interacting. When the tokenizer can&#39;t find the closing quote, it emits an unterminated string that swallows everything until the next &quot;. Now you know why. You also know the fix: language servers use incremental re-tokenization â€” re-scan only from the changed position â€” to handle this efficiently.\nâ†’ The linker&#39;s symbol table (cross-domain): The keyword lookup table you&#39;ll build in M2 (mapping strings like &quot;if&quot; to TokenType.KEYWORD) is the same fundamental data structure as a linker&#39;s symbol table â€” a hash map from name to meaning. In the linker, it maps symbol names to addresses. In your tokenizer, it maps lexeme strings to token types. Hash tables as &quot;name â†’ meaning&quot; registries are one of the most universal data structures in systems programming.\nâ†’ Error recovery as a design philosophy: You made ERROR a token type rather than an exception. This is the same philosophy behind Rust&#39;s Result&lt;T, E&gt; â€” errors are values, not exceptional control flow. Languages that lean on exceptions for lexical errors (like early implementations of many scripting languages) can only report one error per run. Compilers that treat errors as values (GCC, Clang, Rust&#39;s compiler) collect all errors and report them all at once. Your choice here reflects that philosophy at the smallest scale.\nâ†’ Position metadata as a universal pattern: The line and column you&#39;re tracking are metadata â€” information about the data rather than the data itself. This pattern appears everywhere: HTTP headers (metadata about the body), database index pages (metadata about row locations), Git commits (metadata about diffs). Collecting metadata eagerly and cheaply, while the primary computation is happening anyway, is a general principle. Retrofitting it later is always expensive.","id":"you39ve-built-a-character-level-scanner-here39s-what-this-unlocks-formal-automata-theory-your-scanner-is-a-dfa-implementation-the-_single_char_tokens-dispatch-table-is-the-dfa39s-transition-function-for-the-quotstartquot-state-the-ifelifelse-structure-in-_scan_token-enumerates-all-transitions-from-that-state-when-you-add-multi-character-operators-in-m2-you39ll-add-transitions-to-sub-states-the-quotseen-one-quot-state-formal-language-theory-tells-you-that-any-regular-language-the-class-that-contains-all-tokenizable-languages-can-be-recognized-by-a-dfa-your-c-like-language39s-lexical-grammar-is-regular-that39s-why-this-approach-works-parser-expectations-downstream-contract-the-token-stream-you-emit-in-this-milestone-is-the-alphabet-for-the-parser-that-consumes-it-if-your-tokentype-categories-are-coarse-eg-mixing-keyword-and-identifier-into-a-single-name-type-the-parser-must-inspect-every-name-token39s-lexeme-to-decide-what-grammar-rule-applies-if-your-categories-are-fine-grained-the-parser-can-dispatch-purely-on-type-the-parser-is-written-against-the-contract-you-establish-here-changing-tokentype-in-m3-would-require-changing-the-parser-too-syntax-highlighting-in-your-editor-every-editor-that-does-syntax-highlighting-vs-code-vim-emacs-runs-a-tokenizer-on-every-keystroke-the-quotconfused-mid-stringquot-highlighting-you39ve-seen-where-adding-an-unmatched-quot-suddenly-turns-half-your-file-orange-is-exactly-the-error-and-string-token-types-interacting-when-the-tokenizer-can39t-find-the-closing-quote-it-emits-an-unterminated-string-that-swallows-everything-until-the-next-quot-now-you-know-why-you-also-know-the-fix-language-servers-use-incremental-re-tokenization-re-scan-only-from-the-changed-position-to-handle-this-efficiently-the-linker39s-symbol-table-cross-domain-the-keyword-lookup-table-you39ll-build-in-m2-mapping-strings-like-quotifquot-to-tokentypekeyword-is-the-same-fundamental-data-structure-as-a-linker39s-symbol-table-a-hash-map-from-name-to-meaning-in-the-linker-it-maps-symbol-names-to-addresses-in-your-tokenizer-it-maps-lexeme-strings-to-token-types-hash-tables-as-quotname-meaningquot-registries-are-one-of-the-most-universal-data-structures-in-systems-programming-error-recovery-as-a-design-philosophy-you-made-error-a-token-type-rather-than-an-exception-this-is-the-same-philosophy-behind-rust39s-resultltt-egt-errors-are-values-not-exceptional-control-flow-languages-that-lean-on-exceptions-for-lexical-errors-like-early-implementations-of-many-scripting-languages-can-only-report-one-error-per-run-compilers-that-treat-errors-as-values-gcc-clang-rust39s-compiler-collect-all-errors-and-report-them-all-at-once-your-choice-here-reflects-that-philosophy-at-the-smallest-scale-position-metadata-as-a-universal-pattern-the-line-and-column-you39re-tracking-are-metadata-information-about-the-data-rather-than-the-data-itself-this-pattern-appears-everywhere-http-headers-metadata-about-the-body-database-index-pages-metadata-about-row-locations-git-commits-metadata-about-diffs-collecting-metadata-eagerly-and-cheaply-while-the-primary-computation-is-happening-anyway-is-a-general-principle-retrofitting-it-later-is-always-expensive"},{"level":2,"text":"What You&#39;ve Built","id":"what-you39ve-built"},{"level":1,"text":"Milestone 2: Multi-Character Tokens &amp; Maximal Munch","id":"milestone-2-multi-character-tokens-amp-maximal-munch"},{"level":2,"text":"Where You&#39;re Starting","id":"where-you39re-starting"},{"level":2,"text":"The Core Misconception: Characters Know What They Are","id":"the-core-misconception-characters-know-what-they-are"},{"level":2,"text":"The Maximal Munch Principle","id":"the-maximal-munch-principle"},{"level":3,"text":"Connection to Regular Expression Engines","id":"connection-to-regular-expression-engines"},{"level":2,"text":"Why the Parser Needs This","id":"why-the-parser-needs-this"},{"level":2,"text":"The parser consumes tokens one at a time. When it&#39;s looking for an ==, it calls something like consume(TokenType.EQUALS). If your lexer emits two ASSIGN tokens instead, the parser receives ASSIGN + ASSIGN and has no rule that matches â€” it fails. Or worse: it misparses.\nIn a language where a = b means assignment and a == b means equality check, the single-token vs. two-token distinction is the semantic difference between modifying state and reading state. Getting it wrong doesn&#39;t just produce a syntax error â€” in a more permissive parser, it could silently change what the program means.\nThe lexer&#39;s job is to make that distinction correctly and irreversibly. The parser trusts the token stream completely.","id":"the-parser-consumes-tokens-one-at-a-time-when-it39s-looking-for-an-it-calls-something-like-consumetokentypeequals-if-your-lexer-emits-two-assign-tokens-instead-the-parser-receives-assign-assign-and-has-no-rule-that-matches-it-fails-or-worse-it-misparses-in-a-language-where-a-b-means-assignment-and-a-b-means-equality-check-the-single-token-vs-two-token-distinction-is-the-semantic-difference-between-modifying-state-and-reading-state-getting-it-wrong-doesn39t-just-produce-a-syntax-error-in-a-more-permissive-parser-it-could-silently-change-what-the-program-means-the-lexer39s-job-is-to-make-that-distinction-correctly-and-irreversibly-the-parser-trusts-the-token-stream-completely"},{"level":2,"text":"Implementing Two-Character Operators with Lookahead","id":"implementing-two-character-operators-with-lookahead"},{"level":2,"text":"The &gt;== Case: Maximal Munch in Action","id":"the-gt-case-maximal-munch-in-action"},{"level":2,"text":"The key moment is in Step 1: when _match(&#39;=&#39;) is called, it looks at source[1] which is = â€” the second = in the input. It consumes that character (current advances to 2), locking in &gt;= as GREATER_EQUAL. Then in Step 2, the scanner starts fresh at position 2, reads the third character (=), peeks ahead (end of input), and emits ASSIGN.\nThis is exactly right. &gt;== is unambiguously &gt;= followed by = under maximal munch. A human programmer writing if (x &gt;= = 1) would get a parse error (not a lex error), but the tokenizer&#39;s job is correct regardless of what the parser does with it.","id":"the-key-moment-is-in-step-1-when-_match3939-is-called-it-looks-at-source1-which-is-the-second-in-the-input-it-consumes-that-character-current-advances-to-2-locking-in-gt-as-greater_equal-then-in-step-2-the-scanner-starts-fresh-at-position-2-reads-the-third-character-peeks-ahead-end-of-input-and-emits-assign-this-is-exactly-right-gt-is-unambiguously-gt-followed-by-under-maximal-munch-a-human-programmer-writing-if-x-gt-1-would-get-a-parse-error-not-a-lex-error-but-the-tokenizer39s-job-is-correct-regardless-of-what-the-parser-does-with-it"},{"level":2,"text":"Number Literal Scanning","id":"number-literal-scanning"},{"level":3,"text":"The Trailing-Dot Decision","id":"the-trailing-dot-decision"},{"level":2,"text":"Identifier Scanning and the Keyword Lookup Table","id":"identifier-scanning-and-the-keyword-lookup-table"},{"level":3,"text":"The Critical Rule: Scan First, Lookup Second","id":"the-critical-rule-scan-first-lookup-second"},{"level":3,"text":"Why a Dictionary, Not a Long if/elif Chain?","id":"why-a-dictionary-not-a-long-ifelif-chain"},{"level":2,"text":"The Lookahead Budget: LA(1)","id":"the-lookahead-budget-la1"},{"level":2,"text":"Complete M2 _scan_token() â€” Putting It All Together","id":"complete-m2-_scan_token-putting-it-all-together"},{"level":2,"text":"Three-Level View: What &gt;= Means at Each Layer","id":"three-level-view-what-gt-means-at-each-layer"},{"level":2,"text":"Level 1 â€” Source Language: The programmer writes x &gt;= 42. To them, &gt;= is a single operator with a single meaning: &quot;greater than or equal to.&quot; They think in terms of mathematical comparisons.\nLevel 2 â€” The Scanner (what you&#39;re building): The scanner reads &gt;, then peeks =, then consumes =. At this level, &gt;= is two characters that get collapsed into one token. The scanner&#39;s job is exactly this collapse â€” taking the continuous character stream and imposing structure.\nLevel 3 â€” The Parser and Runtime (downstream): The parser sees Token(GREATER_EQUAL, &quot;&gt;=&quot;, ...) as a single atomic unit. Its grammar rule for comparisons matches on GREATER_EQUAL as a token type. The evaluator eventually turns GREATER_EQUAL into a machine instruction (usually a jge or setge on x86). By the time it reaches silicon, &gt;= has been through three representations: characters â†’ token â†’ machine instruction.","id":"level-1-source-language-the-programmer-writes-x-gt-42-to-them-gt-is-a-single-operator-with-a-single-meaning-quotgreater-than-or-equal-toquot-they-think-in-terms-of-mathematical-comparisons-level-2-the-scanner-what-you39re-building-the-scanner-reads-gt-then-peeks-then-consumes-at-this-level-gt-is-two-characters-that-get-collapsed-into-one-token-the-scanner39s-job-is-exactly-this-collapse-taking-the-continuous-character-stream-and-imposing-structure-level-3-the-parser-and-runtime-downstream-the-parser-sees-tokengreater_equal-quotgtquot-as-a-single-atomic-unit-its-grammar-rule-for-comparisons-matches-on-greater_equal-as-a-token-type-the-evaluator-eventually-turns-greater_equal-into-a-machine-instruction-usually-a-jge-or-setge-on-x86-by-the-time-it-reaches-silicon-gt-has-been-through-three-representations-characters-token-machine-instruction"},{"level":2,"text":"Common Pitfalls and How to Avoid Them","id":"common-pitfalls-and-how-to-avoid-them"},{"level":3,"text":"Pitfall 1: Forgetting to Snapshot start_column Before advance()","id":"pitfall-1-forgetting-to-snapshot-start_column-before-advance"},{"level":3,"text":"Pitfall 2: 42abc â€” Number Immediately Followed by Identifier","id":"pitfall-2-42abc-number-immediately-followed-by-identifier"},{"level":3,"text":"Pitfall 3: keyword Inside identifiers Must Not Match","id":"pitfall-3-keyword-inside-identifiers-must-not-match"},{"level":3,"text":"Pitfall 4: Float With Trailing Dot (3.)","id":"pitfall-4-float-with-trailing-dot-3"},{"level":2,"text":"With the current implementation, 3. tokenizes as NUMBER(&quot;3&quot;) followed by ERROR(&quot;.&quot;) (since bare . is not a valid token). This is correct by design. But test it explicitly â€” it&#39;s easy to accidentally consume the dot inside _scan_number() without the guard _peek_next().isdigit().","id":"with-the-current-implementation-3-tokenizes-as-numberquot3quot-followed-by-errorquotquot-since-bare-is-not-a-valid-token-this-is-correct-by-design-but-test-it-explicitly-it39s-easy-to-accidentally-consume-the-dot-inside-_scan_number-without-the-guard-_peek_nextisdigit"},{"level":2,"text":"Testing M2","id":"testing-m2"},{"level":2,"text":"All of these should pass with the implementation above. Run them before moving to M3.","id":"all-of-these-should-pass-with-the-implementation-above-run-them-before-moving-to-m3"},{"level":2,"text":"Knowledge Cascade â€” &quot;Learn One, Unlock Ten&quot;","id":"knowledge-cascade-quotlearn-one-unlock-tenquot"},{"level":2,"text":"You&#39;ve implemented maximal munch, two-character operators, number scanning, and keyword recognition. Here&#39;s what those unlock:\nâ†’ Regular expression engines: The _match() primitive you wrote â€” peek ahead, consume if match â€” is exactly what a regex engine&#39;s greedy quantifier does internally. When you write \\d+ in a regex, the engine&#39;s NFA-to-DFA compiled code runs a loop identical to your while self.peek().isdigit(): self.advance(). You&#39;ve written a regex engine&#39;s inner loop by hand, for a specific pattern. Understanding one gives you the other for free.\nâ†’ Operator precedence in parsers (downstream impact): The parser that consumes your token stream will have precedence rules like &quot;multiplication binds tighter than addition.&quot; Those rules operate on token types. By emitting EQUALS as a single token (not two ASSIGNs), you&#39;ve made it possible for the parser to write a single grammar rule comparison := expr EQUALS expr. If you&#39;d emitted two ASSIGN tokens, the parser would need special-case logic to reassemble them â€” which is fragile and error-prone. Correct lexing is the foundation of correct parsing.\nâ†’ The keyword/identifier boundary in real compilers: GCC and Clang use the exact same &quot;scan full identifier, then hash-lookup&quot; strategy. Clang&#39;s Lexer::LexIdentifier() scans all alphanumeric characters, then calls LookUpIdentifierInfo() which hashes the result and returns a tok::TokenKind. The structure is identical to _scan_identifier(). Real-world lexers differ in performance (perfect hashing, SIMD character classification), not in algorithm.\nâ†’ LA(k) in parsing theory: Your lexer uses LA(1) with a brief excursion to LA(2) for the trailing-dot case. Parsers use LA(k) over tokens (not characters). LL(1) parsers â€” the simplest useful parsers â€” use LA(1): look at one token ahead to decide which grammar rule to apply. LR(1) parsers (used by yacc/bison) also use one token of lookahead but in a more powerful way. The concept scales: the principle of &quot;look ahead by k to resolve ambiguity&quot; is the same whether k is 1 character or 1 token.\nâ†’ Lexer generators (what you could build next): Tools like lex, flex, and ANTLR&#39;s lexer take a set of regex patterns and generate a lexer automatically. Internally, they compile each regex to an NFA, merge the NFAs into one combined NFA, convert to a DFA using subset construction, and minimize the DFA. The result is a state transition table â€” a 2D array indexed by (state, character) â€” that runs faster than any hand-written if/elif chain. You&#39;ve hand-written the conceptual equivalent; the generator just automates the machinery and optimizes it. Crafting Interpreters Chapter 4 by Robert Nystrom walks through this hand-written approach in Java; the LLVM tutorial shows the C++ equivalent.\nâ†’ Number literal complexity in real languages: Your number scanner handles integers and simple floats. Real languages are significantly more complex. C supports 0xFF (hex), 0755 (octal), 0b1010 (binary in C23), 1.5e10 (scientific notation), 1.5f (float suffix), 1ULL (unsigned long long). Python adds 1_000_000 (underscores for readability) and 0o755 (explicit octal). Each of these requires additional states in the number-scanning FSM. The structure you&#39;ve built â€” a loop consuming valid chars, branching on special characters â€” extends naturally to all of these. You&#39;ve built the right architecture; adding cases is mechanical.","id":"you39ve-implemented-maximal-munch-two-character-operators-number-scanning-and-keyword-recognition-here39s-what-those-unlock-regular-expression-engines-the-_match-primitive-you-wrote-peek-ahead-consume-if-match-is-exactly-what-a-regex-engine39s-greedy-quantifier-does-internally-when-you-write-d-in-a-regex-the-engine39s-nfa-to-dfa-compiled-code-runs-a-loop-identical-to-your-while-selfpeekisdigit-selfadvance-you39ve-written-a-regex-engine39s-inner-loop-by-hand-for-a-specific-pattern-understanding-one-gives-you-the-other-for-free-operator-precedence-in-parsers-downstream-impact-the-parser-that-consumes-your-token-stream-will-have-precedence-rules-like-quotmultiplication-binds-tighter-than-additionquot-those-rules-operate-on-token-types-by-emitting-equals-as-a-single-token-not-two-assigns-you39ve-made-it-possible-for-the-parser-to-write-a-single-grammar-rule-comparison-expr-equals-expr-if-you39d-emitted-two-assign-tokens-the-parser-would-need-special-case-logic-to-reassemble-them-which-is-fragile-and-error-prone-correct-lexing-is-the-foundation-of-correct-parsing-the-keywordidentifier-boundary-in-real-compilers-gcc-and-clang-use-the-exact-same-quotscan-full-identifier-then-hash-lookupquot-strategy-clang39s-lexerlexidentifier-scans-all-alphanumeric-characters-then-calls-lookupidentifierinfo-which-hashes-the-result-and-returns-a-toktokenkind-the-structure-is-identical-to-_scan_identifier-real-world-lexers-differ-in-performance-perfect-hashing-simd-character-classification-not-in-algorithm-lak-in-parsing-theory-your-lexer-uses-la1-with-a-brief-excursion-to-la2-for-the-trailing-dot-case-parsers-use-lak-over-tokens-not-characters-ll1-parsers-the-simplest-useful-parsers-use-la1-look-at-one-token-ahead-to-decide-which-grammar-rule-to-apply-lr1-parsers-used-by-yaccbison-also-use-one-token-of-lookahead-but-in-a-more-powerful-way-the-concept-scales-the-principle-of-quotlook-ahead-by-k-to-resolve-ambiguityquot-is-the-same-whether-k-is-1-character-or-1-token-lexer-generators-what-you-could-build-next-tools-like-lex-flex-and-antlr39s-lexer-take-a-set-of-regex-patterns-and-generate-a-lexer-automatically-internally-they-compile-each-regex-to-an-nfa-merge-the-nfas-into-one-combined-nfa-convert-to-a-dfa-using-subset-construction-and-minimize-the-dfa-the-result-is-a-state-transition-table-a-2d-array-indexed-by-state-character-that-runs-faster-than-any-hand-written-ifelif-chain-you39ve-hand-written-the-conceptual-equivalent-the-generator-just-automates-the-machinery-and-optimizes-it-crafting-interpreters-chapter-4-by-robert-nystrom-walks-through-this-hand-written-approach-in-java-the-llvm-tutorial-shows-the-c-equivalent-number-literal-complexity-in-real-languages-your-number-scanner-handles-integers-and-simple-floats-real-languages-are-significantly-more-complex-c-supports-0xff-hex-0755-octal-0b1010-binary-in-c23-15e10-scientific-notation-15f-float-suffix-1ull-unsigned-long-long-python-adds-1_000_000-underscores-for-readability-and-0o755-explicit-octal-each-of-these-requires-additional-states-in-the-number-scanning-fsm-the-structure-you39ve-built-a-loop-consuming-valid-chars-branching-on-special-characters-extends-naturally-to-all-of-these-you39ve-built-the-right-architecture-adding-cases-is-mechanical"},{"level":2,"text":"What You&#39;ve Built","id":"what-you39ve-built"},{"level":1,"text":"Milestone 3: Strings &amp; Comments","id":"milestone-3-strings-amp-comments"},{"level":2,"text":"Where You&#39;re Starting","id":"where-you39re-starting"},{"level":2,"text":"The Core Misconception: The Tokenizer Has One Set of Rules","id":"the-core-misconception-the-tokenizer-has-one-set-of-rules"},{"level":2,"text":"This is not an engineering choice â€” it is a formal necessity. Let&#39;s understand why.","id":"this-is-not-an-engineering-choice-it-is-a-formal-necessity-let39s-understand-why"},{"level":2,"text":"Why Regular Expressions Cannot Tokenize Real Languages","id":"why-regular-expressions-cannot-tokenize-real-languages"},{"level":2,"text":"The Three Modes: A State Machine with Context","id":"the-three-modes-a-state-machine-with-context"},{"level":2,"text":"The / Ambiguity: Division, Line Comment, or Block Comment?","id":"the-ambiguity-division-line-comment-or-block-comment"},{"level":2,"text":"Remove &#39;/&#39; from _SINGLE_CHAR_TOKENS â€” it now has its own branch. The first character is &#39;/&#39; (already consumed by advance()); then _match(&#39;/&#39;) or _match(&#39;*&#39;) peeks at the next character using the same conditional-consume primitive you built in Milestone 2.","id":"remove-3939-from-_single_char_tokens-it-now-has-its-own-branch-the-first-character-is-3939-already-consumed-by-advance-then-_match3939-or-_match3939-peeks-at-the-next-character-using-the-same-conditional-consume-primitive-you-built-in-milestone-2"},{"level":2,"text":"Single-Line Comments: The Simple Case","id":"single-line-comments-the-simple-case"},{"level":2,"text":"Note the deliberate choice: do NOT consume the \\n. Let it be picked up by the main loop&#39;s whitespace handling. This keeps the newline handling in one place â€” advance() â€” rather than scattering it across multiple methods.\nWhy this matters for correctness: the advance() method is responsible for incrementing self.line when it sees \\n. If you consume \\n inside _skip_line_comment() using advance(), the line counter updates correctly. If you somehow consume it without going through advance(), you introduce a line-counting bug. Routing all character consumption through advance() is the key invariant that keeps position tracking accurate.","id":"note-the-deliberate-choice-do-not-consume-the-n-let-it-be-picked-up-by-the-main-loop39s-whitespace-handling-this-keeps-the-newline-handling-in-one-place-advance-rather-than-scattering-it-across-multiple-methods-why-this-matters-for-correctness-the-advance-method-is-responsible-for-incrementing-selfline-when-it-sees-n-if-you-consume-n-inside-_skip_line_comment-using-advance-the-line-counter-updates-correctly-if-you-somehow-consume-it-without-going-through-advance-you-introduce-a-line-counting-bug-routing-all-character-consumption-through-advance-is-the-key-invariant-that-keeps-position-tracking-accurate"},{"level":2,"text":"Multi-Line Comments: The */ Hunt","id":"multi-line-comments-the-hunt"},{"level":3,"text":"The Non-Nesting Rule","id":"the-non-nesting-rule"},{"level":2,"text":"But for this C-like language, nesting is explicitly not supported. The simpler implementation is correct by specification.","id":"but-for-this-c-like-language-nesting-is-explicitly-not-supported-the-simpler-implementation-is-correct-by-specification"},{"level":2,"text":"String Literal Scanning: Entering a New World","id":"string-literal-scanning-entering-a-new-world"},{"level":3,"text":"The Basic Loop","id":"the-basic-loop"},{"level":2,"text":"The crucial invariant: Inside _scan_string(), you never look at the dispatch table. You never check if // is a comment start. You never check if { is punctuation. Every character you encounter â€” /, {, +, anything â€” is consumed as part of the string. The only characters that have special meaning inside a string are &quot; (exit), \\n (error), and \\ (escape prefix). Everything else is content.\nThis is what &quot;mode suspension&quot; means in practice: you&#39;ve entered a different scanning context where the dispatch table simply doesn&#39;t apply.","id":"the-crucial-invariant-inside-_scan_string-you-never-look-at-the-dispatch-table-you-never-check-if-is-a-comment-start-you-never-check-if-is-punctuation-every-character-you-encounter-anything-is-consumed-as-part-of-the-string-the-only-characters-that-have-special-meaning-inside-a-string-are-quot-exit-n-error-and-escape-prefix-everything-else-is-content-this-is-what-quotmode-suspensionquot-means-in-practice-you39ve-entered-a-different-scanning-context-where-the-dispatch-table-simply-doesn39t-apply"},{"level":2,"text":"Escape Sequences: Two Characters, One Meaning","id":"escape-sequences-two-characters-one-meaning"},{"level":3,"text":"The Critical Error Reporting Insight","id":"the-critical-error-reporting-insight"},{"level":2,"text":"Option B is dramatically more useful. Line 5, column 80 might be the end of the file â€” that tells you nothing about where to look. Line 5, column 12 tells you exactly: &quot;go to the quote at column 12, and find the matching closing quote that you forgot.&quot;\nThis is why the implementation stores self.start_column before entering _scan_string(), and uses it in all error tokens. The start_column snapshot â€” taken at the top of _scan_token() before the first advance() â€” captures the position of the opening &quot;. Every error from inside _scan_string() uses this stored position.\nThis is not just a nice-to-have. It&#39;s the difference between a usable error message and a frustrating one. GCC, Clang, and rustc all report the position of the opening delimiter when signaling unterminated strings â€” because experience has proven it&#39;s the right choice.","id":"option-b-is-dramatically-more-useful-line-5-column-80-might-be-the-end-of-the-file-that-tells-you-nothing-about-where-to-look-line-5-column-12-tells-you-exactly-quotgo-to-the-quote-at-column-12-and-find-the-matching-closing-quote-that-you-forgotquot-this-is-why-the-implementation-stores-selfstart_column-before-entering-_scan_string-and-uses-it-in-all-error-tokens-the-start_column-snapshot-taken-at-the-top-of-_scan_token-before-the-first-advance-captures-the-position-of-the-opening-quot-every-error-from-inside-_scan_string-uses-this-stored-position-this-is-not-just-a-nice-to-have-it39s-the-difference-between-a-usable-error-message-and-a-frustrating-one-gcc-clang-and-rustc-all-report-the-position-of-the-opening-delimiter-when-signaling-unterminated-strings-because-experience-has-proven-it39s-the-right-choice"},{"level":2,"text":"Putting It Together: The Complete _scan_token() for M3","id":"putting-it-together-the-complete-_scan_token-for-m3"},{"level":2,"text":"Notice the structure: the division operator case (elif char == &#39;/&#39;) now replaces the &#39;/&#39; entry in _SINGLE_CHAR_TOKENS. The string literal case (elif char == &#39;&quot;&#39;) is added before number and identifier scanning.","id":"notice-the-structure-the-division-operator-case-elif-char-3939-now-replaces-the-3939-entry-in-_single_char_tokens-the-string-literal-case-elif-char-39quot39-is-added-before-number-and-identifier-scanning"},{"level":2,"text":"Complete Scanner Implementation for M3","id":"complete-scanner-implementation-for-m3"},{"level":2,"text":"Three-Level View: What a String Literal Means at Each Layer","id":"three-level-view-what-a-string-literal-means-at-each-layer"},{"level":2,"text":"Level 1 â€” Source Language: The programmer writes greeting = &quot;hello\\nworld&quot;. To them, this is assigning a two-line string to a variable. The \\n is the &quot;newline character&quot; â€” a single logical thing.\nLevel 2 â€” The Scanner (what you&#39;re building): The scanner sees 22 characters. It enters _scan_string() when it hits the &quot;. It reads h, e, l, l, o as ordinary content. When it hits \\, it enters escape-processing: consumes \\, then consumes n â€” two characters, but they&#39;re both part of the string content. It continues until the closing &quot;. The emitted token has lexeme = &#39;&quot;hello\\\\nworld&quot;&#39; â€” a 14-character string including the quotes and the raw backslash-n. The scanner preserves exactly what was written.\nLevel 3 â€” The Evaluator (downstream): When the interpreter or code generator processes the STRING token, it strips the outer quotes and processes \\n â†’ newline, \\t â†’ tab, etc. The runtime string object will contain an actual newline byte (ASCII 10). This is where &quot;hello\\nworld&quot; becomes a 12-character string with a newline in the middle.\nThe scanner deliberately does not perform Level 3&#39;s job. Keeping responsibilities separated means the scanner is simple, testable, and independent of the evaluation semantics.","id":"level-1-source-language-the-programmer-writes-greeting-quothellonworldquot-to-them-this-is-assigning-a-two-line-string-to-a-variable-the-n-is-the-quotnewline-characterquot-a-single-logical-thing-level-2-the-scanner-what-you39re-building-the-scanner-sees-22-characters-it-enters-_scan_string-when-it-hits-the-quot-it-reads-h-e-l-l-o-as-ordinary-content-when-it-hits-it-enters-escape-processing-consumes-then-consumes-n-two-characters-but-they39re-both-part-of-the-string-content-it-continues-until-the-closing-quot-the-emitted-token-has-lexeme-39quothellonworldquot39-a-14-character-string-including-the-quotes-and-the-raw-backslash-n-the-scanner-preserves-exactly-what-was-written-level-3-the-evaluator-downstream-when-the-interpreter-or-code-generator-processes-the-string-token-it-strips-the-outer-quotes-and-processes-n-newline-t-tab-etc-the-runtime-string-object-will-contain-an-actual-newline-byte-ascii-10-this-is-where-quothellonworldquot-becomes-a-12-character-string-with-a-newline-in-the-middle-the-scanner-deliberately-does-not-perform-level-339s-job-keeping-responsibilities-separated-means-the-scanner-is-simple-testable-and-independent-of-the-evaluation-semantics"},{"level":2,"text":"Common Pitfalls and How to Avoid Them","id":"common-pitfalls-and-how-to-avoid-them"},{"level":3,"text":"Pitfall 1: Consuming the Closing &quot; and Leaving It for the Next Token","id":"pitfall-1-consuming-the-closing-quot-and-leaving-it-for-the-next-token"},{"level":3,"text":"Pitfall 2: A Backslash at the End of the String","id":"pitfall-2-a-backslash-at-the-end-of-the-string"},{"level":3,"text":"Pitfall 3: Detecting */ Inside a Block Comment","id":"pitfall-3-detecting-inside-a-block-comment"},{"level":3,"text":"Pitfall 4: Comments Inside Strings","id":"pitfall-4-comments-inside-strings"},{"level":3,"text":"Pitfall 5: Line Count Drift in Multi-Line Comments","id":"pitfall-5-line-count-drift-in-multi-line-comments"},{"level":2,"text":"The invariant is: every character consumed goes through advance(). If you maintain this, position tracking is automatically correct in every context.","id":"the-invariant-is-every-character-consumed-goes-through-advance-if-you-maintain-this-position-tracking-is-automatically-correct-in-every-context"},{"level":2,"text":"Testing M3","id":"testing-m3"},{"level":2,"text":"Run all of these. The ones that test position inside multi-line constructs are the most demanding â€” if your advance() invariant holds, they pass automatically.","id":"run-all-of-these-the-ones-that-test-position-inside-multi-line-constructs-are-the-most-demanding-if-your-advance-invariant-holds-they-pass-automatically"},{"level":2,"text":"Syntax Highlighting: Why Your Editor Sometimes Goes Wrong","id":"syntax-highlighting-why-your-editor-sometimes-goes-wrong"},{"level":2,"text":"The moment you type the opening &quot; without a closing &quot;, watch what happens to the lines below: they often turn orange (or whatever your string-color is), and y, =, and 42 all look like they&#39;re inside the string. Sometimes the entire file changes color.\nThis is your scanner&#39;s behavior, displayed visually. The editor&#39;s tokenizer entered IN_STRING mode when it saw &quot;. It couldn&#39;t find a closing &quot; before the end of the line (or end of file). It emitted an ERROR or a very long STRING token. The syntax highlighter colored everything in that &quot;string&quot; as string content.\nWhen you add the closing &quot;, the tokenizer re-runs (in modern editors, incrementally from the changed point), the string terminates correctly, and everything after it returns to normal colors.\nThis is not a bug in your editor. It&#39;s the correct behavior of a tokenizer that&#39;s lost context. The &quot;fix&quot; in real editors is to limit how far a string can extend without re-synchronization â€” essentially the same non-nesting rule you implemented for comments, applied to strings. Some editors use heuristic re-synchronization: &quot;if we haven&#39;t found a closing quote after N lines, assume the string ended and restart from here.&quot; This produces wrong highlighting sometimes, but it&#39;s better than highlighting an entire 10,000-line file as a string because of one missing quote.\nUnderstanding this transforms a mysterious editor behavior into a comprehensible engineering tradeoff: how much context do you maintain, and when do you sacrifice accuracy for usability?","id":"the-moment-you-type-the-opening-quot-without-a-closing-quot-watch-what-happens-to-the-lines-below-they-often-turn-orange-or-whatever-your-string-color-is-and-y-and-42-all-look-like-they39re-inside-the-string-sometimes-the-entire-file-changes-color-this-is-your-scanner39s-behavior-displayed-visually-the-editor39s-tokenizer-entered-in_string-mode-when-it-saw-quot-it-couldn39t-find-a-closing-quot-before-the-end-of-the-line-or-end-of-file-it-emitted-an-error-or-a-very-long-string-token-the-syntax-highlighter-colored-everything-in-that-quotstringquot-as-string-content-when-you-add-the-closing-quot-the-tokenizer-re-runs-in-modern-editors-incrementally-from-the-changed-point-the-string-terminates-correctly-and-everything-after-it-returns-to-normal-colors-this-is-not-a-bug-in-your-editor-it39s-the-correct-behavior-of-a-tokenizer-that39s-lost-context-the-quotfixquot-in-real-editors-is-to-limit-how-far-a-string-can-extend-without-re-synchronization-essentially-the-same-non-nesting-rule-you-implemented-for-comments-applied-to-strings-some-editors-use-heuristic-re-synchronization-quotif-we-haven39t-found-a-closing-quote-after-n-lines-assume-the-string-ended-and-restart-from-herequot-this-produces-wrong-highlighting-sometimes-but-it39s-better-than-highlighting-an-entire-10000-line-file-as-a-string-because-of-one-missing-quote-understanding-this-transforms-a-mysterious-editor-behavior-into-a-comprehensible-engineering-tradeoff-how-much-context-do-you-maintain-and-when-do-you-sacrifice-accuracy-for-usability"},{"level":2,"text":"Three-Level View: What a Comment Really Is","id":"three-level-view-what-a-comment-really-is"},{"level":2,"text":"Level 1 â€” Source Language: The programmer writes // TODO: fix this. To them, it&#39;s documentation â€” invisible to the compiler, meaningful to humans.\nLevel 2 â€” The Scanner (what you&#39;re building): A comment is a mode change. The scanner enters IN_LINE_COMMENT state, discards all characters until newline, then returns to NORMAL. No token is emitted. From the scanner&#39;s perspective, the comment literally does not exist â€” it&#39;s as if those characters were whitespace.\nLevel 3 â€” The Token Stream (downstream): The parser, type checker, and code generator never see comments. They operate entirely on the token stream, which is comment-free. The only place comments survive is in specialized tools: documentation generators (Javadoc, Rust&#39;s /// doc comments), IDEs building a comment index, and some preprocessors. Those tools run their own passes over the raw source, separate from the compilation pipeline.\nThis three-level view reveals something interesting: comments are stripped at Level 2, earlier than almost everything else. They&#39;re not &quot;ignored by the parser&quot; â€” they never reach the parser. They vanish in the scanner.","id":"level-1-source-language-the-programmer-writes-todo-fix-this-to-them-it39s-documentation-invisible-to-the-compiler-meaningful-to-humans-level-2-the-scanner-what-you39re-building-a-comment-is-a-mode-change-the-scanner-enters-in_line_comment-state-discards-all-characters-until-newline-then-returns-to-normal-no-token-is-emitted-from-the-scanner39s-perspective-the-comment-literally-does-not-exist-it39s-as-if-those-characters-were-whitespace-level-3-the-token-stream-downstream-the-parser-type-checker-and-code-generator-never-see-comments-they-operate-entirely-on-the-token-stream-which-is-comment-free-the-only-place-comments-survive-is-in-specialized-tools-documentation-generators-javadoc-rust39s-doc-comments-ides-building-a-comment-index-and-some-preprocessors-those-tools-run-their-own-passes-over-the-raw-source-separate-from-the-compilation-pipeline-this-three-level-view-reveals-something-interesting-comments-are-stripped-at-level-2-earlier-than-almost-everything-else-they39re-not-quotignored-by-the-parserquot-they-never-reach-the-parser-they-vanish-in-the-scanner"},{"level":2,"text":"Knowledge Cascade â€” &quot;Learn One, Unlock Ten&quot;","id":"knowledge-cascade-quotlearn-one-unlock-tenquot"},{"level":2,"text":"You&#39;ve implemented string scanning with escape sequences, line comments, and block comments. Here&#39;s what those unlock:\nâ†’ JSON parsing â€” you&#39;ve just built the hardest part: JSON&#39;s string parsing is nearly identical to what you implemented. The JSON specification (RFC 8259) defines strings as sequences between &quot;...&quot; with \\n, \\t, \\r, \\&quot;, \\\\, and \\uXXXX escape sequences. Every JSON parser in production â€” Python&#39;s json module, JavaScript&#39;s JSON.parse(), Jackson in Java â€” has a _scan_string() loop that looks exactly like yours. The only addition is \\uXXXX Unicode escapes. You now understand the core of every JSON parser ever written.\nâ†’ Regular expressions have limits â€” and you&#39;ve found the edge: Your IN_STRING and IN_MULTI_COMMENT modes encode context that a single-pass regular expression cannot express. This is not opinion â€” it&#39;s the Pumping Lemma for regular languages, which proves that &quot;string with matching delimiters&quot; is not a regular language. The way you&#39;ve worked around this â€” adding state variables (modes) to extend the DFA â€” is the same workaround used by every real lexer. When someone says &quot;you can&#39;t parse HTML with a regex,&quot; this is the formal reason: HTML&#39;s nested tags are even further beyond regular languages than matched quotes.\nâ†’ Network protocols and escape sequences â€” the same pattern everywhere: HTTP/1.1 uses \\r\\n as a line terminator and percent-encoding (%20 for space) as its escape mechanism. CSV uses &quot;&quot; (doubled quote) to escape a literal quote inside a quoted field. Shell uses \\ for escaping and treats &quot;...&quot; and &#39;...&#39; as distinct quoting modes. SQL uses &#39;&#39; to escape a quote inside a string. Every protocol that transmits text-with-special-characters has an escape mechanism. The structure of your _scan_string() loop â€” &quot;consume characters until delimiter, but treat escape prefix specially&quot; â€” applies to all of them. You can now implement a CSV parser, an HTTP header parser, and a shell tokenizer using the exact same pattern.\n**â†’ Error message quality as UX engineering**: The choice to report unterminated string errors at the opening quote position (not EOF) is a user experience decision disguised as a compiler feature. GCC&#39;s error messages improved dramatically between version 3 and 4 partly because of exactly this kind of positional accuracy work. Clang was designed from the start with &quot;best-in-class error messages&quot; as an explicit goal â€” Clang&#39;s error reporting infrastructure is more complex than many compilers&#39; entire frontends. The insight here is that **where you store and propagate metadata determines the quality of your error messages**. The start_column snapshot you take at the top of _scan_token() is metadata infrastructure. Its value is entirely in the error messages it enables.\n**â†’ Incremental tokenization in language servers**: In your current scanner, scan_tokens() re-processes the entire source every time. In a language server (LSP â€” the Language Server Protocol that powers VS Code&#39;s &quot;go to definition&quot;), tokenizing is triggered on every keypress. For large files, re-tokenizing everything is too slow. Incremental tokenizers track which tokens are &quot;dirty&quot; (potentially affected by the edit) and re-scan only from the last &quot;clean&quot; state. The mode system you&#39;ve built in this milestone is exactly what makes incremental tokenization possible: if an edit is outside a string or comment, the surrounding context is unaffected. If it&#39;s inside a string, only the string changes. Mode boundaries are re-synchronization points.\nâ†’ The formal language hierarchy â€” where you are: The Chomsky hierarchy classifies formal languages into four types. Type 3 (Regular) languages can be recognized by finite automata and expressed as regular expressions. Type 2 (Context-Free) languages need pushdown automata (stack-based machines) â€” this is where most programming language grammars live. Type 1 and Type 0 are more powerful. Your tokenizer extends a DFA with explicit mode-tracking to handle constructs that are technically at the boundary of Type 3. The parser you&#39;d write next would be a full Type 2 machine. Now you can place yourself on the formal language map â€” and understand why the tokenizer and parser are separate stages.","id":"you39ve-implemented-string-scanning-with-escape-sequences-line-comments-and-block-comments-here39s-what-those-unlock-json-parsing-you39ve-just-built-the-hardest-part-json39s-string-parsing-is-nearly-identical-to-what-you-implemented-the-json-specification-rfc-8259-defines-strings-as-sequences-between-quotquot-with-n-t-r-quot-and-uxxxx-escape-sequences-every-json-parser-in-production-python39s-json-module-javascript39s-jsonparse-jackson-in-java-has-a-_scan_string-loop-that-looks-exactly-like-yours-the-only-addition-is-uxxxx-unicode-escapes-you-now-understand-the-core-of-every-json-parser-ever-written-regular-expressions-have-limits-and-you39ve-found-the-edge-your-in_string-and-in_multi_comment-modes-encode-context-that-a-single-pass-regular-expression-cannot-express-this-is-not-opinion-it39s-the-pumping-lemma-for-regular-languages-which-proves-that-quotstring-with-matching-delimitersquot-is-not-a-regular-language-the-way-you39ve-worked-around-this-adding-state-variables-modes-to-extend-the-dfa-is-the-same-workaround-used-by-every-real-lexer-when-someone-says-quotyou-can39t-parse-html-with-a-regexquot-this-is-the-formal-reason-html39s-nested-tags-are-even-further-beyond-regular-languages-than-matched-quotes-network-protocols-and-escape-sequences-the-same-pattern-everywhere-http11-uses-rn-as-a-line-terminator-and-percent-encoding-20-for-space-as-its-escape-mechanism-csv-uses-quotquot-doubled-quote-to-escape-a-literal-quote-inside-a-quoted-field-shell-uses-for-escaping-and-treats-quotquot-and-3939-as-distinct-quoting-modes-sql-uses-3939-to-escape-a-quote-inside-a-string-every-protocol-that-transmits-text-with-special-characters-has-an-escape-mechanism-the-structure-of-your-_scan_string-loop-quotconsume-characters-until-delimiter-but-treat-escape-prefix-speciallyquot-applies-to-all-of-them-you-can-now-implement-a-csv-parser-an-http-header-parser-and-a-shell-tokenizer-using-the-exact-same-pattern-error-message-quality-as-ux-engineering-the-choice-to-report-unterminated-string-errors-at-the-opening-quote-position-not-eof-is-a-user-experience-decision-disguised-as-a-compiler-feature-gcc39s-error-messages-improved-dramatically-between-version-3-and-4-partly-because-of-exactly-this-kind-of-positional-accuracy-work-clang-was-designed-from-the-start-with-quotbest-in-class-error-messagesquot-as-an-explicit-goal-clang39s-error-reporting-infrastructure-is-more-complex-than-many-compilers39-entire-frontends-the-insight-here-is-that-where-you-store-and-propagate-metadata-determines-the-quality-of-your-error-messages-the-start_column-snapshot-you-take-at-the-top-of-_scan_token-is-metadata-infrastructure-its-value-is-entirely-in-the-error-messages-it-enables-incremental-tokenization-in-language-servers-in-your-current-scanner-scan_tokens-re-processes-the-entire-source-every-time-in-a-language-server-lsp-the-language-server-protocol-that-powers-vs-code39s-quotgo-to-definitionquot-tokenizing-is-triggered-on-every-keypress-for-large-files-re-tokenizing-everything-is-too-slow-incremental-tokenizers-track-which-tokens-are-quotdirtyquot-potentially-affected-by-the-edit-and-re-scan-only-from-the-last-quotcleanquot-state-the-mode-system-you39ve-built-in-this-milestone-is-exactly-what-makes-incremental-tokenization-possible-if-an-edit-is-outside-a-string-or-comment-the-surrounding-context-is-unaffected-if-it39s-inside-a-string-only-the-string-changes-mode-boundaries-are-re-synchronization-points-the-formal-language-hierarchy-where-you-are-the-chomsky-hierarchy-classifies-formal-languages-into-four-types-type-3-regular-languages-can-be-recognized-by-finite-automata-and-expressed-as-regular-expressions-type-2-context-free-languages-need-pushdown-automata-stack-based-machines-this-is-where-most-programming-language-grammars-live-type-1-and-type-0-are-more-powerful-your-tokenizer-extends-a-dfa-with-explicit-mode-tracking-to-handle-constructs-that-are-technically-at-the-boundary-of-type-3-the-parser-you39d-write-next-would-be-a-full-type-2-machine-now-you-can-place-yourself-on-the-formal-language-map-and-understand-why-the-tokenizer-and-parser-are-separate-stages"},{"level":2,"text":"Design Decision: Why Store Raw Lexeme, Not Processed Value?","id":"design-decision-why-store-raw-lexeme-not-processed-value"},{"level":2,"text":"What You&#39;ve Built","id":"what-you39ve-built"},{"level":1,"text":"Milestone 4: Integration Testing &amp; Error Recovery","id":"milestone-4-integration-testing-amp-error-recovery"},{"level":2,"text":"Where You&#39;re Starting","id":"where-you39re-starting"},{"level":2,"text":"The Core Misconception: A Lexer Either Succeeds or Fails","id":"the-core-misconception-a-lexer-either-succeeds-or-fails"},{"level":2,"text":"The key insight the Architect flagged: the ERROR token IS a valid token in your token stream. It&#39;s not a special out-of-band signal. It&#39;s a regular entry in the list[Token] returned by scan_tokens(). The scanner produces it, the parser receives it, and the error reporter processes it. Error tokens are part of the API.\nYou&#39;ve actually already implemented this â€” your scanner emits ERROR tokens and keeps running. This milestone is about testing that behavior rigorously, understanding why it matters, and making sure your recovery strategy doesn&#39;t skip too much or too little.","id":"the-key-insight-the-architect-flagged-the-error-token-is-a-valid-token-in-your-token-stream-it39s-not-a-special-out-of-band-signal-it39s-a-regular-entry-in-the-listtoken-returned-by-scan_tokens-the-scanner-produces-it-the-parser-receives-it-and-the-error-reporter-processes-it-error-tokens-are-part-of-the-api-you39ve-actually-already-implemented-this-your-scanner-emits-error-tokens-and-keeps-running-this-milestone-is-about-testing-that-behavior-rigorously-understanding-why-it-matters-and-making-sure-your-recovery-strategy-doesn39t-skip-too-much-or-too-little"},{"level":2,"text":"Error Recovery: The Three Variables","id":"error-recovery-the-three-variables"},{"level":2,"text":"The recovery is implicit in the design. advance() always moves forward by exactly one character, and _scan_token() always returns control to scan_tokens() after one token. There&#39;s no way to get &quot;stuck&quot; on a bad character. Error recovery was free â€” you got it from the architecture.","id":"the-recovery-is-implicit-in-the-design-advance-always-moves-forward-by-exactly-one-character-and-_scan_token-always-returns-control-to-scan_tokens-after-one-token-there39s-no-way-to-get-quotstuckquot-on-a-bad-character-error-recovery-was-free-you-got-it-from-the-architecture"},{"level":2,"text":"Collecting All Errors: The Accumulator Pattern","id":"collecting-all-errors-the-accumulator-pattern"},{"level":2,"text":"But the important point is that scan_tokens() itself doesn&#39;t filter â€” it accumulates everything. The caller decides what to do with errors. A compiler&#39;s driver might print all errors and abort before calling the parser. A language server might pass the entire stream (including errors) to the parser so it can attempt partial analysis for code completion.\nThis &quot;errors as values, not exceptions&quot; design is the same principle behind Python&#39;s Exception classes, Rust&#39;s Result&lt;T, E&gt;, and Go&#39;s (value, error) return tuples. Errors are data. They flow through the system the same way correct results flow. They can be collected, counted, filtered, and reported in batch.","id":"but-the-important-point-is-that-scan_tokens-itself-doesn39t-filter-it-accumulates-everything-the-caller-decides-what-to-do-with-errors-a-compiler39s-driver-might-print-all-errors-and-abort-before-calling-the-parser-a-language-server-might-pass-the-entire-stream-including-errors-to-the-parser-so-it-can-attempt-partial-analysis-for-code-completion-this-quoterrors-as-values-not-exceptionsquot-design-is-the-same-principle-behind-python39s-exception-classes-rust39s-resultltt-egt-and-go39s-value-error-return-tuples-errors-are-data-they-flow-through-the-system-the-same-way-correct-results-flow-they-can-be-collected-counted-filtered-and-reported-in-batch"},{"level":2,"text":"The Canonical Integration Test","id":"the-canonical-integration-test"},{"level":2,"text":"This test catches the bugs that unit tests miss: does the line comment on line 1 correctly produce zero tokens so that if lands on line 2? Does the block comment on line 7 update the line counter so that return lands on line 8? These are the interactions that only surface when the full input runs together.","id":"this-test-catches-the-bugs-that-unit-tests-miss-does-the-line-comment-on-line-1-correctly-produce-zero-tokens-so-that-if-lands-on-line-2-does-the-block-comment-on-line-7-update-the-line-counter-so-that-return-lands-on-line-8-these-are-the-interactions-that-only-surface-when-the-full-input-runs-together"},{"level":2,"text":"Testing Error Recovery Explicitly","id":"testing-error-recovery-explicitly"},{"level":2,"text":"The test test_error_does_not_consume_next_valid_char is particularly important. It verifies that skip-one recovery is truly skip-one â€” it doesn&#39;t accidentally swallow the character after the error. This test would fail if advance() were called twice in the error branch.","id":"the-test-test_error_does_not_consume_next_valid_char-is-particularly-important-it-verifies-that-skip-one-recovery-is-truly-skip-one-it-doesn39t-accidentally-swallow-the-character-after-the-error-this-test-would-fail-if-advance-were-called-twice-in-the-error-branch"},{"level":2,"text":"Edge Cases: The Boundary Tests","id":"edge-cases-the-boundary-tests"},{"level":2,"text":"Position Accuracy Verification: Preventing Drift","id":"position-accuracy-verification-preventing-drift"},{"level":2,"text":"The test_windows_line_endings_not_double_counted test is particularly important. This bug â€” counting \\r\\n as two newlines â€” is remarkably common, and it&#39;s invisible until someone runs your tokenizer on a file edited in Windows Notepad. The test catches it.","id":"the-test_windows_line_endings_not_double_counted-test-is-particularly-important-this-bug-counting-rn-as-two-newlines-is-remarkably-common-and-it39s-invisible-until-someone-runs-your-tokenizer-on-a-file-edited-in-windows-notepad-the-test-catches-it"},{"level":2,"text":"The Complete Integration Test Suite: A Real Program","id":"the-complete-integration-test-suite-a-real-program"},{"level":2,"text":"Performance: The 10,000-Line Benchmark","id":"performance-the-10000-line-benchmark"},{"level":2,"text":"Your implementation already uses the slice approach â€” but if the performance test fails, this is the first thing to check.","id":"your-implementation-already-uses-the-slice-approach-but-if-the-performance-test-fails-this-is-the-first-thing-to-check"},{"level":2,"text":"The Full Test Runner","id":"the-full-test-runner"},{"level":2,"text":"Run this file. Every test should pass. If any fail, the error messages in the assertions will tell you exactly which expectation was violated.","id":"run-this-file-every-test-should-pass-if-any-fail-the-error-messages-in-the-assertions-will-tell-you-exactly-which-expectation-was-violated"},{"level":2,"text":"Three-Level View: What Testing Means at Each Layer","id":"three-level-view-what-testing-means-at-each-layer"},{"level":2,"text":"Common Pitfalls in Integration Testing","id":"common-pitfalls-in-integration-testing"},{"level":3,"text":"Pitfall 1: Testing Only the Happy Path","id":"pitfall-1-testing-only-the-happy-path"},{"level":3,"text":"Pitfall 2: Position Drift That Only Shows Up Late","id":"pitfall-2-position-drift-that-only-shows-up-late"},{"level":3,"text":"Pitfall 3: Asserting Token Count Instead of Token Content","id":"pitfall-3-asserting-token-count-instead-of-token-content"},{"level":2,"text":"Token count tests pass even when the scanner emits KEYWORD instead of IDENTIFIER, or ASSIGN instead of EQUALS. Content tests catch those bugs. Test content, not count.","id":"token-count-tests-pass-even-when-the-scanner-emits-keyword-instead-of-identifier-or-assign-instead-of-equals-content-tests-catch-those-bugs-test-content-not-count"},{"level":2,"text":"Knowledge Cascade â€” &quot;Learn One, Unlock Ten&quot;","id":"knowledge-cascade-quotlearn-one-unlock-tenquot"},{"level":2,"text":"You&#39;ve validated a complete tokenizer with integration tests, error recovery, position accuracy, and performance. Here&#39;s what that knowledge unlocks.\nâ†’ Error recovery in parsers uses the same principle, at a higher level. Your lexer recovers from unknown characters by skipping one character and continuing. Parsers face the same problem with invalid token sequences â€” they hit a token that doesn&#39;t fit any grammar rule and must decide how to continue. The parser-level version of panic mode synchronizes on statement boundaries â€” typically semicolons and closing braces â€” rather than single characters. When a parser sees an unexpected token, it discards tokens until it finds a ; or } that marks the end of a statement, then resumes. Your lexer&#39;s skip-one strategy and the parser&#39;s synchronize-on-boundary strategy are the same algorithm applied at different granularities. If you go on to build a parser, you&#39;ll implement this exact pattern again.\nâ†’ Multi-error reporting is a product decision as much as an engineering one. The Rust compiler&#39;s reputation for excellent error messages is not an accident â€” it was a deliberate design goal, described in early RFC discussions. Rust&#39;s compiler team invested heavily in collecting multiple errors, explaining them in natural language, and suggesting fixes. That quality starts at the lexer, which emits error tokens instead of halting. Go&#39;s compiler also reports multiple errors but is notably more terse. CPython&#39;s tokenizer traditionally stops at the first error (which is why python -c &quot;syntaxerror;another&quot; only reports the first problem). The technical infrastructure for multi-error reporting is the same whether the UX is excellent or poor â€” but Rust put UX investment on top of that infrastructure. Understanding the difference helps you recognize that compiler quality is partly a values decision.\nâ†’ &quot;Expected token stream&quot; tests are how GCC, Clang, and rustc test their frontends. The test pattern you&#39;ve built here â€” assert token_list == expected â€” is called a &quot;golden test&quot; or &quot;snapshot test.&quot; GCC&#39;s test suite contains thousands of source files paired with expected diagnostic output. Clang&#39;s FileCheck utility verifies that specific patterns appear in compiler output. Rust&#39;s compiletest framework runs programs and checks the exact error messages. When you read &quot;this PR fixes a regression in the lexer&#39;s handling of Unicode identifiers&quot; in a compiler changelog, someone added a new golden test that exposed the bug and verified the fix. You&#39;ve built the same infrastructure.\nâ†’ The token stream is an API, and Error tokens are part of that API. Software engineers who build microservices spend a lot of time thinking about error contracts in APIs â€” what does a 400 error mean? What does a 500 error mean? What fields does the error response include? Your token stream has the same structure: ERROR tokens are a defined part of the API contract, with defined fields (lexeme, line, column). A parser written to consume your token stream must handle ERROR tokens gracefully â€” perhaps by skipping them, perhaps by triggering its own error recovery, perhaps by collecting them. The important point is that the parser must handle them, because they&#39;re defined in the contract. Treating errors as values (tokens) rather than exceptions (halts) is the architectural decision that makes this possible.\nâ†’ Lexer performance is I/O bound in practice; CPU-bound in Python. A C or Rust lexer processing a 10,000-line file is typically limited by memory bandwidth â€” the time to read 300KB of data from RAM into cache â€” not by computation. That&#39;s why production lexers can process gigabytes per second: they&#39;re essentially doing a memcpy with some light branching. CPython is different: the interpreter overhead makes pure Python code significantly slower than native code, so a Python lexer IS CPU-bound in practice. If you needed to make your Python lexer faster, the right approach is to move the hot loop to C extension code (as CPython&#39;s own tokenize module does for its scanner) or use PyPy (which JIT-compiles Python to native code and would make your scanner 5â€“10x faster). Understanding what bottleneck you&#39;re fighting â€” memory bandwidth vs. interpreter overhead vs. algorithm complexity â€” changes how you approach optimization.","id":"you39ve-validated-a-complete-tokenizer-with-integration-tests-error-recovery-position-accuracy-and-performance-here39s-what-that-knowledge-unlocks-error-recovery-in-parsers-uses-the-same-principle-at-a-higher-level-your-lexer-recovers-from-unknown-characters-by-skipping-one-character-and-continuing-parsers-face-the-same-problem-with-invalid-token-sequences-they-hit-a-token-that-doesn39t-fit-any-grammar-rule-and-must-decide-how-to-continue-the-parser-level-version-of-panic-mode-synchronizes-on-statement-boundaries-typically-semicolons-and-closing-braces-rather-than-single-characters-when-a-parser-sees-an-unexpected-token-it-discards-tokens-until-it-finds-a-or-that-marks-the-end-of-a-statement-then-resumes-your-lexer39s-skip-one-strategy-and-the-parser39s-synchronize-on-boundary-strategy-are-the-same-algorithm-applied-at-different-granularities-if-you-go-on-to-build-a-parser-you39ll-implement-this-exact-pattern-again-multi-error-reporting-is-a-product-decision-as-much-as-an-engineering-one-the-rust-compiler39s-reputation-for-excellent-error-messages-is-not-an-accident-it-was-a-deliberate-design-goal-described-in-early-rfc-discussions-rust39s-compiler-team-invested-heavily-in-collecting-multiple-errors-explaining-them-in-natural-language-and-suggesting-fixes-that-quality-starts-at-the-lexer-which-emits-error-tokens-instead-of-halting-go39s-compiler-also-reports-multiple-errors-but-is-notably-more-terse-cpython39s-tokenizer-traditionally-stops-at-the-first-error-which-is-why-python-c-quotsyntaxerroranotherquot-only-reports-the-first-problem-the-technical-infrastructure-for-multi-error-reporting-is-the-same-whether-the-ux-is-excellent-or-poor-but-rust-put-ux-investment-on-top-of-that-infrastructure-understanding-the-difference-helps-you-recognize-that-compiler-quality-is-partly-a-values-decision-quotexpected-token-streamquot-tests-are-how-gcc-clang-and-rustc-test-their-frontends-the-test-pattern-you39ve-built-here-assert-token_list-expected-is-called-a-quotgolden-testquot-or-quotsnapshot-testquot-gcc39s-test-suite-contains-thousands-of-source-files-paired-with-expected-diagnostic-output-clang39s-filecheck-utility-verifies-that-specific-patterns-appear-in-compiler-output-rust39s-compiletest-framework-runs-programs-and-checks-the-exact-error-messages-when-you-read-quotthis-pr-fixes-a-regression-in-the-lexer39s-handling-of-unicode-identifiersquot-in-a-compiler-changelog-someone-added-a-new-golden-test-that-exposed-the-bug-and-verified-the-fix-you39ve-built-the-same-infrastructure-the-token-stream-is-an-api-and-error-tokens-are-part-of-that-api-software-engineers-who-build-microservices-spend-a-lot-of-time-thinking-about-error-contracts-in-apis-what-does-a-400-error-mean-what-does-a-500-error-mean-what-fields-does-the-error-response-include-your-token-stream-has-the-same-structure-error-tokens-are-a-defined-part-of-the-api-contract-with-defined-fields-lexeme-line-column-a-parser-written-to-consume-your-token-stream-must-handle-error-tokens-gracefully-perhaps-by-skipping-them-perhaps-by-triggering-its-own-error-recovery-perhaps-by-collecting-them-the-important-point-is-that-the-parser-must-handle-them-because-they39re-defined-in-the-contract-treating-errors-as-values-tokens-rather-than-exceptions-halts-is-the-architectural-decision-that-makes-this-possible-lexer-performance-is-io-bound-in-practice-cpu-bound-in-python-a-c-or-rust-lexer-processing-a-10000-line-file-is-typically-limited-by-memory-bandwidth-the-time-to-read-300kb-of-data-from-ram-into-cache-not-by-computation-that39s-why-production-lexers-can-process-gigabytes-per-second-they39re-essentially-doing-a-memcpy-with-some-light-branching-cpython-is-different-the-interpreter-overhead-makes-pure-python-code-significantly-slower-than-native-code-so-a-python-lexer-is-cpu-bound-in-practice-if-you-needed-to-make-your-python-lexer-faster-the-right-approach-is-to-move-the-hot-loop-to-c-extension-code-as-cpython39s-own-tokenize-module-does-for-its-scanner-or-use-pypy-which-jit-compiles-python-to-native-code-and-would-make-your-scanner-510x-faster-understanding-what-bottleneck-you39re-fighting-memory-bandwidth-vs-interpreter-overhead-vs-algorithm-complexity-changes-how-you-approach-optimization"},{"level":2,"text":"What You&#39;ve Built","id":"what-you39ve-built"},{"level":1,"text":"TDD","id":"tdd"},{"level":1,"text":"MODULE SPECIFICATION: Token Types &amp; Scanner Foundation (tokenizer-m1)","id":"module-specification-token-types-amp-scanner-foundation-tokenizer-m1"},{"level":2,"text":"1. Module Charter","id":"1-module-charter"},{"level":2,"text":"2. File Structure","id":"2-file-structure"},{"level":2,"text":"3. Complete Data Model","id":"3-complete-data-model"},{"level":3,"text":"3.1 TokenType (Enum)","id":"31-tokentype-enum"},{"level":3,"text":"3.2 Token (Dataclass)","id":"32-token-dataclass"},{"level":3,"text":"3.3 Scanner State","id":"33-scanner-state"},{"level":3,"text":"3.4 Memory &amp; Object Layout (Expert Note)","id":"34-memory-amp-object-layout-expert-note"},{"level":2,"text":"4. Interface Contracts","id":"4-interface-contracts"},{"level":3,"text":"4.1 Scanner.init","id":"41-scannerinit"},{"level":3,"text":"4.2 Scanner.advance","id":"42-scanneradvance"},{"level":3,"text":"4.3 Scanner.peek","id":"43-scannerpeek"},{"level":3,"text":"4.4 Scanner.scan_tokens","id":"44-scannerscan_tokens"},{"level":2,"text":"5. Algorithm Specification: The Foundation Scan Loop","id":"5-algorithm-specification-the-foundation-scan-loop"},{"level":3,"text":"5.1 The _scan_token Procedure","id":"51-the-_scan_token-procedure"},{"level":3,"text":"5.2 Position Tracking Invariant","id":"52-position-tracking-invariant"},{"level":2,"text":"6. Error Handling Matrix","id":"6-error-handling-matrix"},{"level":2,"text":"7. Implementation Sequence with Checkpoints","id":"7-implementation-sequence-with-checkpoints"},{"level":3,"text":"Phase 1: Data Structures (1 Hour)","id":"phase-1-data-structures-1-hour"},{"level":3,"text":"Phase 2: Primitive Cursor (1 Hour)","id":"phase-2-primitive-cursor-1-hour"},{"level":3,"text":"Phase 3: Token Generation (1 Hour)","id":"phase-3-token-generation-1-hour"},{"level":3,"text":"Phase 4: Whitespace &amp; Errors (0.5 Hour)","id":"phase-4-whitespace-amp-errors-05-hour"},{"level":2,"text":"8. Test Specification","id":"8-test-specification"},{"level":3,"text":"8.1 Happy Path: Single-Character Symbols","id":"81-happy-path-single-character-symbols"},{"level":3,"text":"8.2 Edge Case: Multi-line Whitespace","id":"82-edge-case-multi-line-whitespace"},{"level":3,"text":"8.3 Failure Case: Invalid Characters","id":"83-failure-case-invalid-characters"},{"level":3,"text":"8.4 Boundary Case: Empty File","id":"84-boundary-case-empty-file"},{"level":2,"text":"9. Performance Targets","id":"9-performance-targets"},{"level":2,"text":"10. Concurrency &amp; Thread Safety","id":"10-concurrency-amp-thread-safety"},{"level":2,"text":"11. Dispatch Map Summary","id":"11-dispatch-map-summary"},{"level":1,"text":"MODULE SPECIFICATION: Multi-Character Tokens &amp; Maximal Munch (tokenizer-m2)","id":"module-specification-multi-character-tokens-amp-maximal-munch-tokenizer-m2"},{"level":2,"text":"1. Module Charter","id":"1-module-charter"},{"level":2,"text":"2. File Structure","id":"2-file-structure"},{"level":2,"text":"3. Complete Data Model","id":"3-complete-data-model"},{"level":3,"text":"3.1 Updated TokenType (Enum)","id":"31-updated-tokentype-enum"},{"level":3,"text":"3.2 Keyword Lookup Table","id":"32-keyword-lookup-table"},{"level":3,"text":"3.3 Scanner Internal State (Logic View)","id":"33-scanner-internal-state-logic-view"},{"level":2,"text":"4. Interface Contracts","id":"4-interface-contracts"},{"level":3,"text":"4.1 _match(expected: str) -&gt; bool","id":"41-_matchexpected-str-gt-bool"},{"level":3,"text":"4.2 _peek_next() -&gt; str","id":"42-_peek_next-gt-str"},{"level":3,"text":"4.3 _scan_number() -&gt; Token","id":"43-_scan_number-gt-token"},{"level":3,"text":"4.4 _scan_identifier() -&gt; Token","id":"44-_scan_identifier-gt-token"},{"level":2,"text":"5. Algorithm Specification","id":"5-algorithm-specification"},{"level":3,"text":"5.1 Maximal Munch Operator Dispatch","id":"51-maximal-munch-operator-dispatch"},{"level":3,"text":"5.2 Floating Point Disambiguation (The Trailing-Dot Guard)","id":"52-floating-point-disambiguation-the-trailing-dot-guard"},{"level":2,"text":"6. Error Handling Matrix","id":"6-error-handling-matrix"},{"level":2,"text":"7. Implementation Sequence with Checkpoints","id":"7-implementation-sequence-with-checkpoints"},{"level":3,"text":"Phase 1: Enum Expansion (0.25 Hours)","id":"phase-1-enum-expansion-025-hours"},{"level":3,"text":"Phase 2: Lookahead Helpers (0.5 Hours)","id":"phase-2-lookahead-helpers-05-hours"},{"level":3,"text":"Phase 3: Operator Dispatch (1 Hour)","id":"phase-3-operator-dispatch-1-hour"},{"level":3,"text":"Phase 4: Literals &amp; Identifiers (1 Hour)","id":"phase-4-literals-amp-identifiers-1-hour"},{"level":2,"text":"8. Test Specification","id":"8-test-specification"},{"level":3,"text":"8.1 Happy Path: Comparison Operators","id":"81-happy-path-comparison-operators"},{"level":3,"text":"8.2 Happy Path: Numbers","id":"82-happy-path-numbers"},{"level":3,"text":"8.3 Happy Path: Identifiers &amp; Keywords","id":"83-happy-path-identifiers-amp-keywords"},{"level":3,"text":"8.4 Edge Case: Maximal Munch Ambiguity","id":"84-edge-case-maximal-munch-ambiguity"},{"level":3,"text":"8.5 Failure Case: Trailing Dot","id":"85-failure-case-trailing-dot"},{"level":2,"text":"9. Performance Targets","id":"9-performance-targets"},{"level":2,"text":"10. Trace Table: Identifier vs Keyword","id":"10-trace-table-identifier-vs-keyword"},{"level":2,"text":"11. Dispatch Logic (Implementation Snippet)","id":"11-dispatch-logic-implementation-snippet"},{"level":1,"text":"MODULE SPECIFICATION: Strings &amp; Comments (tokenizer-m1)","id":"module-specification-strings-amp-comments-tokenizer-m1"},{"level":2,"text":"1. Module Charter","id":"1-module-charter"},{"level":2,"text":"2. File Structure","id":"2-file-structure"},{"level":2,"text":"3. Complete Data Model","id":"3-complete-data-model"},{"level":3,"text":"3.1 Scanner Mode Transitions","id":"31-scanner-mode-transitions"},{"level":3,"text":"3.2 Token Representation for M3","id":"32-token-representation-for-m3"},{"level":2,"text":"4. Interface Contracts","id":"4-interface-contracts"},{"level":3,"text":"4.1 _skip_line_comment(self) -&gt; None","id":"41-_skip_line_commentself-gt-none"},{"level":3,"text":"4.2 _skip_block_comment(self) -&gt; Token | None","id":"42-_skip_block_commentself-gt-token-none"},{"level":3,"text":"4.3 _scan_string(self) -&gt; Token","id":"43-_scan_stringself-gt-token"},{"level":2,"text":"5. Algorithm Specification","id":"5-algorithm-specification"},{"level":3,"text":"5.1 The Three-Way / Dispatch","id":"51-the-three-way-dispatch"},{"level":3,"text":"5.2 Escape Sequence Jump-Over","id":"52-escape-sequence-jump-over"},{"level":3,"text":"5.3 Error Position Pinning","id":"53-error-position-pinning"},{"level":2,"text":"6. Error Handling Matrix","id":"6-error-handling-matrix"},{"level":2,"text":"7. Implementation Sequence with Checkpoints","id":"7-implementation-sequence-with-checkpoints"},{"level":3,"text":"Phase 1: Dispatch Overhaul (0.5 Hours)","id":"phase-1-dispatch-overhaul-05-hours"},{"level":3,"text":"Phase 2: Comment Logic (1 Hour)","id":"phase-2-comment-logic-1-hour"},{"level":3,"text":"Phase 3: String Scanner (1.5 Hours)","id":"phase-3-string-scanner-15-hours"},{"level":3,"text":"Phase 4: Integration &amp; Multi-line (1 Hour)","id":"phase-4-integration-amp-multi-line-1-hour"},{"level":2,"text":"8. Test Specification","id":"8-test-specification"},{"level":3,"text":"8.1 String Escape Isolation","id":"81-string-escape-isolation"},{"level":3,"text":"8.2 Block Comment Boundaries","id":"82-block-comment-boundaries"},{"level":3,"text":"8.3 The &quot;False Comment&quot; Test","id":"83-the-quotfalse-commentquot-test"},{"level":3,"text":"8.4 Unterminated Errors","id":"84-unterminated-errors"},{"level":2,"text":"9. Performance Targets","id":"9-performance-targets"},{"level":2,"text":"10. Concurrency Specification","id":"10-concurrency-specification"},{"level":2,"text":"11. Mode Isolation Logic (Implementation Snippet)","id":"11-mode-isolation-logic-implementation-snippet"},{"level":1,"text":"MODULE SPECIFICATION: Integration Testing &amp; Error Recovery (tokenizer-m4)","id":"module-specification-integration-testing-amp-error-recovery-tokenizer-m4"},{"level":2,"text":"1. Module Charter","id":"1-module-charter"},{"level":2,"text":"2. File Structure","id":"2-file-structure"},{"level":2,"text":"3. Complete Data Model","id":"3-complete-data-model"},{"level":3,"text":"3.1 Test Expectation Schema","id":"31-test-expectation-schema"},{"level":3,"text":"3.2 Position Metadata Mapping","id":"32-position-metadata-mapping"},{"level":2,"text":"4. Interface Contracts","id":"4-interface-contracts"},{"level":3,"text":"4.1 integration_harness.assert_tokens(actual: List[Token], expected: List[TokenExpectation])","id":"41-integration_harnessassert_tokensactual-listtoken-expected-listtokenexpectation"},{"level":3,"text":"4.2 perf_benchmark.generate_synthetic_program(line_count: int) -&gt; str","id":"42-perf_benchmarkgenerate_synthetic_programline_count-int-gt-str"},{"level":3,"text":"4.3 Scanner.scan_tokens (Re-verification)","id":"43-scannerscan_tokens-re-verification"},{"level":2,"text":"5. Algorithm Specification: Error Recovery &amp; Benchmarking","id":"5-algorithm-specification-error-recovery-amp-benchmarking"},{"level":3,"text":"5.1 Skip-One Recovery Trace","id":"51-skip-one-recovery-trace"},{"level":3,"text":"5.2 Performance: Avoiding Quadratic Slicing","id":"52-performance-avoiding-quadratic-slicing"},{"level":2,"text":"6. Error Handling Matrix (Integration Focus)","id":"6-error-handling-matrix-integration-focus"},{"level":2,"text":"7. Implementation Sequence with Checkpoints","id":"7-implementation-sequence-with-checkpoints"},{"level":3,"text":"Phase 1: Canonical Integration (1 Hour)","id":"phase-1-canonical-integration-1-hour"},{"level":3,"text":"Phase 2: Error Recovery Validation (1 Hour)","id":"phase-2-error-recovery-validation-1-hour"},{"level":3,"text":"Phase 3: Position Tracking &amp; CRLF (1 Hour)","id":"phase-3-position-tracking-amp-crlf-1-hour"},{"level":3,"text":"Phase 4: Edge Cases (1 Hour)","id":"phase-4-edge-cases-1-hour"},{"level":3,"text":"Phase 5: Performance Benchmarking (1 Hour)","id":"phase-5-performance-benchmarking-1-hour"},{"level":2,"text":"8. Test Specification (High Precision)","id":"8-test-specification-high-precision"},{"level":3,"text":"8.1 Happy Path: The &quot;Golden Program&quot;","id":"81-happy-path-the-quotgolden-programquot"},{"level":3,"text":"8.2 Edge Case: Max-Length Identifier","id":"82-edge-case-max-length-identifier"},{"level":3,"text":"8.3 Failure Case: Multi-line Comment Unterminated at EOF","id":"83-failure-case-multi-line-comment-unterminated-at-eof"},{"level":3,"text":"8.4 Boundary Case: Whitespace-only file","id":"84-boundary-case-whitespace-only-file"},{"level":2,"text":"9. Performance Targets","id":"9-performance-targets"},{"level":2,"text":"10. Concurrency Specification","id":"10-concurrency-specification"},{"level":2,"text":"11. Implementation Detail: Position Drift Test","id":"11-implementation-detail-position-drift-test"},{"level":2,"text":"12. Skip-One Recovery vs. Multi-Character Logic","id":"12-skip-one-recovery-vs-multi-character-logic"},{"level":2,"text":"13. Comprehensive Canonical Test Program","id":"13-comprehensive-canonical-test-program"},{"level":2,"text":"14. Performance Hotspot Check: The __repr__ Trap","id":"14-performance-hotspot-check-the-__repr__-trap"},{"level":1,"text":"Project Structure: Tokenizer / Lexer","id":"project-structure-tokenizer-lexer"},{"level":2,"text":"Directory Tree","id":"directory-tree"},{"level":2,"text":"Creation Order","id":"creation-order"},{"level":2,"text":"File Count Summary","id":"file-count-summary"},{"level":1,"text":"ðŸ“š Beyond the Atlas: Further Reading","id":"-beyond-the-atlas-further-reading"},{"level":3,"text":"ðŸ§  Foundational Theory: Automata &amp; Formal Languages","id":"-foundational-theory-automata-amp-formal-languages"},{"level":3,"text":"ðŸ› ï¸ The Craft of Lexing: Implementation Guides","id":"-the-craft-of-lexing-implementation-guides"},{"level":3,"text":"ðŸ—ï¸ Real-World Tokenizers: Case Studies","id":"-real-world-tokenizers-case-studies"},{"level":3,"text":"âš ï¸ Error Handling &amp; Recovery","id":"-error-handling-amp-recovery"},{"level":3,"text":"ðŸ§µ Strings, Escapes, and Character Sets","id":"-strings-escapes-and-character-sets"}],"title":"ðŸŽ¯ Project Charter: The C-Like Tokenizer","markdown":"# ðŸŽ¯ Project Charter: The C-Like Tokenizer\n\n## What You Are Building\nA robust, character-by-character lexical scanner that transforms raw source text into a structured stream of categorized tokens. You will implement a Deterministic Finite Automaton (DFA) in code to recognize complex lexemes, including multi-character operators like `>=` using the \"maximal munch\" principle, numeric literals (integers and floats), and string literals with backslash escape sequences.\n\n## Why This Project Exists\nLexical analysis is the \"front door\" of every compiler, interpreter, and syntax highlighter. By building a tokenizer from scratch, you demystify how programming languages resolve ambiguity at the character level and learn why a simple `split(' ')` is insufficient for parsing real-world code.\n\n## What You Will Be Able to Do When Done\n- **Transform Raw Text:** Convert a flat string of source code into a sequence of typed `Token` objects.\n- **Implement Maximal Munch:** Resolve lexical ambiguity (e.g., distinguishing between `>` followed by `=` and the single `>=` operator).\n- **Manage Contextual Modes:** Build a scanner that switches behavior when entering \"string mode\" or \"comment mode.\"\n- **Track Precise Metadata:** Calculate 1-based line and column numbers for every token to power actionable error messages.\n- **Build Resilient Tools:** Implement \"Skip-One\" error recovery so the scanner can report multiple errors in a single pass without halting.\n\n## Final Deliverable\nA standalone `Scanner` module (~250â€“400 lines of code) that processes a multi-line C-like source file. It must pass an integration suite verifying it can tokenize a complex program (containing nested logic, comments, and escaped strings) into a precise list of tokens ending with an `EOF` sentinel.\n\n## Is This Project For You?\n**You should start this if you:**\n- Are comfortable with string indexing and character-by-character loops.\n- Understand how to use Enums and Hash Maps (Dictionaries) in your chosen language.\n- Want to learn the architectural foundations of developer tools and DSLs.\n\n**Come back after you've learned:**\n- Basic data structure implementation (Classes/Structs).\n- [Python Enum Module](https://docs.python.org/3/library/enum.html) (if using the recommended language).\n\n## Estimated Effort\n| Phase | Time |\n|-------|------|\n| **M1: Foundation & Single-Char Tokens** | ~3 hours |\n| **M2: Multi-Character Operators & Numbers** | ~4 hours |\n| **M3: Strings & Comments (Context Tracking)** | ~4 hours |\n| **M4: Integration Testing & Performance** | ~3 hours |\n| **Total** | **~14 hours** |\n\n## Definition of Done\nThe project is complete when:\n- The scanner produces the exact expected token stream for the provided \"Golden Program\" integration test.\n- Every token includes accurate `line` and `column` metadata that does not \"drift\" over a 100+ line file.\n- Unterminated strings or comments produce an `ERROR` token at the *opening* delimiter's position.\n- The scanner processes a 10,000-line synthetic source file in under 1 second.\n- The system emits a final `EOF` token regardless of whether the input was valid or empty.\n\n---\n\n# Tokenizer / Lexer â€” Interactive Atlas\n\nThis project builds a complete character-level lexer for a simple C-like language. You will implement a finite state machine that reads source code one character at a time, applies the maximal munch principle to resolve ambiguities, and emits a categorized stream of tokens. Along the way you will handle string escape sequences, nested comment edge cases, position tracking, and error recovery â€” the same problems faced by every real-world compiler frontend from GCC to V8.\n\nThe tokenizer is the first stage of any compiler or interpreter pipeline: it transforms a flat string of characters into structured tokens that a parser can consume. Understanding this stage deeply unlocks insight into how programming languages actually work, from syntax highlighting in your editor to the error messages your compiler produces.\n\nBy project's end you will have a robust, tested lexer that can tokenize complete multi-line programs, report multiple errors with accurate line/column information, and recover gracefully from invalid input â€” a genuine tool, not a toy.\n\n\n\n<!-- MS_ID: tokenizer-m1 -->\n# Milestone 1: Token Types & Scanner Foundation\n## Where You're Starting\nBefore writing a single line of scanning logic, you need to answer two questions: **What are you scanning for?** and **How do you move through the input?** This milestone answers both.\nYou'll define the complete vocabulary of your tokenizer â€” the `TokenType` enumeration â€” and the data structure that holds a recognized token. Then you'll build the core engine: the `Scanner` class with the two primitive operations (`advance` and `peek`) that everything else is built from. Finally, you'll handle the simplest cases: single-character tokens, whitespace, end-of-file, and error recovery for unrecognized input.\nBy the end of this milestone, you'll have a working system that can scan source code character-by-character and emit a stream of tokens. It won't handle multi-character operators, number literals, strings, or comments yet â€” that's Milestones 2 and 3. But the foundation you lay here determines how cleanly everything else falls into place.\n\n![Tokenizer System â€” Satellite Map (Home Base)](./diagrams/diag-satellite-map.svg)\n\n---\n## The Core Misconception: Tokenizing Is Not Splitting\nYou've almost certainly used Python's `str.split()` before. It takes a string and slices it apart at whitespace (or a delimiter you specify). When you first hear \"a tokenizer breaks source code into tokens,\" it's natural to imagine something similar â€” maybe `source.split()` with some extra logic for operators.\nThis mental model breaks the moment you try it:\n```python\nsource = \"if(x>=42){return true;}\"\ntokens = source.split()\n# Result: [\"if(x>=42){return\", \"true;}\"]\n```\nZero whitespace means zero splits. Two \"tokens.\" Neither is useful.\nBut a real tokenizer handles this correctly â€” producing `Keyword(if)`, `LParen`, `Identifier(x)`, `GreaterEqual`, `Number(42)`, `RParen`, `LBrace`, `Keyword(return)`, `Keyword(true)`, `Semicolon`, `RBrace`, `EOF` â€” with no whitespace anywhere in the input.\n**The real model**: a tokenizer is a character-by-character finite state machine. It does not look for spaces. It reads one character at a time, asks \"what state am I in and what does this character mean?\", and decides: extend the current token, or emit what I have and start fresh. Whitespace is just another character â€” one that happens to trigger \"emit and don't include this character.\" A digit followed by a letter triggers \"emit the number, start an identifier.\" An `=` followed by another `=` triggers \"emit a two-character `==`.\"\nThe boundaries are **state transitions**, not spaces. That's why `x>=42` tokenizes correctly with zero whitespace â€” the scanner changes state every time the character class changes, and those state changes are the boundaries.\nKeep this model in your head as you build: every branch in your scanner's main loop is a state transition in a finite automaton. You are building a DFA (deterministic finite automaton â€” a machine that reads input character by character and transitions between a finite number of states deterministically) compiled to imperative code.\n---\n## Finite State Machines: The Theoretical Heart\n\n> **ðŸ”‘ Foundation: FSMs as the foundation of lexical analysis**\n> \n> **What it IS**\nA Finite State Machine (FSM) is a mathematical model of computation that exists in exactly one of a finite number of **states** at any given time. It moves from one state to another (a **transition**) in response to external inputs. \n\nIn the context of lexical analysis, an FSM acts as a \"character eater.\" It starts in an initial state and consumes characters one by one. If it lands in an **accepting state** (often drawn with a double circle) when the input ends or a delimiter is reached, it has successfully recognized a valid pattern, such as a keyword or an integer. \n\nThere are two primary flavors:\n*   **Deterministic Finite Automata (DFA):** For every state and input, there is exactly one transition to a next state. It is fast and predictable.\n*   **Non-deterministic Finite Automata (NFA):** There can be multiple possible transitions for the same input, or transitions that happen without any input at all. While easier to design for complex patterns, computers usually convert NFAs into DFAs to actually run them.\n\n**WHY you need it right now**\nYou are building a lexer. Without an FSM, your code would likely become a tangled \"if-else\" nightmare of lookaheads and nested loops. FSMs provide a formal, bug-resistant way to map raw text into tokens. Instead of writing logic for every possible edge case, you define the states (e.g., `IN_STRING`, `IN_NUMBER`, `START`) and let the state transitions handle the complexity of the input stream.\n\n**Key Insight: The \"Memoryless\" Model**\nAn FSM doesn't need to remember the entire history of how it got to its current state; the current state *is* the history. If you are in the `IDENTIFIER` state, it doesn't matter if you've read two letters or twentyâ€”the machine only cares about what the next character allows it to do.\n\nThe practical upshot: your scanner has **implicit states** encoded as control flow branches. When you're in the middle of scanning a number, you're in the \"scanning number\" state. When you hit a non-digit, you transition to \"emit number, return to start.\" You won't build an explicit state table in this milestone, but every `if/elif` in your main scan loop IS a state transition. Keep that mapping in mind â€” it explains why the code looks the way it does.\n---\n## Designing the Token Type Enumeration\nYour scanner's job is to categorize every lexeme (raw text fragment) it finds. The categories form a closed, exhaustive set â€” there's no token type \"other than all of these.\" That makes an **enum** the perfect tool.\n\n> **ðŸ”‘ Foundation: Python Enum as a way to define closed**\n> \n> **What it IS**\nA Python `Enum` (from the built-in `enum` module) is a way to create a set of symbolic names bound to unique, constant values. Unlike a standard variable or a string, an Enum represents a **closed set** of possibilities. \n\nUsing `from enum import Enum, auto`, you can define your categories:\n```python\nclass TokenType(Enum):\n    INTEGER = auto()\n    PLUS = auto()\n    EOF = auto()\n```\nThe `auto()` helper assigns unique values to the members so you don't have to manage integers manually.\n\n**WHY you need it right now**\nIn a compiler or lexer, you deal with categories of data (Token Types) constantly. You *could* use \"magic strings\" like `\"INT\"` or `\"PLUS\"`, but strings are dangerous: a single typo like `\"ITN\"` will fail at runtime and can be hard to debug. \n\nEnums provide:\n1.  **Exhaustiveness:** You can use tools like MyPy to ensure your `match` or `if` statements cover every possible token type.\n2.  **Readability:** `TokenType.INTEGER` is self-documenting.\n3.  **Identity over Equality:** Comparing Enums (`token.type is TokenType.INTEGER`) is faster and safer than string comparison.\n\n**Key Insight: The Fixed Universe**\nThink of an Enum as a \"Fixed Universe.\" By using an Enum for your Token Types, you are telling the computer: \"In this program, theseâ€”and ONLY theseâ€”categories exist.\" This prevents \"category creep\" and ensures that your logic remains predictable across the entire pipeline.\n\nHere are the categories your C-like language needs:\n```python\nfrom enum import Enum, auto\nclass TokenType(Enum):\n    # Literals\n    NUMBER     = auto()   # 42, 3.14\n    STRING     = auto()   # \"hello world\"\n    # Names\n    IDENTIFIER = auto()   # x, myVariable, count\n    KEYWORD    = auto()   # if, else, while, return, true, false, null\n    # Operators (single and multi-character)\n    OPERATOR   = auto()   # +, -, *, /, ==, !=, <, >, <=, >=, =\n    # Punctuation / grouping / delimiters\n    PUNCTUATION = auto()  # (, ), {, }, [, ], ;, ,\n    # Control\n    EOF   = auto()   # end of input â€” sentinel for the parser\n    ERROR = auto()   # unrecognized character â€” carries error message\n```\n\n![Token Type Enumeration â€” Complete Category Map](./diagrams/diag-token-type-enum.svg)\n\nA few design notes:\n**Why `OPERATOR` and `PUNCTUATION` as separate categories?** Semantically, operators participate in expressions (they have operands and produce values) while punctuation is purely structural (parentheses group, semicolons terminate). A parser cares about this distinction when building its grammar rules. Grouping them together would make the parser's job harder.\n**Why a single `KEYWORD` type instead of `IF`, `ELSE`, `WHILE`, etc.?** Both approaches are used in real compilers. A flat enum like this is simpler to start with; you look at the lexeme string to know *which* keyword. Many production lexers (including Python's own) use per-keyword variants for faster parser dispatch. For a teaching tokenizer, `KEYWORD` with a lexeme check is clean.\n**Why `EOF` as a token type?** This is critical. When the scanner exhausts the input, it could return `None`, raise an exception, or emit a special sentinel token. `None` forces the parser to null-check every token access. A raised exception makes lookahead awkward. An `EOF` token lets the parser read naturally â€” `while token.type != TokenType.EOF: ...` â€” and the sentinel serves as an unconditional stopping condition. Without it, parsers crash in mysterious ways.\n**Why `ERROR`?** Because the tokenizer should keep running after encountering an unrecognized character. If it raises an exception immediately, you can only report one error per compilation. Instead, emit an `ERROR` token (with position and the offending character), then continue scanning. By the end you've collected *all* the lexical errors in a single pass.\n---\n## The Token Data Structure\nA token is not just a type. It needs to carry enough information for every downstream consumer â€” the parser, the error reporter, the syntax highlighter. The four fields you need are:\n| Field | Type | Purpose |\n|-------|------|---------|\n| `type` | `TokenType` | Category â€” what kind of token is this? |\n| `lexeme` | `str` | The exact raw text from source â€” `\"42\"`, `\">=\"`, `\"if\"` |\n| `line` | `int` | 1-based line number where this token starts |\n| `column` | `int` | 1-based column offset where this token starts |\n```python\nfrom dataclasses import dataclass\n@dataclass\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\n```\n\n![Token Data Structure â€” Memory/Field Layout](./diagrams/diag-token-struct-layout.svg)\n\nA few decisions worth understanding:\n**Why store `lexeme` (raw text) rather than an already-processed value?** Because transforming the lexeme into a value (e.g., converting `\"42\"` to the integer `42`) is the parser's or evaluator's job, not the tokenizer's. The tokenizer's job is categorization. Storing raw text also preserves the exact source characters, which matters for error messages: if you report `unexpected token`, you want to show the user exactly what appeared in their source file, not your normalized version of it.\n**Why `line` and `column` at the token level?** Position metadata transforms useless error messages into actionable ones. `SyntaxError` is worthless. `SyntaxError at line 42, column 7: unexpected ']'` tells the user exactly where to look. This information is essentially free to collect â€” you're scanning character-by-character anyway, so incrementing two counters costs nothing. Failing to collect it here means you'll never have it downstream.\n**Why `@dataclass`?** Python's `dataclass` decorator auto-generates `__init__`, `__repr__`, and `__eq__` from field declarations â€” exactly what you need for a value object like a token. No boilerplate.\n\n![Lifecycle of a Token â€” From Character to Structured Data](./diagrams/diag-token-lifecycle.svg)\n\n---\n## Building the Scanner Infrastructure\nNow the engine. Your `Scanner` class wraps the source string and provides a clean interface for consuming characters. Everything else â€” multi-character operators, number scanning, string literals â€” will be built on top of exactly two primitive operations.\n```python\nclass Scanner:\n    def __init__(self, source: str) -> None:\n        self.source: str = source\n        self.tokens: list[Token] = []\n        # Position in source string\n        self.start: int = 0    # start of the current lexeme being scanned\n        self.current: int = 0  # next character to read\n        # Human-readable position for error messages\n        self.line: int = 1\n        self.column: int = 1\n        # Column at the start of the current lexeme\n        self.start_column: int = 1\n```\nThree index concepts to keep straight:\n- **`start`**: where the current token began (marks the left edge of the lexeme)\n- **`current`**: the position we're about to read next (right edge, exclusive)\n- **`source[start:current]`**: the lexeme accumulated so far\nThink of `start` as pinned to the first character of the token being scanned, and `current` as a cursor advancing one character at a time.\n### The Two Primitives: `advance()` and `peek()`\n\n![Scanner Core: advance() vs peek() â€” Consumption Model](./diagrams/diag-scanner-advance-peek.svg)\n\n```python\ndef is_at_end(self) -> bool:\n    \"\"\"True when the cursor has consumed all input.\"\"\"\n    return self.current >= len(self.source)\ndef advance(self) -> str:\n    \"\"\"Consume the current character and return it. Updates position tracking.\"\"\"\n    char = self.source[self.current]\n    self.current += 1\n    if char == '\\n':\n        self.line += 1\n        self.column = 1\n    else:\n        self.column += 1\n    return char\ndef peek(self) -> str:\n    \"\"\"Return the current character WITHOUT consuming it. Returns '' at end.\"\"\"\n    if self.is_at_end():\n        return ''\n    return self.source[self.current]\n```\nThe asymmetry between `advance` and `peek` is the entire basis for **lookahead**: you can look at the next character to make a decision without committing to consuming it. You'll use this pattern constantly in later milestones â€” \"if the next character is `=`, then this is `==`, not just `=`.\"\n**Why does `peek()` return `''` at end-of-input instead of raising an exception?** Because you'll call `peek()` in conditions: `while peek() != '\\n'` should terminate cleanly at EOF without needing a separate `is_at_end()` check in every loop. An empty string compares `False` to any non-empty character string, so the termination condition handles EOF naturally.\n---\n## Position Tracking: Line and Column\n\n![Line & Column Tracking â€” State Evolution Through Characters](./diagrams/diag-position-tracking.svg)\n\nPosition tracking deserves its own discussion because it's where most beginner implementations drift and produce wrong error messages.\nThe rule is simple: **update position when you consume a character, not when you emit a token.** In `advance()`, after reading a `\\n`, you increment `line` and reset `column` to 1. For any other character, you increment `column`.\nBut there's a subtlety with **token start position**. When you're about to scan a new token, you record the position *before* starting to consume characters. That's why you need `start_column` in addition to `column`:\n```python\ndef _make_token(self, token_type: TokenType) -> Token:\n    \"\"\"Create a token from the characters consumed since `start`.\"\"\"\n    lexeme = self.source[self.start:self.current]\n    return Token(token_type, lexeme, self.line, self.start_column)\n```\nAnd at the beginning of each token scan, you snapshot the current column:\n```python\ndef _scan_token(self) -> Token | None:\n    self.start = self.current\n    self.start_column = self.column  # snapshot BEFORE advance()\n    char = self.advance()\n    # ... dispatch on char\n```\n**Common pitfall â€” Windows line endings (`\\r\\n`)**: The sequence `\\r\\n` is a single logical newline in Windows text files. If you increment `line` for both `\\r` and `\\n`, a Windows file will appear to have twice as many lines as it actually does. The fix: consume `\\r` silently (or as whitespace) and only count `\\n` as a newline. An alternative: normalize the source string to `\\n`-only before scanning begins.\n**Common pitfall â€” Tab characters**: A tab character advances the cursor by one position in the source string but might represent 4 or 8 columns visually. The simplest approach is to count a tab as 1 column (consistent with cursor/byte position) and document that behavior. If you want visual accuracy, use a configurable `tab_width`:\n```python\nelif char == '\\t':\n    self.column += self.tab_width - 1  # -1 because advance() already did +1\n```\nFor this project, advancing tabs by 1 column is fine. Document it and move on.\n---\n## The Main Scanning Loop\nThe `scan_tokens()` method drives the entire process. It's a loop that runs until input is exhausted, calling `_scan_token()` for each token, and then appends the EOF sentinel.\n```python\ndef scan_tokens(self) -> list[Token]:\n    \"\"\"Scan the entire source and return the token list.\"\"\"\n    while not self.is_at_end():\n        token = self._scan_token()\n        if token is not None:  # whitespace returns None\n            self.tokens.append(token)\n    # Always emit EOF as the final sentinel\n    self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))\n    return self.tokens\n```\nNote that `_scan_token()` returns `None` for whitespace â€” consumed and discarded silently. Every other case returns a `Token` object (including errors).\n---\n## Single-Character Token Dispatch\nFor single-character tokens, dispatch is a simple table lookup. Python's `match` statement (3.10+) or a dictionary both work well. Here's the dispatch using a dictionary for clarity:\n```python\n# Class-level constant â€” defined once, not rebuilt every call\n_SINGLE_CHAR_TOKENS: dict[str, TokenType] = {\n    '+': TokenType.OPERATOR,\n    '-': TokenType.OPERATOR,\n    '*': TokenType.OPERATOR,\n    '/': TokenType.OPERATOR,   # NOTE: will need special handling in M3 for comments\n    '(': TokenType.PUNCTUATION,\n    ')': TokenType.PUNCTUATION,\n    '{': TokenType.PUNCTUATION,\n    '}': TokenType.PUNCTUATION,\n    '[': TokenType.PUNCTUATION,\n    ']': TokenType.PUNCTUATION,\n    ';': TokenType.PUNCTUATION,\n    ',': TokenType.PUNCTUATION,\n}\n```\n\n![Single-Character Token Dispatch Table](./diagrams/diag-single-char-dispatch.svg)\n\nAnd the dispatch logic inside `_scan_token()`:\n```python\ndef _scan_token(self) -> Token | None:\n    self.start = self.current\n    self.start_column = self.column\n    char = self.advance()\n    # Single-character tokens\n    if char in self._SINGLE_CHAR_TOKENS:\n        return self._make_token(self._SINGLE_CHAR_TOKENS[char])\n    # Whitespace â€” consume silently\n    elif char in (' ', '\\t', '\\r', '\\n'):\n        return None\n    # Error â€” unrecognized character\n    else:\n        return Token(\n            TokenType.ERROR,\n            char,\n            self.line,\n            self.start_column\n        )\n```\n\n![Whitespace Consumption â€” The Silent Consumer](./diagrams/diag-whitespace-consumption.svg)\n\n**Why put `/` in the single-character dispatch now, knowing it will need special handling for `//` and `/*` comments later?** This is a deliberate scaffolding choice. In Milestone 3, you'll replace the `'/'` entry with a more complex handler. Until then, `/` simply produces a division operator token, which is correct for expressions like `x / 2`. Building incrementally â€” with each milestone adding behavior to a working foundation â€” is better than trying to handle every case at once.\n**Why is `\\r` in the whitespace set?** To handle Windows `\\r\\n` endings. The `\\r` is consumed silently here; the `\\n` will be consumed on the next call to `advance()` and will trigger the line increment. This prevents double-counting.\n---\n## Putting It Together: The Complete M1 Scanner\nHere is the complete, runnable implementation of everything covered in this milestone:\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nclass TokenType(Enum):\n    NUMBER      = auto()\n    STRING      = auto()\n    IDENTIFIER  = auto()\n    KEYWORD     = auto()\n    OPERATOR    = auto()\n    PUNCTUATION = auto()\n    EOF         = auto()\n    ERROR       = auto()\n@dataclass\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\nclass Scanner:\n    _SINGLE_CHAR_TOKENS: dict[str, TokenType] = {\n        '+': TokenType.OPERATOR,\n        '-': TokenType.OPERATOR,\n        '*': TokenType.OPERATOR,\n        '/': TokenType.OPERATOR,\n        '(': TokenType.PUNCTUATION,\n        ')': TokenType.PUNCTUATION,\n        '{': TokenType.PUNCTUATION,\n        '}': TokenType.PUNCTUATION,\n        '[': TokenType.PUNCTUATION,\n        ']': TokenType.PUNCTUATION,\n        ';': TokenType.PUNCTUATION,\n        ',': TokenType.PUNCTUATION,\n    }\n    def __init__(self, source: str, tab_width: int = 1) -> None:\n        self.source = source\n        self.tab_width = tab_width\n        self.tokens: list[Token] = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n        self.start_column = 1\n    # â”€â”€ Primitives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def is_at_end(self) -> bool:\n        return self.current >= len(self.source)\n    def advance(self) -> str:\n        char = self.source[self.current]\n        self.current += 1\n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n        elif char == '\\t':\n            self.column += self.tab_width\n        else:\n            self.column += 1\n        return char\n    def peek(self) -> str:\n        if self.is_at_end():\n            return ''\n        return self.source[self.current]\n    # â”€â”€ Token construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _make_token(self, token_type: TokenType) -> Token:\n        lexeme = self.source[self.start:self.current]\n        return Token(token_type, lexeme, self.line, self.start_column)\n    # â”€â”€ Core scan loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_token(self) -> Token | None:\n        self.start = self.current\n        self.start_column = self.column\n        char = self.advance()\n        if char in self._SINGLE_CHAR_TOKENS:\n            return self._make_token(self._SINGLE_CHAR_TOKENS[char])\n        elif char in (' ', '\\t', '\\r', '\\n'):\n            return None\n        else:\n            return Token(TokenType.ERROR, char, self.line, self.start_column)\n    def scan_tokens(self) -> list[Token]:\n        while not self.is_at_end():\n            token = self._scan_token()\n            if token is not None:\n                self.tokens.append(token)\n        self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))\n        return self.tokens\n```\n---\n## Testing Your Foundation\nBefore moving on, verify each behavior explicitly. Tests document expected behavior and catch regressions when you add new scanning logic in later milestones.\n```python\ndef test_empty_input():\n    scanner = Scanner(\"\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1\ndef test_single_char_operators():\n    scanner = Scanner(\"+-*/\")\n    tokens = scanner.scan_tokens()\n    # 4 operators + EOF\n    assert len(tokens) == 5\n    assert all(t.type == TokenType.OPERATOR for t in tokens[:4])\n    assert tokens[0].lexeme == '+'\n    assert tokens[3].lexeme == '/'\ndef test_punctuation():\n    scanner = Scanner(\"(){};,\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 7  # 6 punctuation + EOF\n    assert all(t.type == TokenType.PUNCTUATION for t in tokens[:6])\ndef test_whitespace_consumed():\n    scanner = Scanner(\"  \\t  \\n  \")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1  # only EOF\n    assert tokens[0].type == TokenType.EOF\ndef test_position_tracking():\n    scanner = Scanner(\"(\\n+\")\n    tokens = scanner.scan_tokens()\n    lparen, plus, eof = tokens\n    assert lparen.type == TokenType.PUNCTUATION\n    assert lparen.line == 1\n    assert lparen.column == 1\n    assert plus.type == TokenType.OPERATOR\n    assert plus.line == 2       # after the newline\n    assert plus.column == 1     # first character on new line\ndef test_error_token():\n    scanner = Scanner(\"@\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2  # ERROR + EOF\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].lexeme == '@'\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1\ndef test_error_then_valid():\n    \"\"\"After an error, scanning continues normally.\"\"\"\n    scanner = Scanner(\"@+\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR   # '@'\n    assert tokens[1].type == TokenType.OPERATOR  # '+'\n    assert tokens[2].type == TokenType.EOF\ndef test_mixed_single_char():\n    scanner = Scanner(\"( ) { } [ ]\")\n    tokens = scanner.scan_tokens()\n    lexemes = [t.lexeme for t in tokens[:-1]]  # exclude EOF\n    assert lexemes == ['(', ')', '{', '}', '[', ']']\ndef test_column_reset_on_newline():\n    scanner = Scanner(\";\\n;\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].column == 1   # first ';' at column 1\n    assert tokens[1].column == 1   # second ';' at column 1 of new line\n    assert tokens[1].line == 2\ndef test_multiple_errors():\n    \"\"\"All errors in input are collected, not just the first.\"\"\"\n    scanner = Scanner(\"@#$\")\n    tokens = scanner.scan_tokens()\n    error_tokens = [t for t in tokens if t.type == TokenType.ERROR]\n    assert len(error_tokens) == 3\n```\nRun all of these. They should pass with the implementation above. If any fail, the position tracking is the most common culprit â€” check the `advance()` method and the `start_column` snapshot timing carefully.\n---\n## Three-Level View: What's Really Happening\nIt's worth seeing this milestone from all three levels of the compiler stack.\n**Level 1 â€” Source Language**: Your user types `(x + 42)`. They see five tokens with three spaces. The spaces are invisible scaffolding â€” boundary hints, but not the actual boundaries.\n**Level 2 â€” The Scanner (what you're building now)**: The scanner reads 8 characters. It has no concept of \"a token\" when it starts â€” it builds up the token character by character, emitting when the state transition fires. `(` â†’ single-char dispatch â†’ emit immediately. `x` â†’ not in dispatch table â†’ (in M2, will become identifier scanner). `+` â†’ single-char â†’ emit. `4` â†’ digit â†’ (in M2, will enter number-scanning state). `)` â†’ single-char â†’ emit.\n**Level 3 â€” The Parser (downstream consumer)**: The parser receives a `list[Token]` and never sees characters again. It thinks in terms of token types, calling something like `consume(TokenType.PUNCTUATION, '(')`. The token stream is the interface contract between lexer and parser. Every mistake in your `TokenType` design forces the parser to work around it.\n---\n## Design Decision: Why `advance()` Updates Position\nYou might wonder: why does `advance()` update `line` and `column`? Why not update them separately, or in `scan_token()`?\n| Approach | Pros | Cons |\n|----------|------|------|\n| **Update in `advance()` (chosen)** | Position is always correct right after consuming a char; no chance of forgetting to update | Must be careful about `start_column` snapshot timing |\n| Update in `scan_token()` after each token | Simpler to reason about | Position is wrong *during* multi-character token scanning â€” string literals spanning lines would report wrong end position |\n| Update lazily (recount from `start`) | Simple `advance()` | O(n) position computation â€” unacceptable for large files |\nUpdating position in `advance()` is the right choice because it keeps position **in sync with the character stream**, not the token stream. Multi-character tokens (strings, comments, identifiers) span multiple characters. If you only updated position at token boundaries, position tracking inside those tokens would be wrong.\n---\n## Knowledge Cascade â€” \"Learn One, Unlock Ten\"\nYou've built a character-level scanner. Here's what this unlocks:\n**â†’ Formal automata theory**: Your scanner IS a DFA implementation. The `_SINGLE_CHAR_TOKENS` dispatch table is the DFA's transition function for the \"start\" state. The `if/elif/else` structure in `_scan_token()` enumerates all transitions from that state. When you add multi-character operators in M2, you'll add transitions to sub-states (the \"seen one `=`\" state). Formal language theory tells you that any regular language â€” the class that contains all tokenizable languages â€” can be recognized by a DFA. Your C-like language's lexical grammar is regular; that's *why* this approach works.\n**â†’ Parser expectations (downstream contract)**: The token stream you emit in this milestone is the alphabet for the parser that consumes it. If your `TokenType` categories are coarse (e.g., mixing `KEYWORD` and `IDENTIFIER` into a single `NAME` type), the parser must inspect every `NAME` token's lexeme to decide what grammar rule applies. If your categories are fine-grained, the parser can dispatch purely on type. The parser is written against the contract you establish here â€” changing `TokenType` in M3 would require changing the parser too.\n**â†’ Syntax highlighting in your editor**: Every editor that does syntax highlighting (VS Code, Vim, Emacs) runs a tokenizer on every keystroke. The \"confused mid-string\" highlighting you've seen â€” where adding an unmatched `\"` suddenly turns half your file orange â€” is exactly the `ERROR` and `STRING` token types interacting. When the tokenizer can't find the closing quote, it emits an unterminated string that swallows everything until the next `\"`. Now you know why. You also know the fix: language servers use incremental re-tokenization â€” re-scan only from the changed position â€” to handle this efficiently.\n**â†’ The linker's symbol table (cross-domain)**: The keyword lookup table you'll build in M2 (mapping strings like `\"if\"` to `TokenType.KEYWORD`) is the same fundamental data structure as a linker's symbol table â€” a hash map from name to meaning. In the linker, it maps symbol names to addresses. In your tokenizer, it maps lexeme strings to token types. Hash tables as \"name â†’ meaning\" registries are one of the most universal data structures in systems programming.\n**â†’ Error recovery as a design philosophy**: You made `ERROR` a token type rather than an exception. This is the same philosophy behind Rust's `Result<T, E>` â€” errors are values, not exceptional control flow. Languages that lean on exceptions for lexical errors (like early implementations of many scripting languages) can only report one error per run. Compilers that treat errors as values (GCC, Clang, Rust's compiler) collect all errors and report them all at once. Your choice here reflects that philosophy at the smallest scale.\n**â†’ Position metadata as a universal pattern**: The `line` and `column` you're tracking are metadata â€” information about the data rather than the data itself. This pattern appears everywhere: HTTP headers (metadata about the body), database index pages (metadata about row locations), Git commits (metadata about diffs). Collecting metadata eagerly and cheaply, while the primary computation is happening anyway, is a general principle. Retrofitting it later is always expensive.\n---\n## What You've Built\nYou now have a functioning scanner skeleton. It can:\n- Categorize every lexeme in a closed type system (`TokenType`)\n- Store tokens with full position information (`Token`)\n- Consume source code character-by-character with `advance()`\n- Look ahead without consuming with `peek()`\n- Track `line` and `column` accurately through newlines and tabs\n- Emit single-character operator and punctuation tokens\n- Silently consume whitespace\n- Emit a sentinel `EOF` token when input is exhausted\n- Emit `ERROR` tokens for unrecognized characters and continue scanning\nIn Milestone 2, you'll build on `peek()` to implement multi-character operators (`==`, `!=`, `<=`, `>=`), number literal scanning, and identifier + keyword recognition. Everything in M2 uses `advance()` and `peek()` as its only interface to the source string â€” the primitives you built here carry the entire scanner.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m2 -->\n<!-- MS_ID: tokenizer-m2 -->\n# Milestone 2: Multi-Character Tokens & Maximal Munch\n## Where You're Starting\nYou have a working scanner skeleton. It reads characters, tracks positions, emits single-character tokens, swallows whitespace, and reports errors â€” all in under 100 lines of clean Python. The foundation is solid.\nBut try to scan `x >= 42` with what you've built and you'll get: `Identifier(x)`, `OPERATOR(>)`, `ERROR(=)`, `Number(42)`. The `>=` falls apart into two pieces because your scanner doesn't look ahead. That broken output would crash any parser trying to consume it.\nThis milestone fixes that â€” and in doing so, teaches you the single most important principle in lexical analysis: **maximal munch**.\nBy the end of M2, your scanner will correctly handle:\n- Two-character operators (`==`, `!=`, `<=`, `>=`) as single indivisible tokens\n- Integer and floating-point number literals\n- Identifiers and keyword recognition via a lookup table\n- The tricky edge case `>==` (maximal munch gives you `>=` then `=`, never `>` then `==`)\nEverything builds on `advance()` and `peek()` â€” the two primitives you already have.\n\n![Tokenizer System â€” Satellite Map (Home Base)](./diagrams/diag-satellite-map.svg)\n\n---\n## The Core Misconception: Characters Know What They Are\nHere's what most beginners instinctively do when they see `=`:\n```python\n# âŒ The intuitive but wrong approach\nif char == '=':\n    return self._make_token(TokenType.ASSIGN)\n```\nIt feels right. You saw `=`, you know what `=` is, you emit `ASSIGN`. Done.\nExcept it's not done. `=` doesn't exist in isolation â€” it exists in a stream. The character *after* it changes everything:\n- `=` followed by `=` â†’ `==` (equality comparison, `GreaterEqual`)\n- `=` followed by anything else â†’ `=` (assignment, `ASSIGN`)\nThe moment you emit `ASSIGN` without checking what comes next, you've made an irrecoverable commitment. You can't un-emit a token. You can't \"put it back.\" The parser will see `ASSIGN` + `ASSIGN` instead of `EQUALS` and it will fail â€” or worse, silently misparse the code.\n**The real model**: a character's identity is not determined by what it is alone, but by what it is *plus* the context of what follows. This is the insight maximal munch encodes.\n---\n## The Maximal Munch Principle\n\n![Maximal Munch â€” Why Greedy Wins](./diagrams/diag-maximal-munch-principle.svg)\n\nMaximal munch is a rule for resolving ambiguity in lexical analysis: **always consume the longest sequence of characters that forms a valid token.**\nWhen you reach a character that *could* begin multiple different tokens, you don't emit immediately. You look ahead. If the next character extends the current token into a longer valid token, you consume it. You keep extending until the next character would break the pattern. Only then do you emit.\nFormally: among all possible tokenizations of the input, choose the one where each token is as long as possible.\nHere's the same input, two strategies side by side:\n```\nInput: \"==\"\nGreedy (correct, maximal munch):\n  Read '=' â†’ could be ASSIGN or start of EQUALS\n  Peek '=' â†’ extends to EQUALS, consume it\n  Peek ';' â†’ stops, emit EQUALS(\"==\")\nNon-greedy (wrong):\n  Read '=' â†’ emit ASSIGN(\"=\")\n  Read '=' â†’ emit ASSIGN(\"=\")\n  Result: two ASSIGN tokens â€” broken\n```\nThe principle sounds obvious when stated plainly, but implementing it requires discipline: **every character that might begin a multi-character token must peek before emitting.**\n### Connection to Regular Expression Engines\nMaximal munch is not an invention of compiler theory. You've already seen it in action if you've used regular expressions.\nWhen you write the pattern `a+` (one or more `a`s) and apply it to the string `\"aaab\"`, the regex engine matches `\"aaa\"` â€” not just `\"a\"`. That behavior is called **greedy matching**, and it is exactly maximal munch. The engine consumes as many `a`s as possible before stopping.\nEvery regex engine implements maximal munch by default. Understanding this connection means you now understand two things for the price of one: regex greediness and lexer tokenization are the same algorithmic principle applied in two different contexts.\n> ðŸ”­ **Deep Dive**: If you want to understand the formal connection between regular expressions and finite automata (the mathematical machinery underneath both), see Sipser's *Introduction to the Theory of Computation*, Chapter 1. The key result â€” that any pattern expressible as a regex can be recognized by a DFA â€” is the theorem that makes lexical analysis tractable.\n---\n## Why the Parser Needs This\nBefore diving into implementation, understand *why* the parser needs `==` as a single token.\nConsider a grammar rule for comparison:\n```\nequality_expr := expr '==' expr\n```\nThe parser consumes tokens one at a time. When it's looking for an `==`, it calls something like `consume(TokenType.EQUALS)`. If your lexer emits two `ASSIGN` tokens instead, the parser receives `ASSIGN` + `ASSIGN` and has no rule that matches â€” it fails. Or worse: it misparses.\nIn a language where `a = b` means assignment and `a == b` means equality check, the *single-token vs. two-token distinction is the semantic difference between modifying state and reading state*. Getting it wrong doesn't just produce a syntax error â€” in a more permissive parser, it could silently change what the program means.\nThe lexer's job is to make that distinction correctly and irreversibly. The parser trusts the token stream completely.\n---\n## Implementing Two-Character Operators with Lookahead\n\n![Lookahead Decision Tree for '=', '!', '<', '>'](./diagrams/diag-lookahead-decision-tree.svg)\n\nThe pattern for every two-character operator is identical: consume the first character, peek at the second, branch on whether the second extends the token.\nHere's a helper method that encodes this pattern once:\n```python\ndef _match(self, expected: str) -> bool:\n    \"\"\"\n    If the current character (not yet consumed) equals `expected`,\n    consume it and return True. Otherwise return False.\n    This is the single-character lookahead for maximal munch.\n    \"\"\"\n    if self.is_at_end():\n        return False\n    if self.source[self.current] != expected:\n        return False\n    # It matches â€” consume it\n    self.advance()\n    return True\n```\n`_match()` is \"conditional advance\": it peeks, and if the character is what you expect, it consumes. This is your lookahead primitive for the two-character operator cases.\nNow add the operator dispatch to `_scan_token()`. Replace the existing `_SINGLE_CHAR_TOKENS` dispatch for the ambiguous characters (`=`, `!`, `<`, `>`) with branching logic:\n```python\ndef _scan_token(self) -> Token | None:\n    self.start = self.current\n    self.start_column = self.column\n    char = self.advance()\n    # â”€â”€ Two-character operator candidates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if char == '=':\n        return self._make_token(\n            TokenType.EQUALS if self._match('=') else TokenType.ASSIGN\n        )\n    elif char == '!':\n        if self._match('='):\n            return self._make_token(TokenType.NOT_EQUAL)\n        else:\n            # '!' alone is not a valid token in our C-like language\n            return Token(TokenType.ERROR, char, self.line, self.start_column)\n    elif char == '<':\n        return self._make_token(\n            TokenType.LESS_EQUAL if self._match('=') else TokenType.LESS_THAN\n        )\n    elif char == '>':\n        return self._make_token(\n            TokenType.GREATER_EQUAL if self._match('=') else TokenType.GREATER_THAN\n        )\n    # â”€â”€ Single-character tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    elif char in self._SINGLE_CHAR_TOKENS:\n        return self._make_token(self._SINGLE_CHAR_TOKENS[char])\n    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    elif char in (' ', '\\t', '\\r', '\\n'):\n        return None\n    # â”€â”€ Error â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    else:\n        return Token(TokenType.ERROR, char, self.line, self.start_column)\n```\nYou'll also need to extend `TokenType` with the new operator variants. Update the enum:\n```python\nclass TokenType(Enum):\n    NUMBER       = auto()\n    STRING       = auto()\n    IDENTIFIER   = auto()\n    KEYWORD      = auto()\n    # Distinguish operators semantically\n    OPERATOR     = auto()   # arithmetic: +, -, *, /\n    ASSIGN       = auto()   # =\n    EQUALS       = auto()   # ==\n    NOT_EQUAL    = auto()   # !=\n    LESS_THAN    = auto()   # <\n    LESS_EQUAL   = auto()   # <=\n    GREATER_THAN = auto()   # >\n    GREATER_EQUAL = auto()  # >=\n    PUNCTUATION  = auto()\n    EOF          = auto()\n    ERROR        = auto()\n```\n> **Design note â€” Why split operators into individual types instead of keeping a single `OPERATOR`?**\n> Either approach works for the *tokenizer*, but the parser downstream benefits enormously from distinct types. A parser rule for `comparison := expr ('<' | '<=' | '>' | '>=' | '==' | '!=') expr` can dispatch on `token.type` directly. With a single `OPERATOR` type, every parser rule would need to check `token.lexeme` to distinguish `<` from `<=` from `==`. Individual token types move that discrimination cost to where it's cheapest: the lexer, which is already reading character-by-character.\n---\n## The `>==` Case: Maximal Munch in Action\nLet's trace through `>==` step by step. This is the canonical test for whether your maximal munch implementation is correct.\n\n![Tricky Input: '>==' â€” Maximal Munch Step-by-Step](./diagrams/diag-gteqeq-trace.svg)\n\n```\nInput: \">==\"\n       ^\n       current=0, start=0\nStep 1: _scan_token() called\n  start = 0, start_column = 1\n  advance() â†’ returns '>', current=1, column=2\n  char == '>' â†’ branch to '>'-handler\n  _match('='):\n    source[1] == '=' â†’ YES\n    advance() â†’ consumes '=', current=2, column=3\n    returns True\n  _make_token(GREATER_EQUAL)\n  lexeme = source[0:2] = \">=\"\n  â†’ emit Token(GREATER_EQUAL, \">=\", 1, 1)\nStep 2: _scan_token() called again\n  start = 2, start_column = 3\n  advance() â†’ returns '=', current=3, column=4\n  char == '=' â†’ branch to '='-handler\n  _match('='):\n    is_at_end() â†’ True (current=3 == len(\">==\"))\n    returns False\n  _make_token(ASSIGN)\n  lexeme = source[2:3] = \"=\"\n  â†’ emit Token(ASSIGN, \"=\", 1, 3)\nFinal token stream: [GREATER_EQUAL(\">=\"), ASSIGN(\"=\"), EOF]\n```\nThe key moment is in Step 1: when `_match('=')` is called, it looks at `source[1]` which is `=` â€” the *second* `=` in the input. It consumes that character (current advances to 2), locking in `>=` as `GREATER_EQUAL`. Then in Step 2, the scanner starts fresh at position 2, reads the *third* character (`=`), peeks ahead (end of input), and emits `ASSIGN`.\nThis is exactly right. `>==` is unambiguously `>=` followed by `=` under maximal munch. A human programmer writing `if (x >= = 1)` would get a parse error (not a lex error), but the tokenizer's job is correct regardless of what the parser does with it.\n---\n## Number Literal Scanning\n\n![Number Literal Scanning â€” Finite State Machine](./diagrams/diag-number-scanning-fsm.svg)\n\nNumber literals are the first case where you need to stay in a scanning loop â€” consuming multiple characters before emitting. The FSM has three states:\n- **INTEGER**: consuming digits\n- **SAW_DOT**: consumed a dot after digits (might be float)\n- **FLOAT**: consuming digits after the dot\nHere's the scanning logic:\n```python\ndef _scan_number(self) -> Token:\n    \"\"\"\n    Called after consuming the first digit. Scans the rest of the number.\n    Handles integers (42, 0) and floats (3.14).\n    \"\"\"\n    # Consume remaining integer digits\n    while peek_is_digit(self.peek()):\n        self.advance()\n    # Check for a decimal point followed by more digits (float)\n    if self.peek() == '.' and peek_is_digit(self._peek_next()):\n        self.advance()  # consume the '.'\n        while peek_is_digit(self.peek()):\n            self.advance()\n    return self._make_token(TokenType.NUMBER)\n```\nYou need two helper functions and a `_peek_next()` method:\n```python\ndef _peek_next(self) -> str:\n    \"\"\"\n    Look TWO characters ahead without consuming. Returns '' at/past end.\n    Used to distinguish '3.' (trailing dot, integer) from '3.14' (float).\n    \"\"\"\n    if self.current + 1 >= len(self.source):\n        return ''\n    return self.source[self.current + 1]\n```\n```python\ndef peek_is_digit(ch: str) -> bool:\n    return ch.isdigit()  # or: ch in '0123456789'\n```\nAnd integrate into `_scan_token()`:\n```python\nelif char.isdigit():\n    return self._scan_number()\n```\n### The Trailing-Dot Decision\nWhat should your scanner do with `3.`? There's a dot after the digits, but no digits after the dot. You have three options:\n| Decision | Example | Emits | Used By |\n|----------|---------|-------|---------|\n| **Reject (chosen âœ“)** | `3.` | `NUMBER(3)` then `PUNCTUATION(.)` | This project â€” simplest, consistent |\n| Accept as float | `3.` | `NUMBER(3.)` | JavaScript, Python |\n| Reject with error | `3.` | `NUMBER(3)` + ERROR or just NUMBER(3) | Some strict compilers |\nThe condition `self.peek() == '.' and peek_is_digit(self._peek_next())` implements the reject-and-backtrack approach: you only consume the dot if a digit follows it. If `3.` appears in source, the scanner emits `NUMBER(\"3\")` and leaves the `.` for the next call â€” which will see a lone `.` and emit an `ERROR` token (since `.` alone isn't a valid token in this C-like language).\nThis uses **two characters of lookahead** (`peek()` for the dot, `_peek_next()` for the digit after it) â€” a brief excursion beyond LA(1). This is common in real lexers; the formal claim that \"lexers use LA(1)\" is a simplification. The important point: you document the behavior and apply it consistently.\n> **What about leading dots?** `.5` as a float literal? Not supported in this language. If the scanner sees `.` in the character dispatch, it's not a digit, so it won't reach `_scan_number()`. It will fall through to the error case. This is a deliberate design choice â€” document it, and it's a valid language decision.\n---\n## Identifier Scanning and the Keyword Lookup Table\n\n![Keyword Detection â€” Scan First, Lookup Second](./diagrams/diag-keyword-vs-identifier.svg)\n\nIdentifiers â€” the variable names, function names, and type names a programmer writes â€” follow a simple rule: start with a letter or underscore, then any mix of letters, digits, and underscores.\n```python\ndef _scan_identifier(self) -> Token:\n    \"\"\"\n    Called after consuming the first character (letter or underscore).\n    Scans the rest of the identifier, then checks the keyword table.\n    \"\"\"\n    while self.peek().isalnum() or self.peek() == '_':\n        self.advance()\n    # Extract the complete identifier text\n    text = self.source[self.start:self.current]\n    # Keyword check â€” lookup AFTER scanning the full identifier\n    token_type = self._KEYWORDS.get(text, TokenType.IDENTIFIER)\n    return self._make_token(token_type)\n```\nThe keyword table:\n```python\n_KEYWORDS: dict[str, TokenType] = {\n    'if':     TokenType.KEYWORD,\n    'else':   TokenType.KEYWORD,\n    'while':  TokenType.KEYWORD,\n    'return': TokenType.KEYWORD,\n    'true':   TokenType.KEYWORD,\n    'false':  TokenType.KEYWORD,\n    'null':   TokenType.KEYWORD,\n}\n```\nIntegrate into `_scan_token()`:\n```python\nelif char.isalpha() or char == '_':\n    return self._scan_identifier()\n```\n### The Critical Rule: Scan First, Lookup Second\nThis ordering â€” scan the complete identifier, *then* check if it's a keyword â€” is not just implementation convenience. It is the **correct** approach, and doing it in the opposite order causes a real bug.\nImagine a naive implementation that checks for keywords character-by-character:\n```python\n# âŒ WRONG â€” checking mid-scan\nelif char == 'i':\n    if self.peek() == 'f':\n        self.advance()\n        return self._make_token(TokenType.KEYWORD)  # emits 'if' keyword\n```\nWhat happens with the identifier `iffy`?\n- Scanner reads `i`\n- Peeks `f` â†’ triggers keyword match for `if`\n- Emits `KEYWORD(\"if\")`\n- Continues from `fy` â†’ emits `IDENTIFIER(\"fy\")`\nWrong. `iffy` is a single identifier. The scanner has silently broken it.\nThe scan-first approach handles this correctly:\n```python\n# Input: \"iffy\"\n# _scan_identifier() consumes 'i', 'f', 'f', 'y'\n# text = \"iffy\"\n# _KEYWORDS.get(\"iffy\", IDENTIFIER) â†’ IDENTIFIER (not in table)\n# â†’ emit IDENTIFIER(\"iffy\")  âœ“\n```\nAnd for actual `if`:\n```python\n# Input: \"if \"\n# _scan_identifier() consumes 'i', 'f'\n# peek() == ' ' â†’ not alnum or _, stops\n# text = \"if\"\n# _KEYWORDS.get(\"if\", IDENTIFIER) â†’ KEYWORD\n# â†’ emit KEYWORD(\"if\")  âœ“\n```\nThe identifier scanner is greedy (maximal munch again): it consumes every valid identifier character before stopping. The keyword check is a post-processing step on the complete text. This guarantees that `iffy`, `iff`, `i`, `if_`, and `if` are all handled correctly by the same code path.\n### Why a Dictionary, Not a Long `if/elif` Chain?\nYou could write:\n```python\nif text == 'if':\n    return self._make_token(TokenType.KEYWORD)\nelif text == 'else':\n    return self._make_token(TokenType.KEYWORD)\n# ... etc.\n```\nA dictionary lookup is strictly better:\n- **O(1) average** versus O(n) for a chain of n keywords\n- **Extensible**: adding a keyword is one line in the dict, not a new `elif`\n- **Data separate from logic**: the keyword table can be loaded from configuration or extended by a macro system without touching scanner code\nProduction lexers (GCC, Clang, the CPython tokenizer) all use hash maps for keyword lookup. Some use **perfect hashing** â€” a hash function computed specifically for the known keyword set â€” to guarantee O(1) with no collision chains. For seven keywords, a plain Python dict is more than fast enough.\n---\n## The Lookahead Budget: LA(1)\nYour lexer is described as using **LA(1)** â€” one character of lookahead. This is worth understanding precisely, because it explains why your design works and where its limits are.\nLA(k) means: to decide what token to emit, you need to look at most *k* characters beyond the current position. In your scanner:\n- Single-character tokens: LA(0) â€” you know what to emit the moment you consume the character\n- Two-character operators: LA(1) â€” you consume one char, peek one more\n- Number literals: LA(1) for the integer path, LA(2) for the trailing-dot check (is it `3.1` or `3.`)\n- Identifiers: LA(1) â€” peek one char ahead to see if the identifier continues\nThe LA(2) case for number literals is a pragmatic extension. Purists could eliminate it by accepting `3.` as a float, but that changes the language semantics. In practice, LA(1) is the *default* and LA(2) is the *exception for one specific disambiguation*. Real lexers like Clang's also occasionally peek two characters ahead for specific cases.\n> **Formal note**: If your language's lexical grammar requires unbounded lookahead to tokenize, you can't implement it with a finite state machine â€” you'd need a pushdown automaton, which is for context-free languages. The fact that our lexer works with LA(1) or LA(2) confirms that the lexical grammar is regular (or close enough to it). Parsers, which handle hierarchical structure, need LA(k) for tokens (not characters) and are where full context-free power is needed.\n---\n## Complete M2 `_scan_token()` â€” Putting It All Together\nHere is the full updated `_scan_token()` and all helpers:\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nclass TokenType(Enum):\n    # Literals\n    NUMBER        = auto()\n    STRING        = auto()\n    # Names\n    IDENTIFIER    = auto()\n    KEYWORD       = auto()\n    # Arithmetic operators\n    OPERATOR      = auto()   # +, -, *, /\n    # Comparison & assignment operators (split for parser convenience)\n    ASSIGN        = auto()   # =\n    EQUALS        = auto()   # ==\n    NOT_EQUAL     = auto()   # !=\n    LESS_THAN     = auto()   # <\n    LESS_EQUAL    = auto()   # <=\n    GREATER_THAN  = auto()   # >\n    GREATER_EQUAL = auto()   # >=\n    # Structural\n    PUNCTUATION   = auto()\n    # Control\n    EOF           = auto()\n    ERROR         = auto()\n@dataclass\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\nclass Scanner:\n    _SINGLE_CHAR_TOKENS: dict[str, TokenType] = {\n        '+': TokenType.OPERATOR,\n        '-': TokenType.OPERATOR,\n        '*': TokenType.OPERATOR,\n        '/': TokenType.OPERATOR,   # will become comment-aware in M3\n        '(': TokenType.PUNCTUATION,\n        ')': TokenType.PUNCTUATION,\n        '{': TokenType.PUNCTUATION,\n        '}': TokenType.PUNCTUATION,\n        '[': TokenType.PUNCTUATION,\n        ']': TokenType.PUNCTUATION,\n        ';': TokenType.PUNCTUATION,\n        ',': TokenType.PUNCTUATION,\n    }\n    _KEYWORDS: dict[str, TokenType] = {\n        'if':     TokenType.KEYWORD,\n        'else':   TokenType.KEYWORD,\n        'while':  TokenType.KEYWORD,\n        'return': TokenType.KEYWORD,\n        'true':   TokenType.KEYWORD,\n        'false':  TokenType.KEYWORD,\n        'null':   TokenType.KEYWORD,\n    }\n    def __init__(self, source: str, tab_width: int = 1) -> None:\n        self.source = source\n        self.tab_width = tab_width\n        self.tokens: list[Token] = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n        self.start_column = 1\n    # â”€â”€ Primitives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def is_at_end(self) -> bool:\n        return self.current >= len(self.source)\n    def advance(self) -> str:\n        char = self.source[self.current]\n        self.current += 1\n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n        elif char == '\\t':\n            self.column += self.tab_width\n        else:\n            self.column += 1\n        return char\n    def peek(self) -> str:\n        \"\"\"Current character, not consumed. Returns '' at end.\"\"\"\n        if self.is_at_end():\n            return ''\n        return self.source[self.current]\n    def _peek_next(self) -> str:\n        \"\"\"One character beyond current, not consumed. Returns '' at/past end.\"\"\"\n        if self.current + 1 >= len(self.source):\n            return ''\n        return self.source[self.current + 1]\n    def _match(self, expected: str) -> bool:\n        \"\"\"Consume current character if it equals `expected`. Return success.\"\"\"\n        if self.is_at_end():\n            return False\n        if self.source[self.current] != expected:\n            return False\n        self.advance()\n        return True\n    # â”€â”€ Token construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _make_token(self, token_type: TokenType) -> Token:\n        lexeme = self.source[self.start:self.current]\n        return Token(token_type, lexeme, self.line, self.start_column)\n    # â”€â”€ Scanners for multi-character tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_number(self) -> Token:\n        \"\"\"Scans remaining digits after the first digit has been consumed.\"\"\"\n        while self.peek().isdigit():\n            self.advance()\n        # Float detection: dot must be followed by at least one digit\n        if self.peek() == '.' and self._peek_next().isdigit():\n            self.advance()  # consume '.'\n            while self.peek().isdigit():\n                self.advance()\n        return self._make_token(TokenType.NUMBER)\n    def _scan_identifier(self) -> Token:\n        \"\"\"Scans remaining identifier chars after the first has been consumed.\"\"\"\n        while self.peek().isalnum() or self.peek() == '_':\n            self.advance()\n        text = self.source[self.start:self.current]\n        token_type = self._KEYWORDS.get(text, TokenType.IDENTIFIER)\n        return self._make_token(token_type)\n    # â”€â”€ Core scan loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_token(self) -> Token | None:\n        self.start = self.current\n        self.start_column = self.column\n        char = self.advance()\n        # Two-character operators (must check before single-char table)\n        if char == '=':\n            return self._make_token(\n                TokenType.EQUALS if self._match('=') else TokenType.ASSIGN\n            )\n        elif char == '!':\n            if self._match('='):\n                return self._make_token(TokenType.NOT_EQUAL)\n            return Token(TokenType.ERROR, char, self.line, self.start_column)\n        elif char == '<':\n            return self._make_token(\n                TokenType.LESS_EQUAL if self._match('=') else TokenType.LESS_THAN\n            )\n        elif char == '>':\n            return self._make_token(\n                TokenType.GREATER_EQUAL if self._match('=') else TokenType.GREATER_THAN\n            )\n        # Number literals\n        elif char.isdigit():\n            return self._scan_number()\n        # Identifiers and keywords\n        elif char.isalpha() or char == '_':\n            return self._scan_identifier()\n        # Single-character tokens\n        elif char in self._SINGLE_CHAR_TOKENS:\n            return self._make_token(self._SINGLE_CHAR_TOKENS[char])\n        # Whitespace â€” consume silently\n        elif char in (' ', '\\t', '\\r', '\\n'):\n            return None\n        # Error â€” unrecognized character\n        else:\n            return Token(TokenType.ERROR, char, self.line, self.start_column)\n    def scan_tokens(self) -> list[Token]:\n        while not self.is_at_end():\n            token = self._scan_token()\n            if token is not None:\n                self.tokens.append(token)\n        self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))\n        return self.tokens\n```\n---\n## Three-Level View: What `>=` Means at Each Layer\n**Level 1 â€” Source Language**: The programmer writes `x >= 42`. To them, `>=` is a single operator with a single meaning: \"greater than or equal to.\" They think in terms of mathematical comparisons.\n**Level 2 â€” The Scanner (what you're building)**: The scanner reads `>`, then peeks `=`, then consumes `=`. At this level, `>=` is two *characters* that get collapsed into one *token*. The scanner's job is exactly this collapse â€” taking the continuous character stream and imposing structure.\n**Level 3 â€” The Parser and Runtime (downstream)**: The parser sees `Token(GREATER_EQUAL, \">=\", ...)` as a single atomic unit. Its grammar rule for comparisons matches on `GREATER_EQUAL` as a token type. The evaluator eventually turns `GREATER_EQUAL` into a machine instruction (usually a `jge` or `setge` on x86). By the time it reaches silicon, `>=` has been through three representations: characters â†’ token â†’ machine instruction.\n---\n## Common Pitfalls and How to Avoid Them\n### Pitfall 1: Forgetting to Snapshot `start_column` Before `advance()`\n```python\n# âŒ Wrong ordering\ndef _scan_token(self) -> Token | None:\n    char = self.advance()           # advance() changes self.column\n    self.start = self.current - 1\n    self.start_column = self.column  # BUG: column is already past the first char\n```\n```python\n# âœ… Correct ordering\ndef _scan_token(self) -> Token | None:\n    self.start = self.current\n    self.start_column = self.column  # snapshot BEFORE advance()\n    char = self.advance()\n```\nThe `start_column` must be captured *before* consuming the first character. After `advance()`, `self.column` has already moved forward.\n### Pitfall 2: `42abc` â€” Number Immediately Followed by Identifier\nYour `_scan_number()` stops when `peek()` is not a digit or `.`. So `42abc` would produce `NUMBER(\"42\")` then `IDENTIFIER(\"abc\")`. This is arguably correct â€” most C-like languages treat `42abc` as a lex error, but our simple approach produces two valid tokens.\nIf you want a strict error here, add a check after `_scan_number()`:\n```python\ndef _scan_number(self) -> Token:\n    while self.peek().isdigit():\n        self.advance()\n    if self.peek() == '.' and self._peek_next().isdigit():\n        self.advance()\n        while self.peek().isdigit():\n            self.advance()\n    # Optional: flag identifier immediately after number\n    if self.peek().isalpha() or self.peek() == '_':\n        # consume the rest and report error\n        while self.peek().isalnum() or self.peek() == '_':\n            self.advance()\n        return Token(TokenType.ERROR,\n                     self.source[self.start:self.current],\n                     self.line, self.start_column)\n    return self._make_token(TokenType.NUMBER)\n```\nFor this project, the simpler \"emit NUMBER then IDENTIFIER\" is acceptable. Document the behavior either way.\n### Pitfall 3: `keyword` Inside `identifiers` Must Not Match\nThis pitfall was already addressed architecturally (scan first, lookup second), but it's worth testing explicitly:\n```python\n# These must ALL be IDENTIFIER, not KEYWORD:\n# \"iffy\", \"while_loop\", \"return_value\", \"nullify\", \"trueness\", \"elsewhere\"\n```\nIf any of these tokenize incorrectly, your keyword matching is running *during* scanning rather than *after*.\n### Pitfall 4: Float With Trailing Dot (`3.`)\nWith the current implementation, `3.` tokenizes as `NUMBER(\"3\")` followed by `ERROR(\".\")` (since bare `.` is not a valid token). This is correct by design. But test it explicitly â€” it's easy to accidentally consume the dot inside `_scan_number()` without the guard `_peek_next().isdigit()`.\n---\n## Testing M2\n```python\ndef test_two_char_operators():\n    scanner = Scanner(\"== != <= >=\")\n    tokens = scanner.scan_tokens()\n    types = [t.type for t in tokens[:-1]]  # exclude EOF\n    assert types == [\n        TokenType.EQUALS,\n        TokenType.NOT_EQUAL,\n        TokenType.LESS_EQUAL,\n        TokenType.GREATER_EQUAL,\n    ]\ndef test_maximal_munch_equals():\n    \"\"\"'==' must be ONE token, not two ASSIGN tokens.\"\"\"\n    scanner = Scanner(\"==\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2  # EQUALS + EOF\n    assert tokens[0].type == TokenType.EQUALS\n    assert tokens[0].lexeme == \"==\"\ndef test_single_equals():\n    scanner = Scanner(\"=\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ASSIGN\n    assert tokens[0].lexeme == \"=\"\ndef test_gteqeq_maximal_munch():\n    \"\"\">== must tokenize as GREATER_EQUAL + ASSIGN, not GREATER_THAN + EQUALS.\"\"\"\n    scanner = Scanner(\">==\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.GREATER_EQUAL\n    assert tokens[0].lexeme == \">=\"\n    assert tokens[1].type == TokenType.ASSIGN\n    assert tokens[1].lexeme == \"=\"\n    assert tokens[2].type == TokenType.EOF\ndef test_integer_literal():\n    scanner = Scanner(\"42\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.NUMBER\n    assert tokens[0].lexeme == \"42\"\ndef test_float_literal():\n    scanner = Scanner(\"3.14\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.NUMBER\n    assert tokens[0].lexeme == \"3.14\"\ndef test_trailing_dot_not_float():\n    \"\"\"'3.' should emit NUMBER('3') then ERROR('.') â€” dot not consumed as part of float.\"\"\"\n    scanner = Scanner(\"3.\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.NUMBER\n    assert tokens[0].lexeme == \"3\"\n    assert tokens[1].type == TokenType.ERROR\n    assert tokens[1].lexeme == \".\"\ndef test_identifier_basic():\n    scanner = Scanner(\"myVar\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == \"myVar\"\ndef test_identifier_with_underscore():\n    scanner = Scanner(\"_count\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == \"_count\"\ndef test_keyword_if():\n    scanner = Scanner(\"if\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.KEYWORD\n    assert tokens[0].lexeme == \"if\"\ndef test_keyword_not_in_identifier():\n    \"\"\"'iffy' must be IDENTIFIER, not KEYWORD('if') + IDENTIFIER('fy').\"\"\"\n    scanner = Scanner(\"iffy\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2  # IDENTIFIER + EOF\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == \"iffy\"\ndef test_all_keywords():\n    keywords = ['if', 'else', 'while', 'return', 'true', 'false', 'null']\n    for kw in keywords:\n        scanner = Scanner(kw)\n        tokens = scanner.scan_tokens()\n        assert tokens[0].type == TokenType.KEYWORD, f\"'{kw}' should be KEYWORD\"\n        assert tokens[0].lexeme == kw\ndef test_full_expression():\n    \"\"\"The milestone's canonical integration test.\"\"\"\n    scanner = Scanner(\"if (x >= 42) { return true; }\")\n    tokens = scanner.scan_tokens()\n    expected = [\n        (TokenType.KEYWORD,       \"if\"),\n        (TokenType.PUNCTUATION,   \"(\"),\n        (TokenType.IDENTIFIER,    \"x\"),\n        (TokenType.GREATER_EQUAL, \">=\"),\n        (TokenType.NUMBER,        \"42\"),\n        (TokenType.PUNCTUATION,   \")\"),\n        (TokenType.PUNCTUATION,   \"{\"),\n        (TokenType.KEYWORD,       \"return\"),\n        (TokenType.KEYWORD,       \"true\"),\n        (TokenType.PUNCTUATION,   \";\"),\n        (TokenType.PUNCTUATION,   \"}\"),\n        (TokenType.EOF,           \"\"),\n    ]\n    assert len(tokens) == len(expected), (\n        f\"Expected {len(expected)} tokens, got {len(tokens)}: {tokens}\"\n    )\n    for token, (exp_type, exp_lexeme) in zip(tokens, expected):\n        assert token.type == exp_type, (\n            f\"Expected {exp_type.name}, got {token.type.name} for '{token.lexeme}'\"\n        )\n        assert token.lexeme == exp_lexeme, (\n            f\"Expected lexeme {exp_lexeme!r}, got {token.lexeme!r}\"\n        )\ndef test_less_than_no_consume():\n    \"\"\"'<' alone emits LESS_THAN, not consuming the next char.\"\"\"\n    scanner = Scanner(\"< \")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.LESS_THAN\n    assert tokens[0].lexeme == \"<\"\ndef test_not_equal():\n    scanner = Scanner(\"!=\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.NOT_EQUAL\n    assert tokens[0].lexeme == \"!=\"\ndef test_bare_exclamation_is_error():\n    \"\"\"'!' not followed by '=' is not a valid token.\"\"\"\n    scanner = Scanner(\"!\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\ndef test_number_position():\n    scanner = Scanner(\"  42\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].line == 1\n    assert tokens[0].column == 3   # two spaces before '4'\ndef test_identifier_position():\n    scanner = Scanner(\"\\nfoo\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].line == 2\n    assert tokens[0].column == 1\n```\nAll of these should pass with the implementation above. Run them before moving to M3.\n---\n## Knowledge Cascade â€” \"Learn One, Unlock Ten\"\nYou've implemented maximal munch, two-character operators, number scanning, and keyword recognition. Here's what those unlock:\n**â†’ Regular expression engines**: The `_match()` primitive you wrote â€” peek ahead, consume if match â€” is exactly what a regex engine's greedy quantifier does internally. When you write `\\d+` in a regex, the engine's NFA-to-DFA compiled code runs a loop identical to your `while self.peek().isdigit(): self.advance()`. You've written a regex engine's inner loop by hand, for a specific pattern. Understanding one gives you the other for free.\n**â†’ Operator precedence in parsers (downstream impact)**: The parser that consumes your token stream will have precedence rules like \"multiplication binds tighter than addition.\" Those rules operate on *token types*. By emitting `EQUALS` as a single token (not two `ASSIGN`s), you've made it possible for the parser to write a single grammar rule `comparison := expr EQUALS expr`. If you'd emitted two `ASSIGN` tokens, the parser would need special-case logic to reassemble them â€” which is fragile and error-prone. Correct lexing is the foundation of correct parsing.\n**â†’ The keyword/identifier boundary in real compilers**: GCC and Clang use the exact same \"scan full identifier, then hash-lookup\" strategy. Clang's `Lexer::LexIdentifier()` scans all alphanumeric characters, then calls `LookUpIdentifierInfo()` which hashes the result and returns a `tok::TokenKind`. The structure is identical to `_scan_identifier()`. Real-world lexers differ in performance (perfect hashing, SIMD character classification), not in algorithm.\n**â†’ LA(k) in parsing theory**: Your lexer uses LA(1) with a brief excursion to LA(2) for the trailing-dot case. Parsers use LA(k) over *tokens* (not characters). LL(1) parsers â€” the simplest useful parsers â€” use LA(1): look at one token ahead to decide which grammar rule to apply. LR(1) parsers (used by yacc/bison) also use one token of lookahead but in a more powerful way. The concept scales: the principle of \"look ahead by k to resolve ambiguity\" is the same whether k is 1 character or 1 token.\n**â†’ Lexer generators (what you could build next)**: Tools like `lex`, `flex`, and ANTLR's lexer take a set of regex patterns and generate a lexer automatically. Internally, they compile each regex to an NFA, merge the NFAs into one combined NFA, convert to a DFA using subset construction, and minimize the DFA. The result is a state transition table â€” a 2D array indexed by (state, character) â€” that runs faster than any hand-written if/elif chain. You've hand-written the conceptual equivalent; the generator just automates the machinery and optimizes it. *Crafting Interpreters* Chapter 4 by Robert Nystrom walks through this hand-written approach in Java; the LLVM tutorial shows the C++ equivalent.\n**â†’ Number literal complexity in real languages**: Your number scanner handles integers and simple floats. Real languages are significantly more complex. C supports `0xFF` (hex), `0755` (octal), `0b1010` (binary in C23), `1.5e10` (scientific notation), `1.5f` (float suffix), `1ULL` (unsigned long long). Python adds `1_000_000` (underscores for readability) and `0o755` (explicit octal). Each of these requires additional states in the number-scanning FSM. The structure you've built â€” a loop consuming valid chars, branching on special characters â€” extends naturally to all of these. You've built the right architecture; adding cases is mechanical.\n---\n## What You've Built\nYour scanner now handles the full range of single and multi-character tokens:\n- **Two-character operators** (`==`, `!=`, `<=`, `>=`) recognized as single indivisible tokens via single-character lookahead\n- **Maximal munch** applied consistently â€” `>==` becomes `GREATER_EQUAL` + `ASSIGN`, never `GREATER_THAN` + `EQUALS`\n- **Integer literals** (`42`, `0`, `1000`) scanned by looping over digits\n- **Float literals** (`3.14`, `0.5`) scanned by detecting dot-then-digit\n- **Identifiers** scanned by consuming alphanumeric and underscore sequences\n- **Keywords** recognized by hash-map lookup after scanning the complete identifier â€” `iffy` stays `IDENTIFIER`, `if` becomes `KEYWORD`\nIn Milestone 3, you'll build the two remaining complex cases: string literals with escape sequences, and comment filtering (both single-line `//` and multi-line `/* */`). String scanning introduces a new kind of state â€” you're inside a quoted region where most normal dispatch rules are suspended â€” and comments require you to distinguish `/` (division) from `//` (line comment) from `/*` (block comment start) using exactly the `_match()` primitive you just built.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m3 -->\n<!-- MS_ID: tokenizer-m3 -->\n# Milestone 3: Strings & Comments\n## Where You're Starting\nYour scanner is genuinely useful now. It handles single-character tokens, two-character operators, number literals, identifiers, and keywords. You can tokenize `if (x >= 42) { return true; }` perfectly, producing exactly the right token stream.\nBut try scanning a real program and you'll hit two categories of input your scanner cannot yet handle:\n```python\nsource = '''\n// Find the maximum value\nx = getValue(); /* default */\ngreeting = \"hello // not a comment\";\nresult = /* start */ 42;\n'''\n```\nThe `//` line comment makes your scanner emit `OPERATOR('/')` + `OPERATOR('/')` + `IDENTIFIER('Find')` + ... â€” noise. The string `\"hello // not a comment\"` starts a string, then triggers comment logic mid-string. The `/* */` block comment gets consumed as `/`, `*`, `/`, more tokens.\nEverything breaks. The scanner doesn't know it's \"inside\" anything special.\nThis milestone fixes that â€” and in doing so, teaches you the deepest concept in lexical analysis: **the tokenizer is a state machine with distinct modes, and inside each mode, all the normal rules are suspended.**\n\n![Tokenizer System â€” Satellite Map (Home Base)](./diagrams/diag-satellite-map.svg)\n\n---\n## The Core Misconception: The Tokenizer Has One Set of Rules\nHere's what the previous two milestones might have led you to believe: the scanner reads characters, consults its dispatch table, calls the right handler, and emits a token. Strings and comments are just \"different character patterns\" â€” maybe a bit more complex, but handled the same way.\nThis model is wrong, and breaking it is the revelation of this milestone.\nConsider what happens when you're scanning this input and you reach the `/` inside the string:\n```\n\"hello // world\"\n```\nYour current `_scan_token()` dispatch would see `/` and emit `OPERATOR('/')`. But `/` here is not an operator â€” it's just a character that happens to appear inside a string literal. The rule for `/` is completely different depending on **where you are**.\nThe same applies in reverse. When you're inside a `/* comment */`, you see `\"` characters â€” but they do not begin strings. The `\"` inside a comment is inert. It's just a character.\n**The fundamental insight**: once you enter a string or comment, you are in a completely different context with its own rules. The normal dispatch table â€” the one that maps characters to token types â€” is *suspended*. You stay in this special context until a specific exit condition is met (the closing `\"`, `\\n`, or `*/`). Only then do you return to normal scanning.\n\n![Scanner State Machine â€” NORMAL vs STRING vs COMMENT Modes](./diagrams/diag-scanner-state-modes.svg)\n\nThis is not an engineering choice â€” it is a formal necessity. Let's understand why.\n---\n## Why Regular Expressions Cannot Tokenize Real Languages\n> ðŸ”­ **Deep Dive**: The formal proof that balanced delimiters (matching quotes, brackets, etc.) cannot be expressed by any regular expression is given by the Pumping Lemma for regular languages. For a solid foundation, see Sipser's *Introduction to the Theory of Computation*, Chapter 1 (Section 1.4, \"Nonregular Languages\"). It's about 10 pages and provides the canonical result.\nHere's the intuition, without the formalism.\nA **regular language** is a language that can be recognized by a finite automaton â€” a machine with a fixed, finite number of states and no memory beyond which state it's currently in. Your scanner so far IS a finite automaton: it has a \"start\" state, transitions based on the current character, and reaches an accepting state (emitting a token) at the right moment.\nThe critical word is **finite**. A finite automaton cannot count. It cannot remember \"I opened a string literal three characters ago.\" It has no stack, no counter, no history beyond its current state.\nBut recognizing `\"...\"` â€” a string that starts with `\"` and ends with `\"` â€” *requires* remembering that you opened a quote. You need to know: \"I am inside a string right now.\" That knowledge is **context** â€” exactly what a finite automaton lacks.\nThe workaround is to encode that context into the state itself. Instead of having one \"start\" state, you have multiple states: `NORMAL`, `IN_STRING`, `IN_SINGLE_COMMENT`, `IN_MULTI_COMMENT`. Each is a different mode of the machine. Within `IN_STRING`, the character `\"` means \"exit string mode and emit a STRING token\" â€” but in `NORMAL` mode, the same character means \"enter string mode.\"\nThe key insight is that you are not making the language more complex â€” you are making your *representation* of the language richer by adding states. The machine is still finite (you have a fixed set of modes), but the modes themselves encode the \"I am inside a string\" knowledge that a pure regular expression cannot express.\nThis is formally why \"a regex can't tokenize real programming languages.\" It's not a limitation of skill â€” it's a theorem. The workaround is mode-tracking in an augmented DFA. That's what you're building in this milestone.\n---\n## The Three Modes: A State Machine with Context\n\n![Scanner State Machine â€” NORMAL vs STRING vs COMMENT Modes](./diagrams/diag-scanner-state-modes.svg)\n\nYour scanner after this milestone will operate in four conceptually distinct modes:\n| Mode | Trigger to Enter | Trigger to Exit | Rules |\n|------|-----------------|----------------|-------|\n| `NORMAL` | start of input | â€” | Normal dispatch table applies |\n| `IN_STRING` | `\"` in NORMAL mode | `\"` or EOF/newline | All chars are string content; `\\` triggers escape |\n| `IN_SINGLE_COMMENT` | `//` in NORMAL mode | `\\n` or EOF | All chars discarded; no tokens emitted |\n| `IN_MULTI_COMMENT` | `/*` in NORMAL mode | `*/` or EOF | All chars discarded; newlines update line count |\nYou won't implement these as explicit state variables â€” instead, they manifest as separate methods (`_scan_string()`, `_skip_line_comment()`, `_skip_block_comment()`) that each own their scanning loop. The \"mode\" is implicit in which method is currently executing.\nThe beauty of this design: each method is a complete mini-scanner for its context. It doesn't need to know about the main dispatch table. When it finishes (emits a token or runs out of input), control returns to `_scan_token()` in NORMAL mode.\n---\n## The `/` Ambiguity: Division, Line Comment, or Block Comment?\nBefore diving into strings, let's resolve the `'/'` problem â€” because it's the gateway to comments and the cleanest example of mode-switching.\nIn your current scanner, `'/'` appears in `_SINGLE_CHAR_TOKENS` and emits `OPERATOR`. But `/` can mean three completely different things depending on what follows it:\n- `/` followed by anything other than `/` or `*` â†’ Division operator\n- `/` followed by `/` â†’ Start of a single-line comment (consume everything until `\\n`)\n- `/` followed by `*` â†’ Start of a multi-line comment (consume everything until `*/`)\n\n![The '/' Ambiguity â€” Division, Single-Line, or Multi-Line?](./diagrams/diag-comment-vs-division.svg)\n\nThis is maximal munch applied to comment detection â€” the same principle from Milestone 2, now extended to a three-way branch. Here's the dispatch:\n```python\nelif char == '/':\n    if self._match('/'):\n        # Single-line comment: consume until newline or EOF\n        self._skip_line_comment()\n        return None  # comments produce no token\n    elif self._match('*'):\n        # Multi-line comment: consume until */ or EOF\n        result = self._skip_block_comment()\n        return result  # might be None (success) or ERROR (unterminated)\n    else:\n        # Just a division operator\n        return self._make_token(TokenType.OPERATOR)\n```\nRemove `'/'` from `_SINGLE_CHAR_TOKENS` â€” it now has its own branch. The first character is `'/'` (already consumed by `advance()`); then `_match('/')` or `_match('*')` peeks at the next character using the same conditional-consume primitive you built in Milestone 2.\n---\n## Single-Line Comments: The Simple Case\nA single-line comment starting with `//` consumes everything until end-of-line. It produces no token â€” it simply advances `current` past the comment characters and returns `None` to the main loop (which silently discards `None` results).\n```python\ndef _skip_line_comment(self) -> None:\n    \"\"\"\n    Called after consuming '//'. Advances past all characters until\n    newline or EOF. Does NOT consume the newline itself â€” the main loop\n    will encounter \\n and handle whitespace normally.\n    \"\"\"\n    while self.peek() != '\\n' and not self.is_at_end():\n        self.advance()\n    # The '\\n' (if present) is left for the main loop to consume as whitespace.\n    # This ensures line counting remains accurate.\n```\nNote the deliberate choice: do NOT consume the `\\n`. Let it be picked up by the main loop's whitespace handling. This keeps the newline handling in one place â€” `advance()` â€” rather than scattering it across multiple methods.\n**Why this matters for correctness**: the `advance()` method is responsible for incrementing `self.line` when it sees `\\n`. If you consume `\\n` inside `_skip_line_comment()` using `advance()`, the line counter updates correctly. If you somehow consume it without going through `advance()`, you introduce a line-counting bug. Routing all character consumption through `advance()` is the key invariant that keeps position tracking accurate.\n---\n## Multi-Line Comments: The `*/` Hunt\nMulti-line comments are more interesting. You must scan forward looking for the two-character sequence `*/`, consuming everything â€” including newlines â€” until you find it. If you reach EOF before finding `*/`, that's an error.\n```python\ndef _skip_block_comment(self) -> Token | None:\n    \"\"\"\n    Called after consuming '/*'. Advances past all characters until\n    '*/' is found or EOF is reached.\n    Returns None on success (comment fully consumed).\n    Returns an ERROR token if the comment is unterminated.\n    The error position is the position of the opening '/*', stored\n    in self.start_column / self.line at the time this is called.\n    \"\"\"\n    # Capture the opening position for potential error reporting\n    error_line = self.line\n    error_col = self.start_column\n    while not self.is_at_end():\n        char = self.advance()\n        if char == '*' and self.peek() == '/':\n            self.advance()  # consume the closing '/'\n            return None     # comment successfully terminated\n    # Reached EOF without finding '*/'\n    return Token(\n        TokenType.ERROR,\n        '/*',           # the lexeme that caused the problem\n        error_line,\n        error_col\n    )\n```\n\n![Line Number Updates Inside Multi-Line Comments and Strings](./diagrams/diag-line-tracking-in-multiline.svg)\n\n**Line tracking inside comments**: Notice that `self.advance()` handles newlines everywhere, including inside the comment loop. When the scanner reads `\\n` inside a block comment, `advance()` increments `self.line` and resets `self.column` â€” exactly as it does everywhere else. You don't write any special line-tracking code in `_skip_block_comment()`. The invariant holds automatically.\nThis is the payoff for the design decision made in Milestone 1: routing all character consumption through `advance()`. Every path through the scanner â€” normal tokens, string literals, comments â€” goes through the same function, so position tracking is always correct.\n### The Non-Nesting Rule\n\n![Multi-Line Comments Do NOT Nest â€” The /* /* */ Trap](./diagrams/diag-multiline-comment-nonnesting.svg)\n\nMulti-line comments in C and most C-like languages do **not** nest. This surprises many developers who assume that `/* /* */ */` is a complete comment. It is not:\n```\n/* /* */    â† this closes the comment (first */ encountered)\n*/          â† this is now outside the comment, and */ is a syntax error\n```\nYour implementation above already handles this correctly by design. The loop searches for `*` followed by `/` â€” the moment it finds the first `*/`, it exits. It does not maintain a nesting counter. It does not look for additional `/*` openers inside the comment.\nIf you wanted nested comments (like OCaml or Rust's doc comments support), you'd maintain a depth counter:\n```python\n# HOW nested comments WOULD work (not implemented here):\ndepth = 1\nwhile not self.is_at_end() and depth > 0:\n    char = self.advance()\n    if char == '/' and self.peek() == '*':\n        self.advance()\n        depth += 1   # nested open\n    elif char == '*' and self.peek() == '/':\n        self.advance()\n        depth -= 1   # nested close\n```\nBut for this C-like language, nesting is explicitly not supported. The simpler implementation is correct by specification.\n---\n## String Literal Scanning: Entering a New World\nStrings are the most complex single construct in this milestone because they involve:\n1. Entering a new mode where all normal dispatch rules are suspended\n2. Processing escape sequences â€” special two-character sequences that represent single logical characters\n3. Detecting two error conditions: EOF before closing `\"`, and newline before closing `\"`\n4. Reporting errors with the position of the *opening* quote, not where the problem was detected\nLet's build this up step by step.\n### The Basic Loop\nAfter consuming the opening `\"`, you enter a loop that consumes characters until one of three things happens:\n- You see `\"` â†’ emit the complete string token\n- You see `\\n` or hit EOF â†’ unterminated string, emit error\n- You see any other character â†’ it's part of the string, keep consuming\n```python\ndef _scan_string(self) -> Token:\n    \"\"\"\n    Called after consuming the opening '\"'. Scans the string body\n    and closing quote.\n    Returns a STRING token on success.\n    Returns an ERROR token if the string is unterminated.\n    \"\"\"\n    while not self.is_at_end() and self.peek() != '\"':\n        if self.peek() == '\\n':\n            # Newline inside string is not allowed in this language.\n            # Report error at the opening quote position (start_column).\n            return Token(\n                TokenType.ERROR,\n                self.source[self.start:self.current],\n                self.line,\n                self.start_column\n            )\n        if self.peek() == '\\\\':\n            self.advance()  # consume the backslash\n            if self.is_at_end():\n                break  # will be caught as unterminated below\n            self.advance()  # consume the escape character\n        else:\n            self.advance()\n    if self.is_at_end():\n        # EOF before closing quote\n        return Token(\n            TokenType.ERROR,\n            self.source[self.start:self.current],\n            self.line,\n            self.start_column\n        )\n    # Consume the closing '\"'\n    self.advance()\n    return self._make_token(TokenType.STRING)\n```\nAnd in `_scan_token()`, add before the single-character dispatch:\n```python\nelif char == '\"':\n    return self._scan_string()\n```\n\n![Comments Inside Strings vs Strings Inside Comments](./diagrams/diag-context-overlap.svg)\n\n**The crucial invariant**: Inside `_scan_string()`, you never look at the dispatch table. You never check if `//` is a comment start. You never check if `{` is punctuation. Every character you encounter â€” `/`, `{`, `+`, anything â€” is consumed as part of the string. The only characters that have special meaning inside a string are `\"` (exit), `\\n` (error), and `\\` (escape prefix). Everything else is content.\nThis is what \"mode suspension\" means in practice: you've entered a different scanning context where the dispatch table simply doesn't apply.\n---\n## Escape Sequences: Two Characters, One Meaning\n\n![Escape Sequence Processing Inside Strings](./diagrams/diag-string-escape-processing.svg)\n\nWhen you see a backslash `\\` inside a string, it means \"the next character is special.\" The pair `\\` + `n` represents a newline character. The pair `\\` + `\"` represents a literal double-quote (without ending the string). The pair `\\` + `\\` represents a literal backslash.\nThis two-character â†’ one-character substitution is called an **escape sequence** (the backslash \"escapes\" the normal meaning of the following character). It appears everywhere in computing: Python strings, C strings, JSON, CSV, HTTP headers, shell quoting, SQL queries. The mechanism is always the same: a special escape character announces that the following character should be interpreted differently.\n> **Cross-domain connection**: JSON uses the exact same escape mechanism for strings: `\\\"` for a literal quote, `\\\\` for a literal backslash, `\\n` for newline, `\\t` for tab. Any JSON parser you've ever used has the same `_scan_string()` loop with the same backslash-detection logic. If you've ever wondered how JSON distinguishes `\"hello\"` (the string `hello`) from `\"he said \\\"hi\\\"\"` (the string `he said \"hi\"`), now you know: it's the same escape processing you're implementing here.\nThe escape sequences your C-like language supports:\n| Written in source | Represents | ASCII code |\n|-------------------|------------|-----------|\n| `\\n` | Newline | 10 |\n| `\\t` | Tab | 9 |\n| `\\r` | Carriage return | 13 |\n| `\\\"` | Double quote | 34 |\n| `\\\\` | Backslash | 92 |\nFor this tokenizer, you're storing the raw lexeme â€” the exact characters from source â€” rather than processing escape sequences into their final values. The lexeme for `\"hello\\nworld\"` will be the 15-character string `\"hello\\nworld\"` (with a literal backslash-n), not a 12-character string with an actual newline embedded.\nThis is correct tokenizer behavior. **The tokenizer's job is categorization, not interpretation.** Transforming `\\n` into a newline character is the job of the evaluator or code generator â€” a later stage. For now, the tokenizer just needs to correctly identify where the string ends.\nThe escape handling in `_scan_string()` above does the right thing:\n```python\nif self.peek() == '\\\\':\n    self.advance()  # consume the backslash\n    if self.is_at_end():\n        break  # unterminated: backslash at end of input\n    self.advance()  # consume whatever follows the backslash\n```\nWhen you see `\\`, consume it, then unconditionally consume the next character â€” whatever it is. This correctly handles `\\\"` (consumes `\\` then `\"`, leaving the string open) and `\\\\` (consumes `\\` then another `\\`, leaving the string open). The double-consume is the key: it prevents the `\"` after `\\` from being seen as the string terminator.\n**What about invalid escapes like `\\z`?** For this project, silently accept them â€” consume the two characters and include them in the string lexeme. Production compilers typically warn here but continue, because crashing on a bad escape would prevent reporting all other errors. Document the behavior in your code.\n### The Critical Error Reporting Insight\nWhen a string is unterminated, which position do you report in the error?\nOption A â€” the position of the character that revealed the problem (EOF, or the newline):\n```\nError at line 5, column 80: unterminated string\n```\nOption B â€” the position of the opening quote:\n```\nError at line 5, column 12: unterminated string\n```\n\n![Unterminated String â€” Error Detection & Position Reporting](./diagrams/diag-unterminated-string.svg)\n\nOption B is dramatically more useful. Line 5, column 80 might be the end of the file â€” that tells you nothing about where to look. Line 5, column 12 tells you exactly: \"go to the quote at column 12, and find the matching closing quote that you forgot.\"\nThis is why the implementation stores `self.start_column` *before* entering `_scan_string()`, and uses it in all error tokens. The `start_column` snapshot â€” taken at the top of `_scan_token()` before the first `advance()` â€” captures the position of the opening `\"`. Every error from inside `_scan_string()` uses this stored position.\nThis is not just a nice-to-have. It's the difference between a usable error message and a frustrating one. GCC, Clang, and rustc all report the position of the opening delimiter when signaling unterminated strings â€” because experience has proven it's the right choice.\n---\n## Putting It Together: The Complete `_scan_token()` for M3\nHere is the full updated `_scan_token()` with all three milestones integrated:\n```python\ndef _scan_token(self) -> Token | None:\n    self.start = self.current\n    self.start_column = self.column\n    char = self.advance()\n    # â”€â”€ Two-character operators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if char == '=':\n        return self._make_token(\n            TokenType.EQUALS if self._match('=') else TokenType.ASSIGN\n        )\n    elif char == '!':\n        if self._match('='):\n            return self._make_token(TokenType.NOT_EQUAL)\n        return Token(TokenType.ERROR, char, self.line, self.start_column)\n    elif char == '<':\n        return self._make_token(\n            TokenType.LESS_EQUAL if self._match('=') else TokenType.LESS_THAN\n        )\n    elif char == '>':\n        return self._make_token(\n            TokenType.GREATER_EQUAL if self._match('=') else TokenType.GREATER_THAN\n        )\n    # â”€â”€ Division or comment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    elif char == '/':\n        if self._match('/'):\n            self._skip_line_comment()\n            return None\n        elif self._match('*'):\n            return self._skip_block_comment()\n        else:\n            return self._make_token(TokenType.OPERATOR)\n    # â”€â”€ String literals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    elif char == '\"':\n        return self._scan_string()\n    # â”€â”€ Number literals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    elif char.isdigit():\n        return self._scan_number()\n    # â”€â”€ Identifiers and keywords â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    elif char.isalpha() or char == '_':\n        return self._scan_identifier()\n    # â”€â”€ Single-character tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    elif char in self._SINGLE_CHAR_TOKENS:\n        return self._make_token(self._SINGLE_CHAR_TOKENS[char])\n    # â”€â”€ Whitespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    elif char in (' ', '\\t', '\\r', '\\n'):\n        return None\n    # â”€â”€ Error: unrecognized character â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    else:\n        return Token(TokenType.ERROR, char, self.line, self.start_column)\n```\nNotice the structure: the division operator case (`elif char == '/'`) now replaces the `'/'` entry in `_SINGLE_CHAR_TOKENS`. The string literal case (`elif char == '\"'`) is added before number and identifier scanning.\n---\n## Complete Scanner Implementation for M3\nHere is the full, self-contained implementation incorporating all three milestones:\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nclass TokenType(Enum):\n    # Literals\n    NUMBER        = auto()\n    STRING        = auto()\n    # Names\n    IDENTIFIER    = auto()\n    KEYWORD       = auto()\n    # Arithmetic operators\n    OPERATOR      = auto()   # +, -, *, /\n    # Comparison & assignment operators\n    ASSIGN        = auto()   # =\n    EQUALS        = auto()   # ==\n    NOT_EQUAL     = auto()   # !=\n    LESS_THAN     = auto()   # <\n    LESS_EQUAL    = auto()   # <=\n    GREATER_THAN  = auto()   # >\n    GREATER_EQUAL = auto()   # >=\n    # Structural\n    PUNCTUATION   = auto()\n    # Control\n    EOF   = auto()\n    ERROR = auto()\n@dataclass\nclass Token:\n    type: TokenType\n    lexeme: str\n    line: int\n    column: int\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, {self.line}:{self.column})\"\nclass Scanner:\n    _SINGLE_CHAR_TOKENS: dict[str, TokenType] = {\n        '+': TokenType.OPERATOR,\n        '-': TokenType.OPERATOR,\n        '*': TokenType.OPERATOR,\n        # '/' is NOT here â€” handled separately for comment detection\n        '(': TokenType.PUNCTUATION,\n        ')': TokenType.PUNCTUATION,\n        '{': TokenType.PUNCTUATION,\n        '}': TokenType.PUNCTUATION,\n        '[': TokenType.PUNCTUATION,\n        ']': TokenType.PUNCTUATION,\n        ';': TokenType.PUNCTUATION,\n        ',': TokenType.PUNCTUATION,\n    }\n    _KEYWORDS: dict[str, TokenType] = {\n        'if':     TokenType.KEYWORD,\n        'else':   TokenType.KEYWORD,\n        'while':  TokenType.KEYWORD,\n        'return': TokenType.KEYWORD,\n        'true':   TokenType.KEYWORD,\n        'false':  TokenType.KEYWORD,\n        'null':   TokenType.KEYWORD,\n    }\n    def __init__(self, source: str, tab_width: int = 1) -> None:\n        self.source = source\n        self.tab_width = tab_width\n        self.tokens: list[Token] = []\n        self.start = 0\n        self.current = 0\n        self.line = 1\n        self.column = 1\n        self.start_column = 1\n    # â”€â”€ Primitives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def is_at_end(self) -> bool:\n        return self.current >= len(self.source)\n    def advance(self) -> str:\n        char = self.source[self.current]\n        self.current += 1\n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n        elif char == '\\t':\n            self.column += self.tab_width\n        else:\n            self.column += 1\n        return char\n    def peek(self) -> str:\n        if self.is_at_end():\n            return ''\n        return self.source[self.current]\n    def _peek_next(self) -> str:\n        if self.current + 1 >= len(self.source):\n            return ''\n        return self.source[self.current + 1]\n    def _match(self, expected: str) -> bool:\n        if self.is_at_end():\n            return False\n        if self.source[self.current] != expected:\n            return False\n        self.advance()\n        return True\n    # â”€â”€ Token construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _make_token(self, token_type: TokenType) -> Token:\n        lexeme = self.source[self.start:self.current]\n        return Token(token_type, lexeme, self.line, self.start_column)\n    # â”€â”€ Comment scanners â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _skip_line_comment(self) -> None:\n        \"\"\"\n        Called after consuming '//'. Advances past all characters until\n        newline or EOF. Does NOT consume the newline itself.\n        \"\"\"\n        while not self.is_at_end() and self.peek() != '\\n':\n            self.advance()\n    def _skip_block_comment(self) -> Token | None:\n        \"\"\"\n        Called after consuming '/*'. Advances past all characters until\n        '*/' or EOF.\n        Returns None on success, ERROR token if unterminated.\n        Error position is the opening '/*' (stored in self.start_column).\n        \"\"\"\n        error_line = self.line\n        error_col = self.start_column\n        while not self.is_at_end():\n            char = self.advance()\n            if char == '*' and self.peek() == '/':\n                self.advance()  # consume the closing '/'\n                return None     # successfully terminated\n        # EOF reached without finding '*/'\n        return Token(TokenType.ERROR, '/*', error_line, error_col)\n    # â”€â”€ String scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_string(self) -> Token:\n        \"\"\"\n        Called after consuming the opening '\"'. Scans string body\n        and the closing '\"'.\n        Handles escape sequences by consuming '\\' + next char as a unit\n        (raw lexeme stored; escape interpretation is the evaluator's job).\n        Returns STRING token on success.\n        Returns ERROR token if unterminated (newline or EOF before closing '\"').\n        Error position is the opening '\"' (self.start_column).\n        \"\"\"\n        while not self.is_at_end() and self.peek() != '\"':\n            if self.peek() == '\\n':\n                # Newline before closing quote â€” unterminated string.\n                # Report error at the OPENING quote position.\n                return Token(\n                    TokenType.ERROR,\n                    self.source[self.start:self.current],\n                    self.line,\n                    self.start_column\n                )\n            if self.peek() == '\\\\':\n                self.advance()  # consume the backslash\n                if not self.is_at_end():\n                    self.advance()  # consume the escaped character\n                # If EOF after backslash, the outer loop catches it next iteration\n            else:\n                self.advance()\n        if self.is_at_end():\n            # EOF before closing quote â€” unterminated string.\n            return Token(\n                TokenType.ERROR,\n                self.source[self.start:self.current],\n                self.line,\n                self.start_column\n            )\n        # Consume the closing '\"'\n        self.advance()\n        return self._make_token(TokenType.STRING)\n    # â”€â”€ Number scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_number(self) -> Token:\n        while self.peek().isdigit():\n            self.advance()\n        if self.peek() == '.' and self._peek_next().isdigit():\n            self.advance()  # consume '.'\n            while self.peek().isdigit():\n                self.advance()\n        return self._make_token(TokenType.NUMBER)\n    # â”€â”€ Identifier scanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_identifier(self) -> Token:\n        while self.peek().isalnum() or self.peek() == '_':\n            self.advance()\n        text = self.source[self.start:self.current]\n        token_type = self._KEYWORDS.get(text, TokenType.IDENTIFIER)\n        return self._make_token(token_type)\n    # â”€â”€ Core scan loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def _scan_token(self) -> Token | None:\n        self.start = self.current\n        self.start_column = self.column\n        char = self.advance()\n        if char == '=':\n            return self._make_token(\n                TokenType.EQUALS if self._match('=') else TokenType.ASSIGN\n            )\n        elif char == '!':\n            if self._match('='):\n                return self._make_token(TokenType.NOT_EQUAL)\n            return Token(TokenType.ERROR, char, self.line, self.start_column)\n        elif char == '<':\n            return self._make_token(\n                TokenType.LESS_EQUAL if self._match('=') else TokenType.LESS_THAN\n            )\n        elif char == '>':\n            return self._make_token(\n                TokenType.GREATER_EQUAL if self._match('=') else TokenType.GREATER_THAN\n            )\n        elif char == '/':\n            if self._match('/'):\n                self._skip_line_comment()\n                return None\n            elif self._match('*'):\n                return self._skip_block_comment()\n            else:\n                return self._make_token(TokenType.OPERATOR)\n        elif char == '\"':\n            return self._scan_string()\n        elif char.isdigit():\n            return self._scan_number()\n        elif char.isalpha() or char == '_':\n            return self._scan_identifier()\n        elif char in self._SINGLE_CHAR_TOKENS:\n            return self._make_token(self._SINGLE_CHAR_TOKENS[char])\n        elif char in (' ', '\\t', '\\r', '\\n'):\n            return None\n        else:\n            return Token(TokenType.ERROR, char, self.line, self.start_column)\n    def scan_tokens(self) -> list[Token]:\n        while not self.is_at_end():\n            token = self._scan_token()\n            if token is not None:\n                self.tokens.append(token)\n        self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))\n        return self.tokens\n```\n---\n## Three-Level View: What a String Literal Means at Each Layer\n**Level 1 â€” Source Language**: The programmer writes `greeting = \"hello\\nworld\"`. To them, this is assigning a two-line string to a variable. The `\\n` is the \"newline character\" â€” a single logical thing.\n**Level 2 â€” The Scanner (what you're building)**: The scanner sees 22 characters. It enters `_scan_string()` when it hits the `\"`. It reads `h`, `e`, `l`, `l`, `o` as ordinary content. When it hits `\\`, it enters escape-processing: consumes `\\`, then consumes `n` â€” two characters, but they're both part of the string content. It continues until the closing `\"`. The emitted token has `lexeme = '\"hello\\\\nworld\"'` â€” a 14-character string including the quotes and the raw backslash-n. The scanner preserves exactly what was written.\n**Level 3 â€” The Evaluator (downstream)**: When the interpreter or code generator processes the `STRING` token, it strips the outer quotes and processes `\\n` â†’ newline, `\\t` â†’ tab, etc. The runtime string object will contain an actual newline byte (ASCII 10). This is where `\"hello\\nworld\"` becomes a 12-character string with a newline in the middle.\nThe scanner deliberately does *not* perform Level 3's job. Keeping responsibilities separated means the scanner is simple, testable, and independent of the evaluation semantics.\n---\n## Common Pitfalls and How to Avoid Them\n### Pitfall 1: Consuming the Closing `\"` and Leaving It for the Next Token\nThis is a subtle off-by-one that produces confusing behavior:\n```python\n# âŒ Wrong: forgetting to consume the closing quote\ndef _scan_string(self) -> Token:\n    while not self.is_at_end() and self.peek() != '\"':\n        self.advance()\n    # Bug: we stop when peek() == '\"', but don't consume it\n    return self._make_token(TokenType.STRING)  # lexeme is missing the closing '\"'\n    # Next call to _scan_token() will see '\"' and start ANOTHER string!\n```\n```python\n# âœ… Correct: consume the closing '\"' before emitting\n    # ... loop ...\n    self.advance()  # consume the closing '\"'\n    return self._make_token(TokenType.STRING)\n```\nIf you forget to consume the closing `\"`, the next `_scan_token()` call sees it and starts scanning another string â€” producing cascading garbage tokens for everything that follows.\n### Pitfall 2: A Backslash at the End of the String\nInput: `\"hello\\` (backslash at EOF, no closing quote)\nWithout careful handling, this could cause an out-of-bounds access:\n```python\n# âŒ Dangerous: consuming the character after '\\' without checking EOF first\nif self.peek() == '\\\\':\n    self.advance()  # consume '\\'\n    self.advance()  # consume next char -- but what if is_at_end() is now True?\n```\n```python\n# âœ… Safe: guard the second advance\nif self.peek() == '\\\\':\n    self.advance()  # consume '\\'\n    if not self.is_at_end():\n        self.advance()  # only consume escaped char if it exists\n```\nPython's string indexing raises `IndexError` for out-of-bounds access, so this would crash without the guard. After the guarded version, `is_at_end()` will be true, and the main while-loop condition catches the unterminated string.\n### Pitfall 3: Detecting `*/` Inside a Block Comment\nThe naive approach â€” checking if `self.peek() == '*'` and then checking the character after that â€” requires two peek operations:\n```python\n# âŒ Naive: requires peek-ahead of two characters\nwhile not self.is_at_end():\n    if self.peek() == '*' and self._peek_next() == '/':\n        self.advance()  # consume '*'\n        self.advance()  # consume '/'\n        return None\n    self.advance()\n```\nThe implementation given earlier uses a cleaner one-consume-then-peek pattern:\n```python\n# âœ… Cleaner: advance then peek\nchar = self.advance()\nif char == '*' and self.peek() == '/':\n    self.advance()   # consume the '/'\n    return None\n```\nBoth are correct, but the second version uses `advance()` consistently as the primary consumer and `peek()` for the single-character lookahead â€” consistent with the rest of the scanner.\n### Pitfall 4: Comments Inside Strings\nThis is the whole point of mode-based scanning, but it's easy to accidentally break:\n```python\n# âŒ Wrong design: checking for '//' INSIDE the string loop\ndef _scan_string(self) -> Token:\n    while not self.is_at_end() and self.peek() != '\"':\n        if self.peek() == '/' and self._peek_next() == '/':\n            break  # Bug: treating comment syntax inside string as a comment!\n        self.advance()\n```\nThe correct `_scan_string()` has NO knowledge of `//` or `/*`. It only knows about `\"`, `\\n`, and `\\`. Any other character â€” including `/` â€” is consumed unconditionally as string content. The mode isolation is total.\n### Pitfall 5: Line Count Drift in Multi-Line Comments\nIf your multi-line comment scanner uses any mechanism other than `advance()` to consume characters, newlines won't be counted:\n```python\n# âŒ Wrong: skipping characters without updating line/column\ndef _skip_block_comment(self) -> Token | None:\n    while not self.is_at_end():\n        if self.source[self.current] == '\\n':\n            self.line += 1  # manual update -- easy to forget or get wrong\n            self.current += 1\n        elif ...:\n```\n```python\n# âœ… Correct: always use advance(), which handles newlines automatically\nwhile not self.is_at_end():\n    char = self.advance()   # advance() updates line/column for every char\n    ...\n```\nThe invariant is: **every character consumed goes through `advance()`**. If you maintain this, position tracking is automatically correct in every context.\n---\n## Testing M3\n```python\n# â”€â”€ String literal tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_simple_string():\n    scanner = Scanner('\"hello\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"hello\"'\n    assert tokens[1].type == TokenType.EOF\ndef test_empty_string():\n    scanner = Scanner('\"\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"\"'\ndef test_string_with_escape_newline():\n    scanner = Scanner(r'\"hello\\nworld\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == r'\"hello\\nworld\"'\ndef test_string_with_escaped_quote():\n    scanner = Scanner(r'\"say \\\"hi\\\"\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    # The lexeme includes the raw backslash-quote sequences\n    assert '\\\\\"' in tokens[0].lexeme\ndef test_string_with_escaped_backslash():\n    scanner = Scanner(r'\"path\\\\file\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\ndef test_unterminated_string_eof():\n    \"\"\"String without closing quote before EOF produces ERROR at opening quote.\"\"\"\n    scanner = Scanner('\"hello')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1   # position of the opening '\"'\ndef test_unterminated_string_newline():\n    \"\"\"String without closing quote before newline produces ERROR at opening quote.\"\"\"\n    scanner = Scanner('\"hello\\nworld\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1   # position of the opening '\"'\ndef test_string_position():\n    scanner = Scanner('  \"hi\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].column == 3   # '\"' is at column 3\ndef test_comment_inside_string_not_treated_as_comment():\n    \"\"\"'hello // world' is a single STRING token, not STRING + comment.\"\"\"\n    scanner = Scanner('\"hello // world\"')\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2   # STRING + EOF\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"hello // world\"'\ndef test_block_comment_marker_inside_string():\n    \"\"\"'/* inside */' within quotes is a string, not a comment.\"\"\"\n    scanner = Scanner('\"/* inside */\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert len(tokens) == 2   # STRING + EOF\ndef test_string_all_escape_sequences():\n    \"\"\"All supported escape sequences remain in raw lexeme.\"\"\"\n    scanner = Scanner(r'\"\\n\\t\\r\\\"\\\\\"')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n# â”€â”€ Single-line comment tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_line_comment_no_tokens():\n    \"\"\"A line comment produces no tokens (only EOF).\"\"\"\n    scanner = Scanner(\"// this is a comment\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\ndef test_line_comment_then_code():\n    \"\"\"Code after a comment on the next line is tokenized correctly.\"\"\"\n    scanner = Scanner(\"// comment\\nx = 1\")\n    tokens = scanner.scan_tokens()\n    # x, =, 1, EOF â€” the comment produces nothing\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == 'x'\n    assert tokens[0].line == 2\ndef test_line_comment_mixed():\n    \"\"\"Tokens before a comment are unaffected.\"\"\"\n    scanner = Scanner(\"x // comment\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2   # IDENTIFIER('x') + EOF\n    assert tokens[0].lexeme == 'x'\ndef test_line_comment_does_not_consume_newline():\n    \"\"\"The newline after '//' is NOT consumed by the comment scanner.\"\"\"\n    scanner = Scanner(\"// comment\\n+\")\n    tokens = scanner.scan_tokens()\n    # Only '+' and EOF â€” '+' is on line 2, column 1\n    assert tokens[0].type == TokenType.OPERATOR\n    assert tokens[0].line == 2\n    assert tokens[0].column == 1\ndef test_slash_not_comment():\n    \"\"\"'/' alone (not '//' or '/*') is a division operator.\"\"\"\n    scanner = Scanner(\"x / y\")\n    tokens = scanner.scan_tokens()\n    types = [t.type for t in tokens[:-1]]\n    assert types == [TokenType.IDENTIFIER, TokenType.OPERATOR, TokenType.IDENTIFIER]\n    assert tokens[1].lexeme == '/'\n# â”€â”€ Multi-line comment tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_block_comment_no_tokens():\n    \"\"\"A block comment produces no tokens (only EOF).\"\"\"\n    scanner = Scanner(\"/* comment */\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\ndef test_block_comment_with_newlines():\n    \"\"\"Multi-line block comment updates line counter correctly.\"\"\"\n    scanner = Scanner(\"/* line one\\nline two\\nline three */\\nx\")\n    tokens = scanner.scan_tokens()\n    # 'x' is on line 4 (3 newlines inside comment + starting line 1)\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].line == 4\ndef test_block_comment_inline():\n    \"\"\"Code before and after a block comment tokenizes correctly.\"\"\"\n    scanner = Scanner(\"x /* comment */ y\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 3   # IDENTIFIER('x'), IDENTIFIER('y'), EOF\n    assert tokens[0].lexeme == 'x'\n    assert tokens[1].lexeme == 'y'\ndef test_unterminated_block_comment():\n    \"\"\"Block comment without closing '*/' produces ERROR at opening '/*'.\"\"\"\n    scanner = Scanner(\"/* unterminated\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].lexeme == '/*'\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1\ndef test_block_comment_does_not_nest():\n    \"\"\"'/* /* */' ends at the FIRST '*/', not the second.\"\"\"\n    scanner = Scanner(\"/* /* */ x\")\n    tokens = scanner.scan_tokens()\n    # Comment closes at first '*/'; 'x' is a real token\n    # The remaining ' x' after '*/': one IDENTIFIER and EOF\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == 'x'\ndef test_block_comment_star_inside():\n    \"\"\"'*' inside a block comment that isn't followed by '/' doesn't close it.\"\"\"\n    scanner = Scanner(\"/* a * b */\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\ndef test_unterminated_block_comment_position():\n    \"\"\"Unterminated comment error reports the position of '/*', not EOF.\"\"\"\n    scanner = Scanner(\"x\\n  /* unterminated\")\n    tokens = scanner.scan_tokens()\n    # 'x' is first token, then ERROR for the unterminated comment\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[1].type == TokenType.ERROR\n    assert tokens[1].line == 2\n    assert tokens[1].column == 3  # '/*' starts at column 3 on line 2\n# â”€â”€ Context isolation tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_string_inside_comment_ignored():\n    \"\"\"A '\\\"' inside a block comment does not start a string.\"\"\"\n    scanner = Scanner('/* \"not a string\" */')\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1   # only EOF\n    assert tokens[0].type == TokenType.EOF\ndef test_line_numbers_accurate_after_multiline_comment():\n    \"\"\"Verify line tracking across multi-line constructs.\"\"\"\n    source = \"a\\n/* line 2\\nline 3\\n*/\\nb\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    # 'a' is on line 1, 'b' is on line 5\n    assert tokens[0].lexeme == 'a'\n    assert tokens[0].line == 1\n    assert tokens[1].lexeme == 'b'\n    assert tokens[1].line == 5\ndef test_string_then_comment():\n    \"\"\"String followed by comment: both handled correctly.\"\"\"\n    scanner = Scanner('\"hello\" // rest is comment')\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2   # STRING + EOF\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"hello\"'\n```\nRun all of these. The ones that test position inside multi-line constructs are the most demanding â€” if your `advance()` invariant holds, they pass automatically.\n---\n## Syntax Highlighting: Why Your Editor Sometimes Goes Wrong\nNow you understand something you've always seen but probably never knew the reason for.\nOpen any code editor. Type:\n```python\nx = \"hello\ny = 42\n```\nThe moment you type the opening `\"` without a closing `\"`, watch what happens to the lines below: they often turn orange (or whatever your string-color is), and `y`, `=`, and `42` all look like they're inside the string. Sometimes the entire file changes color.\nThis is your scanner's behavior, displayed visually. The editor's tokenizer entered `IN_STRING` mode when it saw `\"`. It couldn't find a closing `\"` before the end of the line (or end of file). It emitted an ERROR or a very long STRING token. The syntax highlighter colored everything in that \"string\" as string content.\nWhen you add the closing `\"`, the tokenizer re-runs (in modern editors, incrementally from the changed point), the string terminates correctly, and everything after it returns to normal colors.\nThis is not a bug in your editor. It's the correct behavior of a tokenizer that's lost context. The \"fix\" in real editors is to limit how far a string can extend without re-synchronization â€” essentially the same non-nesting rule you implemented for comments, applied to strings. Some editors use heuristic re-synchronization: \"if we haven't found a closing quote after N lines, assume the string ended and restart from here.\" This produces wrong highlighting sometimes, but it's better than highlighting an entire 10,000-line file as a string because of one missing quote.\nUnderstanding this transforms a mysterious editor behavior into a comprehensible engineering tradeoff: how much context do you maintain, and when do you sacrifice accuracy for usability?\n---\n## Three-Level View: What a Comment Really Is\n**Level 1 â€” Source Language**: The programmer writes `// TODO: fix this`. To them, it's documentation â€” invisible to the compiler, meaningful to humans.\n**Level 2 â€” The Scanner (what you're building)**: A comment is a mode change. The scanner enters `IN_LINE_COMMENT` state, discards all characters until newline, then returns to `NORMAL`. No token is emitted. From the scanner's perspective, the comment literally does not exist â€” it's as if those characters were whitespace.\n**Level 3 â€” The Token Stream (downstream)**: The parser, type checker, and code generator never see comments. They operate entirely on the token stream, which is comment-free. The only place comments survive is in specialized tools: documentation generators (Javadoc, Rust's `///` doc comments), IDEs building a comment index, and some preprocessors. Those tools run their own passes over the raw source, separate from the compilation pipeline.\nThis three-level view reveals something interesting: comments are stripped at Level 2, earlier than almost everything else. They're not \"ignored by the parser\" â€” they never reach the parser. They vanish in the scanner.\n---\n## Knowledge Cascade â€” \"Learn One, Unlock Ten\"\nYou've implemented string scanning with escape sequences, line comments, and block comments. Here's what those unlock:\n**â†’ JSON parsing â€” you've just built the hardest part**: JSON's string parsing is nearly identical to what you implemented. The JSON specification (RFC 8259) defines strings as sequences between `\"...\"` with `\\n`, `\\t`, `\\r`, `\\\"`, `\\\\`, and `\\uXXXX` escape sequences. Every JSON parser in production â€” Python's `json` module, JavaScript's `JSON.parse()`, Jackson in Java â€” has a `_scan_string()` loop that looks exactly like yours. The only addition is `\\uXXXX` Unicode escapes. You now understand the core of every JSON parser ever written.\n**â†’ Regular expressions have limits â€” and you've found the edge**: Your `IN_STRING` and `IN_MULTI_COMMENT` modes encode context that a single-pass regular expression cannot express. This is not opinion â€” it's the Pumping Lemma for regular languages, which proves that \"string with matching delimiters\" is not a regular language. The way you've worked around this â€” adding state variables (modes) to extend the DFA â€” is the same workaround used by every real lexer. When someone says \"you can't parse HTML with a regex,\" this is the formal reason: HTML's nested tags are even further beyond regular languages than matched quotes.\n**â†’ Network protocols and escape sequences â€” the same pattern everywhere**: HTTP/1.1 uses `\\r\\n` as a line terminator and percent-encoding (`%20` for space) as its escape mechanism. CSV uses `\"\"` (doubled quote) to escape a literal quote inside a quoted field. Shell uses `\\` for escaping and treats `\"...\"` and `'...'` as distinct quoting modes. SQL uses `''` to escape a quote inside a string. Every protocol that transmits text-with-special-characters has an escape mechanism. The structure of your `_scan_string()` loop â€” \"consume characters until delimiter, but treat escape prefix specially\" â€” applies to all of them. You can now implement a CSV parser, an HTTP header parser, and a shell tokenizer using the exact same pattern.\n**â†’ Error message quality as UX engineering**: The choice to report unterminated string errors at the opening quote position (not EOF) is a user experience decision disguised as a compiler feature. GCC's error messages improved dramatically between version 3 and 4 partly because of exactly this kind of positional accuracy work. Clang was designed from the start with \"best-in-class error messages\" as an explicit goal â€” Clang's error reporting infrastructure is more complex than many compilers' entire frontends. The insight here is that **where you store and propagate metadata determines the quality of your error messages**. The `start_column` snapshot you take at the top of `_scan_token()` is metadata infrastructure. Its value is entirely in the error messages it enables.\n**â†’ Incremental tokenization in language servers**: In your current scanner, `scan_tokens()` re-processes the entire source every time. In a language server (LSP â€” the Language Server Protocol that powers VS Code's \"go to definition\"), tokenizing is triggered on every keypress. For large files, re-tokenizing everything is too slow. Incremental tokenizers track which tokens are \"dirty\" (potentially affected by the edit) and re-scan only from the last \"clean\" state. The mode system you've built in this milestone is exactly what makes incremental tokenization possible: if an edit is outside a string or comment, the surrounding context is unaffected. If it's inside a string, only the string changes. Mode boundaries are re-synchronization points.\n**â†’ The formal language hierarchy â€” where you are**: The Chomsky hierarchy classifies formal languages into four types. Type 3 (Regular) languages can be recognized by finite automata and expressed as regular expressions. Type 2 (Context-Free) languages need pushdown automata (stack-based machines) â€” this is where most programming language *grammars* live. Type 1 and Type 0 are more powerful. Your tokenizer extends a DFA with explicit mode-tracking to handle constructs that are technically at the boundary of Type 3. The parser you'd write next would be a full Type 2 machine. Now you can place yourself on the formal language map â€” and understand why the tokenizer and parser are separate stages.\n---\n## Design Decision: Why Store Raw Lexeme, Not Processed Value?\nYou might wonder: why store `'\"hello\\\\nworld\"'` (the raw 15 characters) instead of `'hello\\nworld'` (the 12-character processed string)? Couldn't you process escape sequences right here in the scanner, saving the evaluator work?\n| Option | Pros | Cons | Used By |\n|--------|------|------|---------|\n| **Store raw lexeme (chosen âœ“)** | Scanner stays simple; evaluation deferred; raw source preserved | Downstream must process escapes | CPython, Clang |\n| Store processed value | Downstream simpler | Scanner does two jobs; source text lost; harder to re-emit source | Some embedded language implementations |\n| Store both | Maximum flexibility | Memory doubled; complexity added for little gain | Some IDEs for source-accurate formatting |\nThe chosen approach respects the **single responsibility principle**: the scanner categorizes, the evaluator interprets. If you process escapes in the scanner, what do you do with invalid escapes like `\\z`? Emit an error? Return a partial token? The scanner's job is to find the token boundary â€” not to validate or transform the content. Keeping those separate makes both halves easier to reason about and test.\nReal-world validation: CPython's tokenizer (`Lib/tokenize.py`) stores the raw token string. The compiler's AST builder processes string escapes when building the AST. Clang's lexer stores the raw spelling, and a separate `getSpelling()` call processes escape sequences when needed. The pattern is universal across production compilers.\n---\n## What You've Built\nYour scanner now handles the complete set of lexical constructs for a real C-like language:\n- **String literals** scanned character-by-character with the closing `\"` consumed correctly\n- **Escape sequences** (`\\n`, `\\t`, `\\r`, `\\\"`, `\\\\`) handled by consuming backslash + next character as a unit, preserving raw lexeme\n- **Unterminated strings** (no closing `\"` before newline or EOF) reported as ERROR tokens with the position of the *opening* quote\n- **Single-line comments** (`//`) consuming all characters to end-of-line, emitting no token\n- **Multi-line block comments** (`/* */`) consuming all characters including newlines until `*/`, emitting no token\n- **Unterminated block comments** (no `*/` before EOF) reported as ERROR at the position of `/*`\n- **Non-nesting comments** â€” `/* /* */` closes at the first `*/`; there is no nesting depth counter\n- **Mode isolation** â€” `//` inside `\"...\"` is string content, not a comment; `\"` inside `/* */` is comment content, not a string\n- **Line tracking** accurate inside both multi-line strings and multi-line comments, because every character passes through `advance()`\nIn Milestone 4, you'll test this complete scanner against full multi-line programs, implement error recovery strategies, verify position accuracy across long inputs, and measure performance. The hard algorithmic work is done â€” M4 is about validation and robustness.\n---\n<!-- END_MS -->\n\n\n<!-- MS_ID: tokenizer-m4 -->\n<!-- MS_ID: tokenizer-m4 -->\n# Milestone 4: Integration Testing & Error Recovery\n## Where You're Starting\nYou have a complete tokenizer. It handles single-character tokens, two-character operators with maximal munch, number literals (integer and float), identifiers, keyword lookup, string literals with escape sequences, single-line comments, and multi-line block comments. The individual pieces exist and work in isolation.\nBut software built from individually-tested pieces can still fail when those pieces run together. A real source file is not a sequence of isolated test cases â€” it's a continuous stream of characters where identifiers sit next to operators, comments precede expressions, strings contain characters that look like tokens, and newlines scatter across hundreds of lines. The pieces must work *in combination*.\nThis milestone is about proving they do. You'll write integration tests that feed complete multi-line programs through the scanner and verify the exact token stream produced. You'll build and measure error recovery â€” the mechanism that keeps the scanner running through bad input. You'll verify that position tracking doesn't drift over long inputs. And you'll run a performance sanity check to confirm the scanner completes in under a second on 10,000 lines.\nBy the end, your scanner is not just functionally complete â€” it's *validated*. That's a different and higher bar.\n\n![Tokenizer System â€” Satellite Map (Home Base)](./diagrams/diag-satellite-map.svg)\n\n---\n## The Core Misconception: A Lexer Either Succeeds or Fails\nHere's what most developers instinctively expect from a compiler tool when it encounters bad input:\n```\n$ compile myfile.c\nError: unrecognized character '@' at line 42, column 7.\n```\nAnd then nothing. One error, then stop. The compiler either processes your file successfully, or it stops at the first problem and makes you fix it before it'll tell you anything more.\nThis model is deeply ingrained from experiences with build systems, linters, and simple interpreters. It feels natural â€” the moment something is wrong, the tool's output can't be trusted, so why continue?\n**It is also wrong for production-quality tools.** Stopping at the first error forces developers into an agonizing loop:\n1. Fix error #1\n2. Recompile\n3. Discover error #2 (which was always there)\n4. Fix error #2\n5. Recompile\n6. Discover error #3\n7. Repeat...\nIf a file has ten lexical errors, stopping at the first forces ten compile-fix cycles. A developer who just made ten mistakes in a hurry â€” perhaps they're learning the language, or they copied a large block of code from a different language â€” would spend far more time discovering errors than fixing them.\n**Production lexers never stop at the first error.** They emit an `ERROR` token, skip the problematic character(s), and continue scanning. A single tokenizer pass over a file with ten errors produces a token stream containing ten `ERROR` tokens alongside all the valid tokens. The parser and error reporter can then display all ten errors in one shot.\nThis strategy has a name: **panic mode recovery** at the lexical level. The word \"panic\" comes from compiler theory â€” when the scanner hits an unrecognized character, it \"panics,\" emits what information it can (the error token with position), skips forward to try to re-synchronize, and then resumes normal operation. The scanner doesn't halt; it recovers.\n\n![Error Recovery â€” Skip and Continue](./diagrams/diag-error-recovery-flow.svg)\n\nThe key insight the Architect flagged: **the `ERROR` token IS a valid token in your token stream.** It's not a special out-of-band signal. It's a regular entry in the `list[Token]` returned by `scan_tokens()`. The scanner produces it, the parser receives it, and the error reporter processes it. Error tokens are part of the API.\nYou've actually already implemented this â€” your scanner emits `ERROR` tokens and keeps running. This milestone is about testing that behavior rigorously, understanding *why* it matters, and making sure your recovery strategy doesn't skip too much or too little.\n---\n## Error Recovery: The Three Variables\nWhen your scanner hits an unrecognized character, it faces three decisions:\n**1. What to emit?**\nYou emit an `ERROR` token containing the offending character and its exact position. This is the right answer for single unrecognized characters â€” it gives the error reporter everything it needs to tell the user \"unexpected `@` at line 42, column 7.\"\n**2. How much to skip?**\nThis is the hard part. After emitting the error, how far do you advance before resuming normal scanning?\n- **Skip one character (chosen âœ“)**: Emit `ERROR` for the unrecognized char, then let `_scan_token()` restart normally from the very next character.\n- **Skip to next whitespace**: Faster re-synchronization but risks swallowing valid tokens immediately after the bad character.\n- **Skip to next newline**: Very aggressive â€” useful for recovering from malformed directives, but skips too much for inline errors.\n- **Skip to next statement boundary**: Parser-level recovery, not lexer-level â€” the lexer doesn't know what a \"statement boundary\" is.\nFor lexical errors (single unrecognized characters), **skip-one is the right answer**. The reason: most lexical errors are isolated â€” a stray `@` in the middle of otherwise valid code. After the `@`, the scanner is perfectly positioned to continue normally. There's no cascading damage to recover from. Skip too much and you'll miss real subsequent tokens.\nThe situation would be different if you were recovering from a multi-character malformed construct. But for isolated unrecognized characters, skip-one is the production choice â€” used by GCC, Clang, and the CPython tokenizer.\n**3. Keep state or reset?**\nSkip-one naturally resets state, because you return from `_scan_token()` with an `ERROR` token, the main loop calls `_scan_token()` again, and `start`/`start_column` are re-snapshotted at the new position. No explicit state reset needed. The architecture handles it.\n\n![Multi-Error Reporting â€” Collecting All Errors in One Pass](./diagrams/diag-multi-error-collection.svg)\n\nHere's the critical implementation point: **your scanner already implements skip-one recovery** because of how `_scan_token()` is structured. When it hits an unrecognized character in the `else` branch, it returns an `ERROR` token for that single character. The main `scan_tokens()` loop then calls `_scan_token()` again, starting fresh from `current` (which is already advanced past the bad character). No additional code is needed.\n```python\n# This is ALREADY in your scanner â€” error recovery is built in\ndef _scan_token(self) -> Token | None:\n    self.start = self.current\n    self.start_column = self.column\n    char = self.advance()           # consumes exactly one character\n    # ... all the normal dispatch ...\n    else:\n        # Unknown character: emit ERROR for just this one char.\n        # 'advance()' already moved past it. Next call to _scan_token()\n        # starts fresh at 'current' â€” which is the character AFTER the bad one.\n        return Token(TokenType.ERROR, char, self.line, self.start_column)\n```\nThe recovery is implicit in the design. `advance()` always moves forward by exactly one character, and `_scan_token()` always returns control to `scan_tokens()` after one token. There's no way to get \"stuck\" on a bad character. Error recovery was free â€” you got it from the architecture.\n---\n## Collecting All Errors: The Accumulator Pattern\nThe main `scan_tokens()` loop appends every non-`None` token to `self.tokens`, regardless of its type. `ERROR` tokens go in exactly like `NUMBER` or `KEYWORD` tokens:\n```python\ndef scan_tokens(self) -> list[Token]:\n    while not self.is_at_end():\n        token = self._scan_token()\n        if token is not None:\n            self.tokens.append(token)\n    self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))\n    return self.tokens\n```\nIf you want to separate errors from valid tokens after the fact â€” for a cleaner API â€” you can filter the returned list:\n```python\ndef get_errors(self) -> list[Token]:\n    \"\"\"Extract all ERROR tokens from the scanned token stream.\"\"\"\n    return [t for t in self.tokens if t.type == TokenType.ERROR]\ndef get_valid_tokens(self) -> list[Token]:\n    \"\"\"Token stream with ERROR tokens removed.\"\"\"\n    return [t for t in self.tokens if t.type != TokenType.ERROR]\n```\nBut the important point is that `scan_tokens()` itself doesn't filter â€” it accumulates everything. The *caller* decides what to do with errors. A compiler's driver might print all errors and abort before calling the parser. A language server might pass the entire stream (including errors) to the parser so it can attempt partial analysis for code completion.\nThis \"errors as values, not exceptions\" design is the same principle behind Python's `Exception` classes, Rust's `Result<T, E>`, and Go's `(value, error)` return tuples. **Errors are data.** They flow through the system the same way correct results flow. They can be collected, counted, filtered, and reported in batch.\n---\n## The Canonical Integration Test\nBefore writing edge cases, write the canonical test: the exact token stream for a known program, verified token-by-token. This is the most important test in your suite because it exercises the entire scanner end-to-end with no mocking or isolation.\n{{DIAGRAM:diag-full-program-trace}}\nThe acceptance criteria specify this test explicitly:\n> Token stream for `if (x >= 42) { return true; }` produces exactly: `Keyword(if)`, `LParen`, `Ident(x)`, `GreaterEqual`, `Number(42)`, `RParen`, `LBrace`, `Keyword(return)`, `Keyword(true)`, `Semicolon`, `RBrace`, `EOF`\nLet's write it formally:\n```python\ndef test_canonical_expression():\n    \"\"\"\n    The single most important integration test.\n    Verifies every token type, every lexeme, and proves the full pipeline works.\n    \"\"\"\n    scanner = Scanner(\"if (x >= 42) { return true; }\")\n    tokens = scanner.scan_tokens()\n    expected = [\n        (TokenType.KEYWORD,       \"if\"),\n        (TokenType.PUNCTUATION,   \"(\"),\n        (TokenType.IDENTIFIER,    \"x\"),\n        (TokenType.GREATER_EQUAL, \">=\"),\n        (TokenType.NUMBER,        \"42\"),\n        (TokenType.PUNCTUATION,   \")\"),\n        (TokenType.PUNCTUATION,   \"{\"),\n        (TokenType.KEYWORD,       \"return\"),\n        (TokenType.KEYWORD,       \"true\"),\n        (TokenType.PUNCTUATION,   \";\"),\n        (TokenType.PUNCTUATION,   \"}\"),\n        (TokenType.EOF,           \"\"),\n    ]\n    assert len(tokens) == len(expected), (\n        f\"Token count mismatch: expected {len(expected)}, got {len(tokens)}\\n\"\n        f\"Actual: {tokens}\"\n    )\n    for i, (token, (exp_type, exp_lexeme)) in enumerate(zip(tokens, expected)):\n        assert token.type == exp_type, (\n            f\"Token {i}: type mismatch. \"\n            f\"Expected {exp_type.name}, got {token.type.name} \"\n            f\"(lexeme={token.lexeme!r})\"\n        )\n        assert token.lexeme == exp_lexeme, (\n            f\"Token {i}: lexeme mismatch. \"\n            f\"Expected {exp_lexeme!r}, got {token.lexeme!r} \"\n            f\"(type={token.type.name})\"\n        )\n```\nNotice the error messages inside each assertion. When a test fails, \"AssertionError\" tells you nothing. \"Token 3: type mismatch. Expected GREATER_EQUAL, got OPERATOR (lexeme='>')\" tells you exactly which token is wrong and why. Good assertions are part of good testing.\nNow write the integration test for a complete multi-line program. This is what makes the test a real end-to-end check:\n```python\nMULTILINE_PROGRAM = \"\"\"\\\n// Compute absolute value\nif (x >= 0) {\n    result = x;\n} else {\n    result = 0 - x;\n}\n/* Return the result */\nreturn result;\n\"\"\"\ndef test_multiline_program_complete():\n    \"\"\"\n    Full multi-line program tokenized token-by-token.\n    Tests interaction of: line comments, identifiers, operators,\n    numbers, keywords, punctuation, and block comments.\n    \"\"\"\n    scanner = Scanner(MULTILINE_PROGRAM)\n    tokens = scanner.scan_tokens()\n    # Exclude EOF for easier assertion\n    non_eof = [t for t in tokens if t.type != TokenType.EOF]\n    # Verify no ERROR tokens in a valid program\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    assert errors == [], f\"Unexpected errors in valid program: {errors}\"\n    # Spot-check key positions\n    # Line 1 is the comment â€” should produce NO tokens\n    # First token is 'if' on line 2\n    assert non_eof[0].type == TokenType.KEYWORD\n    assert non_eof[0].lexeme == \"if\"\n    assert non_eof[0].line == 2\n    assert non_eof[0].column == 1\n    # Verify '0 - x' on line 5: NUMBER(0), OPERATOR(-), IDENTIFIER(x)\n    # Find the '0' token\n    zero_tokens = [t for t in tokens\n                   if t.type == TokenType.NUMBER and t.lexeme == \"0\"]\n    assert len(zero_tokens) >= 1\n    # Find the 'return' keyword â€” should be on line 8 (after block comment)\n    return_tokens = [t for t in tokens\n                     if t.type == TokenType.KEYWORD and t.lexeme == \"return\"]\n    assert len(return_tokens) == 1\n    assert return_tokens[0].line == 8, (\n        f\"'return' should be on line 8 (after 7 lines), \"\n        f\"got line {return_tokens[0].line}\"\n    )\n    # Last non-EOF token is ';' after 'result'\n    assert non_eof[-1].type == TokenType.PUNCTUATION\n    assert non_eof[-1].lexeme == \";\"\n    # EOF should be present\n    assert tokens[-1].type == TokenType.EOF\n```\nThis test catches the bugs that unit tests miss: does the line comment on line 1 correctly produce zero tokens so that `if` lands on line 2? Does the block comment on line 7 update the line counter so that `return` lands on line 8? These are the interactions that only surface when the full input runs together.\n---\n## Testing Error Recovery Explicitly\nYour error recovery is implicit in the architecture, but you must test it explicitly. Tests prove behavior; architecture proves nothing.\n```python\ndef test_single_error_continues():\n    \"\"\"After one error, scanning resumes and finds the next valid token.\"\"\"\n    scanner = Scanner(\"@+\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].lexeme == \"@\"\n    assert tokens[1].type == TokenType.OPERATOR\n    assert tokens[1].lexeme == \"+\"\n    assert tokens[2].type == TokenType.EOF\ndef test_multiple_errors_all_collected():\n    \"\"\"\n    All errors in a single input are collected in one pass.\n    This is the fundamental multi-error reporting test.\n    \"\"\"\n    scanner = Scanner(\"@#$%\")\n    tokens = scanner.scan_tokens()\n    error_tokens = [t for t in tokens if t.type == TokenType.ERROR]\n    assert len(error_tokens) == 4, (\n        f\"Expected 4 errors (one per bad char), got {len(error_tokens)}\"\n    )\n    # Verify each error has a distinct, correct position\n    assert error_tokens[0].lexeme == \"@\" and error_tokens[0].column == 1\n    assert error_tokens[1].lexeme == \"#\" and error_tokens[1].column == 2\n    assert error_tokens[2].lexeme == \"$\" and error_tokens[2].column == 3\n    assert error_tokens[3].lexeme == \"%\" and error_tokens[3].column == 4\ndef test_errors_interspersed_with_valid():\n    \"\"\"\n    Errors interspersed with valid tokens: all tokens (valid and error)\n    appear in the correct order.\n    \"\"\"\n    scanner = Scanner(\"x @ y # z\")\n    tokens = scanner.scan_tokens()\n    types = [t.type for t in tokens]\n    expected_types = [\n        TokenType.IDENTIFIER,   # x\n        TokenType.ERROR,        # @\n        TokenType.IDENTIFIER,   # y\n        TokenType.ERROR,        # #\n        TokenType.IDENTIFIER,   # z\n        TokenType.EOF,\n    ]\n    assert types == expected_types, f\"Token type sequence wrong: {types}\"\ndef test_error_does_not_consume_next_valid_char():\n    \"\"\"\n    Error recovery skips exactly ONE character.\n    The character immediately after the bad one is NOT consumed by recovery.\n    \"\"\"\n    scanner = Scanner(\"@if\")\n    tokens = scanner.scan_tokens()\n    # '@' is ERROR, 'if' is KEYWORD â€” NOT '@i' as error and 'f' as identifier\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].lexeme == \"@\"\n    assert tokens[1].type == TokenType.KEYWORD\n    assert tokens[1].lexeme == \"if\"\ndef test_fifteen_errors_all_reported():\n    \"\"\"\n    Simulate a realistic scenario: 15 errors in one file.\n    Confirms multi-error collection scales.\n    \"\"\"\n    bad_chars = \"@#$%^&~`|\\\\??\"  # 12 unrecognized characters\n    scanner = Scanner(bad_chars)\n    tokens = scanner.scan_tokens()\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    # Every bad character should produce exactly one ERROR token\n    assert len(errors) == len(bad_chars), (\n        f\"Expected {len(bad_chars)} errors, got {len(errors)}\"\n    )\n    assert tokens[-1].type == TokenType.EOF\ndef test_error_position_on_second_line():\n    \"\"\"\n    Error on line 2 reports the correct line and column,\n    proving line tracking is accurate even after a newline.\n    \"\"\"\n    scanner = Scanner(\"valid\\n@bad\")\n    tokens = scanner.scan_tokens()\n    at_token = next(t for t in tokens if t.type == TokenType.ERROR)\n    assert at_token.line == 2, f\"Expected line 2, got {at_token.line}\"\n    assert at_token.column == 1, f\"Expected column 1, got {at_token.column}\"\n```\nThe test `test_error_does_not_consume_next_valid_char` is particularly important. It verifies that skip-one recovery is truly skip-*one* â€” it doesn't accidentally swallow the character after the error. This test would fail if `advance()` were called twice in the error branch.\n---\n## Edge Cases: The Boundary Tests\n\n![Edge Case Gallery â€” Empty Input, Single Char, Boundaries](./diagrams/diag-edge-cases-map.svg)\n\nEdge cases are the inputs that live at the boundary of your specification. They're where bugs hide because they're the inputs nobody thinks to test when they're writing the main logic.\n```python\ndef test_empty_input():\n    \"\"\"\n    Empty string produces exactly one token: EOF.\n    EOF must be present even with zero characters of input.\n    \"\"\"\n    scanner = Scanner(\"\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1, f\"Empty input should produce exactly 1 token, got {len(tokens)}\"\n    assert tokens[0].type == TokenType.EOF\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1\ndef test_single_character_valid():\n    \"\"\"Single valid character produces the right token + EOF.\"\"\"\n    for char, expected_type in [\n        (\"+\", TokenType.OPERATOR),\n        (\"(\", TokenType.PUNCTUATION),\n        (\"0\", TokenType.NUMBER),\n        (\"x\", TokenType.IDENTIFIER),\n    ]:\n        scanner = Scanner(char)\n        tokens = scanner.scan_tokens()\n        assert len(tokens) == 2, f\"Single '{char}' should produce 1 token + EOF\"\n        assert tokens[0].type == expected_type, (\n            f\"'{char}' â†’ expected {expected_type.name}, got {tokens[0].type.name}\"\n        )\n        assert tokens[1].type == TokenType.EOF\ndef test_single_character_invalid():\n    \"\"\"Single invalid character produces ERROR + EOF.\"\"\"\n    scanner = Scanner(\"@\")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].lexeme == \"@\"\n    assert tokens[1].type == TokenType.EOF\ndef test_single_character_whitespace():\n    \"\"\"Single whitespace character produces only EOF.\"\"\"\n    for ws in [\" \", \"\\t\", \"\\n\", \"\\r\"]:\n        scanner = Scanner(ws)\n        tokens = scanner.scan_tokens()\n        assert len(tokens) == 1, f\"Whitespace {ws!r} should produce only EOF\"\n        assert tokens[0].type == TokenType.EOF\ndef test_maximum_length_identifier():\n    \"\"\"\n    Very long identifiers are handled correctly.\n    Python strings have no practical length limit, so this tests\n    that the scanner loop doesn't have an artificial cutoff.\n    \"\"\"\n    long_id = \"a\" * 10_000\n    scanner = Scanner(long_id)\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 2   # IDENTIFIER + EOF\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].lexeme == long_id\n    assert len(tokens[0].lexeme) == 10_000\ndef test_maximum_length_keyword_prefix():\n    \"\"\"\n    An identifier that STARTS with a keyword but is longer must be IDENTIFIER.\n    'returning' starts with 'return' but is not the keyword.\n    \"\"\"\n    for kw, longer in [\n        (\"if\", \"iffy\"),\n        (\"else\", \"elsewhere\"),\n        (\"while\", \"while_true\"),\n        (\"return\", \"returning\"),\n        (\"true\", \"trueness\"),\n        (\"false\", \"falsehood\"),\n        (\"null\", \"nullify\"),\n    ]:\n        scanner = Scanner(longer)\n        tokens = scanner.scan_tokens()\n        assert tokens[0].type == TokenType.IDENTIFIER, (\n            f\"'{longer}' starts with keyword '{kw}' but must be IDENTIFIER\"\n        )\n        assert tokens[0].lexeme == longer\ndef test_all_whitespace_types():\n    \"\"\"All whitespace characters (space, tab, CR, LF) produce only EOF.\"\"\"\n    scanner = Scanner(\"   \\t\\t\\r\\n\\t  \\r\\n  \")\n    tokens = scanner.scan_tokens()\n    assert len(tokens) == 1\n    assert tokens[0].type == TokenType.EOF\ndef test_operator_at_eof():\n    \"\"\"A valid operator as the last character (before EOF) is handled correctly.\"\"\"\n    scanner = Scanner(\"+\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.OPERATOR\n    assert tokens[0].lexeme == \"+\"\n    assert tokens[1].type == TokenType.EOF\ndef test_number_zero():\n    \"\"\"The number '0' is a valid integer literal.\"\"\"\n    scanner = Scanner(\"0\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.NUMBER\n    assert tokens[0].lexeme == \"0\"\ndef test_consecutive_operators():\n    \"\"\"Multiple operators with no whitespace are tokenized individually.\"\"\"\n    scanner = Scanner(\"+-*/\")\n    tokens = scanner.scan_tokens()\n    lexemes = [t.lexeme for t in tokens[:-1]]\n    assert lexemes == [\"+\", \"-\", \"*\", \"/\"]\ndef test_string_immediately_followed_by_token():\n    \"\"\"String literal followed immediately by a token, no whitespace.\"\"\"\n    scanner = Scanner('\"hello\"+')\n    tokens = scanner.scan_tokens()\n    assert tokens[0].type == TokenType.STRING\n    assert tokens[0].lexeme == '\"hello\"'\n    assert tokens[1].type == TokenType.OPERATOR\n    assert tokens[1].lexeme == \"+\"\n```\n---\n## Position Accuracy Verification: Preventing Drift\nPosition tracking is the feature that's hardest to test manually but most important to get right. Errors in line and column numbers accumulate â€” a single off-by-one at line 5 means everything from line 5 onward reports the wrong position.\n{{DIAGRAM:diag-position-drift}}\nThis category of test verifies that position information doesn't *drift* â€” that the accumulated error over a multi-line input is zero, not just small.\n```python\ndef test_line_tracking_across_newlines():\n    \"\"\"\n    Each newline increments the line counter by exactly 1.\n    Verify multiple tokens across multiple lines.\n    \"\"\"\n    source = \"a\\nb\\nc\\nd\\ne\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    # Tokens: a(line1), b(line2), c(line3), d(line4), e(line5), EOF\n    for i, token in enumerate(tokens[:-1]):   # skip EOF\n        expected_line = i + 1\n        assert token.line == expected_line, (\n            f\"Token {token.lexeme!r}: expected line {expected_line}, \"\n            f\"got {token.line}\"\n        )\n        assert token.column == 1, (\n            f\"Token {token.lexeme!r}: expected column 1, got {token.column}\"\n        )\ndef test_column_tracking_across_tokens():\n    \"\"\"Tokens on the same line have correctly incrementing columns.\"\"\"\n    scanner = Scanner(\"a + b\")\n    tokens = scanner.scan_tokens()\n    # 'a' at column 1, '+' at column 3, 'b' at column 5\n    assert tokens[0].column == 1   # 'a'\n    assert tokens[1].column == 3   # '+' (space between)\n    assert tokens[2].column == 5   # 'b'\ndef test_column_resets_after_newline():\n    \"\"\"After a newline, column resets to 1 for the first token on the new line.\"\"\"\n    scanner = Scanner(\"ab\\ncd\")\n    tokens = scanner.scan_tokens()\n    assert tokens[0].lexeme == \"ab\"\n    assert tokens[0].line == 1\n    assert tokens[0].column == 1\n    assert tokens[1].lexeme == \"cd\"\n    assert tokens[1].line == 2\n    assert tokens[1].column == 1   # reset after '\\n'\ndef test_position_in_multiline_string_block():\n    \"\"\"\n    Position after a multi-line comment is correct,\n    demonstrating that newlines inside comments are tracked.\n    \"\"\"\n    source = (\n        \"a\\n\"                # line 1: identifier 'a'\n        \"/* line 2\\n\"        # line 2: start of block comment\n        \"line 3\\n\"           # line 3: inside block comment\n        \"line 4 */\\n\"        # line 4: end of block comment\n        \"b\"                  # line 5: identifier 'b'\n    )\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    non_eof = [t for t in tokens if t.type != TokenType.EOF]\n    assert len(non_eof) == 2   # 'a' and 'b', comment produced nothing\n    assert non_eof[0].lexeme == \"a\"\n    assert non_eof[0].line == 1\n    assert non_eof[1].lexeme == \"b\"\n    assert non_eof[1].line == 5, (\n        f\"'b' should be on line 5 (after 4-line comment block), \"\n        f\"got line {non_eof[1].line}\"\n    )\ndef test_position_fifty_line_program():\n    \"\"\"\n    Verify position accuracy on a 50-line program.\n    Simulates a real multi-line input and checks the final token's position.\n    \"\"\"\n    lines = [f\"x{i} = {i};\" for i in range(1, 51)]\n    source = \"\\n\".join(lines)\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    # Find all tokens on line 50\n    line_50_tokens = [t for t in tokens if t.line == 50]\n    assert line_50_tokens, \"Should have tokens on line 50\"\n    # First token on line 50 should be identifier 'x50'\n    first_on_50 = line_50_tokens[0]\n    assert first_on_50.type == TokenType.IDENTIFIER\n    assert first_on_50.lexeme == \"x50\"\n    assert first_on_50.column == 1\ndef test_error_position_accuracy_midline():\n    \"\"\"\n    An error character in the middle of a line reports the exact column.\n    \"\"\"\n    scanner = Scanner(\"abc @ def\")\n    tokens = scanner.scan_tokens()\n    error = next(t for t in tokens if t.type == TokenType.ERROR)\n    assert error.line == 1\n    assert error.column == 5   # 'abc ' = 4 chars, '@' is at column 5\n    assert error.lexeme == \"@\"\ndef test_windows_line_endings_not_double_counted():\n    \"\"\"\n    Windows CRLF line endings (\\r\\n) count as ONE line, not two.\n    '\\r' is whitespace, '\\n' triggers line increment.\n    \"\"\"\n    source = \"a\\r\\nb\\r\\nc\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    non_eof = [t for t in tokens if t.type != TokenType.EOF]\n    assert non_eof[0].line == 1   # 'a'\n    assert non_eof[1].line == 2   # 'b' â€” not line 3 (if \\r\\n counted twice)\n    assert non_eof[2].line == 3   # 'c'\n```\nThe `test_windows_line_endings_not_double_counted` test is particularly important. This bug â€” counting `\\r\\n` as two newlines â€” is remarkably common, and it's invisible until someone runs your tokenizer on a file edited in Windows Notepad. The test catches it.\n---\n## The Complete Integration Test Suite: A Real Program\nNow write the comprehensive test that ties everything together. This is the test that a compiler project would use as its regression suite â€” the \"golden test\" that must pass before any change is merged.\n```python\nCOMPLETE_TEST_PROGRAM = \"\"\"\\\n// Program: factorial\n// Tests all token types\n/* Multi-line comment block\n   testing comment handling */\nx = 0;\nresult = null;\nif (x != 0) {\n    while (x >= 1) {\n        result = result * x;\n        x = x - 1;\n    }\n} else {\n    result = 1;\n}\ngreeting = \"hello\\\\nworld\";\ncheck = (result == 42);\nvalid = true;\nreturn result;\n\"\"\"\ndef test_complete_program_no_errors():\n    \"\"\"Complete program produces no ERROR tokens.\"\"\"\n    scanner = Scanner(COMPLETE_TEST_PROGRAM)\n    tokens = scanner.scan_tokens()\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    assert errors == [], (\n        f\"Expected no errors in valid program, got: {errors}\"\n    )\ndef test_complete_program_ends_with_eof():\n    \"\"\"Token stream always ends with EOF.\"\"\"\n    scanner = Scanner(COMPLETE_TEST_PROGRAM)\n    tokens = scanner.scan_tokens()\n    assert tokens[-1].type == TokenType.EOF\ndef test_complete_program_token_types():\n    \"\"\"All expected token types appear in the complete program.\"\"\"\n    scanner = Scanner(COMPLETE_TEST_PROGRAM)\n    tokens = scanner.scan_tokens()\n    token_types_present = {t.type for t in tokens}\n    # Every type used in the program should appear\n    expected_types = {\n        TokenType.KEYWORD,       # if, else, while, return, true, null\n        TokenType.IDENTIFIER,    # x, result, greeting, check, valid\n        TokenType.NUMBER,        # 0, 1, 42\n        TokenType.STRING,        # \"hello\\nworld\"\n        TokenType.OPERATOR,      # *, -\n        TokenType.ASSIGN,        # =\n        TokenType.NOT_EQUAL,     # !=\n        TokenType.GREATER_EQUAL, # >=\n        TokenType.EQUALS,        # ==\n        TokenType.PUNCTUATION,   # (, ), {, }, ;\n        TokenType.EOF,\n    }\n    missing = expected_types - token_types_present\n    assert missing == set(), f\"Expected token types not found: {missing}\"\ndef test_complete_program_keyword_counts():\n    \"\"\"Verify keyword occurrences match source.\"\"\"\n    scanner = Scanner(COMPLETE_TEST_PROGRAM)\n    tokens = scanner.scan_tokens()\n    keywords = [t.lexeme for t in tokens if t.type == TokenType.KEYWORD]\n    assert keywords.count(\"if\") == 1\n    assert keywords.count(\"else\") == 1\n    assert keywords.count(\"while\") == 1\n    assert keywords.count(\"return\") == 1\n    assert keywords.count(\"true\") == 1\n    assert keywords.count(\"null\") == 1\ndef test_complete_program_string_token():\n    \"\"\"The string literal in the program is tokenized correctly.\"\"\"\n    scanner = Scanner(COMPLETE_TEST_PROGRAM)\n    tokens = scanner.scan_tokens()\n    string_tokens = [t for t in tokens if t.type == TokenType.STRING]\n    assert len(string_tokens) == 1\n    # Raw lexeme includes the quotes and the raw backslash-n\n    assert string_tokens[0].lexeme == '\"hello\\\\\\\\nworld\"' or \\\n           \"hello\" in string_tokens[0].lexeme  # raw lexeme check\ndef test_complete_program_comment_lines_produce_no_tokens():\n    \"\"\"\n    Lines 1, 2, and 4-5 are comments.\n    First real token ('x') should be on line 7.\n    \"\"\"\n    scanner = Scanner(COMPLETE_TEST_PROGRAM)\n    tokens = scanner.scan_tokens()\n    first_real_token = tokens[0]\n    assert first_real_token.type == TokenType.IDENTIFIER\n    assert first_real_token.lexeme == \"x\"\n    # Line 7 in the source (after 2 line comments + blank + block comment + blank)\n    assert first_real_token.line == 7, (\n        f\"First token should be on line 7, got line {first_real_token.line}\"\n    )\n```\n---\n## Performance: The 10,000-Line Benchmark\nA tokenizer that works correctly but takes 30 seconds on a typical source file is not useful. The performance requirement is concrete: 10,000 lines in under 1 second.\nLet's understand why this is achievable, and why it's the right threshold.\n**Why a well-written lexer is I/O bound, not CPU bound:**\nA Python lexer reading a 10,000-line file (roughly 300,000 characters at 30 chars/line average) performs about 300,000 character comparisons. Python executes roughly 10â€“50 million simple operations per second. At the lower bound, 300,000 comparisons takes about 6 milliseconds of CPU time. Even accounting for Python's overhead, function call costs, and list appends, the total time should be well under 1 second.\nIn production (C/C++/Rust implementations), lexers process gigabytes per second because they're limited by memory bandwidth â€” data transfer from RAM â€” not computation. CPython's tokenizer can process over 100,000 lines per second. GCC's lexer processes millions of lines per second. The 1-second/10,000-line requirement is a very generous budget.\n**What would make a lexer slow?**\n- **Excessive string allocation**: creating many small intermediate strings in the inner loop\n- **Quadratic operations**: searching or slicing in O(n) inside an O(n) loop = O(nÂ²)\n- **Regex compilation**: compiling a regex pattern inside the scan loop rather than once at startup\n- **Unnecessary I/O**: reading from disk inside the scan loop (rare, but catastrophic)\nYour implementation avoids all of these by design: you maintain a `current` index into the already-loaded source string and slice only at token boundaries (`source[start:current]`). The inner loop does constant-time work per character.\n```python\nimport time\ndef generate_test_program(num_lines: int) -> str:\n    \"\"\"\n    Generate a synthetic program with num_lines lines.\n    Each line contains a variety of token types to exercise all scanner paths.\n    \"\"\"\n    lines = []\n    for i in range(num_lines):\n        # Rotate through different line patterns for realistic coverage\n        pattern = i % 6\n        if pattern == 0:\n            lines.append(f\"    x{i} = {i};\")\n        elif pattern == 1:\n            lines.append(f\"    // comment on line {i}\")\n        elif pattern == 2:\n            lines.append(f'    label{i} = \"string {i}\";')\n        elif pattern == 3:\n            lines.append(f\"    if (x{i} >= {i}) {{\")\n        elif pattern == 4:\n            lines.append(f\"        result = result + x{i};\")\n        else:\n            lines.append(f\"    }}\")\n    return \"\\n\".join(lines)\ndef test_performance_ten_thousand_lines():\n    \"\"\"\n    Tokenizing 10,000 lines must complete in under 1 second.\n    This verifies the scanner has no O(nÂ²) behavior or pathological cases.\n    \"\"\"\n    source = generate_test_program(10_000)\n    # Warm up Python's JIT-like optimizations (though CPython doesn't JIT,\n    # this ensures module imports and initial overhead don't count)\n    Scanner(source[:100]).scan_tokens()\n    start_time = time.perf_counter()\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    elapsed = time.perf_counter() - start_time\n    # Basic correctness: should have many tokens and end with EOF\n    assert tokens[-1].type == TokenType.EOF\n    assert len(tokens) > 10_000  # at least one token per line on average\n    assert elapsed < 1.0, (\n        f\"Performance failure: 10,000-line input took {elapsed:.3f}s \"\n        f\"(limit: 1.0s). Token count: {len(tokens)}\"\n    )\n    # Also report the performance (useful even when test passes)\n    lines_per_second = 10_000 / elapsed\n    print(f\"\\nPerformance: {lines_per_second:.0f} lines/second \"\n          f\"({elapsed*1000:.1f}ms for 10,000 lines)\")\ndef test_performance_pathological_string():\n    \"\"\"\n    A string literal with 10,000 characters shouldn't cause O(nÂ²) behavior.\n    The string scanner is a simple loop â€” this verifies no quadratic slicing.\n    \"\"\"\n    long_string = '\"' + \"x\" * 10_000 + '\"'\n    start_time = time.perf_counter()\n    scanner = Scanner(long_string)\n    tokens = scanner.scan_tokens()\n    elapsed = time.perf_counter() - start_time\n    assert tokens[0].type == TokenType.STRING\n    assert len(tokens[0].lexeme) == 10_002   # 10000 chars + 2 quotes\n    assert elapsed < 0.5, f\"Long string took {elapsed:.3f}s (too slow)\"\ndef test_performance_many_errors():\n    \"\"\"\n    A file full of error characters doesn't cause degenerate behavior.\n    Error recovery (skip-one) should be O(n) on the number of bad chars.\n    \"\"\"\n    all_errors = \"@#$%\" * 2_500   # 10,000 error characters\n    start_time = time.perf_counter()\n    scanner = Scanner(all_errors)\n    tokens = scanner.scan_tokens()\n    elapsed = time.perf_counter() - start_time\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    assert len(errors) == 10_000\n    assert elapsed < 1.0, (\n        f\"Error-heavy input took {elapsed:.3f}s (limit: 1.0s)\"\n    )\n```\n**Interpreting the performance numbers:**\nIf your scanner takes 0.05 seconds on 10,000 lines, that's 200,000 lines/second â€” excellent for Python. If it takes 0.5 seconds, that's 20,000 lines/second â€” still well within spec. If it takes 3+ seconds, there's an O(nÂ²) bug somewhere, likely in string slicing inside an inner loop.\nThe most common O(nÂ²) mistake is accumulating the lexeme character-by-character using string concatenation inside the scan loop:\n```python\n# âŒ O(nÂ²) â€” string concatenation creates a new string each time\ndef _scan_identifier(self) -> Token:\n    lexeme = \"\"\n    while self.peek().isalnum() or self.peek() == \"_\":\n        lexeme += self.advance()  # new string object every iteration!\n    ...\n```\n```python\n# âœ… O(n) â€” slice once at the end\ndef _scan_identifier(self) -> Token:\n    while self.peek().isalnum() or self.peek() == \"_\":\n        self.advance()  # just advance the index\n    text = self.source[self.start:self.current]  # ONE slice at the end\n    ...\n```\nYour implementation already uses the slice approach â€” but if the performance test fails, this is the first thing to check.\n---\n## The Full Test Runner\nOrganize all your tests into a runnable suite. Python's built-in `unittest` module or `pytest` (installed separately) both work:\n```python\n# test_scanner.py â€” complete test file\nimport time\nfrom scanner import Scanner, Token, TokenType  # adjust import to your file\n# â”€â”€ Milestone 1: Foundation tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_m1_empty_input():\n    tokens = Scanner(\"\").scan_tokens()\n    assert len(tokens) == 1 and tokens[0].type == TokenType.EOF\ndef test_m1_single_char_operators():\n    tokens = Scanner(\"+-*/\").scan_tokens()\n    assert len(tokens) == 5  # 4 operators + EOF\n    assert all(t.type == TokenType.OPERATOR for t in tokens[:4])\n# â”€â”€ Milestone 2: Multi-character tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_m2_canonical_expression():\n    tokens = Scanner(\"if (x >= 42) { return true; }\").scan_tokens()\n    expected_types = [\n        TokenType.KEYWORD, TokenType.PUNCTUATION, TokenType.IDENTIFIER,\n        TokenType.GREATER_EQUAL, TokenType.NUMBER, TokenType.PUNCTUATION,\n        TokenType.PUNCTUATION, TokenType.KEYWORD, TokenType.KEYWORD,\n        TokenType.PUNCTUATION, TokenType.PUNCTUATION, TokenType.EOF,\n    ]\n    assert [t.type for t in tokens] == expected_types\n# â”€â”€ Milestone 3: Strings and comments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_m3_string_with_escape():\n    tokens = Scanner(r'\"hello\\nworld\"').scan_tokens()\n    assert tokens[0].type == TokenType.STRING\ndef test_m3_block_comment_updates_line():\n    tokens = Scanner(\"/* line1\\nline2\\n*/x\").scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert tokens[0].line == 3\n# â”€â”€ Milestone 4: Integration and error recovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef test_m4_error_recovery_skip_one():\n    tokens = Scanner(\"@+\").scan_tokens()\n    assert tokens[0].type == TokenType.ERROR\n    assert tokens[0].lexeme == \"@\"\n    assert tokens[1].type == TokenType.OPERATOR\ndef test_m4_fifteen_errors_all_reported():\n    bad = \"@#$%^&~`|\\\\?? \"  # mix of bad chars and whitespace\n    tokens = Scanner(bad).scan_tokens()\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    assert len(errors) > 0  # at least some errors\ndef test_m4_max_length_identifier():\n    long_id = \"x\" * 10_000\n    tokens = Scanner(long_id).scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER\n    assert len(tokens[0].lexeme) == 10_000\ndef test_m4_position_fifty_lines():\n    source = \"\\n\".join(f\"x{i}\" for i in range(1, 51))\n    tokens = Scanner(source).scan_tokens()\n    last_id = [t for t in tokens if t.type == TokenType.IDENTIFIER][-1]\n    assert last_id.line == 50\ndef test_m4_performance():\n    source = (\"x = 42;\\n\" * 10_000)\n    start = time.perf_counter()\n    tokens = Scanner(source).scan_tokens()\n    elapsed = time.perf_counter() - start\n    assert elapsed < 1.0, f\"Too slow: {elapsed:.3f}s\"\n    assert tokens[-1].type == TokenType.EOF\nif __name__ == \"__main__\":\n    # Simple test runner without pytest\n    import sys\n    tests = [v for k, v in list(globals().items()) if k.startswith(\"test_\")]\n    passed = failed = 0\n    for test in tests:\n        try:\n            test()\n            print(f\"  âœ“ {test.__name__}\")\n            passed += 1\n        except AssertionError as e:\n            print(f\"  âœ— {test.__name__}: {e}\")\n            failed += 1\n        except Exception as e:\n            print(f\"  âœ— {test.__name__}: EXCEPTION: {e}\")\n            failed += 1\n    print(f\"\\n{passed} passed, {failed} failed\")\n    sys.exit(0 if failed == 0 else 1)\n```\nRun this file. Every test should pass. If any fail, the error messages in the assertions will tell you exactly which expectation was violated.\n---\n## Three-Level View: What Testing Means at Each Layer\n**Level 1 â€” Source Language (the program under test)**: To the programmer, `if (x >= 42) { return true; }` is a conditional expression. They expect their compiler to understand it. If the tokenizer breaks it, they see a mysterious parse error even though the syntax is correct. Correct tokenization is invisible â€” users only notice it when it's wrong.\n**Level 2 â€” The Scanner (what you're testing)**: At this level, the integration test is a verification of the *interface contract*: given this exact input string, produce this exact token list. The token list is the scanner's public API. Tests at this level pin down the API's behavior completely â€” they're the spec expressed as executable code.\n**Level 3 â€” The Parser and Downstream (future consumers)**: The parser that will consume your token stream needs to trust it completely. If your scanner emits `OPERATOR('>')` + `ASSIGN('=')` instead of `GREATER_EQUAL('>=')`, the parser fails. The integration tests you write here are simultaneously tests of the scanner *and* documentation of the interface that the parser will rely on. Every test is a contract between the scanner and its consumers.\n\n![Where the Tokenizer Fits â€” The Full Compiler Pipeline](./diagrams/diag-tokenizer-in-compiler-pipeline.svg)\n\n---\n## Common Pitfalls in Integration Testing\n### Pitfall 1: Testing Only the Happy Path\nThe most dangerous gap in test suites is missing error cases. Unit tests for string scanning often only test valid strings. Integration tests often only use valid programs. But the acceptance criteria explicitly require error recovery â€” which means you need programs with errors.\n```python\ndef test_program_with_deliberate_errors():\n    \"\"\"\n    A program containing invalid characters should tokenize everything\n    it CAN tokenize correctly, and report all errors.\n    \"\"\"\n    source = \"x = @42;\\ny = #5;\"\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    # Two error tokens: '@' and '#'\n    errors = [t for t in tokens if t.type == TokenType.ERROR]\n    assert len(errors) == 2\n    assert errors[0].lexeme == \"@\"\n    assert errors[0].line == 1\n    assert errors[1].lexeme == \"#\"\n    assert errors[1].line == 2\n    # Valid tokens still present: x, =, 42, ;, y, =, 5, ;\n    valid = [t for t in tokens if t.type != TokenType.ERROR\n             and t.type != TokenType.EOF]\n    identifiers = [t for t in valid if t.type == TokenType.IDENTIFIER]\n    assert len(identifiers) == 2   # 'x' and 'y'\n    numbers = [t for t in valid if t.type == TokenType.NUMBER]\n    assert len(numbers) == 2   # '42' and '5'\n```\n### Pitfall 2: Position Drift That Only Shows Up Late\nA position tracking bug might be invisible for the first 5 lines and only manifest on line 100. This is why the 50-line and performance tests are valuable even if they seem redundant with the earlier line-tracking tests. Accumulated drift â€” a bug that's off by 1 line every 10 lines â€” is caught by longer tests.\n```python\ndef test_no_position_drift_over_long_input():\n    \"\"\"\n    Generate 100 single-token lines and verify EVERY token's line number.\n    Catches any drift that accumulates over multiple lines.\n    \"\"\"\n    num_lines = 100\n    lines = [f\"x{i}\" for i in range(num_lines)]\n    source = \"\\n\".join(lines)\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    non_eof = [t for t in tokens if t.type != TokenType.EOF]\n    assert len(non_eof) == num_lines\n    for i, token in enumerate(non_eof):\n        expected_line = i + 1\n        assert token.line == expected_line, (\n            f\"Token {i} ('{token.lexeme}'): expected line {expected_line}, \"\n            f\"got {token.line}. Position drift detected.\"\n        )\n        assert token.column == 1, (\n            f\"Token {i} ('{token.lexeme}'): expected column 1, \"\n            f\"got {token.column}\"\n        )\n```\n### Pitfall 3: Asserting Token Count Instead of Token Content\n```python\n# âŒ Fragile: tests only that SOME tokens exist, not what they are\ndef test_expression():\n    tokens = Scanner(\"x + y\").scan_tokens()\n    assert len(tokens) == 4   # wrong: tells you nothing about content\n# âœ… Thorough: tests type AND lexeme of every token\ndef test_expression():\n    tokens = Scanner(\"x + y\").scan_tokens()\n    assert tokens[0].type == TokenType.IDENTIFIER and tokens[0].lexeme == \"x\"\n    assert tokens[1].type == TokenType.OPERATOR and tokens[1].lexeme == \"+\"\n    assert tokens[2].type == TokenType.IDENTIFIER and tokens[2].lexeme == \"y\"\n    assert tokens[3].type == TokenType.EOF\n```\nToken count tests pass even when the scanner emits `KEYWORD` instead of `IDENTIFIER`, or `ASSIGN` instead of `EQUALS`. Content tests catch those bugs. Test content, not count.\n---\n## Knowledge Cascade â€” \"Learn One, Unlock Ten\"\nYou've validated a complete tokenizer with integration tests, error recovery, position accuracy, and performance. Here's what that knowledge unlocks.\n**â†’ Error recovery in parsers uses the same principle, at a higher level.** Your lexer recovers from unknown characters by skipping one character and continuing. Parsers face the same problem with invalid token sequences â€” they hit a token that doesn't fit any grammar rule and must decide how to continue. The parser-level version of panic mode synchronizes on *statement boundaries* â€” typically semicolons and closing braces â€” rather than single characters. When a parser sees an unexpected token, it discards tokens until it finds a `;` or `}` that marks the end of a statement, then resumes. Your lexer's skip-one strategy and the parser's synchronize-on-boundary strategy are the same algorithm applied at different granularities. If you go on to build a parser, you'll implement this exact pattern again.\n**â†’ Multi-error reporting is a product decision as much as an engineering one.** The Rust compiler's reputation for excellent error messages is not an accident â€” it was a deliberate design goal, described in early RFC discussions. Rust's compiler team invested heavily in collecting multiple errors, explaining them in natural language, and suggesting fixes. That quality starts at the lexer, which emits error tokens instead of halting. Go's compiler also reports multiple errors but is notably more terse. CPython's tokenizer traditionally stops at the first error (which is why `python -c \"syntaxerror;another\"` only reports the first problem). The technical infrastructure for multi-error reporting is the same whether the UX is excellent or poor â€” but Rust put UX investment on top of that infrastructure. Understanding the difference helps you recognize that compiler quality is partly a values decision.\n**â†’ \"Expected token stream\" tests are how GCC, Clang, and rustc test their frontends.** The test pattern you've built here â€” `assert token_list == expected` â€” is called a \"golden test\" or \"snapshot test.\" GCC's test suite contains thousands of source files paired with expected diagnostic output. Clang's `FileCheck` utility verifies that specific patterns appear in compiler output. Rust's `compiletest` framework runs programs and checks the exact error messages. When you read \"this PR fixes a regression in the lexer's handling of Unicode identifiers\" in a compiler changelog, someone added a new golden test that exposed the bug and verified the fix. You've built the same infrastructure.\n**â†’ The token stream is an API, and Error tokens are part of that API.** Software engineers who build microservices spend a lot of time thinking about error contracts in APIs â€” what does a 400 error mean? What does a 500 error mean? What fields does the error response include? Your token stream has the same structure: `ERROR` tokens are a defined part of the API contract, with defined fields (`lexeme`, `line`, `column`). A parser written to consume your token stream must handle `ERROR` tokens gracefully â€” perhaps by skipping them, perhaps by triggering its own error recovery, perhaps by collecting them. The important point is that the parser must handle them, because they're defined in the contract. Treating errors as values (tokens) rather than exceptions (halts) is the architectural decision that makes this possible.\n**â†’ Lexer performance is I/O bound in practice; CPU-bound in Python.** A C or Rust lexer processing a 10,000-line file is typically limited by memory bandwidth â€” the time to read 300KB of data from RAM into cache â€” not by computation. That's why production lexers can process gigabytes per second: they're essentially doing a memcpy with some light branching. CPython is different: the interpreter overhead makes pure Python code significantly slower than native code, so a Python lexer IS CPU-bound in practice. If you needed to make your Python lexer faster, the right approach is to move the hot loop to C extension code (as CPython's own `tokenize` module does for its scanner) or use PyPy (which JIT-compiles Python to native code and would make your scanner 5â€“10x faster). Understanding what bottleneck you're fighting â€” memory bandwidth vs. interpreter overhead vs. algorithm complexity â€” changes how you approach optimization.\n---\n## What You've Built\nWith Milestone 4 complete, you have:\n- **Integration tests** that verify the complete token stream for multi-line programs token-by-token, catching interactions that unit tests miss\n- **Proven error recovery**: after any unrecognized character, scanning continues from the next character, collecting all errors in a single pass\n- **Multi-error reporting**: a file with 15 lexical errors produces a token stream with 15 `ERROR` tokens â€” developers see all their mistakes at once\n- **The canonical expression test**: `if (x >= 42) { return true; }` produces exactly the right 12 tokens, verified type by type and lexeme by lexeme\n- **Edge case coverage**: empty input, single characters, maximum-length identifiers, whitespace-only input, Windows line endings\n- **Position accuracy verification**: no drift over 50+ line inputs, correct column reset on newlines, correct line increment inside multi-line comments\n- **Performance validation**: 10,000 lines in under 1 second, with no O(nÂ²) bottlenecks\nYour tokenizer is complete. It transforms a flat string of characters into a structured, categorized, position-annotated token stream â€” the same fundamental transformation that powers every compiler frontend from GCC to V8, every IDE's syntax highlighting, every linter's error reporting, and every language server's autocomplete. You've built the real thing.\nThe next stage in the compiler pipeline â€” the parser â€” will receive this token stream and build an Abstract Syntax Tree from it, imposing hierarchical structure on the flat sequence you've produced. Every bug your tokenizer would have introduced â€” wrong token types, wrong positions, missed errors â€” would have corrupted the AST silently. Because your tokenizer is tested and correct, the parser gets a clean, reliable input.\n---\n<!-- END_MS -->\n\n\n\n\n# TDD\n\nA character-level finite state machine lexer for a simple C-like language, built milestone-by-milestone in strict TDD fashion. Every data structure is pinned by a memory layout diagram, every state transition by a state machine diagram, every algorithm by a step-by-step trace. The token stream is the public API contract; tests are the spec expressed as executable code.\n\n\n\n<!-- TDD_MOD_ID: tokenizer-m1 -->\n# MODULE SPECIFICATION: Token Types & Scanner Foundation (tokenizer-m1)\n\n## 1. Module Charter\nThe **Token Types & Scanner Foundation** module serves as the entry point for the lexical analysis phase. Its primary responsibility is to define the structured vocabulary of the compiler (the `TokenType`) and the basic infrastructure required to traverse raw source text character-by-character. \n\n**Core Responsibilities:**\n- Define a closed set of categories for source code fragments (Lexemes).\n- Implement a stateful `Scanner` that maintains a cursor and human-readable position metadata (line/column).\n- Provide low-level primitives for \"consuming\" characters (`advance`) and \"inspecting\" characters (`peek`).\n- Distinguish between significant tokens (operators, punctuation) and insignificant characters (whitespace).\n- Emit a sentinel `EOF` (End Of File) token to prevent parser overrun.\n- Implement \"Skip-One\" error recovery for unrecognized characters.\n\n**Out of Scope:**\n- Scanning multi-character tokens such as `>=` or `!=` (reserved for M2).\n- Scanning numeric literals or identifiers (reserved for M2).\n- Handling strings, escape sequences, or comments (reserved for M3).\n\n**Invariants:**\n- The `current` pointer must never exceed the length of the source string.\n- The `line` and `column` metadata must stay synchronized with every character consumed via `advance()`.\n- Every successful call to `scan_tokens()` must result in a list ending with a `TokenType.EOF` token.\n\n---\n\n## 2. File Structure\nThe implementation shall be organized into the following files, created in this order:\n\n1. `token_type.py`: Definition of the `TokenType` enumeration.\n2. `token_class.py`: Definition of the `Token` data structure.\n3. `scanner.py`: The core `Scanner` class logic and character traversal.\n4. `test_foundation.py`: Unit tests for foundation-level scanning.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 TokenType (Enum)\nWe use a Python `Enum` to define the closed universe of token categories. Using `auto()` ensures unique integer identity without manual value management.\n\n```python\nfrom enum import Enum, auto\n\nclass TokenType(Enum):\n    # Literals (Foundational types)\n    NUMBER      = auto()\n    STRING      = auto()\n    \n    # Names\n    IDENTIFIER  = auto()\n    KEYWORD     = auto()\n    \n    # Operators (M1 only handles single-char versions)\n    OPERATOR    = auto()   # +, -, *, /\n    \n    # Structural Punctuation\n    PUNCTUATION = auto()   # (, ), {, }, [, ], ;, ,\n    \n    # Special Tokens\n    EOF         = auto()\n    ERROR       = auto()\n```\n\n### 3.2 Token (Dataclass)\nThe `Token` is a value object representing a recognized lexeme.\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass Token:\n    type: TokenType   # The category\n    lexeme: str       # The raw substring from source\n    line: int         # 1-based line number\n    column: int       # 1-based column number (at start of lexeme)\n\n    def __repr__(self) -> str:\n        return f\"Token({self.type.name}, {self.lexeme!r}, line={self.line}, col={self.column})\"\n```\n\n### 3.3 Scanner State\nThe `Scanner` is a stateful iterator over the source string.\n\n| Field | Type | Purpose |\n| :--- | :--- | :--- |\n| `source` | `str` | The raw input text. |\n| `tokens` | `list[Token]` | The accumulated list of recognized tokens. |\n| `start` | `int` | Byte offset of the first character in the current lexeme being scanned. |\n| `current` | `int` | Byte offset of the next character to be read. |\n| `line` | `int` | Current 1-based line number in the source. |\n| `column` | `int` | Current 1-based column number in the source. |\n| `start_column`| `int` | Snapshot of `column` at the start of the current lexeme. |\n| `tab_width` | `int` | Configurable number of columns per `\\t` character. |\n\n### 3.4 Memory & Object Layout (Expert Note)\nWhile Python handles memory, it is critical to understand the overhead for large-scale tokenization:\n- **String Interning**: Python may intern short lexemes, but every `Token` instance is a full heap object. A 100k line file may generate 500k+ tokens.\n- **Scanner Overhead**: The scanner itself is O(1) in space (excluding the output list), as it merely holds an reference to the input string and a few integers.\n- **Byte Offsets**: `start` and `current` are indices into Python's UTF-8/UTF-16/UTF-32 internal string representation. \n\n\n![TokenType Enum â€” Complete Variant Map](./diagrams/tdd-diag-1.svg)\n\n*(Visual representation of Scanner pointers: `start` pinned at lexeme beginning, `current` advancing character by character, `source[start:current]` forming the lexeme)*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 Scanner.__init__\n- **Signature**: `def __init__(self, source: str, tab_width: int = 1) -> None`\n- **Preconditions**: `source` must be a valid string (can be empty).\n- **Postconditions**: Scanner initialized at (1,1) with `current = 0`.\n\n### 4.2 Scanner.advance\n- **Signature**: `def advance(self) -> str`\n- **Logic**: Consumes and returns `source[current]`. Increments `current`.\n- **Position Tracking**: \n    - If consumed char is `\\n`: `line += 1`, `column = 1`.\n    - If consumed char is `\\t`: `column += tab_width`.\n    - Else: `column += 1`.\n- **Constraints**: Must not be called if `is_at_end()` is true.\n\n### 4.3 Scanner.peek\n- **Signature**: `def peek(self) -> str`\n- **Logic**: Returns `source[current]` without incrementing the pointer.\n- **Boundary**: Returns `\\0` or `''` if `is_at_end()` is true to avoid index errors.\n\n### 4.4 Scanner.scan_tokens\n- **Signature**: `def scan_tokens(self) -> list[Token]`\n- **Logic**: Main loop calling `_scan_token` until `is_at_end()`.\n- **Termination**: Appends a `TokenType.EOF` token at the final `line` and `column`.\n\n---\n\n## 5. Algorithm Specification: The Foundation Scan Loop\n\nThe foundation scan loop implements a basic Deterministic Finite Automaton (DFA) where each transition is a simple character match or a fallthrough to whitespace/error.\n\n### 5.1 The `_scan_token` Procedure\n1. **Reset Lexeme Boundaries**: Set `self.start = self.current` and `self.start_column = self.column`.\n2. **Fetch Character**: `char = self.advance()`.\n3. **Dispatch Table Check**:\n    - If `char` in `{'+', '-', '*', '/', '(', ')', '{', '}', '[', ']', ';', ','}`:\n        - Identify `TokenType` (OPERATOR or PUNCTUATION).\n        - Call `_make_token(type)`.\n4. **Whitespace Handling**:\n    - If `char` in `{' ', '\\r', '\\t', '\\n'}`:\n        - Return `None` (indicates no token emitted, loop continues).\n5. **Error Recovery**:\n    - If no match:\n        - Emit `Token(TokenType.ERROR, char, line, start_column)`.\n        - The `advance()` call in step 2 already moved the cursor, effectively \"skipping\" the invalid char.\n\n### 5.2 Position Tracking Invariant\nThe scanner maintains a \"Human-Visible Coordinate\" system. \n- **Rule 1**: Newlines are the only characters that reset the column.\n- **Rule 2**: The `start_column` must be captured *before* the first `advance()` of a token to ensure the token position points to its first character.\n\n{{DIAGRAM:tdd-diag-2}}\n*(Sequence diagram showing: scan_tokens -> _scan_token -> advance -> update line/col -> _make_token)*\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| Unrecognized Character | `_scan_token` else-branch | Emit `ERROR` token, continue to next char. | Yes (as ERROR token) |\n| Unexpected EOF | `is_at_end()` check | Stop loop, emit `EOF` sentinel. | No |\n| Tab/Newline Drift | `advance()` logic | Standardize increments in one method. | Indirectly (wrong error positions) |\n\n---\n\n## 7. Implementation Sequence with Checkpoints\n\n### Phase 1: Data Structures (1 Hour)\n1. Define `TokenType` enum with M1-required variants.\n2. Define `Token` dataclass with `__repr__` for debugging.\n3. **Checkpoint**: Instantiate a `Token` and print it. Ensure fields are accessible.\n\n### Phase 2: Primitive Cursor (1 Hour)\n1. Implement `Scanner.__init__` and `is_at_end()`.\n2. Implement `advance()` with line/column tracking logic.\n3. Implement `peek()`.\n4. **Checkpoint**: Create a scanner for `\"A\\nB\"`. Call `advance()` three times. Verify `line` becomes 2 and `column` becomes 2.\n\n### Phase 3: Token Generation (1 Hour)\n1. Implement `_make_token()` helper using string slicing `source[start:current]`.\n2. Implement `_scan_token()` with single-character dispatch.\n3. Implement `scan_tokens()` main loop.\n4. **Checkpoint**: Scan `\"+-\"`. Verify output is `[Token(OPERATOR, \"+\"), Token(OPERATOR, \"-\"), Token(EOF, \"\")]`.\n\n### Phase 4: Whitespace & Errors (0.5 Hour)\n1. Add whitespace cases to `_scan_token()` (return `None`).\n2. Add the `else` branch for `ERROR` tokens.\n3. **Checkpoint**: Scan `\" + @ \"`. Verify list is `[Token(OPERATOR, \"+\"), Token(ERROR, \"@\"), Token(EOF, \"\")]`.\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Single-Character Symbols\n- **Input**: `( ) { } ; , + - * /`\n- **Expected**: A list of 10 tokens + EOF. Lexemes must match inputs exactly. Line/Column must increment.\n\n### 8.2 Edge Case: Multi-line Whitespace\n- **Input**: ` \\n  \\t  \\r `\n- **Expected**: Exactly one token: `EOF`.\n- **Verification**: `tokens[0].line` should be 2. `tokens[0].column` should be accurate based on `tab_width`.\n\n### 8.3 Failure Case: Invalid Characters\n- **Input**: `+ @ -`\n- **Expected**: `[Token(OPERATOR, \"+\"), Token(ERROR, \"@\"), Token(OPERATOR, \"-\"), Token(EOF)]`.\n- **Positioning**: The error token must report column 3 (if space is col 2).\n\n### 8.4 Boundary Case: Empty File\n- **Input**: `\"\"`\n- **Expected**: `[Token(EOF, \"\", 1, 1)]`.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | Measurement |\n| :--- | :--- | :--- |\n| `advance()` | O(1) | No loops or complex logic inside. |\n| `scan_tokens()` | O(N) | Single pass over string of length N. |\n| Memory Overhead | < 2x Source Size | Tokens store references to source slices; metadata is minimal. |\n\n---\n\n## 10. Concurrency & Thread Safety\nThe `Scanner` is **not thread-safe**. It is intended to be used as a short-lived, single-threaded object per source file. If multiple files need to be scanned, instantiate multiple `Scanner` objects.\n\n---\n\n## 11. Dispatch Map Summary\n\nFor M1, the scanner uses a rigid mapping for the first character encountered:\n\n| Character | TokenType | Category |\n| :--- | :--- | :--- |\n| `(` | PUNCTUATION | Grouping |\n| `)` | PUNCTUATION | Grouping |\n| `{` | PUNCTUATION | Grouping |\n| `}` | PUNCTUATION | Grouping |\n| `[` | PUNCTUATION | Grouping |\n| `]` | PUNCTUATION | Grouping |\n| `;` | PUNCTUATION | Delimiter |\n| `,` | PUNCTUATION | Delimiter |\n| `+` | OPERATOR | Arithmetic |\n| `-` | OPERATOR | Arithmetic |\n| `*` | OPERATOR | Arithmetic |\n| `/` | OPERATOR | Arithmetic |\n| ` ` | `None` | Skip |\n| `\\t` | `None` | Skip |\n| `\\r` | `None` | Skip |\n| `\\n` | `None` | Skip |\n\nAny character not in this table triggers a `TokenType.ERROR`.\n\n```python\n# Implementation Reference for the Dispatch Map\n_SINGLE_CHAR_MAP = {\n    '(': TokenType.PUNCTUATION, ')': TokenType.PUNCTUATION,\n    '{': TokenType.PUNCTUATION, '}': TokenType.PUNCTUATION,\n    '[': TokenType.PUNCTUATION, ']': TokenType.PUNCTUATION,\n    ';': TokenType.PUNCTUATION, ',': TokenType.PUNCTUATION,\n    '+': TokenType.OPERATOR,    '-': TokenType.OPERATOR,\n    '*': TokenType.OPERATOR,    '/': TokenType.OPERATOR,\n}\n```\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m2 -->\n# MODULE SPECIFICATION: Multi-Character Tokens & Maximal Munch (tokenizer-m2)\n\n## 1. Module Charter\nThe **Multi-Character Tokens & Maximal Munch** module extends the foundational scanner to handle ambiguous lexical structures. It implements the \"Maximal Munch\" principle, ensuring the scanner always prefers the longest possible valid token (e.g., `==` over `=` followed by `=`). This module introduces lookahead primitives (`_match`, `_peek_next`) and specialized sub-scanners for complex lexemes like numeric literals (integers and floats) and identifiers. It also handles keyword recognition by post-processing scanned identifiers against a reserved hash table. By the end of this module, the scanner can process nearly all C-like expression syntax excluding strings and comments.\n\n**Invariants:**\n- Every multi-character operator must be resolved using at most LA(1) (one character lookahead) via `_match()`.\n- Numeric literals allow LA(2) specifically for disambiguating a trailing decimal point from a valid floating-point fractional part.\n- Keywords are never recognized as partial identifiers; the full identifier must be scanned before keyword categorization occurs.\n- The scanner state (`current`, `line`, `column`) must remain consistent even when lookahead fails to consume a character.\n\n---\n\n## 2. File Structure\nThe implementation follows this creation order, building upon the `tokenizer-m1` foundation:\n\n1.  `token_type.py`: Update existing `TokenType` Enum with new operator and keyword-related variants.\n2.  `scanner.py`: Add `_match`, `_peek_next`, `_scan_number`, and `_scan_identifier` methods. Update `_scan_token` dispatch logic.\n3.  `test_multi_char.py`: New integration tests for multi-character sequences, number literals, and keywords.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 Updated TokenType (Enum)\nWe expand the `TokenType` enumeration from M1. Note the specific distinction between `ASSIGN` (`=`) and `EQUALS` (`==`).\n\n```python\nfrom enum import Enum, auto\n\nclass TokenType(Enum):\n    # Foundational (from M1)\n    NUMBER      = auto()\n    STRING      = auto()\n    IDENTIFIER  = auto()\n    KEYWORD     = auto()\n    OPERATOR    = auto()   # Used for +, -, *, /\n    PUNCTUATION = auto()\n    EOF         = auto()\n    ERROR       = auto()\n\n    # New Multi-character / Ambiguous Operators (M2)\n    ASSIGN        = auto()   # =\n    EQUALS        = auto()   # ==\n    NOT_EQUAL     = auto()   # !=\n    LESS_THAN     = auto()   # <\n    LESS_EQUAL    = auto()   # <=\n    GREATER_THAN  = auto()   # >\n    GREATER_EQUAL = auto()   # >=\n```\n\n### 3.2 Keyword Lookup Table\nTo maintain O(1) average lookup performance and keep the dispatch logic clean, keywords are stored in a static dictionary.\n\n```python\n# Inside Scanner class or as a module-level constant\n_KEYWORDS: dict[str, TokenType] = {\n    \"if\":     TokenType.KEYWORD,\n    \"else\":   TokenType.KEYWORD,\n    \"while\":  TokenType.KEYWORD,\n    \"return\": TokenType.KEYWORD,\n    \"true\":   TokenType.KEYWORD,\n    \"false\":  TokenType.KEYWORD,\n    \"null\":   TokenType.KEYWORD,\n}\n```\n\n### 3.3 Scanner Internal State (Logic View)\nWhile the `Token` struct remains unchanged from M1, the Scanner methods now operate with a \"lookahead budget\":\n\n| Operation | Lookahead Depth | Purpose |\n| :--- | :--- | :--- |\n| `_match(c)` | LA(1) | Check if next char matches `c`; consume if yes. |\n| `peek()` | LA(1) | Inspect next char without consuming. |\n| `_peek_next()` | LA(2) | Inspect char after next (used for float dot check). |\n\n\n![Maximal Munch Principle â€” Greedy Token Selection](./diagrams/tdd-diag-11.svg)\n\n*(Diagram: Decision tree for '>', '>=', and '>=='. Shows how LA(1) resolves the first two and how the main loop restarts to resolve the third as GTE + ASSIGN)*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `_match(expected: str) -> bool`\n- **Goal**: Conditional consumption for maximal munch.\n- **Parameters**: `expected` (single-character string).\n- **Logic**: \n    1. If `is_at_end()`, return `False`.\n    2. If `source[current] != expected`, return `False`.\n    3. Call `advance()` and return `True`.\n- **Note**: This is the atomic \"greedy step\" for operators.\n\n### 4.2 `_peek_next() -> str`\n- **Goal**: Look ahead two characters.\n- **Logic**: \n    1. If `current + 1 >= len(source)`, return `''`.\n    2. Return `source[current + 1]`.\n\n### 4.3 `_scan_number() -> Token`\n- **Precondition**: `advance()` has already consumed the first digit.\n- **Logic**:\n    1. Loop `advance()` while `peek()` is a digit.\n    2. Look ahead: If `peek() == '.'` AND `_peek_next()` is a digit:\n        - `advance()` to consume the dot.\n        - Loop `advance()` while `peek()` is a digit.\n    3. Return `_make_token(TokenType.NUMBER)`.\n\n### 4.4 `_scan_identifier() -> Token`\n- **Precondition**: `advance()` has already consumed a letter or underscore.\n- **Logic**:\n    1. Loop `advance()` while `peek()` is alphanumeric or `_`.\n    2. Extract lexeme `text = source[start:current]`.\n    3. `type = _KEYWORDS.get(text, TokenType.IDENTIFIER)`.\n    4. Return `_make_token(type)`.\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 Maximal Munch Operator Dispatch\nThe `_scan_token` method must be updated to handle multi-character operators *before* falling back to the single-character map.\n\n**Step-by-Step Dispatch:**\n1.  **Read First Char**: `char = advance()`.\n2.  **Case `=`**: Return `EQUALS` if `_match('=')`, else `ASSIGN`.\n3.  **Case `!`**: If `_match('=')`, return `NOT_EQUAL`. Else return `ERROR` (bare `!` is invalid).\n4.  **Case `<`**: Return `LESS_EQUAL` if `_match('=')`, else `LESS_THAN`.\n5.  **Case `>`**: Return `GREATER_EQUAL` if `_match('=')`, else `GREATER_THAN`.\n6.  **Case Digit**: Call `_scan_number()`.\n7.  **Case Alpha/Underscore**: Call `_scan_identifier()`.\n\n### 5.2 Floating Point Disambiguation (The Trailing-Dot Guard)\nThe language grammar defines a float as `DIGIT+ \".\" DIGIT+`. A trailing dot (e.g., `3.`) is invalid as a float.\n1.  Current: `3.`\n2.  Scanner consumes `3`.\n3.  Scanner peeks `.`.\n4.  Scanner calls `_peek_next()`. It is not a digit (it is EOF or whitespace).\n5.  Scanner **stops**. It emits `NUMBER(\"3\")`.\n6.  Main loop starts. Scanner reads `.`. It is not in the dispatch map.\n7.  Scanner emits `ERROR(\".\")`.\n\n\n![_match() Helper â€” Conditional Consume State Machine](./diagrams/tdd-diag-12.svg)\n\n*(State Machine for Numbers: States [START] -> [INT] --('.') and peek_next(DIGIT)--> [FLOAT] --(NON_DIGIT)--> [ACCEPT])*\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| Bare Exclamation | `_scan_token` branch `!` | Emit `ERROR` for `!`, continue. | Yes |\n| Trailing Dot | `_scan_number` lookahead | Emit `NUMBER` for digits, let dot fall to `ERROR`. | Yes (as separate ERROR) |\n| Digit-prefixed Identifier | Dispatch logic | Emits `NUMBER` then `IDENTIFIER`. | No (Standard lexer behavior) |\n| Invalid Start Char | `_scan_token` else | Emit `ERROR`, skip char. | Yes |\n\n---\n\n## 7. Implementation Sequence with Checkpoints\n\n### Phase 1: Enum Expansion (0.25 Hours)\n1. Update `TokenType` in `token_type.py` with 7 new operator types.\n2. **Checkpoint**: Verify `TokenType.GREATER_EQUAL` exists and is distinct.\n\n### Phase 2: Lookahead Helpers (0.5 Hours)\n1. Implement `_match(expected)` in `Scanner`.\n2. Implement `_peek_next()` in `Scanner`.\n3. **Checkpoint**: Test `_match` on a string `\">=\"`. Call `advance()`, then `_match('=')`. Verify it returns `True` and `current` is 2.\n\n### Phase 3: Operator Dispatch (1 Hour)\n1. Implement the `if/elif` branches for `=`, `!`, `<`, `>` in `_scan_token`.\n2. **Checkpoint**: Scan `\">==\"`. Verify tokens are `GREATER_EQUAL` and `ASSIGN`.\n\n### Phase 4: Literals & Identifiers (1 Hour)\n1. Implement `_scan_number` with float lookahead.\n2. Implement `_scan_identifier` with keyword lookup.\n3. **Checkpoint**: Scan `\"if 3.14 x\"`. Verify tokens are `KEYWORD(if)`, `NUMBER(3.14)`, `IDENTIFIER(x)`.\n\n---\n\n## 8. Test Specification\n\n### 8.1 Happy Path: Comparison Operators\n- **Input**: `== != <= >= < > =`\n- **Expected**: `[EQUALS, NOT_EQUAL, LESS_EQUAL, GREATER_EQUAL, LESS_THAN, GREATER_THAN, ASSIGN, EOF]`\n- **Invariants**: Ensure no token lexeme is truncated.\n\n### 8.2 Happy Path: Numbers\n- **Input**: `123 3.1415 0`\n- **Expected**: `[NUMBER(\"123\"), NUMBER(\"3.1415\"), NUMBER(\"0\"), EOF]`\n\n### 8.3 Happy Path: Identifiers & Keywords\n- **Input**: `while iffy while_loop return true`\n- **Expected**: `[KEYWORD(while), IDENTIFIER(iffy), IDENTIFIER(while_loop), KEYWORD(return), KEYWORD(true), EOF]`\n\n### 8.4 Edge Case: Maximal Munch Ambiguity\n- **Input**: `>==`\n- **Expected**: `[GREATER_EQUAL(\">=\"), ASSIGN(\"=\"), EOF]`\n- **Trace**:\n    - `advance()` -> `>`\n    - `_match('=')` -> True. Consume `=`. Lexeme `\">=\"`.\n    - Next loop: `advance()` -> `=`. `_match('=')` -> False. Lexeme `\"=\"`.\n\n### 8.5 Failure Case: Trailing Dot\n- **Input**: `42.`\n- **Expected**: `[NUMBER(\"42\"), ERROR(\".\"), EOF]`\n- **Reasoning**: `_peek_next()` check prevents consumption of the dot.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | Measurement |\n| :--- | :--- | :--- |\n| Keyword Lookup | O(1) | Python dictionary `get()` is constant time average. |\n| Number Scanning | O(K) | Where K is number of digits. Single pass. |\n| Memory usage | Minimal | No temporary string buffers; uses integer offsets into source. |\n\n---\n\n## 10. Trace Table: Identifier vs Keyword\n\n| Step | Action | `current` | `lexeme` | Result |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | `advance()` -> 'i' | 1 | \"i\" | Start Identifier |\n| 2 | `peek()` -> 'f' (isalnum) | 1 | \"i\" | Continue |\n| 3 | `advance()` -> 'f' | 2 | \"if\" | Continue |\n| 4 | `peek()` -> 'f' (isalnum) | 2 | \"if\" | Continue |\n| 5 | `advance()` -> 'f' | 3 | \"iff\" | Continue |\n| 6 | `peek()` -> ' ' (not alnum) | 3 | \"iff\" | **Stop** |\n| 7 | `_KEYWORDS.get(\"iff\")` | 3 | \"iff\" | Not found -> `IDENTIFIER` |\n\n---\n\n## 11. Dispatch Logic (Implementation Snippet)\n\n```python\ndef _scan_token(self) -> Token | None:\n    self.start = self.current\n    self.start_column = self.column\n    char = self.advance()\n\n    # Ambiguous Operators\n    if char == '=':\n        return self._make_token(TokenType.EQUALS if self._match('=') else TokenType.ASSIGN)\n    if char == '!':\n        if self._match('='): return self._make_token(TokenType.NOT_EQUAL)\n        return Token(TokenType.ERROR, char, self.line, self.start_column)\n    if char == '<':\n        return self._make_token(TokenType.LESS_EQUAL if self._match('=') else TokenType.LESS_THAN)\n    if char == '>':\n        return self._make_token(TokenType.GREATER_EQUAL if self._match('=') else TokenType.GREATER_THAN)\n\n    # Multi-character Literals/Names\n    if char.isdigit():\n        return self._scan_number()\n    if char.isalpha() or char == '_':\n        return self._scan_identifier()\n\n    # M1 Dispatch Fallback...\n```\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m3 -->\n# MODULE SPECIFICATION: Strings & Comments (tokenizer-m1)\n\n## 1. Module Charter\nThe **Strings & Comments** module upgrades the scanner from a simple mapping-based dispatcher to a **Context-Aware State Machine**. This module is responsible for identifying and processing regions of source code where the standard lexical rules are suspended: string literals and comments. \n\n**Core Responsibilities:**\n- Implement a three-way dispatch for the `/` character to distinguish between division, single-line comments (`//`), and multi-line block comments (`/* */`).\n- Implement `_scan_string()` to recognize double-quoted literals, supporting internal escape sequences (e.g., `\\\"`, `\\n`) without terminating the string.\n- Enforce **Mode Isolation**: ensuring that token-like sequences inside strings (like `//`) or comments (like `\"...\"`) do not trigger the scanner's standard dispatch logic.\n- Maintain accurate line and column metadata through multi-line constructs by routing all internal consumption through the `advance()` primitive.\n- Implement precise error reporting for unterminated constructs, pinning the error location to the opening delimiter (`\"` or `/*`) rather than the point of failure (EOF).\n\n**Out of Scope:**\n- Processing escape sequences into their character values (e.g., converting `\\n` to a byte 10). This is deferred to the Evaluator/AST phase.\n- Supporting nested block comments (Standard C-style behavior: non-nesting).\n\n**Invariants:**\n- Comments produce NO tokens; they return `None` to the main loop and are discarded.\n- String lexemes include the surrounding double quotes.\n- The scanner must return to the `NORMAL` state (standard dispatch) immediately after a closing delimiter.\n\n---\n\n## 2. File Structure\nImplementation proceeds by modifying existing foundation files and adding specific test suites:\n\n1.  `token_type.py`: No new types needed (uses existing `STRING`, `OPERATOR`, `ERROR`).\n2.  `scanner.py`: \n    - Modification: Remove `/` from `_SINGLE_CHAR_TOKENS`.\n    - Addition: `_scan_string()`, `_skip_line_comment()`, `_skip_block_comment()`.\n    - Modification: Update `_scan_token()` dispatch logic.\n3.  `test_strings_comments.py`: Comprehensive test suite for multi-line context and escape sequences.\n\n---\n\n## 3. Complete Data Model\n\n### 3.1 Scanner Mode Transitions\nThe scanner transitions from the `NORMAL` dispatch state into specialized loops. While these are not explicit variables, the instruction pointer residing in a specific sub-method constitutes the \"Mode\".\n\n| Mode | Trigger (in NORMAL) | Exit Condition | Output |\n| :--- | :--- | :--- | :--- |\n| **NORMAL** | â€” | EOF | Various Tokens |\n| **STRING** | `\"` | `\"` or `\\n` or EOF | `STRING` or `ERROR` |\n| **LINE_COMMENT** | `//` | `\\n` or EOF | `None` |\n| **BLOCK_COMMENT**| `/*` | `*/` or EOF | `None` or `ERROR` |\n\n### 3.2 Token Representation for M3\nStrings are stored as raw lexemes.\n\n```python\n# Example Lexeme Storage\n# Source: \"Line1\\nLine2\"\n# Token.lexeme -> '\"Line1\\\\nLine2\"' (Length 14)\n# Token.type   -> TokenType.STRING\n```\n\n\n![Scanner Mode State Machine â€” NORMAL / IN_STRING / IN_LINE_COMMENT / IN_BLOCK_COMMENT](./diagrams/tdd-diag-20.svg)\n\n*(State Machine Diagram: Shows NORMAL state branching into STRING, LINE_COMMENT, and BLOCK_COMMENT. Highlights that while in STRING or COMMENT modes, the standard dispatch transitions are disabled.)*\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `_skip_line_comment(self) -> None`\n- **Logic**: Consumes characters using `self.advance()` as long as `peek() != '\\n'` and `not is_at_end()`.\n- **Note**: Does NOT consume the `\\n`. The `\\n` is left for the `NORMAL` loop to handle as whitespace, ensuring line-counting consistency.\n\n### 4.2 `_skip_block_comment(self) -> Token | None`\n- **Logic**: Hunts for the `*/` sequence.\n- **Success**: Returns `None`.\n- **Failure**: If EOF is reached before `*/`, returns `Token(TokenType.ERROR, \"/*\", self.line, self.start_column)`.\n- **Invariant**: Must handle internal `\\n` via `advance()` to prevent position drift.\n\n### 4.3 `_scan_string(self) -> Token`\n- **Logic**:\n    1. Loop while `peek() != '\"'` and `not is_at_end()`.\n    2. If `peek() == '\\n'`: Terminate and return `ERROR` (Language Spec: No multi-line strings without escapes).\n    3. If `peek() == '\\\\'` (Backslash):\n        - `advance()` (consumes `\\`).\n        - If `not is_at_end()`, `advance()` (consumes the escaped character, e.g., `\"` or `n`).\n    4. If loop exits due to `\"`, `advance()` the closing quote and return `STRING`.\n    5. Else, return `ERROR`.\n\n---\n\n## 5. Algorithm Specification\n\n### 5.1 The Three-Way `/` Dispatch\nThe character `/` is the most ambiguous start-character in the C-family grammar. The implementation must follow this priority:\n\n1.  **Consume** `/`.\n2.  **Match** `/`: If successful, enter `_skip_line_comment()`.\n3.  **Match** `*`: If successful, enter `_skip_block_comment()`.\n4.  **Else**: Return `TokenType.OPERATOR` (Division).\n\n\n![The '/' Ambiguity â€” Three-Way Dispatch](./diagrams/tdd-diag-21.svg)\n\n*(Decision Tree: Root '/' -> Path A: '/' (Line Comm) -> Path B: '*' (Block Comm) -> Path C: Default (DIV Operator))*\n\n### 5.2 Escape Sequence Jump-Over\nTo correctly handle `\"` inside a string (e.g., `\"He said \\\"Hi\\\"\"`), the algorithm uses a \"Jump-Over\" strategy.\n\n**Step-by-Step Procedure:**\n1.  Enter `_scan_string()`.\n2.  `peek()` sees `\\`.\n3.  Call `advance()`: Cursor moves past `\\`.\n4.  Check `is_at_end()`: If true, the loop will terminate and report an unterminated string.\n5.  Call `advance()`: Cursor moves past the next character (e.g., `\"`).\n6.  **Crucial Result**: The closing quote check `peek() != '\"'` in the next loop iteration is bypassed because the cursor is now *past* the escaped quote.\n\n### 5.3 Error Position Pinning\nStandard error reporting often points to where the error was *discovered* (e.g., EOF). For UX, we must point to where the construct *started*.\n\n**Algorithm:**\n1.  In `_scan_token()`, snapshot `self.start_column = self.column` and `self.start_line = self.line`.\n2.  Enter sub-scanner (e.g., `_scan_string`).\n3.  On error, instantiate `Token` using the snapshotted `self.start_line` and `self.start_column`.\n\n\n![_scan_string() State Machine â€” Escape and Termination Logic](./diagrams/tdd-diag-22.svg)\n\n*(Memory/Pointer Snapshot: Shows `start_column` pointing to the opening `\"` at index 10, while `current` is at index 500 (EOF). The resulting ERROR token uses index 10 coordinates.)*\n\n---\n\n## 6. Error Handling Matrix\n\n| Error | Detected By | Recovery | User-Visible? |\n| :--- | :--- | :--- | :--- |\n| Unterminated String (EOF) | `_scan_string` while-loop | Emit `ERROR` at start position, stop. | Yes |\n| Unterminated String (Newline) | `_scan_string` `peek() == '\\n'` | Emit `ERROR` at start position, resume NORMAL. | Yes |\n| Backslash at EOF | `_scan_string` escape check | Emit `ERROR`, stop. | Yes |\n| Unterminated Block Comment | `_skip_block_comment` loop | Emit `ERROR` at `/*` position, stop. | Yes |\n| Nested Block Comment | N/A (By Design) | First `*/` closes; remainder is scanned as code. | Yes (likely Parse Error) |\n\n---\n\n## 7. Implementation Sequence with Checkpoints\n\n### Phase 1: Dispatch Overhaul (0.5 Hours)\n1. Remove `'/'` from the `_SINGLE_CHAR_MAP`.\n2. Update `_scan_token` to handle `/` with `_match` logic.\n3. **Checkpoint**: Scanning `1 / 2` should still produce `NUMBER, OPERATOR, NUMBER`. Scanning `1 // 2` should produce `NUMBER, EOF`.\n\n### Phase 2: Comment Logic (1 Hour)\n1. Implement `_skip_line_comment`.\n2. Implement `_skip_block_comment` with `*/` detection.\n3. Ensure block comments update `self.line` via `advance()`.\n4. **Checkpoint**: Scan `/* \\n */ +`. Verify `+` is on `line: 2`.\n\n### Phase 3: String Scanner (1.5 Hours)\n1. Implement `_scan_string` with simple quote matching.\n2. Add escape sequence handling (backslash + next char consumption).\n3. Add `\\n` error check.\n4. **Checkpoint**: Scan `\"a\\\"b\"`. Verify lexeme is `\"a\\\"b\"` (length 5).\n\n### Phase 4: Integration & Multi-line (1 Hour)\n1. Verify interaction: Strings containing `//`, Comments containing `\"`.\n2. Run position-drift tests on large multi-line comment blocks.\n3. **Checkpoint**: Scan code where a string follows a block comment. Verify column reset.\n\n---\n\n## 8. Test Specification\n\n### 8.1 String Escape Isolation\n- **Input**: `\"line1\\nline2\\\"quote\\\"\"`\n- **Expected**: `[TokenType.STRING]`\n- **Lexeme**: Exact match of input including quotes.\n- **Isolation**: Ensure `\\n` inside the string is not caught by the `_scan_token` newline handler.\n\n### 8.2 Block Comment Boundaries\n- **Input**: `/* *** */`\n- **Expected**: `[]` (No tokens).\n- **Internal**: Test `/* / */` (should close) vs `/* * / */` (should close).\n\n### 8.3 The \"False Comment\" Test\n- **Input**: `\"http://example.com\"`\n- **Expected**: `[TokenType.STRING]`\n- **Verification**: If it produces `STRING(\"http:\")` followed by a comment, Mode Isolation has failed.\n\n### 8.4 Unterminated Errors\n- **Input**: `/* unterminated ...`\n- **Expected**: `[TokenType.ERROR]`\n- **Requirement**: `token.line` and `token.column` must point to the start of `/*`.\n\n---\n\n## 9. Performance Targets\n\n| Operation | Target | Measurement |\n| :--- | :--- | :--- |\n| String Scanning | O(N) | Single pass, no nested loops, no string building. |\n| Comment Skipping | O(N) | Single pass, standard `advance()` overhead. |\n| Memory | Zero Allocation | Uses `source` slicing only at the very end of `_scan_string`. |\n\n---\n\n## 10. Concurrency Specification\n- **Thread Safety**: The Scanner remains **not thread-safe**.\n- **Context Handling**: Method-local loops for strings/comments ensure that \"Mode\" is handled via the call stack. This prevents complex state-flag management and minimizes bug surface area.\n\n---\n\n## 11. Mode Isolation Logic (Implementation Snippet)\n\n```python\ndef _scan_string(self) -> Token:\n    # We are now in STRING mode. Standard dispatch rules DO NOT APPLY.\n    while self.peek() != '\"' and not self.is_at_end():\n        if self.peek() == '\\n':\n            return self._err_at_start(\"Unterminated string (newline)\")\n            \n        if self.peek() == '\\\\':\n            self.advance() # Skip backslash\n            self.advance() # Skip escaped char (even if it is '\"')\n        else:\n            self.advance()\n\n    if self.is_at_end():\n        return self._err_at_start(\"Unterminated string (EOF)\")\n\n    self.advance() # The closing quote\n    return self._make_token(TokenType.STRING)\n```\n\n---\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: tokenizer-m4 -->\n# MODULE SPECIFICATION: Integration Testing & Error Recovery (tokenizer-m4)\n\n## 1. Module Charter\nThe **Integration Testing & Error Recovery** module is the final validation phase of the tokenizer. Its primary purpose is to ensure that the individual scanning logic components from Milestones 1-3 function correctly in aggregate when processing complex, multi-line source files. This module does not introduce new lexical features; instead, it formalizes the \"Skip-One\" error recovery mechanism and subjects the scanner to rigorous boundary testing and performance benchmarking. It verifies that position tracking (line/column) remains accurate without drift over long inputs and that the scanner maintains $O(N)$ time complexity, specifically meeting the target of 10,000 lines per second. By the end of this module, the tokenizer is considered \"Production-Ready\" for consumption by a downstream parser.\n\n**Core Responsibilities:**\n- Implement \"Golden Path\" integration tests for complete C-like programs.\n- Validate the implicit \"Skip-One\" error recovery strategy to ensure no valid tokens are swallowed after an error.\n- Verify multi-error collection in a single pass (errors as values in the token stream).\n- Detect and prevent \"Position Drift\" in line/column tracking across multi-line constructs (strings/comments).\n- Benchmark throughput to ensure no $O(N^2)$ algorithmic bottlenecks exist in the scanner's inner loops.\n\n**Invariants:**\n- Every input, regardless of validity, must result in a `list[Token]` ending in `TokenType.EOF`.\n- Error recovery must never consume more than one character per `ERROR` token emitted.\n- The `column` counter must reset to `1` exactly after a `\\n` character is consumed, regardless of context (Normal, String, or Comment).\n\n---\n\n## 2. File Structure\nThe integration and testing suite shall be organized as follows:\n\n1.  `tests/integration_harness.py`: Helper utilities for asserting token stream equality.\n2.  `tests/test_integration.py`: \"Golden Path\" and multi-error tests.\n3.  `tests/test_position.py`: Specific tests for line/column drift and CRLF handling.\n4.  `tests/test_boundaries.py`: Edge cases for empty files, max-length identifiers, etc.\n5.  `benchmarks/perf_benchmark.py`: Throughput tests for 10k lines and 10k strings.\n\n---\n\n## 3. Complete Data Model\n\nWhile the core `Token` and `Scanner` classes were defined in M1-M3, M4 introduces a specific \"Expectation Format\" used for high-density testing.\n\n### 3.1 Test Expectation Schema\nTo avoid verbose object instantiation in tests, we use a tuple-based comparison format.\n\n```python\nfrom typing import Tuple, List, Union\nfrom token_type import TokenType\n\n# An Expectation is a tuple of (Type, Lexeme, [Optional: Line, Optional: Column])\nTokenExpectation = Union[\n    Tuple[TokenType, str],           # Basic check\n    Tuple[TokenType, str, int, int]  # Full positional check\n]\n```\n\n### 3.2 Position Metadata Mapping\nThe scanner must maintain absolute synchronization between the byte-offset `current` and the human-readable `(line, column)`.\n\n| Property | Mapped From | Constraint |\n| :--- | :--- | :--- |\n| `line` | Number of `\\n` seen + 1 | Must increment inside `_scan_string` and `_skip_block_comment`. |\n| `column` | Chars since last `\\n` | Must reset to 1 immediately after `\\n`. |\n| `start_column` | Snapshot of `column` | Must be taken BEFORE the first `advance()` of a lexeme. |\n\n\n![Skip-One Error Recovery â€” Mechanism and Invariant](./diagrams/tdd-diag-30.svg)\n\n\n![Skip-One Error Recovery â€” Mechanism and Invariant](./diagrams/tdd-diag-30.svg)\n\n\n---\n\n## 4. Interface Contracts\n\n### 4.1 `integration_harness.assert_tokens(actual: List[Token], expected: List[TokenExpectation])`\n- **Goal**: Provide deep-diffing for token streams.\n- **Logic**: \n    - Iterates through both lists. \n    - Compares `TokenType` and `lexeme`. \n    - If `expected` tuple has 4 elements, also compares `line` and `column`.\n- **Error Reporting**: Must raise `AssertionError` with a detailed message showing the index of the first mismatching token and the surrounding context.\n\n### 4.2 `perf_benchmark.generate_synthetic_program(line_count: int) -> str`\n- **Goal**: Generate a deterministic, large-scale test file.\n- **Logic**: Repeats a set of 10 patterns (if-statements, assignments, comments) to reach the target line count.\n\n### 4.3 `Scanner.scan_tokens` (Re-verification)\n- **Error Recovery Path**: When `_scan_token` enters the `else` branch (unrecognized character):\n    - It captures the character into an `ERROR` token.\n    - It returns that token.\n    - The main loop appends it and calls `_scan_token` again.\n- **Recovery Invariant**: Ensure no recursive calls or \"skip-to-newline\" behavior exists unless explicitly added.\n\n---\n\n## 5. Algorithm Specification: Error Recovery & Benchmarking\n\n### 5.1 Skip-One Recovery Trace\nThe \"Skip-One\" strategy is the most robust for lexical analysis because lexical errors are usually local (a stray character).\n\n**Procedure for Input `@if`**:\n1. `_scan_token()` starts. `start=0`, `start_column=1`.\n2. `advance()` returns `@`. `current=1`, `column=2`.\n3. Dispatch hits `else` branch.\n4. Returns `Token(ERROR, \"@\", line=1, col=1)`.\n5. `scan_tokens()` appends error.\n6. `_scan_token()` starts again. `start=1`, `start_column=2`.\n7. `advance()` returns `i`. `current=2`, `column=3`.\n8. `_scan_identifier()` consumes `f`.\n9. Returns `Token(KEYWORD, \"if\", line=1, col=2)`.\n\n\n![Multi-Error Collection â€” '@#$%' All Reported](./diagrams/tdd-diag-31.svg)\n\n\n![Multi-Error Collection â€” '@#$%' All Reported](./diagrams/tdd-diag-31.svg)\n\n\n### 5.2 Performance: Avoiding Quadratic Slicing\nIn Python, string slicing `source[start:current]` is $O(K)$ where $K$ is the length of the slice. If a user tokenizes a 1MB file containing a single string literal, a naive implementation that slices *inside* the loop would become $O(N^2)$.\n\n**Correct Approach (M4 Requirement)**:\n- All sub-scanners (`_scan_number`, `_scan_string`, `_scan_identifier`) must only slice the source string **once** at the end of their loop.\n- **Benchmark Test**: A 10,000 character string literal must be scanned in near-constant time relative to its length (linear).\n\n---\n\n## 6. Error Handling Matrix (Integration Focus)\n\n| Failure Mode | Detection | Recovery | User Impact |\n| :--- | :--- | :--- | :--- |\n| **Position Drift** | `test_position_fifty_line_program` | Fix `advance()` to handle `\\n` globally. | Wrong error locations in large files. |\n| **Over-Consumption** | `test_error_does_not_consume_next_char` | Ensure `ERROR` branch doesn't call `advance()` twice. | Missing valid code after an error. |\n| **Infinite Loop** | `test_empty_input` / `is_at_end` | Ensure `advance()` always moves `current`. | Compiler hangs. |\n| **Memory Exhaustion**| `test_max_length_identifier` | Rely on Python heap (approx 100MB for 1M tokens). | Crash on massive files. |\n\n---\n\n## 7. Implementation Sequence with Checkpoints\n\n### Phase 1: Canonical Integration (1 Hour)\n1. Implement the `assert_tokens` harness.\n2. Create `COMPLETE_TEST_PROGRAM` containing every feature from M1-M3.\n3. Run `scan_tokens()` and compare against the master expectation list.\n4. **Checkpoint**: `test_canonical_expression` passes with 12 specific tokens.\n\n### Phase 2: Error Recovery Validation (1 Hour)\n1. Write tests for `@#$%` (consecutive errors).\n2. Write tests for `x @ y` (interleaved errors).\n3. Write `test_error_does_not_consume_next_valid_char` for `@if`.\n4. **Checkpoint**: Scanner correctly identifies `if` as a keyword even when preceded by a junk character.\n\n### Phase 3: Position Tracking & CRLF (1 Hour)\n1. Implement `test_windows_line_endings_not_double_counted`.\n2. Write a 100-line test where every line is a single identifier `x1` ... `x100`. Verify `token.line == i`.\n3. **Checkpoint**: Zero drift detected over 100 lines.\n\n### Phase 4: Edge Cases (1 Hour)\n1. `test_empty_input`: Verify only `EOF` at (1,1).\n2. `test_maximum_length_identifier`: 10k character name.\n3. `test_keyword_prefix_identifiers`: `iffy`, `returning`.\n4. **Checkpoint**: All boundary tests pass.\n\n### Phase 5: Performance Benchmarking (1 Hour)\n1. Implement `perf_benchmark.py`.\n2. Generate 10k line file.\n3. Measure `time.perf_counter()` for `scan_tokens()`.\n4. **Checkpoint**: Execution time < 1.0s. If slower, profile for $O(N^2)$ slicing.\n\n---\n\n## 8. Test Specification (High Precision)\n\n### 8.1 Happy Path: The \"Golden Program\"\n```python\nsource = \"\"\"\nif (count >= 10) {\n    print(\"Success\"); // comment\n}\n\"\"\"\n# Expected (Excerpt):\n# [\n#   (TokenType.KEYWORD, \"if\", 2, 1),\n#   (TokenType.PUNCTUATION, \"(\", 2, 4),\n#   (TokenType.IDENTIFIER, \"count\", 2, 5),\n#   ...\n# ]\n```\n\n### 8.2 Edge Case: Max-Length Identifier\n- **Input**: `\"a\" * 1_000_000`\n- **Target**: Ensure Python handles the 1MB string and slice without `RecursionError` or timeout.\n- **Verification**: `tokens[0].lexeme == source`.\n\n### 8.3 Failure Case: Multi-line Comment Unterminated at EOF\n- **Input**: `/* line 1\\n line 2`\n- **Expectation**: `Token(ERROR, \"/*\", 1, 1)` followed by `EOF`.\n- **Note**: The error must be reported on line 1, not line 2.\n\n### 8.4 Boundary Case: Whitespace-only file\n- **Input**: `\"   \\n   \\t  \"`\n- **Expectation**: `[Token(EOF, \"\", 2, 6)]`.\n\n---\n\n## 9. Performance Targets\n\n| Metric | Target | measurement_method |\n| :--- | :--- | :--- |\n| **Throughput** | > 10,000 lines / sec | `perf_counter()` over `generate_synthetic_program(10000)` |\n| **Tokenization Speed**| < 100ns / character | Total time / source length (CPython 3.10) |\n| **Recovery Overhead** | < 1.5x Normal | Compare time for 10k valid lines vs 10k error lines. |\n\n---\n\n## 10. Concurrency Specification\n\nWhile the `Scanner` is single-threaded, the testing harness should be compatible with `pytest-xdist` for parallel execution.\n- **Statelessness**: Every test case must instantiate its own `Scanner`. \n- **No Shared Buffers**: Do not use global variables for the keyword table or single-character map if they are mutable.\n\n---\n\n## 11. Implementation Detail: Position Drift Test\n\nThis logic is mandatory for Phase 3:\n\n```python\ndef test_position_drift_validation():\n    # Construct source: 100 lines of \"x;\"\n    source = \"x;\\n\" * 100\n    scanner = Scanner(source)\n    tokens = scanner.scan_tokens()\n    \n    # We expect 2 tokens per line (ID, PUNCT) + EOF\n    # Total tokens = 201\n    assert len(tokens) == 201\n    \n    for line_idx in range(100):\n        t1 = tokens[line_idx * 2]     # 'x'\n        t2 = tokens[line_idx * 2 + 1] # ';'\n        \n        expected_line = line_idx + 1\n        assert t1.line == expected_line\n        assert t1.column == 1\n        assert t2.line == expected_line\n        assert t2.column == 2\n```\n\n\n![Golden Test Pattern â€” Token Stream Verification Structure](./diagrams/tdd-diag-32.svg)\n\n\n![Golden Test Pattern â€” Token Stream Verification Structure](./diagrams/tdd-diag-32.svg)\n\n\n---\n\n## 12. Skip-One Recovery vs. Multi-Character Logic\n\nOne critical integration risk is that a multi-character scanner (like `_scan_number`) might fail halfway and leave the `current` pointer in an invalid state.\n\n**Validation Criteria**:\n- If `3.a` is scanned:\n    - `_scan_number` sees `3`.\n    - `peek()` sees `.`. `_peek_next()` sees `a`. \n    - `_scan_number` stops. Emits `NUMBER(\"3\")`.\n    - `_scan_token` restarts. Sees `.`. Emits `ERROR(\".\")`.\n    - `_scan_token` restarts. Sees `a`. Emits `IDENTIFIER(\"a\")`.\n- **Test Case**: `test_recovery_from_malformed_float`.\n\n---\n\n## 13. Comprehensive Canonical Test Program\n\nThe following source must be used for Phase 1 integration verification:\n\n```python\n# canonical_source.txt\n/* \n  Multi-line block\n*/\nif (x >= 42.0) {\n    // Single line\n    return \"Result: \" + x;\n} else {\n    return null;\n}\n```\n\n**Required Token Sequence (Condensed)**:\n1. `KEYWORD(\"if\")` @ 5:1\n2. `PUNCTUATION(\"(\")` @ 5:4\n3. `IDENTIFIER(\"x\")` @ 5:5\n4. `GREATER_EQUAL(\">=\")` @ 5:7\n5. `NUMBER(\"42.0\")` @ 5:10\n6. `PUNCTUATION(\")\")` @ 5:14\n7. `PUNCTUATION(\"{\")` @ 5:16\n8. `KEYWORD(\"return\")` @ 7:5\n9. `STRING(\"\\\"Result: \\\"\")` @ 7:12\n10. `OPERATOR(\"+\")` @ 7:23\n11. `IDENTIFIER(\"x\")` @ 7:25\n12. `PUNCTUATION(\";\")` @ 7:26\n13. `PUNCTUATION(\"}\")` @ 8:1\n14. `KEYWORD(\"else\")` @ 8:3\n15. `PUNCTUATION(\"{\")` @ 8:8\n16. `KEYWORD(\"return\")` @ 9:5\n17. `KEYWORD(\"null\")` @ 9:12\n18. `PUNCTUATION(\";\")` @ 9:16\n19. `PUNCTUATION(\"}\")` @ 10:1\n20. `EOF(\"\")` @ 11:1\n\n---\n\n## 14. Performance Hotspot Check: The `__repr__` Trap\n\nIn Python, generating string representations for thousands of tokens can be slower than the scanning itself.\n- **Guidance**: Tests should compare `token.type` and `token.lexeme` directly rather than comparing `repr(token_list)`.\n- **Target**: Benchmarks must measure only `scan_tokens()`, excluding the time spent printing or verifying results.\n<!-- END_TDD_MOD -->\n\n\n# Project Structure: Tokenizer / Lexer\n\n## Directory Tree\n\n```markdown\ntokenizer-lexer/\nâ”œâ”€â”€ src/                     # Core Implementation Logic\nâ”‚   â”œâ”€â”€ __init__.py          # Package initialization\nâ”‚   â”œâ”€â”€ token_type.py        # TokenType Enum (M1: Foundations, M2: Operators/Keywords)\nâ”‚   â”œâ”€â”€ token_class.py       # Token dataclass (M1: Lexeme/Line/Col storage)\nâ”‚   â””â”€â”€ scanner.py           # Core FSM logic (M1: Primitives, M2: Lookahead, M3: Modes)\nâ”œâ”€â”€ tests/                   # Test Suites\nâ”‚   â”œâ”€â”€ __init__.py          # Test package initialization\nâ”‚   â”œâ”€â”€ test_foundation.py   # M1: Single-char & whitespace tests\nâ”‚   â”œâ”€â”€ test_multi_char.py   # M2: Operators, numbers, and keywords\nâ”‚   â”œâ”€â”€ test_strings.py      # M3: Escape sequences & quote logic\nâ”‚   â”œâ”€â”€ test_comments.py     # M3: Single/Multi-line comment isolation\nâ”‚   â”œâ”€â”€ integration_harness.py # M4: Deep-diffing utilities for token streams\nâ”‚   â”œâ”€â”€ test_integration.py  # M4: \"Golden Path\" full program tests\nâ”‚   â”œâ”€â”€ test_position.py     # M4: Line/Column drift & CRLF validation\nâ”‚   â””â”€â”€ test_boundaries.py   # M4: Edge cases (empty files, max-length IDs)\nâ”œâ”€â”€ benchmarks/              # Performance Validation\nâ”‚   â””â”€â”€ perf_benchmark.py    # M4: Throughput tests (10k lines/sec target)\nâ”œâ”€â”€ diagrams/                # Technical Reference (SVG/Mermaid)\nâ”‚   â”œâ”€â”€ fsm_states.svg       # State machine transitions\nâ”‚   â””â”€â”€ scanner_modes.svg    # Mode isolation (Normal/String/Comment)\nâ”œâ”€â”€ Makefile                 # Build automation (test execution, linting)\nâ”œâ”€â”€ .gitignore               # Python-specific ignore rules (__pycache__, etc.)\nâ”œâ”€â”€ requirements.txt         # Minimal dependencies (pytest, mypy)\nâ””â”€â”€ README.md                # Project overview and usage instructions\n```\n\n## Creation Order\n\n1.  **Project Foundation** (30 min)\n    *   Create directory structure: `mkdir -p src tests benchmarks diagrams`\n    *   Initialize `requirements.txt` and `.gitignore`.\n    *   Set up `Makefile` with a `test` target.\n\n2.  **Module 1: Basic Scanning** (1-2 hours)\n    *   `src/token_type.py`: Define M1 variants.\n    *   `src/token_class.py`: Implement `Token` dataclass.\n    *   `src/scanner.py`: Implement `advance()`, `peek()`, and single-char dispatch.\n    *   `tests/test_foundation.py`: Verify whitespace and punctuation.\n\n3.  **Module 2: Greedy Matching** (1-2 hours)\n    *   `src/token_type.py`: Add M2 operator and keyword variants.\n    *   `src/scanner.py`: Add `_match()`, `_peek_next()`, `_scan_number()`, and `_scan_identifier()`.\n    *   `tests/test_multi_char.py`: Verify \"Maximal Munch\" and keyword lookup.\n\n4.  **Module 3: Context-Aware Modes** (2 hours)\n    *   `src/scanner.py`: Implement `_scan_string()`, `_skip_line_comment()`, and `_skip_block_comment()`.\n    *   `tests/test_strings.py` & `tests/test_comments.py`: Verify mode isolation (e.g., comments inside strings).\n\n5.  **Module 4: Hardening & Benchmarks** (1-2 hours)\n    *   `tests/integration_harness.py`: Build the `assert_tokens` helper.\n    *   `tests/test_integration.py`: Run the \"Golden Program\" trace.\n    *   `tests/test_position.py`: Check for line/column drift over large files.\n    *   `benchmarks/perf_benchmark.py`: Run 10k line performance test.\n\n## File Count Summary\n- **Total Logic Files**: 3 (`src/`)\n- **Total Test Files**: 8 (`tests/` + `benchmarks/`)\n- **Total Configuration Files**: 4 (`Makefile`, `README`, etc.)\n- **Estimated Lines of Code**: ~400â€“600 lines (Logic) / ~800 lines (Tests)\n- **Target Language**: Python 3.10+ (requires `dataclasses` and `enum`)\n\n# ðŸ“š Beyond the Atlas: Further Reading\n\n### ðŸ§  Foundational Theory: Automata & Formal Languages\n\n**Paper**: [A Technique for General Writing of Code Generators](https://dl.acm.org/doi/10.1145/363347.363387) (Brooker & Morris, 1962).\n- **Code**: [The RE2 Lexer Logic](https://github.com/google/re2/blob/main/re2/parse.cc) (specifically the `Parse` function).\n- **Best Explanation**: *Introduction to the Theory of Computation* by Michael Sipser, **Chapter 1: Regular Languages**.\n- **Why**: This is the rigorous mathematical proof that finite automata can recognize exactly the class of languages definable by regular expressions.\n- **Pedagogical Timing**: Read **BEFORE Milestone 1** to understand the mathematical constraints of what you are building.\n\n**Paper**: [Regular Expression Search Algorithm](https://dl.acm.org/doi/10.1145/363347.363387) (Ken Thompson, 1968).\n- **Best Explanation**: [Regular Expression Matching Can Be Simple and Fast](https://swtch.com/~rsc/regexp/regexp1.html) by Russ Cox.\n- **Why**: It explains how to build an NFA from a regex, which is the automated version of the hand-written state machine you are building.\n- **Pedagogical Timing**: Read **AFTER Milestone 2** to see how the \"Maximal Munch\" logic you wrote manually can be generated automatically from patterns.\n\n---\n\n### ðŸ› ï¸ The Craft of Lexing: Implementation Guides\n\n**Best Explanation**: *Crafting Interpreters* by Robert Nystrom, **Chapter 4: Scanning**.\n- **Code**: [Lox Scanner.java](https://github.com/munificent/craftinginterpreters/blob/master/java/com/craftinginterpreters/lox/Scanner.java).\n- **Why**: The most modern, approachable, and pedagogically sound walkthrough of building a hand-written scanner for a C-like language.\n- **Pedagogical Timing**: Read **DURING Milestone 1**; it serves as a parallel guide to the logic implemented in this Atlas.\n\n**Spec**: [The Lexical Grammar of C11](https://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf) (ISO/IEC 9899:201x, **Section 6.4**).\n- **Code**: [Clangâ€™s Lexer.cpp](https://github.com/llvm/llvm-project/blob/main/clang/lib/Lex/Lexer.cpp).\n- **Why**: It defines the \"gold standard\" rules for the language your project is mimicking, including tricky edge cases for numbers and strings.\n- **Pedagogical Timing**: Read **DURING Milestone 2** to see how a professional language defines the boundaries of integers and floating-point literals.\n\n---\n\n### ðŸ—ï¸ Real-World Tokenizers: Case Studies\n\n**Code**: [CPythonâ€™s Lib/tokenize.py](https://github.com/python/cpython/blob/main/Lib/tokenize.py).\n- **Best Explanation**: [The Python Tokenizer Documentation](https://docs.python.org/3/library/tokenize.html).\n- **Why**: Shows how a production-grade language uses a hybrid approach (regex + manual logic) to handle indentation-sensitive lexing.\n- **Pedagogical Timing**: Read **AFTER Milestone 2** to compare your identifier and keyword lookup logic with Python's implementation.\n\n**Code**: [V8â€™s scanner.cc](https://github.com/v8/v8/blob/master/src/parsing/scanner.cc).\n- **Why**: V8's scanner is optimized for extreme performance, showing how to handle Unicode/UTF-8 characters at scale.\n- **Pedagogical Timing**: Read **AFTER Milestone 4** once you have met your performance benchmarks to see how \"The Big Boys\" do it.\n\n---\n\n### âš ï¸ Error Handling & Recovery\n\n**Paper**: [Panic Mode Recovery in Lexical Analysis](https://dl.acm.org/doi/10.1145/359545.359565) (James, 1972).\n- **Best Explanation**: *Compilers: Principles, Techniques, and Tools* (The Dragon Book), **Section 3.1.4: Lexical Errors**.\n- **Why**: It introduces the \"Panic Mode\" philosophy used in your project, where the scanner recovers by skipping to the next \"safe\" character.\n- **Pedagogical Timing**: Read **BEFORE Milestone 4** to understand why emitting an `ERROR` token is superior to throwing an exception.\n\n---\n\n### ðŸ§µ Strings, Escapes, and Character Sets\n\n**Spec**: [The Unicode Standard, Version 15.0](https://www.unicode.org/versions/Unicode15.0.0/), **Chapter 3: Conformance (UTF-8)**.\n- **Best Explanation**: [The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/) by Joel Spolsky.\n- **Why**: Explains why \"character-at-a-time\" scanning is significantly more complex once you leave the ASCII range.\n- **Pedagogical Timing**: Read **BEFORE Milestone 3** to prepare for the complexities of scanning string content.\n\n**Spec**: [RFC 8259: The JavaScript Object Notation (JSON) Data Interchange Format](https://datatracker.ietf.org/doc/html/rfc8259#section-7).\n- **Why**: Defines the most widely used standard for string escape sequences (`\\n`, `\\uXXXX`), which mirrors the logic in M3.\n- **Pedagogical Timing**: Read **DURING Milestone 3** to validate your escape sequence logic against the JSON standard."}