{"html":"<h1 id=\"transformer-from-scratch\">Transformer from Scratch</h1>\n<p>Build the complete Transformer architecture—the foundation of GPT, BERT, T5, and virtually every modern language model—from first principles. This project takes you through the mathematical and engineering core of attention mechanisms, showing exactly how queries, keys, and values combine to create context-aware representations. You&#39;ll implement every component: scaled dot-product attention with numerical stability, multi-head parallel processing, sinusoidal positional encodings, encoder-decoder stacks with residual connections, and autoregressive generation with beam search. By the end, you&#39;ll understand not just how transformers work, but why each design decision was made and what happens when you change them.</p>\n<!-- MS_ID: transformer-scratch-m1 -->\n<h1 id=\"milestone-1-scaled-dot-product-attention\">Milestone 1: Scaled Dot-Product Attention</h1>\n<h2 id=\"where-we-are-in-the-transformer\">Where We Are in the Transformer</h2>\n<p><img src=\"/api/project/transformer-scratch/architecture-doc/asset?path=diagrams%2Fdiag-satellite-transformer.svg\" alt=\"Transformer Architecture Map\"></p>\n<p>You&#39;re building the smallest unit that makes transformers work: <strong>attention</strong>. Everything else in a transformer—multi-head processing, encoder stacks, decoding strategies—is built on top of this operation. Master this, and the rest is composition.</p>\n<hr>\n<h2 id=\"the-tension-why-attention-had-to-exist\">The Tension: Why Attention Had to Exist</h2>\n<p>Before 2017, sequence models faced an impossible tradeoff. Recurrent networks (RNNs, LSTMs) processed tokens one at a time, maintaining a hidden state that compressed everything seen so far. This created two fatal problems:</p>\n<ol>\n<li><p><strong>Information bottleneck</strong>: By the time an RNN reaches position 50, it has forgotten fine details about position 3. The hidden state is a lossy compression of the entire history.</p>\n</li>\n<li><p><strong>Sequential dependency</strong>: You cannot parallelize. Position 50 cannot be computed until positions 1-49 are done. On a GPU capable of 10^13 operations per second, you&#39;re bottlenecked by sequential dependencies.</p>\n</li>\n</ol>\n<p>The transformer paper asked: <em>What if every position could directly access every other position?</em></p>\n<p>The answer is attention—a mechanism that computes, for each position, a <strong>weighted average of all other positions</strong>. The weights aren&#39;t fixed; they&#39;re computed dynamically based on content relevance. Position 3 asking about &quot;the bank&quot; can attend strongly to position 47 where &quot;river&quot; appeared, and weakly to position 12 where &quot;money&quot; appeared.</p>\n<p><strong>The cost</strong>: Attention is O(n²) in sequence length. A 1000-token sequence requires computing 1,000,000 pairwise relationships. This is the fundamental tension—you get direct access to everything, but you pay quadratically for it.</p>\n<hr>\n<h2 id=\"the-revelation-attention-is-differentiable-dictionary-lookup\">The Revelation: Attention Is Differentiable Dictionary Lookup</h2>\n<p>Here&#39;s what most explanations get wrong.</p>\n<p><strong>What you might think</strong>: Attention is a neural network layer that learns mysterious relationships between tokens. It&#39;s a black box with trainable parameters that somehow discovers connections.</p>\n<p><strong>The reality</strong>: Attention is pure linear algebra with <strong>one learned component</strong>: the projections that create Q, K, and V. The core operation is entirely deterministic:</p>\n<p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p>\n<p><img src=\"/api/project/transformer-scratch/architecture-doc/asset?path=diagrams%2Fdiag-attention-qkv.svg\" alt=\"Query-Key-Value Decomposition\"></p>\n<p>Think of it as <strong>differentiable database lookup</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Database Concept</th>\n<th>Attention Equivalent</th>\n<th>Shape</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Query</strong></td>\n<td>What I&#39;m looking for</td>\n<td><code>[batch, seq_len, d_k]</code></td>\n</tr>\n<tr>\n<td><strong>Key</strong></td>\n<td>What I match against (indexed column)</td>\n<td><code>[batch, seq_len, d_k]</code></td>\n</tr>\n<tr>\n<td><strong>Value</strong></td>\n<td>What I retrieve (row data)</td>\n<td><code>[batch, seq_len, d_v]</code></td>\n</tr>\n</tbody></table>\n<p>In a database, <code>SELECT value FROM table WHERE key = query</code> returns exactly one row. Attention softens this: it returns a <strong>weighted sum of all values</strong>, where weights are high for keys matching the query and low for mismatching keys.</p>\n<p>The &quot;learning&quot; in attention is just learning:</p>\n<ul>\n<li><strong>Q projection</strong>: What to search for</li>\n<li><strong>K projection</strong>: What to be searchable by</li>\n<li><strong>V projection</strong>: What information to actually return</li>\n</ul>\n<p>Once you have Q, K, V, the attention computation itself has <strong>no parameters</strong>. It&#39;s matrix multiplication and softmax.</p>\n<hr>\n<h2 id=\"the-three-level-view-what39s-actually-being-computed\">The Three-Level View: What&#39;s Actually Being Computed</h2>\n<h3 id=\"level-1-the-operation-what-you-write\">Level 1 — The Operation (What You Write)</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Q </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> K.transpose(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> math.sqrt(d_k)  </span><span style=\"color:#6A737D\"># [batch, seq, seq]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">attention_weights </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> F.softmax(scores, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)       </span><span style=\"color:#6A737D\"># [batch, seq, seq]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention_weights </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> V                       </span><span style=\"color:#6A737D\"># [batch, seq, d_v]</span></span></code></pre></div>\n\n<p>Three lines. That&#39;s the entire attention mechanism.</p>\n<h3 id=\"level-2-the-math-soul-what39s-being-computed\">Level 2 — The Math Soul (What&#39;s Being Computed)</h3>\n<p>Let&#39;s trace dimensions for a single sequence (ignoring batch):</p>\n<ol>\n<li><p><strong>Query-Key similarity</strong>: <code>Q @ K^T</code> produces a <code>[seq_len, seq_len]</code> matrix</p>\n<ul>\n<li>Entry (i, j) = dot product of query_i and key_j</li>\n<li>High value = position i wants information from position j</li>\n</ul>\n</li>\n<li><p><strong>Scaling</strong>: Divide by <code>√d_k</code></p>\n<ul>\n<li>Prevents dot products from growing large (more on this shortly)</li>\n</ul>\n</li>\n<li><p><strong>Softmax normalization</strong>: Rows sum to 1</p>\n<ul>\n<li>Each position gets a probability distribution over all positions</li>\n</ul>\n</li>\n<li><p><strong>Weighted retrieval</strong>: <code>weights @ V</code></p>\n<ul>\n<li>Position i&#39;s output = weighted average of all values, weighted by attention</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"level-3-the-gradient-flow-what-flows-backward\">Level 3 — The Gradient Flow (What Flows Backward)</h3>\n<p><img src=\"/api/project/transformer-scratch/architecture-doc/asset?path=diagrams%2Fdiag-attention-gradient-flow.svg\" alt=\"Gradient Flow Through Attention\"></p>\n<p>When you call <code>loss.backward()</code>, gradients flow through attention in a specific pattern:</p>\n<ul>\n<li><p><strong>∂L/∂V</strong> is weighted by attention scores. If position 5 attended 80% to position 2&#39;s value, then 80% of position 5&#39;s error flows to position 2&#39;s value.</p>\n</li>\n<li><p><strong>∂L/∂Q</strong> and <strong>∂L/∂K</strong> flow through the softmax and dot product. Positions that were strongly attended to receive larger gradient signals.</p>\n</li>\n</ul>\n<p>This is why attention is powerful for training: <strong>gradient flows directly to relevant positions</strong>, not through a chain of recurrent states.</p>\n<hr>\n<h2 id=\"why-divide-by-d_k-the-softmax-saturation-problem\">Why Divide by √d_k? The Softmax Saturation Problem</h2>\n<p>Here&#39;s a detail that trips up everyone.</p>\n<p>The dot product <code>Q @ K^T</code> grows with dimension. If Q and K have 512 dimensions, each dot product is a sum of 512 terms. Assuming elements are roughly unit-variance, the dot product has variance ~512.</p>\n<p><strong>The problem</strong>: Softmax on large values produces near-one-hot distributions.</p>\n<p>$$\\text{softmax}([10, 20, 30]) \\approx [0.00002, 0.0001, 0.9999]$$</p>\n<p>This kills gradients. If softmax outputs are nearly 0 or 1, the gradient is nearly 0 everywhere—<strong>backpropagation stalls</strong>.</p>\n<p><strong>The solution</strong>: Scale by <code>1/√d_k</code> to normalize variance back to ~1.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Without scaling (d_k = 512)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Q </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> K.T  </span><span style=\"color:#6A737D\"># Values might range from -50 to +50</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">softmax(scores)   </span><span style=\"color:#6A737D\"># Nearly one-hot, gradients vanish</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># With scaling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (Q </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> K.T) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> math.sqrt(</span><span style=\"color:#79B8FF\">512</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Values range from -3 to +3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">softmax(scores)   </span><span style=\"color:#6A737D\"># Softer distribution, healthy gradients</span></span></code></pre></div>\n\n\n<p><img src=\"/api/project/transformer-scratch/architecture-doc/asset?path=diagrams%2Fdiag-scaled-dot-product.svg\" alt=\"Scaled Dot-Product Attention Computation\"></p>\n<p><strong>The math</strong>: If Q and K elements have variance σ², then <code>Q·K</code> has variance <code>d_k × σ²</code>. Dividing by <code>√d_k</code> gives variance <code>σ²</code> again.</p>\n<hr>\n<h2 id=\"masking-telling-attention-what-it-cannot-see\">Masking: Telling Attention What It Cannot See</h2>\n<p>Two types of masks solve two different problems:</p>\n<h3 id=\"padding-mask\">Padding Mask</h3>\n<p>Your batch has sequences of different lengths. You pad shorter ones with <code>&lt;PAD&gt;</code> tokens to form a rectangular tensor. But attention shouldn&#39;t attend to padding—it&#39;s meaningless.</p>\n<p><strong>Implementation</strong>: Set padding positions to <code>-inf</code> before softmax.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># mask: [batch, seq_len] where True = padding position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (tokens </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> PAD_ID</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># [batch, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> mask.unsqueeze(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)   </span><span style=\"color:#6A737D\"># [batch, 1, seq_len] for broadcasting</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores.masked_fill(mask, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'-inf'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># After softmax: -inf becomes 0.0</span></span></code></pre></div>\n\n<h3 id=\"causal-mask-for-decoders\">Causal Mask (For Decoders)</h3>\n<p>During generation, position 5 shouldn&#39;t see positions 6, 7, 8... (the future). The model must predict based only on what came before.</p>\n<p><strong>Implementation</strong>: Upper-triangular mask.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">seq_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores.size(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">causal_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.triu(torch.ones(seq_len, seq_len), </span><span style=\"color:#FFAB70\">diagonal</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">).bool()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># [[0, 1, 1, 1],</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#  [0, 0, 1, 1],</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#  [0, 0, 0, 1],</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#  [0, 0, 0, 0]]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores.masked_fill(causal_mask, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'-inf'</span><span style=\"color:#E1E4E8\">))</span></span></code></pre></div>\n\n\n<p><img src=\"/api/project/transformer-scratch/architecture-doc/asset?path=diagrams%2Fdiag-mask-application.svg\" alt=\"Padding and Causal Masking\"></p>\n<p><strong>Critical detail</strong>: Mask BEFORE softmax, not after. Softmax converts <code>-inf</code> to 0.0 cleanly. If you mask after softmax, you get 0s that don&#39;t sum to 1, breaking the probability interpretation.</p>\n<hr>\n<h2 id=\"softmax-numerical-stability\">Softmax Numerical Stability</h2>\n<p>Softmax has a hidden numerical pitfall. The formula:</p>\n<p>$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$</p>\n<p>If <code>x_i = 1000</code>, then <code>e^1000</code> overflows float32. The trick: <strong>subtract the maximum before exponentiating</strong>.</p>\n<p>$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$$</p>\n<p>This is mathematically identical (numerator and denominator scale by the same factor) but numerically stable.</p>\n<p><img src=\"/api/project/transformer-scratch/architecture-doc/asset?path=diagrams%2Fdiag-softmax-stability.svg\" alt=\"Softmax Numerical Stability\"></p>\n<p><strong>Good news</strong>: PyTorch&#39;s <code>F.softmax</code> does this automatically. But you must understand it because:</p>\n<ol>\n<li>You might implement softmax from scratch in other contexts</li>\n<li>The same principle applies to log-softmax for numerical stability</li>\n<li>Interviewers love this question</li>\n</ol>\n<hr>\n<h2 id=\"implementation-step-by-step\">Implementation: Step by Step</h2>\n<h3 id=\"step-1-q-k-v-projections\">Step 1: Q, K, V Projections</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ScaledDotProductAttention</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_model  </span><span style=\"color:#6A737D\"># For simplicity, d_k = d_v = d_model</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Three learned linear projections</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">W_Q</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> nn.Linear(d_model, d_model, </span><span style=\"color:#FFAB70\">bias</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">W_K</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> nn.Linear(d_model, d_model, </span><span style=\"color:#FFAB70\">bias</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">W_V</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> nn.Linear(d_model, d_model, </span><span style=\"color:#FFAB70\">bias</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x, mask</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # x: [batch, seq_len, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Q </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.W_Q(x)  </span><span style=\"color:#6A737D\"># [batch, seq_len, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        K </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.W_K(x)  </span><span style=\"color:#6A737D\"># [batch, seq_len, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        V </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.W_V(x)  </span><span style=\"color:#6A737D\"># [batch, seq_len, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # ... attention computation (next step)</span></span></code></pre></div>\n\n<h3 id=\"step-2-scaled-dot-product\">Step 2: Scaled Dot-Product</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x, mask</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Q </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.W_Q(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        K </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.W_K(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        V </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.W_V(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch_size, seq_len, d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Q.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute attention scores</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.matmul(Q, K.transpose(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">))  </span><span style=\"color:#6A737D\"># [batch, seq, seq]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> math.sqrt(d_k)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply mask (if provided)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> mask </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores.masked_fill(mask </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'-inf'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Softmax normalization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attention_weights </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.softmax(scores, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># [batch, seq, seq]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Weighted sum of values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.matmul(attention_weights, V)  </span><span style=\"color:#6A737D\"># [batch, seq, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> output, attention_weights</span></span></code></pre></div>\n\n<h3 id=\"step-3-mask-builders\">Step 3: Mask Builders</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_padding_mask</span><span style=\"color:#E1E4E8\">(tokens: torch.Tensor, pad_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Returns mask where padding positions are 0 (to be filled with -inf).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # tokens: [batch, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (tokens </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> pad_id).unsqueeze(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># [batch, 1, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> mask</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_causal_mask</span><span style=\"color:#E1E4E8\">(seq_len: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'cpu'</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Returns causal mask where future positions are 0.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.tril(torch.ones(seq_len, seq_len, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">device))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> mask.bool()</span></span></code></pre></div>\n\n<h3 id=\"step-4-verification-against-pytorch-reference\">Step 4: Verification Against PyTorch Reference</h3>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn.functional </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> F</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> verify_attention</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    torch.manual_seed(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch, seq_len, d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attention </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ScaledDotProductAttention(d_model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(batch, seq_len, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Our implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_custom, weights_custom </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # PyTorch reference (using same Q, K, V)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Q </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention.W_Q(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    K </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention.W_K(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    V </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention.W_V(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_ref </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> F.scaled_dot_product_attention(Q, K, V)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (output_custom </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> output_ref).abs().max().item()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Max difference: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">diff</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> diff </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Verification failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">diff</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Matches PyTorch reference within 1e-5\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<hr>\n<h2 id=\"common-pitfalls-debug-these-first\">Common Pitfalls (Debug These First)</h2>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shape error in <code>Q @ K^T</code></td>\n<td>Forgot transpose on K</td>\n<td><code>K.transpose(-2, -1)</code></td>\n</tr>\n<tr>\n<td>All attention weights ≈ 1/n</td>\n<td>Forgot scaling by √d_k</td>\n<td>Add <code>/ math.sqrt(d_k)</code></td>\n</tr>\n<tr>\n<td>NaN in output</td>\n<td>Masking after softmax</td>\n<td>Mask before softmax with <code>-inf</code></td>\n</tr>\n<tr>\n<td>Model attends to padding</td>\n<td>Padding mask not applied</td>\n<td>Check mask broadcasting</td>\n</tr>\n<tr>\n<td>Gradual gradient explosion</td>\n<td>No gradient clipping</td>\n<td>Add <code>torch.nn.utils.clip_grad_norm_</code></td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"knowledge-cascade-what-this-unlocks\">Knowledge Cascade: What This Unlocks</h2>\n<p>Now that you understand scaled dot-product attention, you&#39;ve unlocked:</p>\n<ol>\n<li><p><strong>Cross-attention (Milestone 4)</strong>: The decoder queries the encoder using the exact same mechanism. The only difference: Q comes from decoder, K and V come from encoder. Same math, different source.</p>\n</li>\n<li><p><strong>Database query analogy (cross-domain)</strong>: Q is your SQL <code>WHERE</code> clause, K is the indexed column, V is the <code>SELECT</code> columns. Attention is <code>SELECT V FROM table ORDER BY similarity(Q, K) DESC</code> with soft ranking.</p>\n</li>\n<li><p><strong>Attention heads as ensemble learners</strong>: Multi-head attention (next milestone) creates multiple Q, K, V projection sets. Each head learns different retrieval patterns—one might attend to syntactic dependencies, another to semantic similarity.</p>\n</li>\n<li><p><strong>Why sparse attention works</strong>: Longformer and BigBird prune attention to O(n) complexity. If attention is weighted retrieval, most weights are near-zero. Pruning low-attention connections preserves ~95% of information at ~10% of compute cost.</p>\n</li>\n<li><p><strong>Gradient flow intuition</strong>: When debugging training, remember—errors flow backward proportional to attention weights. If position 17 isn&#39;t learning, check if anything is attending to it.</p>\n</li>\n</ol>\n<hr>\n<p>[[CRITERIA_JSON: {&quot;milestone_id&quot;: &quot;transformer-scratch-m1&quot;, &quot;criteria&quot;: [&quot;Compute Q, K, V from input embeddings via three separate learned linear projections (nn.Linear)&quot;, &quot;Scaled dot-product computes softmax(QK^T / sqrt(d_k))V with correct output dimensions [batch, seq_len, d_v]&quot;, &quot;Padding mask sets attention weights to zero for padding tokens by applying -inf before softmax&quot;, &quot;Causal mask prevents attending to future positions by applying -inf to upper-triangular entries before softmax&quot;, &quot;Implementation is fully vectorized—no Python loops over batch or sequence dimensions&quot;, &quot;Numerical correctness verified: output matches PyTorch&#39;s F.scaled_dot_product_attention within 1e-5 tolerance on random inputs&quot;, &quot;Softmax numerical stability ensured by subtracting max before exponentiation (or verifying PyTorch&#39;s softmax does this)&quot;]}]</p>\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m2 -->\n<p>Let me read the existing architecture doc to understand the style and ensure consistency.</p>\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m3 -->\n<p>Let me read the existing architecture doc to understand the style and ensure consistency.</p>\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m4 -->\n<p>Let me read the existing architecture document to understand the style and ensure consistency.</p>\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m5 -->\n<p>Let me read the existing architecture document to understand the style and ensure consistency.</p>\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m6 -->\n<p>Let me first read the existing architecture document to understand the style and ensure consistency with previous milestones.I&#39;ll read the architecture document now.</p>\n<!-- END_MS -->\n\n\n\n\n<h1 id=\"tdd\">TDD</h1>\n<p>Implement the complete Transformer architecture (encoder-decoder) from first principles, including attention, positional encodings, training, and inference.</p>\n<!-- TDD_MOD_ID: transformer-scratch-m1 -->\n<p>Let me read the existing architecture document to understand the style and ensure consistency.</p>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m2 -->\n<p>I&#39;ll read the existing architecture document first to understand the style and ensure consistency.</p>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m3 -->\n<p>I&#39;ll read the existing architecture document to understand the style and ensure consistency with the previous milestones.</p>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m4 -->\n<p>I&#39;ll read the existing architecture document to understand the style and ensure consistency with the previous milestones.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">read_file</span><pre class=\"arch-pre shiki-highlighted\"><code>architecture-docs/transformer-scratch/index.md</code></pre></div>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m5 -->\n<p>I&#39;ll read the existing architecture document first to understand the style and ensure consistency with previous milestones.</p>\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m6 -->\n<p>I&#39;ll read the existing architecture document first to understand the style and ensure consistency.</p>\n<!-- END_TDD_MOD -->\n\n\n<p>I&#39;ll read the project structure to understand what exists and then identify the Deep Dive blocks and foundational concepts from the Atlas/TDD content provided.</p>\n","toc":[{"level":1,"text":"Transformer from Scratch","id":"transformer-from-scratch"},{"level":1,"text":"Milestone 1: Scaled Dot-Product Attention","id":"milestone-1-scaled-dot-product-attention"},{"level":2,"text":"Where We Are in the Transformer","id":"where-we-are-in-the-transformer"},{"level":2,"text":"The Tension: Why Attention Had to Exist","id":"the-tension-why-attention-had-to-exist"},{"level":2,"text":"The Revelation: Attention Is Differentiable Dictionary Lookup","id":"the-revelation-attention-is-differentiable-dictionary-lookup"},{"level":2,"text":"The Three-Level View: What&#39;s Actually Being Computed","id":"the-three-level-view-what39s-actually-being-computed"},{"level":3,"text":"Level 1 — The Operation (What You Write)","id":"level-1-the-operation-what-you-write"},{"level":3,"text":"Level 2 — The Math Soul (What&#39;s Being Computed)","id":"level-2-the-math-soul-what39s-being-computed"},{"level":3,"text":"Level 3 — The Gradient Flow (What Flows Backward)","id":"level-3-the-gradient-flow-what-flows-backward"},{"level":2,"text":"Why Divide by √d_k? The Softmax Saturation Problem","id":"why-divide-by-d_k-the-softmax-saturation-problem"},{"level":2,"text":"Masking: Telling Attention What It Cannot See","id":"masking-telling-attention-what-it-cannot-see"},{"level":3,"text":"Padding Mask","id":"padding-mask"},{"level":3,"text":"Causal Mask (For Decoders)","id":"causal-mask-for-decoders"},{"level":2,"text":"Softmax Numerical Stability","id":"softmax-numerical-stability"},{"level":2,"text":"Implementation: Step by Step","id":"implementation-step-by-step"},{"level":3,"text":"Step 1: Q, K, V Projections","id":"step-1-q-k-v-projections"},{"level":3,"text":"Step 2: Scaled Dot-Product","id":"step-2-scaled-dot-product"},{"level":3,"text":"Step 3: Mask Builders","id":"step-3-mask-builders"},{"level":3,"text":"Step 4: Verification Against PyTorch Reference","id":"step-4-verification-against-pytorch-reference"},{"level":2,"text":"Common Pitfalls (Debug These First)","id":"common-pitfalls-debug-these-first"},{"level":2,"text":"Knowledge Cascade: What This Unlocks","id":"knowledge-cascade-what-this-unlocks"},{"level":1,"text":"TDD","id":"tdd"}],"title":"Transformer from Scratch","markdown":"# Transformer from Scratch\n\nBuild the complete Transformer architecture—the foundation of GPT, BERT, T5, and virtually every modern language model—from first principles. This project takes you through the mathematical and engineering core of attention mechanisms, showing exactly how queries, keys, and values combine to create context-aware representations. You'll implement every component: scaled dot-product attention with numerical stability, multi-head parallel processing, sinusoidal positional encodings, encoder-decoder stacks with residual connections, and autoregressive generation with beam search. By the end, you'll understand not just how transformers work, but why each design decision was made and what happens when you change them.\n\n\n\n<!-- MS_ID: transformer-scratch-m1 -->\n# Milestone 1: Scaled Dot-Product Attention\n\n## Where We Are in the Transformer\n\n\n![Transformer Architecture Map](./diagrams/diag-satellite-transformer.svg)\n\n\nYou're building the smallest unit that makes transformers work: **attention**. Everything else in a transformer—multi-head processing, encoder stacks, decoding strategies—is built on top of this operation. Master this, and the rest is composition.\n\n---\n\n## The Tension: Why Attention Had to Exist\n\nBefore 2017, sequence models faced an impossible tradeoff. Recurrent networks (RNNs, LSTMs) processed tokens one at a time, maintaining a hidden state that compressed everything seen so far. This created two fatal problems:\n\n1. **Information bottleneck**: By the time an RNN reaches position 50, it has forgotten fine details about position 3. The hidden state is a lossy compression of the entire history.\n\n2. **Sequential dependency**: You cannot parallelize. Position 50 cannot be computed until positions 1-49 are done. On a GPU capable of 10^13 operations per second, you're bottlenecked by sequential dependencies.\n\nThe transformer paper asked: *What if every position could directly access every other position?*\n\nThe answer is attention—a mechanism that computes, for each position, a **weighted average of all other positions**. The weights aren't fixed; they're computed dynamically based on content relevance. Position 3 asking about \"the bank\" can attend strongly to position 47 where \"river\" appeared, and weakly to position 12 where \"money\" appeared.\n\n**The cost**: Attention is O(n²) in sequence length. A 1000-token sequence requires computing 1,000,000 pairwise relationships. This is the fundamental tension—you get direct access to everything, but you pay quadratically for it.\n\n---\n\n## The Revelation: Attention Is Differentiable Dictionary Lookup\n\nHere's what most explanations get wrong.\n\n**What you might think**: Attention is a neural network layer that learns mysterious relationships between tokens. It's a black box with trainable parameters that somehow discovers connections.\n\n**The reality**: Attention is pure linear algebra with **one learned component**: the projections that create Q, K, and V. The core operation is entirely deterministic:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\n\n![Query-Key-Value Decomposition](./diagrams/diag-attention-qkv.svg)\n\n\nThink of it as **differentiable database lookup**:\n\n| Database Concept | Attention Equivalent | Shape |\n|-----------------|---------------------|-------|\n| **Query** | What I'm looking for | `[batch, seq_len, d_k]` |\n| **Key** | What I match against (indexed column) | `[batch, seq_len, d_k]` |\n| **Value** | What I retrieve (row data) | `[batch, seq_len, d_v]` |\n\nIn a database, `SELECT value FROM table WHERE key = query` returns exactly one row. Attention softens this: it returns a **weighted sum of all values**, where weights are high for keys matching the query and low for mismatching keys.\n\nThe \"learning\" in attention is just learning:\n- **Q projection**: What to search for\n- **K projection**: What to be searchable by\n- **V projection**: What information to actually return\n\nOnce you have Q, K, V, the attention computation itself has **no parameters**. It's matrix multiplication and softmax.\n\n---\n\n## The Three-Level View: What's Actually Being Computed\n\n### Level 1 — The Operation (What You Write)\n\n```python\nscores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)  # [batch, seq, seq]\nattention_weights = F.softmax(scores, dim=-1)       # [batch, seq, seq]\noutput = attention_weights @ V                       # [batch, seq, d_v]\n```\n\nThree lines. That's the entire attention mechanism.\n\n### Level 2 — The Math Soul (What's Being Computed)\n\nLet's trace dimensions for a single sequence (ignoring batch):\n\n1. **Query-Key similarity**: `Q @ K^T` produces a `[seq_len, seq_len]` matrix\n   - Entry (i, j) = dot product of query_i and key_j\n   - High value = position i wants information from position j\n\n2. **Scaling**: Divide by `√d_k`\n   - Prevents dot products from growing large (more on this shortly)\n\n3. **Softmax normalization**: Rows sum to 1\n   - Each position gets a probability distribution over all positions\n\n4. **Weighted retrieval**: `weights @ V`\n   - Position i's output = weighted average of all values, weighted by attention\n\n### Level 3 — The Gradient Flow (What Flows Backward)\n\n\n![Gradient Flow Through Attention](./diagrams/diag-attention-gradient-flow.svg)\n\n\nWhen you call `loss.backward()`, gradients flow through attention in a specific pattern:\n\n- **∂L/∂V** is weighted by attention scores. If position 5 attended 80% to position 2's value, then 80% of position 5's error flows to position 2's value.\n\n- **∂L/∂Q** and **∂L/∂K** flow through the softmax and dot product. Positions that were strongly attended to receive larger gradient signals.\n\nThis is why attention is powerful for training: **gradient flows directly to relevant positions**, not through a chain of recurrent states.\n\n---\n\n## Why Divide by √d_k? The Softmax Saturation Problem\n\nHere's a detail that trips up everyone.\n\nThe dot product `Q @ K^T` grows with dimension. If Q and K have 512 dimensions, each dot product is a sum of 512 terms. Assuming elements are roughly unit-variance, the dot product has variance ~512.\n\n**The problem**: Softmax on large values produces near-one-hot distributions.\n\n$$\\text{softmax}([10, 20, 30]) \\approx [0.00002, 0.0001, 0.9999]$$\n\nThis kills gradients. If softmax outputs are nearly 0 or 1, the gradient is nearly 0 everywhere—**backpropagation stalls**.\n\n**The solution**: Scale by `1/√d_k` to normalize variance back to ~1.\n\n```python\n# Without scaling (d_k = 512)\nscores = Q @ K.T  # Values might range from -50 to +50\nsoftmax(scores)   # Nearly one-hot, gradients vanish\n\n# With scaling\nscores = (Q @ K.T) / math.sqrt(512)  # Values range from -3 to +3\nsoftmax(scores)   # Softer distribution, healthy gradients\n```\n\n\n![Scaled Dot-Product Attention Computation](./diagrams/diag-scaled-dot-product.svg)\n\n\n**The math**: If Q and K elements have variance σ², then `Q·K` has variance `d_k × σ²`. Dividing by `√d_k` gives variance `σ²` again.\n\n---\n\n## Masking: Telling Attention What It Cannot See\n\nTwo types of masks solve two different problems:\n\n### Padding Mask\n\nYour batch has sequences of different lengths. You pad shorter ones with `<PAD>` tokens to form a rectangular tensor. But attention shouldn't attend to padding—it's meaningless.\n\n**Implementation**: Set padding positions to `-inf` before softmax.\n\n```python\n# mask: [batch, seq_len] where True = padding position\nmask = (tokens == PAD_ID)  # [batch, seq_len]\nmask = mask.unsqueeze(1)   # [batch, 1, seq_len] for broadcasting\n\nscores = scores.masked_fill(mask, float('-inf'))\n# After softmax: -inf becomes 0.0\n```\n\n### Causal Mask (For Decoders)\n\nDuring generation, position 5 shouldn't see positions 6, 7, 8... (the future). The model must predict based only on what came before.\n\n**Implementation**: Upper-triangular mask.\n\n```python\nseq_len = scores.size(-1)\ncausal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n# [[0, 1, 1, 1],\n#  [0, 0, 1, 1],\n#  [0, 0, 0, 1],\n#  [0, 0, 0, 0]]\n\nscores = scores.masked_fill(causal_mask, float('-inf'))\n```\n\n\n![Padding and Causal Masking](./diagrams/diag-mask-application.svg)\n\n\n**Critical detail**: Mask BEFORE softmax, not after. Softmax converts `-inf` to 0.0 cleanly. If you mask after softmax, you get 0s that don't sum to 1, breaking the probability interpretation.\n\n---\n\n## Softmax Numerical Stability\n\nSoftmax has a hidden numerical pitfall. The formula:\n\n$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n\nIf `x_i = 1000`, then `e^1000` overflows float32. The trick: **subtract the maximum before exponentiating**.\n\n$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$$\n\nThis is mathematically identical (numerator and denominator scale by the same factor) but numerically stable.\n\n\n![Softmax Numerical Stability](./diagrams/diag-softmax-stability.svg)\n\n\n**Good news**: PyTorch's `F.softmax` does this automatically. But you must understand it because:\n1. You might implement softmax from scratch in other contexts\n2. The same principle applies to log-softmax for numerical stability\n3. Interviewers love this question\n\n---\n\n## Implementation: Step by Step\n\n### Step 1: Q, K, V Projections\n\n```python\nimport torch\nimport torch.nn as nn\nimport math\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.d_k = d_model  # For simplicity, d_k = d_v = d_model\n        \n        # Three learned linear projections\n        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n        self.W_K = nn.Linear(d_model, d_model, bias=False)\n        self.W_V = nn.Linear(d_model, d_model, bias=False)\n    \n    def forward(self, x, mask=None):\n        # x: [batch, seq_len, d_model]\n        \n        Q = self.W_Q(x)  # [batch, seq_len, d_model]\n        K = self.W_K(x)  # [batch, seq_len, d_model]\n        V = self.W_V(x)  # [batch, seq_len, d_model]\n        \n        # ... attention computation (next step)\n```\n\n### Step 2: Scaled Dot-Product\n\n```python\n    def forward(self, x, mask=None):\n        Q = self.W_Q(x)\n        K = self.W_K(x)\n        V = self.W_V(x)\n        \n        batch_size, seq_len, d_k = Q.shape\n        \n        # Compute attention scores\n        scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch, seq, seq]\n        scores = scores / math.sqrt(d_k)\n        \n        # Apply mask (if provided)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Softmax normalization\n        attention_weights = torch.softmax(scores, dim=-1)  # [batch, seq, seq]\n        \n        # Weighted sum of values\n        output = torch.matmul(attention_weights, V)  # [batch, seq, d_model]\n        \n        return output, attention_weights\n```\n\n### Step 3: Mask Builders\n\n```python\ndef create_padding_mask(tokens: torch.Tensor, pad_id: int) -> torch.Tensor:\n    \"\"\"Returns mask where padding positions are 0 (to be filled with -inf).\"\"\"\n    # tokens: [batch, seq_len]\n    mask = (tokens != pad_id).unsqueeze(1)  # [batch, 1, seq_len]\n    return mask\n\ndef create_causal_mask(seq_len: int, device: str = 'cpu') -> torch.Tensor:\n    \"\"\"Returns causal mask where future positions are 0.\"\"\"\n    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n    return mask.bool()\n```\n\n### Step 4: Verification Against PyTorch Reference\n\n```python\nimport torch.nn.functional as F\n\ndef verify_attention():\n    torch.manual_seed(42)\n    batch, seq_len, d_model = 2, 5, 64\n    \n    attention = ScaledDotProductAttention(d_model)\n    x = torch.randn(batch, seq_len, d_model)\n    \n    # Our implementation\n    output_custom, weights_custom = attention(x)\n    \n    # PyTorch reference (using same Q, K, V)\n    Q = attention.W_Q(x)\n    K = attention.W_K(x)\n    V = attention.W_V(x)\n    output_ref = F.scaled_dot_product_attention(Q, K, V)\n    \n    # Verify\n    diff = (output_custom - output_ref).abs().max().item()\n    print(f\"Max difference: {diff:.2e}\")\n    assert diff < 1e-5, f\"Verification failed: {diff}\"\n    print(\"✓ Matches PyTorch reference within 1e-5\")\n```\n\n---\n\n## Common Pitfalls (Debug These First)\n\n| Symptom | Likely Cause | Fix |\n|---------|--------------|-----|\n| Shape error in `Q @ K^T` | Forgot transpose on K | `K.transpose(-2, -1)` |\n| All attention weights ≈ 1/n | Forgot scaling by √d_k | Add `/ math.sqrt(d_k)` |\n| NaN in output | Masking after softmax | Mask before softmax with `-inf` |\n| Model attends to padding | Padding mask not applied | Check mask broadcasting |\n| Gradual gradient explosion | No gradient clipping | Add `torch.nn.utils.clip_grad_norm_` |\n\n---\n\n## Knowledge Cascade: What This Unlocks\n\nNow that you understand scaled dot-product attention, you've unlocked:\n\n1. **Cross-attention (Milestone 4)**: The decoder queries the encoder using the exact same mechanism. The only difference: Q comes from decoder, K and V come from encoder. Same math, different source.\n\n2. **Database query analogy (cross-domain)**: Q is your SQL `WHERE` clause, K is the indexed column, V is the `SELECT` columns. Attention is `SELECT V FROM table ORDER BY similarity(Q, K) DESC` with soft ranking.\n\n3. **Attention heads as ensemble learners**: Multi-head attention (next milestone) creates multiple Q, K, V projection sets. Each head learns different retrieval patterns—one might attend to syntactic dependencies, another to semantic similarity.\n\n4. **Why sparse attention works**: Longformer and BigBird prune attention to O(n) complexity. If attention is weighted retrieval, most weights are near-zero. Pruning low-attention connections preserves ~95% of information at ~10% of compute cost.\n\n5. **Gradient flow intuition**: When debugging training, remember—errors flow backward proportional to attention weights. If position 17 isn't learning, check if anything is attending to it.\n\n---\n\n[[CRITERIA_JSON: {\"milestone_id\": \"transformer-scratch-m1\", \"criteria\": [\"Compute Q, K, V from input embeddings via three separate learned linear projections (nn.Linear)\", \"Scaled dot-product computes softmax(QK^T / sqrt(d_k))V with correct output dimensions [batch, seq_len, d_v]\", \"Padding mask sets attention weights to zero for padding tokens by applying -inf before softmax\", \"Causal mask prevents attending to future positions by applying -inf to upper-triangular entries before softmax\", \"Implementation is fully vectorized—no Python loops over batch or sequence dimensions\", \"Numerical correctness verified: output matches PyTorch's F.scaled_dot_product_attention within 1e-5 tolerance on random inputs\", \"Softmax numerical stability ensured by subtracting max before exponentiation (or verifying PyTorch's softmax does this)\"]}]\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m2 -->\nLet me read the existing architecture doc to understand the style and ensure consistency.\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m3 -->\nLet me read the existing architecture doc to understand the style and ensure consistency.\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m4 -->\nLet me read the existing architecture document to understand the style and ensure consistency.\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m5 -->\nLet me read the existing architecture document to understand the style and ensure consistency.\n<!-- END_MS -->\n\n\n<!-- MS_ID: transformer-scratch-m6 -->\nLet me first read the existing architecture document to understand the style and ensure consistency with previous milestones.I'll read the architecture document now.\n<!-- END_MS -->\n\n\n\n\n# TDD\n\nImplement the complete Transformer architecture (encoder-decoder) from first principles, including attention, positional encodings, training, and inference.\n\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m1 -->\nLet me read the existing architecture document to understand the style and ensure consistency.\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m2 -->\nI'll read the existing architecture document first to understand the style and ensure consistency.\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m3 -->\nI'll read the existing architecture document to understand the style and ensure consistency with the previous milestones.\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m4 -->\nI'll read the existing architecture document to understand the style and ensure consistency with the previous milestones.\n\n```read_file\narchitecture-docs/transformer-scratch/index.md\n```\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m5 -->\nI'll read the existing architecture document first to understand the style and ensure consistency with previous milestones.\n<!-- END_TDD_MOD -->\n\n\n<!-- TDD_MOD_ID: transformer-scratch-m6 -->\nI'll read the existing architecture document first to understand the style and ensure consistency.\n<!-- END_TDD_MOD -->\n\n\nI'll read the project structure to understand what exists and then identify the Deep Dive blocks and foundational concepts from the Atlas/TDD content provided."}